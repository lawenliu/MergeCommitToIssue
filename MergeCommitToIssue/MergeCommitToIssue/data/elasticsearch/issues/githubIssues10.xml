<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Snapshot restore request should accept indices options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13357</link><project id="" key="" /><description>Fixes an issue introduced in #10744 as a result of which the restore request stopped accepting indices options such as ignore_unavailable.

Fixes #13335
</description><key id="104974734">13357</key><summary>Snapshot restore request should accept indices options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>review</label><label>v1.7.2</label><label>v2.0.0-beta2</label></labels><created>2015-09-04T21:42:47Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-11T17:52:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-09T15:23:49Z" id="138945463">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`org.elasticsearch/elasticsearch/jars/elasticsearch-1.7.1.jar` contains `org/joda/convert/ToString.class`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13356</link><project id="" key="" /><description>This looks like a mistake. With the rest of the embedded Joda libraries, you've prefixed the namespace with `org.elasticsearch`; however, there are two files that made their way through are are being bundled directly in `elasticsearch` itself:
- `org/joda/convert/FromString.class`
- `org/joda/convert/ToString.class`

This caused a packaging conflict for us, which we were able to resolve by telling the assembly system to pick the first of the two (at... random :/). That's not ideal.
</description><key id="104973816">13356</key><summary>`org.elasticsearch/elasticsearch/jars/elasticsearch-1.7.1.jar` contains `org/joda/convert/ToString.class`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">timcharper</reporter><labels><label>:Packaging</label><label>build</label><label>v1.7.2</label></labels><created>2015-09-04T21:34:30Z</created><updated>2015-09-05T17:20:07Z</updated><resolved>2015-09-05T17:18:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timcharper" created="2015-09-05T17:20:07Z" id="137978353">Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.base.Objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13355</link><project id="" key="" /><description>This commit removes and now forbids all uses of
`com.google.common.base.Objects` across the codebase. This is a small
step in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="104972414">13355</key><summary>Remove and forbid use of com.google.common.base.Objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-04T21:22:41Z</created><updated>2015-09-09T02:19:45Z</updated><resolved>2015-09-05T13:12:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-04T22:24:47Z" id="137870782">The PR says it forbids usage of this class but I don't see it in the diff?
</comment><comment author="jasontedor" created="2015-09-04T22:32:12Z" id="137871623">@jpountz My mistake. Thanks for noticing. I pushed 56898601fd835e4e72db867ef5c0bcf0a607476b to add that.
</comment><comment author="jpountz" created="2015-09-04T22:39:33Z" id="137872443">LGTM
</comment><comment author="nik9000" created="2015-09-05T02:22:25Z" id="137894903">LGTM
</comment><comment author="jasontedor" created="2015-09-05T14:18:15Z" id="137959842">Thank you for reviewing @jpountz and @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test rpm and deb signing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13354</link><project id="" key="" /><description>Adds a dummy gpg key to src/test/resources and uses it to sign the deb
and rpm every build. Then it tests that they are properly signed in the
vagrant tests.

Also switches out the chef vagrant images for those made by boxcutter. The
chef images seem to have disappeared off of the vagrant atlas.

Note: this changes some of the -D parameters used to sign packages and will
likely break the release process.

Closes #13182
</description><key id="104954506">13354</key><summary>Test rpm and deb signing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-04T19:24:38Z</created><updated>2015-11-18T19:35:39Z</updated><resolved>2015-11-18T19:35:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-04T20:55:46Z" id="137850316">@tlrx yet more bats to review.
</comment><comment author="spinscale" created="2015-09-08T12:24:57Z" id="138540907">I see different uses of `gpg.key` and `gpg.keyname` across different `pom.xml` and the release scripts. Should we unify that?
</comment><comment author="spinscale" created="2015-09-08T13:58:56Z" id="138568192">`mvn verify -Dskip.unit.tests -Dtests.vagrant` passed on my machine, looks good with the last minor thing being the different naming
</comment><comment author="nik9000" created="2015-09-08T17:58:23Z" id="138649096">&gt; I see different uses of gpg.key and gpg.keyname across different pom.xml and the release scripts. Should we unify that?

Huh! I hadn't done that search. It looks like `gpg.keyname` is used in dev-tools to sign its artifacts. Or, at least that is my guess. I can certainly change it to `gpg.key` to line up with what the deb and rpm modules do but I'd be making that change a bit bind because I don't know what that part of dev-tools's pom is supposed to do so I can't test it.
</comment><comment author="spinscale" created="2015-09-10T15:50:09Z" id="139287960">makes sense to use the same properties everywhere.. I already fixed it in the release script
</comment><comment author="nik9000" created="2015-09-10T15:51:07Z" id="139288224">&gt; makes sense to use the same properties everywhere.. I already fixed it in the release script

Got it. Will do.
</comment><comment author="nik9000" created="2015-09-10T17:42:37Z" id="139321857">@spinscale all done
</comment><comment author="spinscale" created="2015-09-11T07:54:49Z" id="139477178">left one last comment, apart from that LGTM. Awesome work, thank you so much @nik9000 !
</comment><comment author="nik9000" created="2015-09-14T19:54:29Z" id="140188947">I plan to grab this soon and rebase it which will squash away the merge conflicts.
</comment><comment author="nik9000" created="2015-09-16T15:16:26Z" id="140773185">Squashed and rebased. @spinscale would you mind having another look. There are lots of changes and I want to make sure it still looks sane to you.
</comment><comment author="nik9000" created="2015-09-22T14:27:24Z" id="142305182">Rebased to resolve merge conflicts. @spinscale, would you mind having a last look. There merge conflicts after your last LGTM and I wanted to make sure I resolved them sensibly.
</comment><comment author="nik9000" created="2015-10-06T15:05:11Z" id="145887130">Bumped from 2.0.0 and added to 2.2.0 as that branch is live. I'll try and resolve the conflicts later today. @spinscale , can you have another look at this when you get a chance?
</comment><comment author="nik9000" created="2015-10-06T16:34:38Z" id="145921178">Rebased clean.
</comment><comment author="spinscale" created="2015-10-08T07:18:48Z" id="146441280">ran this on fedora 20 and worked fine, left on last note on a requirement to install, but apart from that LGTM
</comment><comment author="nik9000" created="2015-11-18T19:27:25Z" id="157832394">Wow - I had no idea I hadn't merged this...... It just slipped my mind.
</comment><comment author="nik9000" created="2015-11-18T19:32:11Z" id="157834384">I'm going to recreate this against 2.x because master will need a gradle port.
</comment><comment author="nik9000" created="2015-11-18T19:35:35Z" id="157835233">Done: https://github.com/elastic/elasticsearch/pull/14846
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove use of underscore as an identifier</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13353</link><project id="" key="" /><description>As a refinement to Project Coin ([JEP-213](http://openjdk.java.net/jeps/213), [JDK-8042880](https://bugs.openjdk.java.net/browse/JDK-8042880)), Java 9 is going
to disallow the use of `_` as a one-character identifier. This will be
done by adding `_` as a keyword to the Java language ([JDK-8065599](https://bugs.openjdk.java.net/browse/JDK-8065599)).
Currently, uses of `_` as a one-character identifier are warnings in
the Java 8 compiler. This commit removes all uses of `_` as a
one-character identifier from the codebase.
</description><key id="104942177">13353</key><summary>Remove use of underscore as an identifier</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-04T18:21:11Z</created><updated>2015-09-05T17:18:32Z</updated><resolved>2015-09-04T18:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-04T18:24:02Z" id="137812198">Here's an example warning that shows up now after #13314:

```
'_' used as an identifier
(use of '_' as an identifier might not be supported in releases after Java SE 8)
```
</comment><comment author="nik9000" created="2015-09-04T18:24:10Z" id="137812227">LGTM. I don't believe we have the infrastruture to ban particular variable names set up but maybe we should think about it.
</comment><comment author="jasontedor" created="2015-09-04T18:27:33Z" id="137812878">@nik9000 We do not, but I'm not sure if it's worth the effort. In this particular case, when JDK 9 is stable enough for the CI infrastructure to start running builds against it again, those failing builds will be loud enough (assuming that the `_` proposal is actually merged into mainline JDK 9) to get our attention. Whether or not there is a general need for banning certain identifier names I'm not sure.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Agg filtering values using array of values works but not documented</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13352</link><project id="" key="" /><description>#6782 which I think was fixed in 1.5 series is not reflected in the current docs of https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-aggregations-bucket-terms-aggregation.html#_filtering_values.  Could borrow content from https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-facets-terms-facet.html#_excluding_terms
</description><key id="104930744">13352</key><summary>Terms Agg filtering values using array of values works but not documented</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">nezda</reporter><labels><label>:Aggregations</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-09-04T17:10:11Z</created><updated>2015-09-08T11:07:33Z</updated><resolved>2015-09-08T07:59:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-06T13:28:59Z" id="138085875">@markharwood looks like your doc changes in https://github.com/elastic/elasticsearch/pull/7529 got lost somehow?
</comment><comment author="markharwood" created="2015-09-07T15:33:18Z" id="138326125">@clintongormley Don't think I dropped anything here?

The opening paragraph says `based on regular expression strings or arrays of exact values.`
I'm not sure we need to provide an example of what an array of strings looks like here?
</comment><comment author="nezda" created="2015-09-07T16:27:42Z" id="138335594">Mea culpa - I didn't read text carefully and was looking for the example in separate section from regexes like it was in Facet docs (which I personally prefer).
</comment><comment author="clintongormley" created="2015-09-08T11:07:33Z" id="138519327">@markharwood turns out it was my fault - i removed the example you had from the 1.x docs.  Added back here: https://github.com/elastic/elasticsearch/commit/63f4af5090bd77c77b3cde0bc46df360d88107dc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Distance from [lon,lat] to geo_shape</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13351</link><project id="" key="" /><description>Currently, we can do distance math for filter (geo_distance) and sort (_geo_distance) from a given [lon,lat] to a geo_point.  We cannot do this currently for a [lon,lat] to the surface of a geo_shape object (distance "N").  There's a bit of a workaround for the geo_distance filter: you can create a circle of a specified radius M and check for an "intersects" relation.  However, there are some cases where this does not work (e.g. if M &gt; (N + [span of geo_shape]) it's now "within").  

A few examples:
- A vehicle is supposed to follow an indexed LineString / MultiLineString path and we want to find when it has deviated from the path by more than 100 meters [filter]
- Find the nearest landmark (geo-shape) to my present location [sort] 
</description><key id="104919402">13351</key><summary>Distance from [lon,lat] to geo_shape</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">eskibars</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2015-09-04T16:01:39Z</created><updated>2017-02-21T12:02:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Ghost93" created="2016-01-09T08:55:33Z" id="170211252">Hi,
any news on this?

We also need `geo_distance` to support `geo_shape` fields.
</comment><comment author="clintongormley" created="2016-01-28T14:16:10Z" id="176203069">@nknize any thoughts on this?
</comment><comment author="nknize" created="2016-02-01T04:13:15Z" id="177754031">This will likely follow the Lucene 5.5 release which includes the API and performance improvements needed to make this perform at scale.
</comment><comment author="akamanocha" created="2016-05-25T17:03:18Z" id="221640439">+1 for this. I guess es is now on 5.5 any plan to do this. 

Or suggested architecture/ workaround please?
</comment><comment author="imalygin" created="2016-07-27T20:30:02Z" id="235710801">Do you have any updates for this by any chance? 
</comment><comment author="clintongormley" created="2016-07-28T10:53:01Z" id="235863298">Not yet.  A new `geo` field type is being worked on which combines geo-point and geo-shape functionality, but it is early days still
</comment><comment author="jfitz1" created="2016-11-18T19:36:53Z" id="261621978">+1 
</comment><comment author="kaxil" created="2016-12-08T11:12:10Z" id="265715795">+1</comment><comment author="riconegri" created="2017-02-06T20:11:02Z" id="277798748">+1</comment><comment author="juanpujol" created="2017-02-06T20:11:30Z" id="277798851">&#128077; </comment><comment author="akamanocha" created="2017-02-21T12:02:59Z" id="281325174">Anything on this?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move repository-hdfs to elasticsearch repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13350</link><project id="" key="" /><description>We want to import existing code from `repository-hdfs` from https://github.com/elastic/elasticsearch-hadoop to elasticsearch repository as it's a core plugin.

What this PR bring:
- Copy from its origin source files and git history
- Move from gradle to maven
- Fix compilation issues
- Try to fix tests (unit tests are skipped for now)
- Integration tests are also skipped because of Jar Hell!

Still on the list:
- do we need to provide 3 artifacts (with hadoop1, hadoop2 and without hadoop)?
- why hadoop1 is marked as provided by default?
- fix jar hell
- fix integration tests
- fix unit tests
- check `.put("plugin.types", HdfsPlugin.class.getName())` which I believe does not exist anymore.

Would be nice to be able to start a hdfs mock cluster when running the REST IT so we could play with a HDFS fake repository and make sure everything is working as expected.

@costin I can also push that branch to elasticsearch repo so we can together iterate on the branch. What do you prefer?

For others, any feedback will be greatly appreciated.

Marking this for 2.0.0 as a blocker. @clintongormley feel free to remove the blocker if you think it's not necessary.
</description><key id="104918710">13350</key><summary>Move repository-hdfs to elasticsearch repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository HDFS</label></labels><created>2015-09-04T15:57:45Z</created><updated>2015-12-30T16:58:59Z</updated><resolved>2015-09-17T21:00:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-07T13:16:30Z" id="138295382">@costin I opened an issue in your repo. The plugin does not work with 2.0: https://github.com/elastic/elasticsearch-hadoop/issues/545
</comment><comment author="dadoonet" created="2015-09-07T20:37:10Z" id="138374513">@costin I updated the PR. I can now generate 3 hadoop plugins within the same module as you did with gradle:
- repository-hdfs containing hadoop1
- repository-hdfs-hadoop2 containing hadoop2
- repository-hdfs-light without hadoop

Note that the release script will may be have to be adapted because we create 3 files in target/releases

We still have JarHell issues but when installing the plugin:

```
   [plugin] ERROR: java.lang.IllegalStateException: jar hell!
   [plugin] class: org.objectweb.asm.AnnotationVisitor
   [plugin] jar1: /Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/plugins/repository-hdfs/target/integ-tests/elasticsearch-3.0.0-SNAPSHOT/lib/asm-5.0.4.jar
   [plugin] jar2: /var/folders/r_/r14sy86n2zb91jyz1ptb5b4w0000gn/T/6789445656019995176/temp_name-923855364/asm-3.1.jar
```

 Here is the content for now for each plugin:

## Hadoop1

```
 Archive:  target/releases/repository-hdfs-3.0.0-SNAPSHOT.zip
   Length     Date   Time    Name
  --------    ----   ----    ----
      2439  09-07-15 22:27   plugin-descriptor.properties
    147933  08-03-15 16:26   jersey-json-1.8.jar
     26514  10-27-14 17:18   stax-api-1.0.1.jar
    105134  10-30-14 13:55   jaxb-api-2.2.2.jar
     62983  10-25-14 15:12   activation-1.1.jar
    207271  08-03-15 16:26   jackson-core-asl-1.7.1.jar
     17820  08-03-15 16:26   jackson-jaxrs-1.7.1.jar
     31723  08-03-15 16:26   jackson-xc-1.7.1.jar
    694352  08-03-15 16:26   jersey-server-1.8.jar
     45024  10-25-14 15:19   hamcrest-core-1.3.jar
     62050  01-19-15 09:24   commons-logging-1.1.3.jar
    298829  08-03-15 16:26   commons-configuration-1.6.jar
    261809  10-25-14 15:25   commons-lang-2.4.jar
    143602  01-03-15 14:33   commons-digester-1.8.jar
    206035  08-03-15 16:26   commons-beanutils-core-1.8.0.jar
    539912  11-24-14 18:20   jetty-6.1.26.jar
   3566844  08-03-15 16:26   core-3.1.1.jar
   4203713  08-03-15 16:26   hadoop-core-1.2.1.jar
    282793  06-18-15 17:27   httpcore-4.3.3.jar
     14573  09-07-15 22:27   repository-hdfs-3.0.0-SNAPSHOT.jar
     15010  08-03-15 16:26   xmlenc-0.52.jar
     67758  10-30-14 13:54   jettison-1.1.jar
    890168  10-30-14 13:55   jaxb-impl-2.2.3-1.jar
     43033  08-03-15 16:26   asm-3.1.jar
    163151  10-25-14 15:22   commons-io-2.1.jar
    245039  10-25-14 15:19   junit-4.11.jar
    575389  10-25-14 15:20   commons-collections-3.2.1.jar
    188671  10-25-14 15:20   commons-beanutils-1.7.0.jar
    180792  08-03-15 16:26   commons-net-1.4.1.jar
    134133  10-25-14 15:20   servlet-api-2.5-20081211.jar
     76698  08-03-15 16:26   jasper-runtime-5.5.12.jar
    405086  08-03-15 16:26   jasper-compiler-5.5.12.jar
   1034049  07-10-15 11:40   ant-1.6.5.jar
    706710  08-03-15 16:26   hsqldb-1.8.0.10.jar
    458233  08-03-15 16:26   jersey-core-1.8.jar
     23346  10-30-14 13:55   stax-api-1.0-2.jar
    668564  08-03-15 16:26   jackson-mapper-asl-1.8.8.jar
    279781  01-03-15 14:33   commons-httpclient-3.0.1.jar
     58160  11-21-14 07:10   commons-codec-1.4.jar
    832410  08-03-15 16:26   commons-math-2.1.jar
     65261  10-25-14 15:20   oro-2.0.8.jar
    177131  11-24-14 18:20   jetty-util-6.1.26.jar
    134910  08-03-15 16:26   jsp-api-2.1-6.1.14.jar
    132368  07-03-15 09:30   servlet-api-2.5-6.1.14.jar
   1024680  08-03-15 16:26   jsp-2.1-6.1.14.jar
    112341  07-10-15 11:40   commons-el-1.0.jar
    321806  08-03-15 16:26   jets3t-0.6.1.jar
    592008  06-18-15 17:27   httpclient-4.3.6.jar
  --------                   -------
  20528039                   48 files
```

## Hadoop2

```
 Archive:  target/releases/repository-hdfs-hadoop2-3.0.0-SNAPSHOT.zipp
   Length     Date   Time    Name
  --------    ----   ----    ----
      2446  09-07-15 22:24   plugin-descriptor.properties
     17037  08-03-15 17:16   hadoop-annotations-2.4.1.jar
    303139  08-03-15 17:16   avro-1.7.4.jar
    241367  10-31-14 13:20   commons-compress-1.4.1.jar
     94672  10-31-14 13:20   xz-1.0.jar
    533455  02-23-15 17:16   protobuf-java-2.5.0.jar
     50525  08-03-15 17:16   hadoop-auth-2.4.1.jar
    592008  06-18-15 17:27   httpclient-4.3.6.jar
    487973  08-03-15 17:16   hadoop-mapreduce-client-app-2.4.1.jar
    662822  08-03-15 17:16   hadoop-mapreduce-client-common-2.4.1.jar
    107565  08-03-15 17:16   hadoop-yarn-client-2.4.1.jar
    130458  08-03-15 17:16   jersey-client-1.9.jar
   1493531  08-03-15 17:16   hadoop-mapreduce-client-core-2.4.1.jar
      2560  08-03-15 17:16   hadoop-client-2.4.1.jar
   1599627  08-03-15 17:16   commons-math3-3.1.1.jar
   6829695  08-03-15 17:16   hadoop-hdfs-2.4.1.jar
     62050  01-19-15 09:24   commons-logging-1.1.3.jar
      9752  12-16-14 15:01   slf4j-log4j12-1.6.2.jar
     29555  08-03-15 17:16   paranamer-2.3.jar
    995968  08-03-15 17:16   snappy-java-1.0.4.1.jar
    282793  06-18-15 17:27   httpcore-4.3.3.jar
     33015  10-25-14 15:12   jsr305-1.3.9.jar
    779974  08-03-15 17:16   zookeeper-3.4.5.jar
   1407445  08-03-15 17:16   hadoop-yarn-common-2.4.1.jar
    213670  08-03-15 17:16   hadoop-yarn-server-common-2.4.1.jar
     25680  08-03-15 17:16   hadoop-mapreduce-client-shuffle-2.4.1.jar
    105112  10-25-14 15:13   servlet-api-2.5.jar
     35726  08-03-15 17:16   hadoop-mapreduce-client-jobclient-2.4.1.jar
     14573  09-07-15 22:27   repository-hdfs-3.0.0-SNAPSHOT.jar
   2908722  08-03-15 17:16   hadoop-common-2.4.1.jar
   1638528  08-03-15 17:16   hadoop-yarn-api-2.4.1.jar
  --------                   -------
  21691443                   31 files
```

## Light

```
 Archive:  target/releases/repository-hdfs-light-3.0.0-SNAPSHOT.zip
   Length     Date   Time    Name
  --------    ----   ----    ----
      2444  09-07-15 22:24   plugin-descriptor.properties
     14573  09-07-15 22:27   repository-hdfs-3.0.0-SNAPSHOT.jar
    592008  06-18-15 17:27   httpclient-4.3.6.jar
    282793  06-18-15 17:27   httpcore-4.3.3.jar
     62050  01-19-15 09:24   commons-logging-1.1.3.jar
  --------                   -------
    953868                   5 files
```
</comment><comment author="dadoonet" created="2015-09-17T21:00:40Z" id="141227168">I'm closing this one for now. We will reopen a new PR when everything will be ready to move it to elasticsearch repo. cc @costin 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid the use of com.google.common.base.Predicate(s)?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13349</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.base.Predicate and com.google.common.base.Predicates
across the codebase. This is one of the many steps in the eventual
removal of Guava as a dependency. This was enabled by #13314.

Relates #13224
</description><key id="104917938">13349</key><summary>Remove and forbid the use of com.google.common.base.Predicate(s)?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-04T15:53:26Z</created><updated>2015-09-06T11:24:28Z</updated><resolved>2015-09-06T11:24:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-04T23:23:12Z" id="137877242">I'm generally a bit concerned about lambdas that they can easily hurt visibility but I think you did a good job at keeping things readable, I just left minor suggestions. LGTM
</comment><comment author="jpountz" created="2015-09-05T12:56:44Z" id="137952180">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MLT with IDs and Aliases mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13348</link><project id="" key="" /><description>Hi,

I have a question that could be a bug but I don't know if I am having the wrong expectation.

Suppose I have the following setup:

```
IndexA
    Doc1 {'text': 'lucene beta'}
    Doc2 {'text': 'lucene release'}
IndexB
    Doc3 {'text': 'elasticsearch beta'}
    Doc4 {'text': 'elasticsearch release'}
Alias all -&gt; IndexA, IndexB
```

When I query against the `all` alias like this:

``` json
{"query": {"mlt": {"like_text": "lucene beta", "min_term_freq": 1, "min_doc_freq": 2}}}
```

Documents `Doc1`, `Doc2` and `Doc3` are returned. Fine. But when I use the `ids` parameter like this:

``` json
{"query": {"mlt": {"ids": ["Doc1"], "min_term_freq": 1, "min_doc_freq": 2}}}
```

only `Doc2` is returned. The same happends when I am searching not with the `all` alias but with `test1,test2` as index.

I then tried to reproduce the issue as a MLT Integration test here: https://github.com/truemped/elasticsearch/commit/ba810e28d0c04d30ae584aa343a633aa730f6d3b which passes. So at this level the query returns both `Doc2` and `Doc3`.

Finally I created this Python script using the ES client that fails on `assert '3' in ids`:

``` python
from elasticsearch import Elasticsearch

docs = {'1': {'text': 'lucene beta'},
        '2': {'text': 'lucene release'},
        '3': {'text': 'elasticsearch beta'},
        '4': {'text': 'elasticsearch release'}}

e = Elasticsearch()
index_settings = {'settings': {'number_of_shards': 1, 'number_of_replicas': 0}}

e.indices.delete('test1')
e.indices.delete('test2')

e.indices.create('test1', body=index_settings)
e.indices.create('test2', body=index_settings)
e.indices.put_alias(name='all', index='test1')
e.indices.put_alias(name='all', index='test2')

e.index('test1', 'test', docs['1'], '1')
e.index('test1', 'test', docs['2'], '2')
e.index('test2', 'test', docs['3'], '3')
e.index('test2', 'test', docs['4'], '4')

e.indices.refresh('test1')
e.indices.refresh('test2')

query = {'query': {'mlt': {'like_text': 'lucene beta',
                           'min_doc_freq': 1,
                           'min_term_freq': 1}}}

r = e.search('all', body=query)
ids = [hit['_id'] for hit in r['hits']['hits']]

assert len(ids) == 3
assert sorted(ids) == ['1', '2', '3']

del(query['query']['mlt']['like_text'])
query['query']['mlt']['ids'] = ['1']

r = e.search('all', body=query)
ids = [hit['_id'] for hit in r['hits']['hits']]

assert '3' in ids
assert len(ids) == 2
assert sorted(ids) == ['2', '3']
```

Bug? Or bad expectations? Or a bad test?

Thanks for any feedback. Daniel
</description><key id="104917659">13348</key><summary>MLT with IDs and Aliases mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">truemped</reporter><labels /><created>2015-09-04T15:52:04Z</created><updated>2015-09-19T13:50:52Z</updated><resolved>2015-09-06T13:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-06T13:11:50Z" id="138085234">Hiya @truemped 

Bad expectations and thus bad test :) (btw, providing the recreation in curl/sense syntax makes it easier to run).  To explain:

```
PUT test1
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}
PUT test2
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

PUT test1/test/1
{
  "text": "lucene beta"
}
PUT test1/test/2
{
  "text": "lucene release"
}
PUT test2/test/3
{
  "text": "elasticsearch beta"
}
PUT test2/test/4
{
  "text": "elasticsearch release"
}

GET test1,test2/_validate/query?explain&amp;rewrite
{
  "query": {
    "mlt": {
      "like_text": "lucene beta",
      "min_doc_freq": 1,
      "min_term_freq": 1
    }
  }
}
```

returns:

```
"explanations": [
  {
     "index": "test1",
     "valid": true,
     "explanation": "_all:lucene _all:beta"
  },
  {
     "index": "test2",
     "valid": true,
     "explanation": "_all:beta"
  }
]
```

note the use of the `_all` field.

the version with the IDs:

```
GET test1,test2/_validate/query?explain&amp;rewrite
{
  "query": {
    "mlt": {
      "ids": ["1"],
      "min_doc_freq": 1,
      "min_term_freq": 1
    }
  }
}
```

returns: 

```
"explanations": [
  {
     "index": "test1",
     "valid": true,
     "explanation": "(text:lucene text:beta) -ConstantScore(_uid:test#1)"
  },
  {
     "index": "test2",
     "valid": true,
     "explanation": "() -ConstantScore(_uid:test#1)"
  }
]
```

In other words it tries get document `1` from each index, but `test2` doesn't return a doc and so nothing matches.  The `ids` syntax is deprecated in favour of the multi-get syntax:

```
GET test1,test2/_validate/query?explain&amp;rewrite
{
  "query": {
    "mlt": {
      "docs": [
        {
          "_index": "test1",
          "_type": "test",
          "_id": "1"
        }
      ],
      "min_doc_freq": 1,
      "min_term_freq": 1
    }
  }
}
```

the above request specifies the index and type for the document, so the correct document is used for both indices, returning:

```
"explanations": [
  {
     "index": "test1",
     "valid": true,
     "explanation": "(text:lucene text:beta) -ConstantScore(_uid:test#1)"
  },
  {
     "index": "test2",
     "valid": true,
     "explanation": "text:beta -ConstantScore(_uid:test#1)"
  }
]
```

Note: it excludes document `1`, and runs the query on the `text` field, not the `_all` field
</comment><comment author="truemped" created="2015-09-07T08:50:33Z" id="138241176">Hey Clinton,

thanks for the response. I don't really understand the difference between the `ids` and `multi get` query, even though the former one is deprecated now. My understanding of MLT in this case was a two phase algorithm, in which I ask ES to retrieve a certain document, create queries from it and search on the provided indices. Obviously it does so in the `multi get` version so I think this behavior is at least counter intuitive.

In my case for example the software querying ES might not even know in which index the document is stored. My use case for the `ids` or `multi get` is to save round trip time from the client. If I need to know the exact index in which the document is stored, the `ids/multi get` version is pretty useless to me?!

Does that make sense?
</comment><comment author="clintongormley" created="2015-09-07T14:24:01Z" id="138311329">&gt; My use case for the ids or multi get is to save round trip time from the client. If I need to know the exact index in which the document is stored, the ids/multi get version is pretty useless to me?!

get and multi-get don't work on aliases which point to multiple indices.  that's not get.  That's a search.  If you don't know which indices your documents are in, then you need to do a search instead. No way around it.
</comment><comment author="markmacmahon" created="2015-09-09T12:40:37Z" id="138896596">This is an unfortunate limitation of the MLT API as it means that everywhere we store Ids in our system, we also have to store knowledge of the index from which the Id resides.

While this is not a showstopper, it does cause extra complications downstream when we want to do  things like re-indexing without any downtime.

This approach seems inconsistent with the description of aliases in this part of the [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/guide/current/index-aliases.html).

&gt; An index alias is like a shortcut or symbolic link, which can point to one or more indices, 
&gt; and can be used in any API that expects an index name

@clintongormley  Have you got any suggestions on how this could be avoided in an efficient way?
</comment><comment author="clintongormley" created="2015-09-19T13:50:52Z" id="141670355">&gt; This approach seems inconsistent with the description of aliases in this part of the Elasticsearch Documentation.

This limitation is documented here: https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-index.html

&gt; While this is not a showstopper, it does cause extra complications downstream when we want to do things like re-indexing without any downtime.

Actually, reindexing is just fine.  With bulk responses, you get back the original `_index`, `_type`, and `_id` of every document, which you can use when reindexing.

&gt; Have you got any suggestions on how this could be avoided in an efficient way? 

If you don't know what index a document is in, then essentially you have to do a query for that document using the `_id` in a term filter. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES shards red status after old indices removing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13347</link><project id="" key="" /><description>Hi guys. We was removed old indices files and now ES dont work properly.
In ES log we find:
"failed to fetch index version after copying it over"
And we find solution - https://github.com/elastic/elasticsearch/issues/4674
But it didnt helped:)
Some shards are red. Also ES log file growing and growing and growing.
I never worked with ES before and have no idea how to fix it. 
http://localhost:9200/_refresh show
{"_shards":{"total":1192,"successful":1058,"failed":0}}
</description><key id="104903411">13347</key><summary>ES shards red status after old indices removing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iTonych</reporter><labels /><created>2015-09-04T14:41:31Z</created><updated>2015-09-07T05:15:28Z</updated><resolved>2015-09-06T13:15:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="iTonych" created="2015-09-04T14:51:44Z" id="137757661">[2015-09-04 11:50:48,452][WARN ][cluster.action.shard     ] [Laura Dean] [logstash-2015.04.24][1] sending failed shard for [logstash-2015.04.24][1], node[O4MVugiuQg2vMrBo93wEXQ], [P], s[INITIALIZING], indexUUID [WicD5EdOTUaPy3glrY34gg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2015.04.24][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[logstash-2015.04.24][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: [segments_1g]]; nested: FileNotFoundException[No such file [_9p6.si]]; ]]
</comment><comment author="iTonych" created="2015-09-04T15:59:17Z" id="137776767">1.4.0 elasticsearch version 
1.4.2 logstash verison
</comment><comment author="clintongormley" created="2015-09-06T13:15:14Z" id="138085364">Hi @iTonych 

The correct place to ask questions like these is in the forums: https://discuss.elastic.co/

It looks like the index is corrupt.  You should delete it.  Deleting via the API should work but failing that, just delete the shard on disk.  I also recommend upgrading.
</comment><comment author="iTonych" created="2015-09-07T05:15:28Z" id="138188535">Thanks @clintongormley ! It works)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid write outage when deploying to master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13346</link><project id="" key="" /><description>Whenever I do a rolling deployment there is a small spike in write errors when deploying to the current master node.  I get this error message:
Could not complete the operation due to- blocked by: [SERVICE_UNAVAILABLE/2/no master]

I'm guessing this means that some writes are timing out while waiting for a new master to be elected.  Is there anyway to force a master node election before brining down the current master node to avoid this?  Currently I call _all/_flush/synced and _cluster/nodes/_local/_shutdown before bringing down a node.
</description><key id="104898253">13346</key><summary>Avoid write outage when deploying to master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">laundrei</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-09-04T14:11:02Z</created><updated>2016-01-28T14:15:37Z</updated><resolved>2016-01-28T14:15:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-05T10:39:33Z" id="137941739">since 1.4, master election takes 3 seconds. During that time nodes will only serve reads but writes will stall, waiting for the election to complete. We could relatively easy add an API to force a new master election but that will still take 3s and the old master is likely to re-elected. We could, in theory, introduce a clean master hand-off to another node but this is tricky and I think that are other places we can improve and have more impact (for example, ideas to reduce the 3s duration). 

All that said, I wonder why you see the no master block. All write api should wait for 1 minute before timing out and other operations that require a master should timeout only after 30s. Both of which are much longer than the time it takes to elect a new master.
</comment><comment author="nik9000" created="2015-09-07T01:31:33Z" id="138152715">&gt; All that said, I wonder why you see the no master block. All write api should wait for 1 minute before timing out and other operations that require a master should timeout only after 30s. Both of which are much longer than the time it takes to elect a new master.

Do you mean [this](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#timeout)?
</comment><comment author="laundrei" created="2015-09-07T13:13:33Z" id="138294588">So my cluster consists of 3 dedicated master nodes and 30 data nodes.  When doing rolling deployments I deploy to the master nodes first and then the data node.  I only see that error when deploying to the master nodes.  

I didn't realize that master election is only supposed to take 3 seconds, perhaps it is taking longer than usual for some reason.  Btw I am using elastic search 1.7.1.  Also I am not specifying a timeout in the write requests to elastic search but my HTTP client will wait 30 seconds before timing out.  I'll look through the log files to see if I can figure out how long master election is taking.
</comment><comment author="bleskes" created="2015-09-07T13:47:19Z" id="138303945">@nik9000 yeah, I meant that for indexing requests.

@laundrei thank you for the extra info. let me know how long the election takes or if you find anything of interest in the logs.
</comment><comment author="clintongormley" created="2016-01-28T14:15:37Z" id="176202873">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check mapping compatibility ignored when upgrading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13345</link><project id="" key="" /><description>A 2.0 node starts even though there is a conflicting mapping.

Steps to reproduce:
1. Start a 1.7.1 node
2. Add an index with field type conflict and add a few documents:

```
PUT /test
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent",
        "fielddata": {
          "loading": "eager_global_ordinals"
        }
      }
    }
  }
}

PUT /test/parent/1
{}

PUT /test/child/1?parent=1
{}
```
1. Stop the 1.7.1 node
2. Start a 2.0.0-beta2-SNAPSHOT node
3. Two things can happen. 

a) It fails with an error that I can't update like this:

```
Exception in thread "main" java.lang.IllegalStateException: unable to upgrade the mappings for the index [test], reason: [Mapper for [_parent] conflicts with existing mapping in other types:
[mapper [_parent] is used by multiple types. Set update_all_types to true to update [fielddata] across all types.]]
Likely root cause: java.lang.IllegalArgumentException: Mapper for [_parent] conflicts with existing mapping in other types:
[mapper [_parent] is used by multiple types. Set update_all_types to true to update [fielddata] across all types.]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:345)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:296)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:242)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:331)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:113)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:226)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
```

b) The node starts with a conflicting mapping
1. If it fails to start then try to start start it a few more times and eventually the node will start even though a conflicting mapping is active.

The 2.0.0-beta2 node should never start with a conflicting mapping. I don't know yet what is causing this.
</description><key id="104887578">13345</key><summary>Check mapping compatibility ignored when upgrading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Upgrade API</label><label>blocker</label><label>bug</label></labels><created>2015-09-04T13:09:28Z</created><updated>2015-09-06T13:33:59Z</updated><resolved>2015-09-06T13:33:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-09-04T14:49:30Z" id="137757175">This is tricky one. In order to check the correctness of the mapping MetaDataIndexUpgradeService [loads and applies one type](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java#L329) at a time and checks it for conflicts with already loaded mappings. If it loads the parent first and then the child, it gets Mapper conflict exception as expected and fails to start the node. However, if it loads the child first and then parent, no exception is thrown and index gets successfully loaded and marked as upgraded to 2.0.0. The problem is that types are stored in map and their order is unpredictable and can change on over load. So, if somebody starts the node enough times, it will eventually get the child type before the parent type and upgrade the index. 

It's possible to simulate the issue by running the following script on 2.0.0-beta1, which recreates the conflicting mapping within 2.0.0-beta1.

```
curl -XDELETE "localhost:9200/test?pretty"
curl -XPUT "localhost:9200/test?pretty"
curl -XPUT "localhost:9200/test/child/_mapping?pretty" -d '{
    "child": {
        "_parent": {
            "type": "parent",
            "fielddata": {
                "loading": "eager_global_ordinals"
            }
        }
    }
}'
curl -XPUT "localhost:9200/test/parent/_mapping?pretty" -d '{
    "parent": {}
}'
curl -XGET "localhost:9200/test/_mapping?pretty"
```

If you try to put this mapping as a whole into 2.0.0 it fails:

```
curl -XDELETE "localhost:9200/test2?pretty"
curl -XPUT "localhost:9200/test2?pretty" -d '{
    "mappings": {
        "parent": {},
        "child": {
            "_parent": {
                "type": "parent",
                "fielddata": {
                    "loading": "eager_global_ordinals"
                }
            }
        }
    }
}'
```

We discussed it with @martijnvg and he suggested that it's a version of #13169. We might also want to look into making the mapping upgrade procedure more consistent. Maybe apply mappings in alphabetical order?
</comment><comment author="imotov" created="2015-09-04T19:02:18Z" id="137823129">After some research, it looks like applying types in alphabetical order will not really help us that much. If we could figure out initial type creation order we could have followed it every time we validate the mapping. But since this order is not preserved, there is little value in enforcing a completely different order later on. It looks like the only way to prevent the issue above is to make sure that mapper works the same way independently of the order of merged types. So, unless somebody has some better ideas I am going to close this issue.
</comment><comment author="clintongormley" created="2015-09-06T13:33:59Z" id="138086139">@imotov agreed.  I think this is only an issue for the `_parent` field, which should be dealt with in #13169
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to Jackson 2.6.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13344</link><project id="" key="" /><description>This pull request updates the Jackson dependencies from 2.5.3 to 2.6.2.

Changelog:
- https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.6
- https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.6.1
- https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.6.2

Jackson 2.6 provides very interesting filtering feature that works with JSON streaming, both at parsing and generating times. Once we move to this version we should be able to reimplement our `filter_path` feature (introduced by #10980) to a more understandable and maintainable implementation.

The test `TestMergeMapperTests.testConcurrentMergeTest` failed with jackson 2.6.1 with an issue similar to FasterXML/jackson-core#207. After being able [to reproduce the issue in a test](https://github.com/FasterXML/jackson-core/issues/207#issuecomment-138854278) a fix has been merged in FasterXML/jackson-core#213 and solved the issue.
</description><key id="104887177">13344</key><summary>Update to Jackson 2.6.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>review</label><label>upgrade</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-04T13:06:48Z</created><updated>2015-09-29T12:43:46Z</updated><resolved>2015-09-29T09:07:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-28T21:34:21Z" id="143880797">LGTM.
</comment><comment author="kimchy" created="2015-09-29T08:51:02Z" id="143992676">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] fix hanging testRefreshDoesNotMissShards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13343</link><project id="" key="" /><description>We waited for relocation to start on node_2 but never made sure node_1 knows about
this too. If node_1 does not yet have the cluster state that should trigger relocation
then relocation will never start because we block cluster state processing on node_1.
The log then fills with these messages:

1&gt; [2015-09-02 20:34:59,153][INFO ][discovery                ] sent: internal:index/shard/recovery/start_recovery, relocation starts
1&gt; [2015-09-02 20:34:59,153][DEBUG][indices.recovery         ] [node_t1] delaying recovery of [test][0] as it is not listed as assigned to target node

We have to wait until node_1 sends an acknowledgement that relocation starts
(response to internal:index/shard/recovery/start_recovery).

closes #13316
</description><key id="104880926">13343</key><summary>[test] fix hanging testRefreshDoesNotMissShards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-09-04T12:25:33Z</created><updated>2015-11-22T10:11:46Z</updated><resolved>2015-09-22T09:49:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-08T13:55:00Z" id="138567221">@bleskes can you take a look?
</comment><comment author="brwe" created="2015-09-15T12:13:29Z" id="140369869">We discussed before that it would be much better to have some sort of mock for the cluster service or use the listeners in cluster service to block particular cluster states instead of relying on fuzzy message passing and hoping we are quick enough with blocking. I wonder how much sense it makes to merge this pr if we decide to do this.
</comment><comment author="brwe" created="2015-09-15T13:14:15Z" id="140384277">Discussed with @s1monw and we might just want to remove the test. @bleskes let us know if you agree.
</comment><comment author="bleskes" created="2015-09-15T18:34:24Z" id="140492762">+1

&gt; On 15 Sep 2015, at 15:14, Britta Weber notifications@github.com wrote:
&gt; 
&gt; Discussed with @s1monw and we might just want to remove the test. @bleskes let us know if you agree.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] move heapdump to target/heapdump dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13342</link><project id="" key="" /><description>Each time you run mvn test, you end up creating a `.logs` empty dir.

Instead of writing for each plugin in the `./logs` dir, I'd recommend writing this in `./target/heapdump`.

This dir is cleanup automatically when you run `mvn clean`.
</description><key id="104877269">13342</key><summary>[build] move heapdump to target/heapdump dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-04T12:00:00Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-07T13:35:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-04T15:38:51Z" id="137771131">+1 for the idea.

I think there is an ant target that builds the directory over and over again. I see it fly by from time to time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix exception handling for unavailable shards in broadcast replication action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13341</link><project id="" key="" /><description>Before #13068 refresh and flush ignored all exceptions that matched
TransportActions.isShardNotAvailableException(e) and this should not change.
In addition, refresh and flush which are based on broadcast replication
might now get UnavailableShardsException from TransportReplicationAction if a shard
is unavailable and this is not caught by TransportActions.isShardNotAvailableException(e).
This must be ignored as well.
</description><key id="104851288">13341</key><summary>Fix exception handling for unavailable shards in broadcast replication action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-04T08:44:10Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-07T12:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-04T08:49:34Z" id="137681956">Good catch. left one question.
</comment><comment author="brwe" created="2015-09-04T09:26:19Z" id="137688087">pushed another commit
</comment><comment author="bleskes" created="2015-09-07T10:28:41Z" id="138265184">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem with geo_point and script_fields/fielddata_fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13340</link><project id="" key="" /><description>Hi,

Using fielddata_fields with geo_point raises an exception when running at least two instances of Elasticsearch.

To reproduce it : 
- run two instances of elasticsearch (on the same machine)
- run the gist : https://gist.github.com/clement-tourriere/8d06a0985f859666ae532
- You maybe need to run the search request twice to see the error

A patch could be : https://gist.github.com/clement-tourriere/2aa7219bd1da96393cbd

For more information, I have create a thread here (with another geo_point problem) : 
https://discuss.elastic.co/t/problems-with-geo-point-type-and-doc-values/28256.
</description><key id="104850307">13340</key><summary>Problem with geo_point and script_fields/fielddata_fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clement-tourriere</reporter><labels><label>:Fielddata</label><label>adoptme</label><label>bug</label><label>v2.0.0</label></labels><created>2015-09-04T08:37:57Z</created><updated>2016-03-28T10:50:36Z</updated><resolved>2015-10-08T10:56:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-04T19:12:42Z" id="137827791">Hi @clement-tourriere the gist you provided in here provides a 404 (looks like you added an extra "2" at the end of the URL).  https://gist.github.com/clement-tourriere/8d06a0985f859666ae53 should be the correct URL. Correct me if I'm wrong here please :)
</comment><comment author="clement-tourriere" created="2015-09-05T07:37:17Z" id="137924838">You're right, the correct link is the one without the '2' at the end. Sorry about that, I thank you for pointing this  out 
</comment><comment author="clintongormley" created="2015-09-06T13:44:43Z" id="138086480">Hi @clement-tourriere 

Thanks for the good recreation.  I can confirm that this is still broken in master.  Would you like to send a PR?
</comment><comment author="temsa" created="2016-02-11T11:13:13Z" id="182812522">@clintongormley Would it be possible seeing this patch to land in 1.x ?

Some features just are not there in 2.x and some people, like me, need to keep 1.x while being affected by this bug (in fact it's not exactly the same one, it affect `scripted_fields`, but I do believe this patch fixes it too, for the moment my workaround is far from nice...)
</comment><comment author="n0mer" created="2016-03-28T10:50:36Z" id="202339668">@clintongormley @temsa i'm also having this problem in ES 1.5.2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`span_containing` and `span_within` override default boost coming from lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13339</link><project id="" key="" /><description>SpanContainingQueryParser and SpanWithinQueryParser always set the boost to the parsed lucene query, even if it is the default one. The default boost of the main query though is the boost coming from the inner little (or big) query, value that we end up overriding all the time. We should instead set the boost to the main query only if it differs from the default, to mimic lucene's behaviour.

Relates to #13272
</description><key id="104847326">13339</key><summary>`span_containing` and `span_within` override default boost coming from lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-04T08:10:54Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-04T15:32:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-04T13:02:56Z" id="137730024">Indeed these span queries do weird things in the constructor. To be consistent, I suggest that we just multiply boosts?
</comment><comment author="javanna" created="2015-09-04T13:07:17Z" id="137730740">that was my second option... but my thinking here was to mimic lucene's behaviour. If we set a boost to a SpanWithinQuery that is what we get (no multiply done there I believe), otherwise we get the inner boost. So if we set a main boost in the dsl we should have the same, the inner boost simply gets replaced. Makes sense? 
</comment><comment author="jpountz" created="2015-09-04T14:00:11Z" id="137745015">ok, this is fair I think
LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problems with Apache reverse proxy and elasticsearch plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13338</link><project id="" key="" /><description>I've problems with mi reverse proxy configuration, i need not open 9200 port to the outside.

I put my apache configuration for the kibana side:

&lt;IfModule mod_proxy.c&gt;

  ProxyRequests On
  ProxyPreserveHost On

/# Redireccion para kibana
  ProxyPass "/logs" http://serverhostname:5601
  ProxyPassReverse "/logs" http://serverhostname:5601

/# Redireccion para elasticsearch

  ProxyPass "/els"  http://serverhostname:9200
  ProxyPassReverse "/els"  http://serverhostname:9200

/# Redireccion para Bigdesk

  ProxyPass "/bigdesk"  http://serverhostname:9200/_plugin/bigdesk _(my problems is here)_
  ProxyPassReverse "/bigdesk" http://serverhostname:9200/_plugin/bigdesk  _(my problems is here)_

&lt;/IfModule&gt;

/# Proxy options

&lt;Proxy http://serverhostname:5601&gt;
    Order Allow,Deny
    Allow from all
    AuthType Basic
    AuthName "LogServer"
    AuthUserFile /example/htpasswd.controlusers
    Require valid-user
&lt;/Proxy&gt;

&lt;Proxy http://serverhostname:9200&gt;
    Order Allow,Deny
    Allow from all
    AuthType Basic
    AuthName "ELS Server"
    AuthUserFile /example/htpasswd.controlusers
    Require valid-user
&lt;/Proxy&gt;

*I sustituded some parameters for the security og my company, don't worry.

When i try to open http://myserver/bigdesk  only works if i put the port :9200 but is just to need exclude, the other locations /els and /logs works correctly.

Thax so much.
</description><key id="104845840">13338</key><summary>Problems with Apache reverse proxy and elasticsearch plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dfaropennetwork</reporter><labels /><created>2015-09-04T07:57:18Z</created><updated>2016-05-19T14:15:26Z</updated><resolved>2015-09-06T12:39:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-06T12:39:54Z" id="138080519">Hi @dfaropennetwork 

Please ask questions like these on the forums instead http://discuss.elastic.co/

You don't actually say what the problem was, but possibly it is something to do with your CORS settings.  See the docs here https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html
</comment><comment author="dfaropennetwork" created="2015-09-07T07:02:35Z" id="138210685">OK thx i'll see the CORS setting.
</comment><comment author="dfaropennetwork" created="2015-11-21T01:51:27Z" id="158574204">Ok i resolved the problem thx so much.
</comment><comment author="michellaporte" created="2016-05-19T14:15:26Z" id="220337215">Hey @dfaropennetwork  What did you do to get this working?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test failure: org.elasticsearch.search.aggregations.pipeline.PercentilesBucketIT.testNested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13337</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_centos/7343/testReport/junit/org.elasticsearch.search.aggregations.pipeline/PercentilesBucketIT/testNested/

```
java.lang.IllegalStateException: percents must only contain non-null doubles from 0.0-100.0 inclusive
    at org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator$Factory.doValidate(PercentilesBucketPipelineAggregator.java:147)
    at org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory.validate(PipelineAggregatorFactory.java:66)
    at org.elasticsearch.search.aggregations.AggregatorFactories.validate(AggregatorFactories.java:108)
    at org.elasticsearch.search.aggregations.AggregatorFactory.validate(AggregatorFactory.java:78)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:221)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:103)
    at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:834)
```
</description><key id="104844431">13337</key><summary>Test failure: org.elasticsearch.search.aggregations.pipeline.PercentilesBucketIT.testNested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>jenkins</label></labels><created>2015-09-04T07:48:23Z</created><updated>2015-09-04T12:35:45Z</updated><resolved>2015-09-04T12:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-04T08:47:11Z" id="137681459">The issue is due to the fact that the parser keeps state. So if you pass some percents in a percentiles_bucket agg, and then create another percentiles_bucket agg whithout specifying percents, it will reuse the ones that were previously set. In that case, it reuses the percents from another test that makes sure that we fail when percents are out of range.

I will mark the test with @AwaitsFix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build]&#160;mvn javadoc:javadoc fails with Java 8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13336</link><project id="" key="" /><description>Based on @jprante comment here https://github.com/elastic/elasticsearch/pull/13314#issuecomment-137662468

&gt; Just a note: `mvn javadoc:javadoc` fails, because Java 8 javadoc performs a strict doclint check. See also http://openjdk.java.net/jeps/172
</description><key id="104840728">13336</key><summary>[build]&#160;mvn javadoc:javadoc fails with Java 8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>build</label></labels><created>2015-09-04T07:11:39Z</created><updated>2015-09-21T21:02:11Z</updated><resolved>2015-09-21T21:02:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>snapshot restore ignore_unavailable not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13335</link><project id="" key="" /><description>The `ignore_unavailable` option for snapshot restore documented [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_restore) does not appear to be working:

```
POST _snapshot/my_repo/curator-20150903160109/_restore
{
  "indices": "data-index-1",
  "ignore_unavailable": true,
  "include_global_state": false
}
```

```
{
   "error": "ElasticsearchIllegalArgumentException[failed to parse repository source [{\n  \"indices\": \"data-index-1\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\n]]; nested: ElasticsearchIllegalArgumentException[Unknown parameter ignore_unavailable]; ",
   "status": 400
}
```

Getting this error on 1.7.1, but it works on 1.4.5.
</description><key id="104809316">13335</key><summary>snapshot restore ignore_unavailable not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">qwerty4030</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2015-09-04T00:56:07Z</created><updated>2015-09-11T17:52:33Z</updated><resolved>2015-09-11T17:52:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Multiple wildcards don't seem to work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13334</link><project id="" key="" /><description>Tested v1.7.0 and v2.0.0-beta1

Put some data:

```
curl -XPOST localhost:9200/foo-one-baz/fizz -d '{}'
curl -XPOST localhost:9200/foo-two-baz/fizz -d '{}'
curl -XPOST localhost:9200/foo-three-baz/fizz -d '{}'
```

Ask about the indices:

```
# Against 1.7.0, "foo-*-*"
% curl localhost:9200/foo-*-*/_count 
{"count":3,"_shards":{"total":15,"successful":15,"failed":0}}

# Against 2.0.0-beta1, "foo-*-*" - doesn't find any results
% curl localhost:9200/foo-*-*/_count
{"count":0,"_shards":{"total":0,"successful":0,"failed":0}}

# Against 2.0.0-beta1, "foo-*" - finds results
% curl localhost:9200/foo-*/_count    
{"count":3,"_shards":{"total":15,"successful":15,"failed":0}}
```
</description><key id="104802663">13334</key><summary>Multiple wildcards don't seem to work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0-rc1</label></labels><created>2015-09-03T23:43:38Z</created><updated>2015-10-01T11:20:05Z</updated><resolved>2015-09-15T18:05:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nick125" created="2015-09-03T23:51:07Z" id="137603689">Looking at IndexNameExpressionResolver, it looks like it treats expressions that end in wildcards differently than expressions with wildcards elsewhere:

``` java
                } else if (expression.endsWith("*")) {
                    // Suffix wildcard:
                    assert expression.length() &gt;= 2 : "expression [" + expression + "] should have at least a length of 2";
                    String fromPrefix = expression.substring(0, expression.length() - 1);
                    char[] toPrefixCharArr = fromPrefix.toCharArray();
                    toPrefixCharArr[toPrefixCharArr.length - 1]++;
                    String toPrefix = new String(toPrefixCharArr);
                    matches = metaData.getAliasAndIndexLookup().subMap(fromPrefix, toPrefix);
                } else {
```

In this case, it will ignore any wildcards that occur elsewhere in the expression and not expand them, as evident by a query like such:

```
&gt;&gt; curl localhost:9200/foo-*-*z/_count
{"count":3,"_shards":{"total":15,"successful":15,"failed":0}}
```
</comment><comment author="xuzha" created="2015-09-14T19:23:02Z" id="140182189">I put a fix a few days ago, please let me know if this is the right thing to do.
</comment><comment author="martijnvg" created="2015-09-15T09:04:04Z" id="140329862">@xuzha I reviewed your PR and it is the right fix. Thanks for fixing this!
</comment><comment author="xuzha" created="2015-09-15T18:23:05Z" id="140489877">Thanks @martijnvg, pushed, master https://github.com/elastic/elasticsearch/commit/c6da8d5e133f5804c3387b954d1f2445039396ab, 2.x https://github.com/elastic/elasticsearch/commit/defe1de9e9db52c80d2f581b8f7d358631eec5cb, 2.0 https://github.com/elastic/elasticsearch/commit/0e0c89af547e7768cb89f450cf3afe7bb6f0a725
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split HasChildQueryParser into toQuery and formXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13333</link><project id="" key="" /><description>This is an intial commit that splits HasChildQueryParser / Builder into
the two seperate steps. This one is particularly nasty since it transports
a pretty wild InnerHits object that needs heavy refactoring. Yet, this commit
has still some nocommits and needs more tests and maybe another cleanup but
it's a start to get the code out there.
</description><key id="104785867">13333</key><summary>Split HasChildQueryParser into toQuery and formXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-03T21:30:58Z</created><updated>2015-09-08T15:26:47Z</updated><resolved>2015-09-08T15:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-04T10:08:50Z" id="137695407">I didi a first review round, especially on the query refactoring aspects, and left some comments. I'll have to dive a bit deeper into the inner_hits changes to better understand it.
</comment><comment author="s1monw" created="2015-09-06T20:18:02Z" id="138123118">@javanna I think it's ready can you give it another go
</comment><comment author="s1monw" created="2015-09-07T13:24:59Z" id="138297682">@cbuescher since luca is out can you take a look?
</comment><comment author="cbuescher" created="2015-09-08T10:26:02Z" id="138507174">Added one more minor comment, LGTM otherwise.
</comment><comment author="javanna" created="2015-09-08T12:44:31Z" id="138548792">did another round, went deeper into the inner_hits changes, they look good, left a few comments around those.
</comment><comment author="s1monw" created="2015-09-08T14:49:19Z" id="138590054">@javanna I fixed the SearchSourceBuilder/InnerHit issue in my last commit. I will for completeness record the JavaAPI break also
</comment><comment author="s1monw" created="2015-09-08T14:57:35Z" id="138592413">@javanna I rebased to use the latest master changes
</comment><comment author="javanna" created="2015-09-08T15:08:51Z" id="138595288">LGTM
</comment><comment author="s1monw" created="2015-09-08T15:09:23Z" id="138595441">w00t!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cloud-aws] signer type option does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13332</link><project id="" key="" /><description>Reported at: 
- https://github.com/elastic/elasticsearch-cloud-aws/issues/242
- https://github.com/elastic/elasticsearch-cloud-aws/issues/223

This option does not work for EC2 like API or S3 like API.

We need to find a way to fix it and also to test it. A simple way for manually testing could be by using Google Storage S3 compatible API: https://cloud.google.com/storage/docs/migrating?hl=en
</description><key id="104782103">13332</key><summary>[cloud-aws] signer type option does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>bug</label></labels><created>2015-09-03T21:06:20Z</created><updated>2015-09-08T05:08:50Z</updated><resolved>2015-09-08T05:08:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-09-08T01:59:15Z" id="138408234">@dadoonet  could we close this, is there any more work need to be done?
</comment><comment author="dadoonet" created="2015-09-08T05:08:50Z" id="138435930">Sure. Next time in your commit message, add "Closes #XXXX" and Github will close it for you. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`simple_query_string` overrides boost coming from lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13331</link><project id="" key="" /><description>SimpleQueryStringParser applies whatever boost the query holds, even if the default 1, to the query obtained from parsing of the query string. that might contain its boost, for instance if it resolved to a simple query like term (single term query against a single field). We should rather multiply the existing boost with the boost set to the query, same as we do in query_string

Relates to #13272
</description><key id="104775342">13331</key><summary>`simple_query_string` overrides boost coming from lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T20:31:06Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-04T14:59:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-04T12:54:56Z" id="137728616">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cloud-azure] Split azure plugin in 3 plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13330</link><project id="" key="" /><description>Until now we had a cloud-azure plugin which is providing 3 distinct features:
- discovery on Azure
- snapshot/restore on Aure
- SMB store

This commit splits the plugin by feature so people can use either one or the other or both features.

Doc is updated accordingly.
</description><key id="104774931">13330</key><summary>[cloud-azure] Split azure plugin in 3 plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>:Plugin Repository Azure</label><label>:Plugin Store SMB</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T20:29:00Z</created><updated>2016-03-10T18:55:07Z</updated><resolved>2015-09-21T17:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-15T17:50:04Z" id="140480350">LGTM. I left a few minor non-code comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>old plugin repos need a note that they have moved into the main ES repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13329</link><project id="" key="" /><description>Stuff like https://github.com/elastic/elasticsearch-cloud-aws is confusing, because it doesnt mention that the plugin moved to main ES repository for the 2.0 release. Users only see "master: build from source" and probably think we have done nothing at all.

I think we should have at the very top of the README files for all the plugins we have moved in, some note that points you to their new location for 2.0+
</description><key id="104771936">13329</key><summary>old plugin repos need a note that they have moved into the main ES repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>docs</label></labels><created>2015-09-03T20:09:39Z</created><updated>2015-09-07T07:59:36Z</updated><resolved>2015-09-07T07:59:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-03T20:46:19Z" id="137569642">I agree. I started this for analysis plugins (see https://github.com/elastic/elasticsearch-analysis-phonetic/commit/96bf53cb25c6accde96adc73e0295170d29d9b43) but I still need to do it for other repos...

Thanks for the heads up. Assigning this to me.
</comment><comment author="clintongormley" created="2015-09-06T12:28:42Z" id="138078861">I think we should probably disable github issues on each plugin repo, once the "see core" notice has been added to the README
</comment><comment author="dadoonet" created="2015-09-07T07:01:59Z" id="138210576">@clintongormley It makes sense. I'll do that.
</comment><comment author="dadoonet" created="2015-09-07T07:59:36Z" id="138220966">I updated and disabled issues for:
- https://github.com/elastic/elasticsearch-analysis-icu
- https://github.com/elastic/elasticsearch-analysis-stempel
- https://github.com/elastic/elasticsearch-analysis-kuromoji
- https://github.com/elastic/elasticsearch-analysis-phonetic
- https://github.com/elastic/elasticsearch-analysis-smartcn
- https://github.com/elastic/elasticsearch-lang-javascript
- https://github.com/elastic/elasticsearch-lang-python
- https://github.com/elastic/elasticsearch-lang-groovy (project STOPPED)
- https://github.com/elastic/elasticsearch-lang-mvel (project STOPPED)
- https://github.com/elastic/elasticsearch-river-rabbitmq (project STOPPED)
- https://github.com/elastic/elasticsearch-river-wikipedia (project STOPPED)
- https://github.com/elastic/elasticsearch-river-couchdb (project STOPPED)
- https://github.com/elastic/elasticsearch-river-twitter (project STOPPED)
- https://github.com/elastic/elasticsearch-transport-thrift (project STOPPED)
- https://github.com/elastic/elasticsearch-transport-memcached (project STOPPED)

I did not disable issues when a lot of issues are still opened. We first need to look at them and move them to elasticsearch repo if it makes sense. Only documentation has been updated:
- https://github.com/elastic/elasticsearch-cloud-aws
- https://github.com/elastic/elasticsearch-cloud-gce
- https://github.com/elastic/elasticsearch-cloud-azure
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove broadcast address check.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13328</link><project id="" key="" /><description>This was supposed to just help the user, in case they misconfigured something.
Broadcast is an ipv4 only thing, the only way you can really detect its a broadcast
address, is to look and see if an interface has that address as its broadcast address.

But we cannot trust that container interfaces won't have a crazy setup...

Closes #13327
</description><key id="104770946">13328</key><summary>Remove broadcast address check.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T20:03:22Z</created><updated>2015-09-14T17:14:30Z</updated><resolved>2015-09-03T20:15:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-03T20:14:11Z" id="137561711">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can't bind to all interfaces with some docker configurations?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13327</link><project id="" key="" /><description>At least in one of our jenkins servers we have:

```
docker0
        inet 172.17.42.1 netmask:255.255.0.0 broadcast:0.0.0.0 scope:site
        hardware 56:84:7A:FE:97:99
        MULTICAST mtu:1500 index:3
```

Is this normal? The fact it has a broadcast address like that (which looks totally bogus), means that if the user configures 0.0.0.0, we will fail. I can fix the logic to deal with it in several ways (the interface is not marked up, we can not do the check explicitly for a wildcard address) but if anyone understands this I'd love to know why this docker interface looks like that.
</description><key id="104768018">13327</key><summary>can't bind to all interfaces with some docker configurations?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T19:45:20Z</created><updated>2015-09-14T17:16:38Z</updated><resolved>2015-09-03T20:15:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-03T19:45:51Z" id="137555209">Relevant jenkins fail: http://build-us-00.elastic.co/job/es_g1gc_master_metal/16773/
</comment><comment author="drewr" created="2015-09-03T19:52:42Z" id="137556666">It's not only `docker0`, might be an issue with LXC in general. This is on Ubuntu 15.04 Vivid:

```
lxcbr0    Link encap:Ethernet  HWaddr 9e:ee:28:2e:2e:85
          inet addr:10.0.3.1  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::9cee:28ff:fe2e:2e85/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:371713 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 B)  TX bytes:29988240 (29.9 MB)
```
</comment><comment author="rmuir" created="2015-09-03T19:54:29Z" id="137557033">ok, its just intended as a safety check to prevent the user from making a mistake. I'll remove the broadcast check completely since apparently these containers don't understand how networking works.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The delete-by-query plugin wraps all queries in a `query` element</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13326</link><project id="" key="" /><description>The delete-by-query plugin wraps all queries in a `query` element, so the parsed queries are wrapperd in a QueryWrapperFilter in the end. This does not change the matching documents but prevents me from removing the `query` query which is deprecated (which takes a query and turns it into a filter).
</description><key id="104765074">13326</key><summary>The delete-by-query plugin wraps all queries in a `query` element</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Plugin Delete By Query</label><label>bug</label></labels><created>2015-09-03T19:29:32Z</created><updated>2015-10-27T14:15:34Z</updated><resolved>2015-10-27T14:15:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose lucene FingerprintFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13325</link><project id="" key="" /><description>I felt bad leaving this one out (not adding a factory) in https://github.com/elastic/elasticsearch/pull/13324, but I figure @markharwood probably already has ideas about what he wants to do.
</description><key id="104764046">13325</key><summary>Expose lucene FingerprintFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Analysis</label><label>adoptme</label><label>feature</label><label>low hanging fruit</label><label>v5.0.0-alpha2</label></labels><created>2015-09-03T19:24:03Z</created><updated>2016-04-20T20:10:56Z</updated><resolved>2016-04-20T20:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-09-03T20:25:21Z" id="137563950">Yep, I'd like to add a factory plus a pre-defined analyzer that chains fingerprint TokenFilter with lowercase, stopword filters etc as per [OpenRefine's fingerprint algo](https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade master to lucene 5.4-snapshot r1701068</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13324</link><project id="" key="" /><description>Lets get back to being able to iterate with lucene
</description><key id="104762464">13324</key><summary>Upgrade master to lucene 5.4-snapshot r1701068</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T19:14:37Z</created><updated>2015-09-30T01:58:02Z</updated><resolved>2015-09-03T19:28:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T19:17:32Z" id="137546485">LGTM
</comment><comment author="rmuir" created="2015-09-30T01:58:02Z" id="144257455">I backported this to 2.x just now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>create a script for lucene snapshot upgrades</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13323</link><project id="" key="" /><description>This is a little painful right now, as far as updating the sha's everywhere etc, which change every lucene snapshot.

I have the feeling it might cause a ton of jenkins fails etc if we don't make it a bit easier. i think it can just be a shell script that runs the correct maven commands (generate distribution/zip with `mvn package`, run license checker update, then for each plugin: generate its zip with `mvn package`, run license checker update), etc.
</description><key id="104757087">13323</key><summary>create a script for lucene snapshot upgrades</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>build</label></labels><created>2015-09-03T18:41:45Z</created><updated>2016-01-28T14:14:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-03T19:04:55Z" id="137543808">Now that #13322 is fixed this one should be relatively straightforward. I'll try to look into it if nobody beats me to it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>license checker's update adds es jars then gets mad about them later</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13322</link><project id="" key="" /><description>e.g. if i run it for analysis-kuromoji:

```
Adding analysis-kuromoji-3.0.0-SNAPSHOT.jar.sha1
Adding lucene-analyzers-kuromoji-5.4.0-snapshot-1701068.jar.sha1
Deleting lucene-analyzers-kuromoji-5.3.0.jar.sha1
SHAs updated
```

Now it will later fail because of analysis-kuromoji itself... is there something we can improve here?
</description><key id="104755495">13322</key><summary>license checker's update adds es jars then gets mad about them later</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T18:31:46Z</created><updated>2016-03-10T18:16:07Z</updated><resolved>2015-09-03T19:00:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-03T18:39:24Z" id="137537987">Thanks for raising that.

@clintongormley could we use the same ignore option as we have when checking files?
Is it only a documentation issue?

BTW may be we should add a new antrun job to help running that in an easier way than using the command line?
</comment><comment author="clintongormley" created="2015-09-03T19:02:21Z" id="137543248">@rmuir I changed the output which says "run this command to update the SHAs" to include the prefix of jars to ignore, eg in this case it'd be:

```
check_license_and_sha.pl --update plugins/analysis-kuromoji/licenses/ plugins/analysis-kuromoji/target/releases/analysis-kuromoji-2.0.0-beta2-SNAPSHOT.zip analysis-kuromoji
```
</comment><comment author="rmuir" created="2015-09-03T19:03:52Z" id="137543570">thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cleanup] remove non needed resources</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13321</link><project id="" key="" /><description>I removed here in the first commit:
- core/src/main/assemblies/common-bin.xml
- core/src/packaging/common/scripts/postrm

And in a second one (in case I'm wrong):
- core/src/test/resources/jmeter/*

I think it should go to 2.0, 2.x and master.
</description><key id="104753531">13321</key><summary>[cleanup] remove non needed resources</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T18:19:42Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-03T20:41:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T19:31:36Z" id="137550131">LGTM
</comment><comment author="rmuir" created="2015-09-03T19:39:03Z" id="137553365">nuke it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RestUtils.decodeQueryString does not correctly handle fragments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13320</link><project id="" key="" /><description>For example, this is a simple unit test that fails:

``` java
public void testCrazyURL() {
    Map&lt;String, String&gt; params = newHashMap();

    // This is a valid URL
    String uri = "example.com/:@-._~!$&amp;'()*+,=;:@-._~!$&amp;'()*+,=:@-._~!$&amp;'()*+,==?/?:@-._~!$'()*+,;=/?:@-._~!$'()*+,;==#/?:@-._~!$&amp;'()*+,;=";
    RestUtils.decodeQueryString(uri, uri.indexOf('?') + 1, params);
    assertThat(params.get("/?:@-._~!$'()* ,;"), equalTo("/?:@-._~!$'()* ,;=="));
    assertThat(params.size(), equalTo(1));
}
```
</description><key id="104749579">13320</key><summary>RestUtils.decodeQueryString does not correctly handle fragments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:REST</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-09-03T17:54:58Z</created><updated>2015-09-21T20:19:00Z</updated><resolved>2015-09-21T20:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>if an index cannot be created, its bogusly retried in a loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13319</link><project id="" key="" /><description>This sucks horribly when upgrading lucene, because people scatter asserts in the code to remind us to upgrade things, but then tests just silently hang.

If there is an exception e.g. coming from IndexShard, then why in the hell does it do this?
</description><key id="104748953">13319</key><summary>if an index cannot be created, its bogusly retried in a loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Cluster</label><label>bug</label><label>PITA</label></labels><created>2015-09-03T17:51:03Z</created><updated>2015-11-11T09:39:22Z</updated><resolved>2015-11-11T09:39:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T21:27:58Z" id="142113357">I theorize that the trouble was [an assert in IndexShard](https://github.com/elastic/elasticsearch/commit/f216d92d19cf2f12fbc460c7aef66f38998ef9fd#diff-3d9da880b4a90ac1e9545d4039d5c4e6L257) and I added the assert back. Then I ran a random test, `CommonTermsQueryParserTests`, and it took 30 seconds for it to fail. It timed out waiting for the cluster to go green.

@rmuir , is that what you were seeing?
</comment><comment author="rmuir" created="2015-09-21T21:38:42Z" id="142116975">yeah thats it. and you see that timed out failure versus the assert. so when those asserts go in "remove this on upgrade to lucene X", they do not have the desired impact.
</comment><comment author="nik9000" created="2015-09-21T21:58:46Z" id="142122223">Part of the problem is that we've set `es.logger.level` to `ERROR` but the error is being logged at `WARN`:

```
  1&gt; [2015-09-21 17:56:04,262][WARN ][indices.cluster          ] [node_s_0] [[test-index][0]] marking and sending shard failed due to [failed to create shard]
  1&gt; [test-index][[test-index][0]] ElasticsearchException[failed to create shard]; nested: AssertionError;
  1&gt;    at org.elasticsearch.index.IndexService.createShard(IndexService.java:376)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:643)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:543)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:178)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:496)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.lang.AssertionError
  1&gt;    at org.elasticsearch.index.shard.IndexShard.&lt;init&gt;(IndexShard.java:250)
  1&gt;    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  1&gt;    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  1&gt;    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  1&gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
  1&gt;    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
  1&gt;    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
  1&gt;    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:110)
  1&gt;    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
  1&gt;    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
  1&gt;    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
  1&gt;    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
  1&gt;    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
  1&gt;    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
  1&gt;    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
  1&gt;    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
  1&gt;    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
  1&gt;    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
  1&gt;    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
  1&gt;    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:162)
  1&gt;    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
  1&gt;    at org.elasticsearch.index.IndexService.createShard(IndexService.java:374)
  1&gt;    ... 9 more
```

Is there a good reason not to set `es.logger.level` to `WARN`?

The other issue I see is that we wait 30 seconds before giving up on the cluster ever getting into the desired state.
</comment><comment author="rjernst" created="2015-09-21T22:37:50Z" id="142129026">I think the problem is one of delayed action. AFAIK, "creating an index" starts by just putting something in the cluster state that says "there is a new uninitialized index" and returning. Then the master goes off and does allocation and such, and fails. In lieu of fixing the real issue (synchronous index creation), it seems like there the cluster health with waitFor request should fail once it is impossible to reach that state. So eg the index creation fails, it should return with a failure right away, not continue waiting for the timeout.
</comment><comment author="nik9000" created="2015-09-21T22:48:12Z" id="142130584">&gt; I think the problem is one of delayed action. AFAIK, "creating an index" starts by just putting something in the cluster state that says "there is a new uninitialized index" and returning. Then the master goes off and does allocation and such, and fails. In lieu of fixing the real issue (synchronous index creation), it seems like there the cluster health with waitFor request should fail once it is impossible to reach that state. So eg the index creation fails, it should return with a failure right away, not continue waiting for the timeout.

I believe your assessment is correct. I started debugging around this afternoon looking for something to put a watch on.
</comment><comment author="bleskes" created="2015-09-22T08:06:57Z" id="142207830">&gt; Then the master goes off and does allocation and such, and fails.

Small correction - the master doesn't fail to allocate (typically), but rather the node that receives the cluster state and goes and allocates the shard fails in creating it and notifies the master about it. This is where the asynchronicity  comes from.

&gt; it seems like there the cluster health with waitFor request should fail once it is impossible to reach that state

This is trickier than it seems - how do you define that it is impossible to reach a state in  30s? maybe there is a network hick up that caused a node to drop off the cluster and it will be back? Maybe , like in our tests, we are busy forming a cluster and the nodes will join in 10 seconds?

The problem with this issue is we currently don't keep track of all the places we tried to create a shard and it failed. The master sees the shard fails message from a node, and tries another. If that fails it may go back to the first one. If you only have one node, the master will first clean the shard by sending the node a state where the shard is not assigned to it and then will try again. We should find a place to keep track of this and be able to say "we tried all options so far, so no point in trying again".  Perhaps the new unassigned info can be extended to keep "tried and failed" node list, which will be cleared upon successful allocation (because unassigned info goes away) and will be used to stop this infinite loop.
</comment><comment author="nik9000" created="2015-09-22T13:43:26Z" id="142293179">&gt; The problem with this issue is we currently don't keep track of all the places we tried to create a shard and it failed. The master sees the shard fails message from a node, and tries another. If that fails it may go back to the first one. If you only have one node, the master will first clean the shard by sending the node a state where the shard is not assigned to it and then will try again. We should find a place to keep track of this and be able to say "we tried all options so far, so no point in trying again". Perhaps the new unassigned info can be extended to keep "tried and failed" node list, which will be cleared upon successful allocation (because unassigned info goes away) and will be used to stop this infinite loop.

At that point you could indeed "give up" on things - so long as you knew that you weren't going to add new nodes to the cluster which would give it more chances. But something like this seems like a long and windy journey. A journey with greatness at the end, for sure.

Minimally I think we should make sure that the actual failure is logged, even if we can't timeout eagerly. Could we set `es.logger.level` to `WARN` or even `INFO` for the tests? I liked the old days better, when tests were noisy and we used randomized testing to keep them quiet if they passed but they spit out a kajillion lines of useful logs if they failed.

When we timeout waiting for the cluster to go green can we attach the last hand full of errors as "potential causes"?
</comment><comment author="rjernst" created="2015-09-28T05:58:25Z" id="143646403">IMO definitely not `INFO`. We moved away from that because it is way too verbose when running tests. When tests fail, you want to see the failure. I would be ok with `WARN`. I have the gradle branch set to this, because I found there are some errors sometimes emitted as a warning (not sure why...we should fix that too but I've already forgot what the error was...).
</comment><comment author="s1monw" created="2015-11-11T09:39:22Z" id="155716770">I added only the assertion back in master and ran `IndexShardTests.java` with this output:

```
[2015-11-11 10:36:26,734][INFO ][org.elasticsearch.index.shard] --&gt; idxPath: [/private/var/folders/qj/rsr2js6n275f3r88r1z5bbgw0000gn/T/org.elasticsearch.index.shard.IndexShardTests_A92392023D5F82E0-001/OibDuggcVx]
[2015-11-11 10:36:26,940][INFO ][org.elasticsearch.cluster.metadata] [node_s_0] [test] creating index, cause [api], templates [], shards [1]/[0], mappings []
[2015-11-11 10:36:27,042][WARN ][org.elasticsearch.indices.cluster] [node_s_0] [[test][0]] marking and sending shard failed due to [failed to create shard]
java.lang.AssertionError
    at org.elasticsearch.index.shard.IndexShard.&lt;init&gt;(IndexShard.java:264)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:279)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:627)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:527)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:517)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-11 10:36:27,121][WARN ][org.elasticsearch.cluster.action.shard] [node_s_0] [test][0] received shard failed for [test][0], node[tTPyckqMRDaf8clvc9SzOA], [P], v[1], s[INITIALIZING], a[id=OlU9n_QSS9OXBvk_TgNzeA], unassigned_info[[reason=INDEX_CREATED], at[2015-11-11T09:36:26.943Z]], indexUUID [6qDGq_IkToCsYtCvoKMWUg], message [failed to create shard], failure [AssertionError[null]]
java.lang.AssertionError
    at org.elasticsearch.index.shard.IndexShard.&lt;init&gt;(IndexShard.java:264)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:279)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:627)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:527)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:517)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

you will now see this in master once per shard but after 30 sec the test times out and all is well. This is purely caused by guice which is gone in master. I am closing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Workaround pitfall in Java 8 target-type inference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13318</link><project id="" key="" /><description>Target-type inference has been improved in Java 8. This leads to these
lines now being interpreted as invoking `String#valueOf(char[])` whereas
they previously were interpreted as invoking `String#valueOf(Object)`.
This change leads to `ClassCastException`s during test execution. Simply
casting the parameter to `Object` restores the old invocation.

Closes #13315
</description><key id="104739867">13318</key><summary>Workaround pitfall in Java 8 target-type inference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T17:02:46Z</created><updated>2016-03-10T18:39:35Z</updated><resolved>2015-09-03T17:10:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-03T17:05:10Z" id="137512632">looks good, thanks. I had to do this in other places but didn't look into this one.
</comment><comment author="rmuir" created="2015-09-03T17:05:52Z" id="137512779">oh and of course, we should remove the awaitsfix :)
</comment><comment author="jasontedor" created="2015-09-03T17:29:28Z" id="137520980">Yeah, the problem here is that `String.valueOf` has so many overloads that it doesn't lead to a compile-time failure like some of the ones that you had to change, but instead a runtime failure.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What is core/src/main/assemblies/common-bin.xml doing?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13317</link><project id="" key="" /><description>Is this just dead stuff? what do we need this for. distribution logic is all in distribution/....

At least if we need to keep it for some screwed up reason, can we avoid listing all the dependencies again?
</description><key id="104734591">13317</key><summary>What is core/src/main/assemblies/common-bin.xml doing?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T16:30:08Z</created><updated>2016-03-10T18:16:07Z</updated><resolved>2015-09-03T20:41:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-03T17:02:49Z" id="137512148">I probably forgot to remove it. 
</comment><comment author="rmuir" created="2015-09-03T17:04:22Z" id="137512486">cool, if we don't need it, lets nuke it. then its one less place when managing dependencies.
</comment><comment author="dadoonet" created="2015-09-03T17:13:34Z" id="137515975">I'll remove it. I think it should be fixed in 2.0, 2.x and master.
</comment><comment author="dadoonet" created="2015-09-03T18:08:54Z" id="137531099">Sounds like https://github.com/elastic/elasticsearch/blob/master/core/src/packaging/common/scripts/postrm does not make sense either...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DiscoveryWithServiceDisruptionsIT frequently hangs completely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13316</link><project id="" key="" /><description>Looks like this:

HEARTBEAT J3 PID(22805@beast): 2015-09-03T12:01:58, stalled for  891s at: DiscoveryWithServiceDisruptionsIT.testReadOnPostRecoveryShards

Its a big time-waster when trying to get changes in, because it forces me to totally restart mvn verify.
</description><key id="104731387">13316</key><summary>DiscoveryWithServiceDisruptionsIT frequently hangs completely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>test</label></labels><created>2015-09-03T16:13:09Z</created><updated>2015-09-22T09:49:56Z</updated><resolved>2015-09-22T09:49:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-03T16:30:47Z" id="137504077">I muted the test for now. sorry, I did not get around to it today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>InnerHitsIT has java 8 compatibility problems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13315</link><project id="" key="" /><description>When compiled with source level=8 this test fails.
</description><key id="104730887">13315</key><summary>InnerHitsIT has java 8 compatibility problems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>test</label></labels><created>2015-09-03T16:10:06Z</created><updated>2015-09-03T17:10:40Z</updated><resolved>2015-09-03T17:10:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-03T16:52:29Z" id="137509705">This is due to the improvements in [target-type inference](http://openjdk.java.net/jeps/101) in Java 8. I'll open a pull request that addresses.

There are two tests failing here for the same reason.

Prior to Java 7, the line

```
String.valueOf(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue())
```

would be interpreted as invoking [`String.valueOf(String)`](http://docs.oracle.com/javase/7/docs/api/java/lang/String.html#valueOf%28java.lang.Object%29). However, due to the improvements in target-type inference this line is now interpreted as invoking [`String.valueOf(char[])`](http://docs.oracle.com/javase/7/docs/api/java/lang/String.html#valueOf%28char[]%29). This causes the test to now throw a `ClassCastException`. Simply casting the parameter to `Object` will force the previously invoked overload to be invoked.
</comment><comment author="rmuir" created="2015-09-03T16:54:39Z" id="137510367">great, thanks for looking into this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bump master (3.0-snapshot) to java 8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13314</link><project id="" key="" /><description /><key id="104725106">13314</key><summary>Bump master (3.0-snapshot) to java 8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>upgrade</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T15:43:36Z</created><updated>2015-09-06T12:37:38Z</updated><resolved>2015-09-03T16:56:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-03T15:48:17Z" id="137490921">Wooo! LGTM
</comment><comment author="rjernst" created="2015-09-03T15:55:14Z" id="137492619">Awesome, LGTM too
</comment><comment author="jasontedor" created="2015-09-03T16:00:32Z" id="137494995">Looks beautiful to me.
</comment><comment author="rmuir" created="2015-09-03T16:47:31Z" id="137507966">I added some commits. full mvn verify passes
</comment><comment author="rjernst" created="2015-09-03T16:54:02Z" id="137510229">Still LGTM
</comment><comment author="jasontedor" created="2015-09-03T17:05:49Z" id="137512764">Relates #10799, #13224, #13315 
</comment><comment author="jprante" created="2015-09-04T07:09:31Z" id="137662468">Just a note: `mvn javadoc:javadoc` fails, because Java 8 javadoc performs a strict doclint check. See also http://openjdk.java.net/jeps/172
</comment><comment author="dadoonet" created="2015-09-04T07:12:05Z" id="137662749">Thanks @jprante! I opened an issue #13336
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove DisableAllocationDecider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13313</link><project id="" key="" /><description>The `EnableAllocatiorDecider` has replaced the
`DisableAllocationDecider`, which has been deprecated. It can now be
removed.
</description><key id="104717774">13313</key><summary>Remove DisableAllocationDecider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T15:11:56Z</created><updated>2016-03-03T19:12:23Z</updated><resolved>2015-09-11T15:41:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-10T21:37:29Z" id="139386368">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: simplify filtered query conversion to lucene query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13312</link><project id="" key="" /><description>We have some optimization in FilteredQueryParser that tries to mimic what the rewrite method in lucene does, based on what gets parsed we return the simplest query possible. That might cause issues with boost values though, if specified in both the main query and the inner query that we shortcut to. We should rather rely on lucene's rewrite method to simplify the lucene representation of the query, and always build a filtered query instead.

relates to #13272
</description><key id="104713597">13312</key><summary>Internal: simplify filtered query conversion to lucene query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T14:50:40Z</created><updated>2015-09-03T16:06:07Z</updated><resolved>2015-09-03T15:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T15:03:56Z" id="137478111">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize scrolls for constant-score queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13311</link><project id="" key="" /><description>We currently optimize scroll when sort=_doc because docs are returned in order.
But documents are also returned in order when sorting by score and the query
gives constant scores. This optimization has the nice side-effect of also
optimizing scrolls with the default `match_all` query.
</description><key id="104704357">13311</key><summary>Optimize scrolls for constant-score queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T14:13:28Z</created><updated>2015-09-08T13:19:19Z</updated><resolved>2015-09-03T14:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-03T14:35:00Z" id="137467822">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the scan and count search types.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13310</link><project id="" key="" /><description>These search types have been deprecated in 2.1 and 2.0 respectively, and will
be removed in 3.0.
</description><key id="104694546">13310</key><summary>Remove the scan and count search types.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T13:34:00Z</created><updated>2015-09-07T14:21:16Z</updated><resolved>2015-09-07T14:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-03T13:38:27Z" id="137448846">Nice stats!
</comment><comment author="s1monw" created="2015-09-03T14:42:51Z" id="137471563">awesome!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow deleting closed indices with shadow replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13309</link><project id="" key="" /><description>Previously we skip deleting the index store for indices on a shared
filesystem, because we don't want to delete the data when the shard is
relocating around the cluster. This adds a flag to the
`deleteIndexStore` method signifying that the index is closed and that
we should allow deleting the contents even if it is on a shared
filesystem.

Includes a unit test for the IndicesService.canDeleteIndexContents and
integration tests ensure a closed shadow replica index deletes files
correctly.

Resolves #13297
</description><key id="104677077">13309</key><summary>Allow deleting closed indices with shadow replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T12:11:04Z</created><updated>2016-03-10T18:14:25Z</updated><resolved>2015-09-04T22:03:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-03T12:11:25Z" id="137418782">@s1monw can you take a look at this?
</comment><comment author="s1monw" created="2015-09-04T20:46:58Z" id="137848366">left one comment LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix deprecations introduced by the upgrade to Lucene 5.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13308</link><project id="" key="" /><description>This changes construction of Phrase and Boolean queries to use the builder,
and replaces BitDocIdSetFilter with BitSetProducer for nested and parent/child
queries. I had to remove the ParentIdsFilter for the case when there was a
single parent as it was using the source of BitSets for parents as a regular
Filter, which is not possible anymore now. I don't think this is an issue since
this case rarely occurs, and the alternative logic for when there are several
matching parent ids should not be much worse.
</description><key id="104675074">13308</key><summary>Fix deprecations introduced by the upgrade to Lucene 5.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T11:57:04Z</created><updated>2015-09-06T12:42:36Z</updated><resolved>2015-09-04T08:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-03T16:01:25Z" id="137495380">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] remove comment about function_score faster than script sort. It&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13307</link><project id="" key="" /><description>&#8230; is not so.
</description><key id="104664405">13307</key><summary>[doc] remove comment about function_score faster than script sort. It&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-09-03T10:30:12Z</created><updated>2015-09-03T10:36:33Z</updated><resolved>2015-09-03T10:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T10:36:10Z" id="137404576">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Add script to validate mvn repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13306</link><project id="" key="" /><description>This script allows to ensure that artifacts have been pushed to
a repository after running `mvn deploy`. This will allow us to
check that all of our artifacts have been deployed to sonatype and
the S3 bucket.

Basically it takes the contents of a local mvn repository and
runs HTTP HEAD requests against all artifacts. It also compares if
the length returned by the Content-Length header is the same as the
size of the artifact locally.

Relates #13209
</description><key id="104663729">13306</key><summary>Release: Add script to validate mvn repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T10:26:01Z</created><updated>2016-03-10T18:14:25Z</updated><resolved>2015-09-08T12:14:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-03T11:22:04Z" id="137411402">I can't really comment on the code but it looks like to me a great addition! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure: AwarenessAllocationIT.testAwarenessZones</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13305</link><project id="" key="" /><description>Build URL: http://build-us-00.elastic.co/job/elasticsearch-20-win2012/131/testReport/junit/org.elasticsearch.cluster.allocation/AwarenessAllocationIT/testAwarenessZones/

Reproduce:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=179D7B92393C913 -Dtests.class=org.elasticsearch.cluster.allocation.AwarenessAllocationIT -Dtests.method="testAwarenessZones" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseSerialGC -XX:-UseCompressedOops -XX:+AggressiveOpts -Djava.net.preferIPv4Stack=true" -Dtests.locale=de_DE -Dtests.timezone=ACT
```

Stack trace:

```
java.lang.AssertionError: 
Expected: (&lt;2&gt; or &lt;3&gt;)
     but: was &lt;5&gt;
    at __randomizedtesting.SeedInfo.seed([179D7B92393C913:87A23D3AC18A574C]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.cluster.allocation.AwarenessAllocationIT.testAwarenessZones(AwarenessAllocationIT.java:140)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1638)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:847)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:897)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:856)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:792)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:803)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="104656662">13305</key><summary>Test Failure: AwarenessAllocationIT.testAwarenessZones</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>jenkins</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T09:39:31Z</created><updated>2015-09-16T13:39:28Z</updated><resolved>2015-09-14T06:59:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-09-08T08:19:59Z" id="138473061">This test fails regularly the last 3 weeks:

---

Build URL   http://build-us-00.elastic.co/job/es_core_master_window-2012/1711/
Project:    es_core_master_window-2012
Randomization:  8u11,network,heap[1024m],-server +UseConcMarkSweepGC -UseCompressedOops +AggressiveOpts
Date of build:  Mon, 07 Sep 2015 21:32:27 +0000
Build duration: 2 hr 46 min

---

Build URL   http://build-us-00.elastic.co/job/es_core_master_window-2012/1694/
Project:    es_core_master_window-2012
Randomization:  8u11,network,heap[512m],-server +UseParallelGC -UseCompressedOops +AggressiveOpts
Date of build:  Fri, 04 Sep 2015 16:11:54 +0000
Build duration: 2 hr 53 min

---

Build URL   http://build-us-00.elastic.co/job/elasticsearch-20-win2012/131/
Project:    elasticsearch-20-win2012
Randomization:  8u11,network,heap[512m],-server +UseSerialGC -UseCompressedOops +AggressiveOpts
Date of build:  Thu, 03 Sep 2015 01:14:29 +0000
Build duration: 2 hr 45 min

---

Build URL   http://build-us-00.elastic.co/job/es_core_master_window-2012/1676/
Project:    es_core_master_window-2012
Randomization:  7u55,network,heap[1024m],-server +UseConcMarkSweepGC -UseCompressedOops,assert off
Date of build:  Tue, 01 Sep 2015 15:02:56 +0000
Build duration: 2 hr 47 min

---

Build URL   http://build-us-00.elastic.co/job/elasticsearch-20-win2012/98/
Project:    elasticsearch-20-win2012
Randomization:  7u55,network,heap[936m],-server +UseParallelGC +UseCompressedOops
Date of build:  Fri, 28 Aug 2015 00:42:24 +0000
Build duration: 2 hr 45 min

---

Build URL   http://build-us-00.elastic.co/job/elasticsearch-20-win2008/67/
Project:    elasticsearch-20-win2008
Randomization:  7u55,network,heap[512m],-server +UseConcMarkSweepGC -UseCompressedOops
Date of build:  Thu, 20 Aug 2015 19:48:41 +0000
Build duration: 1 hr 59 min
</comment><comment author="tlrx" created="2015-09-08T15:30:03Z" id="138600921">After looking at the test and the logs, it looks like all failures have the same scenario. 

I try to explain my understanding, hopefully I'm not completely wrong:

The four nodes are started in an asynchronous manner with min.master.nodes set to 3. Node_t0 starts first and try to find other nodes on localhost with unicast. While it tries successive ports,  node_t1, node_t2 and node_t3 also start (all nodes are started asynchronously).

Node_t0 finally connects to another node (let's say, on the port used by node_t1) and gets back a `RemoteTransportException` because the remote node is still initializing and not yet started.

Then the remote node is started and both nodes, node_t0 and node_t1 are connected.

The same thing happen for node_t2 and node_t3: node_t0 tries to connect and gets a remote exception, then nodes are able to connect to each other.

But once node_t0 is connected to node_1 and node_2 it elects itself as master:

```
[node_t0] elected as master, waiting for incoming joins ([2] needed)
```

and waits for join requests. 

At this point, I suspect that the previous remote exceptions are handled and provoke the disconnection of node_t0 from nodes t1..t3:

```
[2015-09-04 18:30:57,932][DEBUG][discovery.zen            ] [node_t0] elected as master, waiting for incoming joins ([2] needed)
[2015-09-04 18:30:57,932][DEBUG][transport.netty          ] [node_t0] disconnecting from [{#zen_unicast_8_YoSSyfnvSnmF0a8ViPEw7w#}{127.0.0.1}{127.0.0.1:9402}{mode=network, zone=b}] due to explicit disconnect call
[2015-09-04 18:30:57,932][DEBUG][transport.netty          ] [node_t0] disconnecting from [{#zen_unicast_3#}{127.0.0.1}{127.0.0.1:9402}] due to explicit disconnect call
[2015-09-04 18:30:57,932][DEBUG][transport.netty          ] [node_t0] disconnecting from [{#zen_unicast_6_ESxZGa4IQXymN4ZTTRD9kQ#}{127.0.0.1}{127.0.0.1:9401}{mode=network, zone=b}] due to explicit disconnect call
[2015-09-04 18:30:57,932][DEBUG][transport.netty          ] [node_t0] disconnecting from [{#zen_unicast_7_ESxZGa4IQXymN4ZTTRD9kQ#}{127.0.0.1}{127.0.0.1:9401}{mode=network, zone=b}] due to explicit disconnect call
```

node_t0 will wait for joins but nothing will come and after a given delay:

```
[2015-09-04 18:31:23,906][WARN ][discovery                ] [node_t0] waited for 30s and no initial state was set by the discovery
[2015-09-04 18:31:23,906][DEBUG][gateway                  ] [node_t0] can't wait on start for (possibly) reading state from gateway, will do it asynchronously
[2015-09-04 18:31:23,906][INFO ][node                     ] [node_t0] started
```

In the meanwhile, the 3 other nodes keep chatting together and elect another as master:

```
[2015-09-04 18:31:01,177][DEBUG][discovery.zen            ] [node_t3] elected as master, waiting for incoming joins ([2] needed)
```

Since the 3 other nodes are correctly connected to each other, the test can create the index "test" with 5 shards / 1 replica that will be allocated on the 3 nodes. With the awareness settings, one of the node will get 5 shards allocated, 2 on another one and 3 on the last one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to delete child documents by query that uses &#8220;has_parent&#8221;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13304</link><project id="" key="" /><description>Hi,

I'm using the query below in my search to find all users without parents.
The search works as expected, but delete by query fails with **has_parent] query and filter unsupported in delete_by_query api**

I've searched and it seems that it should be supported.

```
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "user.service_id": "12345"
          }
        },
        {
          "filtered": {
            "filter": {
              "not": {
                "filter": {
                  "has_parent": {
                    "query": {
                      "match_all": {}
                    },
                    "parent_type": "account"
                  }
                }
              }
            }
          }
        }
      ]
    }
  }
}
```
</description><key id="104652842">13304</key><summary>Unable to delete child documents by query that uses &#8220;has_parent&#8221;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ron-totango</reporter><labels /><created>2015-09-03T09:12:21Z</created><updated>2015-09-04T18:47:51Z</updated><resolved>2015-09-04T18:47:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-04T18:47:51Z" id="137818748">Parent child queries aren't supported in the delete by query api: https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html#limitations

Also the delete by query api has been replaced with the delete-by-query plugin in 2.0:
https://www.elastic.co/blog/core-delete-by-query-is-a-plugin?q=delete%20by%20query
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a dedicate queue for incoming ClusterStates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13303</link><project id="" key="" /><description>The initial implementation of two phase commit based cluster state publishing (#13062) relied on a single in memory "pending" cluster state that is only processed by ZenDiscovery once committed by the master. While this is fine on it's own, it resulted in an issue with acknowledged APIs, such as the open index API, in the extreme case where a node falls behind and receives a commit message after a new cluster state has been published. Specifically:

1) Master receives and acked-API call and publishes cluster state CS1
2) Master waits for a min-master nodes to receives CS1 and commits it.
3) All nodes that have responded to CS1 are sent a commit message, however, node N didn't respond yet
4) Master waits for publish timeout (defaults to 30s) for all nodes to process the commit. Node N fails to do so.
5) Master publishes a cluster state CS2. Node N responds to cluster state CS1's publishing but receives cluster state CS2 before the commit for CS1 arrives.
6) The commit message for cluster CS1 is processed on node N, but fails because CS2 is pending. This caused the acked API in step 1 to return (but CS2 , is not yet processed).

In this case, the action indicated by CS1 is not yet executed on node N and therefore the acked API calls return pre-maturely. Note that once CS2 is processed but the change in CS1 takes effect (cluster state operations are safe to batch and we do so all the time).

An example failure can be found on: http://build-us-00.elastic.co/job/es_feature_two_phase_pub/314/

This commit extracts the already existing pending cluster state queue (`processNewClusterStates`) from ZenDiscovery into it's own class, which serves as a temporary container for in-flight cluster states. Once committed the cluster states are transferred to ZenDiscovery as they used to before. This allows "lagging" cluster states to still be successfully committed and processed (and likely to be ignored as a newer cluster state has already been processed).  
As a side effect, all batching logic is now extracted from ZenDiscovery and is unit tested.

Note: this PR is done against the feature/two-phase-pub branch
</description><key id="104647880">13303</key><summary>Add a dedicate queue for incoming ClusterStates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T08:40:23Z</created><updated>2015-09-14T08:04:46Z</updated><resolved>2015-09-14T08:04:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-03T09:13:36Z" id="137387442">One more note - this commit changes the beavior of how we respond to an incoming cluster state while the local node is master. Before we would tie break and step down or ask the other master to step down. This is removed due to two reasons:
1) We already have this logic when receiving node fault detection pings from the other master.
2) It is better to reject the incoming cluster state (as would other nodes do) to avoid the other master committing it by mistake if the two masters form together a quorum (3 master nodes scenario).
</comment><comment author="s1monw" created="2015-09-03T09:25:49Z" id="137389951">+1 just for the sake of the test
</comment><comment author="dakrone" created="2015-09-08T21:55:26Z" id="138715358">Went through this and left some questions.

I have a naive question, instead of an unbounded queue that could potentially grow very large, can we cap the queue at a specific length, and when a new state comes in, drop the oldest state currently in the queue?
</comment><comment author="imotov" created="2015-09-08T23:33:29Z" id="138731017">Left one minor comment, but I have the same question that Lee asked above about unbounded queue growth. 
</comment><comment author="bleskes" created="2015-09-09T14:00:48Z" id="138917960">@dakrone @imotov @brwe  I addressed all comments.

Regarding unbound queue- yeah, I was thinking about it but it's tricky (FWIW - the queue was unbound before, but we were also never holding on to things more than until a cluster state thread came along). First question is - how long should it be? second, what should you do when the queue is full? you can reject incoming cluster states from the master, which may or may not prevent them from being committed. But once they are rejected (but committed by the master), the node will never process them. This probably something we should deal with as well (upon what publishing failures should we remove a node from the cluster?) but I rather not address it now. 

I suggest that we flush pending states that have been in the queue for some ridiculous time - say 10m. That will make sure that we never remove states that are "leaked" and will also make sure no state is rejected. Also, it will help with the testClusterJoinDespiteOfPublishingIssues test that leaks cluster states "by design" by swallowing commit requests. Note though that as far as I know that is impossible in real networks (unless someone messes up with the iptables)

If you agree I'll do that in a follow up commit. We also need some monitoring tool for this (another issue). Until then I added an awaitFix to the `testClusterJoinDespiteOfPublishingIssues` test
</comment><comment author="bleskes" created="2015-09-09T15:05:51Z" id="138940041">I chatted more with @dakrone and we came up with a simpler solution - bound the queue and just drop old cluster states if need be (see java docs for the reasoning why that's OK to do). I also worked around the expected leakage in testClusterJoinDespiteOfPublishingIssues.
</comment><comment author="dakrone" created="2015-09-10T17:14:58Z" id="139314855">@bleskes left some more (mostly minor) comments on this
</comment><comment author="bleskes" created="2015-09-10T18:40:53Z" id="139339064">@dakrone thx. Pushed another update.
</comment><comment author="dakrone" created="2015-09-10T20:23:55Z" id="139369066">Thanks, LGTM
</comment><comment author="bleskes" created="2015-09-11T08:28:56Z" id="139484598">Thx @dakrone @imotov &amp; @brwe . I push this to feature/two_phase_pub to give CI some time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>This commit removes com.google.common.io</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13302</link><project id="" key="" /><description>This PR removes and forbids `com.google.common.io.Files`, `com.google.common.io.ByteStreams`, and `com.google.common.io.Resources`

See https://github.com/elastic/elasticsearch/issues/13224.
</description><key id="104641483">13302</key><summary>This commit removes com.google.common.io</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">xuzha</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-03T07:53:57Z</created><updated>2015-10-13T08:25:01Z</updated><resolved>2015-10-10T16:47:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-09-03T07:55:27Z" id="137369266">Just came across to it, not sure if we need it. 
If @jasontedor or anyone else has been working on it, I'm sorry and  just close this one. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mention node restart after plugin removal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13301</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-plugins.html#_removing_plugins) we tell users the command to use to remove a plugin, but we should also mention that _most_ plugins also require a restart of the node. Just as we do for the installation part :)

Via [the forums](https://discuss.elastic.co/t/removing-marvel-indices-from-elasticsearch/28488/3).
</description><key id="104632933">13301</key><summary>Mention node restart after plugin removal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2015-09-03T06:52:32Z</created><updated>2015-10-07T16:11:38Z</updated><resolved>2015-10-07T16:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-03T07:59:22Z" id="137371277">Indeed. It makes sense.

Do you plan to send a PR for this Mark?
</comment><comment author="markwalkom" created="2015-09-03T08:01:08Z" id="137371567">Yeah. Just out at the minute :)
On 3 Sep 2015 6:00 pm, "David Pilato" notifications@github.com wrote:

&gt; Indeed. It makes sense.
&gt; 
&gt; Do you plan to send a PR for this Mark?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13301#issuecomment-137371277
&gt; .
</comment><comment author="markwalkom" created="2015-09-21T05:01:38Z" id="141878215">Have the docs changed here, I can't seem to find this any longer in master? (Or am I blind)
</comment><comment author="dadoonet" created="2015-09-21T05:17:00Z" id="141879686">Here for example? 
https://www.elastic.co/guide/en/elasticsearch/plugins/2.0/installation.html

Note that per plugin we already wrote that
https://www.elastic.co/guide/en/elasticsearch/plugins/2.0/cloud-aws.html#cloud-aws-install
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm postun scriptlet failing 1.6.0-1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13300</link><project id="" key="" /><description>```
post remove script called with unknown argument `1'
warning: %postun(elasticsearch-1.6.0-1.noarch) scriptlet failed, exit status 1
```

OS is either RHEL6 or RHEL7
</description><key id="104627086">13300</key><summary>rpm postun scriptlet failing 1.6.0-1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">faxm0dem</reporter><labels /><created>2015-09-03T05:52:08Z</created><updated>2015-09-08T12:51:28Z</updated><resolved>2015-09-05T12:31:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-05T12:31:34Z" id="137949320">Closed by https://github.com/elastic/elasticsearch/pull/12630
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve situation when network.host is set to wildcard (e.g. 0.0.0.0)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13299</link><project id="" key="" /><description>Users might specify something like -Des.network.host=0.0.0.0, as that
was the old default with previous versions of elasticsearch. This means
to bind to all interfaces, but it makes no sense as a publish address.

Pick a good one in this case, just like we do in other cases where
publish isn't explicitly specified and we are bound to multiple (e.g.
when configured by interface, or dns hostname with multiple addresses).
However, in this case warn the user about it: since its arbitrarily
picking the first non-loopback address like the old versions
did, thats a little too heuristical, but lets make the cutover easy.

Separately, fail hard if things like multicast or broadcast addresses are
configured as bind or publish addresses, as that is simply invalid.

Closes #13274
</description><key id="104613654">13299</key><summary>Improve situation when network.host is set to wildcard (e.g. 0.0.0.0)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-03T03:24:54Z</created><updated>2015-09-14T17:14:30Z</updated><resolved>2015-09-03T12:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T07:22:52Z" id="137358607">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better management of deletion for yellow/red indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13298</link><project id="" key="" /><description>When an index that is red/yellow - perhaps due to the node that holds the other shards having an outage - is deleted we only delete the currently available shards along with the index. However if the missing node comes back we then recover whatever shards existed on it via dangling indices.

It seems that if a user requests an index to be deleted, having those other shards come back is aberrant  behaviour and instead we should make note that some shards are not available, and if/when they come back they are then also deleted (maybe add a timeout value to that as well?).

Sure, the user could turn dangling indices off, but that seems incomplete and still leaves the data on disk when it's obviously not wanted - presuming that node comes back of course.
</description><key id="104604601">13298</key><summary>Better management of deletion for yellow/red indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-09-03T01:39:02Z</created><updated>2016-05-25T13:11:19Z</updated><resolved>2016-05-25T13:11:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-04T09:21:45Z" id="137686702">I think this is the same issue as in https://github.com/elastic/elasticsearch/issues/10054 . Having a timeout would be a lot of hassle and seems dangerous to me. 
This should be documented better though.
</comment><comment author="clintongormley" created="2015-09-06T12:45:43Z" id="138080704">The only way we could prevent these indices from being imported would be to remember every index that has ever been deleted in the cluster state, which could very quickly get out of hand.  If you delete an index while nodes are missing, then you should expect some weird behaviour.  ++ to mentioning explicitly in the docs.
</comment><comment author="kkirsche" created="2015-10-07T19:14:22Z" id="146299855">@clintongormley why is that the case? Wouldn't during the connection process the existing cluster(s) state take priority? I would have assumed that the active config and indices would take priority during the sync process and the old indices would be trashed. I understand per @brwe's comment some of the reasoning behind not doing it this way regarding data safety, but why can't this be configurable?
</comment><comment author="clintongormley" created="2015-10-08T12:16:51Z" id="146519812">@kkirsche imagine you start a new node, that didn't have the old cluster state, and it became master.  Any node that joined this new cluster would just delete all the data it had locally (because the new cluster state takes priority).  Instead, we prefer to err on the side of importing old (maybe irrelevant) data and letting the user decide what should be deleted.
</comment><comment author="brwe" created="2016-05-25T12:25:43Z" id="221558603">@abeyad is this fixed by https://github.com/elastic/elasticsearch/pull/18250 ?
</comment><comment author="abeyad" created="2016-05-25T13:05:32Z" id="221569506">@brwe yes, that's correct, the tombstones in the cluster state fixes this 
</comment><comment author="brwe" created="2016-05-25T13:11:19Z" id="221571122">Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleting closed shadow replica index doesnt delete off disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13297</link><project id="" key="" /><description>Using 1.7.1 and shadow replicas. When I close an index that was created with these settings `{"index.shadow_replicas": "true","index.data_path": "/data/es"}` the index and its data is not deleted.
If the index is open, the data is deleted off disk but the top most directory (eg: `/data/es/0/myindex-2015-09-01`) still exists.
Obviously 'normal' indices are completely deleted whether they've been closed or not.
Not sure if this is by design - if so, I think it should be documented.

Full steps taken to reproduce the issue are here: https://gist.github.com/natefox/306c3429825ce5eb182b
I made a Discuss thread here as well: https://discuss.elastic.co/t/deleting-indices-with-shadow-replicas/28261
</description><key id="104589123">13297</key><summary>Deleting closed shadow replica index doesnt delete off disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">natefox</reporter><labels><label>:Shadow Replicas</label><label>adoptme</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T23:05:59Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-09-04T22:03:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-03T07:44:45Z" id="137364798">@dakrone  can you please take a look at this?
</comment><comment author="dakrone" created="2015-09-03T11:48:30Z" id="137415118">@natefox I'm definitely able to reproduce this now, thanks for opening this, I'll take a look!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't surround -Xloggc log filename with quotes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13296</link><project id="" key="" /><description>The `-Xloggc:filename.log` parameter has very strict filename semantics:

```
[A-Z][a-z][0-9]-_.%[p|t]
```

Our script specifies \" and \" to surround it, which makes Java think we
are sending: -Xloggc:"foo.log" and it fails with:

```
Invalid file name for use with -Xloggc: Filename can only contain the characters [A-Z][a-z][0-9]-_.%[p|t] but it has been "foo.log"
Note %p or %t can only be used once
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
```

We can't quote this, and we should not need to since the valid
characters don't include a space character, so we don't need to worry
about quoting.

Resolves #13277
</description><key id="104587113">13296</key><summary>Don't surround -Xloggc log filename with quotes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T22:49:03Z</created><updated>2016-02-21T20:22:24Z</updated><resolved>2015-09-03T04:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-02T23:57:17Z" id="137276271">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated param name to match type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13295</link><project id="" key="" /><description /><key id="104581806">13295</key><summary>Updated param name to match type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sksamuel</reporter><labels><label>non-issue</label><label>v2.1.0</label></labels><created>2015-09-02T22:07:21Z</created><updated>2015-09-03T07:23:40Z</updated><resolved>2015-09-03T07:23:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T07:23:40Z" id="137358807">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename test suffix so we only use "Tests"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13294</link><project id="" key="" /><description>We currently have a small number of test classes with the suffix "Test",
yet most use the suffix "Tests". This change renames all the "Test"
classes, so that we have a simple rule: "Non-inner classes ending with
Tests".
</description><key id="104576896">13294</key><summary>Rename test suffix so we only use "Tests"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.1.0</label></labels><created>2015-09-02T21:39:21Z</created><updated>2015-09-02T22:19:27Z</updated><resolved>2015-09-02T22:19:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-02T21:59:30Z" id="137258013">LGTM
</comment><comment author="kimchy" created="2015-09-02T22:08:26Z" id="137259672">thank you @rjernst!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Move naming convention tests to a test case, so plugins can have the same convention checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13293</link><project id="" key="" /><description>The current plugin setup uses the same test pattern rules we have in
elasticsearch (at least by default). Yet they don't check naming
conventions, so failures can pop up if naming conventions change, and
tests simply start failing. This change makes the naming convention
checks available to plugins that use the test jar.
</description><key id="104570259">13293</key><summary>Tests: Move naming convention tests to a test case, so plugins can have the same convention checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-09-02T21:00:32Z</created><updated>2016-03-14T10:15:47Z</updated><resolved>2016-03-10T16:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T11:59:16Z" id="137416953">LGTM
</comment><comment author="jpountz" created="2015-09-24T15:32:20Z" id="142964561">@rjernst any reason not to merge?
</comment><comment author="nik9000" created="2015-11-13T22:45:04Z" id="156580357">++ This has been on my whiteboard for forever but I've just not done it.
</comment><comment author="rjernst" created="2016-01-18T23:05:53Z" id="172677148">@jpountz @nik9000 I've had this PR sitting here for quite a while, but I'm not sure what to do with it. I have 2 problems with it:
1. It still requires work from end users (ie other plugins in ES) to trigger. I would be much happier if we automatically set up such a test, otherwise the usefulness is lessened and we might miss it on new plugins.
2. The existing rules are really for ES core, which are _not_ the same as plugins. In plugins, all tests, even ones that extend ESIntegTestCase, are named with the Tests suffix.
</comment><comment author="nik9000" created="2016-01-18T23:10:50Z" id="172677867">&gt; It still requires work from end users (ie other plugins in ES) to trigger. I would be much happier if we automatically set up such a test, otherwise the usefulness is lessened and we might miss it on new plugins.

I wonder if you can sneak it into the test list somehow or execute this test in its own phase.

&gt; The existing rules are really for ES core, which are not the same as plugins. In plugins, all tests, even ones that extend ESIntegTestCase, are named with the Tests suffix.

I think the right thing to do here is just make it optional and turn it on in our build.gradle file.

And, yeah, I think we should just do this in master so we don't have to solve the problem twice.
</comment><comment author="rjernst" created="2016-01-18T23:45:02Z" id="172683683">I agree we can sneak it in via gradle for executing, that is doable. But I dont want to make it just optional. It is already confusing that currently tests in plugins that extend ESIntegTestCase end with Tests, but those in core must end in IT. I think we should add another task, which is the same as what core currently has for "integTest" which runs the ESIntegTestCase tests. I'm not sure what to call it, or how we should name tests to distinguish them. It's possible we could use a `*RestIT` pattern for rest integ tests. I prefer to call the ESIntegTestCase tests task something like `fantasyTest` as this is a fantasy land of all nodes running in the same jvm. :)
</comment><comment author="nik9000" created="2016-01-19T00:58:20Z" id="172695237">&gt; I think we should add another task, which is the same as what core currently has for "integTest" which runs the ESIntegTestCase tests.

I agree. I think if we're going to make a new phase we should make `restTest` let the `integTest` be the phase in which we run stuff that extends from ESIntegTestCase. I'm all for removing the funky "your allowed to extends from ESIntegTest in plugins" rule entirely.

The thing I wanted to make optional is running the naming rules entirely. We should enable them everywhere but I don't think its our place to force plugin developers to use our (pretty sane but arbitrary) naming conventions. PluginBuildPlugin shouldn't force them to do it, I mean.
</comment><comment author="rmuir" created="2016-01-19T01:00:35Z" id="172695514">Well if its a separate task, then isn't it still optional by definition? `separateTask.enabled = false`
</comment><comment author="rjernst" created="2016-01-19T01:01:57Z" id="172695667">&gt; PluginBuildPlugin shouldn't force them to do it, I mean

Except if we don't force it, tests will not be run. The naming conventions are there so we don't (and so a plugin author would not) unknowingly skip a test simply because it didn't match the pattern the test runner uses.
</comment><comment author="rjernst" created="2016-01-19T01:03:21Z" id="172695858">I also don't like renaming the real integ tests to `restTest`, because they are the real integ tests! Plus eg we have client tests which are not really rest tests, and may not even use the rest test plugin in the future, but should still be considered integTests. Really the integ tests in core are the thing that aren't really testing integration.
</comment><comment author="rjernst" created="2016-01-19T01:05:03Z" id="172696051">@rmuir's point about a separate task is valid too. If we move this to a gradle task, a user can disable it like any other precommit check, just like today they can disable eg the license check if they don't want to follow our license rules.
</comment><comment author="rjernst" created="2016-01-19T01:16:13Z" id="172697343">@nik9000 what do you think about `distributedTest` and renaming `ESIntegTestCase` to `ESDistributedTestCase`? I think it makes it more clear that the point of those tests are really for distributed logic, and eg mappings tests should be written with them.
</comment><comment author="nik9000" created="2016-01-19T01:23:16Z" id="172698148">&gt; Well if its a separate task, then isn't it still optional by definition? separateTask.enabled = false

Good point.

&gt; If we move this to a gradle task, a user can disable it like any other precommit check, just like today they can disable eg the license check if they don't want to follow our license rules.

Sold. If its a gradle task then it can even be "UP-TO-DATE" which is cool.

&gt; I also don't like renaming the real integ tests to restTest, because they are the real integ tests!

It makes a certain kind of sense to call the classes that extend ESIntegTest "mock tests" because they are allowed spin up a cluster with test mocks whereas the REST tests need to deal with Elasticsearch as packaged. They are also allowed to "reach into" the running test cluster and extract anything registered in guice which is pretty funky. So I guess I'm convinced that we should leave the REST tests in `integTest` and I'm proposing that we move the ESIntegTests to `mockTest`. BTW I love that this is possible in gradle.

I'd be quite happy if we could write some tests in java that dealt with elasticsearch the same way the rest tests deal with it - as a black box. I miss loops and other niceties when I write REST tests. It'd give us a way to transition some of the mock tests to more real black box tests. I like that rest tests can be shared and I like that we actually use REST rather than the java Client, though. If only we had a java client that spoke REST.....
</comment><comment author="nik9000" created="2016-01-19T01:24:28Z" id="172698304">&gt; @nik9000 what do you think about distributedTest and renaming ESIntegTestCase to ESDistributedTestCase?

Sorry, I missed that while I was typing my other reply. I still like `mockTest` better because it better lines up with what fixtures are needed to run the tests.
</comment><comment author="s1monw" created="2016-01-20T13:37:02Z" id="173205994">can this test basically be a regular expression of some sort  `class *IT extends (*IT | ESIntegTest)` and `class *Tests extends *Tests|EsTestCase)` I think if we could do that we can simply run a pre-build step like forbidden APIs? Maybe I am missing something?
</comment><comment author="rjernst" created="2016-01-20T19:39:22Z" id="173336053">I don't think we need regexes, we can look at the .class files and determine if a test class follows the rules. The question is what to call this 3rd type of test, since today we overload IT to be both ESIntegTestCase, as well as ESRestTestCase.
</comment><comment author="rjernst" created="2016-03-10T16:15:30Z" id="194930535">This has been superseded by the work in #16811.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node initialisation issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13292</link><project id="" key="" /><description>If the tribe node has started then sometimes api calls can fail for a while until the tribe node has finally all the information of all the other clusters it has been configured with. This can take a while sometimes.

Concrete scenario:
1) Start cluster
2) Create index and wait for yellow
3) Start cluster
4) Create index and wait for yellow
5) Start tribe node
6) Execute index call. This hangs sometimes, because the tribe node doesn't have collected all the cluster states from the cluster it has been configured with. This can take a while (until a cluster update from one of the cluster it is following comes in).

I think we can improve this by making sure that the cluster states have examined by the tribe node during startup. 
</description><key id="104568146">13292</key><summary>Tribe node initialisation issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Tribe Node</label><label>adoptme</label><label>bug</label></labels><created>2015-09-02T20:48:22Z</created><updated>2016-01-28T14:12:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove stress tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13291</link><project id="" key="" /><description>These are not actually tests, but command line applications that must be
run manually. This change removes the entire stresstest package. We can
add back individual tests that we find necessary, and make them real
tests (whether integ or not).
</description><key id="104564465">13291</key><summary>Remove stress tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.1.0</label></labels><created>2015-09-02T20:27:12Z</created><updated>2015-09-02T20:45:08Z</updated><resolved>2015-09-02T20:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T20:32:28Z" id="137236349">Makes sense to me though I'd love to hear from someone else if they think these are valuable or they are using them.
</comment><comment author="dakrone" created="2015-09-02T20:34:56Z" id="137236866">+1 to this, it's frustrating having to continually sync runnable classes that aren't actually ever run, like Nik, I left a comment about the one that you decided not to remove
</comment><comment author="s1monw" created="2015-09-02T20:42:27Z" id="137238421">lets get rid of it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail quick when executing master level write operations via a tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13290</link><project id="" key="" /><description>If a master level write operation is executed via a tribe node then we should fail quick with a descriptive error instead of waiting for the master timeout the expire (1m).

It is clearly documented that master level write operations are not allowed to be executed via a tribe node, but failing how we do currently is not super clear.

Also when indexing into an index that does't exist yet an error is returned when the timeout has expired, which isn't really obvious, especially when you think the index does exist.
</description><key id="104563611">13290</key><summary>Fail quick when executing master level write operations via a tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Tribe Node</label><label>adoptme</label><label>enhancement</label></labels><created>2015-09-02T20:23:05Z</created><updated>2015-09-04T09:07:46Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-04T09:07:41Z" id="137684618">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor ignore_malformed and coerce GeoPointFieldType to Builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13289</link><project id="" key="" /><description>This PR moves `ignore_malformed` and `coerce` from the `GeoPointFieldType` to the `Builder` in `GeoPointFieldMapper` since they are only needed at parse time. This makes these options consistent with the behavior of other ES core types.

closes #13285 
</description><key id="104559584">13289</key><summary>Refactor ignore_malformed and coerce GeoPointFieldType to Builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T19:57:08Z</created><updated>2015-09-16T13:38:50Z</updated><resolved>2015-09-03T22:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-03T16:08:28Z" id="137497967">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] Integration Testing Tips / Guidlines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13288</link><project id="" key="" /><description>If you are new to Elasticsearch, there are some subtle issues that can trip you up while writing integration tests:
- Need to wait for newly created indices to reach green before indexing/searching
- Should call explicit refresh after indexing but before searching
- DFS might be needed if you are checking scoring order or something similar
- Change cluster name to something unique
- How to completely clear cluster between tests (data, templates, settings, etc)

Probably more that I'm not thinking of.  

It doesn't need to be a large guide, just something that gives tips and explains some common exceptions to faciliate googling upon the page.  E.g. you might get `IndexShardMissingException` and `IllegalIndexShardStateException` if your index isn't green yet.  Without context, these exceptions lead you to much different problems when searching for a resolution (corruption indexes, missing nodes, etc)

I have no idea where this would actually go in the docs.

Issue prompted from a twitter convo: https://twitter.com/frgtn/status/639146742861201408
</description><key id="104558982">13288</key><summary>[Docs] Integration Testing Tips / Guidlines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>docs</label></labels><created>2015-09-02T19:53:38Z</created><updated>2017-01-27T13:17:39Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-05T12:16:44Z" id="137948827">@polyfractal in TESTING.asciidoc?

https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc
</comment><comment author="janko-m" created="2017-01-27T13:17:39Z" id="275663011">I would also like to add that we've experienced a significant slowdown in our test suite when migrating from 1.4 to 2.4, and setting `index.translog.durability=async` sped up the tests ~3x. So perhaps this setting should be recommended for running tests.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test upgrading from an older version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13287</link><project id="" key="" /><description>Adds a test for upgrading from 2.0.0-beta1 to the version of that is built.

Closes #13183
</description><key id="104557542">13287</key><summary>Test upgrading from an older version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T19:44:11Z</created><updated>2016-03-10T18:13:57Z</updated><resolved>2015-09-04T19:35:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-02T20:13:05Z" id="137230178">The bash portions of this look good to me, the maven portions.... look like XML that does Magic&#8482;
</comment><comment author="jpountz" created="2015-09-03T07:34:53Z" id="137361234">LGTM. I'm fine with doing it with 2.0.0 beta1 for now in order to build the infra but since we don't guarantee anything in terms of compatibility of this version, I think we should move to 2.0.0 as soon as it's released? If you agree, maybe we should leave a comment about it?
</comment><comment author="nik9000" created="2015-09-04T19:26:56Z" id="137832546">&gt; LGTM. I'm fine with doing it with 2.0.0 beta1 for now in order to build the infra but since we don't guarantee anything in terms of compatibility of this version, I think we should move to 2.0.0 as soon as it's released? If you agree, maybe we should leave a comment about it?

Agreed. I'll leave a comment.
</comment><comment author="nik9000" created="2015-09-04T19:35:32Z" id="137835166">Ok - pushed some extra docs like the reviewers asked for. OTOH they both said it looked good to them so I'm just going to merge this as well.
</comment><comment author="nik9000" created="2015-09-04T20:26:01Z" id="137844765">Pushed to master, 2.x, and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Vagrant packaging tests should test uninstall, then reinstall</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13286</link><project id="" key="" /><description>A user on IRC ran into a problem on debian where after doing an `apt-get purge` of the elasticsearch package, then re-installing it, debian complained loudly about being unable to remove/re-add the "elasticsearch" user.

For our vagrant tests, we already test installing the package, but we should test removing it, then re-installing so we can see these issues as part of packaging bugs.
</description><key id="104556021">13286</key><summary>Vagrant packaging tests should test uninstall, then reinstall</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T19:33:52Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-09-08T17:47:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-03T13:53:49Z" id="137455347">Here is an example of a failure during packaging: 

```
root@ecm:/etc# dpkg --purge elasticsearch
(Reading database ... 169011 files and directories currently installed.)
Removing elasticsearch (1.5.2) ...
Purging configuration files for elasticsearch (1.5.2) ...
Removing user `elasticsearch' ...
Warning: group `elasticsearch' has no more members.
Done.
The group `elasticsearch' does not exist.
dpkg: warning: while removing elasticsearch, directory '/etc/elasticsearch' not empty so not removed
root@ecm:/etc# mc

root@ecm:/usr/share# /var/lib
bash: /var/lib: Is a directory

root@ecm:/home/system# mk old
bash: mk: command not found

root@ecm:/home/system# mkdir old

root@ecm:/home/system# mv elasticsearch old/.

root@ecm:/etc# apt-get install elasticsearch
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  elasticsearch
0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.
Need to get 0 B/27.0 MB of archives.
After this operation, 31.0 MB of additional disk space will be used.
dpkg: unrecoverable fatal error, aborting:
 unknown user 'elasticsearch' in statoverride file
E: Sub-process /usr/bin/dpkg returned an error code (2)
```
</comment><comment author="electrical" created="2015-09-03T14:27:56Z" id="137465948">fyi, i do installs and uninstalls ( purge ) with the puppet module and never ran into this issue.
</comment><comment author="nik9000" created="2015-09-04T21:09:35Z" id="137853984">I can't reproduce the issue either - the `statoverride` thing makes me thing the user is seeing #12189.
</comment><comment author="nik9000" created="2015-09-04T21:10:01Z" id="137854092">But I'll still add purge/reinstall to the tests just to verify that that works. It should and we don't do it in a single test run.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor ignore_malformed and coerce options from geo_point field type to mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13285</link><project id="" key="" /><description>The `geo_point` type `ignore_malformed` and `coerce` properties are currently part of the GeoPointField.  This is inconsistent with other core types.  This issue refactors these options from the GeoPointFieldMapper.GeoPointFieldType class to the GeoPointFieldMapper.Builder.
</description><key id="104550440">13285</key><summary>Refactor ignore_malformed and coerce options from geo_point field type to mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T19:02:14Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-09-03T22:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: query_string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13284</link><project id="" key="" /><description>Refactoring of the query_string query: adds serialization bits, validate, equals, hashcode and tests for QueryStringQueryBuilder. Lucene query generation moved to toQuery method. Used QueryParserSettings as intermediate object, created on the data node, to provide arguments to MapperQueryParser for the lucene query parsing.

Relates to #10217
</description><key id="104546808">13284</key><summary>Query refactoring: query_string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>breaking</label><label>review</label></labels><created>2015-09-02T18:44:04Z</created><updated>2015-09-14T20:42:23Z</updated><resolved>2015-09-10T08:35:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-02T18:49:38Z" id="137207750">This PR contains a little breakage for the java api, as the `QueryStringQueryBuilder#field(String)` method doesn't allow to specify the boost in form `field("field^2")`. If you want to specify a boost you need to switch to `QueryStringQueryBuilder#field(String, float)` and do `field("field", 2)`;

It also moves to parsing the locale using `Locale#forLanguageTag` which may cause backwards compatibility issue (see #13229).
</comment><comment author="colings86" created="2015-09-08T10:04:39Z" id="138502854">@javanna I left a few comments
</comment><comment author="s1monw" created="2015-09-08T15:25:04Z" id="138599624">I added some comments
</comment><comment author="javanna" created="2015-09-09T13:41:30Z" id="138912059">I have addressed some comments, replied to others, also added a TODO on unifying settings and query options for later on. It should be ready.
</comment><comment author="s1monw" created="2015-09-09T21:20:29Z" id="139049299">LGTM
</comment><comment author="javanna" created="2015-09-10T08:35:11Z" id="139167814">Merged https://github.com/elastic/elasticsearch/commit/484fcd49e571f1d4d334e10e7660f129cbee7a94
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase suggester documentation doesn't mention shingle filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13283</link><project id="" key="" /><description>If you run the phrase suggester against a field without the ngram filter applied to it then the phrase suggester reverts to behavior similar to the term suggester. It just doesn't work properly. We should add an example of setting up the mapping properly to use the phrase suggester to the docs on it.
</description><key id="104546430">13283</key><summary>Phrase suggester documentation doesn't mention shingle filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-09-02T18:42:33Z</created><updated>2017-02-14T14:32:31Z</updated><resolved>2017-02-14T14:32:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T18:50:13Z" id="137207937">Err - I had the filter wrong. Its the shingle filter and it does mention it but not very prominently.
</comment><comment author="nik9000" created="2017-02-14T14:32:31Z" id="279722515">I fixed this a while back....</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove test class exclusion for Abstract prefix and rename classes accordingly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13282</link><project id="" key="" /><description>While the list of having exclusions is small, it shouldn't be necessary
at all. Base test cases should be suffixed with TestCase so they are not
picked up by the test class name pattern. This same rule works for
abstract classes as well.

This change renames abstract tests to use the TestCase suffix, adds a
check in naming convention tests, and removes the exclusion from our
test runner configuration. It also excludes inner classes (the only exclude
we should have IMO), so that we have no need to @Ignore the inner test
classes for naming convention tests.
</description><key id="104539438">13282</key><summary>Remove test class exclusion for Abstract prefix and rename classes accordingly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.1.0</label></labels><created>2015-09-02T18:01:12Z</created><updated>2015-09-02T19:34:56Z</updated><resolved>2015-09-02T18:13:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T18:10:39Z" id="137194847">LGTM
</comment><comment author="dadoonet" created="2015-09-02T18:50:48Z" id="137208112">@rjernst This commit seems to break plugins like Azure:

```
Suite: org.elasticsearch.index.store.AbstractAzureFsTest
ERROR   0.01s J0 | AbstractAzureFsTest.initializationError &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.RuntimeException: Suite class org.elasticsearch.index.store.AbstractAzureFsTest should be a concrete class (not abstract).
   &gt;    at com.carrotsearch.randomizedtesting.Validation$ClassValidation.isConcreteClass(Validation.java:90)
   &gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
Completed [14/14] on J0 in 0.02s, 1 test, 1 error &lt;&lt;&lt; FAILURES!


Tests with failures (first 3 out of 4):
  - org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTest.initializationError
  - org.elasticsearch.cloud.azure.AbstractAzureRepositoryServiceTest.initializationError
  - org.elasticsearch.cloud.azure.AbstractAzureTest.initializationError
```
</comment><comment author="rjernst" created="2015-09-02T19:34:56Z" id="137222234">I just pushed a fix for the plugins failures.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>use query_string in bool filter docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13281</link><project id="" key="" /><description>Closes #13279

`s/queryString/query_string/`

Probably needs to be backported to other 1.x branches as well
</description><key id="104533758">13281</key><summary>use query_string in bool filter docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">w33ble</reporter><labels><label>docs</label></labels><created>2015-09-02T17:35:21Z</created><updated>2015-09-02T19:46:31Z</updated><resolved>2015-09-02T19:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T19:10:56Z" id="137216791">LGTM
</comment><comment author="nik9000" created="2015-09-02T19:13:33Z" id="137217396">I'll merge in a bit. For the most part we aren't backporting anything much behind 1.7. I'll certainly commit it to the 2.0 and master branches though.
</comment><comment author="w33ble" created="2015-09-02T19:26:28Z" id="137220503">The 2.0 docs mention bool filters are deprecated in favor of bool queries, so there's nothing to update in the 2.0 docs. 

Only reason I see to backport is that the rest of the 1.x docs are likely the same, so we may want to ensure that google sends people to docs with the right info regardless of the version of the docs. 
</comment><comment author="nik9000" created="2015-09-02T19:46:23Z" id="137224467">&gt; The 2.0 docs mention bool filters are deprecated in favor of bool queries, so there's nothing to update in the 2.0 docs.

Ah! In that case we're fine.

&gt; Only reason I see to backport is that the rest of the 1.x docs are likely the same, so we may want to ensure that google sends people to docs with the right info regardless of the version of the docs.

I don't think this is a big deal. I'll ping @clintongormley who might have another opinion though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Custom Analyzer does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13280</link><project id="" key="" /><description>I am trying to create a new index in Elastic Search as follows

  PUT /abtest2
    {
      "settings":{

```
   "analysis":{

     "char_filter":{

       "replaceJunk":{

         "type":"pattern_replace",
         "pattern":"[^\\d+]",
         "replacement":""
       }
     },

      "analyzer":{

 "dateAnalyzer":{

 "type":"custom",
 "tokenizer":"standard",
 "char_filter":["replaceJunk"]
 }
}
```

   }
  },

```
  "mappings":{

        "fix":{

            "properties": {

               "Date": {
                  "type": "date",
                  "analyzer": "dateAnalyzer"
               }
            }
         }
    }
}
```

so basically, i want to create a custom analyzer for my Date field which might have some non numeric characters coming in. Now, i try to pass a value like

PUT /abtest2/fix/5
{
     "Date":"14186196000-0005"
}
but this gives me an error

```
{
```

   "error": "MapperParsingException[failed to parse [Date]]; nested: MapperParsingException[failed to parse date field [14186196000-0005], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"14186196000-0005\" is malformed at \"00-0005\"]; ",
   "status": 400
}

shouldn't this field have been analyzed and transformed ?

please advise
</description><key id="104533177">13280</key><summary>Custom Analyzer does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2015-09-02T17:31:49Z</created><updated>2015-09-05T12:11:55Z</updated><resolved>2015-09-02T18:06:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-02T18:06:46Z" id="137193723">Hi @abtpst 

Analyzers are for strings, not for date fields.  In 2.0 this mapping would give you an exception.
</comment><comment author="abtpst" created="2015-09-02T18:44:48Z" id="137206252">thanks Clinton, but even if i change it to string, it still does not remove the - character
</comment><comment author="abtpst" created="2015-09-02T18:48:48Z" id="137207476">even if i set the pattern to be just -

"replaceJunk":{ "type":"pattern_replace", "pattern":"-", "replacement":"" } 
</comment><comment author="clintongormley" created="2015-09-05T12:11:55Z" id="137948676">@abtpst Please ask questions like these in the forum http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use of queryString in Bool Filter docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13279</link><project id="" key="" /><description>In the [Bool Filter docs](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/query-dsl-bool-filter.html), the query string parameter is presented as `queryString`, but elsewhere in the docs ([query filter](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/query-dsl-query-filter.html), for example), the parameter is presented as `query_string`. Kibana also uses `query_string` to make sense of the filters.

This page should probably use `query_string` instead.
</description><key id="104533100">13279</key><summary>Use of queryString in Bool Filter docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">w33ble</reporter><labels /><created>2015-09-02T17:31:26Z</created><updated>2015-09-03T18:13:46Z</updated><resolved>2015-09-03T18:13:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-03T18:13:46Z" id="137532146">Fixed via #13281
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Searching multiple aliases using wildcard and ignore_unavailable=true throws index closed exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13278</link><project id="" key="" /><description>Each of my `logstash-YYYY.MM.DD` indices has a filtered alias added to it named `error_messages-YYYY.MM.DD`.  Any that are older than 30 days are closed.

I'm seeing strange behavior when trying to do a match_all query on `error_messages-*`.

This is using the Java  transport client. ES version 1.6.0.

```
SearchResponse sr = client.prepareSearch("error_messages-*")
  .setQuery(QueryBuilders.matchAllQuery())
  .execute().actionGet();
throws:
org.elasticsearch.indices.IndexClosedException: [logstash-2015.07.30] closed
```

So I set `IndicesOptions.lenientExpandOpen()` to ignore unavailable indices and got a different (but similiar) error.

```
SearchResponse sr = client.prepareSearch("error_messages-*")
  .setQuery(QueryBuilders.matchAllQuery())
  .setIndicesOptions(IndicesOptions.lenientExpandOpen())
  .execute().actionGet();
throws:
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [FORBIDDEN/4/index closed]
```

Interestingly, this query works via HTTP API in Marvel:

```
POST error_messages-*/_search?ignore_unavailable=true
{
  "query": {
    "match_all": {}
  }
}
```

Also interesting is if I use the concrete index pattern `logstash-*`, the queries work in Java.
</description><key id="104532766">13278</key><summary>Searching multiple aliases using wildcard and ignore_unavailable=true throws index closed exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bradvido</reporter><labels><label>:Aliases</label><label>:Index APIs</label><label>bug</label></labels><created>2015-09-02T17:29:21Z</created><updated>2016-01-11T12:31:45Z</updated><resolved>2016-01-11T12:31:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-05T12:06:59Z" id="137948380">@javanna any ideas here?
</comment><comment author="javanna" created="2015-10-15T16:44:50Z" id="148450655">sorry it took me a while to get to this, I confirm it's a bug, we don't look at the state of the aliases although we should after #9057. Need to dig deeper to see if this is a regression or just something that we never really covered.
</comment><comment author="bradvido" created="2015-10-15T16:48:31Z" id="148451882">Nice, thanks for the confirmation 
</comment><comment author="oferbar" created="2016-01-09T21:12:56Z" id="170281329">I'm seeing the same issue. When can we expect a fix and in which version of es? (currently on 1.6)
Thx
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0.0beta1 does not start if  ES_GC_LOG_FILE value is set in init file.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13277</link><project id="" key="" /><description>No logs generated for ES (in Debug mode) or system log. process does not start at all. Commenting the line , elasticsearch starts with no issues. Tried creating directory path and empty file with full permission on gc.log ES_GC_LOG_FILE=/var/log/elasticsearch/gc.log, still unable to start with no error message any where.
</description><key id="104531979">13277</key><summary>Elasticsearch 2.0.0beta1 does not start if  ES_GC_LOG_FILE value is set in init file.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels><label>bug</label></labels><created>2015-09-02T17:24:35Z</created><updated>2015-09-05T12:27:31Z</updated><resolved>2015-09-03T04:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-02T22:32:52Z" id="137263865">@ajaybhatnagar when I specify it on the command line, I get this error:

```
&#187; ES_GC_LOG_FILE=/tmp/foo/gc.log bin/elasticsearch
Invalid file name for use with -Xloggc: Filename can only contain the characters [A-Z][a-z][0-9]-_.%[p|t] but it has been "/tmp/foo/gc.log"
Note %p or %t can only be used once
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
```

Which file are you putting it in? It could be the same error but just not being printed to the right place.

Note that even if I specify a super simple log filename ("foo"), I still get the same error, so I think this is a bug:

```
&#187; ES_GC_LOG_FILE=foo bin/elasticsearch
Invalid file name for use with -Xloggc: Filename can only contain the characters [A-Z][a-z][0-9]-_.%[p|t] but it has been "foo"
Note %p or %t can only be used once
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Update integration tests documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13276</link><project id="" key="" /><description>Our docs were lacking behind master/2.0 releases with all the recent test infra changes
</description><key id="104527877">13276</key><summary>Docs: Update integration tests documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>docs</label><label>review</label></labels><created>2015-09-02T17:00:27Z</created><updated>2015-09-10T17:06:08Z</updated><resolved>2015-09-10T17:06:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T17:05:08Z" id="137172731">LGTM - @dadoonet might want to have a look at how you specify that elasticsearch dependency but otherwise its fine.
</comment><comment author="spinscale" created="2015-09-09T11:03:30Z" id="138876482">incorporated all review comments
</comment><comment author="dadoonet" created="2015-09-10T07:11:54Z" id="139135478">I left a small comment. It looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test that packages don't start elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13275</link><project id="" key="" /><description>We don't want either the deb or rpm package to start elasticsearch as soon
as they install nor do we want the package to register elasticsearch to
start on restart. That action is reserved for the administrator. This adds
tests for that.

Closes #13122
</description><key id="104525699">13275</key><summary>Test that packages don't start elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T16:48:35Z</created><updated>2016-03-10T18:13:57Z</updated><resolved>2015-09-10T16:33:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T16:49:53Z" id="137166854">Ping @tlrx more bats to review! This one verifies that we're consistent across the rpm and deb on starting Elasticsearch. We've had some talks on github about whether the deb should start elasticsearch when it installs it and we came down on that being a bad idea. Its agains debian policy, but we're in good company in not liking that policy.
</comment><comment author="dakrone" created="2015-09-09T14:37:27Z" id="138930150">Left some minor comments, otherwise LGTM
</comment><comment author="drewr" created="2015-09-09T14:38:56Z" id="138931115">I absolutely :+1: this, glad that it's finally fixed
</comment><comment author="nik9000" created="2015-09-10T17:26:49Z" id="139317670">Merged to master, 2.x, and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fail if publish host is set to wildcard address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13274</link><project id="" key="" /><description>or probably broadcast/multicast/etc.

In the case of wildcard (e.g. 0.0.0.0), this is ok as a _bind_host_ for listening only. But for a publish_host these addresses will be problematic. we should just fail.

separately, If you specify `Des.network.bind_host=0.0.0.0` es will seemingly do the semi-right thing and pick a publish host like 127.0.0.1. But this should be special cased as well, because javadocs say (http://docs.oracle.com/javase/7/docs/api/java/net/NetworkInterface.html#getByInetAddress%28java.net.InetAddress%29);

```
If the specified IP address is bound to multiple network interfaces it is not defined which network interface is returned.
```
</description><key id="104521395">13274</key><summary>fail if publish host is set to wildcard address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T16:27:46Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-09-03T12:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-02T16:48:05Z" id="137166212">I think we can do two cases. We can handle `0.0.0.0` just like we handle multiple addresses today, using the same logic we'd use if you picked a hostname that resolved to all the addresses on your machine. we pick by v4/v6 preference, then by reachability. 

For multicast and broadcast we should fail, thats just a misconfiguration.
</comment><comment author="s1monw" created="2015-09-02T18:29:50Z" id="137201060">sounds good to me
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How will Java clients survive the 1.x -&gt; 2.x upgrade?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13273</link><project id="" key="" /><description>I'm raising this out of concern for the up and coming 2.0 release.

Currently, java clients communicating with an elasticsearch cluster use the following dependency.

``` xml
&lt;dependency&gt;
  &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
  &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
  &lt;version&gt;1.x.y or 2.x.y&lt;/version&gt;
&lt;/dependency&gt;
```

This is a _very large_ package, essentially containing all the code necessary to operate an elasticsearch node. What clients typically are interested in is the TransportClient code.

A major version bump is allowed to carry incompatible changes, and I will be encountering several scenarios where I will have to write java clients that communicate with both `1.x` and `2.x` clusters.

Traditionally you have been very good at maintaining backwards compatibility, I have 1.4 clients communicating with 1.7 clusters. So Minor versions have not been that much of an issue as long as you control the feature set you use for each cluster version.

The canonical way of solving this would be to extract the public API into a separate artifact, and under a different package name (e.g. `org.elasticsearch2`) to allow the two different client versions to co-exist both in maven, and in the classpath.

Considering the **important** comment on [your java-api client documentation](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/client.html) this seems to ramp up towards becoming a major hassle for me.

Will there be some way of combating this?
</description><key id="104515099">13273</key><summary>How will Java clients survive the 1.x -&gt; 2.x upgrade?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">udoprog</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2015-09-02T16:00:12Z</created><updated>2016-07-08T16:16:32Z</updated><resolved>2016-07-08T16:16:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2015-11-16T16:56:08Z" id="157096283">We also have the need to read / write to two versions at the same time that are not compatible.  Have to change to REST API to do so (at least for one of them).  
</comment><comment author="adrianluisgonzalez" created="2015-11-25T17:22:40Z" id="159679489">I'm facing a similar issue.  We use the java 1.7 transport client to talk to our 1.7 cluster.  I don't see how to upgrade to 2.x without down time and upgrading all clients at the same time.  Or am I missing something?
</comment><comment author="rtijoriwala" created="2016-01-05T23:44:28Z" id="169170832">Is the java 2.0.0 client backwards compatible i.e. can it talk to 1.7 ES server? If so, would it work if clients are upgraded first and then server?
</comment><comment author="adrianluisgonzalez" created="2016-01-06T01:24:15Z" id="169189577">@rtijoriwala, no unfortunately the 2.0.0 client is not backwards compatible if using the binary protocol based on my testing.
</comment><comment author="udoprog" created="2016-05-25T00:32:48Z" id="221441905">So I'm now encountering this in trying to implement support for Elasticsearch `2.x` in [Heroic](https://github.com/spotify/heroic).

I can't just drop existing `1.x` support, since I have about 16 clusters still running it. And without `2.x` support I won't be able to start migrating the clusters.

I've tried to build custom relocations, but unfortunately Apache Lucene has deprecated support for version 3, and relocating Lucene versions is an absolute PITA to the degree that I can't figure out how to do it after several hours of work (loading Codecs, etc... through reflection requires consistent paths).

My last alternative is to fork the entire codebase until I've upgraded all the clusters. But it seems backwards of me to provide two different versions of my service depending on which version of a particular sub-component it can use. If every dependency required this it would be a combinatoric nightmare.

Are there any work being done on this front?
</comment><comment author="nik9000" created="2016-05-25T01:29:22Z" id="221449741">Yes. We are building a rest based Java client which should be as tolerant
of the upgrade cycle as the other rest clients. That is to say: I expect
you'll still have to have some branch statements but it ought to be
manageable. I expect it'll be some time before the rest based Java client
is as nice as the transport client but I'm confident that time will come.
On May 24, 2016 8:32 PM, "John-John Tedro" notifications@github.com wrote:

&gt; So I'm now encountering this in trying to implement support for
&gt; Elasticsearch 2.x in Heroic https://github.com/spotify/heroic.
&gt; 
&gt; I can't just drop existing 1.x support, since I have about 16 clusters
&gt; still running it. And without 2.x support I won't be able to start
&gt; migrating the clusters.
&gt; 
&gt; I've tried to build custom relocations, but unfortunately Apache Lucene
&gt; has deprecated support for version 3, and relocating Lucene versions is an
&gt; absolute PITA to the degree that I can't figure out how to do it after
&gt; several hours of work (loading Codecs, etc... through reflection requires
&gt; consistent paths).
&gt; 
&gt; My last alternative is to fork the entire codebase until I've upgraded all
&gt; the clusters. But it seems backwards of me to provide two different
&gt; versions of my service depending on which version of a particular
&gt; sub-component it can use. If every dependency required this it would be a
&gt; combinatoric nightmare.
&gt; 
&gt; Are there any work being done on this front?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13273#issuecomment-221441905
</comment><comment author="s1monw" created="2016-05-25T07:35:01Z" id="221495883">If you are using the Java API you are considered to be a full blown part of the cluster. Since major version require full-cluster restarts you have to restart the client as well. We all know that this sucks and we are working on a real java client since today we don't have one. The java client is really an elasticsaerch node (lightweight) even if you use the transport "client". What others have build is a custom REST interface tailored to their domain such that they can talk to different version of the cluster etc. I am not saying that is what you should do but there is no other way than replacing the java client with the new version and the new java client we are working on is not going to help for 1.x to 2.x - or lets say very very unlikely
</comment><comment author="udoprog" created="2016-05-25T07:57:33Z" id="221500239">@s1monw the only request I'd like to make is that it should be possible for two major versions of the future client to co-exist in the same VM.

I know of two ways to accomplishing this:
**1)** The best way would be if distinct packages and/or client entry-points were used for each major version. E.g. `org.elasticsearch.client.RestClient1`, and `org.elasticsearch.client.RestClient2`. Both would need to have all their dependencies and classes located in isolation as well, like `org.elasticsearch.client10` and `org.elasticsearch.client20`.
**2)** Spend a tremendous amount of effort and painstaking attention to detail to build a backwards compatible and a granular, versioned client. Like Datastax has done for Cassandra. I dislike this approach for complex APIs (like Elasticsearch) because it necessitates a lot of version management.
</comment><comment author="s1monw" created="2016-05-25T08:05:17Z" id="221501889">&gt; @s1monw the only request I'd like to make is that it should be possible for two major versions of the future client to co-exist in the same VM.

the java clinet as it exists today will be removed. We will have a client that talks to the rest interface which works cross major versions.
</comment><comment author="rtijoriwala" created="2016-06-22T20:54:26Z" id="227873292">@s1monw  - any idea when will this new client be available?
</comment><comment author="brusic" created="2016-06-22T21:21:28Z" id="227880550">@rtijoriwala You can follow the progress here: https://github.com/elastic/elasticsearch/pull/18735
</comment><comment author="cnwarden" created="2016-07-08T03:16:49Z" id="231265752">@s1monw , **we have this same upgrade issue, from 1.7.3 to 2.3.2**, now we are using the 1.7.3 Java API with TransportClient, does it mean we must change API to REST, then it can cross the major version? is there any other way?
</comment><comment author="cnwarden" created="2016-07-08T07:28:27Z" id="231293771">@nik9000 , may I know whether we can use it now?
</comment><comment author="clintongormley" created="2016-07-08T16:16:32Z" id="231403186">You can certainly start using it when 5.0 is out.  Note: it is very early days and initially won't provide the IDE assistance that you get from the transport client, but it will allow you to talk to clusters of other major versions (barring any changes that may have happened to the REST API in between versions)

Given that the REST client is where we will be focussing our attention going forwards, I'm going to close this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>combine different query boost values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13272</link><project id="" key="" /><description>With #11744 we made sure that every query in our DSL supports the `boost` and `_name` fields, meaning that those fields always get parsed and applied to the corresponding lucene query (done in the query-refactoring branch). When dealing with a compound query though, there might be multiple boost values specified in the query like in the following case:

```
{
    "filtered" : {
        "query" : {
            "term" : {
                "field" : {
                    "query" : "value",
                    "boost" : 2
                }
            }
        },
        "boost": 3
    }
}
```

The example above contains for instance a single term query, so the final lucene query will simply be a term query (rather than a filtered query), but what should the boost be? At the moment we ignore the main boost (3) and apply 2 only to the resulting query (in master). Note that only a few amount of compound queries are affected by this "problem". For instance the bool query will always create a lucene boolean query, no matter how many clauses there are (even with a single one), so all the potential boost values are preserved and will be handled by lucene when running the query.

The query_string has a similar, yet a bit different behaviour:

```
{
    "query_string" : {
        "query" : "(field1:value1^3 field2:value2^2)^5",
        "boost" : 2
    }
}
```

In the above case we multiply whatever boost comes out of parsing the query string, with the main boost that gets explicitly set.

In the query_refactoring branch, having centralized where the boost gets applied to the final lucene query, we have the chance to standardize what we do in these few cases, while in master we handle it case by case. I can see at least three options:
- keep the inner query boost (even if 1.0 which is the default) and ignore the main one whatever that is
- keep the main boost (only if != 1.0 which is the default?) and ignore the inner one
- combine the two boost values, e.g. multiply them

I am leaning towards the last option to be honest, but I think this is worth some discussion. What do people think about this?
</description><key id="104513488">13272</key><summary>combine different query boost values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>enhancement</label></labels><created>2015-09-02T15:52:11Z</created><updated>2015-09-08T16:01:11Z</updated><resolved>2015-09-08T16:01:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-02T15:58:08Z" id="137143400">I am a little confused about the example, is it because of the query/filter refactoring for 2.0?

Historically, setting boost on `filtered` should work as lucene's FilteredQuery supported that (and multiplied the boost into the inner weight itself).

But now filtered is really gone, its replaced by BooleanQuery. So IMO that boost on `filtered` should be applied to booleanquery which is the implementation of `filtered`
</comment><comment author="jpountz" created="2015-09-02T16:14:31Z" id="137148811">&gt; The example above contains for instance a single term query, so the final lucene query will simply be a term query (rather than a filtered query), but what should the boost be?

Why wouldn't it be a filtered query? (or rather a boolean query now) This is something that query rewriting will happily simplify when rewriting the query (single-clause boolean queries rewrite to the wrapped clause, and merge boosts by multiplying them) so I don't think we should try to optimize things at parsing time?
</comment><comment author="javanna" created="2015-09-02T17:33:01Z" id="137181122">sorry the main example was around filtered query which is deprecated now, but this is not caused by the filter refactoring, it is not a lucene issue at all, it is something that we do in elasticsearch while parsing queries and converting them back to lucene queries... have a look [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java#L97) for instance. From your answers I would say we should fix the filtered query to always create a boolean query under the hood. That sounds good to me.

query_string seems to behave correctly by manually multiplying the boost parsed from the query and the boost set to the query.

Other cases where we might have a similar problem are wrapper query (available only in the java api),  query filter (deprecated) and template queries. Those are all wrapper for queries, but they do allow to specify a boost (new in query-refactoring branch), so you might end up with the main boost and the boost of the inner query, which will become the actual lucene query. I don't know yet what we should do with those boosts, although I realize that this is an edge case and it doesn't make much sense to set the boost to some wrapper query...
</comment><comment author="rmuir" created="2015-09-02T19:34:20Z" id="137222129">&gt; sorry the main example was around filtered query which is deprecated now, but this is not caused by the filter refactoring, it is not a lucene issue at all, it is something that we do in elasticsearch while parsing queries and converting them back to lucene queries... have a look here for instance. From your answers I would say we should fix the filtered query to always create a boolean query under the hood. That sounds good to me.

ok I get it. Looks like we do a lot of "rewrite()" type logic in the parser itself and it has some bugs where it drops boosts. I just see that as a bug: in general for cases like that, multiply the boosts. 
</comment><comment author="jpountz" created="2015-09-03T18:14:58Z" id="137532382">@javanna Can it be closed now that #13312 is merged or is there more stuff to do?
</comment><comment author="javanna" created="2015-09-03T18:22:32Z" id="137534152">@jpountz I have a little more things to do, will close when done
</comment><comment author="javanna" created="2015-09-08T16:01:08Z" id="138610737">We can close this now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[qa] Add smoke test client module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13271</link><project id="" key="" /><description>This commit adds a new smoke test for testing client as a end Java user.

It starts a cluster in `pre-integration-test` phase, then execute the client operations defined as JUnit tests within `integration-test` phase and then stop the external cluster in `post-integration-test` phase.

You can also run test classes from your IDE.
- Start an external node on your machine with `bin/elasticsearch` (note that you can test Java API regressions if you run an older or newer node version)
- Run the JUnit test. By default, it will run tests on `localhost:9300` but you can change this setting using system property `tests.cluster`. It also expects the default `cluster.name` (`elasticsearch`).

This commit also starts adding [snippets as defined by Maven](https://maven.apache.org/guides/mini/guide-snippet-macro.html) to help keeping automatically synchronized the Java reference guide with the current code.

Our documentation builder tool does not support snippets though but we will most likely support it at some point.

This is a followup for #13252. See https://github.com/elastic/elasticsearch/pull/13252#discussion_r38508098
</description><key id="104504490">13271</key><summary>[qa] Add smoke test client module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T15:15:41Z</created><updated>2015-11-22T10:13:30Z</updated><resolved>2015-09-02T15:36:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-02T15:22:10Z" id="137128237">thanks for doing this test. looks good to me. i dont understand the maven snippet stuff, but then again its just code comments so it does not hurt.
</comment><comment author="dadoonet" created="2015-09-02T15:33:15Z" id="137135105">The goal for SNIPPETS is to be able to write in our asciidoc something like:

``` asciidoc
[source, java]
-----------------------
%{snippet|id=java-doc-index-doc-simple|file=qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java}
-----------------------
```

When rendering the doc, this will actually replace all snippets and will generate a new version of our asciidoc:

``` asciidoc
[source, java]
-----------------------
        client.prepareIndex(index, "doc", "1")  // Index, Type, Id
                .setSource("foo", "bar")        // Simple document: { "foo" : "bar" }
                .get();                         // Execute and wait for the result
-----------------------
```

Which will be rendered nicely in the reference Java doc instead of writing and maintaining code in doc like [this example](https://github.com/elastic/elasticsearch/blob/master/docs/java-api/docs/index_.asciidoc).

As tests are compiled and ran, we make sure that our documentation is always accurate!

Make sense?

But we have some pre-process work to do before that in the building doc script.
</comment><comment author="rmuir" created="2015-09-02T15:53:03Z" id="137141632">&gt; As tests are compiled and ran, we make sure that our documentation is always accurate!
&gt; 
&gt; Make sense?

Yes, this looks very interesting. I was just mentioning i had never seen it before. It is otherwise nearly impossible to keep code examples working!
</comment><comment author="dadoonet" created="2015-09-02T15:58:15Z" id="137143487">I used that a lot formerly. `mvn site` basically renders a full web site for you out of the box and docs can contains snippets, reports like test coverage, checkstyle, findbugs...

We don't use the site plugin but we can take some of the good ideas Maven have! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Manual synchronization when iterating over listeners in InternalClusterInfoService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13270</link><project id="" key="" /><description>Iterating over a synchronized collection created with `Collections.synchronizedSet()` must be manually synchronized.

I hit this one when running an integration test multiple times (`-Dtest.iters=1000`) that uses `NodeService` and `DiskThresholdDecider`:

``` java
Rgs 02, 2015 1:49:36 PM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException
WARNING: Uncaught exception in thread: Thread[elasticsearch[node_s0][management][T#3],5,TGRP-NodeStatsCollectorTests]
 java.util.ConcurrentModificationException
    at __randomizedtesting.SeedInfo.seed([F74F37441AC8278A]:0)
    at java.util.HashMap$HashIterator.nextNode(HashMap.java:1429)
    at java.util.HashMap$KeyIterator.next(HashMap.java:1453)
    at org.elasticsearch.cluster.InternalClusterInfoService$ClusterInfoUpdateJob.run(InternalClusterInfoService.java:396)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="104502562">13270</key><summary>Manual synchronization when iterating over listeners in InternalClusterInfoService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T15:09:14Z</created><updated>2016-03-10T18:13:57Z</updated><resolved>2015-09-04T11:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-03T10:56:08Z" id="137407521">Good catch. This is easy to forget (and sucks). Can't we just use copy on write list? we don't really use the set functionality... 
</comment><comment author="tlrx" created="2015-09-04T09:25:53Z" id="137687934">@bleskes I agree, using CopyOnWriteArrayList is better: I just updated this PR. Is it OK?
</comment><comment author="bleskes" created="2015-09-04T10:42:46Z" id="137702259">LGTM
</comment><comment author="kimchy" created="2015-09-04T10:47:19Z" id="137703219">should we backport it to 2.0 branch as well? seems like a bug worth fixing for it.
</comment><comment author="bleskes" created="2015-09-04T10:49:02Z" id="137703416">+1 on my end though the implications are very minor. 

&gt; On 04 Sep 2015, at 12:47, Shay Banon notifications@github.com wrote:
&gt; 
&gt; should we backport it to 2.0 branch as well? seems like a bug worth fixing for it.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="tlrx" created="2015-09-04T10:49:41Z" id="137703506">@kimchy yes, I think we should backport this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding back decommissioned nodes...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13269</link><project id="" key="" /><description>Hello,

I'm hoping that someone can help me...

I've used the following command:- 
        curl -XPUT localhost:9200/_cluster/settings -d '{  "transient" : {        "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
          }
         }'

to decommission a node from a cluster. That went fine with no problems and ES excluded it and didn't put any shards or replicas on it which is the expected behaviour. Cluster health was yellow when node was initially decommissioned and green when it was safe to now terminate the node as all the data had been reallocated.

I now had a need to add back the decommissioned node(what i hadn't terminated) and ran the following (on the next day):-

I've used the following command:- 
        curl -XPUT localhost:9200/_cluster/settings -d '{ "transient" : {
            "cluster.routing.allocation.include._ip" : "10.0.0.1"
           }
          }'

believing that this would add the excluded node back into the cluster and then ES would immediately begin to redistribute the shards and replicas across it. However what happened was that my ES cluster immediately went red, logstash stopped bringing messages through and looking at my cluster through the head plugin, I could see that allocation of shards had completely stopped and that messages had begun queuing in our message manager(Redis).

Can you please tell me what is the correct way to add back a decommissioned node ? 

Many thanks
</description><key id="104489033">13269</key><summary>Adding back decommissioned nodes...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trekr5</reporter><labels /><created>2015-09-02T14:07:15Z</created><updated>2015-09-05T10:59:09Z</updated><resolved>2015-09-05T10:59:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-05T10:59:09Z" id="137943234">Hi @trekr5 

This will do it for you:

```
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.exclude._ip": ""
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] print test start and end of test setup and cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13268</link><project id="" key="" /><description>Currently we do not print start and end of SuiteScope tests which makes it tricky to debug failures. This pr changes this. To show how this looks like I made 3 test classes with annotations 

`@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST)` (ScopeTestTest)
`@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE)` (ScopeSuiteTest)
`@ESIntegTestCase.SuiteScopeTestCase` (SuiteScopeTest)

and printed out the messages which are now:

```
$ mvn test -Dtests.class=org.elasticsearch.test.test.test.* -Dtests.output=always -Des.logger.level=INFO | grep 'SuiteScopeTest\|ScopeSuiteTest\|ScopeTestTest'
HEARTBEAT J2 PID(33413@darkstar-2): 2015-09-02T12:00:06, stalled for 10.4s at: SuiteScopeTest (suite)
Suite: org.elasticsearch.test.test.test.SuiteScopeTest
  1&gt; [2015-09-02 11:59:59,737][INFO ][test.test.test           ] [SuiteScopeTest]: setup suite
  1&gt; [2015-09-02 12:00:08,918][INFO ][test.test.test           ] [SuiteScopeTest#test1]: starting test
  1&gt; [2015-09-02 12:00:08,918][INFO ][test.test.test           ] [SuiteScopeTest#test1]: finished test
  1&gt; [2015-09-02 12:00:09,086][INFO ][test.test.test           ] [SuiteScopeTest#test2]: starting test
  1&gt; [2015-09-02 12:00:09,087][INFO ][test.test.test           ] [SuiteScopeTest#test2]: finished test
  1&gt; [2015-09-02 12:00:09,088][INFO ][test.test.test           ] [SuiteScopeTest]: cleaning up after suite
Suite: org.elasticsearch.test.test.test.ScopeSuiteTest
  1&gt; [2015-09-02 11:59:59,715][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: setup test
  1&gt; [2015-09-02 12:00:09,361][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: starting test
  1&gt; [2015-09-02 12:00:09,361][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: finished test
  1&gt; [2015-09-02 12:00:09,362][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: cleaning up after test
  1&gt; [2015-09-02 12:00:09,503][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: cleaned up after test
  1&gt; [2015-09-02 12:00:09,753][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: setup test
  1&gt; [2015-09-02 12:00:09,757][INFO ][test                     ] Successfully wiped data directory for node location: /Users/britta/es/core/target/J1/temp/org.elasticsearch.test.test.test.ScopeSuiteTest_D2C2EBA0E34B87CC-001/tempDir-001/data/SUITE-CHILD_VM=[1]-CLUSTER_SEED=[5187104418634291104]-HASH=[753730814E6B]-cluster/nodes/0
  1&gt; [2015-09-02 12:00:09,828][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: starting test
  1&gt; [2015-09-02 12:00:09,828][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: finished test
  1&gt; [2015-09-02 12:00:09,828][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: cleaning up after test
  1&gt; [2015-09-02 12:00:09,862][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: cleaned up after test
Suite: org.elasticsearch.test.test.test.ScopeTestTest
  1&gt; [2015-09-02 11:59:59,404][INFO ][test.test.test           ] [ScopeTestTest#test1]: setup test
  1&gt; [2015-09-02 12:00:09,574][INFO ][test.test.test           ] [ScopeTestTest#test1]: starting test
  1&gt; [2015-09-02 12:00:09,574][INFO ][test.test.test           ] [ScopeTestTest#test1]: finished test
  1&gt; [2015-09-02 12:00:09,576][INFO ][test.test.test           ] [ScopeTestTest#test1]: cleaning up after test
  1&gt; [2015-09-02 12:00:09,708][INFO ][test.test.test           ] [ScopeTestTest#test1]: cleaned up after test
  1&gt; [2015-09-02 12:00:09,940][INFO ][test.test.test           ] [ScopeTestTest#test2]: setup test
  1&gt; [2015-09-02 12:00:10,299][INFO ][test.test.test           ] [ScopeTestTest#test2]: starting test
  1&gt; [2015-09-02 12:00:10,299][INFO ][test.test.test           ] [ScopeTestTest#test2]: finished test
  1&gt; [2015-09-02 12:00:10,299][INFO ][test.test.test           ] [ScopeTestTest#test2]: cleaning up after test
  1&gt; [2015-09-02 12:00:10,377][INFO ][test.test.test           ] [ScopeTestTest#test2]: cleaned up after test
```

and were before:

```
$ mvn test -Dtests.class=org.elasticsearch.test.test.test.* -Dtests.output=always -Des.logger.level=INFO | grep 'SuiteScopeTest\|ScopeSuiteTest\|ScopeTestTest'
HEARTBEAT J2 PID(33463@darkstar-2): 2015-09-02T12:03:41, stalled for 11.7s at: SuiteScopeTest (suite)
Suite: org.elasticsearch.test.test.test.ScopeSuiteTest
  1&gt; [2015-09-02 12:03:44,686][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: before test
  1&gt; [2015-09-02 12:03:44,688][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: cleaning up after test
  1&gt; [2015-09-02 12:03:45,058][INFO ][test.test.test           ] [ScopeSuiteTest#test1]: cleaned up after test
  1&gt; [2015-09-02 12:03:45,291][INFO ][test                     ] Successfully wiped data directory for node location: /Users/britta/es/core/target/J1/temp/org.elasticsearch.test.test.test.ScopeSuiteTest_851DF8D3BEED2F10-001/tempDir-001/data/SUITE-CHILD_VM=[1]-CLUSTER_SEED=[2791181108065013943]-HASH=[7569101256AA]-cluster/nodes/0
  1&gt; [2015-09-02 12:03:45,292][INFO ][test                     ] Successfully wiped data directory for node location: /Users/britta/es/core/target/J1/temp/org.elasticsearch.test.test.test.ScopeSuiteTest_851DF8D3BEED2F10-001/tempDir-001/data/SUITE-CHILD_VM=[1]-CLUSTER_SEED=[2791181108065013943]-HASH=[7569101256AA]-cluster/nodes/2
  1&gt; [2015-09-02 12:03:45,294][INFO ][test                     ] Successfully wiped data directory for node location: /Users/britta/es/core/target/J1/temp/org.elasticsearch.test.test.test.ScopeSuiteTest_851DF8D3BEED2F10-001/tempDir-001/data/SUITE-CHILD_VM=[1]-CLUSTER_SEED=[2791181108065013943]-HASH=[7569101256AA]-cluster/nodes/1
  1&gt; [2015-09-02 12:03:45,323][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: before test
  1&gt; [2015-09-02 12:03:45,324][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: cleaning up after test
  1&gt; [2015-09-02 12:03:45,413][INFO ][test.test.test           ] [ScopeSuiteTest#test2]: cleaned up after test
Suite: org.elasticsearch.test.test.test.SuiteScopeTest
  1&gt; [2015-09-02 12:03:44,946][INFO ][test.test.test           ] [SuiteScopeTest]: before suite
  1&gt; [2015-09-02 12:03:45,203][INFO ][test.test.test           ] [SuiteScopeTest]: cleaning up after suite
  1&gt; [2015-09-02 12:03:45,576][INFO ][test.test.test           ] [SuiteScopeTest]: cleaned up after suite
Suite: org.elasticsearch.test.test.test.ScopeTestTest
  1&gt; [2015-09-02 12:03:43,715][INFO ][test.test.test           ] [ScopeTestTest#test1]: before test
  1&gt; [2015-09-02 12:03:43,717][INFO ][test.test.test           ] [ScopeTestTest#test1]: cleaning up after test
  1&gt; [2015-09-02 12:03:43,994][INFO ][test.test.test           ] [ScopeTestTest#test1]: cleaned up after test
  1&gt; [2015-09-02 12:03:45,478][INFO ][test.test.test           ] [ScopeTestTest#test2]: before test
  1&gt; [2015-09-02 12:03:45,481][INFO ][test.test.test           ] [ScopeTestTest#test2]: cleaning up after test
  1&gt; [2015-09-02 12:03:45,573][INFO ][test.test.test           ] [ScopeTestTest#test2]: cleaned up after test
```
</description><key id="104446908">13268</key><summary>[test] print test start and end of test setup and cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-02T10:14:09Z</created><updated>2015-11-22T10:13:30Z</updated><resolved>2015-09-02T17:44:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-02T15:19:31Z" id="137125820">LGTM.
</comment><comment author="javanna" created="2015-09-02T17:43:29Z" id="137184382">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure: ClusterInfoServiceIT.testClusterInfoServiceCollectsInformation - data path in shard routing was null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13267</link><project id="" key="" /><description>Build URL: http://build-us-00.elastic.co/job/es_core_master_centos/7253/testReport/junit/org.elasticsearch.cluster/ClusterInfoServiceIT/testClusterInfoServiceCollectsInformation/

reproduce command:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=3236D73A80EAE74A -Dtests.class=org.elasticsearch.cluster.ClusterInfoServiceIT -Dtests.method="testClusterInfoServiceCollectsInformation" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=789m -Dtests.locale=no_NO -Dtests.timezone=America/Blanc-Sablon
```

stack trace:

```
java.lang.AssertionError
    at __randomizedtesting.SeedInfo.seed([3236D73A80EAE74A:28AF9A8B95E09069]:0)
    at org.junit.Assert.fail(Assert.java:86)
    at org.junit.Assert.assertTrue(Assert.java:41)
    at org.junit.Assert.assertNotNull(Assert.java:621)
    at org.junit.Assert.assertNotNull(Assert.java:631)
    at org.elasticsearch.cluster.ClusterInfoServiceIT.testClusterInfoServiceCollectsInformation(ClusterInfoServiceIT.java:205)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1638)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:847)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:897)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:856)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:792)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:803)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="104439481">13267</key><summary>Test Failure: ClusterInfoServiceIT.testClusterInfoServiceCollectsInformation - data path in shard routing was null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label><label>v2.1.0</label></labels><created>2015-09-02T09:36:38Z</created><updated>2015-10-23T07:26:05Z</updated><resolved>2015-10-23T07:26:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T07:26:05Z" id="150496524">this has been fixed a while ago
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure: SearchQueryIT.testIssue3177 - forced merged attempted on closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13266</link><project id="" key="" /><description>Build URL: http://build-us-00.elastic.co/job/elasticsearch-20-strong/311/testReport/junit/org.elasticsearch.search.query/SearchQueryIT/testIssue3177/

Cannot reproduce locally

Reproduction command:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=B91C259E8901B4E2 -Dtests.class=org.elasticsearch.search.query.SearchQueryIT -Dtests.method="testIssue3177" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=835m -Dtests.locale=pt -Dtests.timezone=Africa/Asmera
```

Stack trace for failure:

```
java.lang.AssertionError: Unexpected ShardFailures: [[test][2] failed, reason [BroadcastShardOperationFailedException[operation indices:admin/optimize failed]; nested: ForceMergeFailedEngineException[force merge failed]; nested: EngineClosedException[CurrentState[CLOSED] Closed]; ]]
Expected: &lt;0&gt;
     but: was &lt;1&gt;
    at __randomizedtesting.SeedInfo.seed([B91C259E8901B4E2:681C883A5FC3B16F]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures(ElasticsearchAssertions.java:343)
    at org.elasticsearch.test.ESIntegTestCase.optimize(ESIntegTestCase.java:1226)
    at org.elasticsearch.search.query.SearchQueryIT.testIssue3177(SearchQueryIT.java:148)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1638)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:847)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:897)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:856)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:792)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:803)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="104435248">13266</key><summary>Test Failure: SearchQueryIT.testIssue3177 - forced merged attempted on closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label><label>v2.0.0</label></labels><created>2015-09-02T09:17:43Z</created><updated>2015-10-05T15:27:36Z</updated><resolved>2015-10-05T15:27:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-02T16:59:47Z" id="137170889">The test fails because optimize is called and no errors are expected but optimize fails because a shard is relocating. Here is how:

First of all, when a shard relocates while optimize is called the call might fail with 
`ForceMergeFailedEngineException[force merge failed]; nested: AlreadyClosedException[this IndexWriter is closed]` if:
- the optimize request was send to a shard that is currently relocating
- the request is started while the shard is still active and open but executed after it was closed (which happens very rarely)

To avoid this in the test we create an index, wait for relocations to finish and only then call optimize. 
This is the part of the code:

```
        createIndex("test");
        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").get();
        client().prepareIndex("test", "type1", "2").setSource("field1", "value2").get();
        client().prepareIndex("test", "type1", "3").setSource("field1", "value3").get();
        waitForRelocation();
        optimize();
```

This assumes that after `waitForRelocation()` has returned shards are no more relocated and optimize will succeed always. 

However, because the test does not wait for green status, relocations can still start after `waitForRelocation()` has returned successfully. 
In this case, here is the cluster state after all shards have been assigned but not all allocated:

```
nodes: 
{node_s2}{EXoFzKQETGiqaiRDrLPbCg}{127.0.0.1}{127.0.0.1:9402}{mode=network}
{node_s0}{iUkDDsM2T-OGXFNYdljzBQ}{127.0.0.1}{127.0.0.1:9400}{mode=network}, master
{node_s1}{gW0qrQwiQpOzwgOfzKUlPg}{127.0.0.1}{127.0.0.1:9401}{mode=network}
[...]
routing_nodes:
-----node_id[gW0qrQwiQpOzwgOfzKUlPg][V]
--------[test][1], node[gW0qrQwiQpOzwgOfzKUlPg], [R], v[2], s[INITIALIZING], a[id=bdfh-y94SOi9bfw_zJ9OYQ], unassigned_info[[reason=INDEX_CREATED], at[2015-09-02T16:40:03.287Z]]
--------[test][3], node[gW0qrQwiQpOzwgOfzKUlPg], [P], v[3], s[STARTED], a[id=LfWoFMG-Rr2b-T5xXcYfNQ]
--------[test][2], node[gW0qrQwiQpOzwgOfzKUlPg], [R], v[3], s[STARTED], a[id=PJUO1DYETwujMkyCh9iR3A]
--------[test][5], node[gW0qrQwiQpOzwgOfzKUlPg], [R], v[3], s[STARTED], a[id=HUnQpyvbTFycsVRZOYe87w]
--------[test][0], node[gW0qrQwiQpOzwgOfzKUlPg], [P], v[2], s[STARTED], a[id=fbzg-cteQo6A9nIqCpfKXQ]
-----node_id[iUkDDsM2T-OGXFNYdljzBQ][V]
--------[test][1], node[iUkDDsM2T-OGXFNYdljzBQ], [P], v[2], s[STARTED], a[id=B7XeUI8-TqSXKA7ZnASn0Q]
--------[test][4], node[iUkDDsM2T-OGXFNYdljzBQ], [P], v[3], s[STARTED], a[id=Xne9jkeJRZCLgxeU4ZDEUw]
--------[test][0], node[iUkDDsM2T-OGXFNYdljzBQ], [R], v[2], s[INITIALIZING], a[id=IPATKM4TTRac_duGlNgEWQ], unassigned_info[[reason=INDEX_CREATED], at[2015-09-02T16:40:03.287Z]]
-----node_id[EXoFzKQETGiqaiRDrLPbCg][V]
--------[test][3], node[EXoFzKQETGiqaiRDrLPbCg], [R], v[3], s[STARTED], a[id=8BDoiJRgS0euiPu9svsgNA]
--------[test][2], node[EXoFzKQETGiqaiRDrLPbCg], [P], v[3], s[STARTED], a[id=MbLEmN88TPOnZzD7Kj4Vcg]
--------[test][4], node[EXoFzKQETGiqaiRDrLPbCg], [R], v[3], s[STARTED], a[id=rSdf8UHSQpuFZeDLbH6RNw]
--------[test][5], node[EXoFzKQETGiqaiRDrLPbCg], [P], v[3], s[STARTED], a[id=dEuJ6wkYTo-XeJAch4AVYA]
---- unassigned

tasks: (1):
131/URGENT/shard-started ([test][0], node[iUkDDsM2T-OGXFNYdljzBQ], [R], v[2], s[INITIALIZING], a[id=IPATKM4TTRac_duGlNgEWQ], unassigned_info[[reason=INDEX_CREATED], at[2015-09-02T16:40:03.287Z]]), reason [after recovery (replica) from node [{node_s1}{gW0qrQwiQpOzwgOfzKUlPg}{127.0.0.1}{127.0.0.1:9401}{mode=network}]]/10.6s

```

Because of the uneven distribution of shards one shard will be relocated from `node_s0` to `node_s1` after waiting for relocations is done and the subsequent optimize might then fail.

An ensureGreen() will fix the issue but I do wonder why we start of with such an odd distribution of shards to begin with. I will investigate this.
</comment><comment author="s1monw" created="2015-09-02T18:21:36Z" id="137198389">&gt; An ensureGreen() will fix the issue but I do wonder why we start of with such an odd distribution of shards to begin with. I will investigate this.

this allocation is a corner case that I tried to special case in the shard allocator here: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java#L647

in the last allocation round we have wo shards left:

```
--------[test][1], node[gW0qrQwiQpOzwgOfzKUlPg], [R], v[2], s[INITIALIZING], a[id=bdfh-y94SOi9bfw_zJ9OYQ], unassigned_info[[reason=INDEX_CREATED], at[2015-09-02T16:40:03.287Z]]
--------[test][0], node[iUkDDsM2T-OGXFNYdljzBQ], [R], v[2], s[INITIALIZING], a[id=IPATKM4TTRac_duGlNgEWQ], unassigned_info[[reason=INDEX_CREATED], at[2015-09-02T16:40:03.287Z]]
```

ideally we would put both on `-----node_id[iUkDDsM2T-OGXFNYdljzBQ][V]` but it already has shard `--------[test][1]` so we had to put it somewhere else... maybe there is a bug in the code or we should take another look at solving it differently.
</comment><comment author="brwe" created="2015-09-07T17:36:25Z" id="138346888">We ( @bleskes and I) discussed further about the fact that we get a failure at all for optimize while relocating. Before I wrote

&gt; The test fails because optimize is called and no errors are expected but optimize fails because a shard is relocating.

But actually, optimize should not fail at all just because a shard is relocating. The reason why we get a failure is that the Exceptions returned by the call because a shard is closed are not all handled the same. I made a pr to fix this (#13380).

However, while discussing this we found another issue with this: when we call optimize in tests and shards are relocating we might actually miss some because they are relocating and maybe the coordinating node for the request does not have the new address of the shards yet, request is not sent to initializing shards etc. We should fix that as well as it might cause test failures too.

In addition other apis like stats does not seem to behave as expected too when shards are relocating. @tlrx might add more to that.

Overall I now think the problem is not so much that shards are relocating in tests unexpectedly but more that apis don't behave as expected when shards are relocating?
</comment><comment author="tlrx" created="2015-09-08T08:11:42Z" id="138470051">&gt; In addition other apis like stats does not seem to behave as expected too when shards are relocating. @tlrx might add more to that.

I noticed that Indices Stats API, when executed at shard level, throws a ShardNotFoundException if the shard has no routing entry (see code [here](https://github.com/elastic/elasticsearch/blob/6e2dc7302366e9af861efa23b850999cd83748aa/core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java#L99-L102)). 

Exceptions are then accumulated at `TransportBroadcastByNodeAction` level and shards operations that throws exceptions are just ignored and excluded from indices stats.
</comment><comment author="brwe" created="2015-10-05T15:27:36Z" id="145569730">Test was fixed. I opened #13719 for a general discussion on how different apis deal with relocating shards.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Detecting exising indices and shards is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13265</link><project id="" key="" /><description>Today we rely on FS operations to find the indices on disk or to find the shards for an index. This is super error prone and requires String parsing. We should on each level `index` -&gt; `shard` know exactly what to expect and don't use directory listings which are expensive and subject to change. There should be a metadata file on each level that is atomically written that we open and see what we have to expect no matter of what's on the FS.
</description><key id="104413729">13265</key><summary>Detecting exising indices and shards is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>discuss</label></labels><created>2015-09-02T06:49:48Z</created><updated>2016-03-15T03:30:44Z</updated><resolved>2016-03-15T03:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-02T13:54:20Z" id="137089368">+1 to moving away from parsing things from disk
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Locking in NodeEnvironment is completely broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13264</link><project id="" key="" /><description>after the lucene 5.3 upgrade, i looked at how ES uses lucene's filesystem locking. most places are ok, obtaining a lock and doing stuff in a try/finally. However NodeEnvironment is a totally different story. Can we fix the use of locking here?
1. `deleteShardDirectorySafe` is anything but safe. it calls `deleteShardDirectoryUnderLock` which doesn't actually delete under a lock either!!!! It calls this bogus method: `acquireFSLockForPaths` which acquires _then releases_ locks. Why? Why? Why?
2. `assertEnvIsLocked` is only called under assert. why? Look at `findAllIndices`, its about to do something really expensive, why can't the call to `ensureValid` be a real check?
3. `assertEnvIsLocked` has a bunch of leniency, why in the hell would it return `true` when closed or when there are no locks at all, thats broken.

After this stuff is fixed, any places here doing heavy operations (e.g. N filesystem operations) should seriously consider calling `ensureValid` on any locks that are supposed to be held. It means you do N+1 operations or whatever but man, if what we are doing is not important, then why are we using fs locks?
</description><key id="104399835">13264</key><summary>Locking in NodeEnvironment is completely broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Store</label><label>bug</label><label>discuss</label></labels><created>2015-09-02T04:16:39Z</created><updated>2016-03-15T03:30:44Z</updated><resolved>2016-03-15T03:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-02T06:42:50Z" id="136953971">&gt; deleteShardDirectorySafe is anything but safe. it calls deleteShardDirectoryUnderLock which doesn't actually delete under a lock either!!!! It calls this bogus method: acquireFSLockForPaths which acquires then releases locks. Why? Why? Why?

so we need to clarify the naming here, we actually delete under lock but it's not the IW lock it's a shard lock that we maintain internally per  Node.

``` Java
public void deleteShardDirectorySafe(ShardId shardId, @IndexSettings Settings indexSettings) throws IOException {
        // This is to ensure someone doesn't use Settings.EMPTY
        assert indexSettings != Settings.EMPTY;
        final Path[] paths = availableShardPaths(shardId);
        logger.trace("deleting shard {} directory, paths: [{}]", shardId, paths);
        try (ShardLock lock = shardLock(shardId)) { // &lt;==== here we lock and keep the lock - it's JVM internal
            deleteShardDirectoryUnderLock(lock, indexSettings);
        }
    }
```

the `Why? Why? Why?` is especially interesting and I think you should talk to the guy who reviewed the change: `https://github.com/elastic/elasticsearch/pull/11127` He said:

`yes, I am +1 for this approach. Maybe we can add a line to the javadocs just mentioning this is the case. Especially if you think about locking on shared filesystems, we should not rush to do something complex.`

I am not sure if the Javadoc happened but it need clarification.

&gt; assertEnvIsLocked is only called under assert. why? Look at findAllIndices, its about to do something really expensive, why can't the call to ensureValid be a real check?

+1 to make it a real check where it makes sense...

&gt; assertEnvIsLocked has a bunch of leniency, why in the hell would it return true when closed or when there are no locks at all, thats broken.

+1 to beef it up.
</comment><comment author="rmuir" created="2015-09-02T06:49:20Z" id="136954687">Yes, I remember this, but now is our chance to fix it, so locking is as good as we can make it. It seems we are broken because of the placement of the lock files being underneath what is deleted, but that is something fixable.

Its 2.0, there is no constraint about back compat here, so I think its time to fix it correctly.

Additionally we spent lots of time, and added lots of paranoia in lucene to actually help with shitty behavior from shared filesystems, so it would be nice if it stands a chance.

As far as the shard lock, i have no idea what that is. How is it better than a filesystem lock? Its definitely got a shitload of abstractions, but i can't tell if its anything more than a in-process RWL.
</comment><comment author="s1monw" created="2015-09-02T07:01:26Z" id="136956523">I think we should make this straight forward and add a `/locks` directory to each root path we are using. This directory can then be full of locks and will never be deleted. The crucial part is that it has to be on the same mount as the actual data it protects otherwise it will likely not help at all in the shared FS case.
</comment><comment author="rmuir" created="2015-09-02T08:14:02Z" id="136972133">yeah, i think something along those lines: though is 'never deleted' a problem with ppl that have tons and tons of shards cycling through? accumulating a bunch of 0-byte files sounds dangerous and eventually the directory is gonna crap its pants. 

Deleting an NIOFS lock file is especially tricky and we just don't do it ever in lucene (we leave the lock file around). I dont know how to fix that without adding a "master" lock file that always stays around and is acquired around individual lock acquire/release+delete.
</comment><comment author="s1monw" created="2015-09-03T07:34:26Z" id="137361080">&gt; Deleting an NIOFS lock file is especially tricky and we just don't do it ever in lucene (we leave the lock file around). I dont know how to fix that without adding a "master" lock file that always stays around and is acquired around individual lock acquire/release+delete.

yeah we won't have a way around that I guess. I think what we can do is to have an `$index_name.lock` that you need to own to make changes to the `$index_name_$shardId.lock` which also allows to delete it. That reduces the set to the number of indices. We can safely delete the `$index_name.lock` once the last shard of the index is deleted?
</comment><comment author="rmuir" created="2015-09-03T07:44:45Z" id="137364794">Why do that? just have global.lock. Its only needed around the actaul _acquire_ and _release+delete_. Its not gonna cause a concurrency issue. 

Doing this in a more fine grained way makes zero sense.
</comment><comment author="s1monw" created="2015-09-03T07:57:52Z" id="137370804">&gt; Doing this in a more fine grained way makes zero sense.

having an index level lock make sense to have here anyway since we also have index metadata we want to protect from concurrent modifications. All I was saying here is that we might be able to get away with not locking the global lock as long as we are in the context of an index. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve jacoco coverage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13263</link><project id="" key="" /><description>Upgrade jacoco version and allow it to run with security manager enabled.
</description><key id="104389614">13263</key><summary>Improve jacoco coverage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.1.0</label></labels><created>2015-09-02T02:16:03Z</created><updated>2015-09-02T02:39:59Z</updated><resolved>2015-09-02T02:39:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T02:18:18Z" id="136915497">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging test for filesystem scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13262</link><project id="" key="" /><description>Adds a tests for loading scripts from the filesystem for search templates
and for search filters.

Closes #13184
</description><key id="104389545">13262</key><summary>Packaging test for filesystem scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-09-02T02:15:02Z</created><updated>2016-03-10T18:13:19Z</updated><resolved>2015-09-16T14:45:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T02:17:10Z" id="136915375">Ping @tlrx for yet more bats tests. These ones won't merge well with #13083 so we should wait until that is in before rebasing but you can review this for the general principle I guess. Or wait.
</comment><comment author="nik9000" created="2015-09-02T13:28:30Z" id="137078972">Rebased and did a little more cleanup.
</comment><comment author="tlrx" created="2015-09-11T08:21:39Z" id="139481778">@nik9000 I just had a look to the changes and it looks good to me.

Please let me know once conflicts are resolved so that I'll be able to run the tests.
</comment><comment author="nik9000" created="2015-09-14T16:43:08Z" id="140140450">@tlrx ready for review again. First commit is a rebase and the second is updates, some from comments, some from running it over and over again.
</comment><comment author="tlrx" created="2015-09-16T07:48:31Z" id="140658574">@nik9000 do the tests succeed for you? I encountered various issues, mainly for `[XX PLUGINS] start elasticsearch with all plugins installed` tests
</comment><comment author="tlrx" created="2015-09-16T08:02:38Z" id="140661005">OK, tests now succeed. LGTM then.
</comment><comment author="nik9000" created="2015-09-16T14:31:48Z" id="140759701">&gt; @nik9000 do the tests succeed for you? I encountered various issues, mainly for [XX PLUGINS] start elasticsearch with all plugins installed tests

That is the `--tries 60` thing I keep adding in. I broke it when I merged something else but it wasn't _consistently_ broken. Master has it so rebasing should get it. I'll do that now.
</comment><comment author="nik9000" created="2015-09-16T14:45:37Z" id="140763471">Squashed and rebased. Will merge to 2.0, 2.x, and master. Thanks for the review @tlrx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increasing filter cache size invalidates the cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13261</link><project id="" key="" /><description>We're currently tuning our caches and I've noticed that when we increase it it causes the entire cache to get emptied out.

I would think this would only be necessary if the cache size was reduced, not increased.

This impacts performance of production environments while data gets re-cached again.
</description><key id="104388870">13261</key><summary>Increasing filter cache size invalidates the cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rayward</reporter><labels /><created>2015-09-02T02:04:49Z</created><updated>2015-09-02T07:33:43Z</updated><resolved>2015-09-02T07:33:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-02T07:33:43Z" id="136961954">@rayward This is true indeed, and necessary since the cache needs to make decisions at creation time based on the total amount of space that is available.

For this reason and a couple others, we removed the ability to configure the cache size in 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve unit test coverage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13260</link><project id="" key="" /><description>See the following coverage reports:
- Unit tests: http://people.apache.org/~rmuir/es-coverage/unit-tests/
- Integ tests: http://people.apache.org/~rmuir/es-coverage/integ-tests/
- Combined: http://people.apache.org/~rmuir/es-coverage/combined/

Unit tests are in sad shape (48%). We should improve this. Integ tests should be really simple and we should do all the heavy lifting with unit tests so things are easier to debug.
</description><key id="104383712">13260</key><summary>Improve unit test coverage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-09-02T01:01:40Z</created><updated>2016-09-14T15:21:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-02T01:05:11Z" id="136906993">See https://github.com/elastic/elasticsearch/commit/517f00b048cb4446665b3090fa12c7ec670bf6ab for how i generated these. 

Be careful to `clean` in between for now, or you will just keep appending sessions. You will see 4 sessions in the unit and integ reports respectively, thats because i run tests with 4 jvms. You see 8 in combined because it just appended the integ data after running the unit tests. 

In the future we can fix the build to make it a little better, by using separate coverage output files, and 'merge' so you could build all 3 reports with one command. But for now this is how to do it.
</comment><comment author="javanna" created="2016-09-14T14:55:43Z" id="247040407">I think we all agree that we should improve test coverage and write many more unit tests. We've been doing that for a while and things have improved already quite a bit, yet there's always a lot to be done on this matter. Does it still make sense to keep this issue open? Seems like this is a common goal to keep in mind with each single PR that we open, nothing concrete that can be done at the moment to close this issue.
</comment><comment author="s1monw" created="2016-09-14T14:57:41Z" id="247041095">any chance to get accurate numbers?
</comment><comment author="javanna" created="2016-09-14T15:21:13Z" id="247049412">you bring up a very valid point, I just noticed that in our TESTING.asciidoc we still refer to maven in the coverage analysis section, I guess that won't work :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`beforeIndexShardCreated` listener for indexModule called twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13259</link><project id="" key="" /><description>If I have a plugin that registers a listener for `beforeIndexShardCreated` via the `indexModules` endpoint, as so:

``` java
public class ListenerPlugin extends Plugin {
    /** ... other stuff here */
    @Override
    public Collection&lt;Module&gt; indexModules(Settings settings) {
        List&lt;Module&gt; modules = new ArrayList&lt;&gt;(super.indexModules(settings));
        modules.add(new ListenerModule());
        return modules;
    }
}
```

And then does:

``` java
public class Listener extends IndicesLifecycle.Listener {

    @Inject
    public Listener(Settings settings, IndicesLifecycle indicesLifecycle, Environment env) {
        indicesLifecycle.addListener(this);
        System.out.println("Listener created!");
    }

    @Override
    public void beforeIndexShardCreated(ShardId shardId, Settings indexSettings) {
        System.out.println("Entering beforeIndexShardCreated()");

        System.out.println("Exiting beforeIndexShardCreated()");
    }
}
```

I expect that the listener's `beforeIndexShardCreated` method will only be called once for each newly created shard, however, in some cases, it is called twice.

Here's how I reproduced this:
- Install plugin on all ES nodes
- Start 3 ES nodes
- Create an index with 1 primary shard and 0 replicas

``` json
POST /myindex
{
  "index": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "data_path": "/tmp/bar/foo",
    "shadow_replicas": true
  }
}
```
- Verify that the shard is created and I see:

Entering beforeIndexShardCreated()
Exiting beforeIndexShardCreated()

**Once**, in the logs (because 1 shard was created)
- Relocate the shard from a node to another node via the `_cluster/reroute` API

Then I see this in the logs:

```
[2015-09-01 15:52:16,468][DEBUG][org.elasticsearch.indices.cluster] [Thermo] [myindex][0] creating shard
Entering beforeIndexShardCreated()
Exiting beforeIndexShardCreated()
Entering beforeIndexShardCreated()
Exiting beforeIndexShardCreated()
```

The listener is being called twice. Looking through the logs, I see the log line `Listener created!` **twice** on this particular node, once when the index was created but no shards were placed on this node, and another time when the actual shard is moved to the node.

It looks like we create class more than once per index, which seems like something we should try to avoid?
</description><key id="104365837">13259</key><summary>`beforeIndexShardCreated` listener for indexModule called twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2015-09-01T22:17:13Z</created><updated>2015-10-23T18:01:33Z</updated><resolved>2015-10-21T20:12:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-06T16:16:58Z" id="145916761">@dakrone I tried to find out how this happens the only thing I could suspect is Guice causing two instances of Listener to be created. I would be very curious to see the code of ListenerModule.
</comment><comment author="s1monw" created="2015-10-08T08:50:23Z" id="146461579">@dakrone can you reproduce it and print the stacktrace for each of the invocations?
</comment><comment author="s1monw" created="2015-10-08T09:27:33Z" id="146470845">ok I can reproduce this problem. It's caused by the fact that the master creates the index once before we actually creating it on other nodes to ensure we can fully create it and don't throw an exception. If you print stacktraces it looks like this:

```
[2015-10-08 11:23:50,331][INFO ][org.elasticsearch.index  ] [ListenerIT#testIt]: starting test
[2015-10-08 11:23:50,604][WARN ][LISTENER                 ] Listener created!
java.lang.RuntimeException
    at org.elasticsearch.index.ListenerIT$MyTestListener.&lt;init&gt;(ListenerIT.java:67)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
    at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.InheritingState.makeAllBindingsToEagerSingletons(InheritingState.java:157)
    at org.elasticsearch.common.inject.InjectorImpl.readOnlyAllSingletons(InjectorImpl.java:909)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:59)
    at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:347)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:364)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:383)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-10-08 11:23:50,688][INFO ][org.elasticsearch.cluster.metadata] [node_s0] [test] creating index, cause [api], templates [random_index_template], shards [1]/[0], mappings [_default_]
[2015-10-08 11:23:50,710][WARN ][LISTENER                 ] Listener created!
java.lang.RuntimeException
    at org.elasticsearch.index.ListenerIT$MyTestListener.&lt;init&gt;(ListenerIT.java:67)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
    at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.InheritingState.makeAllBindingsToEagerSingletons(InheritingState.java:157)
    at org.elasticsearch.common.inject.InjectorImpl.readOnlyAllSingletons(InjectorImpl.java:909)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:59)
    at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:347)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:303)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:172)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:493)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

a short term workaround for this is to de-register the listener which it should do anyway like this:

``` Java
        @Override
        public void afterIndexClosed(Index index, @IndexSettings Settings indexSettings) {
            lifecycle.removeListener(this);
        }
```

But we should really turn this into an index level component that we destroy once the index is destroyed. ie instead of `IndicesLifecycle` we would have an `IndexLifecycle` that we can trash after the index is destroyed
</comment><comment author="s1monw" created="2015-10-08T09:35:39Z" id="146472735">@dakrone  I am going to push this out to 2.1 for now
</comment><comment author="bleskes" created="2015-10-08T09:36:23Z" id="146473095">That&#8217;s indeed why the listener is created twice, but  does it explain why we call beforeIndexShardCreated twice- the master doesn&#8217;t create shards... That&#8217;s where I got stuck looking at this&#8230;

&gt; On 08 Oct 2015, at 11:27, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; ok I can reproduce this problem. It's caused by the fact that the master creates the index once before we actually creating it on other nodes to ensure we can fully create it and don't throw an exception. If you print stacktraces it looks like this:
&gt; 
&gt; [2015-10-08 11:23:50,331][INFO ][org.elasticsearch.index  ] [ListenerIT#testIt]: starting test
&gt; [2015-10-08 11:23:50,604][WARN ][LISTENER                 ] Listener created!
&gt; java.lang.RuntimeException
&gt;     at org.elasticsearch.index.ListenerIT$MyTestListener.&lt;init&gt;(ListenerIT.java:67)
&gt;     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
&gt;     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
&gt;     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
&gt;     at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
&gt;     at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
&gt;     at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
&gt;     at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
&gt;     at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
&gt;     at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
&gt;     at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
&gt;     at org.elasticsearch.common.inject.InheritingState.makeAllBindingsToEagerSingletons(InheritingState.java:157)
&gt;     at org.elasticsearch.common.inject.InjectorImpl.readOnlyAllSingletons(InjectorImpl.java:909)
&gt;     at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:59)
&gt;     at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:347)
&gt;     at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:364)
&gt;     at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:383)
&gt;     at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
&gt;     at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;     at java.lang.Thread.run(Thread.java:745)
&gt; [2015-10-08 11:23:50,688][INFO ][org.elasticsearch.cluster.metadata] [node_s0] [test] creating index, cause [api], templates [random_index_template], shards [1]/[0], mappings [_default_]
&gt; [2015-10-08 11:23:50,710][WARN ][LISTENER                 ] Listener created!
&gt; java.lang.RuntimeException
&gt;     at org.elasticsearch.index.ListenerIT$MyTestListener.&lt;init&gt;(ListenerIT.java:67)
&gt;     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
&gt;     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
&gt;     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
&gt;     at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
&gt;     at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
&gt;     at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
&gt;     at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
&gt;     at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
&gt;     at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
&gt;     at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
&gt;     at org.elasticsearch.common.inject.InheritingState.makeAllBindingsToEagerSingletons(InheritingState.java:157)
&gt;     at org.elasticsearch.common.inject.InjectorImpl.readOnlyAllSingletons(InjectorImpl.java:909)
&gt;     at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:59)
&gt;     at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:347)
&gt;     at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:303)
&gt;     at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:172)
&gt;     at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:493)
&gt;     at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
&gt;     at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;     at java.lang.Thread.run(Thread.java:745)
&gt; 
&gt; a short term workaround for this is to de-register the listener which it should do anyway like this:
&gt; 
&gt; ```
&gt;     @Override
&gt; ```
&gt; 
&gt; public void afterIndexClosed(Index index, @IndexSettings Settings
&gt;  indexSettings) {
&gt;             lifecycle
&gt; .removeListener(this
&gt; );
&gt;         }
&gt; 
&gt; But we should really turn this into an index level component that we destroy once the index is destroyed. ie instead of IndicesLifecycle we would have an IndexLifecycle that we can trash after the index is destroyed
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2015-10-08T09:36:44Z" id="146473230">I also think it's not necessarily a bug, if a listener does this it will leak once an index is moved to another node entirely so it has to cleanup after himself.
</comment><comment author="s1monw" created="2015-10-08T09:37:51Z" id="146473557">&gt; That&#8217;s indeed why the listener is created twice, but  does it explain why we call beforeIndexShardCreated twice- the master doesn&#8217;t create shards... That&#8217;s where I got stuck looking at this&#8230;

it does never unregister itself. so if you allocate a shard on the node that happens to be the master as well you will AGAIN register the listener (a new instance) once it's actually creating the shard.
</comment><comment author="s1monw" created="2015-10-08T09:41:47Z" id="146474812">here is an output of this test https://gist.github.com/s1monw/1a552a6abb7f63c51e6f

```
...
[2015-10-08 11:39:40,526][WARN ][LISTENER                 ] Listener created!
java.lang.RuntimeException
    at org.elasticsearch.index.ListenerIT$MyTestListener.&lt;init&gt;(ListenerIT.java:67)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
    at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.InheritingState.makeAllBindingsToEagerSingletons(InheritingState.java:157)
    at org.elasticsearch.common.inject.InjectorImpl.readOnlyAllSingletons(InjectorImpl.java:909)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:59)
    at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:347)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:364)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:383)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-10-08 11:39:40,714][INFO ][org.elasticsearch.cluster.metadata] [node_s0] [test] creating index, cause [api], templates [random_index_template], shards [1]/[0], mappings [_default_]
[2015-10-08 11:39:40,741][WARN ][LISTENER                 ] Listener created!
java.lang.RuntimeException
    at org.elasticsearch.index.ListenerIT$MyTestListener.&lt;init&gt;(ListenerIT.java:67)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
    at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.InheritingState.makeAllBindingsToEagerSingletons(InheritingState.java:157)
    at org.elasticsearch.common.inject.InjectorImpl.readOnlyAllSingletons(InjectorImpl.java:909)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:59)
    at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:347)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:303)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:172)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:493)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-10-08 11:39:40,748][WARN ][LISTENER                 ] RUNNING beforeIndexShardCreated() on instance org.elasticsearch.index.ListenerIT$MyTestListener@25e3d50a 
[2015-10-08 11:39:40,748][WARN ][LISTENER                 ] RUNNING beforeIndexShardCreated() on instance org.elasticsearch.index.ListenerIT$MyTestListener@7555bbb0 
[2015-10-08 11:39:40,879][WARN ][LISTENER                 ] RUNNING afterIndexShardCreated() on instance org.elasticsearch.index.ListenerIT$MyTestListener@25e3d50a
[2015-10-08 11:39:40,880][WARN ][LISTENER                 ] RUNNING afterIndexShardCreated() on instance org.elasticsearch.index.ListenerIT$MyTestListener@7555bbb0
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Start plugins in package tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13258</link><project id="" key="" /><description>This changes the packaging tests to start Elasticsearch with all plugins
installed and checks `_cat/plugins?h=c` against the list of plugins in
the plugins directory. If the list differs, error! So it proves that the
plugins can be installed using bin/plugin as shipped in the rpm and deb
packages.

Closes #13254
</description><key id="104353799">13258</key><summary>Start plugins in package tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T21:00:05Z</created><updated>2016-03-10T18:13:57Z</updated><resolved>2015-09-10T18:35:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-09T14:41:59Z" id="138932548">LGTM
</comment><comment author="nik9000" created="2015-09-10T18:35:31Z" id="139337832">Rebased on master. I had to make some changes to support the AWS plugin's split but otherwise this code is no different than the previous code. So I'll merge.
</comment><comment author="nik9000" created="2015-09-10T18:59:24Z" id="139345242">Merged to 2.0, 2.x, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed non-valid JSON (though ES would accept it)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13257</link><project id="" key="" /><description /><key id="104346807">13257</key><summary>Fixed non-valid JSON (though ES would accept it)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T20:20:48Z</created><updated>2015-09-14T17:14:29Z</updated><resolved>2015-09-02T13:51:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-02T13:51:14Z" id="137088573">Merged, thanks @eskibars!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Also use PriorityComparator in shard balancer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13256</link><project id="" key="" /><description>Today we try to allocate primaries first and then replicas
but don't take the index creation date and priority into account
as we do in the GatewayAlloactor.

Closes #13249
</description><key id="104339553">13256</key><summary>Also use PriorityComparator in shard balancer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-01T19:43:52Z</created><updated>2015-09-08T13:37:05Z</updated><resolved>2015-09-08T13:34:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-02T07:49:40Z" id="136966851">Left a minor comment, otherwise LGTM
</comment><comment author="s1monw" created="2015-09-02T10:26:52Z" id="137016706">@jpountz pushed a new commit
</comment><comment author="s1monw" created="2015-09-02T10:31:38Z" id="137019414">@pickypg can you give this a go?
</comment><comment author="jpountz" created="2015-09-02T10:46:27Z" id="137025818">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More bats backports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13255</link><project id="" key="" /><description>Backports more of the changes made for testing packages to 2.0. Required to backport #13076.
</description><key id="104330428">13255</key><summary>More bats backports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T18:47:34Z</created><updated>2015-09-14T17:14:29Z</updated><resolved>2015-09-02T12:27:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-01T21:02:04Z" id="136860545">Ping @tlrx for more bats. More! This is just backports.
</comment><comment author="jpountz" created="2015-09-02T07:50:49Z" id="136967038">I haven't looked at the change yet (no bats knowledge here), but I don't think we need pull requests for backports. Unless maybe there were things you had to change to accomodate to 2.0?
</comment><comment author="nik9000" created="2015-09-02T11:19:11Z" id="137035747">&gt; I haven't looked at the change yet (no bats knowledge here), but I don't think we need pull requests for backports. Unless maybe there were things you had to change to accomodate to 2.0?

I figured it was good to open a pull request because it was complex-ish to backport. The issue here was that I had backported these much after they were initially committed to master and so they needed fixing up.

I can just merged it and keep going but I feel more comfortable having a pull request for it because its wasn't a simple cherry-pick.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packing tests should start elasticsearch with all plugins installed and give them a perfunctory check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13254</link><project id="" key="" /><description>After we merge #13076 we'll install all the plugins and make sure the files are in the right place which is a step in the right direction but not quite enough to verify that they work. We should also verify that they are loaded by starting elasticsearch and checking if it picked them up.
</description><key id="104318859">13254</key><summary>Packing tests should start elasticsearch with all plugins installed and give them a perfunctory check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T17:41:37Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-09-10T18:35:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Refactors TemplateQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13253</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="104314796">13253</key><summary>Refactors TemplateQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-09-01T17:18:45Z</created><updated>2015-09-14T21:30:55Z</updated><resolved>2015-09-09T12:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-09-01T19:23:57Z" id="136833400">Left some minor comments. Looks pretty good to me already.
</comment><comment author="alexksikes" created="2015-09-02T11:38:00Z" id="137040921">@MaineC thanks for the review. I have addressed the comments.
</comment><comment author="javanna" created="2015-09-02T12:51:04Z" id="137063420">left a few comments, looks good though!
</comment><comment author="alexksikes" created="2015-09-08T09:48:37Z" id="138498266">@javanna @MaineC Thanks for the review. I addressed the comments.
</comment><comment author="javanna" created="2015-09-09T08:08:07Z" id="138818997">I did a final round of review, change looks good, there is a test problem though. LGTM once tests are fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] remove shaded elasticsearch version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13252</link><project id="" key="" /><description>The shaded version of elasticsearch was built at the very beginning to avoid dependency conflicts in a specific case where:
- People use elasticsearch from Java
- People needs to embed elasticsearch jar within their own application (as it's today the only way to get a `TransportClient`)
- People also embed in their application another (most of the time older) version of dependency we are using for elasticsearch, such as: Guava, Joda, Jackson...

This conflict issue can be solved within the projects themselves by either upgrade the dependency version and use the one provided by elasticsearch or by shading elasticsearch project and relocating some conflicting packages.
## Example

As an example, let's say you want to use within your project `Joda 2.1` but elasticsearch `2.0.0-beta1` provides `Joda 2.8`.
Let's say you also want to run all that with shield plugin.
### Create the shaded version

Create a new maven project or module with:

``` xml
&lt;groupId&gt;fr.pilato.elasticsearch.test&lt;/groupId&gt;
&lt;artifactId&gt;es-shaded&lt;/artifactId&gt;
&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

&lt;properties&gt;
    &lt;elasticsearch.version&gt;2.0.0-beta1&lt;/elasticsearch.version&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
        &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
        &lt;artifactId&gt;shield&lt;/artifactId&gt;
        &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;elasticsearch-releases&lt;/id&gt;
        &lt;url&gt;http://maven.elasticsearch.org/releases&lt;/url&gt;
        &lt;releases&gt;
            &lt;enabled&gt;true&lt;/enabled&gt;
            &lt;updatePolicy&gt;daily&lt;/updatePolicy&gt;
        &lt;/releases&gt;
        &lt;snapshots&gt;
            &lt;enabled&gt;false&lt;/enabled&gt;
        &lt;/snapshots&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
```

And now shade and relocate all packages which conflicts with your own application:

``` xml
&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.4.1&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;phase&gt;package&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;shade&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;configuration&gt;
                        &lt;relocations&gt;
                            &lt;relocation&gt;
                                &lt;pattern&gt;org.joda&lt;/pattern&gt;
                                &lt;shadedPattern&gt;fr.pilato.thirdparty.joda&lt;/shadedPattern&gt;
                            &lt;/relocation&gt;
                        &lt;/relocations&gt;
                        &lt;transformers&gt;
                            &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer" /&gt;
                        &lt;/transformers&gt;
                    &lt;/configuration&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
```

You can create now a shaded version of elasticsearch + shield by running `mvn clean install`.
### Embed this jar within your application

In your project, you can now depend on:

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;fr.pilato.elasticsearch.test&lt;/groupId&gt;
    &lt;artifactId&gt;es-shaded&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;joda-time&lt;/groupId&gt;
    &lt;artifactId&gt;joda-time&lt;/artifactId&gt;
    &lt;version&gt;2.1&lt;/version&gt;
&lt;/dependency&gt;
```

Build then your TransportClient as usual:

``` java
TransportClient client = TransportClient.builder()
        .settings(Settings.builder()
                        .put("path.home", ".")
                        .put("shield.user", "username:password")
                        .put("plugin.types", "org.elasticsearch.shield.ShieldPlugin")
        )
        .build();
client.addTransportAddress(new InetSocketTransportAddress(new InetSocketAddress("localhost", 9300)));

// Index some data
client.prepareIndex("test", "doc", "1").setSource("foo", "bar").setRefresh(true).get();
SearchResponse searchResponse = client.prepareSearch("test").get();
```

If you want to use your own version of Joda, then import for example `org.joda.time.DateTime`. If you want to access to the shaded version (not recommended though), import `fr.pilato.thirdparty.joda.time.DateTime`.

You can run a simple test to make sure that both classes can live together within the same JVM:

``` java
CodeSource codeSource = new org.joda.time.DateTime().getClass().getProtectionDomain().getCodeSource();
System.out.println("unshaded = " + codeSource);

codeSource = new fr.pilato.thirdparty.joda.time.DateTime().getClass().getProtectionDomain().getCodeSource();
System.out.println("shaded = " + codeSource);
```

It will print:

```
unshaded = (file:/path/to/joda-time-2.1.jar &lt;no signer certificates&gt;)
shaded = (file:/path/to/es-shaded-1.0-SNAPSHOT.jar &lt;no signer certificates&gt;)
```

By the way, the project can now build with Maven 3.3.3 so we can relax a bit our maven policy.
</description><key id="104311005">13252</key><summary>[build] remove shaded elasticsearch version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>blocker</label><label>build</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T16:58:00Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-09-02T10:47:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-01T20:51:13Z" id="136857244">I think there are a few stragglers, just dead build logic, comments, and docs:

```
rmuir@beast:~/workspace/elasticsearch$ fgrep -r shade .
./pom.xml:                    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;

./dev-tools/src/main/resources/ant/integration-tests.xml:  &lt;!-- check shaded jar for jar hell --&gt;
./dev-tools/src/main/resources/ant/integration-tests.xml:  &lt;target name="check-shaded-jar-packages"&gt;
...
./dev-tools/src/main/resources/forbidden/core-signatures.txt:# For shaded dependencies, please put signatures in third-party-shaded.txt 
./dev-tools/src/main/resources/forbidden/core-signatures.txt:# and third-party-unshaded.txt instead of here.
...
./docs/java-api/docs/index_.asciidoc:Elasticsearch already uses Jackson but shades it under
```

otherwise looks good.

as a followup we should nuke that fully-loaded module.
</comment><comment author="dadoonet" created="2015-09-02T09:54:17Z" id="137007707">I added a new commit to address your comments.

I started working on a smoke-test-client module but in another PR as I really want to merge this one first as soon as possible.
</comment><comment author="s1monw" created="2015-09-02T09:55:43Z" id="137008366">LGTM
</comment><comment author="udoprog" created="2015-09-03T02:25:13Z" id="137303024">It might not be obvious to an end-user of elasticsearch as-a-library what dependencies typically have to be relocated since it requires the inspection of all project dependencies.

Depending on elasticsearch in applications that themselves have a large set of dependencies is now a frightening proposition since it now has a high probability of leading to incompatibility errors which are very hard to spot.
</comment><comment author="rmuir" created="2015-09-03T03:15:09Z" id="137312458">Try this to look for incompatibilities: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java

But really, incompatibilities across dependencies is a general java problem. If your app depends on "a large set of dependencies" then this is something you must manage. Its not a problem created by elasticsearch.
</comment><comment author="udoprog" created="2015-09-03T03:50:41Z" id="137323042">Having libraries relocate their direct dependencies is a _good_ solution. It's not my preference, but it avoids requiring knowledge about what elasticsearch as-a-library is, and is not compatible with.

I never claimed elasticsearch caused the problem, but I don't believe this compromise is the best for users.
</comment><comment author="rmuir" created="2015-09-03T03:58:25Z" id="137323653">No its not, its a terrible solution. I am so glad to see this pull request, because otherwise it means we'd have to support and test 2x of everything, shaded and unshaded: plugins etc etc. That is just too much complexity.
</comment><comment author="udoprog" created="2015-09-03T04:07:40Z" id="137324554">It is the solution being recommended in this very pull request, so you are pushing the complexity to all your users, even the ones who don't want it.

A shaded module should be a leaf-module, already depending on well-tested modules, why would you test your code twice?
</comment><comment author="udoprog" created="2015-09-03T04:23:02Z" id="137326969">So to be a bit more constructive, I'd consider this a non-issue if the TransportClient was available with a very small set of dependencies which could be easily relocated.

I am not advocating that you use the shaded jar internally for your own QA procedures, you obviously won't have these problems and I understand why this is beneficial for you.
</comment><comment author="dadoonet" created="2015-09-03T05:02:48Z" id="137332845">The problem with shading/unshading is also for plugins. You have basically to generate two versions for each plugin.

And yes, having a client jar is definitely where we are going. It requires some work so don't expect It sioniste. I think that users who have this dependency issue will have then to shade by themselves. We will document this soonish.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Relative data path on Windows can report incorrect disk size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13251</link><project id="" key="" /><description>To repro:

On Windows, set path.data to a relative path.  'data' for example.
Use Powershell to run a remote execution to remotely start Elasticsearch.

With a relative path set, the system reports a disk available size of '-1'.  If the absolute path is set in the configuration, disk available size is correct.  When started locally, the disk available size is also correct.

This may only need a document fix to specify absolute paths for path.data.
</description><key id="104307752">13251</key><summary>Relative data path on Windows can report incorrect disk size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">seang-es</reporter><labels><label>:Stats</label><label>feedback_needed</label><label>v1.7.1</label></labels><created>2015-09-01T16:42:36Z</created><updated>2016-07-01T08:22:08Z</updated><resolved>2016-07-01T08:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-01T17:09:18Z" id="136799048">@tlrx could you look at this please? See if it is the same after your changes in master?
</comment><comment author="tlrx" created="2015-10-15T14:16:54Z" id="148399094">&gt; Use Powershell to run a remote execution to remotely start Elasticsearch.

@seang-es Are you starting elasticsearch as a service with PowerShell using the  &#768;Start-Service` cmdlet?
</comment><comment author="clintongormley" created="2016-05-24T10:30:45Z" id="221229653">@seang-es ping
</comment><comment author="tlrx" created="2016-07-01T08:22:07Z" id="229886459">No feedback and I'm unable to reproduce the issue. I'm closing it, please feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Distributed Search issue </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13250</link><project id="" key="" /><description>When using a Scripted Metric Aggregation, we're facing some weird result inconsistencies:

The `init_script`:

``` groovy
_agg['success'] = [];
_agg['error'] = [];
_agg['tolerated'] = [];
_agg['ignored'] = [];
```

The `map_script`:

``` groovy
tags = doc['tags'].values

if(tags.contains('_gw-successful') || tags.contains('_gw-informational')) {
    return _agg.success.add(1)
}
if(tags.contains('_gw-client-error') || tags.contains('_gw-server-error')) {
    return _agg.error.add(1)
}
if(tags.contains('_gw-unauthorized-origin')) {
    return _agg.ignored.add(1)
}

return _agg.tolerated.add(1)
```

The `combine_script`:

``` groovy
 _combine = {}

 _combine['error'] = 0
for (t in _agg.error) {
    _combine['error'] += t
}

_combine['success'] = 0
for (t in _agg.success) {
    _combine['success'] += t
}

_combine['tolerated'] = 0
for (t in _agg.tolerated) {
    _combine['tolerated'] += t
}

return _combine
```

The `reduce_script`:

``` groovy
error = 0
for (a in _aggs['error']) {
 error += a
}
success = 0
for (a in _aggs['success']) {
    success += a
}
tolerated = 0
for (a in _aggs['tolerated']) {
    tolerated += a
}
total = success + error + tolerated
return total &gt; 0 ? (success + tolerated/2)/total : 1
```

The `query`:

``` json
{
    "query": {
        "bool": {
            "must": [
                {
                    "range": {
                        "@timestamp": {
                            "gte": "now-5h"
                        }
                    }
                },
                {
                    "match": {
                        "_entrypoint.id": 6
                    }
                }
            ]
        }
    },
    "aggs": {
        "resourcesScore": {
            "filters": {
                "filters": {
                    "81": {
                        "term": {
                            "_resource.id": 81
                        }
                    },
                    "83": {
                        "term": {
                            "_resource.id": 83
                        }
                    }
                }
            },
            "aggs": {
                "apdexScore": {
                    "scripted_metric": {
                        "init_script_file": "init_script",
                        "map_script_file": "map_script",
                        "combine_script_file": "combine_script",
                        "reduce_script_file": "reduce_script"
                    }
                },
                "responseTime": {
                    "percentiles": {
                        "field": "entries.entrypoint.time",
                        "percents": [
                            10,
                            80
                        ]
                    }
                }
            }
        }
    }
}
```

This `query` sometimes give us aleatory failures like this:

``` json
{
    "took": 9,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 3,
        "failed": 2,
        "failures": [
            {
                "index": "events-gateway-29-20150901",
                "shard": 0,
                "status": 500,
                "reason": "RemoteTransportException[[Shellshock][inet[/192.168.12.37:9300]][indices:data/read/search[phase/query]]]; nested: IOException[Can't write type [class b033d26c7c699de2870d36961a0ba8fc886a1963$_run_closure1]]; "
            },
            {
                "index": "events-gateway-29-20150901",
                "shard": 2,
                "status": 500,
                "reason": "RemoteTransportException[[Asp][inet[/192.168.12.245:9300]][indices:data/read/search[phase/query]]]; nested: IOException[Can't write type [class b033d26c7c699de2870d36961a0ba8fc886a1963$_run_closure1]]; "
            }
        ]
    },
    "hits": {
        "total": 14,
        "max_score": 0,
        "hits": []
    },
    "aggregations": {
        "resourcesScore": {
            "buckets": {
                "81": {
                    "doc_count": 14,
                    "responseTime": {
                        "values": {
                            "10.0": 17.2,
                            "80.0": 43.6
                        }
                    },
                    "apdexScore": {
                        "value": "0.0357142857"
                    }
                },
                "83": {
                    "doc_count": 0,
                    "responseTime": {
                        "values": {
                            "10.0": "NaN",
                            "80.0": "NaN"
                        }
                    },
                    "apdexScore": {
                        "value": 1
                    }
                }
            }
        }
    }
}
```

Is there something we're missing or is that a issue with Elasticsearch?
</description><key id="104305517">13250</key><summary>Distributed Search issue </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">diegossilveira</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-09-01T16:32:16Z</created><updated>2015-09-02T12:56:16Z</updated><resolved>2015-09-02T11:03:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="felipegs" created="2015-09-01T17:07:31Z" id="136798527">we are using elasticsearch 1.7.1
</comment><comment author="clintongormley" created="2015-09-01T17:13:02Z" id="136800010">@colings86 could you take a look at this please?
</comment><comment author="colings86" created="2015-09-02T10:13:27Z" id="137013728">@felipegs I'm going to try and reproduce this to see what might be going on buy in the mean time would you be able to do the following:
- Confirm that the same errors happen when the `responseTime` agg is removed? (I expect it to be the case but asking for completeness
- Confirm whether the errors occur when the `filters` aggregation is removed and the `scripted_metric` aggregation is run on its own?
- Confirm which of the scripts the error is occurring in by removing the reduce script and seeing if the errors occur and then removing the combine script as well and seeing if the errors occur?
- Confirm if the failing shards are always shards 0 and 2 of `events-gateway-29-20150901` or if the shards change each time?
- Confirm if the errors only occur on that index (`events-gateway-29-20150901`) or if they occur on other indices too?

Thanks
</comment><comment author="colings86" created="2015-09-02T10:48:20Z" id="137026665">I have managed to reproduce the issue using the steps in this gist: https://gist.github.com/colings86/0ee1159ee113e4992457

This is done on a 2-node cluster running version 1.7.1.

It seems that for me the error consistently occurs and it always on the same 2 shards.

I have a look into what is causing this now
</comment><comment author="colings86" created="2015-09-02T11:03:17Z" id="137030948">So the problem seems to be with the way you are initialising `_combine` in the `combine_script`. To initialise an empty map you should use `_combine = [:]` and then populate that map as you are currently (see http://www.groovy-lang.org/groovy-dev-kit.html#Collections-Maps for more details). By using `{}` i think (I'm not an expert in groovy) you are creating a new object type in groovy which Elasticsearch will not know how to serialise. The reason you only get the error for 2 of the shards is probably because they are the non-local shards being used in the request. The local shards (the ones on the same node which received the request from the client) will not need to serialise those objects to send them back for the reduce phase.

I'm going to close this issue as the aggregation is working as expected in that the object emitted from the scripts need to be either primitive types, strings, maps of these types or an array of the types here.
</comment><comment author="colings86" created="2015-09-02T11:14:06Z" id="137034693">I've updated the documentation for the `scripted_metric` aggregation to be clearer on what types are valid to return from the scripts: https://github.com/elastic/elasticsearch/commit/1d9905a798e350f4a732790cb8731df8ab016059
</comment><comment author="diegossilveira" created="2015-09-02T12:56:16Z" id="137064514">Thank you for the reply @colings86! We will make some tests and I will let you know if we find some other problems.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prioritized Replica Recovery is reversed by date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13249</link><project id="" key="" /><description>Prioritized allocation enables the recovery in the order of `index.priority` &gt; `index.creation_date` &gt; `index.name` (reversed). However, I've found that when allowing it to work based on `index.creation_date` (the default mechanism), it does it in the reverse of the expected order **relative to replicas**.

It's easy enough to reproduce with enough daily indices by manually deleting the replicas from one node, throttling the heck out of recovery, and speeding up monitoring:

``` http
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.node_concurrent_recoveries" : 1,
    "indices.recovery.concurrent_streams" : 1,
    "indices.recovery.concurrent_small_file_streams" : 1,
    "indices.recovery.max_bytes_per_sec" : "1mb",
    "marvel.agent.interval" : "500ms"
  }
}
```

As I was watching it, I decided to take some screenshots:
1. ![Reversed Priority 1 of 2](https://cloud.githubusercontent.com/assets/1501235/9609848/04e554c2-50a4-11e5-9984-0175a05bc146.png)
2. ![Reversed Priority 2 of 2](https://cloud.githubusercontent.com/assets/1501235/9609849/04e56f20-50a4-11e5-861b-d7f108bca25b.png)

This also appears to not be honoring the `index.priority` either, as I tried to use it as a workaround and it did not impact the recovery order at all, which makes me assume that this is not even coming into play during replica recovery.
</description><key id="104304363">13249</key><summary>Prioritized Replica Recovery is reversed by date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Recovery</label><label>bug</label></labels><created>2015-09-01T16:26:03Z</created><updated>2015-09-08T13:34:19Z</updated><resolved>2015-09-08T13:34:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-01T17:07:45Z" id="136798624">@s1monw could you take a look at this please?
</comment><comment author="s1monw" created="2015-09-01T18:09:22Z" id="136814985">I don't understand what you are testing here. I can't see the priorities you are giving, I don't see if the replicas where allocated before and if not there will be no ordering as far as I can tell. I don't see if primaries got allocated first and I wonder what you expected to see sorry it's unclear.
</comment><comment author="nik9000" created="2015-09-01T18:11:51Z" id="136815535">&gt; I don't understand what you are testing here. I can't see the priorities you are giving, I don't see if the replicas where allocated before and if not there will be no ordering as far as I can tell. I don't see if primaries got allocated first and I wonder what you expected to see sorry it's unclear.

I'm not familiar with the screenshot source but it looks like the indexes are recovering oldest to newest rather than newest to oldest. But I'm likely reading that wrong.
</comment><comment author="pickypg" created="2015-09-01T18:18:21Z" id="136816994">@s1monw 

&gt; I don't understand what you are testing here.

Replica recovery order with 2 nodes.
1. I throttled recovery as shown.
2. I took the second node offline.
3. I deleted all of its `.marvel-*` indices from the offline node.
4. I restarted the offline node and watched recovery.

&gt; I can't see the priorities you are giving.

I only set `index.priority` after seeing the images above. I picked arbitrary indices in the middle of the group and gave higher values for them individually (e.g., `.marvel-2015.08.22` I gave the priority of 200). All of the creation dates are going to be roughly around midnight of the date of the index (no weirdness or cheating on creation of the indices).

&gt; I don't see if primaries got allocated first

They did. Synced flushed replica shards (not shown) also got recovered before these replicas were recovered.

&gt; I wonder what you expected to see sorry it's unclear.

I expected to see what @nik9000 suggested: the newest to oldest recovery of the **replicas**. Basically, `.marvel-2015.08.28`'s replica should be recovered before `.marvel-2015.08.27`'s replica, which should be recovered before `.marvel-2015.08.26`'s replica (and so on).

It seems like the replica's do not consider priority in their recovery order and the oldest indices are being recovered.
</comment><comment author="s1monw" created="2015-09-01T18:22:20Z" id="136819187">&gt; I deleted all of its .marvel-\* indices from the offline node.

if you don't let the gateway allocator fetch any replicas to recover it won't respect priorities and will leave the rest to the shard balancer. The balancer will do it's own sorting at this point. This has never been implemented
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripts handle newline before a plus sign incorrectly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13248</link><project id="" key="" /><description>I would expected the following (unescaped) script to return `3`, but instead `2` is returned:

```
1
+
2
```

It appears everything before `\n+` is ignored. By removing newline before the plus sign, `3` is correctly returned:

```
1 +
2
```

This is undocumented and IMO incorrect behaviour. I assume this changed when we upgraded to _1.7.0_.

``` json
"number": "1.7.0",
"build_hash": "929b9739cae115e73c346cb5f9a6f24ba735a743",
"build_timestamp": "2015-07-16T14:31:07Z",
"build_snapshot": false,
"lucene_version": "4.10.4"
```
</description><key id="104273062">13248</key><summary>Scripts handle newline before a plus sign incorrectly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">steinar</reporter><labels><label>:Scripting</label><label>feedback_needed</label></labels><created>2015-09-01T13:59:20Z</created><updated>2015-09-05T10:45:41Z</updated><resolved>2015-09-05T10:45:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-01T15:52:39Z" id="136767672">@steinar could you tell us what script language you're using and provide a recreation please?
</comment><comment author="steinar" created="2015-09-01T18:03:31Z" id="136813740">This is using the default Groovy language.

Example query which exhibits the described behaviour:

``` json
{
  "query": {
    "function_score": {
      "functions": [
        {
          "script_score": {
            "script": "1+2\n+3"
          }
        }
      ],
      "boost_mode": "replace"
    }
  },
  "fields": "_score"
}
```

For the query above, all results have `"score": 3`.

If we remove the newline character `\n`, all documents have `"score": 6`.
</comment><comment author="rjernst" created="2015-09-01T19:10:15Z" id="136830612">I don't think json allows line breaks within strings?
</comment><comment author="rjernst" created="2015-09-01T19:11:14Z" id="136830817">At least not directly like that. I think it needs to be maybe `\\n`?
</comment><comment author="steinar" created="2015-09-02T00:32:20Z" id="136902512">To clarify, the following query correctly returns `"score": 6` since it does not contain `\n&lt;operator&gt;`:

```
{
  "query": {
    "function_score": {
      "functions": [
        {
          "script_score": {
            "script": "\n1+\n2+\n3\n"
          }
        }
      ],
      "boost_mode": "replace"
    }
  },
  "fields": "_score"
}
```
</comment><comment author="clintongormley" created="2015-09-05T10:45:41Z" id="137942710">Groovy doesn't require line terminators, so your script is being parsed as:

```
1+2;
return 3;
```

A trailing operator indicates that the line continues on the next line.  Alternatively, you can use `\` to indicate that the line continues:

```
1+2 \
+3;
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>windows as service does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13247</link><project id="" key="" /><description>Trying to follow https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service-win.html doesn't work with master.

I get this error "The data area passed to a system call is too small"
</description><key id="104272213">13247</key><summary>windows as service does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T13:55:52Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-09-09T18:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-01T13:56:28Z" id="136730390">I think this functionality is also untested.
</comment><comment author="clintongormley" created="2015-09-01T15:51:37Z" id="136767406">@gmarz would you be able to take a look at this?
</comment><comment author="gmarz" created="2015-09-01T17:36:52Z" id="136806576">I get the same thing as well:

```
Exception in thread "main" areSettings(InternalSettingsPreparer.java:87)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:108)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:101)
    at org.elasticsearch.bootstrap.BootstrapCLIParser.&lt;init&gt;(BootstrapCLIParser.java:48)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:226)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
The data area passed to a system call is too small.

Failed to start service
```

I'll start digging...
</comment><comment author="nik9000" created="2015-09-01T18:10:29Z" id="136815246">&gt; I think this functionality is also untested.

Very much. I was looking at getting vagrant tests for this too but it'd be very different than the existing bats tests. And I'm not sure what the licenses are for running windows VMs for testing. If someone can point me to something that makes it clear that window's license lets people spin up VMs for testing then I'll dig more into it and try to use the same technique we use for linux. Without bash, I guess.
</comment><comment author="jochen-st" created="2015-09-08T12:51:42Z" id="138550586">I found that changes in elasticsarch.in.bat related to ES_GC_OPTS environment variables caused JAVA_OPTS environment variable to contain "  " two consecutive spaces. These are converted by service.bat into ";;" two consecutive semicolons. This causes procrun to stop adding Java options to service and service start fails with:

```
Java HotSpot(TM) 64-Bit Server VM warning: Using the ParNew young collector with the Serial old 
collector is deprecated and will likely be removed in a future release
Exception in thread "main" areSettings(InternalSettingsPreparer.java:97)
at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:107)
at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:100)
at org.elasticsearch.bootstrap.BootstrapCLIParser.&lt;init&gt;(BootstrapCLIParser.java:48)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:221)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

However even after manipulating ES_GC_OPTS / JAVA_OPTS to not contain two consecutive spaces (and procrun taking all Java options), service start still fails with

```
ERROR: command not specified
```

I found that this can be fixed by adding "--StartParams start" to service.bat line 162 (starts with "%EXECUTABLE%" //IS//%SERVICE_ID%)

Now stopping fails with

```
Method 'static void close(String[])' not found in Class org/elasticsearch/bootstrap/Elasticsearch
```
</comment><comment author="gmarz" created="2015-09-08T13:05:56Z" id="138554040">@jochen-st yea, that's one of the issues with the script.  The other, which you're running into now has to do with the fact that the new elasticsearch CLI now requires a [start](https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch.bat#L46) argument, which the service doesn't pass.

I'm working on a fix and will open a PR shortly.
</comment><comment author="gmarz" created="2015-09-08T15:01:15Z" id="138593350">@jochen-st sorry, just realized I misread your comment and that you already discovered that the script doesn't pass the `start` param.  I just opened the above PR to address the startup issues.

The stopping issue is a bit more involved.  Apache Commons Daemon requires a static stop method (--StopMethod) in order to shutdown gracefully.  This use to exist (`org.elasticsearch.bootstrap.Elasticsearch.close()`) but was removed in ES 2.x.  The script was never updated, hence the error message.

Not sure the reasoning for removing it and how it should be added back.  I'm going to open a new issue for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow reads on shards that are in POST_RECOVERY</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13246</link><project id="" key="" /><description>Currently, we do not allow reads on shards which are in POST_RECOVERY which
unfortunately can cause search failures on shards which just recovered if there no replicas (#9421).
The reason why we did not allow reads on shards that are in POST_RECOVERY is
that after relocating a shard might miss a refresh if the node that executed the
refresh is behind with cluster state processing. If that happens, a user might execute
index/refresh/search but still not find the document that was indexed.

We changed how refresh works now in #13068 to make sure that shards cannot miss a refresh this
way by sending refresh requests the same way that we send write requests.

This commit changes IndexShard to allow reads on POST_RECOVERY now.
In addition it adds two test:
- test for issue #9421 (After relocation shards might temporarily not be searchable if still in POST_RECOVERY)
- test for visibility issue with relocation and refresh if reads allowed when shard is in POST_RECOVERY

closes #9421
</description><key id="104266874">13246</key><summary>Allow reads on shards that are in POST_RECOVERY</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T13:28:11Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-09-02T07:46:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-01T14:38:54Z" id="136744266">I left a minor comment, o.w. LGTM. I wonder if we can simplify the test a bit but we can work/discuss on this later. Let's gets this under CI.
</comment><comment author="brwe" created="2015-09-01T17:06:13Z" id="136797972">@dakrone @bleskes thanks for the reviews! I pushed another commit.  
</comment><comment author="bleskes" created="2015-09-01T19:11:07Z" id="136830797">LGTM. Left one minor request. No need for another review... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When JNA is not found, leave the warning but don't show the stracktrace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13245</link><project id="" key="" /><description>Using a default install on windows, one gets:

```
16:02:51,783  WARN main elasticsearch.bootstrap - JNA not found. native methods will be disabled.
java.lang.ClassNotFoundException: com.sun.jna.Native
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:195)
    at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:43)
    at org.elasticsearch.bootstrap.BootstrapInfo.isMemoryLocked(BootstrapInfo.java:44)
    at org.elasticsearch.monitor.process.ProcessProbe.processInfo(ProcessProbe.java:139)
    at org.elasticsearch.monitor.process.ProcessService.&lt;init&gt;(ProcessService.java:44)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:190)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.hadoop.EsEmbeddedServer.&lt;init&gt;(EsEmbeddedServer.java:47)
    at org.elasticsearch.hadoop.LocalEs.before(LocalEs.java:67)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
16:02:52,563  INFO main elasticsearch.node - [Leader] initialized
16:02:52,563  INFO main elasticsearch.node - [Leader] starting ..
```

The stracktrace is confusing - it looks like there's something fatal/wrong with Elasticsearch when that is not the case.
Considering the warning is properly displayed, there's no value in showing the stracktrace as well even more so in the logs which are user-friendly (as opposed to focused on devs).
</description><key id="104262125">13245</key><summary>When JNA is not found, leave the warning but don't show the stracktrace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>:Core</label><label>discuss</label><label>low hanging fruit</label></labels><created>2015-09-01T13:07:31Z</created><updated>2016-06-23T13:27:01Z</updated><resolved>2016-06-23T13:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-01T13:34:14Z" id="136723752">What default install? JNA works with the .zip file just fine on windows, if its not working, then there is a misconfiguration and we should understand why its not working?
</comment><comment author="costin" created="2015-09-01T13:43:35Z" id="136726570">I'm not using the zip (just checked and it starts fine) but the jar to start an embedded server (to be used in integration tests as seen in the stacktrace above).
Looking at the pom, JNA is marked as an optional dependency which might be the reason why it is not being downloaded and added to the classpath. Which is fine. 
And brings us to the initial report - the warning is in place and the stacktrace is just noise.
</comment><comment author="rmuir" created="2015-09-01T13:44:50Z" id="136727088">But the stacktrace is necessary to determine what is wrong when it does not work: and logging should be 100% geared at elasticsearch.jar being used as a server, nothing else, since thats its purpose.
</comment><comment author="costin" created="2015-09-01T13:58:59Z" id="136731435">Maybe I'm misunderstanding then the use of JNA. Since it's marked as optional and there's a warning it can't be found, my assumption was that it is just that, optional - nice to have but not mandatory. in other words, ES can run just fine without.
If that is not the case, then JNA should marked as non-optional in the POM (resulting in its download).

As for the stacktrace, in this case it indicates no memory information can be displayed since JNA was not found. But ES already knew this yet it looks like it wasn't prepared for it.
</comment><comment author="rmuir" created="2015-09-01T14:10:17Z" id="136734248">&gt; As for the stacktrace, in this case it indicates no memory information can be displayed since JNA was not found. But ES already knew this yet it looks like it wasn't prepared for it.

No, the stacktrace is super valuable, it tells you what is happening here:

```
    at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:43)
    at org.elasticsearch.bootstrap.BootstrapInfo.isMemoryLocked(BootstrapInfo.java:44)
    at org.elasticsearch.monitor.process.ProcessProbe.processInfo(ProcessProbe.java:139)
    at org.elasticsearch.monitor.process.ProcessService.&lt;init&gt;(ProcessService.java:44)
```

So this could be avoided when "using elasticsearch.jar as something other than a server" by changing BootStrapInfo to have booleans set to false, and having bootstrap logic change them to true on success. This way any ProcessService initialization won't cause Natives.class to be loaded. 

See, stacktrace is good :)
</comment><comment author="costin" created="2015-09-01T14:19:59Z" id="136737168">&gt; See, stacktrace is good :)

Agreed. I was paying attention only to the `ClassNotFound` which was redundant considering the previous warning:

```
16:02:51,783  WARN main elasticsearch.bootstrap - JNA not found. native methods will be disabled.
java.lang.ClassNotFoundException: com.sun.jna.Native
```

It turned out it was something else related to JNA.
</comment><comment author="jpountz" created="2015-09-03T18:16:06Z" id="137532616">@costin so should we close now?
</comment><comment author="costin" created="2015-09-03T21:23:51Z" id="137578811">Has there been any update to address/prevent the exception from occurring? I'm not clear whether @rmuir's solution is in place or not.
</comment><comment author="rmuir" created="2015-09-03T22:59:37Z" id="137595984">i havent done anything yet. it would have the disadvantage of being a little harder to follow: these booleans wouldn't be static final and would be modified/set elsewhere and hard to track where they are coming from...
</comment><comment author="ltregan" created="2015-10-02T08:30:39Z" id="144959039">Just adding a testimony as a user: I admit I got confused too for a couple of hours.

I'd propose as short term option to (i) modify the warning message to "WARNING: JNA not found, native methods will be disabled. This is ok for embedded / non-server mode.", and (ii) display the full stack trace only in DEBUG mode ? 
</comment><comment author="rmuir" created="2015-10-02T10:12:58Z" id="144982832">I don't think we should modify this native code, its too important when running as a server.

running as a server is _all that matters_
</comment><comment author="sverhagen" created="2015-10-19T02:33:57Z" id="149080236">It would be great if it were possible to suppress the stack trace, though. It's a red flag to see stack traces, even during integration tests, or wherever we would be running Elasticsearch where we don't have these native libraries available. The way it is set up now, there is no way to reconfigure _just_ that log statement, or to extend `Natives` in a way that let's us override this behavior (I haven't seen a way yet). I appreciate that you consider the stack trace important ("a server is _all that matters_"), so perhaps don't remove the stack trace, but change `Natives` in a way that allows people to change the implementation, if it is really important to these people. Thank you.
</comment><comment author="gary-harpaz" created="2016-02-02T15:05:34Z" id="178621848">sverhagen Agreed.  My Application now starts up with class not found exception in the logs. How many times will someone else see this log and waste time googling about it?
</comment><comment author="eximius313" created="2016-02-06T13:49:24Z" id="180766607">so I'm another victim of this stack trace ;)
</comment><comment author="huntc" created="2016-02-08T21:37:56Z" id="181578373">I'm now quite confused. When running as a server is JNA required? If so then you have another problem. JNA is LGPL thus violating your Apache 2 license declaration. Please clarify on the optionality of JNA and the impact on your license. Thanks. 
</comment><comment author="jasontedor" created="2016-02-08T21:42:51Z" id="181580121">&gt;  If so then you have another problem. JNA is LGPL thus violating your Apache 2 license declaration.

@huntc JNA is dual-licensed with [Apache License 2.0 since version 4.0.0 of JNA](https://github.com/java-native-access/jna/blob/0c28025558cb88431c99bb7b89f631e6888d5bad/LICENSE#L1-L3).
</comment><comment author="huntc" created="2016-02-08T23:24:20Z" id="181619107">Thanks @jasontedor ! I had not noticed the dual license. I shall update our licensing tool!
</comment><comment author="Malu44" created="2016-02-18T16:08:23Z" id="185794082">I'm also confused - should we use JNA for Elastic as a server? Stack trace leads to unnecessary effort. 
e.g. search in google, is a relevant dependency.
</comment><comment author="jasontedor" created="2016-02-21T01:28:42Z" id="186716414">&gt; I'm also confused - should we use JNA for Elastic as a server?

Yes, otherwise Elasticsearch can not `mlockall` and can not use `seccomp` (among some other features that will be disabled).
</comment><comment author="sverhagen" created="2016-02-21T23:18:19Z" id="186942395">We're getting distracted here :wink: I wasn't arguing the (in-)validity of the exception. But it would be nice to suppress it during my integration tests. Any chance? :smile: 
</comment><comment author="Malu44" created="2016-02-22T11:36:40Z" id="187134155">Same problem here: in our jenkins-job, we're starting an elastic server for integration testing.
I've 'fixed' this by suppress the message via log configuration
`&lt;logger name="org.elasticsearch.bootstrap"
        level="ERROR" /&gt;
`
</comment><comment author="Malu44" created="2016-02-23T12:46:07Z" id="187685619">Also you can put the following dependency into your POM
`&lt;dependency&gt;
            &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt;
            &lt;artifactId&gt;jna&lt;/artifactId&gt;
            &lt;version&gt;4.1.0&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
`
</comment><comment author="jillesvangurp" created="2016-05-28T11:31:11Z" id="222303925">The warning is confusing as it is because it doesn't explain the problem properly, nor how to fix it and it.

Two ways to improve: 1) make JNA not optional (why is it optional to begin with?). or 2) explain how to fix it based on @Malu44's suggestion. 
</comment><comment author="s1monw" created="2016-05-28T11:56:39Z" id="222304870">@jillesvangurp I think it used to be LGPL (hence the optional) but it's dual licensed under the ASL 2.0  now since version 4.0.0 upwards. I think we can make it non optional and be done with it, @rmuir @costin WDYT?
</comment><comment author="costin" created="2016-05-28T20:55:41Z" id="222329251">+1
</comment><comment author="martin-g" created="2016-06-23T11:11:29Z" id="228018650">+1
</comment><comment author="jillesvangurp" created="2016-06-23T11:45:22Z" id="228025601">For the record, I don't think optional false/true in the pom file makes any difference legally. What matters with respect to the license is what you ship in the binary (included, I assume so it's not an issue there?) or what the developers that use es download and bundle themselves (through using maven). They can always choose to exclude the dependency for legal or other reasons in their own pom file. Lgpl and apache licenses are not incompatible as far as I know, though admittedly v3 and afferror are tedious to deal with from a legal point of view. 

BTW. I fully appreciate that you guys probably had lengthy internal debates on this already with your legal people and definitely don't want to reboot any of that (I had some exposure to that stuff in Nokia back in the day). Just curious what the legal reasoning is for setting optional to true.
</comment><comment author="clintongormley" created="2016-06-23T12:12:19Z" id="228031275">Reopening to discuss making JNA non-optional
</comment><comment author="nik9000" created="2016-06-23T13:27:01Z" id="228049018">&gt; Reopening to discuss making JNA non-optional

We just made it non-optional. It seems like we had consensus there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lithuanian analysis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13244</link><project id="" key="" /><description>See https://issues.apache.org/jira/browse/LUCENE-6694
</description><key id="104259360">13244</key><summary>Lithuanian analysis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Analysis</label><label>feature</label><label>review</label><label>v2.1.0</label></labels><created>2015-09-01T12:54:36Z</created><updated>2015-09-01T19:42:58Z</updated><resolved>2015-09-01T19:42:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-01T14:18:51Z" id="136736667">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Estimate HyperLogLog bias via k-NN regression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13243</link><project id="" key="" /><description>The implementation this commit replaces was almost k-NN regression with k=2, but had two bugs: (a) it depends on the empirical raw estimates being in strictly non-decreasing order for the binary search (which they are not); and (b) it weights the biases positively with increased distance from the corresponding raw estimate.

&#8220;HyperLogLog in Practice&#8221; leaves the choice of exact algorithm here fairly vague, just noting: &#8220;We use k-nearest neighbor interpolation to get the bias for a given raw estimate (for k = 6).&#8221;  The majority of other open source HyperLogLog++ implementations appear to use k-NN regression with uniform weights (and generally k = 6).  Uniform weighting does decrease variance, but also introduces bias at the domain extrema.  This problem, plus the use of the word &#8220;interpolation&#8221; in the original paper, suggests (inverse) distance-weighted k-NN, as implemented here.
</description><key id="104255634">13243</key><summary>Estimate HyperLogLog bias via k-NN regression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">llasram</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.1.0</label></labels><created>2015-09-01T12:33:48Z</created><updated>2015-09-01T13:24:02Z</updated><resolved>2015-09-01T12:59:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T13:24:02Z" id="136720139">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator with groovy script fails when doc_values are enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13242</link><project id="" key="" /><description>Tested on Elasticsearch 1.7.1.

Prepare the mapping and percolator as follows:

```
curl -XPUT localhost:9200/test -d '{
  "mappings" : {
    "test" : {
      "properties" : {
        "name" : {
          "type" : "string",
          "index" : "not_analyzed",
          "doc_values": true
        }
      }
    }
  }
}'
curl -XPUT localhost:9200/test/.percolator/1 -d '{
  "type": "test",
  "query": {
    "constant_score": {
      "filter": {
        "script": {
          "script": "doc[\"name\"].value.contains(\"a\")",
          "lang": "groovy"
        }
      }
    }
  }
}'
```

This script tests for a match, it keeps looping silently as long as the expected result arrives. When an unexpected result arrives it's printed and the loop terminates:

```
while ! curl -s localhost:9200/test/test/_percolate -d '{ "doc": { "name": "a" } }' | grep -v '"failed":0},"total":1'; do :; done
```

Unfortunately the script fails instantly.

```
{"took":5,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"index":"test","shard":2,"status":400,"reason":"BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: PercolateException[failed to execute]; nested: GroovyScriptExecutionException[NullPointerException[Cannot invoke method contains() on null object]]; "}]},"total":0,"matches":[]}
```

Elasticsearch logs the following stacktrace

```
[2015-09-01 13:51:02,378][DEBUG][action.percolate         ] [esnode-r] [test][2], node[rbfvveU3R3GjVtXyTzQp9g], [P], s[STARTED]: failed to execute [org.elasticsearch.action.percolate.PercolateRequest@d53bdf0]
org.elasticsearch.percolator.PercolateException: failed to percolate
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:195)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:56)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.percolator.PercolateException: failed to execute
    at org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:564)
    at org.elasticsearch.percolator.PercolatorService.percolate(PercolatorService.java:242)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:192)
    ... 5 more
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: NullPointerException[Cannot invoke method contains() on null object]
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:274)
    at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter$ScriptDocSet.matchDoc(ScriptFilterParser.java:187)
    at org.elasticsearch.common.lucene.docset.MatchDocIdSet$NoAcceptDocsIterator.nextDoc(MatchDocIdSet.java:96)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:257)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.common.lucene.Lucene.countWithEarlyTermination(Lucene.java:320)
    at org.elasticsearch.common.lucene.Lucene.countWithEarlyTermination(Lucene.java:308)
    at org.elasticsearch.common.lucene.Lucene.exists(Lucene.java:299)
    at org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:560)
    ... 7 more

```

This issue does not occur when the line `"doc_values": true` is not included in the mapping (it defaults to false in elasticsearch 1.7.1).
What perhaps is worse, if that the wrong result (no result) is returned when the percolator script is changed to `"doc[\"name\"].value == \"a\""`.

I discovered this issue because we use the workaround mentioned in #8879 to avoid percolator concurrency issues. Since elasticsearch uses doc_values by default, we were investigating what the performance inpact is of using docvalues, however we discovered this unexpected problem.
</description><key id="104249810">13242</key><summary>Percolator with groovy script fails when doc_values are enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">WellingR</reporter><labels><label>:Percolator</label><label>adoptme</label><label>bug</label></labels><created>2015-09-01T12:00:52Z</created><updated>2016-03-24T15:26:22Z</updated><resolved>2016-03-24T15:26:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-01T13:35:03Z" id="136723948">In master this fails with:

```
"failed to run inline script [doc[\"name\"].value.contains(\"a\")] using lang [groovy]: IllegalStateException[unexpected docvalues type NONE for field 'name' (expected one of [SORTED, SORTED_SET]). Use UninvertingReader or index with docvalues.]"
```
</comment><comment author="rjernst" created="2015-09-01T14:41:37Z" id="136745465">IIRC, doc values is not supported in MemoryIndex. 
</comment><comment author="s1monw" created="2015-09-01T14:43:39Z" id="136746390">it's not supported though
</comment><comment author="clintongormley" created="2015-09-01T15:10:01Z" id="136755196">Could one possibility be to ignore the doc values setting when creating the in-memory index?
</comment><comment author="jpountz" created="2015-09-01T15:33:10Z" id="136762569">I think this is what already happens today. Then the issue arises at search time because fielddata tries to load through doc values (because the mappings say this is how it should get it) but they are missing so `doc[\"name\"].value` returns `null` instead of the document value, hence the NPE.
</comment><comment author="WellingR" created="2015-09-09T08:03:18Z" id="138818249">I have seen a similar issue when I use a `geo_point` with both `geohash_prefix` and `doc_values` enabled. A `geo_distance` filter never seems to match when doc_values are enabled. Does this have the same cause or is this a separate issue?
</comment><comment author="martijnvg" created="2016-03-24T15:26:21Z" id="200885750">From 5.0 the memory index will support doc values, which will fix the issue reported here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document transport.ping_schedule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13241</link><project id="" key="" /><description>This was added in #10189 and added in 1.6 and above.

Per the title it'd be great to have this added into official docs :)
</description><key id="104243334">13241</key><summary>Document transport.ping_schedule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-09-01T11:17:09Z</created><updated>2015-09-01T11:44:36Z</updated><resolved>2015-09-01T11:44:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add listeners for postIndex, postCreate, and postDelete, backport to 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13240</link><project id="" key="" /><description>Backport of #13203.
Related to #13202
</description><key id="104231565">13240</key><summary>Add listeners for postIndex, postCreate, and postDelete, backport to 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kiryam</reporter><labels><label>:Internal</label><label>bug</label><label>review</label></labels><created>2015-09-01T10:00:34Z</created><updated>2016-01-09T19:29:29Z</updated><resolved>2016-01-09T19:29:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-01T13:14:39Z" id="136715747">Hi @kiryam 

How is this related to #13203?  There is nothing in the PR description. Is this just the backport? (normally we take care of the backport)
</comment><comment author="kiryam" created="2015-09-01T13:21:04Z" id="136718003">@clintongormley Hello, yes! it is backport only. Tests pending
</comment><comment author="s1monw" created="2015-09-01T13:28:57Z" id="136722441">@kiryam don't worry about backporting I will take care of this if we decide it should go in that branch!
</comment><comment author="kiryam" created="2015-09-01T13:29:18Z" id="136722511">Thx!
</comment><comment author="s1monw" created="2016-01-09T19:29:29Z" id="170273231">this can be closed it has been backported afaik
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.3.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13239</link><project id="" key="" /><description>From a user perspective, the main benefit from this upgrade is that the new
Lucene53Codec has disk-based norms. The elasticsearch directory has been fixed
to load these norms through mmap instead of nio.

Other changes include the removal of `max_thread_states`, the fact that
PhraseQuery and BooleanQuery are now immutable, and that deleted docs are now
applied on top of the Scorer API.

This change introduces a couple of `AwaitsFix`s but I don't think it should
hold us from merging.
</description><key id="104231337">13239</key><summary>Upgrade to lucene-5.3.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>review</label><label>upgrade</label><label>v2.1.0</label></labels><created>2015-09-01T09:59:01Z</created><updated>2015-09-25T06:47:51Z</updated><resolved>2015-09-01T12:03:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-01T10:16:21Z" id="136663538">looks great, +1
</comment><comment author="dpursehouse" created="2015-09-01T10:55:09Z" id="136674180">:+1: I just did this change myself locally but you beat me to it.
</comment><comment author="dpursehouse" created="2015-09-25T04:07:39Z" id="143118833">Is there any possibility to include this in the 2.0.0 release?
</comment><comment author="jpountz" created="2015-09-25T06:47:51Z" id="143143886">@dpursehouse We are in the process of stabilizing 2.0 and this is a big change, so it won't be backported. The good news however is that we aim at releasing 2.1 relatively soon after 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timeout for refresh and flush </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13238</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/pull/13068 we changed refresh and flush to work like write requests: refresh and flush requests go to the primary first and are then replicated. One difference to before is though that if a shard is not available (INITIALIZING for example) we wait a little for an indexing request but for refresh we don't and just give up immediately.
Before, refresh requests were just send to the shards regardless of what their state is.

In tests we sometimes create an index, issue an indexing request, refresh and then get the document. But we do not wait until all nodes know that all primaries have ben assigned.
Now potentially one node can be one cluster state behind and not know yet that the shards have ben started. If the refresh is executed through this node then the refresh request will silently fail on shards that are started already because from the nodes perspective they are still initializing. As a consequence, documents that expected to be available in the test are now not.
Example test failures are here: http://build-us-00.elastic.co/job/elasticsearch-20-oracle-jdk7/395/

I will change the timeout to be 1m for refresh and flush also to avoid the test failures but we have to think about if this change is OK or if we want to change TransportReplicationAction instead.
</description><key id="104228744">13238</key><summary>Timeout for refresh and flush </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>discuss</label></labels><created>2015-09-01T09:50:07Z</created><updated>2015-09-07T12:40:26Z</updated><resolved>2015-09-07T12:39:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-01T10:23:46Z" id="136666211">changed timeout to 1m here: 05b48b904d55a237d69db3de28c7069cab901a7b This should stop the test failures.
</comment><comment author="brwe" created="2015-09-07T12:39:50Z" id="138286203">We discussed it ( @bleskes  @clintongormley ) and decided the timeout of 1m is OK. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate eager/lazy norms loading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13237</link><project id="" key="" /><description>Given that norms become disk-based in Lucene 5.3, having the ability to eagerly or lazily load them doesn't make much sense anymore.
</description><key id="104228484">13237</key><summary>Deprecate eager/lazy norms loading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>adoptme</label><label>deprecation</label><label>low hanging fruit</label></labels><created>2015-09-01T09:48:46Z</created><updated>2017-06-15T08:57:17Z</updated><resolved>2017-06-15T08:57:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-01T09:51:54Z" id="136655474">or it could be implemented more appropriately, e.g. https://issues.apache.org/jira/browse/LUCENE-6549
</comment><comment author="jpountz" created="2017-06-15T08:57:17Z" id="308671727">This was done in 5.0, and we now expose a preload setting to allow loading specific parts of the index into the fs cache.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue with time_zone in date_histogram aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13236</link><project id="" key="" /><description>Hi folks,

We are currently using the date_histogram pre_zone/post_zone parameters and would like to change to using time_zone since the others are deprecated.
However they produce different results when using an hour interval.

To see this I created an index with four documents

``` javascript
[{"date": "2014-12-31T23:30:00.000Z"},
{"date": "2014-12-31T23:30:00.000Z"},
{"date": "2015-1-1T00:30:00.000Z"},
{"date": "2015-1-1T01:30:00.000Z"}]
```

Using the pre_zone parameters

``` javascript
{
  "aggs": {
    "articles_over_time": {
      "date_histogram": {
        "field": "date",
        "interval": "hour",
        "pre_zone": "-01:00",
        "post_zone": "-01:00",
        "pre_zone_adjust_large_interval": true
      }
    }
  }
}
```

gives

``` javascript
[
  {"key_as_string": "2014-12-31T22:00:00.000Z", "key": 1420063200000, "doc_count": 1},
  {"key_as_string": "2014-12-31T23:00:00.000Z", "key": 1420066800000, "doc_count": 1},
  {"key_as_string": "2015-01-01T00:00:00.000Z", "key": 1420070400000, "doc_count": 1},
  {"key_as_string": "2015-01-01T01:00:00.000Z", "key": 1420074000000, "doc_count": 1}
]
```

Using the time_zone parameter

``` javascript
{
  "aggs": {
    "articles_over_time": {
      "date_histogram": {
        "field": "date",
        "interval": "hour",
        "time_zone": "-01:00"
      }
    }
  }
}
```

gives

``` javascript
[
  {"key_as_string": "2014-12-31T23:00:00.000Z", "key": 1420066800000, "doc_count": 1},
  {"key_as_string": "2015-01-01T00:00:00.000Z", "key": 1420070400000, "doc_count": 1},
  {"key_as_string": "2015-01-01T01:00:00.000Z", "key": 1420074000000, "doc_count": 1},
  {"key_as_string": "2015-01-01T02:00:00.000Z", "key": 1420077600000, "doc_count": 1}
]
```

Changing the value of time_zone has no impact on the results.
It works fine with larger intervals (day, month, year) just not with hour.
I'm using Elasticsearch 1.7.0 on windows.

Regards

Colm A

P.S. Its not an issue but I wondering why the key_as_string is always returned in UTC even when using time_zone. The time part of the string is correct for the time zone just the lack of timezone in is unexpected.
</description><key id="104219640">13236</key><summary>Issue with time_zone in date_histogram aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">colmaengus</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-09-01T09:02:02Z</created><updated>2015-09-07T19:15:56Z</updated><resolved>2015-09-07T18:04:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-01T13:09:32Z" id="136713938">@cbuescher could you take a look at this please?
</comment><comment author="cbuescher" created="2015-09-07T11:02:38Z" id="138270229">@colmaengus thanks for pointing this out, you are right that in ES 1.7 changing `time_zone` doesn't have an impact on the bucket `key_as_string` yet. This was changes with https://github.com/elastic/elasticsearch/pull/9744 and will be working in the upcoming 2.0 release.

In 1.7. `time_zone` is basically still working like `pre_zone` in that it shifts bucket boundaries for day intervals and larger but doesn't affect the buckey keys. Unfortunately you still need to use `post_zone` if you want to affect the buckey keys. In 2.0 the bucket `key` will always be returned in UTC to avoid confusion with time zone issues, but the `key_as_string` will take the `time_zone` setting of the aggregation into account, giving you the behaviour you probably expected above. Let me know if this clarifies things or if you need more help with this.
</comment><comment author="clintongormley" created="2015-09-07T18:04:33Z" id="138350370">I've updated the 2.0 documentation for `time_zone` in date histograms to demonstrate more clearly how it will work: https://www.elastic.co/guide/en/elasticsearch/reference/2.0/search-aggregations-bucket-datehistogram-aggregation.html#_time_zone

We're not going to change this in 1.7, so I'm going to close this issue
</comment><comment author="colmaengus" created="2015-09-07T19:15:56Z" id="138362311">Thanks for the clarification guys.

On Mon, Sep 7, 2015 at 7:05 PM, Clinton Gormley notifications@github.com
wrote:

&gt; I've updated the 2.0 documentation for time_zone in date histograms to
&gt; demonstrate more clearly how it will work:
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/2.0/search-aggregations-bucket-datehistogram-aggregation.html#_time_zone
&gt; 
&gt; We're not going to change this in 1.7, so I'm going to close this issue
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13236#issuecomment-138350370
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove all write support for Elasticsearch090RWPostingsFormat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13235</link><project id="" key="" /><description>While working on removing Ints and Longs usage for #13224 I found it used
quite heavily in BloomFilter which is only essentially used in our tests. We don't
write this anymore and our upgrade API allows to move to newer postings formats.
We can drop this part of the code and rely on our bwc tests to test the reading of
old indices.
</description><key id="104206630">13235</key><summary>Remove all write support for Elasticsearch090RWPostingsFormat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-09-01T07:48:21Z</created><updated>2015-09-26T18:56:25Z</updated><resolved>2015-09-26T18:56:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T08:46:18Z" id="136638238">I'm a bit nervous about removing PostingsFormatTest. Should we just make it a 3.0 change and remove Elasticsearch090PostingsFormat entirely?
</comment><comment author="s1monw" created="2015-09-01T14:47:44Z" id="136748132">&gt; I'm a bit nervous about removing PostingsFormatTest. Should we just make it a 3.0 change and remove Elasticsearch090PostingsFormat entirely?

I hear you... I really didn't want to change the logic in the BloomFilter to get rid of guava I think that's quite risky. I am happy to push it out to 3.x
</comment><comment author="s1monw" created="2015-09-26T18:56:16Z" id="143479498">superseded by https://github.com/elastic/elasticsearch/pull/13799
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document all possible logging packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13234</link><project id="" key="" /><description>In `logging.yml` you can set things like `gateway: DEBUG` or `discovery: TRACE`.

It'd be great if we could document all of these packages that ES exposes as there's quite a few that we don't mention.
</description><key id="104205553">13234</key><summary>Document all possible logging packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-09-01T07:39:28Z</created><updated>2015-09-03T05:36:02Z</updated><resolved>2015-09-03T05:36:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T19:44:50Z" id="137224150">&gt; It'd be great if we could document all of these packages that ES exposes as there's quite a few that we don't mention.

It'd be great if we could generate that documentation....
</comment><comment author="markwalkom" created="2015-09-03T05:34:41Z" id="137338449">Gotta start somewhere :p
</comment><comment author="markwalkom" created="2015-09-03T05:36:02Z" id="137339083">I just realised I had already raised this in #10211
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add method setDefaultDatePrinter()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13233</link><project id="" key="" /><description>We could not change default TimeZone.
</description><key id="104195830">13233</key><summary>add method setDefaultDatePrinter()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">satoshi-kimura</reporter><labels /><created>2015-09-01T06:31:30Z</created><updated>2015-09-01T08:28:20Z</updated><resolved>2015-09-01T07:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T07:04:21Z" id="136608758">I don't think we show allow to change the default date printer. If you are wrapping extensions to elasticsearch that need to use a different timezone, please use the field(String, Date, DateTimeFormatter)  method that takes a DateTimeFormatter.
</comment><comment author="satoshi-kimura" created="2015-09-01T08:11:01Z" id="136628539">Some QueryBuilder  use field(String, Object), ex: TermQuery, MatchQueryBuilder... .
Should I wrapping any QueryBuilder ?
</comment><comment author="jpountz" created="2015-09-01T08:28:20Z" id="136635130">What are you trying to achieve exactly? Supporting a timezone on term/match queries makes little sense since these queries can only look up exact date with full (millisecond) precision, while timezones only make sense for dates that are specified without time so that elasticsearch can resolve the corresponding milliseconds since Epoch.

On the contrary, range queries allow you to give rounded date, eg. 2015-07-01 and then use the timezone parameter in order to know how to translate it into milliseconds since Epoch.

If you really need to run queries on exact dates (millisecond-precision) then you should use the match/term queries with a Date object. Otherwise use the range query if you would like to match all dates that fall withing eg. a given day.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing null check in ESPolicy.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13232</link><project id="" key="" /><description>This allows reducing privileges with doPrivileged to work,
otherwise it will fail with NPE.

In general, if some code wants to do that, let it. The null
check is needed, even though ProtectionDomain(CodeSource, PermissionCollection)
is more than a bit misleading: "the current Policy will not be consulted".

Additionally add a defensive check for location, since the docs
there are even more confusing: https://bugs.openjdk.java.net/browse/JDK-8129972

The jdk policy impl has both these checks.
</description><key id="104184998">13232</key><summary>Add missing null check in ESPolicy.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-01T04:36:19Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-09-01T10:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T07:05:32Z" id="136608907">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mask issues in /var/lib/elasticsearch/elasticsearch/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13231</link><project id="" key="" /><description>I have a fresh debian 8 system taking elasticsearch 1.7.1 from the repos, running on openjdk. Elasticsearch wouldn't start because it couldn't write to `/var/lib/elasticsearch/elasticsearch/`, which was owned by the ES user, but whose permissions were 655 - no executable bit for the user means that ES couldn't create the subdir `nodes`.

I fixed this with `chmod +x /var/lib/elasticsearch/elasticsearch/` to change the perms to 755, but ran into the same problem again when ES tried to write to `nodes/0`, which was fixed by chmodding `nodes`. At this point ES successfully started, and created the subdir `0`... which didn't have the missing directory execute bit.

I'm not very familiar with java or systemd, and have no idea why the first two directories might be missing the +x but the final directory wasn't. I was thinking it was a mask issue, but the third directory was created correctly. 

Elasticsearch is up and running at this point, but I thought I might mention the issue here (couldn't find a previous ticket)

``` bash
root@mayfly:/etc/systemd# apt-cache policy elasticsearch
elasticsearch:
  Installed: 1.7.1
  Candidate: 1.7.1
  Version table:
 *** 1.7.1 0
        500 http://packages.elastic.co/elasticsearch/1.7/debian/ stable/main amd64 Packages
        100 /var/lib/dpkg/status

root@mayfly:/etc/systemd# java -version
java version "1.7.0_79"
OpenJDK Runtime Environment (IcedTea 2.5.6) (7u79-2.5.6-1~deb8u1)
OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)

root@mayfly:/etc/systemd# journalctl -u elasticsearch.service
(--snip a bit--)
Sep 01 01:51:59 mayfly.aws-sydney.etaskr.com systemd[1]: Starting Elasticsearch...
Sep 01 01:51:59 mayfly.aws-sydney.etaskr.com systemd[1]: Started Elasticsearch.
Sep 01 01:52:00 mayfly.aws-sydney.etaskr.com elasticsearch[1530]: {1.7.1}: Initialization Failed ...
Sep 01 01:52:00 mayfly.aws-sydney.etaskr.com elasticsearch[1530]: - ElasticsearchIllegalStateException[Failed to created node environment]
Sep 01 01:52:00 mayfly.aws-sydney.etaskr.com elasticsearch[1530]: AccessDeniedException[/var/lib/elasticsearch/elasticsearch/nodes]
Sep 01 01:52:00 mayfly.aws-sydney.etaskr.com systemd[1]: elasticsearch.service: main process exited, code=exited, status=3/NOTIMPLEMENTED
Sep 01 01:52:00 mayfly.aws-sydney.etaskr.com systemd[1]: Unit elasticsearch.service entered failed state.
Sep 01 01:56:46 mayfly.aws-sydney.etaskr.com systemd[1]: Starting Elasticsearch...
Sep 01 01:56:46 mayfly.aws-sydney.etaskr.com systemd[1]: Started Elasticsearch.
Sep 01 01:56:47 mayfly.aws-sydney.etaskr.com elasticsearch[1755]: {1.7.1}: Initialization Failed ...
Sep 01 01:56:47 mayfly.aws-sydney.etaskr.com elasticsearch[1755]: - ElasticsearchIllegalStateException[Failed to created node environment]
Sep 01 01:56:47 mayfly.aws-sydney.etaskr.com elasticsearch[1755]: AccessDeniedException[/var/lib/elasticsearch/elasticsearch/nodes/0]
Sep 01 01:56:47 mayfly.aws-sydney.etaskr.com systemd[1]: elasticsearch.service: main process exited, code=exited, status=3/NOTIMPLEMENTED
Sep 01 01:56:47 mayfly.aws-sydney.etaskr.com systemd[1]: Unit elasticsearch.service entered failed state.
Sep 01 01:59:30 mayfly.aws-sydney.etaskr.com systemd[1]: Starting Elasticsearch...
Sep 01 01:59:30 mayfly.aws-sydney.etaskr.com systemd[1]: Started Elasticsearch.
```

I'd installed ES using this ansible ELK playbook (with some edits to update new repos), but there doesn't seem to be anything in it that might be related to this issue: https://github.com/bakhti/ansible-elk/blob/master/tasks/elasticsearch.yml
</description><key id="104169222">13231</key><summary>Mask issues in /var/lib/elasticsearch/elasticsearch/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vacri</reporter><labels /><created>2015-09-01T02:09:42Z</created><updated>2016-01-13T00:15:42Z</updated><resolved>2015-09-01T02:30:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vacri" created="2015-09-01T02:30:55Z" id="136556130">Closing ticket for now - the ansible playbook has custom config for managing ES. I need to look at that in more detail, sorry.

edit: yep, problem disappeared when running according to the docs
</comment><comment author="conorsch" created="2016-01-13T00:06:53Z" id="171105956">@vacri Having the same directory mask problem you encountered with bakhti's role, but I'm completely unable to track down where the problem is slipping in. My best guess at this point is something to do with the `monit` service, so I'll try forking and ripping that out. Wouldn't hurt to test OSes other than Debian 8, as well.
</comment><comment author="conorsch" created="2016-01-13T00:15:42Z" id="171107409">Bizarrely, testing in Ubuntu 14.04 rather than Debian 8 fixes the directory mask problem. No clue why that is. Looks like I'm in for some hair-pulling.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>inner_hits from has_parent not populated when _parent field requested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13230</link><project id="" key="" /><description>Using elasticsearch 1.7.1.
When `"fields" : ["_parent"]` is included in a query, it stops the results of `"has_parent" : { ...,  "inner_hits" : {}}` being included in results.

Minimal repro script that demonstrates the problem:

```
#! /bin/bash

SERVER="localhost:9200"

curl -w "\n" -X DELETE "$SERVER/my_index"
curl -w "\n" -X PUT "$SERVER/my_index"
curl -w "\n" -X PUT "$SERVER/my_index/_mapping/parent_doc_type" -d '{ "parent_doc_type" : { "properties" : { "name" : { "type" : "string" } } } }'
curl -w "\n" -X PUT "$SERVER/my_index/_mapping/child_type" -d '{ "child_type" : { "properties" : { "name" : { "type": "string" } }, "_parent" : { "type": "parent_doc_type" } } }'
curl -w "\n" -X POST "$SERVER/my_index/parent_doc_type/parent_doc" -d '{ "name" : "joe" }'
curl -w "\n" -X POST "$SERVER/my_index/child_type/chid_doc?parent=parent_doc&amp;routing=parent_doc" -d '{ "name" : "jack" }'

sleep 1

echo "contains parent id but not inner hits:"
curl -w "\n" -X POST "$SERVER/my_index/child_type/_search" -d ' 
{
  "query": {
    "has_parent": {
      "parent_type": "parent_doc_type",
      "query": { "match_all": {} },
      "inner_hits" : {}
    }
  },
  "fields": ["_source", "_parent"]
}'

echo "removing _parent field causes inner hits to be populated:"
curl -w "\n" -X POST "$SERVER/my_index/child_type/_search" -d '
{
  "query": {
    "has_parent": {
      "parent_type": "parent_doc_type",
      "query": { "match_all": {} },
      "inner_hits" : {}
    }
  },
  "fields": ["_source"]
}'
```

The output of the script is:

```
contains parent id but not inner hits:
{  
  "took":2,
  "timed_out":false,
  "_shards":{  
    "total":5,
    "successful":5,
    "failed":0
  },
  "hits":{  
    "total":1,
    "max_score":1.0,
    "hits":[  
      {  
        "_index":"my_index",
        "_type":"child_type",
        "_id":"chid_doc",
        "_score":1.0,
        "_source":{  
          "name":"jack"
        },
        "fields":{  
          "_parent":"parent_doc"
        },
        "inner_hits":{  
          "parent_doc_type":{  
            "hits":{  
              "total":0,
              "max_score":null,
              "hits":[  

              ]
            }
          }
        }
      }
    ]
  }
}

removing _parent field causes inner hits to be populated:
{  
  "took":2,
  "timed_out":false,
  "_shards":{  
    "total":5,
    "successful":5,
    "failed":0
  },
  "hits":{  
    "total":1,
    "max_score":1.0,
    "hits":[  
      {  
        "_index":"my_index",
        "_type":"child_type",
        "_id":"chid_doc",
        "_score":1.0,
        "_source":{  
          "name":"jack"
        },
        "inner_hits":{  
          "parent_doc_type":{  
            "hits":{  
              "total":1,
              "max_score":1.0,
              "hits":[  
                {  
                  "_index":"my_index",
                  "_type":"parent_doc_type",
                  "_id":"parent_doc",
                  "_score":1.0,
                  "_source":{  
                    "name":"joe"
                  }
                }
              ]
            }
          }
        }
      }
    ]
  }
}
```
</description><key id="104157317">13230</key><summary>inner_hits from has_parent not populated when _parent field requested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mbuhot</reporter><labels><label>:Inner Hits</label><label>adoptme</label><label>bug</label></labels><created>2015-09-01T00:09:07Z</created><updated>2016-01-28T13:59:44Z</updated><resolved>2016-01-28T13:59:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T13:59:44Z" id="176196530">This appears to have been fixed in 2.0.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Locale parsing issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13229</link><project id="" key="" /><description>In the query-refactoring branch we noticed weird behaviours related to parsing locales. The two queries affected are query_string and simple_query_string, as they allow users to set the `locale` used where applicable, depending on the type of lucene query that gets parsed.

Both queries in master parse the locale through our own `LocaleUtils#parse` method and print it out using`Locale#toString` when needed. It turns out that with some locales, our newly added query tests fail given that printing out and reparsing the same locale leads to a different locale. The problem looks very similar to what is explained here: https://issues.apache.org/jira/browse/LUCENE-4021.

I see that we also use `LocaleUtils#parse` in `DateFieldMapper`. I think that we should discuss how to address this issue and streamline the solution. I think the best solution would be to parse through `Locale#forLanguageTag` and print out using `Locale#toLanguageTag`, but this breaks backwards compatibility it seems. If I make this change `SimpleDateMappingTests#testParseLocal` fails for instance. Is also our own `LocaleUtils` still useful?
</description><key id="104133221">13229</key><summary>Locale parsing issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label></labels><created>2015-08-31T20:58:20Z</created><updated>2015-10-05T11:52:26Z</updated><resolved>2015-10-05T11:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-01T11:53:46Z" id="136686526">@javanna how (and how often) does it break bwc?
</comment><comment author="javanna" created="2015-09-01T12:01:09Z" id="136688298">&gt; how (and how often) does it break bwc?

Some locales that used to get properly parsed might not get parsed correctly anymore. I am not sure how widely these options are used, especially in queries. Maybe removing this option as stated in #9978 would be the best solution.
</comment><comment author="rmuir" created="2015-09-01T12:04:24Z" id="136688805">Yes, using the user's already configured analysis chain rather than applying additional configuration (lowercase_expanded_terms, locale, etc) that is still limited to only case is really a better way to go IMO.

If we solve that issue then if they have turkishlowercase, it gets used, if they use asciifolding, it gets used, and so on, and people stop getting confused about why wildcards "don't work" which I bet happens constantly.
</comment><comment author="javanna" created="2015-09-04T12:19:07Z" id="137720505">seems like the way forward would be to remove these `locale` parameter from query_string and simple_query_string, where we encountered issues. The remaining question is what we should do with date fields in mappings, which do allow to specify a locale too. Moving to parsing using `forLanguageTag` seems to break things, but leaving as-is is buggy. @rmuir what would you suggest?
</comment><comment author="javanna" created="2015-10-05T10:04:10Z" id="145485642">while looking into removing `locale` from query_string and simple_query_string as previously discussed, I bumped into `org.apache.lucene.queryparser.classic.QueryParserBase#setLocale` in lucene. I am then wondering if the removal is the way forward or something else should happen in lucene first. Seems like if we remove we might remove stuff that is still relevant and useful...
</comment><comment author="javanna" created="2015-10-05T11:52:26Z" id="145504626">Closing in favour of #9978. Once we have made it possible to use the analysis chain, by taking out components that are not multi term aware, we can go ahead and remove `lowercase_expanded_terms` as well as `locale`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>For Azure, support multiple storage accounts and secondary endpoints (which are readonly)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13228</link><project id="" key="" /><description>&#8230;to ES master.

Enhancements as discussed on https://github.com/craigwi/elasticsearch-cloud-azure/releases/tag/v2.7.1-craigwi:

1.supports multiple storage accounts; cloud.azure.storage.account and cloud.azure.storage.key may now be an array of accounts / keys; the arrays must be the same length; the account at an index must match the key at the same index.

2.an Azure repository specification ("type" : "azure") allows for two new settings. "account" specifies the name of the account to be used and must be one of the items in cloud.azure.storage.account. If "account" is not specified, the first item in the list of accounts is used. The other new setting "location_mode" may be used to specify the endpoint. This defaults to "primary_only" and may also be "primary_then_secondary", "secondary_only" or "secondary_then_primary".

3.when a repository is registered using "secondary_only" or "secondary_then_primary" as the "location_mode", the verification of the repository is limited to checking that the container specified exists; in particular the tests-\* files are not created because the secondary endpoint is read only.

NOTE: for a given storage account, only one location_mode can be active at a time.

An example showing settings in elasticsearch.yml:

cloud.azure.storage.account: [ "azstorageaccount1", "mystorage2" ]
cloud.azure.storage.key: [ "", "" ]

A sample repository specification using the secondary endpoint:

{ "type": "azure", "settings": { "account" : "mystorage2", "container": "snapshots-20150701",
 "location_mode": "secondary_only"}}
</description><key id="104122380">13228</key><summary>For Azure, support multiple storage accounts and secondary endpoints (which are readonly)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">craigwi</reporter><labels><label>:Plugin Cloud Azure</label></labels><created>2015-08-31T19:52:14Z</created><updated>2015-10-27T14:45:28Z</updated><resolved>2015-10-27T14:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-01T10:18:20Z" id="136664382">Hi @craigwi.

Thank you for bringing that PR here.

As I wrote in https://github.com/elastic/elasticsearch-cloud-azure/pull/93#issuecomment-131810017, I think we should do it a bit differently unless I'm missing something.

Pasting the discussion here:

---

Note that you also raised a valid point which is that we need to support in  `elasticsearch.yml` multiple credentials.
We could imagine that as a generic feature whatever repository type you want to use.

Let say that we can now create something like:

``` yml
cloud:
    azure:
        storage:
            azure1:
              account: your_azure_storage_account1
              key: your_azure_storage_key1
              default: true
            azure2:
              account: your_azure_storage_account2
              key: your_azure_storage_key2
            azure3:
              account: your_azure_storage_account3
              key: your_azure_storage_key3
```

Then when we create the repo, we can specify which credentials we want to use:

```
# use credentials 2
PUT _snapshot/my_backup2
{
  "type": "azure",
  "settings": {
      "credentials": "azure2",
      "container": "backup_container",
      "base_path": "backups"
  }
}

# This one will use the one marked as "default"
PUT _snapshot/my_backup3
{
  "type": "azure"
}
```

---

I know that we need to make one of those repo `readonly`. The infra is [now ready](https://github.com/elastic/elasticsearch/pull/13144). We "just" have basically to expose it in azure plugin now.

I think that you'll be able to define with this something really similar with what you are trying to achieve here.

## elasticsearch.yml

Instead of:

``` yaml
cloud.azure.storage.account: [ "azstorageaccount1", "mystorage2" ]
cloud.azure.storage.key: [ "", "" ]
```

define:

``` yaml
cloud.azure.storage.azure1.account: "azstorageaccount1"
cloud.azure.storage.azure1.key: ""
cloud.azure.storage.azure2.account: "mystorage2"
cloud.azure.storage.azure2.key: ""
```

## Usage

Instead of:

``` json
PUT _snapshot/myrepo
{ 
  "type": "azure", 
  "settings": { 
     "account" : "mystorage2", 
     "container": "snapshots-20150701",
     "location_mode": "secondary_only"
   }
}
```

Define:

``` json
PUT _snapshot/myrepo
{ 
  "type": "azure", 
  "settings": { 
     "credentials" : "azure2", 
     "container": "snapshots-20150701",
     "readonly": true,
     "location_mode": "secondary_only"
   }
}
```

That said, I think we can easily auto detect that we are using secondary endpoint here so we automatically set `readonly` to `true` unless the user explicitly define it to `false`.

I'm also wondering if we should not prefer an easier setting like `use_secondary: true` instead of `location_mode: secondary_only` and `location_mode: primary_only`. Unless you are thinking of a future usage?

While reading the [Azure Storage Replication documentation](https://azure.microsoft.com/en-us/documentation/articles/storage-redundancy/#read-access-geo-redundant-storage), I was also wondering if we really need this flag?

&gt; When you enable read-only access to your data in the secondary region, your data is available on a secondary endpoint, in addition to the primary endpoint for your storage account. The secondary endpoint is similar to the primary endpoint, but appends the suffix `-secondary` to the account name. For example, if your primary endpoint for the Blob service is `myaccount.blob.core.windows.net`, then your secondary endpoint is `myaccount-secondary.blob.core.windows.net`. The access keys for your storage account are the same for both the primary and secondary endpoints.

If I understand it correctly, it means to me that the azure client basically adds `-secondary` in the endpoint when `location_mode` is `secondary_only`.

In that case, does something like the following would work?

``` yaml
cloud.azure.storage.azure1.account: "myaccount"
cloud.azure.storage.azure1.key: ""
cloud.azure.storage.azure2.account: "myaccount-secondary"
cloud.azure.storage.azure2.key: ""
```

``` json
PUT _snapshot/myrepo
{ 
  "type": "azure", 
  "settings": { 
     "credentials" : "azure2", 
     "container": "snapshots-20150701",
     "readonly": true
   }
}
```

I did not check. May be `myaccount-secondary` is a forbidden account name?

What do you think?

@imotov @skearns64 @ppf2 Feel free to add also your thoughts here!
</comment><comment author="dadoonet" created="2015-09-01T10:24:57Z" id="136666571">&gt; The access keys for your storage account are the same for both the primary and secondary endpoints.

I think this answers my last question. The end point is different but the account must be kept as is, right?
</comment><comment author="craigwi" created="2015-09-01T17:24:13Z" id="136802697">Hi David,
I did see your comments.  I ported my previous proposal was because it is a simple, tested and full featured solution and I wanted to start this discussion in the context of a pull request for the new location of the sources for the cloud-azure plugin. 
Taking the points in reverse order, the Azure client apis provide a flexible means of accessing primary and secondary storage endpoints.  There are four cases and my proposal supports them all.  The four cases: 
1.  Primary only
2.  Primary then secondary
3.  Secondary only 
4.  Secondary then primary

To support those combinations, which I see no reason not to support, one passes the account AND the location mode.  The client library uses the secondary endpoint as required.  That is, the concept in Azure is ONE storage account and key with the endpoints derived indirectly from the use case (and potentially other settings).

My conclusion on this: location_mode, as I have implemented, is the correct way in Azure to use primary and secondary endpoints.  The Java client library supports this and my proposed solution supports this.

Regarding the &#8220;readonly&#8221; support for repositories, I like the feature!  While secondary endpoints in Azure are NECESSARILY readonly, we should enable the use of a primary endpoint that it should be accessed readonly.  That is, the concept of location_mode and readonly-ness are mostly orthogonal.  As noted elsewhere, location mode cases #2, #3 and #4 above are implicitly readonly and should be treated as if &#8220;readonly&#8221;: true was set.

My conclusion on this: the new &#8220;readonly&#8221; setting does not eliminate the need for &#8220;location_mode&#8221;.

As for the configuration in yml, independent of the above points, I am fine with either the azure1, azure2 approach or the arrays approach.  I actually started with the approach you suggested, but found the array approach more like the rest of the settings in the yml file and super simple to implement.  It is clear that in general there might be lots of settings per storage account; cf. https://azure.microsoft.com/en-us/documentation/articles/storage-configure-connection-string.  However, it is extremely rare that one would setting them differently for different storage account in one deployment of ES.  Thus it would be reasonable to set, for example, the blob endpoint once for use with all storage accounts.   

My conclusion on this: either approach is fine.

Let me know what you think.

Craig.
</comment><comment author="skearns64" created="2015-09-01T20:51:20Z" id="136857288">@craigwi - I agree with just about all of your points. 

When it comes to the configuration in YML, I prefer the azure1, azure2 approach outlined by @dadoonet 
</comment><comment author="ppf2" created="2015-09-15T16:42:00Z" id="140458001">Hi Craig, spoke with @dadoonet and @skearns64 , we will take the PR from here and modify as needed.  Thx for the contribution!
</comment><comment author="craigwi" created="2015-09-15T16:59:15Z" id="140462629">Sounds good.  Thanks for letting me know.
</comment><comment author="dadoonet" created="2015-10-27T14:45:28Z" id="151525068">I'm closing this one in favor of #13779
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.ImmutableList</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13227</link><project id="" key="" /><description>This pull request removes and now forbids all uses of com.google.common.collect.ImmutableList across the codebase. This is another of the many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="104111779">13227</key><summary>Remove and forbid use of com.google.common.collect.ImmutableList</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-31T18:52:14Z</created><updated>2015-08-31T19:29:37Z</updated><resolved>2015-08-31T19:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-31T19:12:37Z" id="136471549">LGTM

Does what it says on the tin. I read the whole change but my eyes fuzzed over at some points. I'm reasonably sure its all quite correct.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix docs for position_increment_gap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13226</link><project id="" key="" /><description>Closes #13207
</description><key id="104110589">13226</key><summary>Fix docs for position_increment_gap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2015-08-31T18:46:29Z</created><updated>2015-09-01T13:11:35Z</updated><resolved>2015-09-01T13:11:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T07:06:40Z" id="136609092">LGTM
</comment><comment author="clintongormley" created="2015-09-01T12:06:29Z" id="136689134">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch can not response to head</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13225</link><project id="" key="" /><description>elasticsearch version is 1.4.4
http://XX.XX.XX.XX:9200/_status
500 Internal server error
Why? Is the version of the head too old? 
</description><key id="104082331">13225</key><summary>elasticsearch can not response to head</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Judyccb</reporter><labels /><created>2015-08-31T16:06:52Z</created><updated>2015-08-31T17:47:58Z</updated><resolved>2015-08-31T17:47:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-31T17:47:58Z" id="136442012">@Judyccb have a look at the exception in the logs (or in the response body).  Please ask questions like these in the forum
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Guava as a dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13224</link><project id="" key="" /><description>This is a list of the imports from Guava that can be found in core Elasticsearch today. Let's check these off as we go and reference the corresponding issues and pull requests. Most of these will be relatively straightforward with the most serious ones being replacing the caching layer. Those will probably require full issues and careful review cycles.
- [x] remove `import com.google.common.annotations.Beta;` fe46227db871b6d1af6333c21f4bc684d350dd97
- [x] remove `import com.google.common.base.Charsets;` #13533
- [x] remove `import com.google.common.base.Function;` #13533
- [x] remove `import com.google.common.base.Joiner;` #13572 
- [x] remove `import com.google.common.base.Objects;` #13355 
- [x] remove `import com.google.common.base.Preconditions;` #13540 
- [x] remove `import com.google.common.base.Predicate;` #13349 
- [x] remove `import com.google.common.base.Predicates;` #13349 
- [x] remove `import com.google.common.base.Strings;` a6ffe8f6d50e207b44c327c99cb304f20bb07027
- [x] remove `import com.google.common.base.Throwables;` #13409 
- [x] remove `import com.google.common.cache.*;` #13717, #13879  
- [x] remove `import com.google.common.cache.Cache;` #13717, #13879 
- [x] remove `import com.google.common.cache.CacheBuilder;` #13717, #13879 
- [x] remove `import com.google.common.cache.CacheLoader;` 30bd46ab2534e4366eb840e0b44b04fe4713bd19
- [x] remove `import com.google.common.cache.LoadingCache;` 30bd46ab2534e4366eb840e0b44b04fe4713bd19
- [x] remove `import com.google.common.cache.RemovalListener;` #13717, #13879 
- [x] remove `import com.google.common.cache.RemovalNotification;` #13717, #13879 
- [x] remove `import com.google.common.cache.Weigher;` #13717, #13879 
- [x] remove `import com.google.common.collect.*;`
- [x] remove `import com.google.common.collect.ArrayListMultimap;` #13562, 257f3e862f5cd0cd24777efe51bfae6d1d03fedf
- [x] remove `import com.google.common.collect.Collections2;` #13533
- [x] remove `import com.google.common.collect.FluentIterable;` #13349, 257f3e862f5cd0cd24777efe51bfae6d1d03fedf
- [x] remove `import com.google.common.collect.HashMultimap;` #13562, 257f3e862f5cd0cd24777efe51bfae6d1d03fedf 
- [x] remove `import com.google.common.collect.EvictingQueue;` #13903
- [x] remove `import com.google.common.collect.ImmutableCollection;` #13909
- [x] remove `import com.google.common.collect.ImmutableList;` #13227 
- [x] remove `import com.google.common.collect.ImmutableMap.Builder;` #13939 
- [x] remove `import com.google.common.collect.ImmutableMap;` #13762, #13939
- [x] remove `import com.google.common.collect.ImmutableSet;` #13724, #13754
- [x] remove `import com.google.common.collect.ImmutableSortedMap;` #13525 
- [x] remove `import com.google.common.collect.ImmutableSortedSet;` 65353b8a3289d942b391fd71bbf4ef4b693dfea5 
- [x] remove `import com.google.common.collect.Iterators;` #13916
- [x] remove `import com.google.common.collect.ForwardingSet;` #13720
- [x] remove `import com.google.common.collect.Iterables;` #13559
- [x] remove `import com.google.common.collect.Lists;` #13170 
- [x] remove `import com.google.common.collect.Maps;` #13438 
- [x] remove `import com.google.common.collect.Multimap;` #13562
- [x] remove `import com.google.common.collect.MultimapBuilder;` #13562
- [x] remove `import com.google.common.collect.ObjectArrays;` #13562
- [x] remove `import com.google.common.collect.Queues;` #13498 
- [x] remove `import com.google.common.collect.Sets;` #13463
- [x] remove `import com.google.common.collect.UnmodifiableIterator;` #13562
- [x] remove `import com.google.common.hash.HashCode;` #13907
- [x] remove `import com.google.common.hash.HashFunction;` #13907
- [x] remove `import com.google.common.hash.Hashing;` #13907
- [x] remove `import com.google.common.io.ByteStreams;` #13562
- [x] remove `import com.google.common.io.Files;` #13562, 257f3e862f5cd0cd24777efe51bfae6d1d03fedf
- [x] remove `import com.google.common.io.Resources;` #13908
- [x] remove `import com.google.common.jimfs.Configuration;` remain as test-only dependency
- [x] remove `import com.google.common.jimfs.Jimfs;` remain as test-only dependency
- [x] remove `import com.google.common.math.LongMath;` #13575 
- [x] remove `import com.google.common.net.InetAddresses;` #13905
- [x] remove `import com.google.common.primitives.Ints;` #13596
- [x] remove `import com.google.common.primitives.Longs;` #13562
- [x] remove `import com.google.common.util.concurrent.AtomicLongMap;` #13562
- [x] remove `import com.google.common.util.concurrent.Futures;` #13524
- [x] remove `import com.google.common.util.concurrent.ListenableFuture;` #13524
- [x] remove `import com.google.common.util.concurrent.MoreExecutors;` #13524
- [x] remove `import com.google.common.util.concurrent.SettableFuture;` #13524
- [x] remove `import com.google.common.util.concurrent.UncheckedExecutionException;` #13562 
- [x] remove `import static com.google.common.base.Preconditions.*;` #13540 
- [x] remove `import static com.google.common.base.Preconditions.checkArgument;` #13540 
- [x] remove `import static com.google.common.base.Preconditions.checkNotNull;` #13493 
- [x] remove `import static com.google.common.base.Preconditions.checkState;` #13540 
- [x] remove `import static com.google.common.base.Predicates.isNull;` #13349 
- [x] remove `import static com.google.common.collect.Lists.*;` #13170 
- [x] remove `import static com.google.common.collect.Lists.newArrayList;` #13170 
- [x] remove `import static com.google.common.collect.Lists.newArrayListWithCapacity;` #13170
- [x] remove `import static com.google.common.collect.Maps.filterEntries;` #13438 
- [x] remove `import static com.google.common.collect.Maps.newHashMap;` #13438 
- [x] remove `import static com.google.common.collect.Maps.newHashMapWithExpectedSize;` #13438 
- [x] remove `import static com.google.common.collect.Sets.newHashSet;` #13463 

Note that `jimfs` is in an artifact separate from `guava`, but depends on `guava` itself; it will remain as a test-only dependency.

The above was generated via:

```
ack --type java -h "import\s+(static\s+)?com\.google\.common" | sort | uniq | perl -p -e 's/^(.*)/- [ ] remove `$1`/g'
```
</description><key id="104079586">13224</key><summary>Remove Guava as a dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>Meta</label><label>v5.0.0-alpha1</label></labels><created>2015-08-31T15:50:15Z</created><updated>2015-10-10T12:53:25Z</updated><resolved>2015-10-09T19:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-31T15:59:59Z" id="136415042">I think some of these may be worth importing directly - `Splitter` and `Joiner` come to mind. They are cute and convenient. We should just be careful not to drift from Guava's source or upgrades will be difficult.

I think other of these - `newHashMap`, `newHashSet`, `filtering`, etc, are worth banning and removing entirely because they just aren't that useful. The `new*` methods in particular are pretty much obsolete with the diamond operator.

I guess what I'm getting at is that we should handle each of these in a case by case basic.
</comment><comment author="jasontedor" created="2015-08-31T19:10:25Z" id="136471068">Removing the v2.1.0 label; I suspect that some of these will stick around for convenience until we migrate to Java 8 and can replace uses of things like `Iterables.filter`/`Predicate` with `Stream.filter`/native `Predicate`.
</comment><comment author="xuzha" created="2015-09-03T07:57:38Z" id="137370669">Just came across to google.io tonight, not sure if @jasontedor or others has been working on it.
</comment><comment author="nik9000" created="2015-09-07T01:12:05Z" id="138149566">Can we talk about `com.google.common.base.Preconditions`? I kind of like it for times when you don't want an `assert`. I find its generally less clutter-y than lots of `if`s.
</comment><comment author="rjernst" created="2015-09-07T01:37:24Z" id="138153399">Do we need more than `checkArgument`? We can replace all the `checkNotNull` with `Objects.requireNonNull`. And `checkArgument` is a super simple method...doesn't seem worthwhile to keep an entire complex library around just for that.
</comment><comment author="jasontedor" created="2015-09-07T03:17:29Z" id="138168741">@nik9000:

&gt; Can we talk about com.google.common.base.Preconditions? I kind of like it for times when you don't want an assert. I find its generally less clutter-y than lots of ifs.

Have you seen the comment regarding the use of these method in the [Javadocs](http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/base/Preconditions.html#checkArgument%28boolean%29), specifically:

&gt; Remember that parameter values for message construction must all be computed eagerly, and autoboxing and varargs array creation may happen as well, even when the precondition check then succeeds (as it should almost always do in production). In some circumstances these wasted CPU cycles and allocations can add up to a real problem. Performance-sensitive precondition checks can always be converted to the customary form

Reducing object churn in Elasticsearch is immensely important because a garbage collection cycle is no different than a network partition from a failure perspective. Increasing the chance of long runtime latencies from excessive garbage collection cycles is detrimental to cluster health.

This is not to say that all uses of `checkArgument` and similar methods from `Preconditions` are problematic, but it is to raise awareness of an issue that these convenience methods can contribute to.
</comment><comment author="jasontedor" created="2015-09-07T03:17:41Z" id="138168796">@rjernst:

&gt; And `checkArgument` is a super simple method...doesn't seem worthwhile to keep an entire complex library around just for that.

+1
</comment><comment author="jasontedor" created="2015-09-07T03:27:10Z" id="138170494">This should not be taken as a judgement on Guava. Guava is a popular library, and for good reasons. But the rationale here is rather simple:
- All else equal, fewer dependencies is a positive.
- A lot of the benefits of Guava have been supplanted by additions to the JDK in JDK 7 and JDK 8 (e.g., the diamond operator, lambdas, and streams).
- We are no longer shading dependencies in the Elasticsearch jar (see #13252).
- We do not have a proper Java client today; this means that developers that depend on the Elasticsearch jar are likely going to have to deal with Guava version conflicts (which are likely since Guava is popular).
- Developers do embed Elasticsearch inside web containers which creates conflicts with the provided versions of Guava.
- Plugin developers encounter issues with Guava version conflicts (again, because Guava is popular).

In short, the internal benefits have been reduced, and there are external benefits to developers to removing Guava.
</comment><comment author="nik9000" created="2015-09-07T12:58:00Z" id="138291342">&gt; And checkArgument is a super simple method...doesn't seem worthwhile to keep an entire complex library around just for that.

I wasn't clear - I'm not advocating that. I'm advocating that we consider importing Preconditions, or at least portions of it rather than rewriting them all at the call sight.

&gt; Reducing object churn in Elasticsearch is immensely important because a garbage collection cycle is no different than a network partition from a failure perspective. Increasing the chance of long runtime latencies from excessive garbage collection cycles is detrimental to cluster health.

I like this argument much better. To me it goes like this:
1. We want to reduce our object allocation rate.
2. `checkArgument` and its ilk always attempt to make your code more readable at the cost of potentially more allocations.
3. This is often a fine tradeoff but for Elasticsearch its never what we want because of point 1.
4. Its worse because the places where it is a fine tradeoff aren't always obvious in Elasticsearch. Put another way, it can be difficult to tell when you are in a commonly called code path. So its never worth the risk for the small readability payoff.

I promise I won't freak out about all the parts of Guava. Its clearly fine to replace `Charsets.UTF_8` with `StandardCharsets.UTF_8` for example.
</comment><comment author="jasontedor" created="2015-09-07T13:31:10Z" id="138299634">@nik9000:

&gt; I like this argument much better. To me it goes like this:
&gt; 1. We want to reduce our object allocation rate.
&gt; 2. `checkArgument` and its ilk always attempt to make your code more readable at the cost of potentially more allocations.
&gt; 3. This is often a fine tradeoff but for Elasticsearch its never what we want because of point 1.
&gt; 4. Its worse because the places where it is a fine tradeoff aren't always obvious in Elasticsearch. Put another way, it can be difficult to tell when you are in a commonly called code path. So its never worth the risk for the small readability payoff.

Exactly. Tangentially, another place to be aware of this is calls to `logger.(debug|error|info|trace|warn)`; any invocations of these that are allocating strings (e.g., via concatenation or calls to `Object#toString`) to build the method parameters whether or not the corresponding log level is turned on contribute to object churn. In cases where there are allocations occurring, it's best to wrap the calls in the corresponding check against the `logger.is(Debug|Error|Info|Trace|Warn)Enabled` method. I've seen exactly this problem wreak havoc on HBase region server heaps.
</comment><comment author="nik9000" created="2015-09-24T12:29:42Z" id="142914616">I squashed the incomplete items together in the list to make the remaining work easier to read.
</comment><comment author="nik9000" created="2015-10-02T21:06:56Z" id="145155429">It looks like the two remaining ones are claimed by me. I got a good start to them on the plane and will push something reviewable early next week. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport remaining packaging tests work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13223</link><project id="" key="" /><description>This backports the packaging test work that I did in master and hadn't yet backported to 2.0.
</description><key id="104074738">13223</key><summary>Backport remaining packaging tests work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T15:21:43Z</created><updated>2015-09-14T17:14:29Z</updated><resolved>2015-08-31T15:42:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-31T15:40:13Z" id="136410208">Tests are passing for me locally with these patches which is a good thing!
</comment><comment author="tlrx" created="2015-08-31T15:40:36Z" id="136410304">@nik9000 Changes also looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide the plugins to transport client communicating with the the external cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13222</link><project id="" key="" /><description /><key id="104070607">13222</key><summary>Provide the plugins to transport client communicating with the the external cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>review</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-31T14:59:41Z</created><updated>2015-08-31T15:06:53Z</updated><resolved>2015-08-31T15:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-31T15:02:07Z" id="136399392">LGTM
</comment><comment author="jaymode" created="2015-08-31T15:02:36Z" id="136399495">LGTM
</comment><comment author="nik9000" created="2015-08-31T15:02:55Z" id="136399575">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock vagrant to virtualbox</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13221</link><project id="" key="" /><description>Virtualbox is the default virtualization provier for vagrant but folks
override that from time to time. If they do then the build will fail because
the boxes used by the build don't usually support non-virtualbox providers.

Closes #13217
</description><key id="104068564">13221</key><summary>Lock vagrant to virtualbox</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T14:48:46Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T14:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-31T14:49:04Z" id="136395655">Ping @drewr  and @dakrone who brought this up.
</comment><comment author="drewr" created="2015-08-31T14:53:20Z" id="136396611">LGTM :ship: 
</comment><comment author="dakrone" created="2015-08-31T14:54:18Z" id="136396817">LGTM as well
</comment><comment author="nik9000" created="2015-08-31T15:01:41Z" id="136399316">Merged into master and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch running query string query on multiple fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13220</link><project id="" key="" /><description>I have a mapping which consists of a parent child relationship between documents and fields. I want to write a query that matches the parent using the query string query only if 2 fields match the query. 

Quoting from the documentation on Multi Field (https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_multi_field_2)

Is there any way to do a search using the query string query in this manner and still return results :
{
    "query_string": {
      "query": "(content:this AND content:that) AND (value:that AND value:this)"
    }
}
</description><key id="104066683">13220</key><summary>ElasticSearch running query string query on multiple fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet85</reporter><labels /><created>2015-08-31T14:39:11Z</created><updated>2015-08-31T17:44:19Z</updated><resolved>2015-08-31T17:44:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-31T17:44:19Z" id="136441189">Hi @vineet85 

Please ask questions like these in the forum instead. https://discuss.elastic.co/

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add note about multi data path and disk threshold deciders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13219</link><project id="" key="" /><description>Prior to 2.0 we summed up the available space on all disk on a node
due to the raid-0 like behavior. Now we don't do this anymore and use the
min &amp; max disk space to make decisions.

Closes #13106
</description><key id="104058464">13219</key><summary>Add note about multi data path and disk threshold deciders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T13:57:12Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T14:25:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-31T14:09:10Z" id="136381781">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add default impl for resolveIndex()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13218</link><project id="" key="" /><description>spin off from https://github.com/elastic/elasticsearch/pull/13068#discussion_r37846771
</description><key id="104057567">13218</key><summary>add default impl for resolveIndex()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T13:51:47Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T13:53:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-31T13:52:14Z" id="136377422">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bats tests should force the virtualbox provider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13217</link><project id="" key="" /><description>The vagrant tests use vagrant boxes that are only available with virtualbox but don't specify it so people that have configured vagrant to use some other virtualizer by default will get failures. They should just force virtualbox.
</description><key id="104057313">13217</key><summary>bats tests should force the virtualbox provider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T13:50:19Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-08-31T14:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>improved backwards compatibility tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13216</link><project id="" key="" /><description>Now that 2.0beta1 is released, we could try to prototype some more realistic backwards compatibility testing.

Some scheme like this might work:
- qa/backwards-20beta1-rest: starts up bin/elasticsearch from current code, but runs the packaged rest suite from 2.0 beta1 against it (it is included in that test jar already). So its similar to what we do in distribution tests, just with different test dependencies.
- qa/backwards-20beta1-multinode: starts up bin/elasticsearch from current code, then starts up bin/elasticsearch from 2.0-beta1 code and verifies they can join a cluster. So its similar to smoke-test-multinode, with different integ depedencies.
</description><key id="104056076">13216</key><summary>improved backwards compatibility tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-08-31T13:42:31Z</created><updated>2015-10-30T17:44:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-31T13:44:55Z" id="136375780">++

I wonder if we can have a policy to prevent combinatorial explosion?
</comment><comment author="rmuir" created="2015-08-31T13:46:28Z" id="136376074">What do you mean by a policy? I don't see combinatorial explosion... this would be 2 scenarios for each version. Whatever versions we are supposed to be compatible with, I think we should test.
</comment><comment author="s1monw" created="2015-08-31T13:50:58Z" id="136377181">++
</comment><comment author="nik9000" created="2015-08-31T13:56:09Z" id="136378307">&gt; What do you mean by a policy? I don't see combinatorial explosion... this would be 2 scenarios for each version. Whatever versions we are supposed to be compatible with, I think we should test.

I could see us doing:
- 2.0.0-beta1 + current
- 2.0.0 + current
- 2.0.0-beta1 + 2.0.0 + current
- 2.0.1 + current
- 2.0.0 + 2.0.1 + current
- 2.0.0-beta1 + 2.0.0 + 2.0.1 + current

And that list getting very long very quick. If we stick to just having two different versions at a time that is "policy" enough for me. Though we might want to have one rainbow test where we do all the versions or something horrible like that.
</comment><comment author="rmuir" created="2015-08-31T13:57:29Z" id="136378648">&gt; 2.0.0-beta1 + 2.0.0 + current

why would we do that? I don't think we need to support a cluster with a mix of 3 versions. What is the use case for that?

I only see this:
- 2.0.0-beta1 -&gt; current
- 2.0.0 -&gt; current
- 2.0.1 -&gt; current
</comment><comment author="nik9000" created="2015-08-31T14:02:23Z" id="136379828">&gt; why would we do that? I don't think we need to support a cluster with a mix of 3 versions. What is the use case for that?

Its mostly a mistake to use three versions at once. People do it. I've heard of it happening in scenarios where people are using the java client and when people don't upgrade their dedicated master nodes properly.

I do get your point about only testing the two versions at once. You still get a lot of test cases but its not so bad as I was envisioning.
</comment><comment author="rmuir" created="2015-08-31T14:15:54Z" id="136383267">&gt; I do get your point about only testing the two versions at once. You still get a lot of test cases but its not so bad as I was envisioning.

You can even reduce this by just doing previous -&gt; current. Its less thorough but better than nothing.
</comment><comment author="nik9000" created="2015-08-31T14:17:17Z" id="136383587">&gt; You can even reduce this by just doing previous -&gt; current. Its less thorough but better than nothing.

I could see that as the normal thing to run before sending a pull request and the wider range being normal to run nightly or something.
</comment><comment author="nik9000" created="2015-10-30T17:44:32Z" id="152598848">So #13477 makes a start on this - it moves the dependency resolution to maven and sets up a reasonably simple way to add new versions and things like that. It doesn't do the following things that we really should do:
- Launch all elasticsearch using `bin/elasticsearch` - no internal nodes at all.
- Run the rest tests against the mixed version cluster.
- Other stuff I haven't thought about much like running rest tests during a rolling restart.

The backwards compatibility tests as they stand now do the following:
- UnicastBackwardsCompatibilityIT forms a mixed version cluster and connect to it using the TransportClient and counts the data nodes to make sure they are all there as expected. This is the most basic backwards compatibility test.
- BasicBackwardsCompatibilityIT does some indexing, gets, deletes, searches, verifies that indexes can recover from older version to newer versions but not the other way around, rolling upgrades, and a whole bunch of other stuff. 
- Tests like NodesStatsBasicBackwardsCompatIT run the stats commands on a mixed version cluster and verify that what comes back makes sense. These stats APIs can change a lot in a major release so these are pretty important.

The tests as they stand now are import and useful even if they could do with some love to make them more real.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow tests to override whether mock modules are used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13215</link><project id="" key="" /><description /><key id="104055443">13215</key><summary>Allow tests to override whether mock modules are used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>test</label><label>v2.1.0</label></labels><created>2015-08-31T13:39:07Z</created><updated>2015-08-31T19:11:34Z</updated><resolved>2015-08-31T19:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-31T17:16:14Z" id="136432837">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Behaviour different with custom _all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13214</link><project id="" key="" /><description>With the following simple query, I get different results with a custom_all to the other cases:

``` json
GET /test*/_search
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "*"
        }
      }
    }
  }
}
```

| test | description | result count |
| --- | --- | --- |
| test1 | custom _all | 1 |
| test2 | disabled_all | 2 |
| test3 | include_in_all | 2 |
| test4 | normal | 2 |

I thought this might be because one of the documents doesn't add anything to the custom _all field, but neither does one of the documents in the include_in_all example, yet that still returns both documents. I assume when _all is disabled the query is effectively converted to match_all. 
## test1: custom _all

``` bash
curl -XDELETE localhost:9200/test1
curl -XPUT localhost:9200/test1/ -d '
{
  "settings" : {
    "index" : {
      "query.default_field" : "_myall"
    }
  },
  "mappings" : {
    "test" : {
      "_all" : {
        "enabled" : false
      },
      "properties" : {
        "_myall" : {
          "type" : "string",
          "index" : "not_analyzed"
        },
        "test" : {
          "type" : "long",
          "copy_to" : "_myall"
        }
      }
    }
  }
}'
curl -XPOST localhost:9200/test1/test -d '{ "myid" : 4 }'
curl -XPOST localhost:9200/test1/test -d '{ "test" : 4 }'
```
## test2: disabled _all

``` bash
curl -XDELETE localhost:9200/test2
curl -XPUT localhost:9200/test2/ -d '
{
  "mappings" : {
    "test" : {
      "_all" : {
        "enabled" : false
      }
    }
  }
}'
curl -XPOST localhost:9200/test2/test -d '{ "myid" : 4 }'
curl -XPOST localhost:9200/test2/test -d '{ "test" : 4 }'
```
## test3: include_in_all

``` bash
curl -XDELETE localhost:9200/test3
curl -XPUT localhost:9200/test3/ -d '
{
  "mappings" : {
    "test" : {
      "properties" : {
        "myid" : {
          "type" : "long",
          "include_in_all" : true
        },
        "test" : {
          "type" : "long",
          "include_in_all" : false
        }
      }
    }
  }
}'
curl -XPOST localhost:9200/test3/test -d '{ "myid" : 4 }'
curl -XPOST localhost:9200/test3/test -d '{ "test" : 4 }'
```
## test4: default

``` bash
curl -XDELETE localhost:9200/test4
curl -XPUT localhost:9200/test4/ -d '
{
  "mappings" : {
    "test" : {
      "properties" : {
        "myid" : {
          "type" : "long"
        },
        "test" : {
          "type" : "long"
        }
      }
    }
  }
}'
curl -XPOST localhost:9200/test4/test -d '{ "myid" : 4 }'
curl -XPOST localhost:9200/test4/test -d '{ "test" : 4 }'
```
</description><key id="104047171">13214</key><summary>Behaviour different with custom _all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-08-31T12:46:44Z</created><updated>2015-09-04T09:53:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-31T17:13:44Z" id="136432210">Hmmm, interesting finding... The reason for it is that `_all:*` and `foo:*` get rewritten as different queries:

```
GET /test*/_validate/query?explain
{
  "query": {
    "query_string": {
      "query": "*",
      "default_field": "_all"
    }
  }
}
```

returns:

```
"explanation": "ConstantScore(*:*)"
```

while:

```
GET /test*/_validate/query?explain
{
  "query": {
    "query_string": {
      "query": "*",
      "default_field": "foo"
    }
  }
}
```

returns:

```
"explanation": "ConstantScore(cache(BooleanFilter(_field_names:foo)))"
```

The latter translates to "return all documents that have any value in field `foo`".

For this query `*` it seems right to match all documents. For the `foo:*` query, it feels right to return documents that have a value in field `foo`.  However, in your example with query `*` which is rewritten to `foo:*`, we're doing the unexpected... Perhaps the rule should be: "if a wildcard is specified without a field: prefix, then match all docs"
</comment><comment author="jimmyjones2" created="2015-08-31T18:40:36Z" id="136459462">Thanks @clintongormley. I originally found the issue when using Kibana, as when viewing all documents in Discover, I couldn't see all the documents!
</comment><comment author="clintongormley" created="2015-09-04T09:53:17Z" id="137693164">Discussed in FixItFriday, we agree that:
- `"*"` always means match all documents
- `"field:*"` means find all documents that contain a non-null value in the field (incl empty string)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changing index settings during restore doesn't work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13213</link><project id="" key="" /><description>Suppose we have a index with extra index_settings "read_only: false".

``` json
# curl -XPUT "localhost:9200/kibana/_settings?pretty" -d '{
     "index" : {
       "blocks" : {
         "read_only": false
      }
    }
}'
{
  "acknowledged" : true
}
```

Need to close the index to restore it.

``` json
# curl -XPOST 'localhost:9200/kibana/_close?pretty'
{
  "acknowledged" : true
}
```

Changing index settings during restore: settings index.blocks.read_only: true"

``` json
# curl -XPOST "localhost:9200/_snapshot/backup/kibana/_restore?pretty" -d '{
    "indices": "kibana_admin",
    "include_global_state": false,
    "rename_pattern": "(.+)",
    "rename_replacement": "kibana",
    "index_settings": {
       "index.blocks.read_only": true
    }
}'
{
  "accepted" : true
}
```

it looks it's restored including the extra index_settings settings "read_only: true"

``` json
# curl localhost:9200/kibana/_settings?pretty
{
  "kibana" : {
    "settings" : {
      "index" : {
        "creation_date" : "1430332356566",
        "uuid" : "4GxOkG65Tx-GqN7H9jnuNA",
        "blocks" : {
          "read_only" : "true"
        },
        "number_of_replicas" : "1",
        "number_of_shards" : "1",
        "version" : {
          "created" : "1050199"
        }
      }
    }
  }
}
```
## But, it's not read only!!!

``` json
# curl -XPOST 'localhost:9200/kibana/_open?pretty'
{
  "acknowledged" : true
}
```

It it possible to make it read-only?

``` json
# curl -XPUT "localhost:9200/kibana/_settings?pretty" -d '{
     "index" : {
       "blocks" : {
         "read_only": true
      }
    }
}'
{
  "acknowledged" : true
}
```

Yes

``` json
# curl -XPOST 'localhost:9200/kibana/_close?pretty'
{
  "error" : "ClusterBlockException[blocked by: [FORBIDDEN/5/index read-only (api)];]",
  "status" : 403
}
```

``` json
# curl localhost:9200/kibana/_settings?pretty
{
  "kibana" : {
    "settings" : {
      "index" : {
        "creation_date" : "1430332356566",
        "uuid" : "4GxOkG65Tx-GqN7H9jnuNA",
        "blocks" : {
          "read_only" : "true"
        },
        "number_of_replicas" : "1",
        "number_of_shards" : "1",
        "version" : {
          "created" : "1050199"
        }
      }
    }
  }
}
```
</description><key id="104046585">13213</key><summary>Changing index settings during restore doesn't work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">JohanHolman</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-08-31T12:43:03Z</created><updated>2015-10-13T12:18:35Z</updated><resolved>2015-10-13T12:18:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-10-13T12:18:35Z" id="147698708">Thanks for reporting this! It is now fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow plugins to be specified using NodeBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13212</link><project id="" key="" /><description>With the removal of `plugin.types` in #13055, there is no longer a way to specify plugins to load for a node client other than creating a plugins directory and putting all of the plugins in that directory. We should add a `addPlugin` method to the `NodeBuilder` class so that plugins can be specified by their class.
</description><key id="104045862">13212</key><summary>Allow plugins to be specified using NodeBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Plugins</label><label>discuss</label></labels><created>2015-08-31T12:37:27Z</created><updated>2015-12-03T17:40:58Z</updated><resolved>2015-12-03T17:40:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-08-31T13:16:27Z" id="136368233">Agreed, seems like we missed this? @rjernst I think we should?
</comment><comment author="clintongormley" created="2015-09-04T09:59:23Z" id="137694070">@kimchy @rjernst do we have a plan for this yet?
</comment><comment author="s1monw" created="2015-11-27T12:46:22Z" id="160132473">@jaymode is this still relevant with the test framework?
</comment><comment author="jaymode" created="2015-11-30T11:25:54Z" id="160604538">The test framework `MockNode` works for all cases as far as I know, except for testing plugins with a tribe node. The tribe service isn't able to pass down the plugins to the tribe nodes. The workaround for testing plugins with tribe nodes is to use `IT` style integration test with multiple external clusters.
</comment><comment author="joschi" created="2015-11-30T14:06:33Z" id="160640067">It would be nice to (re-) introduce the possibility to programmatically add classpath plugins to an Elasticsearch node. My use case is running an embedded client node in a Java application which should load a plugin to listen to cluster state changes (e. g. nodes added/removed, indices added/removed) and share this information with the embedding application.
</comment><comment author="clintongormley" created="2015-12-03T17:40:58Z" id="161727046">Closing.  See discussion in #15107
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reenable spaces in paths during integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13211</link><project id="" key="" /><description>Reenable running integration tests with spaces in paths now that https://github.com/elastic/elasticsearch/commit/a80317c4b39df375d869093ea5a793a74913b66e lives on both `master` and `2.0`

cc @rjernst @clintongormley 

Close #12848 
</description><key id="104039206">13211</key><summary>Reenable spaces in paths during integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T11:52:31Z</created><updated>2016-03-10T18:13:57Z</updated><resolved>2015-09-11T08:35:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T07:09:38Z" id="136609623">LGTM
</comment><comment author="s1monw" created="2015-09-01T07:12:00Z" id="136610098">lets do it!!
</comment><comment author="rmuir" created="2015-09-10T21:27:04Z" id="139384309">Can we push this? This is useful to detect bad stuff in build logic too :)
</comment><comment author="Mpdreamz" created="2015-09-11T08:38:08Z" id="139486017">merged to master/2.0/2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding path to path.data loses shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13210</link><project id="" key="" /><description>With a simple one node Elasticsearch 1.7.1 instance, with a few indicies, I added another directory to path.data. When I restarted Elasticsearch, a number of the indicies were unavailable. Is this expected behaviour?
</description><key id="104031376">13210</key><summary>Adding path to path.data loses shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Cluster</label><label>bug</label><label>discuss</label></labels><created>2015-08-31T10:59:40Z</created><updated>2016-01-13T03:06:08Z</updated><resolved>2015-09-04T10:03:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-31T12:08:39Z" id="136353406">Hmmm no it is not.  I managed to replicate this by creating 4 indices, then shutting down, adding a new data path and starting. One index wasn't assigned properly.  Restarting fixed the issue.

note: now is not a good time to switch to multi data paths as the implementation is changing completely in 2.0: a single shard will no longer be striped across multiple data paths, instead one shard will be written only to one path.
</comment><comment author="clintongormley" created="2015-09-04T10:03:50Z" id="137694722">Tested this out in master and it works correctly.  Given that this config change (after creating indices) is probably not usual, and there is a workaround, i'm going to close this as won't fix.
</comment><comment author="clintongormley" created="2015-09-04T10:04:24Z" id="137694792">(by master i meant 2.0 and above)
</comment><comment author="haochun" created="2016-01-13T03:06:08Z" id="171146259">@clintongormley @jimmyjones2 i have a encountered the same problem now.i have two shards on one node,but,i found i have no enough space to _optimize.Then, i add a diver to the machine,and add a path to path.data in elasticsearch.yml.But,two shards are UNASSIGNED when i restart elasticsearch.My elasticsearch version is 1.7.3.How can i solve this problem?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improvement of package/artifact deployment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13209</link><project id="" key="" /><description>We need to work on improving our artifact deployments. Collected several thoughts about this
- Should the RPM be signed or not as part of our deployment. If the RPM is not signed, older RPM based distributions will not have a problem to install. As we do sign our artifacts now as part of the mvn deployment (which adds an .asc file), this should not be a problem in terms of integrity. @electrical might chime in here, if we can drop this approach. If we need two artifacts, I am personally fine with having the unsigned one in the sonatype repo (which is also the one linked on elastic.co) and sign the RPM before putting it in the rpm repository (using the tools mentioned in the next bullet point)
- Smoothen out our RPM/DEB deployment process. There are tools like [deb-s3](https://github.com/krobertson/deb-s3) and [rpm-s3](https://github.com/crohr/rpm-s3), which dont require us to create all the additional meta files manually, but just push the package using that tool.
- Verify staging repository (the sonatype one), that it includes all the files... maybe this can be done by comparing it to the local mvn repo via a small script, that just checks via HTTP HEAD if the file exists (no need to check checksums, we should assume so things work)
- Verify signedness/unsignedness of artifacts on sonatype and the package repositories
</description><key id="104011016">13209</key><summary>Improvement of package/artifact deployment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>blocker</label><label>build</label><label>Meta</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T08:38:43Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-09-14T13:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-31T15:58:44Z" id="136414740">Took a first look at `deb-s3`. Like it so far. All you need is an unsigned debian package, specify this debian package on the command line, which upload its automatically, and then asks for the passphrase to sign the `Release` file. No need for complex syncing like done in our scripts right now.

Uploading works like this

```
deb-s3 upload --preserve-versions elasticsearch-2.0.0-beta1.deb -b $BUCKET --prefix alexr-repo-test/elasticsearch/deb --sign $GPG_KEY_ID --arch amd64
```

There are a couple of commands for verifying:

```
# deb-s3 show elasticsearch 1.7.1 amd64 -b $BUCKET --prefix alexr-repo-test/elasticsearch/deb
...
[lists packages contents]
...

# deb-s3 list -b $BUCKET --prefix alexr-repo-test/elasticsearch/deb
elasticsearch  1.7.1  all

# deb-s3 verify -b $BUCKET --prefix alexr-repo-test/elasticsearch/deb
&gt;&gt; Retrieving existing manifests
&gt;&gt; Checking for missing packages in: stable/main amd64
&gt;&gt; Checking for missing packages in: stable/main i386
```

Couple of things I tested
- replacing existing debian packages
- keeping the old packages in place, so people can do `apt-get install elasticsearch=X.Y.Z`
- upgrading debian packages after reinstalling

Installation works like this, as it is a gem. You need `libz-dev` installed for the `nokogiri` package

```
# install
GEM_PATH=/path/deb-s3 GEM_HOME=/path/deb-s3/ gem install deb-s3
# run, no need to specify paths it seems on run
/path/deb-s3/bin/deb-s3
```

Next stop: `rpm-s3`
</comment><comment author="electrical" created="2015-08-31T17:52:42Z" id="136443242">Important thing is that both the repo it self and the packages are signed and that we also are able to have multiple versions of ES in the repo and available ( most tools don't allow for that )

As a side note; we will have the need soon for 2 different signed rpm's. one with the V3 signature ( centos5 / SLES ) and one with V4 signature ( Centos6+, opensuse, etc )
This will also need to be a separate repo ( centos5 ipv centos; for example )
</comment><comment author="jordansissel" created="2015-08-31T18:07:08Z" id="136448990">@spinscale regarding apt/yum facilities, @drewr and I have been looking (as time permits) at some hosted repository options that may meet our needs (packagecloud, etc). I'd be cool if we used the same solution for all our hosting, but I would consider "where are the rpm/debs posted?" a separate problem from how the packages are constructed.
</comment><comment author="spinscale" created="2015-09-01T16:06:55Z" id="136774443">@jordansissel absolutely, I think package construction will be hard to unify across projects, and at best switching the underlying repository should only replace a single call to a tool for each package. We can still go with the above tools and switch whenever needed to push things to packagecloud or whatever we opt for
</comment><comment author="spinscale" created="2015-09-01T16:20:59Z" id="136780307">Spent some time testing [rpm-s3](https://github.com/crohr/rpm-s3) today

## Installation
- Prerequisites: `yum`, `deltarpm` and `boto`: `apt-get install yum python-deltarpm python-boto`
- Clone the github repo: `git clone https://github.com/crohr/rpm-s3 --recurse-submodules`
- Set env vars: `export AWS_ACCESS_KEY="key"` and `export AWS_SECRET_KEY="secret"`
- Change your `.boto` file (see issues with buckets with dots https://github.com/boto/boto/issues/2836) to this:

```
[s3]
calling_format = boto.s3.connection.OrdinaryCallingFormat
```
- Set your `~/.rpmmacros` file to be accordingly with your GPG key

```
%_signature gpg
%_gpg_name Test Example
```

Dont forget to import your key on the redhat test system before proceeding

```
rpm --import http://$BUCKET.s3.amazonaws.com/YOUR-KEY.gpg
```

```
rpm-s3/bin/rpm-s3 -v -b $BUCKET -p $PREFIX --sign elasticsearch-1.7.1.noarch.rpm --visibility public-read -k 0
rpm-s3/bin/rpm-s3 -v -b $BUCKET -p $PREFIX --sign elasticsearch-2.0.0-beta1.rpm --visibility public-read -k 0
```

Short explanation: Bucket and prefix define the path of the rpm, `visibility` must be `public-read` to make the uploads readable by anyone, `-k 0` instructs `rpm-s3` to not delete old packages, but just incrementally add them. Default is to remove packages older than two releases.

## Signing

Verifying the signature can be done via `rpm --checksig -v elasticsearch-1.7.1.noarch.rpm`

Manual signing can be done via `rpm --addsign elasticsearch-1.7.1.noarch.rpm`. Needs to be done currently as the built-in signing seems not to work

## Testing

Create a repo file in `/etc/yum.repos.d/elasticsearch.repo` 

```
[elasticsearch]
name=Elasticsearch repository for RPM packages
baseurl=http://s3.amazonaws.com/$BUCKET/$PREFIX
gpgcheck=1
gpgkey=http://$BUCKET.s3.amazonaws.com/$PREFIX/PUBKEY.gpg
enabled=1
metadata_expire=30
```

The expiration of metadata is just needed for testing as otherwise `yum update` will cache and reuse the local version for six hours.

We could create the above repo file as part of the deployment process, so automated tools can test the repository.

## Summary
- Pro: You only need the signed RPM and thats it, the tool handles the rest
- Pro: Adding new RPMs means just uploading them, thats it. No manual download and reconstruction of the repo structure like we do in the current shell script
- Contra: `rpm --addsign elasticsearch-1.7.1.noarch.rpm` needs to be called manually right now, looks like a bug, filed an issue

Next step: verify completeness of maven staging repo
</comment><comment author="spinscale" created="2015-09-02T10:01:18Z" id="137010448">Worked on a python script, that is using the local mvn repository and compares its contents to a remote destination (like a staging repo)

Script is doing the following steps
- Scans the local maven repo for all files in /org/elasticsearch
- Opens a HTTP connection to the staging repo
- Executes a HEAD request for each file found in step one
- Compares the content-length response header with the real file size
- Return an error if those two numbers differ

Output looks like this:

![Script while running](https://s3.amazonaws.com/uploads.hipchat.com/29251/250169/ZfCor5NQ00atVzF/output.gif)

This should fix the problem of incomplete deployments
</comment><comment author="nik9000" created="2015-09-02T13:31:14Z" id="137079674">I'm about to pick up #13182 and this looks related. At least the signing part. I haven't read all of your notes yet but maybe we should talk some?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Ensure binding on localhost host is consistently ipv4/v6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13208</link><project id="" key="" /><description>The current netty multiport tests bind on localhost and then try to connect
to 127.0.0.1, which may fail, if localhost is resolved to ipv6 by default.

This randomly chooses between 127.0.0.1, localhost and ::1 (if available) for
binding and then uses this throughout the test.

The `NetworkUtils` check for ipv6 is deprecated but lists no alternative, so maybe I havent found the right way to do it.
</description><key id="104000563">13208</key><summary>Tests: Ensure binding on localhost host is consistently ipv4/v6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T07:19:02Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T09:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-31T07:48:59Z" id="136290816">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Position-increment-gap doc need to be updated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13207</link><project id="" key="" /><description>After we changed default `position_increment_gap` to 100, the example below in this [doc](https://github.com/elastic/elasticsearch/blob/master/docs/reference/mapping/params/position-increment-gap.asciidoc) is outdated.

```
PUT /my_index/groups/1
{
    "names": [ "John Abraham", "Lincoln Smith"]
}

GET /my_index/groups/_search
{
    "query": {
        "match_phrase": {
            "names": "Abraham Lincoln" (1)
        }
    }
}
```
</description><key id="103995821">13207</key><summary>Position-increment-gap doc need to be updated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">xuzha</reporter><labels><label>docs</label></labels><created>2015-08-31T06:27:49Z</created><updated>2015-09-01T13:11:28Z</updated><resolved>2015-09-01T13:11:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-31T12:48:58Z" id="136361957">@clintongormley did you start working on this?
</comment><comment author="clintongormley" created="2015-08-31T17:35:35Z" id="136438335">@nik9000 not yet :) just added to my list 
</comment><comment author="nik9000" created="2015-08-31T17:37:24Z" id="136438751">&gt; @nik9000 not yet :) just added to my list

I'm happy to grab it if you want.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix numerous checks for equality and compatibility in mapper field types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13206</link><project id="" key="" /><description>The field type tests for mappings had a huge hole: check compatibility
was not tested directly at all! I had meant for this to happen in a
follow up after #8871, and was relying on existing mapping tests.
However, there were a number of issues.

This change reworks the fieldtype tests to be able to check all settable
properties on a field type work with checkCompatibility. It fixes a
handful of small bugs in various field types. In particular, analyzer
comparison was just wrong: it was comparing reference equality for
search analyzer instead of the analyzer name. There was also no check
for search quote analyzer.

closes #13112
</description><key id="103994255">13206</key><summary>Fix numerous checks for equality and compatibility in mapper field types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T06:06:16Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-09-01T18:44:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T07:16:51Z" id="136610953">I left one question about the equality checks but I don't have better ideas... This is good progress compared to what we have in master though so feel free to push even if you don't have ideas how to improve it either.
</comment><comment author="rjernst" created="2015-09-02T16:11:06Z" id="137147417">I will backport this to 2.0 as well, but am waiting on the backport of some geo changes that will make the backport clean.
</comment><comment author="rjernst" created="2015-09-08T21:07:30Z" id="138699985">Here is the 2.0 commit: 9e10e88
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert upgrade action to broadcast by node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13205</link><project id="" key="" /><description>Several shard-level operations that previously broadcasted a request
per shard were converted to broadcast a request per node. This commit
converts upgrade action to this new model as well.

Closes #13204
</description><key id="103966555">13205</key><summary>Convert upgrade action to broadcast by node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T00:44:43Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T10:02:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-31T06:32:31Z" id="136279691">Thx @jasontedor . Left one minor comment.
</comment><comment author="bleskes" created="2015-08-31T10:01:45Z" id="136321520">LGTM
</comment><comment author="jasontedor" created="2015-08-31T10:13:54Z" id="136323027">Thanks for reviewing @bleskes. I've [merged this to master](https://github.com/elastic/elasticsearch/commit/6e2dc7302366e9af861efa23b850999cd83748aa) and [integrated it into to 2.0](https://github.com/elastic/elasticsearch/commit/832a73cc9826caa6747ba33f4dbb00f822bc8e4c).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert upgrade action to broadcast by node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13204</link><project id="" key="" /><description>Several shard-level operations that previously broadcasted a request per shard were converted to broadcast a request per node in #12944. Upgrade action should be converted to this new model as well.

Relates #7990, #12944
</description><key id="103966067">13204</key><summary>Convert upgrade action to broadcast by node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-31T00:32:33Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-08-31T10:02:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add listeners for postIndex, postCreate, and postDelete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13203</link><project id="" key="" /><description>Closes #13202
</description><key id="103940241">13203</key><summary>Add listeners for postIndex, postCreate, and postDelete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kiryam</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-30T18:57:55Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-09-02T20:52:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-30T19:05:32Z" id="136175321">this looks good though - can we maybe also do this for `postCreate` and can you add a small unittest for it? I also wonder if you could go ahead an sign the CLA so we can pull this in?
</comment><comment author="kiryam" created="2015-08-30T19:37:37Z" id="136177505">CLA Done. Try to do test and postCreate tomorow
</comment><comment author="s1monw" created="2015-08-31T07:23:09Z" id="136286491">&gt; CLA Done. Try to do test and postCreate tomorow

cool - I just looked at it and it seems we don't have the listerener part for `postDelete` etc either... woudl be awesome to have it. Regarding the test - I'd look at `org.elasticsearch.index.shard.IndexShardTests` is should give you all the tools you need. The test doesn't need to be complicated just make sure we call the listener on failure. if you need assistance I am here to help!
</comment><comment author="kiryam" created="2015-09-01T07:57:17Z" id="136620669">Now I concentrate on test. And got few question. Is test should index document and catch is listener was called? 

``` java
public void testPostIndex() throws IOException {
        createIndex("testpostindex");
        ensureGreen();
        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
        IndexService test = indicesService.indexService("testpostindex");
        IndexShard shard = test.shard(0);
        ShardIndexingService shardIndexingService = shard.indexingService();

        final HashMap&lt;String, Boolean&gt; listenerInfo = new HashMap&lt;&gt;();
        listenerInfo.put("postIndexCalled", false);

        shardIndexingService.addListener(new IndexingOperationListener() {
            @Override
            public void postIndex(Engine.Index index) {
                listenerInfo.put("postIndexCalled", true);
                super.postIndex(index);
            }
        });

        Field uidField = new Field("_uid", "45645654", UidFieldMapper.Defaults.FIELD_TYPE);
        Field versionField = new NumericDocValuesField("_version", 0);

        BytesReference source = JsonXContent.contentBuilder().startObject().field("foo", "bar").endObject().bytes();
        ParsedDocument pd = new ParsedDocument(new StringField("uid", "test:id", Field.Store.YES), new IntField("version", 0, Field.Store.YES), "id",
                "test", null, 0, -1, null, source, null);

        shard.index(new Engine.Index(new Term("1#DocUID#1"), pd));

        assertTrue(listenerInfo.get("postIndexCalled"));
    }
```

But it is does not work, because After Lucene try to find document version, she was assert on at org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:501)

``` java
private boolean innerIndex(Index index) throws IOException {
        synchronized (dirtyLock(index.uid())) {
            final long currentVersion;
            VersionValue versionValue = versionMap.getUnderLock(index.uid().bytes());
            if (versionValue == null) {
                currentVersion = loadCurrentVersionFromIndex(index.uid());
            } else {
                if (engineConfig.isEnableGcDeletes() &amp;&amp; versionValue.delete() &amp;&amp; (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) &gt; engineConfig.getGcDeletesInMillis()) {
                    currentVersion = Versions.NOT_FOUND; // deleted, and GC
                } else {
                    currentVersion = versionValue.version();
                }
            }
```

How can I pass Lucene's document version in uid? For skip checking
</comment><comment author="s1monw" created="2015-09-01T13:04:29Z" id="136712013">in `org.elasticsearch.index.engine.InternalEngineTests` we have some code that does that - maybe that helps?

``` Java
 private ParsedDocument testParsedDocument(String uid, String id, String type, String routing, long timestamp, long ttl, Document document, BytesReference source, Mapping mappingUpdate) {
        Field uidField = new Field("_uid", uid, UidFieldMapper.Defaults.FIELD_TYPE);
        Field versionField = new NumericDocValuesField("_version", 0);
        document.add(uidField);
        document.add(versionField);
        return new ParsedDocument(uidField, versionField, id, type, routing, timestamp, ttl, Arrays.asList(document), source, mappingUpdate);
    }
```
</comment><comment author="kiryam" created="2015-09-02T08:08:20Z" id="136971269">Ok, that is! I will want to create test for exception (when postIndex(Index, Exception) will be called) but I don't know how to generate IndexAction with Exception 
</comment><comment author="s1monw" created="2015-09-02T08:16:25Z" id="136972686">&gt; but I don't know how to generate IndexAction with Exception

I'd do it dirty and just close the engine `engine().close()` that should just fire and exception all the time.
</comment><comment author="kiryam" created="2015-09-02T08:58:29Z" id="136982285">@s1monw I did it!
</comment><comment author="s1monw" created="2015-09-02T09:38:21Z" id="136999881">looks great, I added some comments 
</comment><comment author="kiryam" created="2015-09-02T13:25:09Z" id="137077921">Now all ok? Should we "try catch" in postCreate(Engine.Create create, Throwable ex), postIndex(Engine.Index index, Throwable ex) and postDelete(Engine.Delete delete, Throwable ex)
It is already exception. Should we catch exceptions in exception?
</comment><comment author="s1monw" created="2015-09-02T18:24:26Z" id="137199480">I took another look and I am sorry bug you again but now the new listener calls don't have catch clauses. I think they are important though. I will comment inline to make it clear where it's missing
</comment><comment author="s1monw" created="2015-09-02T18:27:43Z" id="137200470">&gt; Now all ok? Should we "try catch" in postCreate(Engine.Create create, Throwable ex), postIndex(Engine.Index index, Throwable ex) and postDelete(Engine.Delete delete, Throwable ex)
&gt; It is already exception. Should we catch exceptions in exception?

yes please :)
</comment><comment author="kiryam" created="2015-09-02T18:57:04Z" id="137211603">@s1monw done!
</comment><comment author="s1monw" created="2015-09-02T20:52:37Z" id="137241107">merged thanks!
</comment><comment author="s1monw" created="2015-09-03T07:18:58Z" id="137358052">I also backport the fix to 2.0
</comment><comment author="kiryam" created="2015-09-03T08:16:16Z" id="137373916">@s1monw how about 1.7?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cant add listener to ShardIndexingService if index failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13202</link><project id="" key="" /><description>ShardIndexingService has functionality for add listener on events preIndex and postIndex
In my project (https://github.com/kiryam/elasticsearch-top) I try to monitor current running index actions. Simple add to Queue if preIndex and remove if postIndex. But if I get failedIndex - index action will freeze in Queue.

&lt;pre&gt;public Engine.Index preIndex(Engine.Index index) {
        totalStats.indexCurrent.inc();
        typeStats(index.type()).indexCurrent.inc();
        for (IndexingOperationListener listener : listeners) {
            index = listener.preIndex(index);
        }
        return index;
    }

public void postIndex(Engine.Index index) {
        long took = index.endTime() - index.startTime();
        totalStats.indexMetric.inc(took);
        totalStats.indexCurrent.dec();
        StatsHolder typeStats = typeStats(index.type());
        typeStats.indexMetric.inc(took);
        typeStats.indexCurrent.dec();
        slowLog.postIndex(index, took);
        for (IndexingOperationListener listener : listeners) {
            try {
                listener.postIndex(index);
            } catch (Exception e) {
                logger.warn("post listener [{}] failed", e, listener);
            }
        }
    }


// HERE no call listeners
public void failedIndex(Engine.Index index) {
      totalStats.indexCurrent.dec();
      typeStats(index.type()).indexCurrent.dec();
  }&lt;/pre&gt;
</description><key id="103933557">13202</key><summary>Cant add listener to ShardIndexingService if index failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kiryam</reporter><labels /><created>2015-08-30T16:39:33Z</created><updated>2015-09-02T20:52:29Z</updated><resolved>2015-09-02T20:52:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kiryam" created="2015-08-30T18:09:50Z" id="136164717">Also ShardSlowLogIndexingService don't write slowlog message if indexing will failed
</comment><comment author="kiryam" created="2015-08-30T18:18:03Z" id="136166106">&lt;pre&gt;
public ParsedDocument index(Engine.Index index) throws ElasticsearchException {
        writeAllowed(index.origin());
        index = indexingService.preIndex(index);
        try {
            if (logger.isTraceEnabled()) {
                logger.trace("index [{}][{}]{}", index.type(), index.id(), index.docs());
            }
            engine().index(index);
            index.endTime(System.nanoTime());
###########
        } catch (RuntimeException ex) { // == no call  indexingService.postIndex(index);
            indexingService.failedIndex(index);
            throw ex;
        }
        indexingService.postIndex(index);
        return index.parsedDoc();
    }
&lt;/pre&gt;
</comment><comment author="kiryam" created="2015-08-30T19:02:03Z" id="136175189">https://github.com/elastic/elasticsearch/pull/13203
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pipeline Aggregation documentation issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13201</link><project id="" key="" /><description>Hi ES team,

I downloaded the Beta release of ES 2.0.0 and was trying out the max_bucket and min_bucket pipeline aggregations, but a small Typo in the documentation present online made it a bit tricky to try out the aggregations, in the following links 
https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-pipeline-max-bucket-aggregation.html  
https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-pipeline-min-bucket-aggregation.html

it's mentioned in the example **buckets_paths** instead of **buckets_path**  

``` json
{
    "aggs" : {
        "sales_per_month" : {
            "date_histogram" : {
                "field" : "date",
                "interval" : "month"
            },
            "aggs": {
                "sales": {
                    "sum": {
                        "field": "price"
                    }
                }
            }
        },
        "max_monthly_sales": {
            "max_bucket": {
                "buckets_paths": "sales_per_month&gt;sales" 
            }
        }
    }
}
```

Notice the extra **s** in **buckets_paths** , and the error isn't very descriptive thus please update the docs as lot of people would be trying out these new features.
</description><key id="103930466">13201</key><summary>Pipeline Aggregation documentation issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tarunsapra</reporter><labels /><created>2015-08-30T15:23:43Z</created><updated>2015-08-31T11:48:23Z</updated><resolved>2015-08-31T11:48:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tarunsapra" created="2015-08-30T20:03:19Z" id="136179229">In this link https://www.elastic.co/guide/en/elasticsearch/reference/2.0/search-aggregations-pipeline-cumulative-sum-aggregation.html as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to change a existing analyzer in Elasticsearch?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13200</link><project id="" key="" /><description>Now that's existing analyzer&#65306; 

```
"analyzer": {
  "xmap_analyzer": {
    "type": "custom",
    "char_filter": [ "html_strip"],
    "filter": [ "lowercase"],
    "tokenizer": "standard"
  }
}
```

I want to remove "filter", and it'll be this: 

```
"analyzer": {
"xmap_analyzer": {
    "type": "custom",
    "char_filter": [ "html_strip"],
    "tokenizer": "standard"
  }
}
```

I have tried to update the settings, like this: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html#update-settings-analysis

But it's invalid, how should I do? Is it a bug?
</description><key id="103899425">13200</key><summary>how to change a existing analyzer in Elasticsearch?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">h3idan</reporter><labels /><created>2015-08-30T03:46:55Z</created><updated>2015-08-30T07:33:05Z</updated><resolved>2015-08-30T07:33:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-30T07:33:05Z" id="136094976">Please ask questions on discuss.elastic.co.

You can not change an existing analyzer and you basically need to reindex.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't document expert segment merge settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13199</link><project id="" key="" /><description>This is just a baby step to address #13185 by removing the docs for merge settings.
</description><key id="103884339">13199</key><summary>Don't document expert segment merge settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-29T21:21:21Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T13:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-29T22:22:38Z" id="136057453">LGTM
</comment><comment author="s1monw" created="2015-08-30T19:06:55Z" id="136175384">maybe we should just make this a javadoc on the relevant class?
</comment><comment author="mikemccand" created="2015-08-30T20:08:02Z" id="136180005">&gt; maybe we should just make this a javadoc on the relevant class?

OK I'll do that!
</comment><comment author="mikemccand" created="2015-08-30T22:15:58Z" id="136207453">OK I carried over the merge docs as javadocs; I think this is ready!
</comment><comment author="s1monw" created="2015-08-31T05:53:42Z" id="136273858">LGTM
</comment><comment author="radu-gheorghe" created="2015-09-29T11:32:47Z" id="144032292">Just my two cents here: I think removing the docs for this will be a barrier for users trying to understand what merges are about. And experiment with settings to see what works best. Sure, few deployments actually benefit from changes to the merge policy, and I completely get removing support for anything other than the tiered merge policy. But some deployments would benefit, for example, from changing the maximum segment size or the thread count.

I think that simply adding a note that says "hey! changing this is highly unlikely to help!" would be a better option. And then still help them change settings if they want to. Like it was done with the _source field - I think that was an excellent solution (simplify settings + warn users of consequences + possible exceptions).

Somewhat off-topic: there are other places where the same discussion of "people tend to fiddle with them and make things worse". Like thread pool. I think docs are useful in all these cases to make people understand:
- what's going on
- what maaaay need to help their particular case

And I think the only missing piece is to say that people should actually test and see if it works better for them, because more often than not, more segments/threads/etc will not make the cluster fly all of a sudden.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update list of available os stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13198</link><project id="" key="" /><description>os cpu information is no longer exposed through the nodes stats api.

Btw, will this be exposed through any other api? Or os cpu information will just no longer be available at all?
</description><key id="103869045">13198</key><summary>update list of available os stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>:Stats</label><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-29T18:41:18Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T14:57:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2015-08-31T10:31:20Z" id="136327738">@clintongormley was removing this intentional? I actually miss the os cpu and also the load average for 1min, 5min, 15min?
</comment><comment author="clintongormley" created="2015-08-31T10:34:14Z" id="136329146">@lmenezes it was intentional.  We removed libsigar because it is unmaintained and started causing segfaults. For now, we're relying only on stats provided by the JVM.  Later we may expand the stats with OS-specific implementations but first things first.
</comment><comment author="lmenezes" created="2015-08-31T10:42:01Z" id="136330091">@clintongormley ah cool, good to know :+1: 
</comment><comment author="tlrx" created="2015-08-31T14:14:36Z" id="136383023">@lmenezes Thanks for your contribution. Could you please [sign the CLA](http://www.elasticsearch.org/contributor-agreement/) so that I can push this in? Thanks
</comment><comment author="lmenezes" created="2015-08-31T14:53:39Z" id="136396686">@tlrx I have already signed this, but I have done it again :)
</comment><comment author="tlrx" created="2015-08-31T14:56:22Z" id="136397342">@lmenezes awesome, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Installing plugin without checksums ends up downloading from github</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13197</link><project id="" key="" /><description>```
bin/plugin install lmenezes/elasticsearch-kopf/develop
-&gt; Installing lmenezes/elasticsearch-kopf/develop...
Trying http://download.elastic.co/lmenezes/elasticsearch-kopf/elasticsearch-kopf-develop.zip ...
Trying http://search.maven.org/remotecontent?filepath=lmenezes/elasticsearch-kopf/develop/elasticsearch-kopf-develop.zip ...
Trying https://oss.sonatype.org/service/local/repositories/releases/content/lmenezes/elasticsearch-kopf/develop/elasticsearch-kopf-develop.zip ...
Trying https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip ...
Downloading .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Verifying https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip checksums if available ...
Trying https://github.com/lmenezes/elasticsearch-kopf/archive/master.zip ...
Downloading ....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Verifying https://github.com/lmenezes/elasticsearch-kopf/archive/master.zip checksums if available ...
```

This happens because we don't have anymore ElasticsearchWrapperException here but standard java exceptions.

Closes #13196.
</description><key id="103843119">13197</key><summary>Installing plugin without checksums ends up downloading from github</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-29T10:20:47Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-29T21:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-29T11:38:08Z" id="135973955">I think we should remove that catch block altogether and just rethrow the first exception from the `HttpDownloadHelper` instead of wrapping it in yet another `IOException`
</comment><comment author="dadoonet" created="2015-08-29T17:27:55Z" id="136014090">@s1monw I pushed a new commit. 
</comment><comment author="s1monw" created="2015-08-29T18:06:26Z" id="136020996">left another comment
</comment><comment author="dadoonet" created="2015-08-29T18:35:50Z" id="136022170">@s1monw pushed another commit.
</comment><comment author="s1monw" created="2015-08-29T20:44:30Z" id="136048418">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem installing plugins without checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13196</link><project id="" key="" /><description>trying to make kopf work with 2.0-beta1 but I just got:

```
[11:01] leonardo.menezes:elasticsearch-2.0.0-beta1/ $ ./bin/plugin install lmenezes/elasticsearch-kopf/develop
-&gt; Installing lmenezes/elasticsearch-kopf/develop...
Trying http://download.elastic.co/lmenezes/elasticsearch-kopf/elasticsearch-kopf-develop.zip ...
Trying http://search.maven.org/remotecontent?filepath=lmenezes/elasticsearch-kopf/develop/elasticsearch-kopf-develop.zip ...
Trying https://oss.sonatype.org/service/local/repositories/releases/content/lmenezes/elasticsearch-kopf/develop/elasticsearch-kopf-develop.zip ...
Trying https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip ...
Downloading .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Verifying https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip checksums if available ...
Trying https://github.com/lmenezes/elasticsearch-kopf/archive/master.zip ...
Downloading ....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Verifying https://github.com/lmenezes/elasticsearch-kopf/archive/master.zip checksums if available ...
```

is this the expected behaviour? I will add checksums if necessary, but going from specified branch to master branch in case no checksum is found doesn't seem like the expected outcome(but maybe it is...)
</description><key id="103840812">13196</key><summary>Problem installing plugins without checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels><label>:Plugins</label><label>blocker</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-29T09:08:44Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-08-29T21:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-29T09:13:30Z" id="135965157">I think you're right. It's a bug IMO.

In the meantime, could you add to your repo a checksum file? May be not doable with github though...
</comment><comment author="lmenezes" created="2015-08-29T09:17:27Z" id="135965521">yeah, will try that in the afternoon, need to leave now. thanks :)
</comment><comment author="dadoonet" created="2015-08-29T09:52:47Z" id="135967681">You can actually run:

``` sh
bin/plugin https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip
```

So you don't have to provide the checksum file for now.
</comment><comment author="dadoonet" created="2015-08-29T09:57:39Z" id="135967849">I see what is happening. We expect to get a `FileNotFoundException` when we download a checksum file if this one does not exist but we get a `IOException`:

```
Can't get https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip.sha1 to /var/folders/r_/r14sy86n2zb91jyz1ptb5b4w0000gn/T/elasticsearch-kopf765881667629279646.zip.sha1
```

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java#L156-156

Will propose a fix.
</comment><comment author="dadoonet" created="2015-08-29T10:14:47Z" id="135968478">Weird...

We have a stacktrace:

```
java.io.IOException: Can't get https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip.sha1 to /var/folders/r_/r14sy86n2zb91jyz1ptb5b4w0000gn/T/elasticsearch-kopf2636540761510642632.zip.sha1
    at org.elasticsearch.common.http.client.HttpDownloadHelper$GetThread.downloadFile(HttpDownloadHelper.java:429)
    at org.elasticsearch.common.http.client.HttpDownloadHelper$GetThread.get(HttpDownloadHelper.java:306)
    at org.elasticsearch.common.http.client.HttpDownloadHelper$GetThread.run(HttpDownloadHelper.java:292)
Caused by: java.io.FileNotFoundException: https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip.sha1
    at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1675)
    at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1673)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.net.www.protocol.http.HttpURLConnection.getChainedException(HttpURLConnection.java:1671)
    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1244)
    at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:254)
    at org.elasticsearch.common.http.client.HttpDownloadHelper$GetThread.downloadFile(HttpDownloadHelper.java:422)
    ... 2 more
Caused by: java.io.FileNotFoundException: https://github.com/lmenezes/elasticsearch-kopf/archive/develop.zip.sha1
    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
    at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
    at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338)
    at org.elasticsearch.common.http.client.HttpDownloadHelper$GetThread.openConnection(HttpDownloadHelper.java:376)
    at org.elasticsearch.common.http.client.HttpDownloadHelper$GetThread.get(HttpDownloadHelper.java:300)
    ... 1 more
```

This code is supposed to unwrap the `FileNotFoundException` but it does not:

``` java
if (ExceptionsHelper.unwrapCause(e) instanceof FileNotFoundException) {
     // checksum file didn't exist
     return false;
}
```

`unwrapCause()` does not work in our case because we don't have any `ElasticsearchWrapperException` here:

``` java
public static Throwable unwrapCause(Throwable t) {
    Throwable result = t;
    while (result instanceof ElasticsearchWrapperException) {
       // Skipped for clarity
    }
    return result;
}
```

I guess this happened when we removed most of the elasticsearch exception and replaced them with standard java exceptions. @s1monw WDYT? 
</comment><comment author="clintongormley" created="2015-08-29T16:31:26Z" id="136007588">thanks for reporting this @lmenezes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take Shard data path into account in DiskThresholdDecider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13195</link><project id="" key="" /><description>The path that a shard is allocated on is not taken into account when
we decide to move a shard away from a node because it passed a watermark.
Even worse we potentially moved away (relocated) a shard that was not even
allocated on that disk but on another on the node in question. This commit
adds a ShardRouting -&gt; dataPath mapping to ClusterInfo that allows to identify
on which disk the shards are allocated on.

Relates to #13106
</description><key id="103840694">13195</key><summary>Take Shard data path into account in DiskThresholdDecider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-29T09:04:48Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-31T08:46:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-29T17:41:55Z" id="136017132">Left just a few comments
</comment><comment author="s1monw" created="2015-08-29T18:08:49Z" id="136021086">@dakrone pushed a new commit
</comment><comment author="dakrone" created="2015-08-29T18:30:28Z" id="136021941">LGTM
</comment><comment author="s1monw" created="2015-08-29T21:11:40Z" id="136052363">@dakrone thx I will give others a chance to look at it and push on monday.
</comment><comment author="bleskes" created="2015-08-31T08:01:25Z" id="136294780">LGTM2 . I do wonder (unrelated to this change), if things will be simpler if we keep the disk usage per path on the cluster info (rather than calc the least free space path) and make the can remain logic check each shard w.r.t it's own path. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_all` should be treated as an ordinary field when returning `fields` or `fielddata_fields`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13194</link><project id="" key="" /><description>The '_all' meta field is not stored, but our fielddata field could work with not stored field. 

Our meta-fields typically contain a single value only, when `toXContent`for meta field, we only return the first value, but _all should be an exception there. This commit adds all values for `_all` field in the  `toXContent` method

closes #13178
</description><key id="103836697">13194</key><summary>`_all` should be treated as an ordinary field when returning `fields` or `fielddata_fields`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Search</label><label>bug</label><label>stalled</label></labels><created>2015-08-29T07:44:57Z</created><updated>2016-04-07T09:39:52Z</updated><resolved>2016-04-07T09:39:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-29T12:10:12Z" id="135978315">@xuzha could you add some description to this PR please, and some documentation?

@jpountz could you review please?
</comment><comment author="xuzha" created="2015-08-30T07:19:38Z" id="136094484">@clintongormley thanks, just update the PR and description. 

Right now the change only add all '_all' values. I think this is more consistent to what we have right now. Still need documentation here?
</comment><comment author="clintongormley" created="2015-08-30T11:44:48Z" id="136130538">@xuzha Because the `_all` field now behaves differently to the other meta-fields, I'd add a section to its documentation explaining how it is different and why https://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-all-field.html

Actually, before we merge this PR, I think we need some further discussion about whether this is the right thing to do...  The `_all` field today can be stored, in which case it is stored as a single value which is returned as a single value at the top-level (like other meta-fields).

The only anomaly is the use of `?fielddata_fields=_all`, which is likely to contain multiple values... But loading fielddata for the `_all` field is a really really bad idea.  That said, it is probably still something we should allow.  Grrr, I'm really not sure what is the right thing to do here.

@jpountz what do you think?
</comment><comment author="rjernst" created="2015-08-30T19:59:04Z" id="136178804">Can we just remove the ability to store/load `_all`? This seems silly to do, and if someone really wants to do it, they can do it with their own regular field? `_all` is meant to be the default for how full text search is done. It isn't meant to be used in other ways (and highlighting is not a real usecase, it doesn't make sense since `_all` is a catchall field and maintains zero information about which field values came from).
</comment><comment author="jpountz" created="2015-09-01T07:27:43Z" id="136612853">I tend to agree with what @rjernst said. I was first a bit reluctant to remove highlighting support, but since highlighting is about locating matches in the original text, it indeed probably doesn't make much sense to highlight on `_all` since it loses information about which fields the snippets come from.
</comment><comment author="clintongormley" created="2015-09-01T17:14:47Z" id="136800390">Had a chat to @nik9000 about highlighting and the `_all` field.  I think we should leave things as they are for now, and revisit this once we have a better story around highlighting.
</comment><comment author="dakrone" created="2016-04-06T20:53:21Z" id="206562588">@clintongormley should this be closed and re-opened if/when appropriate since it's pretty out of date?
</comment><comment author="clintongormley" created="2016-04-07T09:39:51Z" id="206786097">Sure
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Fix link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13193</link><project id="" key="" /><description>Add missing "&gt;" symbol
</description><key id="103804329">13193</key><summary>[DOC] Fix link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmile</reporter><labels><label>docs</label></labels><created>2015-08-28T22:15:06Z</created><updated>2015-08-29T11:12:58Z</updated><resolved>2015-08-29T11:12:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmile" created="2015-08-28T22:18:11Z" id="135900387">Just signed the CLA. Not sure how to force re-check the CLA here.
</comment><comment author="clintongormley" created="2015-08-29T11:12:58Z" id="135972658">Thanks @gmile - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inner Hits returning all records, size option not working as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13192</link><project id="" key="" /><description>I'm relatively new to Elastic Search, but finding it extremely pleasant to work with so far. I have ran into a particular interesting problem detailed below.
##### Mapping

Below is my mapping. I'm using a few nested fields. My data is product data that rolls up to a product code. Many variations of product names can have the same product code. For this reason I used a nested type when mapping.

``` json
{
    "index": "products_en",
    "body": {
        "settings": {
            "refresh_interval": "-1"
        },
        "mappings": {
            "products": {
                "_source": {
                    "enabled": true
                },
                "properties": {
                    "PRODUCT_CODE": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "PRODUCT_NAMES": {
                        "type": "nested",
                        "include_in_parent": true,
                        "properties": {
                            "NAME": {
                                "type": "string",
                                "index": "analyzed",
                                "null_value": "NULL",
                                "fields": {
                                    "lang_analyzed": {
                                        "type": "string",
                                        "analyzer": "english"
                                    }
                                }
                            }
                        }
                    },
                    "PART_NUMBERS": {
                        "type": "nested",
                        "properties": {
                            "NUMBER": {
                                "type": "string",
                                "index": "not_analyzed",
                                "null_value": "NULL"
                            }
                        }
                    },
                    "LOCALE_CODES": {
                        "type": "nested",
                        "properties": {
                            "CODE": {
                                "type": "string",
                                "index": "not_analyzed",
                                "null_value": "NULL"
                            }
                        }
                    },
                    "RECOMMENDED": {
                        "type": "boolean"
                    }
                }
            }
        }
    }
}
```
##### Populating the Index

When populating my index each document will look similar to the example below:

``` json
{
    "body": [
        {
            "index": {
                "_index": "products_en",
                "_type": "products"
            }
        },
        {
            "PRODUCT_CODE": "A00023ABCDEF",
            "PRODUCT_NAMES": {
                "NAME": [
                    "TEST PRODUCT NAME VERSION ABCDEFGH TYPE 1VB",
                    "OLD PRODUCT NAME QZZZ",
                    "NEW PRODUCT NAME AB55",
                    "PHASEOUT PPP 034343 NAME"
                ]
            },
            "PART_NUMBERS": {
                "NUMBER": [
                    "000AZZZ",
                    "QZZZ",
                    "MM5000"
                ]
            },
            "LOCALE_CODES": {
                "CODE": [
                    "EN",
                    "EN_US",
                    "EN_IE",
                    "EN_SG"
                ]
            },
            "RECOMMENDED": true
        }
    ]
}
```
##### Querying

Then when forming my query my goal is search wrap the nested items in a `dis_max` and determine the best possible matches from inside the nested types utilizing a common query.  Below is an example query is to find a product name / part number combination for multiple nested fields:

``` json
{
  "from": 0,
  "size": 10,
  "query": {
    "filtered": {
      "query": {
        "dis_max": {
          "tie_breaker": 0.4,
          "queries": [
            {
              "nested": {
                "path": "products.PRODUCT_NAMES",
                "score_mode": "max",
                "query": {
                  "common": {
                    "PRODUCT_NAMES.NAME": {
                      "query": "PRODUCT NAME QZZZ",
                      "boost": 10,
                      "cutoff_frequency": 0.0007,
                      "minimum_should_match": 2
                    }
                  }
                },
                "inner_hits": {
                  "size": 2
                }
              }
            },
            {
              "nested": {
                "path": "products.PART_NUMBERS",
                "score_mode": "max",
                "query": {
                  "common": {
                    "PART_NUMBERS.NUMBER": {
                      "query": "QZZZ",
                      "boost": 30,
                      "cutoff_frequency": 0.0007
                    }
                  }
                },
                "inner_hits": {
                  "size": 1
                }
              }
            }
          ]
        }
      },
      "filter": {
        "nested": {
          "path": "products.LOCALE_CODES",
          "filter": {
            "bool": {
              "must": [
                {
                  "term": {
                    "LOCALE_CODES.CODE": "EN_US"
                  }
                }
              ]
            }
          },
          "_cache": false
        }
      }
    }
  },
  "track_scores": true
}
```
##### Results

The query above executes, it finds matches, but the `inner_hits` section of each field is returning all the nested values. Shouldn't it just return the nested values that matched?

Here is the `inner_hits` section of the response for the PRODUCT_NAMES.NAME nested field.

``` json
"inner_hits": {
    "products.PRODUCT_NAMES": {
        "hits": {
            "total": 1,
            "max_score": 4.4727473,
            "hits": [
                {
                    "_index": "products_en",
                    "_type": "products",
                    "_id": "AU91ue6l7hxDSEsV00W-",
                    "_nested": {
                        "field": "PRODUCT_NAMES",
                        "offset": 0
                    },
                    "_score": 4.4727473,
                    "_source": {
                        "NAME": [
                            "TEST PRODUCT NAME VERSION ABCDEFGH TYPE 1VB",
                            "OLD PRODUCT NAME QZZZ",
                            "NEW PRODUCT NAME AB55"
                        ]
                    }
                }
            ]
        }
    }
}
```

Notice I had `inner_hits` size set to 2 and the above is returning 3 values:

``` json
"inner_hits": {
  "size": 2
}
```

---
### My problems:
- inner_hits is returning all values of nested field.
- inner_hits size option doesn't seem to work

Any help or pointers are appreciated. Thanks!
</description><key id="103794551">13192</key><summary>Inner Hits returning all records, size option not working as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tegansnyder</reporter><labels /><created>2015-08-28T20:57:59Z</created><updated>2017-04-24T10:39:20Z</updated><resolved>2015-08-29T11:57:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-29T11:57:49Z" id="135975246">Your document format is incorrect.  Nested objects are for arrays of objects, while you're passing a single object with a multi-value string field.  Change the structure to this and your query works as expected:

PUT products_en/products/1
    {
      "PRODUCT_CODE": "A00023ABCDEF",
      "PRODUCT_NAMES": [
        {
          "NAME": "TEST PRODUCT NAME VERSION ABCDEFGH TYPE 1VB"
        },
        {
          "NAME": "OLD PRODUCT NAME QZZZ"
        },
        {
          "NAME": "NEW PRODUCT NAME AB55"
        },
        {
          "NAME": "PHASEOUT PPP 034343 NAME"
        }
      ],
      "PART_NUMBERS": [
        {
          "NUMBER": "000AZZZ"
        },
        {
          "NUMBER": "QZZZ"
        },
        {
          "NUMBER": "MM5000"
        }
      ],
      "LOCALE_CODES": [
        {
          "CODE": "EN"
        },
        {
          "CODE": "EN_US"
        },
        {
          "CODE": "EN_IE"
        },
        {
          "CODE": "EN_SG"
        }
      ],
      "RECOMMENDED": true
    }
</comment><comment author="tegansnyder" created="2015-08-29T14:57:47Z" id="135995853">@clintongormley thanks for pointing out my formatting issue. This resolved my issue. I very much appreciate your guidance.

On a related note, I have read the documentation for inner_hits and it appears highlighting is supported by inner_hits; however, I seem to be having some difficulty show highlighted results... maybe it's due to how I'm referencing the nested field. Example below:

``` json
{
  "from": 0,
  "size": 10,
  "query": {
    "filtered": {
      "query": {
        "dis_max": {
          "tie_breaker": 0.4,
          "queries": [
            {
              "nested": {
                "path": "products.PRODUCT_NAMES",
                "score_mode": "max",
                "query": {
                  "common": {
                    "PRODUCT_NAMES.NAME": {
                      "query": "PRODUCT NAME QZZZ",
                      "boost": 10,
                      "cutoff_frequency": 0.0007,
                      "minimum_should_match": 2
                    }
                  }
                },
                "inner_hits": {
                  "size": 2
                }
              }
            },
            {
              "nested": {
                "path": "products.PART_NUMBERS",
                "score_mode": "max",
                "query": {
                  "common": {
                    "PART_NUMBERS.NUMBER": {
                      "query": "QZZZ",
                      "boost": 30,
                      "cutoff_frequency": 0.0007
                    }
                  }
                },
                "inner_hits": {
                  "size": 1
                }
              }
            }
          ]
        }
      },
      "filter": {
        "nested": {
          "path": "products.LOCALE_CODES",
          "filter": {
            "bool": {
              "must": [
                {
                  "term": {
                    "LOCALE_CODES.CODE": "EN_US"
                  }
                }
              ]
            }
          },
          "_cache": false
        }
      }
    }
  },
  "highlight": {
    "pre_tags": [
      "&lt;mark&gt;"
    ],
    "post_tags": [
      "&lt;/mark&gt;"
    ],
    "fragment_size": 80,
    "no_match_size": 0,
    "fields": {
      "PRODUCT_NAMES.PRODUCT_NAME": {
        "number_of_fragments": 0
      }
    }
  },
  "track_scores": true
}
```
</comment><comment author="abhishek5678" created="2017-04-24T10:39:20Z" id="296613973">Hello Good Afternoon everyone,
when i am applying the query then it's not working.inside this it's showing like that no [query] registered for [filtered].if anyone knows about that then please help me</comment></comments><attachments /><subtasks /><customfields /></item><item><title>get the most critical messages in ELK stack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13191</link><project id="" key="" /><description>Hello 
i have an ELK stack up and running, all things is OK, but i need to move forward to send some alerts to the sysadmins to begin to work in the errors that comes from all servers.
SO i want to write a query to get top 10 critical unique messages stored in Elasticsearch, then i will send an alert regarding each messages.
I tryied this query but it give me a wrong data

```
{
  "size": 0,
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "analyze_wildcard": true,
          "query": "crit"
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "@timestamp": {
                  "gte": 1440712800000,
                  "lte": 1440799199999
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  },
  "aggs": {
    "3": {
      "terms": {
        "field": "message",
        "size": 20,
        "order": {
          "_count": "desc"
        }
      }
    }
  }
}
```

Regards 
</description><key id="103791526">13191</key><summary>get the most critical messages in ELK stack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">albarki</reporter><labels /><created>2015-08-28T20:35:14Z</created><updated>2015-08-29T11:42:35Z</updated><resolved>2015-08-29T11:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-29T11:42:35Z" id="135974161">Hi @albarki 

The place to ask questions like these is in the forums: https://discuss.elastic.co/

Also, when you do, I'd suggest providing some example data and explaining why you think the results are wrong and what results you expect instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Overlap calculations in geo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13190</link><project id="" key="" /><description>It would be useful to be able to return the net overlap/disjoint areas on a geo_shape query, so as to determine similarity of the queried shape to the indexed document.
</description><key id="103789037">13190</key><summary>Overlap calculations in geo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>:Geo</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-08-28T20:19:06Z</created><updated>2016-01-28T13:41:58Z</updated><resolved>2016-01-28T13:41:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Ghost93" created="2016-01-15T20:14:24Z" id="172077637">+1
</comment><comment author="clintongormley" created="2016-01-28T13:41:58Z" id="176189655">Duplicate of #6616
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix test for _cat/nodeattrs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13189</link><project id="" key="" /><description>Closes #12558
</description><key id="103781832">13189</key><summary>Fix test for _cat/nodeattrs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-08-28T19:35:00Z</created><updated>2015-09-19T14:56:32Z</updated><resolved>2015-09-10T15:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-29T03:52:45Z" id="135937349">Doesn't this make the test worthless since it could then not output any attributes and the test would still pass?
</comment><comment author="clintongormley" created="2015-08-29T12:08:09Z" id="135977750">One alternative is to require test nodes to have some node attr in place. We already have other requirements for the REST tests like setting `path.repo` etc, so there is precedent.

Also, we have "skip-feature" functionality in the REST tests which will only run a test if some condition is met.  So we could check if any node attribute is set, then only run the test if that is the case.
</comment><comment author="nik9000" created="2015-08-29T14:32:30Z" id="135992019">&gt; One alternative is to require test nodes to have some node attr in place. We already have other requirements for the REST tests like setting path.repo etc, so there is precedent.

I'll just do this. I was trying to find a simple solution to this while working on something else, just stealing time while something more important was running tests. I'll test it with a specific nodeattr in place.
</comment><comment author="dakrone" created="2015-08-29T14:42:21Z" id="135994032">That sounds like a better alternative to me too
</comment><comment author="nik9000" created="2015-08-31T21:20:45Z" id="136505390">&gt; That sounds like a better alternative to me too

Ok - I think I got it.
</comment><comment author="clintongormley" created="2015-09-01T11:55:09Z" id="136686727">@nik9000 as a side note (ie for a separate change) it would be great to document the settings that are required to run the REST tests successfully.  This info will be important for all the clients too.
</comment><comment author="nik9000" created="2015-09-01T13:12:45Z" id="136715193">&gt; @nik9000 as a side note (ie for a separate change) it would be great to document the settings that are required to run the REST tests successfully. This info will be important for all the clients too.

Yeah - that's a good idea. And I'm not truly sure what that list is. `EsRestTestCase` is probably the right place to look though!
</comment><comment author="nik9000" created="2015-09-04T19:48:45Z" id="137837328">@dakrone could you have another look at this? I think its no longer useless.
</comment><comment author="nik9000" created="2015-09-09T12:27:23Z" id="138894226">@dakrone, I believe this is waiting on a review from you.
</comment><comment author="dakrone" created="2015-09-09T14:31:34Z" id="138927822">LGTM
</comment><comment author="nik9000" created="2015-09-10T15:11:56Z" id="139275552">Squashed and rebased. Retested and all passes. Merging to master and 2.x.
</comment><comment author="nik9000" created="2015-09-10T15:36:43Z" id="139283917">All done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit the size of the result window to a dynamic property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13188</link><project id="" key="" /><description>Requesting a million hits, or page 100,000 is always a bad idea, but users may not be aware of this. This adds a per-index limit on the maximum `size + from` that can be requested which defaults to 10,000.

This should not interfere with deep-scrolling.

Closes #9311
</description><key id="103774218">13188</key><summary>Limit the size of the result window to a dynamic property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>breaking</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-08-28T18:42:28Z</created><updated>2015-09-19T15:12:20Z</updated><resolved>2015-09-10T19:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-28T18:44:09Z" id="135857731">Still a bit of a work in progress but ok for review.
</comment><comment author="nik9000" created="2015-08-28T20:23:40Z" id="135879395">&gt; Still a bit of a work in progress but ok for review.

And now it seems to be working pretty well.
</comment><comment author="clintongormley" created="2015-08-29T12:02:49Z" id="135976468">@nik9000 could you add some description of the PR to make it easier to understand? This PR is what the change log links to.
</comment><comment author="clintongormley" created="2015-08-29T12:05:03Z" id="135977376">Could you document this setting here as well please: https://www.elastic.co/guide/en/elasticsearch/reference/2.0/index-modules.html
</comment><comment author="jpountz" created="2015-09-01T07:32:27Z" id="136613927">Woohoo! LGTM
</comment><comment author="nik9000" created="2015-09-01T13:36:58Z" id="136724368">&gt; @nik9000 could you add some description of the PR to make it easier to understand? This PR is what the change log links to.

Done.
</comment><comment author="nik9000" created="2015-09-01T14:19:02Z" id="136736754">&gt; Could you document this setting here as well please: https://www.elastic.co/guide/en/elasticsearch/reference/2.0/index-modules.html

Done
</comment><comment author="nik9000" created="2015-09-02T12:27:45Z" id="137056109">Ok - I think this is pretty much ready. There is still an open question above about long for the setting. I use a POSITIVE_INTEGER validator so I think that is enough, right? I can add a test to the setting that tries to set it to Integer.MAX_VALUE + 1 and notices the error if you'd like.
</comment><comment author="nik9000" created="2015-09-04T19:49:54Z" id="137837498">Ping @dakrone and @jpountz - this has one open discussion point but seems otherwise ready. Can you have a look above? Its the long vs int discussion.
</comment><comment author="nik9000" created="2015-09-09T12:28:19Z" id="138894529"> Ping @dakrone and @jpountz again - I think this is just waiting on an answer from you to the question above about int vs long?
</comment><comment author="jpountz" created="2015-09-09T12:32:45Z" id="138895250">Sorry I totally missed this message. The validator should work indeed, +1 to push.
</comment><comment author="nik9000" created="2015-09-10T19:40:18Z" id="139356965">&gt; Sorry I totally missed this message. The validator should work indeed, +1 to push.

Cool! I've squashed, rebased, and made some minor corrections to deal with COUNT not being a search type any more.
</comment><comment author="nik9000" created="2015-09-10T20:16:00Z" id="139367284">Pushed to master and 2.1.
</comment><comment author="clintongormley" created="2015-09-19T15:12:20Z" id="141678769">w00t
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable -XX:+UseCompressedOops By Default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13187</link><project id="" key="" /><description>To go along with our suggestion that heaps should never exceed the 32 GB barrier, we should softly enforce it by explicitly enabling `-XX:+UseCompressedOops`, which is on by default for heaps under the supported limit and off when you cross the limit. By enabling it manually, a JVM above the barrier _should_ fail to start (unfortunately, I don't have a machine with enough RAM to test this on).

[I've seen murmurs that enabling the setting may lead to a performance reduction](http://stackoverflow.com/a/11054851/706724) (in the comments), but this makes no sense because it is on by default. Still, this should be tested.
</description><key id="103765096">13187</key><summary>Enable -XX:+UseCompressedOops By Default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>discuss</label></labels><created>2015-08-28T17:51:42Z</created><updated>2015-08-31T13:40:50Z</updated><resolved>2015-08-31T13:40:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-08-28T18:10:33Z" id="135850479">An Oracle JVM will warn but not fail if `-XX:+UseCompressedOops` is set and in conflict with the heap size:

```
13:59:30 [jason:~/src/oops] $ java -Xmx32g -XX:+UseCompressedOops Oops  
Java HotSpot(TM) 64-Bit Server VM warning: Max heap size too large for Compressed Oops
Hello, oops!
```

You can also see how it will be on if you're under the threshold:

```
14:00:04 [jason:~/src/oops] $ java -Xmx30g -XX:+UseCompressedOops -XX:+UnlockDiagnosticVMOptions -XX:+PrintCompressedOopsMode Oops
Protected page at the reserved heap base: 0x000000010af80000 / 524288 bytes
heap address: 0x000000010b000000, size: 30802 MB, Compressed Oops with base: 0x000000010afff000
Hello, oops!
```

But not if you're over the threshold:

```
14:03:12 [jason:~/src/oops] $ java -Xmx32g -XX:+UseCompressedOops -XX:+UnlockDiagnosticVMOptions -XX:+PrintCompressedOopsMode Oops
Java HotSpot(TM) 64-Bit Server VM warning: Max heap size too large for Compressed Oops
Hello, oops!
14:03:12 [jason:~/src/oops] $
```

You can also see it on with:

```
14:06:51 [jason:~/src/oops] $ java -Xmx30g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops
     bool UseCompressedOops                        := true            {lp64_product}
```

And by default:

```
14:06:52 [jason:~/src/oops] $ java -Xmx30g -XX:+PrintFlagsFinal Oops | grep Oops
     bool UseCompressedOops                        := true            {lp64_product}
```

But not if we go over the limit:

```
14:07:43 [jason:~/src/oops] $ java -Xmx32g -XX:+PrintFlagsFinal Oops | grep Oops
     bool UseCompressedOops                         = false           {lp64_product}     
```

Even if we specify the flag:

```
14:08:23 [jason:~/src/oops] $ java -Xmx32g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops
Java HotSpot(TM) 64-Bit Server VM warning: Max heap size too large for Compressed Oops
     bool UseCompressedOops                        := false           {lp64_product}  
```

In short, I don't think there's anything to do here.
</comment><comment author="nik9000" created="2015-08-28T18:27:04Z" id="135854130">&gt; In short, I don't think there's anything to do here.

Can we get at `UseCompressedOops` from Java or is that lost to us?
</comment><comment author="jasontedor" created="2015-08-29T14:49:56Z" id="135994951">&gt; Can we get at `UseCompressedOops` from Java or is that lost to us?

It can be obtained from a management bean on the OpenJDK line of JVMs.

The IBM JVM has a similar concept (compressed pointers below 25 GB heaps) but uses a different flag. I'm not familiar enough with that VM to know kind of introspection is available at runtime (I suspect it's possible, I just don't know).

That said, I'm just not sure if this is a path that we should go down.
</comment><comment author="nik9000" created="2015-08-31T12:55:38Z" id="136363211">&gt; That said, I'm just not sure if this is a path that we should go down.

The trouble with the path we are on is that this is one of the first things you'll want to know when helping people with high memory usage issues. Its not required by any means but its kind of "advanced" not to use it. I mean, its one of those things that you want to have tested and profiled before doing in a high traffic environment.

I could see us adding a setting that stopped elasticsearch if it couldn't verify that compressed oops was enabled with a helpful message. The user could disable the setting in elasticsearch.yaml and go on but they'd have to intentionally do it.

That intentionality is what I'm looking for - right now compressed oops is something we think we rely on but we never test. We even wave our hands at the border. "Max heap is 30GB", "max heap is 31.5GB", stuff like that. If we had a error or even a warning if we went over the compressed oops boundary then we wouldn't have to be so hand wavy.
</comment><comment author="rmuir" created="2015-08-31T13:22:17Z" id="136369334">&gt; That intentionality is what I'm looking for - right now compressed oops is something we think we rely on but we never test.

What relies on this: where are jazillions of objects being created? In most cases at least lucene datastructures are using large byte[] or addressing into memory mapped files.
</comment><comment author="jasontedor" created="2015-08-31T13:28:58Z" id="136371157">&gt; That intentionality is what I'm looking for - right now compressed oops is something we think we rely on but we never test. 

Are you sure that this is correct?

To be clear, I think that we merely recommend it as a performance optimization and to avoid heaps getting too large increasing the likelihood of long-running garbage collection cycles destroying the cluster.  But I don't see that as a reason to completely prevent users that think they need very large heaps from having them. And given that, I don't see the need to add configuration complexity to Elasticsearch so that `UseCompressedOops` being disabled is prevented by default.
</comment><comment author="nik9000" created="2015-08-31T13:33:18Z" id="136372647">&gt; What relies on this: where are jazillions of objects being created? In most cases at least lucene datastructures are using large byte[] or addressing into memory mapped files.

I haven't dug into it. Its an old, old bit of advice. Honestly @pickypg will probably know better than I what happens without it.

&gt; &gt; That intentionality is what I'm looking for - right now compressed oops is something we think we rely on but we never test.
&gt; &gt; Are you sure that this is correct?

I am 100% sure its recommended.

&gt; To be clear, I think that we merely recommend it as a performance optimization and to avoid heaps getting too large increasing the likelihood of long-running garbage collection cycles destroying the cluster. But I don't see that as a reason to completely prevent users that think they need very large heaps from having them.

That is a good point - if its really just a question of GC pauses then Elasticsearch shouldn't bother checking the flag.
</comment><comment author="kimchy" created="2015-08-31T13:34:25Z" id="136373043">Historically, I recommended heaps smaller than 30gb in order to get compression since both ES and Lucene ended up allocating a lot of objects (and non paged ones). It also served as a relatively safe number for not going crazy when it comes to heap size and GC.

Now, many of the data structures, in both Lucene and ES, are properly paged and "bulked", and with doc values, this starts to become less of a concern. I wonder how much compressed ops matter now, compared to 2 years ago.

In general, to me, less than 30gb has been a good number for long GC now days (G1 excluded), so I don't think we need to add this as a flag. We did a lot to improve memory in ES and Lucene, wondering how things like doc values by default, and future improvement will end up our recommendation for heap sizes. For now, it is still safe to say max is 30gb, as we learn more.
</comment><comment author="jasontedor" created="2015-08-31T13:40:04Z" id="136374303">It seems that we've reached some consensus that we shouldn't add this. I'm going to close this issue unless someone thinks otherwise?
</comment><comment author="nik9000" created="2015-08-31T13:40:50Z" id="136374703">&gt; It seems that we've reached some consensus that we shouldn't add this. I'm going to close this issue unless someone thinks otherwise?

Fine by me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `percentiles_bucket` pipeline aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13186</link><project id="" key="" /><description>This pipeline will calculate percentiles over a set of sibling buckets.  This is an exact implementation, not approximate like the `percentiles` metric agg.

This comes with a few limitations: to prevent serializing data around, only the requested percentiles are calculated (unlike the TDigest version, which allows the java API to ask for any percentile). It also needs to store the data in-memory, resulting in some overhead if the requested series is very large.
</description><key id="103751434">13186</key><summary>Add `percentiles_bucket` pipeline aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-08-28T16:30:00Z</created><updated>2015-09-04T12:37:55Z</updated><resolved>2015-09-04T02:36:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-01T12:53:03Z" id="136706161">@polyfractal I left some comments
</comment><comment author="polyfractal" created="2015-09-01T19:04:11Z" id="136829321">Comments addressed!
</comment><comment author="colings86" created="2015-09-02T09:58:55Z" id="137010073">LGTM but I wonder if we should add one more test that tests that we can reference a decimal percentile in a buckets path using the syntax like `histo.percentile_agg[99.9]` as I think people might want to use decimal percentiles as inputs to other pipeline aggs
</comment><comment author="polyfractal" created="2015-09-02T13:30:16Z" id="137079429">Agreed, we should test that explicitly.  Will add it today.
</comment><comment author="jpountz" created="2015-09-04T07:49:05Z" id="137670101">@polyfractal FYI this seems to cause build failures: #13337
</comment><comment author="polyfractal" created="2015-09-04T09:58:09Z" id="137693886">Also http://build-us-00.elastic.co/job/es_core_master_suse/1819/ it seems

Will push fixes as soon as I'm awake :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't allow users to modify merge settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13185</link><project id="" key="" /><description>this is extremely expert and even experts don't touch it. I really wonder if we should allow this at all and if so should we rather provide some kind of optimized profile for `[THROUGHPUT, SEARCH_PERFORMANCE, READ_ONLY]' or something like this?
</description><key id="103750014">13185</key><summary>Don't allow users to modify merge settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2015-08-28T16:20:29Z</created><updated>2015-08-28T17:35:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-28T16:27:42Z" id="135825214">I like the idea of an optimized profile! I think its much much less trappy. Especially if you can transition from one to the other and the "right things" happen.
</comment><comment author="bleskes" created="2015-08-28T16:31:14Z" id="135825959">+1

On Fri, Aug 28, 2015 at 6:27 PM, Nik Everett notifications@github.com
wrote:

&gt; ## I like the idea of an optimized profile! I think its much much less trappy. Especially if you can transition from one to the other and the "right things" happen.
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/13185#issuecomment-135825214
</comment><comment author="polyfractal" created="2015-08-28T16:41:36Z" id="135828119">+1

I can count on one hand the number of clusters helped by merge settings tweaks.  In contrast, you see clusters obliterated by even relatively small merge tweaks all the time.
</comment><comment author="clintongormley" created="2015-08-28T17:30:45Z" id="135839867">+1
</comment><comment author="mikemccand" created="2015-08-28T17:35:57Z" id="135840842">+1

At a minimum we should stop documenting these settings; I think we should do that today for 2.0.

This fits well with other recent simplifications (removing `SerialMergeScheduler`, merge policy control (always use the default `tiered`).

Optimized profiles is a great idea but I think there are challenges.  E.g. increasing `index.merge.policy.segments_per_tier` and/or decreasing `index.merge.policy.max_merge_size` should give better indexing throughput, since it tolerates more segments in the index, but it risks exhausting file handles if the node stores many shards and the risk is very dependent on how large the shard turns out to be and whether compound file format was disabled.

Also, when users change from one profile to another (e.g. THROUGHPUT to SEARCH_PERFORMANCE) suddenly on large shards there could a long running high merging cost for the shard to "catch up" to the new settings.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bats tests should create a script and a template on the filesystem and verify that they work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13184</link><project id="" key="" /><description>This is mostly for the rpm and deb but probably also should be done for the tar tests.

Part of #13138
</description><key id="103749726">13184</key><summary>bats tests should create a script and a template on the filesystem and verify that they work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label></labels><created>2015-08-28T16:18:30Z</created><updated>2015-09-16T14:45:41Z</updated><resolved>2015-09-16T14:45:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Packaging tests should verify upgrades from some previous version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13183</link><project id="" key="" /><description>Right now the bats tests install elasticsearch on a clean system. They should also test that the packages can be upgraded without losing data. Both across major and minor versions.
</description><key id="103748588">13183</key><summary>Packaging tests should verify upgrades from some previous version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-28T16:11:40Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-09-04T19:35:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-02T16:56:08Z" id="137169769">Ok - so for time's sake we might not want to be super duper thorough on this and just test an upgrade from 1.7 to the branch being built or something like that. We could certainly _try_ to do all the others but that is already covered by `OldIndexBackwardsCompatibilityIT` so this would duplicate a ton of effort. We're really just trying to verify that we keep the data on package upgrade.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bats tests should verify that packages are signed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13182</link><project id="" key="" /><description>The bats tests currently simply install the rpm and debs an exercise them. They should also check that they are signed.

This is part of #13138.
</description><key id="103747223">13182</key><summary>bats tests should verify that packages are signed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.3.0</label></labels><created>2015-08-28T16:07:49Z</created><updated>2016-01-28T13:40:29Z</updated><resolved>2016-01-28T13:40:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-28T16:09:16Z" id="135819497">@nik9000 I am not sure we can easily do this our bulids don't sign the builds at all? maybe they should?
</comment><comment author="nik9000" created="2015-08-28T16:10:00Z" id="135820247">&gt; @nik9000 I am not sure we can easily do this our bulids don't sign the builds at all? maybe they should?

Something like that, yeah. Maybe some self signed certificate stuff. We can figure it out.
</comment><comment author="s1monw" created="2015-08-28T16:10:34Z" id="135820719">yeah I wonder if the release script and the smoke tester should do it for now at least?
</comment><comment author="nik9000" created="2015-08-28T16:14:03Z" id="135822247">&gt; yeah I wonder if the release script and the smoke tester should do it for now at least?

Makes sense to me.
</comment><comment author="nik9000" created="2015-09-03T17:57:28Z" id="137528361">Well that was a trip!
https://github.com/mojohaus/rpm-maven-plugin/issues/11
and
rpm from homebrew is a bit wonky and needs love:

``` shell
$ cat ~/.rpmmacros 
%__gpg_check_password_cmd       %{__gpg} \
        gpg --homedir %{_gpg_path} --batch --no-verbose --passphrase-fd 3 -u %{_gpg_name} -so -
__gpg_sign_cmd                 %{__gpg} \
        gpg --homedir %{_gpg_path} --batch --no-verbose --no-armor --passphrase-fd 3 --no-secmem-warning \
        -u %{_gpg_name} -sbo %{__signature_filename} %{__plaintext_filename}
```
</comment><comment author="clintongormley" created="2016-01-28T13:40:28Z" id="176189369">Closed by #13354
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] Update Apache Maven Enforcer plugin to 1.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13181</link><project id="" key="" /><description>``` xml
&lt;plugin&gt;
 &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
 &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt;
 &lt;version&gt;1.4.1&lt;/version&gt;
&lt;/plugin&gt;
```
# Release Notes - Maven Enforcer - Version 1.4.1

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317520&amp;version=12330766
## Bugs:
- [MENFORCER-222] - RequireSameVersion rule regression between 1.3 and 1.4
- [MENFORCER-224] - Regression from 1.3.1 to 1.4 with bannedDependencies rule
- [MENFORCER-229] - Ban Distribution Management documentation example doesn't work
- [MENFORCER-237] - Resources Link to codehaus is wrong
## Improvements:
- [MENFORCER-223] - Upgrade mrm-maven-plugin to 1.0-beta-2
- [MENFORCER-227] - Document nullability with @Nonnull on EnforcerRule API
- [MENFORCER-233] - Upgrade maven-invoker-plugin to 2.0.0
- [MENFORCER-235] - Use maven-fluido-skin 1.4
- [MENFORCER-236] - Upgrade maven-assembly-plugin version from 2.4 to 2.5.5 in integration test
- [MENFORCER-238] - Upgrade plexus-utils to 3.0.22
</description><key id="103743092">13181</key><summary>[build] Update Apache Maven Enforcer plugin to 1.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta2</label></labels><created>2015-08-28T15:47:02Z</created><updated>2015-11-22T10:13:29Z</updated><resolved>2015-08-28T16:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-28T15:55:14Z" id="135813397">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default _cat verbose to false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13180</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/8927 changed the `_cat` APIs to default to verbose.  This was released in 2.0.0-beta1, but then we decided to revert to previous behaviour.

Closes #13156
</description><key id="103738344">13180</key><summary>Default _cat verbose to false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:CAT API</label><label>breaking</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-28T15:21:15Z</created><updated>2015-11-22T10:13:28Z</updated><resolved>2015-08-28T15:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-28T15:21:58Z" id="135804520">I'm still running the tests for this. Its a one line Java change but its far reaching.

Note: this reverts a breaking API change. Its now no longer breaking.
</comment><comment author="jasontedor" created="2015-08-28T15:59:06Z" id="135814173">LGTM.
</comment><comment author="nik9000" created="2015-08-28T16:01:24Z" id="135814641">Merged to master.

Rerunning tests in 2.0 and will merge there once they pass.
</comment><comment author="grantr" created="2015-08-28T20:59:34Z" id="135886545">:sparkles: Thanks @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Top-level pipeline aggs are not validated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13179</link><project id="" key="" /><description>If a pipeline is defined as a top-level agg (e.g a `sum_bucket` at the very root), it won't have any parent aggregations and thus sidesteps the current parser validation in AggregatorsParser.  The final clause which deals with top-level pipelines needs extra logic to validate pipelines ... but it will likely require some changes since the current pipeline validation methods require an agg parent.

/cc @colings86 
</description><key id="103735676">13179</key><summary>Top-level pipeline aggs are not validated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-08-28T15:08:24Z</created><updated>2015-09-10T15:00:25Z</updated><resolved>2015-09-10T15:00:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>fielddata_fields for _all doesn't display all the terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13178</link><project id="" key="" /><description>Tested in 1.7.1.
For the following test

```
PUT /test_qs
{
  "mappings": {
    "test_type": {
      "properties": {
        "text1": {
          "type": "string"
        },
        "text2": {
          "type": "string"
        },
        "text3": {
          "type": "string"
        }
      }
    }
  }
}

POST /test_qs/test_type/_bulk
{"index":{}}
{"text1":"abc","text2":"def","text3":"ghi"}

GET /test_qs/test_type/_search
{
  "fielddata_fields": ["_all"]
}
```

the result is

```
            "fields": {
               "_all": "abc"
            }
```

where it should have been actually `"_all": ["abc", "def", "ghi"]`.
</description><key id="103727879">13178</key><summary>fielddata_fields for _all doesn't display all the terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>:Mapping</label><label>bug</label><label>low hanging fruit</label><label>stalled</label></labels><created>2015-08-28T14:26:15Z</created><updated>2015-09-01T17:15:11Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T14:57:25Z" id="135797373">The background to this bug: meta-fields typically contain a single value only, while other fields may contain zero or more values. So meta-fields are special cased to return just the first value (and in 2.0, to return their values at the "top" level, instead of within the `fields` element).

`_all` is an exception to this rule and should probably be treated as an ordinary field.  really, returning fielddata for the `_all` field is a really bad idea, fine for testing, but will blow up your heap in production.
</comment><comment author="clintongormley" created="2015-09-01T17:15:11Z" id="136800466">Had a chat to @nik9000 about highlighting and the `_all` field.  I think we should leave things as they are for now, and revisit this once we have a better story around highlighting.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator - Utilise Replicas to parallelise percolations on _mpercolate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13177</link><project id="" key="" /><description>Percolator currently does not utilise replicas fully on an mpercolate request.  Documents are batched and sent to 1 candidate within a replica group.  This ticket proposes increasing use of replicas within a single request for greater parallel execution.  Given Percolator is largely CPU bound (provided there is sufficient memory for queries) this should increase throughput and reduce total execution time for the request.

For example, consider sending 10 documents in a request where queries have been indexed into two shards A1 and B2.  Both have replica shards A2 and B2.  Currently, assuming no routing, all 10 documents will either be sent to (A1 or A2) AND (B1 or B2).  Given execution on each shard is sequential, performance will be as fast as the slowest shard to respond.  Each shard has to percolate 10 documents.

The enhancement would split the 10 documents.  In this case 5 documents would be sent to each of the shards A1,A2,B1,B2.  This may not double throughput and changes the CPU load profile across nodes, introducing contention considerations.  However, it may be worth exploring.
</description><key id="103702422">13177</key><summary>Percolator - Utilise Replicas to parallelise percolations on _mpercolate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels><label>:Percolator</label></labels><created>2015-08-28T11:33:30Z</created><updated>2016-07-01T10:08:51Z</updated><resolved>2016-07-01T10:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gingerwizard" created="2015-08-28T11:35:12Z" id="135745656">@martijnvg suggested this was worth consideration
@polyfractal may have some interesting thoughts re this especially regards the relationship between latency and throughput.
</comment><comment author="clintongormley" created="2015-08-28T14:48:23Z" id="135795198">Not sure this would be effective in production scenarios.  Typically, you'll be doing multiple mpercolate requests.  Each mpercolate request would end up on a single shard, but because you're running multiple requests, you already have parallelization (without the added cost of fanning out). If all you're doing is one mpercolate request, then probably latency isn't as important.

It's the same as running a search on a single shard: it has to hit every segment in the shard, which it does serially.  You could search the segments in parallel which could speed up the results if you only ever run one query at a time, but in practice you run lots of queries simultaneously.
</comment><comment author="gingerwizard" created="2015-08-28T14:59:39Z" id="135797901">MPercolate currently just acts a network optimisation.  The most common application seems to be to buffer N documents and send them off for percolation in one go.  This is particularly applicable when users are percolating prior to indexing.  Its unlikely they are running multiple percolate requests in parallel therefore, and hence not utilising the full capacity of their cluster with respect to CPU. 

If the advice is simply to send N smaller mpercolate requests in parallel i'm fine with that if we think it will achieve the same effect. I imagine you would need to set N to the number of replicas you have.
</comment><comment author="martijnvg" created="2015-08-28T20:12:38Z" id="135877159">I think adding this optimisation at some point does make sense (not high priority). Instead of parallelising   the mpercolate request up to `primary_shard` times, a mpercolate request can be parallelised `primary_shard * num_replica` times. Multiple mpercolate requests can be send at the same time, but this is something that we can solve nicely on the ES side and then clients don't have to worry about dividing their mpercolate requests evenly.  
</comment><comment author="martijnvg" created="2016-07-01T10:08:45Z" id="229910273">Closing this issue as the mpercolate api in 5.0 is deprecated and redirects to msearch api. The msearch api does utilise shard copies better than mpercolate api did.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator - Provide ability to not evaluate all queries when requesting N Hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13176</link><project id="" key="" /><description>Currently percolator allows you to control the number of matches with a size parameter.  However, it always reports the total number of matches and thus is forced to evaluate all queries irrespective of the size.  On larger percolator deployments, the total number of matches is sometimes not required and only the 

Request the addition of a total_hits parameter.  If true, all queries will be evaluated. If false, all queries will be evaluated only if no size parameter is provided - otherwise Percolator will simply drop out.

Introduces considerations with sorting that need discussion
</description><key id="103700819">13176</key><summary>Percolator - Provide ability to not evaluate all queries when requesting N Hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels><label>:Percolator</label><label>adoptme</label></labels><created>2015-08-28T11:18:00Z</created><updated>2016-03-21T11:36:42Z</updated><resolved>2016-03-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gingerwizard" created="2015-08-28T11:22:58Z" id="135744116">@martijnvg 
</comment><comment author="clintongormley" created="2015-08-28T14:43:28Z" id="135793320">For consistency, perhaps support the `terminate_after` parameter instead.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-body.html
</comment><comment author="clintongormley" created="2016-01-28T13:16:22Z" id="176180906">@martijnvg what do you think?
</comment><comment author="martijnvg" created="2016-01-28T14:54:36Z" id="176223897">@clintongormley I think supporting `terminate_after` makes sense.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>discovery.zen.ping.unicast.hosts can't deal with an url?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13175</link><project id="" key="" /><description>here's the deal:
I try to use the unicast function to organize a cluster.As I get 6 servers,so I try to write those ips to the /etc/hosts of all the servers by script(etc. 1.1.1.1 es.com),then config elasticsearch.yml " discovery.zen.ping.unicast.hosts: ["es.com"]".hoping es will find out the ips by this url.
of course,this action bring out no result.Then I have to config like "discovery.zen.ping.unicast.hosts: ["1.1.1.1","2.2.2.2"...etc]"
I really think if es unicast support url config will be better when dealing with a number of servers.
Will es unicast support config like " discovery.zen.ping.unicast.hosts: ["es.com"]" in the future?
Or any suggestion to deal with a number of servers supporting add/remove nodes.
</description><key id="103684526">13175</key><summary>discovery.zen.ping.unicast.hosts can't deal with an url?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yuz11</reporter><labels><label>:Discovery</label><label>feedback_needed</label></labels><created>2015-08-28T09:32:23Z</created><updated>2016-03-16T19:43:41Z</updated><resolved>2016-01-28T13:12:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T14:39:59Z" id="135792571">I'm unclear on what you're asking here.  This seems to work for me. if I configure /etc/hosts to have, eg:

```
1.2.3.4   abc.foo.com
1.2.3.5   abc.foo.com
1.2.3.6   abc.foo.com
```

and set `discovery.zen.ping.unicast.hosts` to `abc.foo.com`, then it tries all three IPs
</comment><comment author="clintongormley" created="2016-01-28T13:12:11Z" id="176176858">Nothing further. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze issues with copy_to target fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13174</link><project id="" key="" /><description>Hello,

I'm experiencing strange issues with analyze on fields used as targets for copy_to.
The analyze API always return trivial results but the analyzers from the copy_to origin field seem in some cases to be used either at index or query time, instead of the ones defined for the copy_to target.

Since I'm not 100% sure this is a bug, I opened a thread on SO with a simple test case showing the behavior that's bothering me but it didn't get much attention: http://stackoverflow.com/questions/32250332/

Any help would be greatly appreciated.
Oh, and by the way, elasticsearch is excellent software!

Regards,
</description><key id="103665616">13174</key><summary>Analyze issues with copy_to target fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Shadocko</reporter><labels /><created>2015-08-28T07:53:57Z</created><updated>2015-08-28T10:22:03Z</updated><resolved>2015-08-28T08:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-28T08:10:10Z" id="135667499">The french analyzer seems to return another result then the one your are expecting:

```
DELETE test
PUT test
POST test/_analyze?text=cl&#233;ment&amp;analyzer=french
```

Gives:

``` json
{
   "tokens": [
      {
         "token": "cl",
         "start_offset": 0,
         "end_offset": 2,
         "type": "&lt;ALPHANUM&gt;",
         "position": 1
      },
      {
         "token": "ment",
         "start_offset": 3,
         "end_offset": 7,
         "type": "&lt;ALPHANUM&gt;",
         "position": 2
      }
   ]
}
```

That explains why it does not match your clement query.

So IMO it's not an issue with the "copy_to" feature here.

Closing for now. Feel free to reopen if you think it's an issue.
</comment><comment author="dadoonet" created="2015-08-28T08:12:53Z" id="135667848">BTW, I tried to analyze `clement` and it gives:

```
{
   "tokens": [
      {
         "token": "cle",
         "start_offset": 0,
         "end_offset": 7,
         "type": "&lt;ALPHANUM&gt;",
         "position": 1
      }
   ]
}
```
</comment><comment author="dadoonet" created="2015-08-28T08:21:23Z" id="135670011">I was expecting the standard tokenizer not to break `cl&#233;ment` at the first `&#233;`. Reopening to make more tests.
</comment><comment author="dadoonet" created="2015-08-28T08:42:57Z" id="135683447">Ok. My platform test is badly setup. If I use curl instead of running that from the browser, then I'm getting expecting results.

``` sh
curl -XGET 'localhost:9200/_analyze?analyzer=french&amp;pretty' -d 'cl&#233;ment'
```

``` json
{
  "tokens" : [ {
    "token" : "clement",
    "start_offset" : 0,
    "end_offset" : 7,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  } ]
}
```

And 

``` sh
curl -XGET 'localhost:9200/_analyze?analyzer=french&amp;pretty' -d 'clement'
```

Gives: 

```
{
  "tokens" : [ {
    "token" : "cle",
    "start_offset" : 0,
    "end_offset" : 7,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  } ]
}
```

So no bug IMO with the `copy_to` feature but you probably need to adjust the `french` analyzer to meet your needs instead of using the default configuration: https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#french-analyzer
</comment><comment author="Shadocko" created="2015-08-28T10:22:03Z" id="135728418">Sorry, I overlooked the fact that the default french analyzer did not yield the same results for "cl&#233;ment" and "clement". I'm having a slow week...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the experimental indices.fielddata.cache.expire</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13173</link><project id="" key="" /><description>closes #10781
</description><key id="103643910">13173</key><summary>Remove the experimental indices.fielddata.cache.expire</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Fielddata</label><label>breaking</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-28T04:14:34Z</created><updated>2015-09-01T12:17:43Z</updated><resolved>2015-09-01T07:43:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-28T12:48:55Z" id="135766299">LGTM
</comment><comment author="xuzha" created="2015-08-30T08:17:23Z" id="136097118">Hmm @clintongormley  is this safe to merge? Or we need more people to review this.
</comment><comment author="clintongormley" created="2015-08-30T11:45:45Z" id="136130576">@jpountz please can your review?
</comment><comment author="jpountz" created="2015-09-01T07:35:33Z" id="136615427">Just left a minor comment about the documentation, otherwise LGTM.
</comment><comment author="jpountz" created="2015-09-01T07:41:45Z" id="136616652">LGTM. Please merge. :)
</comment><comment author="xuzha" created="2015-09-01T07:43:39Z" id="136616958">Thanks for the review @nik9000 @jpountz  :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>type question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13172</link><project id="" key="" /><description>1. sorry, I am in china and I don't know why I can't visit discuss.elastic anywhere. I can't visit it neither in Company nor in my home.so I post my question herer.
2. since type is a logical concept and in Lucene index, it is only a indexed field. why conceptial bounding it with index? I asked because many users tend to index all kinds of data into one index and fetch it by id, which is time cost comparing with put data into diffierent index.
3. I'd like to comfirm one thing: if I have different data types and I used es as a key value store and fetch data with ids. I can improve performance by split one index into many index according to data type. but I don't get what I pay for this improvement. 
   please answer my question or give me some resource to understand it.
</description><key id="103630960">13172</key><summary>type question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-08-28T02:10:34Z</created><updated>2015-09-09T10:07:27Z</updated><resolved>2015-08-28T12:51:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T12:51:38Z" id="135766814">Hi @makeyang 

Types are essentially ways of grouping related fields together, nothing more.  In the past, types could have different mappings for fields of the same name but that is no longer supported (caused a lot of issues).

So yes, you could quite happily store different types in different indices.  Types can make sense in a scenario such as the following:

You have a million documents of type `blogpost`, you have 100 million documents of type `comment`, and you have 1000 documents of type `user`, and 5 documents of type `user_group`.  Having a separate index for `user` and `user_group`may be wasteful (as each shard consumes memory, file handles etc).  So for convenience you can just store them as separate types in the main index.
</comment><comment author="ycombinator" created="2015-08-28T13:43:18Z" id="135778449">Hi @makeyang,

&gt; sorry, I am in china and I don't know why I can't visit discuss.elastic anywhere. I can't visit it neither in Company nor in my home.so I post my question herer.

Sorry to hear that you cannot access discuss.elastic.co. Would you mind running a couple of commands from the command line and reporting their results here (I'm assuming you are on a Linux/Mac system but let me know if you are on Windows)?
1. `dig discuss.elastic.co`
2. `curl -v http://discuss.elastic.co`

Thanks!
</comment><comment author="nik9000" created="2015-08-28T13:54:51Z" id="135780671">&gt; Sorry to hear that you cannot access discuss.elastic.co.

Yeah! Part of the point of moving to discus.elastic.co was to make it work in China.
</comment><comment author="makeyang" created="2015-08-31T01:08:18Z" id="136226748">@ycombinator 
dig discuss.elastic.co

; &lt;&lt;&gt;&gt; DiG 9.8.2rc1-RedHat-9.8.2-0.30.rc1.el6_6.3 &lt;&lt;&gt;&gt; discuss.elastic.co
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 17178
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;discuss.elastic.co.        IN  A

;; ANSWER SECTION:
discuss.elastic.co. 5   IN  CNAME   hosted-elastic.discourse.org.
hosted-elastic.discourse.org. 5 IN  A   64.62.211.227

;; Query time: 31 msec
;; SERVER: 192.168.212.2#53(192.168.212.2)
;; WHEN: Mon Aug 31 09:03:56 2015
;; MSG SIZE  rcvd: 94

curl -v http://discuss.elastic.co
- About to connect() to discuss.elastic.co port 80 (#0)
-   Trying 64.62.211.227... connected
- Connected to discuss.elastic.co (64.62.211.227) port 80 (#0)
  &gt; GET / HTTP/1.1
  &gt; User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.14.0.0 zlib/1.2.3 libidn/1.18 libssh2/1.4.2
  &gt; Host: discuss.elastic.co
  &gt; Accept: _/_
  &gt; 
  &lt; HTTP/1.1 301 Moved Permanently
  &lt; Content-length: 0
  &lt; Location: https://discuss.elastic.co/
  &lt; Connection: close
  &lt; 
- Closing connection #0
</comment><comment author="ycombinator" created="2015-08-31T09:16:56Z" id="136310339">@makeyang Thanks, could you follow the redirect in the curl response and issue the following command next, please? 

`curl -v https://discuss.elastic.co`
</comment><comment author="makeyang" created="2015-09-08T02:14:10Z" id="138409959">@ycombinator 
curl -v https://discuss.elastic.co
the result is in the link below:
https://github.com/makeyang/grocery/blob/master/discuss.html

when I open with browser, the pic is in the link below:
https://github.com/makeyang/grocery/blob/master/discuss.png

please help to resolve this
</comment><comment author="ycombinator" created="2015-09-09T02:12:42Z" id="138755261">Hi @makeyang, we don't have a solution yet but here's a quick update on our progress:

We have contacted the nice people who make Discourse, the software powering the https://discuss.elastic.co forums. We have sent them your (and others') findings.&#160;They are proving to be very helpful so thank you very much for providing those to us.

Based on the `ERR_CONNECTION_RESET` errors in the browser console, it appears that there might be some IP blocking going on with some of the CDN assets. So the Discourse team has, in turn, escalated this issue to their CDN provider, Fastly. That's where we stand today. As I get more updates, I will post them here.
</comment><comment author="makeyang" created="2015-09-09T03:32:26Z" id="138769920">@ycombinator 
thanks for you guys' effort
</comment><comment author="makeyang" created="2015-09-09T08:46:07Z" id="138831090">@ycombinator 
today, I can visit discuss.elastic.co
thank you very much.
</comment><comment author="ycombinator" created="2015-09-09T10:07:27Z" id="138862861">@makeyang Yes, the Discourse team just let us know a few hours ago that the Fastly team had put some fixes in place. Thanks for confirming from China that discuss.elastic.co is now accessible to you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AWS Request ID and x-amzn-RequestId messages at the INFO level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13171</link><project id="" key="" /><description>Log file from end user using AWS Cloud plugin 2.7.0.  Seems like these messages are more appropriate at the DEBUG level?

```
[2015-08-27T02:00:19,723Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 5B81B5234FD4A52E
[2015-08-27T02:00:19,734Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,734Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: CC1512968E5B9655
[2015-08-27T02:00:19,742Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,742Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: F055C0BD0737D9CA
[2015-08-27T02:00:19,753Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,753Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 2F417843C3E50FBD
[2015-08-27T02:00:19,758Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,759Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: EAF1700C9F58864D
[2015-08-27T02:00:19,762Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,762Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 29C053443A72E0B7
[2015-08-27T02:00:19,769Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,769Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: EF5D9F3687A06FBA
[2015-08-27T02:00:19,775Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,775Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 9EED35904FACBBE7
[2015-08-27T02:00:19,780Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,780Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 68F980F09D598168
[2015-08-27T02:00:19,784Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,784Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 2F09BC7519C6EC85
[2015-08-27T02:00:19,789Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,790Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 73EE38D4EF7A7490
[2015-08-27T02:00:19,795Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,795Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: EEE27CB05784EBFD
[2015-08-27T02:00:19,805Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,805Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: CE01AA365F95A09F
[2015-08-27T02:00:19,813Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,813Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: C1E938D46082B5B3
[2015-08-27T02:00:19,821Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,822Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: C3E12D79263E18A6
[2015-08-27T02:00:19,838Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,838Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: F96448112E9C9002
[2015-08-27T02:00:19,844Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,844Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: B375AFF70292FE8A
[2015-08-27T02:00:19,854Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,854Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 049A03BC898CA32E
[2015-08-27T02:00:19,854Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,855Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 90C3C136A59FDD40
[2015-08-27T02:00:19,861Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,861Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#5]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 752EDDFCD4A367C6
[2015-08-27T02:00:19,875Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,875Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 441A9D2789154FB2
[2015-08-27T02:00:19,877Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,877Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 91D530F9B5614DEA
[2015-08-27T02:00:19,888Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,888Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#7]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: FD67EB040453A667
[2015-08-27T02:00:19,896Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,896Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#2]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 69F18D28028E7917
[2015-08-27T02:00:19,910Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  x-amzn-RequestId: not available
[2015-08-27T02:00:19,910Z]  [INFO ]  [elasticsearch[node_name][snapshot][T#6]]  [com.amazonaws.http.AmazonHttpClient]  AWS Request ID: 575CF2CB8FBC0246
```
</description><key id="103630262">13171</key><summary>AWS Request ID and x-amzn-RequestId messages at the INFO level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Plugin Cloud AWS</label><label>feedback_needed</label></labels><created>2015-08-28T02:02:54Z</created><updated>2015-09-08T17:34:39Z</updated><resolved>2015-09-08T17:34:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T12:43:27Z" id="135765530">@dadoonet what do you think?
</comment><comment author="dadoonet" created="2015-08-28T12:48:03Z" id="135766164">I guess the user changed the log level at some point.
`com.amazonaws.http` package is supposed to be `WARN`

In master: https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/config/logging.yml#L12
In 1.7: https://github.com/elastic/elasticsearch/blob/1.7/config/logging.yml#L8

@ppf2 could you check what is the user `logging.yml` file?
</comment><comment author="dadoonet" created="2015-09-08T07:07:57Z" id="138457308">@ppf2 Did you get an answer from the user?
</comment><comment author="ppf2" created="2015-09-08T17:21:56Z" id="138639623">Unfortunately, haven't received logging.yml from them yet.  Since the default for that package is WARN, we can probably close this since there is probably not much we can do about com.amazonaws logging these at their INFO level.
</comment><comment author="clintongormley" created="2015-09-08T17:34:39Z" id="138643344">thanks @ppf2 . Closing for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.Lists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13170</link><project id="" key="" /><description>This commit removes and now forbids all uses of
`com.google.common.collect.Lists` across the codebase. This is the first
of many steps in the eventual removal of Guava as a dependency.
</description><key id="103629928">13170</key><summary>Remove and forbid use of com.google.common.collect.Lists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-28T01:58:22Z</created><updated>2015-09-04T21:20:49Z</updated><resolved>2015-08-28T02:19:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-28T02:10:31Z" id="135604860">LGTM, I left one comment that is just a thought for the future.
</comment><comment author="s1monw" created="2015-08-28T16:22:42Z" id="135823973">PR of the month! :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping conflict error upgrading from 1.7 to 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13169</link><project id="" key="" /><description>```
[2015-08-28 01:17:40,304][ERROR][org.elasticsearch.gateway] [bcapp.dev] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [myindex], reason: [Mapper for [content] conflicts with existing mapping in other types:
[mapper [content] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types.]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:337)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:113)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:226)
...
```

The field in question is:

```
"content" : {
    "type" : "string",
    "analyzer": "html_strip"
}
```

This is the relevant analyzer configuration:

```
{
  "index" : {
    "analysis" : {
      "analyzer" : {
        "html_strip" : {
          "filter" : [
            "standard",
            "lowercase",
            "stop",
            "asciifolding",
            "minimal_stemmer"
          ],
          "char_filter" : [
            "html_strip"
          ],
          "tokenizer" : "standard",
          "type": "custom"
        }
      },
      "filter" : {
        "minimal_stemmer" : {
          "type" : "stemmer",
          "name" : "minimal_english"
        }
      }
    }
  }
}
```

This field is defined on 2 out of 6 types that I have and is identical for those two types.
</description><key id="103626639">13169</key><summary>Mapping conflict error upgrading from 1.7 to 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rayward</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-28T01:25:52Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-09-09T09:30:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rayward" created="2015-08-28T01:29:24Z" id="135599761">Also getting a similar error on a `_parent` field which I have on only one type.

```
"_parent": {
  "type": "product",
  "fielddata": {
    "loading": "eager_global_ordinals"
  }
}
```

```
java.lang.IllegalStateException: unable to upgrade the mappings for the index [myindex], reason: [Mapper for [_parent] conflicts with existing mapping in other types:
[mapper [_parent] is used by multiple types. Set update_all_types to true to update [fielddata] across all types.]]
```

Also, the migration plugin only had informational messages (eg. `required` on `_routing`), nothing that would stop an upgrade.
</comment><comment author="clintongormley" created="2015-08-28T12:37:31Z" id="135764396">Hi @rayward 

Thanks for testing out the beta, and for reporting these bugs.  I think the analyzer problem will be fixed by https://github.com/rjernst/elasticsearch/commit/2e4a053b4283f941999e5ab580f3883c36c952b7 but the _parent issue needs further investigation.

To reproduce, create this index on 1.x:

```
PUT /test
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent",
        "fielddata": {
          "loading": "eager_global_ordinals"
        }
      }
    }
  }
}
```
</comment><comment author="rjernst" created="2015-08-31T06:07:39Z" id="136276860">I agree the first issue described should be fixed by #13206.

The issue with `_parent` is a known issue, and unavoidable at this time (without further refactoring of how mappings work). @clintongormley There are two types in your example, `parent`, and `child`. Because parent is added first (types are added one at a time), it gets the default `_parent` setup with lazy loading. When the second `child` type is parsed, the `_parent` field is now different.

The problem is that all types have a `_parent` field. To fix this, we need to selectively add `_parent` (only when the user actually specifies it for the type), but that is much different than we do today (all meta fields are added to all types). I'm also not sure what effect not having `_parent` on all types would have an existing parent/child code.
</comment><comment author="clintongormley" created="2015-08-31T11:54:16Z" id="136349466">@martijnvg could you comment on https://github.com/elastic/elasticsearch/issues/13169#issuecomment-136276860 please?
</comment><comment author="martijnvg" created="2015-09-01T12:23:51Z" id="136694467">@rjernst @clintongormley I think it may work. I'm going to try out if making the `_parent` optional is working out.
</comment><comment author="martijnvg" created="2015-09-01T14:16:14Z" id="136735644">Internally removing  the _parent field from DocumentMapper works out for the old p/c implementation, but not for the new implementation (that uses doc values and Lucene's JoinUtil). The disabled `_parent` for parent types is used to write to a doc values join field and if the parent field is removed has_child/has_parent won't be able to work because parent docs don't write to the join field.

Just thinking out loud, but what we can try to do is:
1) Remove the notion of 'active' from the parent field
2) add the notion that a document can be a child or a parent
3) all type we always have a _parent field in the mapping
4) but it only gets used by documents if it either is a parent or a child
5) I think this way all _parent fields in all types can have the same settings. (besides the `type` there are pointing to, but that is ok)
</comment><comment author="jpountz" created="2015-09-02T08:20:40Z" id="136974040">Maybe the bug is that `loading` should be allowed to be different across types? The `_parent` field actually stores data in a field that depends on the type name (_parent#type) so having different settings per type does not introduce inconsistencies at the Lucene level?
</comment><comment author="rjernst" created="2015-09-02T16:06:32Z" id="137145852">&gt; The _parent field actually stores data in a field that depends on the type name (_parent#type) so having different settings per type

Maybe the problem is we aren't using this name in map of field types? If we use that as the key, instead of `_parent` we won't have any conflicts.
</comment><comment author="clintongormley" created="2015-09-02T18:18:51Z" id="137197262">@rjernst you mean internally, rather than in the API?  Using it in the API would make it difficult to explain
</comment><comment author="rjernst" created="2015-09-02T18:37:11Z" id="137203356">Yes, I mean internally.
</comment><comment author="martijnvg" created="2015-09-03T13:46:53Z" id="137452096">@rjernst @jpountz I like the idea of internally mapping the _parent field under a name that takes the type into account. I quickly checked this and this seem to work out. Both the upgrade works without an error and has_child/has_parent queries work. 

However after running some more tests, I realized that there are other issues. One can include the `_parent` field in the search hits, query by `_parent` field or sort by it. In these case we lookup the field mapping and that doesn't map correctly. There is no `_parent` field, but fields with `_parent*` prefix. Not sure how to resolve this. 
</comment><comment author="jpountz" created="2015-09-03T14:01:30Z" id="137459472">Hmm, so maybe we should be able to have 2 field types? One called _parent that would only be stored and another one called _parent#type that would only be doc-valued?
</comment><comment author="martijnvg" created="2015-09-03T14:12:31Z" id="137462220">Right, that would solve it, just not sure how to would like. The ParentFieldMapper can only produce one `MappedFieldType`. We then need to add another field mapper?

Taking a step back, maybe just allowing the the `loading` part of field data settings to be different is okay for 2.0? This is a much smaller change. We can then work on a  better fix for 2.x and 3.0. The fact that `ParentFieldMapper` now embeds more fields then it used to should be addressed? We maybe break it out in different fields (multiple unique join fields that connects docs of a different type).
</comment><comment author="martijnvg" created="2015-09-04T16:47:58Z" id="137788434">Yesterday we discussed that _parent field should use an unique internal field type name per type. So that when the mapping compatibility check is performed no check would fail since each field would be unique and having different field data settings wouldn't matter.

Making that change has one important implication the `_parent` field can't used directly in many places in all the APIs. The most notable place would be in the query dsl and we decided that we should have a `_parent_id` query to cover this. Also how we lookup the `_parent` id for search hits in the _search api requires a special sub fetch phase. 

I went a head and made change: https://github.com/martijnvg/elasticsearch/commit/26e2e5d95d2b3a3f778a759ed7a7ea2de9b09842

However things got tricky and I had to make more changes that I would like:
- Due to difference of p/c query implementation, several checks had to be in place to make sure that the right fields were picked.
- Sorting and aggregating by _parent field wouldn't work too. Whether we should remain to support this is a good question (I lean towards not). 
- To maintain the support for specifying the parent id in the `_parent` field in a document required a very subtle change for 1.x indices. (this is no longer allowed in 2.x indices)
- In the field data cache we would end up with multiple _parent field entries per field type. (not sure if this is bad)
- Clear cache by _parent field wouldn't work too.

The change grew to a level that I'm not comfortable porting it back to 2.0 and I think we should completely revise the _parent field. For example with the new implementation we don't need to store _parent indexed and stored field. The join doc values fields are sufficient. The _parent Lucene fields just exist for the old parent child implementation. Also if we going to revise types this is going to have a big impact on the _parent field (if it still exists then).

I think for 2.0 we need to make a compromise in order to keep the change small and low risk. Disabling the field data settings would work, but that wouldn't be nice for all the other fields. So I think disabling the field type compatibility check on the _parent field is maybe the right workaround for now:
https://github.com/martijnvg/elasticsearch/commits/mapping/disable_compatibility_check_for_parent_field

The only field type related setting that can be set on _parent field is field data loading. All the other settings aren't configurable. I think that makes this workaround better then disabling field data settings for all fields.
</comment><comment author="clintongormley" created="2015-09-06T13:21:23Z" id="138085593">@martijnvg ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery fails to handle truncated translog files on NTFS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13168</link><project id="" key="" /><description>As captured in #9699 the translog can get truncated during a power failure or a system crash. The actual details of what a partial write looks like depend on the filesystem among other things, and the fix to that bug addresses partial writes that are seen when this failure happens in ext4 (xfs seems to expose a similar behavior) .

In NTFS a partial write often looks different. If the failure happens in the middle of a FlushFileBuffers call (called by force() on Windows) it may catch the filesystem at a point where file size has been updated in metadata but no data has been written (or at least the valid data pointer has not been moved yet). For security reasons NTFS will return all 0s for content when reading past the last successfully written point after the system restarts. 

The end result is either a translog file that has only zeroes in it (when only the start of the translog was written) or more commonly a translog file that has a set of whole transaction records followed by one or more buffers worth of zeroes. In both cases the translog reading logic chokes on these files during recovery and doesn&#8217;t recover the shard (concretely, since there&#8217;s 0s at the translog record boundary, the translog record type is read as 0 and that causes Translog.Operation.fromId() to fail).

I discussed this with the NTFS folks here at Microsoft and they confirmed we can rely on the 0s for cases where metadata was updated but data wasn&#8217;t during a system failure.

In the context of the 1.7 codebase a relatively simple fix for this would be throw TruncatedTranslogException whenever we see a transaction record of type &#8220;0&#8221; since it's never a valid type. This covers both cases, one because 0s start at a record boundary and the other because a file that&#8217;s all 0s makes ES think it&#8217;s an old translog file, and the legacy translog file code skips the size (first 4 bytes) and goes straight for the first record. To be conservative the checking of 0s could be Windows only, although I&#8217;m not sure if that&#8217;s necessary since no healthy translog should have it. It could even be taken further by verifying that some of the subsequent bytes are 0s as well, although that might be too far. 

The 2.0 codebase has moved quite a bit and I&#8217;m less familiar with it, but somehow this would need to be factored in either by having a boundary where a check for 0s is a safe detection strategy or by growing the translog in known increments and placing markers at the previous page that can be used to detect partial writes. Happy to help explore/validate approaches if that would be useful.

I have a few truncated translog files if anyone wants to take a look (couldn't attach them here, github didn't like .binary files). I have a simple fix and tests for this implemented since we needed to address this quickly here, if a PR would help I can send one.
</description><key id="103613872">13168</key><summary>Recovery fails to handle truncated translog files on NTFS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">pablocastro</reporter><labels><label>:Translog</label><label>discuss</label></labels><created>2015-08-27T23:15:22Z</created><updated>2016-01-21T10:42:49Z</updated><resolved>2016-01-21T10:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T12:29:35Z" id="135762917">Hi @pablocastro 

The translog has changed dramatically in 2.0.  It is now fsynced after every request, and has checkpoints which are written and fsynced after the translog fsync.  The checkpoint file is designed to fit into a single block and so writing it should be atomic.  

See https://github.com/elastic/elasticsearch/pull/11143 and specifically https://github.com/elastic/elasticsearch/pull/11143/files#diff-775d4fc503b78c9a0ef6fd7d1a50f7a8R76 for more details.

In our testing (where we randomly kill the power to servers) we are no longer seeing translog corruptions.
</comment><comment author="pablocastro" created="2015-08-29T00:32:15Z" id="135921235">Thanks for looking at this. 

For 1.7, given that most production users are still on it, would you consider a fix that's specific to this codebase? I would be happy to send a pull request if that helps. 

For 2.0, let me catch up on all the changes you guys did first so I can comment with appropriate context. 

In the meanwhile I'd love to see if we can make a 1.7 fix happen.
</comment><comment author="s1monw" created="2015-08-30T18:52:03Z" id="136174761">&gt; In the meanwhile I'd love to see if we can make a 1.7 fix happen.

the 2.0 code is pretty much a rewrite I don't think we should backport this and for a bugfix release such a change is too big anyway. We closed 1.x dev branches so not much is going to happen there. We have to move on and make sure folks move to 2.0 asap to get all the other goodies too. I don't think we should spend much time on fixing 1.7 but rather invest and test 2.0 instead.
</comment><comment author="s1monw" created="2016-01-21T10:42:49Z" id="173533626">@pablocastro 2.0 has been well out there I am closing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Param settings for tribe in elasticsearch.yml config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13167</link><project id="" key="" /><description>It would be useful to have a bunch of knobs, settings we can set, specifically with tribe nodes.

Some settings that come to mind:

. how many indices we want to keep track of per downstream clusters.

. how "recent" the indices, e.g. we're only interested in last 2 days(48hrs) indices from each cluster.

. how often tribe node poll/query downstream for updates, how many tries before giving up, timeouts, etc.

. membreaker settings. e.g. if mem usage get above a certain percent, to flush for safety instead of falling over.

Feel free to add in other settings.

The idea is to help a tribe node deal with large clusters/large number of indices.
</description><key id="103607689">13167</key><summary>Param settings for tribe in elasticsearch.yml config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TinLe</reporter><labels><label>:Tribe Node</label><label>discuss</label></labels><created>2015-08-27T22:24:44Z</created><updated>2017-05-05T14:44:19Z</updated><resolved>2017-05-05T14:44:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2017-05-05T14:44:19Z" id="299484086">I don't think we want to extend the Tribe node given that we have a replacement for it: Cross Cluster Search. See https://www.elastic.co/blog/tribe-nodes-and-cross-cluster-search-the-future-of-federated-search-in-elasticsearch .</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bumped version to 2.0.0-beta2-SNAPSHOT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13166</link><project id="" key="" /><description>- also added bwc index/repo for 2.0.0-beta1
</description><key id="103595110">13166</key><summary>bumped version to 2.0.0-beta2-SNAPSHOT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>non-issue</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T21:05:20Z</created><updated>2015-09-14T17:14:59Z</updated><resolved>2015-08-27T21:08:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-27T21:07:28Z" id="135555214">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use proper comparison operator ttl test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13165</link><project id="" key="" /><description>lessThanOrEqualTo is more appropriate when comparing _ttl than lessThan
because in rare cases, when tests run very fast, the ttl you fetch will
still equal the one you sent.
</description><key id="103591810">13165</key><summary>Use proper comparison operator ttl test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-27T20:46:23Z</created><updated>2015-08-28T12:10:16Z</updated><resolved>2015-08-27T20:49:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-27T20:48:11Z" id="135551268">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>WIP: Transform as plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13164</link><project id="" key="" /><description>Moves transform script into a plugin.

Still very much a work in progress.

Closes #12674
</description><key id="103587883">13164</key><summary>WIP: Transform as plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>breaking</label></labels><created>2015-08-27T20:24:14Z</created><updated>2015-08-31T17:36:01Z</updated><resolved>2015-08-31T12:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-27T20:27:17Z" id="135545326">@jpountz I think the only way I can extract transform to a plugin without massive surgery is to support arbitrary transformations implementable by plugins. The mapping-transform plugin I've started on would simply by the "script" based implementation of that. And Elasticsearch would have no builtin implementation. Potentially `copy_to` is a builtin implementation if we wanted to spend lots of time on this, but I don't think we do.
</comment><comment author="nik9000" created="2015-08-27T21:29:41Z" id="135560122">I'd love to get some opinions on if moving the ScriptTransform out of core is enough to call this "not supported by core". I'm just not sure of a clean way of moving the transform hooks out of core.
</comment><comment author="rjernst" created="2015-08-29T19:42:05Z" id="136032048">My problem is the mapper service still needs the script service. I think we agree these transformations just shouldn't be done at this level. Could we either instead work on a way to add pre-processing to document injest (outside of mappings), or just drop this entirely and tell users they should use a real ETL before sending the document, or with something like logstash?
</comment><comment author="nik9000" created="2015-08-30T03:41:51Z" id="136082951">Oh!  I think I can drop that direct dependency. It'd be mapper depends on a
thing from a plugin which in turn depends on scripeservice.

Users certainly should use a real etl pipeline in most cases. I still think
elasticsearch is the appropriate place to transform stuff when the result
doesn't belong in the source. It's rare that that is a good thing but
sometimes it is.

I'm happy to move it out of mapper into some other level but I don't know a
level to move it to.
On Aug 29, 2015 3:42 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; My problem is the mapper service still needs the script service. I think
&gt; we agree these transformations just shouldn't be done at this level. Could
&gt; we either instead work on a way to add pre-processing to document injest
&gt; (outside of mappings), or just drop this entirely and tell users they
&gt; should use a real ETL before sending the document, or with something like
&gt; logstash?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13164#issuecomment-136032048
&gt; .
</comment><comment author="rjernst" created="2015-08-30T19:50:34Z" id="136178416">Moving script service out of mapper service would make this more bearable, but I still think we should just drop the ability to do this altogether. An ETL, before elasticsearch is the right place to do transformations. I don't see why you would want transformations to not be in _source. That would mean every time you reindex the document you need to reapply the transform. If there is a possibility you may want to change the transformed value (say you have a category field that you are mapping to an integer code), then store the original value as well, and you can still reorder codes when reindexing.
</comment><comment author="rjernst" created="2015-08-30T19:53:05Z" id="136178516">And to be clear, my concrete suggestion here is we completely remove this feature, without a plugin. We can rethink the feature, or ability to do manipulation of document pre/post _source and decide what to do there in a later release. But I don't think we should keep around this feature in its current state for the sake of backcompat, it is too new a feature, that IMO is a failed experiment.
</comment><comment author="nik9000" created="2015-08-31T12:58:28Z" id="136363946">&gt; But I don't think we should keep around this feature in its current state for the sake of backcompat, it is too new a feature, that IMO is a failed experiment.

Fair enough. I'm tired of defending this. I don't know of anyone that is using it in a way that they can't just move to a proper transformation pipeline. It fills a niche that nothing else fills but no one needs that niche. And that niche is advanced enough that if you truly need it you can come on github and argue with us to get it back in. At least then we'd have a non-theoretical "good" use case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expand ClusterInfo to provide min / max disk usage for allocation decider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13163</link><project id="" key="" /><description>Today we sum up the disk usage for the allocation decider which is broken since
we don't stripe across multiple data paths. Each shard has it's own private path
now but the allocation deciders still treat all paths as one big disk. This commit
adds allows allocation deciders to access the least used and most used path to make
better allocation decisions upon canRemain and canAllocate calls.

Yet, this commit doesn't fix all the issues since we still can't tell which shard
can remain and which can't. This problem is out of scope in this commit and will be solved
in a followup commit.

Relates to #13106
</description><key id="103586344">13163</key><summary>Expand ClusterInfo to provide min / max disk usage for allocation decider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T20:16:54Z</created><updated>2015-11-22T10:13:28Z</updated><resolved>2015-08-28T12:19:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-27T20:30:32Z" id="135546506">LGTM, but worth adding something mentioning it in the documentation?
</comment><comment author="s1monw" created="2015-08-27T20:33:02Z" id="135547411">&gt; LGTM, but worth adding something mentioning it in the documentation?

@dakrone you mean the main documenation or javadocs?
</comment><comment author="dakrone" created="2015-08-27T20:33:36Z" id="135547681">The main documentation is mostly what I meant (javadocs never hurt though! ;) )
</comment><comment author="s1monw" created="2015-08-27T20:37:31Z" id="135549097">honestly I don't know what I should add there... I am just fixing a bug here not adding functionality?
</comment><comment author="dakrone" created="2015-08-27T20:49:45Z" id="135551621">&gt; honestly I don't know what I should add there... I am just fixing a
&gt; bug here not adding functionality?

Maybe something like this:

"Prior to 2.0.0, when using multiple data paths, the disk threshold
decider only factored in the usage across all data paths (if you had two
data paths, one with 50b out of 100b free (50% used) and another with
40b out of 50b free (80% used) it would see the node's disk usage as 90b
out of 150b). In 2.0.0, the minimum and maximum disk usages are tracked
separately."

I think it's mostly for people upgrading from 1.x to 2.x to understand a
change in behavior.
</comment><comment author="dakrone" created="2015-08-27T20:52:49Z" id="135552267">It's optional though, it doesn't need to block this PR going in
</comment><comment author="s1monw" created="2015-08-27T20:53:26Z" id="135552390">@dakrone yeah I am still trying to digest if people need to know about this impl detail?
</comment><comment author="dakrone" created="2015-08-27T20:54:41Z" id="135552659">Usually I would say no, but because this can make the difference between "why were my shards allocated in 1.7 but not in 2.0?" I think it is a useful thing to add
</comment><comment author="s1monw" created="2015-08-27T20:56:04Z" id="135552942">&gt; Usually I would say no, but because this can make the difference between "why were my shards allocated in 1.7 but not in 2.0?" I think it is a useful thing to add

I will make sure I add this before I close #13106
</comment><comment author="dakrone" created="2015-08-27T20:56:30Z" id="135553022">&gt; I will make sure I add this before I close #13106

Sounds good to me! Thanks!
</comment><comment author="clintongormley" created="2015-08-28T14:51:22Z" id="135796080">@s1monw i assume you're still planning on backporting this to 2.0? I fixed the version label for master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `discovery.zen.join_timeout` default value logic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13162</link><project id="" key="" /><description>We default the value to be 20x the value of a ping timeout, however we only use the legacy ping timeout settings value for the calculation
</description><key id="103573670">13162</key><summary>Fix `discovery.zen.join_timeout` default value logic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T19:02:00Z</created><updated>2015-11-22T10:13:28Z</updated><resolved>2015-08-28T07:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-28T07:44:08Z" id="135663853">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshots not working.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13161</link><project id="" key="" /><description> Hello,

This could be a user issue, but I'm having issues creating snapshots. I've added the path to elasticsearch.yml.  Any ideas on who to fix this? 

Thanks!
# path

path.repo: ["/data/elasticsearch/backups"] 
# command

curl -XPUT 'http://localhost:9200/_snapshot/new_backup' -d '{
     "type": "fs",
     "settings": {
         "compress" : true,
         "location": "/data/elasticsearch/backups"
     }
 }'
# error

{"error":"RepositoryVerificationException[[.kibana] [XXXX, 'RemoteTransportException[[es03][inet[/192.168.0.3:9300]][internal:admin/repository/verify]]; nested: RepositoryVerificationException[[.kibana] a file written by master to the store [/data/elasticsearch/backups] cannot be accessed on the node [[es03][XXXX][es03][inet[/192.168.0.1:9300]]]. This might indicate that the store [/data/elasticsearch/backups] is not shared between this node and the master node or that permissions on the store don't allow reading files written by the master node]; '], [XXXX, 'RemoteTransportException[[es02][inet[/192.168.0.2:9300]][internal:admin/repository/verify]]; nested: RepositoryVerificationException[[.kibana] a file written by master to the store [/data/elasticsearch/backups] cannot be accessed on the node [[es02][XXXX][es02][inet[/192.168.0.2:9300]]]. This might indicate that the store [/data/elasticsearch/backups] is not shared between this node and the master node or that permissions on the store don't allow reading files written by the master node]; ']]]","status":500}
</description><key id="103573550">13161</key><summary>Snapshots not working.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">theocincotta</reporter><labels /><created>2015-08-27T19:01:17Z</created><updated>2015-08-27T19:33:29Z</updated><resolved>2015-08-27T19:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="theocincotta" created="2015-08-27T19:05:03Z" id="135524956">I'm running elasticsearch 1.6.2.
</comment><comment author="imotov" created="2015-08-27T19:05:40Z" id="135525073">@theocincotta In order to register the shared file system repository it is necessary to mount the same shared filesystem to the same location on all master and data nodes. Please see [Snapshot and Restore](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository) documentation for more information. 
</comment><comment author="theocincotta" created="2015-08-27T19:12:22Z" id="135526451">Thanks!

On Thu, Aug 27, 2015 at 12:06 PM, Igor Motov notifications@github.com
wrote:

&gt; @theocincotta https://github.com/theocincotta In order to register the
&gt; shared file system repository it is necessary to mount the same shared
&gt; filesystem to the same location on all master and data nodes. Please see Snapshot
&gt; and Restore
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository
&gt; documentation for more information.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13161#issuecomment-135525073
&gt; .
</comment><comment author="theocincotta" created="2015-08-27T19:14:28Z" id="135527278">Is it possible to just backup one index from one node without dealing with
mounting a filesystem on all the nodes?

On Thu, Aug 27, 2015 at 12:12 PM, Theo Cincotta theocincotta@gmail.com
wrote:

&gt; Thanks!
&gt; 
&gt; On Thu, Aug 27, 2015 at 12:06 PM, Igor Motov notifications@github.com
&gt; wrote:
&gt; 
&gt; &gt; @theocincotta https://github.com/theocincotta In order to register the
&gt; &gt; shared file system repository it is necessary to mount the same shared
&gt; &gt; filesystem to the same location on all master and data nodes. Please see Snapshot
&gt; &gt; and Restore
&gt; &gt; https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository
&gt; &gt; documentation for more information.
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; https://github.com/elastic/elasticsearch/issues/13161#issuecomment-135525073
&gt; &gt; .
</comment><comment author="imotov" created="2015-08-27T19:19:00Z" id="135528331">@theocincotta we are using github for bug reports and feature requests. A much better place to ask questions about Elasticsearch and get help with elasticsearch issues is our discussion forum https://discuss.elastic.co/. At the moment you have to either share a filesystem or use S3 or azure repository to backup to S3 or azure.
</comment><comment author="theocincotta" created="2015-08-27T19:33:29Z" id="135531095">Gotcha. Thanks!

On Thu, Aug 27, 2015 at 12:19 PM, Igor Motov notifications@github.com
wrote:

&gt; @theocincotta https://github.com/theocincotta we are using github for
&gt; bug reports and feature requests. A much better place to ask questions
&gt; about Elasticsearch and get help with elasticsearch issues is our
&gt; discussion forum https://discuss.elastic.co/. At the moment you have to
&gt; either share a filesystem or use S3 or azure repository to backup to S3 or
&gt; azure.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13161#issuecomment-135528331
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bad groovy script delete-by-query makes shard unrecoverable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13160</link><project id="" key="" /><description>An index is now unrecoverable because I ran a Groovyscript delete-by-query against it, and the query errored out. That query is below.

``````
curl -XDELETE 'http://my_elasticsearch_host/my_index_name/my_type/_query' -d '{
 "query": {
   "filtered": {
     "filter": {
       "script": {
         "script": "_source.body.length() == 0"
       }
     }
   }
 }
}
'````
My goal was to delete all records whose `body` field was an empty string. The error I get after restarting the elasticsearch server is:
``````

[2015-08-27 18:30:11,309][WARN ][cluster.action.shard     ] [Schemer] [daily_worker][1] received shard failed for [daily_worker][1], node[6fa-STJgTT6z-uQbzIDErA], [P], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-08-27T18:30:11.274Z], details[shard failure [failed recovery][IndexShardGatewayRecoveryException[[daily_worker][1] failed to recover shard]; nested: RefreshFailedEngineException[[daily_worker][1] Refresh failed]; nested: GroovyScriptExecutionException[NullPointerException[Cannot invoke method length() on null object]]; ]]], indexUUID [AmmH0azOQzOi2wkzy_H_lw], reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[[daily_worker][1] failed to recover shard]; nested: RefreshFailedEngineException[[daily_worker][1] Refresh failed]; nested: GroovyScriptExecutionException[NullPointerException[Cannot invoke method length() on null object]]; ]]

```
Cluster status is 
```

{

```
"status": "red",
"number_of_shards": 5,
"number_of_replicas": 1,
"active_primary_shards": 0,
"active_shards": 0,
"relocating_shards": 0,
"initializing_shards": 3,
"unassigned_shards": 7
```

},````

Ideally I'd like to be able to recover the index. Please let me know what I can do to fix this; feel free to ask away if there's any more information I can give you about this to help you fix the problem.

Edit: Probably wouldn't hurt to note that I'm running ES 1.7.0. Lucene's CheckIndex command says there's no problems with any of the shards in my index.
</description><key id="103568541">13160</key><summary>bad groovy script delete-by-query makes shard unrecoverable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeremybmerrill</reporter><labels /><created>2015-08-27T18:34:33Z</created><updated>2015-08-28T11:44:11Z</updated><resolved>2015-08-28T11:41:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeremybmerrill" created="2015-08-27T19:39:55Z" id="135532248">Workaround: Turn off scripting entirely in your elasticsearch.yml by adding `script.disable_dynamic: true` (or commenting out the `script.disable_dynamic: false`).
</comment><comment author="s1monw" created="2015-08-27T20:05:45Z" id="135537864">hey @jeremybmerrill thanks for opening this. On the one hand this is a huge disaster and I am sorry for it. But this is exactly on of the reasons why we moved delete by query out of core and made it slowish but not prone to these problems. I will use this as an example in the future :)

as far as I can tell you recovered your index with your workaround, right?
</comment><comment author="jeremybmerrill" created="2015-08-27T21:15:37Z" id="135556762">Hi @s1monw, thanks for your response. Yeah, saw it's deprecated, but didn't realize I'd end up in this sort of trouble... 

I'm not sure if, were I to re-enable dynamic scripting, my index would still be broken. But, yeah, I'm back up and running, so I'm okay.
</comment><comment author="s1monw" created="2015-08-27T21:23:23Z" id="135558169">@jeremybmerrill if you `_flush` your index all DBQ in your translog should be processed and you should be fine again.. I'd recommend you not running these kind of queries in DBQ
</comment><comment author="jeremybmerrill" created="2015-08-27T21:36:54Z" id="135562565">I'll give that a try. POST to `/index/_flush`, right?

Yep, learned my lesson :) Will probably stay away from this DBQ thing from now on.
</comment><comment author="s1monw" created="2015-08-28T11:41:13Z" id="135746551">&gt; I'll give that a try. POST to /index/_flush, right?

correct!

I suggest if you need the functionality to use scan &amp; scroll and delete by id using bulk. that's how the new plugin in 2.0 works

I am closing this for now...thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Crashing during snapshot deletion might result in unreferenced data left in repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13159</link><project id="" key="" /><description>It's currently possible to end up with unreferenced data in a snapshot repository, given the following steps:
1. Create a index, `foobar`, with size `X bytes`
2. Snapshot cluster
3. Start deleting the snapshot, crash after deleting `snapshot-{}` and `metadata-{}` files
4. Delete index `foobar`
5. Snapshot cluster again

Normally, step 5 would cause the files no longer referenced by any snapshots do be deleted, but if the underlying index is deleted as well, they won't get cleaned up. In the example above, there would be `X bytes` of disk space used without any snapshot referencing them. Given sufficiently large values of `X`, this could be a significant amount of storage wasted. Even with small amounts of data, this might accrue over time to become significant.

Suggestion: create a `deleting-{}` file as a sibling of the `snapshot-{}` file that gets written before the files referenced by the snapshot gets deleted. When the deletion has been completed, this file should be the last one deleted. These files indicates that a deletion is in progress or have been attempted so it's possible to tell that the snapshot might be in a half-deleted state (so we can avoid using it). It should also enable later snapshot processes to continue the deletion process where the previous left off.
</description><key id="103568209">13159</key><summary>Crashing during snapshot deletion might result in unreferenced data left in repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>bug</label></labels><created>2015-08-27T18:32:22Z</created><updated>2015-08-28T11:59:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add unit test for ShardPath.selectNewPathForShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13158</link><project id="" key="" /><description>In #11185 we fixed this method to be more careful to not allocate a bunch of sudden new shards to a single path on path.data when other paths have nearly the same usable space.

This change just adds a unit test for that method, by mocking the filesystem that `NodeEnvironment` sees, so we can easily fake how much usable space each path on path.data sees.

The hardest part here was the mocking infra (I spent quite a while battling `ProviderMismatchException`!!), and after that I simplified `selectNewPathForShard` to not take `IndexShard` to make unit testing possible, then I created a basic test case to show the original failure.

Aside/rant: the method really is one giant hack, making a silly guess at how large a shard will grow to be when in many cases (existing shards being replicated/reallocated) we know exactly how much disk space it will eventually use up, at least once it's done copying the segments over.  For newly allocated shards we could probably make more informed guesses, e.g. if it's a Marvel shard it will be small, if it's another daily/hourly index we can predict, etc.  And finally we separately need the disk allocator to be able to reallocate shards from one path on path.data to another, within a single node or across nodes ... but these are all separate from just adding this test case :)
</description><key id="103554398">13158</key><summary>Add unit test for ShardPath.selectNewPathForShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T17:12:45Z</created><updated>2015-11-22T10:13:28Z</updated><resolved>2015-08-28T08:15:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-27T19:01:48Z" id="135524347">thanks mike for doing this! I left minor comments - LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a way to validate XContent rendering produces valid JSON/YAML</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13157</link><project id="" key="" /><description>It's common in tests to have a method that tests serialization and deserialization of the actual Java objects, however, it's quite possible to make a mistake in the `toXContent` method that does not produce either valid JSON or YAML. Right now the only real way to do this is with a REST test (other than a lot of string comparison in a unit test), which is a lot of overhead for something like this.

It would be great if we could have some kind of AssertingToXContent wrapper that could test this to ensure we don't ever generate JSON like:

``` json
{ : { "bar": "foo"} }
```
</description><key id="103551988">13157</key><summary>Add a way to validate XContent rendering produces valid JSON/YAML</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>adoptme</label><label>enhancement</label><label>test</label></labels><created>2015-08-27T16:56:34Z</created><updated>2016-12-16T09:56:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-09-14T14:53:51Z" id="247039722">As part of the search refactoring, we introduced quite some tests around query / search request serialization. As part of those we generate a random builder, call `toXContent` against it, parse it, then check that the generated query is equivalent to the initial one. I think this achieves the goal without string comparisons as we compare only the java objects through the equals method (that are also separately tested).

This is not possible though in all cases as some objects implement `ToXContent` although we have no parsing code for them (e.g. response objects that only get printed out). IN that case we should simply call `toXContent` and check its output? Or call it before and after serialization and check that it's valid json and equivalent on both ends?

I am interested in the `AssertingToXContent` idea. How would that work exactly? Wouldn't we need to write specific tests for each class that implements `ToXContent`? Can you elaborate @dakrone ?
</comment><comment author="javanna" created="2016-12-16T09:56:22Z" id="267557932">I am digging the `AssertingToXContent` idea, but I think it is tricky unless the `ToXContent` contract is better defined. See #16347. In many cases the output of `toXContent` is not a valid object. Wrapping it into a new anonymous object would make it valid, unless it starts already its own anonymous object. In other words I don't think this is feasible at the moment. Thoughts @dakrone ?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat should not default to verbose</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13156</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/8927 changed _cat apis to be verbose by default in ES 2.0, based on motivation from https://github.com/elastic/elasticsearch/issues/8922.

The motivation for the change was easing the cluster state inspection experience for users unfamiliar with the command line and ES docs. A noble goal. I certainly empathize with these users.

The opposition argued that experience is quickly gained, and experienced users want the non-verbose behavior, such that over time the majority of users will become frustrated with the new, verbose default.

Despite my empathy for new users, I 100% agree with the opposition. I'd also like to point out that this is an api specifically designed for scripting (see its [introduction](https://www.elastic.co/blog/introducing-cat-api)) and every script that uses it will break in unpredictable ways when the behavior changes. Changing the default will generate a lot of unpleasant work and delay upgrades to 2.0.

The argument that every script should have used `v=false` to protect against future breakage is particularly galling, since this usage was undocumented. It demonstrates a cavalier attitude toward breaking changes that doesn't inspire trust.

Breaking changes, especially breaking changes in APIs (_cat is definitely an API) should only be considered when the original behavior was fundamentally broken. That isn't the case here.

A command-line, user-friendly state inspection tool would be very useful, but _cat is not that tool. Wow your users with a top-like dashboard shipped with the distribution. Don't discourage them by changing the _cat default.

/cc @dakrone @drewr @TwP
</description><key id="103548524">13156</key><summary>_cat should not default to verbose</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">grantr</reporter><labels><label>:CAT API</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T16:38:59Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-08-28T15:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-27T17:50:53Z" id="135505708">I agree with @grantr on this one. The _cat API was built for computers and followed the grand unix tradition of the defaults being right for computers instead of people. We got used to `du -h` and `ls -lh` and, in our cranky unix way, love them. As we love the _cat api.
</comment><comment author="grantr" created="2015-08-27T17:51:40Z" id="135505865">&gt; It demonstrates a cavalier attitude toward breaking changes that doesn't inspire trust.

I should mention that in general breaking changes seem carefully considered, and I appreciate that. This is an unusual case.
</comment><comment author="dakrone" created="2015-08-27T17:55:04Z" id="135506617">And FWIW, even though I authored the original breaking change, I'm okay with either direction as long as it is best for the general userbase
</comment><comment author="kimchy" created="2015-08-27T17:56:31Z" id="135506900">I vote to default back to non verbose mode
</comment><comment author="clintongormley" created="2015-08-28T09:04:18Z" id="135696173">It makes sense to have it default to verbose when typing ad-hoc commands, which is probably something we could change in Sense instead.

OK, let's revert this change.
</comment><comment author="nik9000" created="2015-08-28T16:42:40Z" id="135828673">Fixed in master and 2.0 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0.beta1 - Exception "path.home is not configured" when starting ES in transport and client mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13155</link><project id="" key="" /><description>Hi,
I just installed the new beta version 2.0.0-beta1 and ran some code that used to work with ES 1.7.0. 
I got an exception where it complained that "path.home is not configured". 
Is this something new? Why would I need to configure anything beside the cluster name in transport mode?

Thanks

```
Exception in thread "main" java.lang.IllegalStateException: path.home is not configured
    at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:99)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:97)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:70)
    at org.elasticsearch.client.transport.TransportClient$Builder.build(TransportClient.java:114)
    at com.thesoftwarefactory.persistence.elasticsearch.DataConnection.open(DataConnection.java:62)
```
</description><key id="103545122">13155</key><summary>2.0.beta1 - Exception "path.home is not configured" when starting ES in transport and client mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephane-bastian</reporter><labels><label>:Java API</label><label>adoptme</label><label>blocker</label><label>breaking</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T16:18:17Z</created><updated>2015-12-25T08:08:20Z</updated><resolved>2015-09-08T20:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T09:12:54Z" id="135702357">Discussed this on FixItFriday.  The client node is a real node, and should require everything that any other node requires.  (Need to add docs for this).

The transport client should be light weight and shouldn't require any config etc.  So let's not require path.home for the transport client, and not load any config by default.  If the user needs to load a config file, then that should be explicitly specified.
</comment><comment author="bleskes" created="2015-08-28T09:14:24Z" id="135703213">as a temporary work around, if you are using the transport client, you can turn off config file loading by calling `TransportClient.Builder#loadConfigSettings(false)`
</comment><comment author="bleskes" created="2015-08-28T12:11:04Z" id="135754504">bleh, it turns out I was wrong. Disabling loading the config settings still results in the exception (we throw it before checking the settings). Sorry for the noise.
</comment><comment author="kimchy" created="2015-09-02T07:51:12Z" id="136967094">I chatted to @rjernst about it a bit, and he suggested something that I agree with. In the case of `TransportClient`, we can simply not bind or handle `Environment`. I like the simplicity of this approach, but we need to double check with our plugins to make sure that the ones that make sense in the context of a transport client will still work (/cc @jaymode).

There is a whole other question around Node(Client), but we should open a different issue where we properly define how things should work when "embedding"  a Node.
</comment><comment author="kimchy" created="2015-09-02T07:56:22Z" id="136968670">@Andy-Peng service.bat should work unrelated to TransportClient, can you open a new issue with what fails for you in this context?
</comment><comment author="rmuir" created="2015-09-02T08:01:33Z" id="136970133">service.bat problems might be related to https://github.com/elastic/elasticsearch/issues/13247
</comment><comment author="clintongormley" created="2015-09-07T11:07:57Z" id="138270883">@rjernst are you working on this?
</comment><comment author="rjernst" created="2015-09-08T00:33:41Z" id="138397414">@clintongormley I just opened a PR for this: #13383
</comment><comment author="jaskmar" created="2015-09-11T13:09:41Z" id="139541322">Is there any work around for this bug for now?
I have set `.put("path.home", "/")` but it is very, very ugly.
</comment><comment author="sweetest" created="2015-11-03T01:46:53Z" id="153214466">I'm still seeing this with elasticsearch 2.0.0, Mac OS Yosemite.

maven dependency 

```
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
        &lt;version&gt;2.0.0&lt;/version&gt;
        &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
```

code that triggers error

```
    node = NodeBuilder.nodeBuilder().data(true).settings(
            Settings.builder()
                    .put(ClusterName.SETTING, "test")
                    .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
                    .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
                    .put(EsExecutors.PROCESSORS, 1)
                    .put("index.store.type", "memory")
    ).build();
```

error stack trace

```
java.lang.IllegalStateException: path.home is not configured
at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:99)
at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:82)
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
```
</comment><comment author="rjernst" created="2015-11-03T02:03:31Z" id="153216833">@sweetest That is expected since you are starting a node. The PR that closed this removed the need for `path.home` to be specified for `TransportClient`s. But any node needs to have `path.home`. 
</comment><comment author="jasontedor" created="2015-11-03T02:03:35Z" id="153216838">&gt; `NodeBuilder.nodeBuilder().data(true)`

That's not building a client node, that's building a data node for which `path.home` _must_ be configured (how else will the node know where to put its data?).
</comment><comment author="jtal" created="2015-11-06T01:38:21Z" id="154258486">&gt; how else will the node know where to put its data?

Doesnt it just use path.data to know where to put the data? 

How does one set path.home? When I google it I just find this thread and the directory layout docs.

Also, I am trying to create a client node and get the same error.

``` Java
nodeBuilder()
.settings(Settings.settingsBuilder().put("http.enabled", false))
.client(true)
.node();
```

Note: I am also trying to upgrade from 1.7.1 to 2.0.0.
</comment><comment author="jasontedor" created="2015-11-06T02:49:08Z" id="154268140">&gt; Doesnt it just use path.data to know where to put the data?

Yes, and by default that is `${path.home}/data`.

&gt; Also, I am trying to create a client node and get the same error.

Are you sure that you're using the released version of Elasticsearch 2.0.0?

&gt; Note: I am also trying to upgrade from 1.7.1 to 2.0.0.

Can you give a small reproduction?
</comment><comment author="hudsonb" created="2015-11-06T20:05:18Z" id="154518207">I am having the same issue when creating a Node(Client):

```
NodeBuilder.nodeBuilder()
                    .settings(Settings.builder()
                                      .put("discovery.zen.ping.multicase.enabled")
                                      .put("discovery.zen.ping.unicast.hosts", hosts)
                                      .put("cluster.name", cluster)
                                      .put("node.name", nodeName)
                                      .build())
                    .client(true)
                    .node();
```

Results in "path.home is not configured"
</comment><comment author="rjernst" created="2015-11-06T20:11:39Z" id="154519792">Adding `path.home` when building a node would be done like this:

```
NodeBuilder.nodeBuilder()
    .settings(Settings.builder()
        .put("path.home", "/path/to/elasticsearch/home/dir")
    .node();
```
</comment><comment author="jtal" created="2015-11-06T20:12:32Z" id="154519994">Yes I'm fairly sure but is there a way I can have the client report what version it is from within the app?

Also, looking at the 2.0.0 source code constructor for Node (line 128) this happens:

```
    Environment tmpEnv = InternalSettingsPreparer.prepareEnvironment(pSettings, null);
```

Then Environment constructor requires path.home (line 96):

```
    if (settings.get("path.home") != null) {
        homeFile = PathUtils.get(cleanPath(settings.get("path.home")));
    } else {
        throw new IllegalStateException("path.home is not configured");
    }
```

Am I still misunderstanding something? If not, what should I set path.home to when I'm not storing data and how does one set path.home since its not in the client node doc example.

 https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/node-client.html

Thank you.

Server:

```
10:01:27-jtal~/pkg/elasticsearch-2.0.0$ ps aux | grep elasticsearch
jtal 18858  0.0  0.0  15940   944 pts/7    S+   10:01   0:00 grep --color=auto elasticsearch
jtal 20994  0.5  1.0 6751776 340708 pts/10 Sl   Nov05   8:13 /home/jtal/pkg/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/home/jtal/pkg/elasticsearch-2.0.0 -cp /home/jtal/pkg/elasticsearch-2.0.0/lib/elasticsearch-2.0.0.jar:/home/jtal/pkg/elasticsearch-2.0.0/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d

10:01:36-jtal~/pkg/elasticsearch-2.0.0$ sha1sum ~/pkg/elasticsearch-2.0.0.zip
13255b26a4d47d557a69fc85c24c117a1d9a33a5  /home/jtal/pkg/elasticsearch-2.0.0.zip
```

Client:

```
libraryDependencies ++= Seq(
  javaJdbc,
  javaEbean,
  cache,
  javaWs,
  "org.pac4j" % "play-pac4j_java" % "1.3.0",
  "org.pac4j" % "pac4j-cas" % "1.7.0",
  "org.elasticsearch" % "elasticsearch" % "2.0.0"
)
```
</comment><comment author="dadoonet" created="2015-11-06T20:13:41Z" id="154520233">Yes the doc needs to be fix.
</comment><comment author="rjernst" created="2015-11-06T20:18:57Z" id="154521250">&gt; what should I set path.home to when I'm not storing data

Set it to any directory. If you are explicitly setting other dirs like path.logs, then nothing should be written to it.
</comment><comment author="hrahal" created="2015-11-12T16:04:14Z" id="156149332">`sudo chown -R elasticsearch:elasticsearch /var/lib/elasticsearch/`

worked for me on ES 2.0.0 
</comment><comment author="zy840055899" created="2015-11-23T08:36:29Z" id="158876845">&#21769;&#65292;&#36824;&#26159;&#27809;&#35299;&#20915;~
</comment><comment author="jeyendranbalakrishnan" created="2015-11-25T17:46:35Z" id="159685553">Is there any fix for this? I am using the release version of elasticsearch 2.0.0 on my laptop, both for the running es server, and for the dependency in my source code where I use es as a client.
I set 
path.home: ${ES_HOME}
in my src/main/resources/elasticsearch.yml config file, and set 
ES_HOME=/path/to/my/es/install
in my environment. 
But running my client code on the laptop still gives the same "path.home is not configured" exception.
Funnily. I don't have a path.home setting in my running server config file (${ES_HOME}/config/elasticsearch.yml.
However, this does not prevent the server from starting up and running properly, responding correctly to curl commands.
</comment><comment author="selfchanger" created="2015-12-24T11:00:41Z" id="167093193"> path.home is not configured
i also have this problem,for details see https://github.com/elastic/elasticsearch/issues/15660
can someone help me?
</comment><comment author="betaxer" created="2015-12-25T05:55:31Z" id="167197421">I am new to ES, and I also faced this problem, any suggested solutions? 
</comment><comment author="selfchanger" created="2015-12-25T08:08:19Z" id="167208418">solution:
add "-Des.path.home=D:\elasticsearch-2.1.0" in your VM options,
see http://blog.csdn.net/jianjun200607/article/details/49821813#reply
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[analysis] Kuromoji: updating user dictionary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13154</link><project id="" key="" /><description>_From @gabriel-tessier on July 17, 2015 2:56_

Hi,
I need to update the user dictionary and I usually open close the index to make the change apply.
One question: Is it the only way to update the dict cause in case of typo (missing a , or one bad space) in the file the index don't open at all and on production server it's not an option.
first question (for confirmation): is it the good way to do? open close the index to reload the dict.
second one: is there a way to have an option like check config for apache [apachectl configtest]? so we can open close the index without fear!!

Thanks.

_Copied from original issue: elastic/elasticsearch-analysis-kuromoji#65_
</description><key id="103543389">13154</key><summary>[analysis] Kuromoji: updating user dictionary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Kuromoji</label><label>discuss</label></labels><created>2015-08-27T16:08:52Z</created><updated>2015-08-28T09:18:23Z</updated><resolved>2015-08-28T09:18:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T16:08:53Z" id="135480080">_From @johtani on July 29, 2015 9:25_

Hi,
Now, I think we don't have a way to reload the dict without open/close.
Because, the settings of mappings is load only in a timing to create index. 
Question about "an option like check config for apache". Does this mean checking only user dictionary?
</comment><comment author="dadoonet" created="2015-08-27T16:08:53Z" id="135480084">_From @gabriel-tessier on July 29, 2015 10:56_

Hi, thanks for the reply,

Question about "an option like check config for apache". Does this mean checking only user dictionary?
I don't know what it's better and how it's works from inside but concerning the plugin yes it can be a useful option.
When you said only user dictionary does it also include synonyms files, stop words, mapping etc... or it's a different system?

Also read about this blog : https://www.elastic.co/blog/support-in-the-wild-my-biggest-elasticsearch-problem-at-scale
It's written that closing the index remove the fielddata so if I need to update the dictionary several time a day and open close the index, it can make the search on this index more slow.

Thanks again for your time.
</comment><comment author="dadoonet" created="2015-08-27T16:08:54Z" id="135480086">_From @johtani on July 30, 2015 9:27_

Yes, if index close and open, then it can make the search on this index more slow.

However, if you update the dictionary several time a day, you have to re-index the existed data before updating the dictionary.
Changing the dictionary means changing the output of tokens. 
Some words can not match documents using old dictionary.

BTW, I think it is useful that we have the validation of mapping before creating index.
</comment><comment author="dadoonet" created="2015-08-27T16:08:54Z" id="135480090">_From @gabriel-tessier on July 30, 2015 9:45_

Hi,

&gt; BTW, I think it is useful that we have the validation of mapping before creating index.

Cool!! Thanks a lot!

&gt; you have to re-index the existed data before updating the dictionary.

it's re-index **after** updating the dictionary??

Thanks again it'll be very helpful!
</comment><comment author="dadoonet" created="2015-08-27T16:08:54Z" id="135480092">_From @johtani on July 30, 2015 9:46_

Yes, "after updating the dictionary".
</comment><comment author="clintongormley" created="2015-08-28T09:18:23Z" id="135705669">I'm not sure there are any action points to take away from this issue.  It looks like a duplicate of others, to do with synonyms etc.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Call `beforeIndexShardCreated` listener earlier in `createShard`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13153</link><project id="" key="" /><description>Some listeners may need to do work before a shard's path is
accessed (such as creating the directory in a plugin), so the listener
should be called before anything happens (as its name implies).
</description><key id="103543169">13153</key><summary>Call `beforeIndexShardCreated` listener earlier in `createShard`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Core</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T16:07:34Z</created><updated>2016-02-21T20:22:05Z</updated><resolved>2015-08-27T17:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-27T16:50:24Z" id="135490317">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] provide more examples for smartcn analysis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13152</link><project id="" key="" /><description>_From @twn39 on August 14, 2015 8:14_

Hi, I'm from china, and recently learning the elasticsearch, but I found it's hard to configure the smartcn analysis.I google this but find little documents. Could you help me or give me some documents, thank you very much.

_Copied from original issue: elastic/elasticsearch-analysis-smartcn#44_
</description><key id="103540855">13152</key><summary>[doc] provide more examples for smartcn analysis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Smartcn</label><label>adoptme</label><label>docs</label></labels><created>2015-08-27T15:56:14Z</created><updated>2016-01-15T12:40:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T15:56:14Z" id="135476867">I agree we should add more doc. in the meantime, you can read this https://github.com/elastic/elasticsearch-analysis-kuromoji
</comment><comment author="dadoonet" created="2015-08-27T15:56:14Z" id="135476871">_From @twn39 on August 14, 2015 12:23_

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0 java api changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13151</link><project id="" key="" /><description>Some not yet documented java api changes.

GetAliasesResponse.getAliases().keysIt() returns a guava UnmodifiableIterator iso a elasticsearch one

ImmutableSettings removed; use Settings.builder() iso ImmutableSettings.builder()

InetSocketTransportAddress(String, int) removed
</description><key id="103540197">13151</key><summary>2.0 java api changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">brackxm</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T15:52:40Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-09-08T19:26:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tarunsapra" created="2015-08-30T10:35:46Z" id="136125502">+1
</comment><comment author="clintongormley" created="2015-08-30T11:46:22Z" id="136130602">@dadoonet could you add these to the java API docs and migration docs please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] Stempel: no information how to reimplement the analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13150</link><project id="" key="" /><description>_From @merito on June 11, 2015 8:16_

Hi,

I would like to reimplement the analyzer, for example add my synonyms, but there is no available filter list, for official language analyzers you can find it here - [elastic documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#english-analyzer). Could you add this to the docs?

_Copied from original issue: elastic/elasticsearch-analysis-stempel#43_
</description><key id="103539605">13150</key><summary>[doc] Stempel: no information how to reimplement the analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Stempel</label><label>adoptme</label><label>docs</label></labels><created>2015-08-27T15:49:28Z</created><updated>2015-12-02T23:06:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T15:49:28Z" id="135474539">_From @mod3st on June 17, 2015 14:55_

+1
</comment><comment author="dadoonet" created="2015-08-27T15:49:29Z" id="135474541">_From @jeanbelhache on July 16, 2015 7:42_

+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose ICUCollationKeyAnalyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13149</link><project id="" key="" /><description>_From @clintongormley on May 7, 2014 15:12_

The `icu_collation` filter is deprecated, and should be replaced by the ICUCollationKeyAnalyzer (exposed as the `icu_collation_key` analyzer).   However, I would recommend making some changes at the same time:

Use `locale` instead of `language`/ `country`/ `variant`, eg : 
- `en`
- `en_US`
- `de_DE@collation=phonebook`
- `zh_CN@collation=pinyin`

Keep `strength`, `rules`, `decomposition`, `alternate`, `case_level`, `case_first`, `numeric`, `variable_top`.

It looks like `hiraganaQuaternaryMode` is no longer used by ICU (but not sure about that)

If no locale is specified, then it should default to ULocale.ROOT (ie DUCET) rather than what it does currently, which is to default to the current locale active in the JVM.

Finally, it would be good to add the `truncate` token filter into the analyzer so that users can truncate a potentially long original string into (eg) the first 10 characters, so that it uses less memory.

NOTE: We need to check that binary sorting is supported correctly in Elasticsearch.  @jpountz thinks this is the case, but needs to be confirmed.

_Copied from original issue: elastic/elasticsearch-analysis-icu#28_
</description><key id="103537597">13149</key><summary>Expose ICUCollationKeyAnalyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis ICU</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-27T15:38:36Z</created><updated>2017-07-07T07:47:22Z</updated><resolved>2017-07-07T07:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T15:38:37Z" id="135471925">_From @clintongormley on May 7, 2014 15:45_

There is an issue with binary sort values. See http://github.com/elasticsearch/elasticsearch/issues/6077
</comment><comment author="GlenRSmith" created="2017-07-06T20:30:33Z" id="313510604">@dadoonet @clintongormley Is this addressed with the release of ICU Collation Keyword Fields in v5.5.0?</comment><comment author="jpountz" created="2017-07-07T07:47:21Z" id="313612306">@GlenRSmith It is! Closing. For reference, https://github.com/elastic/elasticsearch/pull/24126 is the PR that introduced the ICU collation keyword fields.

However this issue had some good points, and I opened https://github.com/elastic/elasticsearch/issues/25587 to address the default locale issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add icu_collation field type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13148</link><project id="" key="" /><description>_From @clintongormley on May 7, 2014 15:40_

Collation fields are used just for sorting - they don't need to be indexed or stored.  

We can use a new `icu_collation_key` field type which accepts the same parameters as the ICUCollationKeyAnalyzer, and isn't indexed but stores its values as doc values, using  ICUCollationDocValuesField.

This field type would need to serialize its sort keys as base64 for inclusion in the JSON response.

_Copied from original issue: elastic/elasticsearch-analysis-icu#29_
</description><key id="103537138">13148</key><summary>Add icu_collation field type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis ICU</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-27T15:36:17Z</created><updated>2016-01-28T13:07:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T15:36:18Z" id="135471337">_From @clintongormley on May 10, 2014 13:45_

Also, should throw an error if the named collation is not found.  Currently this is unchecked:

&gt; If the search fails because no such key is present in any of ICU's locale data (e.g., "de@collation=funky"), the service returns a collator implementing UCA and the return UErrorCode is U_USING_DEFAULT_WARNING.
</comment><comment author="dadoonet" created="2015-08-27T15:37:07Z" id="135471526">See also #6077
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ICU backwards compatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13147</link><project id="" key="" /><description>_From @clintongormley on May 10, 2014 13:50_

Currently, if ICU changes, it could mean needing to reindex all your data.  The ICU docs have this to say:

Versioning

Sort keys are often stored on disk for later reuse. A common example is the use of keys to build indexes in databases. When comparing keys, it is important to know that both keys were generated by the same algorithms and weightings. Otherwise, identical strings with keys generated on two different dates, for example, might compare as unequal. Sort keys can be affected by new versions of ICU or its data tables, new sort key formats, or changes to the Collator. Starting with release 1.8.1, ICU provides a versioning mechanism to identify the version information of the following (but not limited to),
- The run-time executable
- The collation element content
- The Unicode/UCA database
- The tailoring table

The version information of Collator is a 32-bit integer. If a new version of ICU has changes affecting the content of collation elements, the version information will be changed. In that case, to use the new version of ICU collator will require regenerating any saved or stored sort keys. However, since ICU 1.8.1. it is possible to build your program so that it uses more than one version of ICU. Therefore, you could use the current version for the features you need and use the older version for collation.

_Copied from original issue: elastic/elasticsearch-analysis-icu#30_
</description><key id="103536912">13147</key><summary>ICU backwards compatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis ICU</label><label>adoptme</label></labels><created>2015-08-27T15:35:08Z</created><updated>2016-01-28T13:06:59Z</updated><resolved>2016-01-28T13:06:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T13:06:59Z" id="176175325">The right solution here is to reindex your data.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for ICUTokenizerFactory and customizing the rule file in ICU tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13146</link><project id="" key="" /><description>_From @rslinckx on October 21, 2014 10:35_

There has been a commit in lucene (http://svn.apache.org/viewvc?view=revision&amp;revision=1416629) that adds a ICUTokenizerFactory which allows to create a ICUTokenizer with a special config argument enabling the customization of the rule based iterator by providing a custom rules files.

Is it possible to expose that argument in the elasticsearch icu plugin ? This would allow to define an elasticsearch tokenizer like:

```
{
    "index" : {
        "analysis" : {
            "tokenizer" : {
                "custom-icu-tokenizer" : {
                    "type" : "icu_tokenizer",
                    "rulefiles": "Latn:Latin-break-only-on-whitespace.rbbi"
                }
            }
        }
    }
}
```

This allows tailoring the output of the tokenization process (in this example, not splitting on hyphens in the latin script)

Also see the original solr discussion here: https://issues.apache.org/jira/browse/SOLR-4123

_Copied from original issue: elastic/elasticsearch-analysis-icu#42_
</description><key id="103536719">13146</key><summary>Add support for ICUTokenizerFactory and customizing the rule file in ICU tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis ICU</label><label>adoptme</label><label>feature</label><label>low hanging fruit</label><label>v5.0.0-alpha2</label></labels><created>2015-08-27T15:34:05Z</created><updated>2016-04-22T19:59:17Z</updated><resolved>2016-04-22T19:59:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T15:34:06Z" id="135470769">_From @d-claassen on August 27, 2015 11:32_

Is there any progress on this? This could also help us with keywords like c++, c# and R&amp;D. With the current tokenizer, all these special characters are being removed.
</comment><comment author="xuzha" created="2015-09-18T07:45:24Z" id="141372213">I put a PR for this.
I copied two RBBI files and borrowed a test cases from Lucene for testing, I'm not sure if this is appropriate.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>no inFilter alternative in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13145</link><project id="" key="" /><description>It is strange that QueryBuilders.inQuery() is removed in 2.0.
The documentation states that filters from FilterBuilders are available in QueryBuilders.
However that is not the case for FilterBuilders.inFilter().
Apparently QueryBuilders.inQuery() has been removed as it was deprecated. However FilterBuilders.inFilter() was not deprecated.
Code using FilterBuilders.inFilter() but not QueryBuilders.inQuery() never noticed the (then unrelated) deprecation in 1.x and is left without an alternative in 2.0
</description><key id="103531819">13145</key><summary>no inFilter alternative in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">brackxm</reporter><labels /><created>2015-08-27T15:09:09Z</created><updated>2017-06-05T14:00:22Z</updated><resolved>2015-08-27T15:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-27T15:20:45Z" id="135466793">Bleh. Sorry about that. it looks like `inFilter` and was just an alias for `termsFilters` and `inQuery` was just an alias for `termsQuery`. So in 2.0 there is just `termsQuery` and you should just use it if you want that "in any of these terms" behavior.

I'm going to close this because this bug report should be enough of a trail for people to follow to successfully migrate. If it isn't I'll add a note to https://www.elastic.co/guide/en/elasticsearch/reference/master/_query_dsl_changes.html
</comment><comment author="nickminutello" created="2016-05-18T08:28:36Z" id="219959848">Perhaps everyone has moved to 2.x - so there isn't much demand anymore - but I did notice this gem of info was missing from https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_java_api_changes.html
</comment><comment author="martin-g" created="2017-06-05T12:33:41Z" id="306176553">One feature that seems to be missing is that `InFilter` had a method `execution("and|or")` but this is not possible with `termsQuery()`.
</comment><comment author="nik9000" created="2017-06-05T14:00:22Z" id="306194639">I hadn't noticed that! If you want AND then I think the normal way to get that now is with a `bool` query with many `term` queries in the `must` clause.

It is interesting to know that the optimizations that the `terms` query does for OR aren't all that useful if you don't have a fair number of terms. So if you have less than 16 terms it is rewritten internally into a `bool` query with many `term` queries in the `should` clause.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `readonly` option for repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13144</link><project id="" key="" /><description>Closes #7831
Closes #11753
</description><key id="103529030">13144</key><summary>Add `readonly` option for repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-27T14:55:41Z</created><updated>2015-08-28T00:10:56Z</updated><resolved>2015-08-28T00:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T15:17:04Z" id="135465887">It looks good to me. 

Side question: wondering in the future if we can imagine a way to snapshot (write) over HTTP so have writeable URL repositories?
</comment><comment author="imotov" created="2015-08-27T16:12:38Z" id="135481525">@dadoonet URL repository is ignoring this flag. So, it still cannot write anything even if you set readonly to true for it. So, it's not a common option in a sense. So, maybe, I should add it to other repos that support it (s3, azure). What do you think?
</comment><comment author="dadoonet" created="2015-08-27T16:13:35Z" id="135481771">&gt; So, maybe, I should add it to other repos that support it (s3, azure).

++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>migration_x_x.asciidoc never merges cleanly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13143</link><project id="" key="" /><description>I get lots of version conflicts around files like migration_x_x.asciidoc. It seems like this comes from people constantly tacking new things on the end. Could we, maybe, build this file by cating together a directory of scripts or something like that? Something where git is going to be able to merge easily.
</description><key id="103525244">13143</key><summary>migration_x_x.asciidoc never merges cleanly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2015-08-27T14:38:14Z</created><updated>2015-08-28T12:02:37Z</updated><resolved>2015-08-27T18:43:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-27T18:43:07Z" id="135519045">I've just spent a long time rewriting the 2.0 migration docs and breaking them up into separate pages, see https://github.com/elastic/elasticsearch/tree/master/docs/reference/migration/migrate_2_0

This change may account for the conflicts, but it should make conflicts less likely in the future.

The migration docs need to be as readable as any other page in the docs, I really don't want some long disorganised list (which is pretty much what it was before the reorganisation).
</comment><comment author="nik9000" created="2015-08-27T18:57:34Z" id="135523251">&gt; The migration docs need to be as readable as any other page in the docs, I really don't want some long disorganised list (which is pretty much what it was before the reorganisation).

Yeah. I get that. I just find that whenever I have a pull request that's been sitting around for more than a few days it merges cleanly _except_ for migration_2_0 or migration_2_1. I remember you're work to move the change into multiple docs and it helped. But I'm seeing it again with 2.1. I'll poke this with a concrete example if it happens again.
</comment><comment author="clintongormley" created="2015-08-28T12:02:37Z" id="135751131">Not always possible but perhaps adding the breaking change into a particular section, instead of just appending, will make conflicts less likely
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: match_phrase_prefix to take boost into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13142</link><project id="" key="" /><description>The match_phrase_prefix query properly parses the boost etc. but it loses it in its rewrite method. Fixed that by setting the orginal boost to the rewritten query before returning it. Also cleaned up some warning in MultiPhrasePrefixQuery.

Closes #13129
</description><key id="103510869">13142</key><summary>Query DSL: match_phrase_prefix to take boost into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T13:37:11Z</created><updated>2016-03-10T18:13:57Z</updated><resolved>2015-09-03T18:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-01T07:38:33Z" id="136616076">Left one minor comment, but other than that it looks good to me. Good catch!
</comment><comment author="jpountz" created="2015-09-01T07:38:54Z" id="136616128">Sorry, closed by mistake.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analzyer configuration for filters is too lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13141</link><project id="" key="" /><description>If you accidentally specify `filters` instead of `filter`, because the `_analyze` API wants you to specify filters using the `filters` parameter, you will be able to create the index, but without filters being applied.

```
DELETE /crunchbase

PUT /crunchbase
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "my_analyzer" : {
          "tokenizer" : "whitespace",
          "filters" : [ "lowercase" ]
        }
      }
    }
  }
}

GET /crunchbase/_analyze?analyzer=my_analyzer&amp;text=FOO
```

will return `FOO`

Happened on 1.7
</description><key id="103497239">13141</key><summary>Analzyer configuration for filters is too lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Analysis</label></labels><created>2015-08-27T12:28:17Z</created><updated>2015-08-27T18:36:29Z</updated><resolved>2015-08-27T18:36:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-27T13:05:48Z" id="135418784">reason for this seems to be that we load the content as settings, without actually checking it.. easy to reproduce, from then on debugging

``` java
    @Test
    public void testThatParsingIsNotLenient() throws Exception {
        try {
            prepareCreate("test").setSettings(
                    jsonBuilder()
                            .startObject().startObject("settings").startObject("analysis").startObject("analyzer")
                            .startObject("my_analyzer").field("tokenizer", "whitespace").field("filters", "lowercase").endObject()
                            .endObject().endObject().endObject().endObject())
                    .get();
            fail("Expected parsing exception because of specifying filters instead of filter");
        } catch (Exception e) {
            // check type of exception
        }
    }
```
</comment><comment author="clintongormley" created="2015-08-27T18:36:29Z" id="135517511">Yeah, this should be part of the big settings cleanup in https://github.com/elastic/elasticsearch/issues/6732
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot Delete In server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13140</link><project id="" key="" /><description>I hosted my elastic in AWS but, I cannot delete index.
my elasctic version 1.7.1
</description><key id="103493692">13140</key><summary>Cannot Delete In server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tommoholmes10</reporter><labels /><created>2015-08-27T12:04:51Z</created><updated>2015-08-27T12:13:39Z</updated><resolved>2015-08-27T12:13:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-27T12:13:38Z" id="135403845">Hi @tommoholmes10 

Please ask questions like this in the forum http://discuss.elastic.co/ (and I'd provide more information when you do, otherwise you're unlikely to get a response)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update query_dsl.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13139</link><project id="" key="" /><description>Fixed typo.'
</description><key id="103478222">13139</key><summary>Update query_dsl.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">dantuffery</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T10:41:35Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-27T10:48:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-27T10:48:28Z" id="135378469">thanks @dantuffery merged into master and 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM build is broken and should be better tested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13138</link><project id="" key="" /><description>in the past we had so many problems with RPM and it building, testing verification etc. You have to have a specific OS to even build a valid package (at least that was the case but who knows until you install it) The maven task claims to sign the RPM but doesn't and `rpm -K` happily passes if the package is unsigned. To work around this we have to sign the rpm after the fact with some cryptic rpm command where I feel not happy with since it modifies the actual already build package that comes out of our mvn build. This means all the `.asc .md5 .sha1`we produce in the build are useless and broken. 
We don't have tests for this package that run on a regular basis neither do we tests it on release.  This should either be fully tested like our tar and gz file and the signing must work out of the box or this format must go away. I don't care how many people would like to have it or not, it's untested and I rather ship nothing than an untested package.
</description><key id="103457789">13138</key><summary>RPM build is broken and should be better tested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>blocker</label><label>build</label><label>Meta</label><label>v2.0.0</label></labels><created>2015-08-27T08:45:44Z</created><updated>2015-10-06T15:01:53Z</updated><resolved>2015-10-06T15:01:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-27T09:52:12Z" id="135368000">We can't not offer RPM....
</comment><comment author="electrical" created="2015-08-27T10:09:07Z" id="135370577">I tried out the rpm signing process and still works.
Will have to check how the release script is being called and if all required arguments are being passed.
</comment><comment author="electrical" created="2015-08-27T10:17:38Z" id="135372124">The pre-release script sets `-Dgpg.keyname="%s"` which should be `-Dgpg.key="%s"`
because of this the signing didn't happen..
</comment><comment author="electrical" created="2015-08-27T10:25:58Z" id="135374354">deb package is not signed either so will have to fix that as well. will update when i found all params / commands.
</comment><comment author="s1monw" created="2015-08-27T11:14:15Z" id="135389000">folks this is too cumbersome I hate to say it but fixing the arguments is not fixing the problem it's just continuing to fool around. 
</comment><comment author="s1monw" created="2015-08-27T11:15:45Z" id="135389716">@electrical I wonder why it's `gpg.key` their documentation states otherwise: http://www.mojohaus.org/rpm-maven-plugin/adv-params.html
</comment><comment author="electrical" created="2015-08-27T11:16:43Z" id="135390129">@s1monw `&lt;keyname&gt;${gpg.key}&lt;/keyname&gt;`
this made me try out -Dgpg.key and works.
</comment><comment author="electrical" created="2015-08-27T12:09:39Z" id="135403235">I was able to build rpm and deb packages signed and well with the following options:

`-Drpm.sign=true -Ddeb.sign=true -Dgpg.passphrase=password -Dgpg.key=key`
</comment><comment author="electrical" created="2015-08-27T12:11:18Z" id="135403509">Validation commands:

RPM:

```
$ rpm -K ./distribution/rpm/target/releases/elasticsearch-2.1.0-SNAPSHOT.rpm
./distribution/rpm/target/releases/elasticsearch-2.1.0-SNAPSHOT.rpm: rsa sha1 (md5) pgp md5 OK
```

DEB:

```
$ dpkg-sig -c elasticsearch-2.0.0-beta1.deb
Processing elasticsearch-2.0.0-beta1.deb...
GOODSIG _gpgbuilder 46095ACC8548582C1A2699A9D27D666CD88E42B4 1440670973
```
</comment><comment author="electrical" created="2015-08-27T12:40:47Z" id="135409606">Forgot to mention 1 thing. we also need to add the following to the `deb/pom.xml` at the signing part:

`&lt;signRole&gt;builder&lt;/signRole&gt;`
</comment><comment author="nik9000" created="2015-08-27T14:03:02Z" id="135444647">Not offering RPM or DEB packages would be a blow for adoption. I might not be here if it weren't for the deb package and from what I can tell these packages are widely used.

I'm with that its certainly a pain to build them though.

&gt; We don't have tests for this package that run on a regular basis neither do we tests it on release.

Because no one has turned on the vagrant tests in Jenkins.... They are there and test the rpm but I don't think they'd check the gpg signing. We could certainly test against some self signed stuff.

&gt; You have to have a specific OS to even build a valid package

We could work around that with vagrant if we really really wanted to. Right now you need either OSX via homebrew or Linux with rpmbuild installed. Certainly Centos/Fedora/RHEL/OEL would do there. There seem to be rpmbuild packages for Debian and Ubuntu but I don't know that they would work. They might.
</comment><comment author="s1monw" created="2015-08-27T18:50:06Z" id="135520566">Let me clarify I do want to ship RPM and .deb but  the current state of affairs is not even near good. If we have test but they don't run and they apparently didn't even run on a RC then the are not an improvement in this situations. @electrical your comments regarding the commands are not fixing the issue. I had these commands ready as well but thanks anyway. I feel like the current state of these packages are not what I want folks to run in production. If we don't have tests that we run with realistic builds they we have to consider the packages broken. I don't want to ship any broken packages. Nobody should feel like I am finger-pointing since this is not the case at all. I do care about those packages a lot that's why I open these drastic packages.

&gt; We can't not offer RPM....

this is just wrong. We can't offer potentially broken RPM... and I will not offer RPM if they are potentially broken. This must be rock solid but it isn't, we can't rely on manual testing here. Hence the blocker.
</comment><comment author="nik9000" created="2015-08-28T14:05:37Z" id="135782775">I'm going to try to enumerate the issues so we can fix them:
1. We don't run the rpm and deb tests consistently. I've opened an issue with the Elastic infrastructure folks for this. And made sure its obvious that I consider it an important issue.
2. The deb/rpm tests don't cover artifact signing.
3. Artifact signing with maven is jangly and doesn't actually work properly right now.

Is that all?
</comment><comment author="dhollinger" created="2015-08-28T14:14:27Z" id="135784524">@s1monw , while you are right in that you shouldn't offer broken packages, I can definitely tell you there are multiple organizations I have had involvement with that wouldn't even consider Elasticsearch if there are packages that can be installed and tracked using a package management system such as YUM or APT. 

Tracking installed software tends to be an audit requirement in many (but not all) organizations.

Not offering a broken RPM is great, not offering one at all would be unacceptable.
</comment><comment author="s1monw" created="2015-08-28T15:55:37Z" id="135813475">@dhollinger I think you are not getting it right here at all I am sorry

&gt; Not offering a broken RPM is great, not offering one at all would be unacceptable.

if you get something that wipes your entire data because we shipped it just for the sake of it I'd be very unhappy. I am listening to all the rants and dangerous statements here until this is either removed or properly tested then we can release again. This is the mindset we have to live to ship good software either it's tested or we don't ship it. Since this is RPM and .deb I am even more concerned for the reasons you mentioned. But it's black and white sorry! IF we can't fix it I will remove it myself.
</comment><comment author="s1monw" created="2015-08-28T16:02:58Z" id="135815217">@nik9000 I think we should add to the list:
- tests and/or release scripts must verify that packages are signed and fail if not.
- test must verify that packages are cleanly un-installable
- test must verify that packages can be upgraded from a previous version including the index if it already exists. (don't wipe any data)
- test must verify that packages load all relevant user provided configurations 

not sure how much of this we already do but I think that would be a good start?
</comment><comment author="nik9000" created="2015-08-28T16:12:02Z" id="135821442">- [ ]  tests and/or release scripts must verify that packages are signed and fail if not. #13182 (PE: #13354)
- [x] test must verify that packages are cleanly un-installable - already done
- [x] test must verify that packages can be upgraded from a previous version including the index if it already exists. (don't wipe any data) #13183 (PR: ##13287)
</comment><comment author="nik9000" created="2015-08-28T16:13:16Z" id="135822066">&gt; test must verify that packages load all relevant user provided configurations

You mean like the user can modify /etc/elasticsearch/elasticsearch.yml and /etc/defaults/elasticsearch and things happen? They already do some of that.
</comment><comment author="s1monw" created="2015-08-28T16:15:57Z" id="135822647">&gt; You mean like the user can modify /etc/elasticsearch/elasticsearch.yml and /etc/defaults/elasticsearch and things happen? They already do some of that.

yeah pretty much - like provide a template / script on the FS and it's loaded ie. all paths are configured correctly.
</comment><comment author="nik9000" created="2015-08-28T16:19:05Z" id="135823229">- [x] provide a template / script on the FS and it's loaded ie. all paths are configured correctly. #13184 (PR: #13262)
</comment><comment author="nik9000" created="2015-09-18T20:30:03Z" id="141560567">I believe all that is missing here is one final review on testing the signatures (#13354). I got a LGTM on it but it didn't merge cleanly with master and required some help so I'd like one more look there. Once that is in I'll feel much better about the RPM and deb builds.
</comment><comment author="nik9000" created="2015-10-06T15:01:53Z" id="145886286">Ok - this is done except for #13354 which I think might have to be pushed to 2.1/2.0.1 or something. I'm going to close this and @s1monw can reopen if he feels strongly about it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix doc parser to still pre/post process metadata fields on disabled type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13137</link><project id="" key="" /><description>I improved the tests more so we won't hit this problem again.

closes #13017
</description><key id="103418019">13137</key><summary>Fix doc parser to still pre/post process metadata fields on disabled type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T03:24:27Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-30T19:14:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-27T07:58:43Z" id="135330337">LGTM thanks @rjernst 
</comment><comment author="tlrx" created="2015-08-31T07:29:55Z" id="136287404">@rjernst thanks !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchPhaseExecutionException does not keep the original call stacktrace information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13136</link><project id="" key="" /><description>When I send a query which failed at server side, I will got a SearchPhaseExecutionException with description "Failed to execute phase [query], all shards failed; shardFailures {[JuFndtmqRW2...".

Unfortunately the thrown SearchPhaseExecutionException is built from an async action without the call stack trace information of where the wrong/failed query was required. All of what we can get is just as following exception stacktrace:

```
Caused by: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query], all shards failed; shardFailures {[JuFndtmqRW2C4kCzvLZ0SQ][xxx][0]: ....
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:237) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:183) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.search.action.SearchServiceTransportAction$6.handleException(SearchServiceTransportAction.java:249) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:190) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:180) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:130) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-1.7.1.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_51]
```

The above information is completely useless for bug analyzing, I believe the call stack with where the query request is created and invoked is the most important information for developers.

Additionally, our cluster is at version 1.6.2, and I noticed this issue with the same version of client library, then I tried the newest version 1.7.1 at client side, got the same result.
</description><key id="103411221">13136</key><summary>SearchPhaseExecutionException does not keep the original call stacktrace information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xzer</reporter><labels><label>:Exceptions</label><label>discuss</label></labels><created>2015-08-27T02:28:43Z</created><updated>2016-07-01T14:37:39Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-27T11:12:54Z" id="135388688">Hi @xzer 

From the above it looks like it is missing the real reason for the failure, which presumably was either reported in response to the request or somewhere else in the logs.  2.0 has made a number of changes to exception reporting and stack trace logging.  Could you give 2.0.0-beta1 a go and let us know if it has addressed your needs?
</comment><comment author="xzer" created="2015-08-31T05:23:04Z" id="136262455">The exception message contains the response body which described what is wrong at shard side, I replaced the error messages by dots in the paste. The problem is not what is wrong with the query, the point is we do not know where the wrong query is created at the client side source.

I will try the 2.0 later.
</comment><comment author="xzer" created="2015-11-19T06:11:37Z" id="157961601">I am sorry to reply this issue too late, I am now migrating my program to version 2.0, and when I got some server side query errors, I can actually get the real stack information of where the wrong query is created.

So I think this issue can be closed since the new 2.0 is no longer bad at that.
</comment><comment author="clintongormley" created="2015-11-19T12:27:16Z" id="158043604">thanks for letting us know @xzer 
</comment><comment author="xzer" created="2016-07-01T04:08:00Z" id="229849487">I am here to report that this issue is not fixed completely, there are still places that lack of calling site stacktrace information.

The following is what I got when there are errors in my query:

&gt; Caused by: org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
&gt;     at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:855) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:833) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:387) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     ... 3 common frames omitted
&gt; Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: Result window is too large, from + size must be less than or equal to: [10000] but was [10118]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.
&gt;     at org.elasticsearch.search.internal.DefaultSearchContext.preProcess(DefaultSearchContext.java:212) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:103) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.search.SearchService.createContext(SearchService.java:676) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:620) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:371) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-2.3.3.jar:2.3.3]
&gt;     ... 3 common frames omitted

Completely lost the calling site information which causes it is difficult to find out where the query is performed.

I digged the source and found the following logic may not be so ideal for this issue:

elasticsearch/core/src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java:79

``` java
    static RuntimeException rethrowExecutionException(ExecutionException e) {
        if (e.getCause() instanceof ElasticsearchException) {
            ElasticsearchException esEx = (ElasticsearchException) e.getCause();
            Throwable root = esEx.unwrapCause();
            if (root instanceof ElasticsearchException) {
                return (ElasticsearchException) root;
            } else if (root instanceof RuntimeException) {
                return (RuntimeException) root;
            }
            return new UncategorizedExecutionException("Failed execution", root);
        } else if (e.getCause() instanceof RuntimeException) {
            return (RuntimeException) e.getCause();
        } else {
            return new UncategorizedExecutionException("Failed execution", e);
        }
    }
```

In the above method, the original Exception was unrapped and then to be thrown, which cuts off the current calling stacktrace information from the exception causing chain, I believe the better way to throw the exception is to simply wrap the exception to a new RuntimeException as following:

``` java
    static RuntimeException rethrowExecutionException(ExecutionException e) {
        if (e.getCause() instanceof ElasticsearchException) {
            ElasticsearchException esEx = (ElasticsearchException) e.getCause();
            Throwable root = esEx.unwrapCause();
            if (root instanceof ElasticsearchException) {
                return new RunTimeException(root);
            } else if (root instanceof RuntimeException) {
                return new RunTimeException(root);
            }
            return new UncategorizedExecutionException("Failed execution", root);
        } else if (e.getCause() instanceof RuntimeException) {
            return new RunTimeException(e.getCause());
        } else {
            return new UncategorizedExecutionException("Failed execution", e);
        }
    }
```

I am not sure whether your guys can got notification from a closed ticket, if there is no reply after days, I will recreate a ticket the report this issue again.
</comment><comment author="clintongormley" created="2016-07-01T11:52:14Z" id="229927780">What version is this on @xzer ?
</comment><comment author="xzer" created="2016-07-01T12:36:25Z" id="229935842">the newest 2.3.3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove usage of tuple as a method parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13135</link><project id="" key="" /><description>This commit removes all instances of
org.elasticsearch.common.collect.Tuple as a method parameter.

Note for the reviewer: this pull request must either be integrated into the 2.0 branch before 2.0 GA, or held back until there is a home for 3.0. This is due to the fact that there are breaking public surface API changes contained in this pull request.

Closes #10787
</description><key id="103407145">13135</key><summary>Remove usage of tuple as a method parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-27T02:11:04Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-27T08:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-27T03:50:18Z" id="135282917">This looks great, +1!
I don't think we need to worry about backcompat here. These are internal apis, and whether this is in 2.0 or 2.1, I think the break is fine to make.
</comment><comment author="s1monw" created="2015-08-27T07:15:58Z" id="135317847">LGTM +1 to go into 2.0?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Defining default_index and default_search analyzers sets type-level index and search analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13134</link><project id="" key="" /><description>The migration plugin is complaining that I have type-level analyzers configured for all my types:

&gt; Type-level analyzer settings
&gt; analyzer, search_analyzer and index_analyzer settings have been removed and will use the index defaults instead, in type: foo.

I've pinned it down to the `default_index` and `default_search` definitions on the index analysis settings.

```
curl -XPUT localhost:9200/test -d '
{
  "index": {
    "analysis": {
      "analyzer": {
        "default_search": {
          "filter": [
            "standard",
            "lowercase",
            "stop",
            "asciifolding"
          ],
          "tokenizer": "standard",
          "type": "custom"
        },
        "default_index": {
          "filter": [
            "standard",
            "lowercase",
            "stop",
            "asciifolding"
          ],
          "tokenizer": "standard",
          "type": "custom"
        }
      }
    }
  }
}
'
```

```
curl -XPUT localhost:9200/test/foo/_mapping -d '{"foo": {"properties":{}}}'
```

```
curl -XGET localhost:9200/test/foo/_mapping?pretty
```

``` json
{
  "test" : {
    "mappings" : {
      "foo" : {
        "index_analyzer" : "default_index",
        "search_analyzer" : "default_search",
        "properties" : { }
      }
    }
  }
}
```

Is the how the intended behaviour of `default_index` and `default_search` (to set type-level analyzers)?

Does this mean in 2.0 I have to explicitly set the analyzers for all string fields now rather than being able to use a default that I define?

Perhaps the docs for default analyzers also needs updating then? 
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html#default-analyzers
</description><key id="103403486">13134</key><summary>Defining default_index and default_search analyzers sets type-level index and search analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rayward</reporter><labels /><created>2015-08-27T01:48:43Z</created><updated>2015-08-27T11:17:51Z</updated><resolved>2015-08-27T11:17:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-27T03:46:26Z" id="135281654">@rayward You can certainly still define a default analyzer. Previously there were two different levels of default analyzer you could set. The first was on the entire index. The second was on each type. The latter was removed with 2.0 (see the reasoning in #8874), but the default analyzer can still be set on the entire index.

If you remove the `_index` suffix from your analyzer name (that is going away with #11861), and remove the `index_analyzer` and `search_analyzer` settings from your type, the migration tool should be happy. `default` and `default_search` are special names for analyzer. If you don't specify an analyzer for a field, it will use these.
</comment><comment author="rayward" created="2015-08-27T04:05:08Z" id="135285229">@rjernst thanks for clarifying, I think it's just a bit confusing because I never explicitly specified the type-level analyzers on the mapping, ES picked it up from from the index level settings.

I'm not even sure how to even remove them actually (perhaps because they're not actually set on the 

```
curl -XPUT localhost:9200/test/foo/_mapping -d '{"foo": {"properties":{}, "search_analyzer": null}}'
{"error":"NullPointerException[null]","status":500}
```

```
curl -XPUT localhost:9200/test/foo/_mapping -d '{"foo": {"properties":{}, "search_analyzer": ""}}'
{"error":"MapperParsingException[Analyzer [] not found for search_analyzer setting on root type [foo]]","status":400}
```

Nor will it even let me change it:

```
curl -XPUT localhost:9200/test/foo/_mapping -d '{"foo": {"properties":{}, "search_analyzer": "standard"}}'
```

The mapping still looks the same:

```
curl -XGET localhost:9200/test/foo/_mapping?pretty
{
  "test" : {
    "mappings" : {
      "foo" : {
        "index_analyzer" : "default_index",
        "search_analyzer" : "default_search",
        "properties" : { }
      }
    }
  }
}
```

Am I doing something fundamentally wrong?
</comment><comment author="rjernst" created="2015-08-27T04:12:05Z" id="135285764">It sounds like the get mappings on 1.x always output the type level if it was "set", even if by specifying an index level default. I think this should actually work when upgrading (it will just be ignored). It looks like this is a test index. Would you mind trying the upgrade and reporting back? @clintongormley can speak to how we might improve the migration plugin to detect this situation (and not error in that case).
</comment><comment author="rjernst" created="2015-08-27T04:13:05Z" id="135285834">I'll reopen for now, until we determine how to improve the migration plugin and/or if there is work needed to allow the upgrade error free in this case.
</comment><comment author="clintongormley" created="2015-08-27T11:17:35Z" id="135390661">@rayward thanks for pointing this out.  I'll move the issue to the migration repo and address this there
</comment><comment author="clintongormley" created="2015-08-27T11:17:51Z" id="135390779">This issue was moved to elastic/elasticsearch-migration#23
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix generation scripts for bwc indexes, and add 2.0 beta1 index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13133</link><project id="" key="" /><description>The scripts to download and build static bwc indexes had some issues that prevented them from working. They were trying to do things on 2.0 that were no longer supported. This change limits the old behavior to pre 2.0 index versions.
</description><key id="103395909">13133</key><summary>Fix generation scripts for bwc indexes, and add 2.0 beta1 index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-08-27T00:45:53Z</created><updated>2015-08-27T19:56:29Z</updated><resolved>2015-08-27T17:21:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-27T07:18:55Z" id="135318276">left one comment LGTM otherwise
</comment><comment author="s1monw" created="2015-08-27T07:19:24Z" id="135318338">hmm I don't see the BWC index here are you adding this in a different PR? I also wonder if you can flip the snapshot bit on Version.java for this such that the index gets tested :)
</comment><comment author="rjernst" created="2015-08-27T17:21:18Z" id="135497736">Thanks @s1monw I fixed the download script to still work with old indexes. This PR is just to make the scripts runnable again. I didn't want to tie that to updating the version constants.
</comment><comment author="s1monw" created="2015-08-27T19:56:29Z" id="135536007">++ thx ryan
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent handling of string numerics in geo aggregation "precision" parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13132</link><project id="" key="" /><description>The setup:

```
curl -XPUT http://127.0.0.1:9200/my_index -d '{
  "mappings": {
    "my_type": {
      "properties": {
        "my_loc": { "type": "geo_point" },
        "my_num": { "type": "integer" }
      }
    }
  }
}'

curl -XPUT http://127.0.0.1:9200/my_index/my_type/1 -d '{ "my_loc": [0,0] }'
curl -XPUT http://127.0.0.1:9200/my_index/my_type/2 -d '{ "my_loc": [1.25,1.25] }'
curl -XPUT http://127.0.0.1:9200/my_index/my_type/3 -d '{ "my_loc": [1.5,1.5] }'
curl -XPUT http://127.0.0.1:9200/my_index/my_type/4 -d '{ "my_loc": [1.75,1.75] }'
curl -XPUT http://127.0.0.1:9200/my_index/my_type/5 -d '{ "my_loc": [2,2] }'
curl -XPUT http://127.0.0.1:9200/my_index/my_type/6 -d '{ "my_num": 10 }'
```

ES is perfectly happy to convert over pure-numeric query data contained in quotes (e.g. "7"):

```
curl -XGET http://127.0.0.1:9200/my_index/my_type/_search?pretty -d '{
  "filter": {
    "range": {
      "my_num": {
        "gte": "7"
      }
    }
  }
}'
```

Returning the expected doc (my_index/my_type/6).  This also "just works" with numeric aggs that I've tested (e.g. range aggs).

However, this behavior is inconsistent when specifying the "precision" variable in a geohash_grid agg:

```
curl -XGET http://127.0.0.1:9200/my_index/my_type/_search?pretty -d '{
  "aggs": {
    "my_agg": {
      "geohash_grid": {
        "field": "my_loc",
        "precision": "3"
      }
    }
  },
  "size":0
}'
```

This ignores the precision of 3 and reverts to the default of 5 (unless "3" is removed from quotes)
</description><key id="103395666">13132</key><summary>Inconsistent handling of string numerics in geo aggregation "precision" parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>:Geo</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-08-27T00:43:09Z</created><updated>2015-11-04T08:11:51Z</updated><resolved>2015-11-04T08:08:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Native (search) script parameters are being overwritten (scripted_metric agg)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13131</link><project id="" key="" /><description>**Disclaimer**: apologies in advance if it turns out I just did something stupid, but I've been banging my head against it and it makes no sense to me.

Got a weird one here. I'm writing a simple native script for the `scripted_metric` aggregation to group `DocLookup` objects by the `_parent` field of the documents being used. The problem is that the `_agg` object (part of my `params`) is being overwritten somewhere (i.e. not by myself). The fact I know there's a bug here is because if I look at the result set, it tells me that I have **4** `install` events... but I _only_ have **3** of them stored inside Elasticsearch. The keys seem to be arbitrarily re-assigned to random values.

As part of my investigation, I decided to log the state of a specific `_parent` key after each run of the script (the `debug_parent` field). Here's what came back:

```
/* 'install' is added to a list assigned to the parent */
[2015-08-27 00:05:22,185][INFO ][mapper                   ] adding [install] to d04caf67-8f6b-49fc-a449-91f3192748c7
/* so this is immediately after on the same run() call */
[2015-08-27 00:05:22,187][INFO ][mapper                   ] [install]
/* on subsequent calls, it's gone, other stuff is there */
[2015-08-27 00:05:22,188][INFO ][mapper                   ] [node-stats-cpu]
[2015-08-27 00:05:22,188][INFO ][mapper                   ] [node-stats-memory]
[2015-08-27 00:05:22,189][INFO ][mapper                   ] [foreground]
[2015-08-27 00:05:22,189][INFO ][mapper                   ] [background]
```

So you can see that something is clearly overwriting my previously stored event names, but the Java should be appending to the previously stored list. 

Here is my native script (it's for a mapping phase).

``` java
public class ParentContextProvider implements NativeScriptFactory {

    @Override
    public ExecutableScript newScript(@Nullable Map&lt;String, Object&gt; params) {

        final HashMap&lt;String, List&lt;DocLookup&gt;&gt; _agg
                = (HashMap&lt;String, List&lt;DocLookup&gt;&gt;) params.get("_agg");

        final String debug_parent = "d04caf67-8f6b-49fc-a449-91f3192748c7";

        final Logger logger = Logger.getLogger("mapper");

        return new AbstractSearchScript() {

            @Override
            public Object run() {
                DocLookup doc = doc();

                // only interested in docs with parents
                if (doc.containsKey("_parent")) {

                    // pull out the parent value of the DocLookup object
                    String parent = getStringDocValue(doc, "_parent");

                    // Retrieve either the previous list, or a new and empty list
                    List&lt;DocLookup&gt; tuples = _agg.getOrDefault(parent, Lists.newArrayList());

                    // add the doc to the list
                    tuples.add(doc);

                    // if it's the debug parent, log out what we added
                    if(debug_parent.equals(parent)) {
                        logger.info("adding " + doc.get("event") + " to " + parent);
                    }

                    // overwrite the list in the _agg param
                    _agg.put(parent, tuples);
                }

                // if the _agg has our debug parent
                if(_agg.get(debug_parent) != null) {

                    // create a string joiner
                    StringJoiner s = new StringJoiner(",");

                    // add every 'event' to the string
                    _agg.get(debug_parent).forEach(e -&gt; s.add(e.get("event").toString()));

                     // log the string, this should be all events for that _parent
                     logger.info(s.toString());
                }

                return _agg;
            }

            private String getStringDocValue(DocLookup doc, String key){
                return ((ScriptDocValues.Strings) doc.get(key)).getValue();
            }
        };

    }

}
```
</description><key id="103386474">13131</key><summary>Native (search) script parameters are being overwritten (scripted_metric agg)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitfin</reporter><labels><label>:Aggregations</label></labels><created>2015-08-26T23:27:22Z</created><updated>2015-08-28T14:25:28Z</updated><resolved>2015-08-28T12:16:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="whitfin" created="2015-08-26T23:33:10Z" id="135205266">Also, if you copy the logging of `_agg.get(debug_parent)` to the very start of `run()`, you can see that `install` is already gone by the time `run()` is next called - so it must be something outside of the script above (unless there's something bizarre with references going on that I'm missing - although I've tried every form of dereferencing that I know).
</comment><comment author="clintongormley" created="2015-08-27T11:08:31Z" id="135386863">@colings86 could you take a look at this please?
</comment><comment author="colings86" created="2015-08-27T11:56:04Z" id="135399208">@iwhitfield Could you post (maybe in a gist) of a Sense or curl recreation of this issue, including the commands for creating the index, indexing the events, and and search request for the scripted metric aggregation that uses your native script above?
</comment><comment author="whitfin" created="2015-08-27T15:54:11Z" id="135476336">@colings86 I'll dump my index, although I'm not sure the index is the issue, let me get back to you ASAP.
</comment><comment author="colings86" created="2015-08-27T15:57:39Z" id="135477229">@iwhitfield no, I don't think the index is the issue either but having the curl recreation will make it alot easier to reproduce the issue and work out whats going on
</comment><comment author="whitfin" created="2015-08-27T15:59:46Z" id="135478057">@colings86 sure thing, I'll get that to you - I just have to remove some private info in there, but I'll get it to you shortly.
</comment><comment author="colings86" created="2015-08-27T16:00:19Z" id="135478184">@iwhitfield ok great, thanks :)
</comment><comment author="whitfin" created="2015-08-27T17:28:55Z" id="135500406">@colings86 ok here's a gist - I hope I got everything you need: https://gist.github.com/iwhitfield/86b3193280bdd1d4edf2

I decided to keep the names as-is, rather than rename them to "eventX" because it'd just make it harder for you to read :)

`raw_data_dump.json` can be loaded by https://github.com/taskrabbit/elasticsearch-dump if you have it accessible to you (it's also handy if you wanna just use JS or something to scan it), otherwise `raw_data_sense.txt` should be a file you can just load up into Sense (let me know if I did that wrong, though).

`test_curl.txt` is just the curl query I have been testing with. Although I don't think it's relevant, I'll note that I also had a combiner and reducer in most of my testing, but I narrowed it down to the mapping phase being the issue and so I have removed those from the curl (i.e. the logs in the OP prove it's a mapping issue).

Your help is much appreciated, let me know if I can do anything else to point you in a helpful direction - I've kept the index around that displays this issue, so I can run anything you like on it.

&lt;s&gt;Also, it appears that the keys and values are not mixed up at random - subsequent runs will always show the same progression as shown in the logs above, so perhaps that's a hint towards something.&lt;/s&gt; **Edit:** strike this, it appears that this has changed (I have restarted ES several times since last trying). 
</comment><comment author="whitfin" created="2015-08-27T23:36:38Z" id="135584508">Have verified that this is the case consistently after reloading the gisted dataset on my installation of ES1.7.1, and on another installation of ES1.6.1. So @colings86, you should be able to replicate using the info above :)
</comment><comment author="colings86" created="2015-08-28T12:16:01Z" id="135756397">@iwhitfield Sorry, it took me a little while to realise what the code in your script is doing. 

You are trying to store the DocLookup object in your `_agg` object but this will not work. This object does not represent a particular document, it is an object that can be used to access particular fields for the current document. When the aggregation moves onto the next document the DocLookup object stayed the same (you can see this as it has the same object id) but now calls to DocLookup will return values for the next document. This is why you are seeing this strange behaviour. 

You will need to change your script to retrieve the values that you need for each document (it looks like this is the value of `event` in your script above) and store that value in your `_agg` object rather than trying to store the DocLookup object.

I am going to close this issue as Elasticsearch it working correctly here.
</comment><comment author="whitfin" created="2015-08-28T14:25:28Z" id="135787672">@colings86 that makes perfect sense, thank you for summing it up so quickly :) I knew it must be something with the way things are referenced. Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds stats counter for failed indexing requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13130</link><project id="" key="" /><description>#8938 - This adds a field index_failed to _stats showing how many index requests failed.
</description><key id="103384728">13130</key><summary>Adds stats counter for failed indexing requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">andrestc</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-08-26T23:09:43Z</created><updated>2015-09-10T14:46:36Z</updated><resolved>2015-09-10T14:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-27T13:23:39Z" id="135426703">Looks pretty good. I left some comments about the test and compatibility with multiple node versions which I assume is a thing because this is (rightly) targeting master and not the 2.0 branch.
</comment><comment author="andrestc" created="2015-08-27T14:19:46Z" id="135449830">@nik9000 I made some questions regarding some of your comments (mostly because im a new contributor so I don't know much about the codebase).
Will update my PR once I have the answers.

Thanks for the review.
</comment><comment author="nik9000" created="2015-08-27T14:21:02Z" id="135450468">&gt; Will update my PR once I have the answers.

I think I just answered actually. Good timing!

The best thing is to send a new commit that changes the changes and squash later (or not). Its easier to review that way.
</comment><comment author="andrestc" created="2015-08-27T14:39:11Z" id="135455050">@nik9000 Just pushed it!
Thanks.
</comment><comment author="nik9000" created="2015-08-27T14:47:51Z" id="135457101">Looks good to me. I'd love to get a review from someone else just to be 100% sure. If someone doesn't show up in the next few hours I'll go hunting.
</comment><comment author="jasontedor" created="2015-08-29T14:45:54Z" id="135994753">@andrestc Do you think that you can add this to the cat API as well?
</comment><comment author="andrestc" created="2015-08-29T14:50:56Z" id="135994986">@jasontedor Sure, ill give it a shot and ask for help if i get stuck.
</comment><comment author="jasontedor" created="2015-08-29T14:52:26Z" id="135995050">@andrestc Sounds good, thank you!
</comment><comment author="andrestc" created="2015-08-30T03:38:15Z" id="136082874">@jasontedor There it is!
</comment><comment author="nik9000" created="2015-08-31T13:16:29Z" id="136368244">Thanks for remembering the _cat api, @jasontedor. I forgot it often.
</comment><comment author="nik9000" created="2015-08-31T14:03:39Z" id="136380308">Ok! I found another fun thing to test. You can test the backwards compatibility by writing an `ESBackcompatTestCase`. It looks like `IndexingStats` doesn't have one by `NodeStats` does. So you could use it as a starting point. That'd verify the need for that Version check.

@andrestc, are you up for it? You've been a great sport so far! You'll have to look into testing.asciidoc for instructions on running these. The python script that fetches the index is out of date on your branch so you'd need to run it on the master branch and switch back to your branch.
</comment><comment author="andrestc" created="2015-08-31T17:43:13Z" id="136440943">@nik9000 I'll give it a shot during the week, sure!
Will get back to you if i get stuck.
Thanks!
</comment><comment author="andrestc" created="2015-09-03T01:52:47Z" id="137300122">@nik9000 I'm having some problems to run the backwards tests.
First, On what version should i run against? 
I followed the [Testing](https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc#backwards-compatibility-tests). 
I've tried it with 1.7.0 by doing the following:
After downloading, etc etc. I ran:

```
python ev-tools/create_bwc_index.py 1.7.0
```

When i run:

```
mvn test -Dtests.filter="@backwards" -Dtests.bwc.version=1.7.0 -Dtests.security.manager=false
```

Its fails and tells me there were no executed tests(i tried with "@Backwards", too). So i guess the problem is with the filter on mvn test.

Any ideas?
</comment><comment author="dakrone" created="2015-09-09T14:53:54Z" id="138936913">This LGTM, I do agree with @nik9000 though that it would be nice to have BWC tests for IndexingStats

@nik9000 it looks like you opened an issue to fix the BWC test infrastructure, do you want to wait on this until that is fixed, or do the testing in a subsequent issue?
</comment><comment author="nik9000" created="2015-09-09T15:43:13Z" id="138951244">&gt; @nik9000 it looks like you opened an issue to fix the BWC test infrastructure, do you want to wait on this until that is fixed, or do the testing in a subsequent issue?

Either one is fine with me. @andrestc, if you are ok with me merging this as is I'll create a followup issue for the the backwards compatibility test. I don't want to block this pr forever just because we've broken part of our infrastructure.
</comment><comment author="andrestc" created="2015-09-09T15:48:29Z" id="138953003">@nik9000 I'm definitely okay. As soon as the backwards infrastructure is okay I can submit a PR with this tests.
</comment><comment author="nik9000" created="2015-09-09T15:49:48Z" id="138953586">&gt; @nik9000 I'm definitely okay. As soon as the backwards infrastructure is okay I can submit a PR with this tests.

Cool. I'll merge it soon-ish then. I have 5 tabs worth of code reviews and merges to do when I stop concentrating on fixing the backwards compatibility tests.
</comment><comment author="nik9000" created="2015-09-10T13:49:29Z" id="139239618">OK! I picked this up to merge it this morning. I squashed it and reran the tests. One of the rest tests was failing so I'll ship a commit along with the merge that fixes it. It didn't merge 100% clean with master but the only conflict was obvious to resolve. I'll be rerunning the tests with the work staged on master locally and when all that passes I'll push to master. After that I'll backport to the 2.x branch which will eventually be used to form the 2.1 release.
</comment><comment author="nik9000" created="2015-09-10T14:14:25Z" id="139247634">Merged to master: https://github.com/elastic/elasticsearch/commit/3839d15ea075a55dcacbc6adae408c5fce119434
</comment><comment author="nik9000" created="2015-09-10T14:37:21Z" id="139266064">And merged to 2.x: https://github.com/elastic/elasticsearch/commit/1e347c351df16ea15be8c6850049ebf4b1387fa8

This should show up in 2.1.0 whenever that is released.
</comment><comment author="nik9000" created="2015-09-10T14:43:38Z" id="139268103">Thanks so much for this @andrestc. Thanks for working through all the changes. I'm sorry you were the first person to hit the backwards compatibility tests being busted (#13425). We'll get those fixed up as soon as we can!

Thanks again!
</comment><comment author="andrestc" created="2015-09-10T14:46:36Z" id="139268868">@nik9000 Np! Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match phrase prefix query seems to ignore boost value.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13129</link><project id="" key="" /><description>There seems to be no effect on score on changing boost value on match_phrase_prefix query. 

My query is:
&lt;pre&gt;&lt;code&gt; "query": {
        "bool": {
            "should": [
                {"match_phrase_prefix": {
                        "firstName": {
                            "query": "is", 
                            "boost":2.0
                        }
                    }} ,
                    {"match_phrase_prefix": {
                        "userName.raw": {
                            "query": "Tiv", 
                            "boost":4.0
                        }
                    }}
              ]
        }
    }&lt;/pre&gt;&lt;/code&gt;

I am trying this on Elasticsearch-1.7
</description><key id="103346815">13129</key><summary>Match phrase prefix query seems to ignore boost value.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mohittt8</reporter><labels><label>:Query DSL</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-08-26T19:28:35Z</created><updated>2015-09-03T18:12:55Z</updated><resolved>2015-09-03T18:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-27T10:53:12Z" id="135380392">Boost does seem to be ignored by the match_phrase_prefix query:

```
PUT t/t/1
{
  "one": "foo",
  "two": "bar"
}

GET t/_search?explain
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "one": {
              "query": "foo",
              "boost": 5
            }
          }
        },
        {
          "match_phrase_prefix": {
            "two": {
              "query": "bar",
              "boost": 50
            }
          }
        }
      ]
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-08-27T10:53:26Z" id="135380605">This is also broken in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `stats_bucket` / `extended_stats_bucket` pipeline aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13128</link><project id="" key="" /><description>These are the complements to the stats/extended_stats metric aggregations, and can be used
to calculate a variety of statistics over buckets

Pretty straight forward addition, but a few notes:
- These extend `metrics.stats.InternalStats` &amp; `metrics.stats.extended.ExtendedInternalStats` to reduce copy/paste of basically the same code
- The pipeline aggs register the stream of their specific internal representation (see `StatsBucketPipelineAggregator.registerStreams()` for example).  I thought this was cleaner than cluttering up the SearchModule registration list with 2x registrations for each, but happy to change if this is too indirect
- Provides two interfaces (`StatsBucket` &amp; `ExtendedStatsBucket`) which are basically just identical subclasses of the metric versions.  Thought it would still be good to have these for the end-user.

/cc @colings86 if you have some spare time, no rush though.
</description><key id="103337189">13128</key><summary>Add `stats_bucket` / `extended_stats_bucket` pipeline aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-08-26T18:35:10Z</created><updated>2015-09-04T19:29:30Z</updated><resolved>2015-09-04T19:25:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-27T10:37:54Z" id="135376126">Left a couple of comments but I think this is looking good. 
</comment><comment author="polyfractal" created="2015-09-01T19:04:04Z" id="136829287">Comments addressed!
</comment><comment author="colings86" created="2015-09-02T10:00:13Z" id="137010273">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain node restart for plugin install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13127</link><project id="" key="" /><description /><key id="103334727">13127</key><summary>Explain node restart for plugin install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">c4urself</reporter><labels><label>docs</label></labels><created>2015-08-26T18:20:44Z</created><updated>2015-08-27T09:48:04Z</updated><resolved>2015-08-27T09:47:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-27T09:48:03Z" id="135367275">thanks @c4urself - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to building rpm  package for elasticsearch?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13126</link><project id="" key="" /><description>I want to building my own rpm package for elasticsearch
</description><key id="103326080">13126</key><summary>How to building rpm  package for elasticsearch?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">lygstate</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2015-08-26T17:32:09Z</created><updated>2015-08-27T09:18:42Z</updated><resolved>2015-08-26T20:21:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-08-26T17:48:45Z" id="135121629">@lygstate Only in case you are not aware, [Elastic](https://www.elastic.co) does provide [repositories](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html#_yum) with prebuilt RPMs.

If that's not sufficient, do you want to build off of master, or some other branch of the elasticsearch source code repository?
</comment><comment author="jpountz" created="2015-08-26T20:21:44Z" id="135159552">Closing as this is not an issue report.
</comment><comment author="lygstate" created="2015-08-27T01:24:16Z" id="135234141">@jasontedor I wanna to building from the 2.0-beta1 branch, and I want to building it by my own.
</comment><comment author="MaineC" created="2015-08-27T09:16:31Z" id="135357717">Building Elasticsearch with maven from the 2.0beta1 branch will also generate an rpm package automatically. If this doesn't fit your needs you will need to turn to the rpm documentation.

YMMV depending on the distribution you use. As a starting point I'd suggest reading the RPM HowTo of the Linux documentation project:

http://www.tldp.org/HOWTO/RPM-HOWTO/build.html 

Also the tool "alien" might help you with converting between rpm/tar/tar.gz formats.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search Request performance drops significantly when setting size to Integer.MAX_VALUE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13125</link><project id="" key="" /><description>We are updating some data in our els store. The data has a creation timestamp so we fetch buckets of 1 hour length from the database. Therefore we need to fetch all items from this date bucket. Since we don&#180;t know how many items could be in the bucket, we se the size of search to Integer.MAX_VALUE. 

We&#180;re seeing a dramatic drop in performance when setting the hit size that high. When we set the size closer to the actual size, the performance of the request is again expectably fast.

**Scroll to my [comment](https://github.com/elastic/elasticsearch/issues/13125#issuecomment-136964243) for a solution using the scroll API**

we tried to reproduce the problem as well using the REST API directly:
With Integer.MAX_VALUE (POST {{elastic_search_host}}/resolve_log_v1/entry/_search?pretty)

```
{
    "size" : 2147483647,
    "query" : {
        "bool" : {
            "must" : [
                { "range" : { "trigger.timestamp" : { "gte": "2015-07-26T16:00:00.000Z", "lte" : "2015-07-26T17:00:00.000Z" } } }
            ]
        }
    }
}
```

Result:

```
{
  "took": 22856,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 21,
    "max_score": 1,
    "hits": [
```

 same request with size set to 21

```
{
  "took": 16,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 21,
    "max_score": 1,
    "hits": [
      {
        "_index": "resolve_log_v1",
        "_type": "entry",
        "_id": "4851a61c-3a17-494d-b72e-9bce172d45e6",
```

 when setting the size to 0 we still get a quick response.

The total count of entry objects (:

```
curl -X POST -H "Content-Type: application/json" -d '{    "size" : 0 }' 'http://localhost:9222/resolve_log_v1/entry/_search?pretty':
{
  "took": 26,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1022824,
    "max_score": 0,
    "hits": []
  }
}
```

To optimize, we now do two queries, 1 count, one to geht the hits:

```
BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery().must(rangeQuery("trigger.timestamp").gte(fromJoda.toString()).lte(toJoda.toString()));
SearchResponse sizeResponse = client.prepareSearch(SearchProvider.INDEX)
                    .setTypes(TYPE)
                    .setSearchType(SearchType.COUNT)
                    .setQuery(queryBuilder)
                    .execute()
                    .actionGet();

SearchResponse response = client.prepareSearch(SearchProvider.INDEX)
                    .setTypes(TYPE)
                    .setQuery(queryBuilder)
                    .setSize((int) sizeResponse.getHits().getTotalHits())
                    .execute()
                    .actionGet();
```

this is not ideal, as the bucket theoretically could change.
</description><key id="103321166">13125</key><summary>search Request performance drops significantly when setting size to Integer.MAX_VALUE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">falkorichter</reporter><labels><label>:Search</label><label>feedback_needed</label></labels><created>2015-08-26T17:04:28Z</created><updated>2015-09-02T07:50:34Z</updated><resolved>2015-08-26T18:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-26T18:30:34Z" id="135132006">I'm pretty sure this is a duplicate. I'll hunt down what this is a duplicate of. OTOH you've left a workaround so thanks for that.
</comment><comment author="jasontedor" created="2015-08-26T18:31:20Z" id="135132171">Have you considered using a [scroll](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-scroll.html)? I would advise against setting the result `size` so large given the data structures etc. that are allocated to handle result sets. The performance impact you're seeing is not a surprise.

&gt; Scrolling is not intended for real time user requests, but rather for processing large amounts of data, e.g. in order to reindex the contents of one index into a new index with a different configuration.

Note that such a request opens a "scroll context" that will enable you to continue to fetch the results of the initial search request.
</comment><comment author="nik9000" created="2015-08-26T18:34:34Z" id="135133013">&gt; Have you considered using a scroll? I would advise against setting the result size so large given the data structures etc. that are allocated to handle result sets. The performance impact you're seeing is not a surprise.

Yeah - this is really much better than your workaround. I'd love for this to be less of a thing - for elasticsearch to reject requests that are unreasonably large with a helpful message pointing you to scroll. And for some requests to be streamable. But neither of those are implemented yet. I thought there were issues opened for them but I've misplaced them.
</comment><comment author="clintongormley" created="2015-08-26T18:38:00Z" id="135133742">See https://github.com/elastic/elasticsearch/issues/9311 and https://github.com/elastic/elasticsearch/issues/11511

Definitely a duplicate, and scrolling is the right answer
</comment><comment author="nik9000" created="2015-08-26T18:38:55Z" id="135133931">&gt; Definitely a duplicate, and scrolling is the right answer

There we go. Thanks.
</comment><comment author="falkorichter" created="2015-08-27T11:56:35Z" id="135399280">I think I got the solution. Scroll to get a snapshot of the request that I can work with, then scroll through the list and collect all the items.

I&#180;ll post my working solution when I find time testing and finishin it. Thx for the good hints and answers. :bow: 
</comment><comment author="jasontedor" created="2015-08-27T17:26:44Z" id="135499943">That's exactly right.
</comment><comment author="falkorichter" created="2015-09-02T07:43:30Z" id="136964243">here is my solution, hopefully it helps people that come across the same problem:

``` java
SearchRequestBuilder sizeQuery = client.prepareSearch(SearchProvider.INDEX)
        .setTypes(TYPE)
        .setSearchType(SearchType.SCAN)
        .setSize(10)
        .setScroll(TimeValue.timeValueSeconds(60))
        .setQuery(queryBuilder);

SearchResponse scrollResponse = sizeQuery.execute().actionGet();
long totalHits = scrollResponse.getHits().getTotalHits();

//might be an optimization, delete the scroll when size is 0, it&#180;s deleted anyways when getting the first result...
//if (totalHits == 0) {
//    ClearScrollRequest clear = new ClearScrollRequest();
//    clear.addScrollId(scrollResponse.getScrollId());
//    client.clearScroll(clear);
//    return result;
//}

String scrollID = scrollResponse.getScrollId();
SearchResponse response;
do {
    response = client.prepareSearchScroll(scrollID)
            .setScroll(TimeValue.timeValueSeconds(60))
            .get();
    scrollID = response.getScrollId();
    for (SearchHit searchHit : response.getHits()) {
        EsEntry esEntry = new EsEntry();
        esEntry.setId(searchHit.getId());
        esEntry.setRawData(searchHit.getSource());
        result.getEntryList().add(esEntry);
    }
} while (response.getHits().hits().length &gt; 0);
return result;
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make sure JAVA_HOME is set before tests are run</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13124</link><project id="" key="" /><description>relates to #12961 

We did not enforce it before but we need in now since #12961.

Alternatively we could try and find some other way to get the path to `jps` in [stop-node](https://github.com/elastic/elasticsearch/blob/master/dev-tools/src/main/resources/ant/integration-tests.xml#L215).
</description><key id="103314183">13124</key><summary>make sure JAVA_HOME is set before tests are run</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-08-26T16:35:17Z</created><updated>2015-11-22T10:11:46Z</updated><resolved>2015-09-16T09:55:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-15T12:25:39Z" id="140372186">Wow, I did not know maven would even allow you to do this. One alternative we could do would be to enforce it in the whole maven build: http://maven.apache.org/enforcer/enforcer-rules/requireEnvironmentVariable.html

But +1 to this for right now, it is needed.
</comment><comment author="nik9000" created="2015-09-15T12:39:10Z" id="140374487">LGTM
</comment><comment author="dadoonet" created="2015-09-15T12:44:27Z" id="140375740">+1 with @rmuir approach about enforcer. This way we don't have to maintain a Ant script.
</comment><comment author="brwe" created="2015-09-15T15:31:19Z" id="140432959">Ok, changed to use maven-enforcer-plugin now. @dadoonet thanks for looking into the enforcer version issue! 
</comment><comment author="nik9000" created="2015-09-15T15:32:30Z" id="140433627">This looks better to me.
</comment><comment author="dadoonet" created="2015-09-15T16:00:29Z" id="140442459">LGTM as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>deb package should register elasticsearch to start on restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13123</link><project id="" key="" /><description>The default for DEB packages is to register the thing they install to start on restart. Elasticsearch should do that too.
</description><key id="103303479">13123</key><summary>deb package should register elasticsearch to start on restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>discuss</label><label>enhancement</label></labels><created>2015-08-26T15:49:28Z</created><updated>2015-09-16T13:33:50Z</updated><resolved>2015-08-28T09:28:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-26T15:50:26Z" id="135076895">@clintongormley I'm not 100% sure this is new.
</comment><comment author="nik9000" created="2015-08-26T18:12:27Z" id="135127596">&gt; @clintongormley I'm not 100% sure this is new.

Ok - it is new. 1.7.1 doesn't auto start on restart. You have to add it to the runlevels yourself. The [reference](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html) even tells you how.

I've added the discuss label. It seems like for the most part deb packaged start their services on install but that isn't a super popular thing. Some packages don't do it and some people hate it when packages do that. I suspect the registration thing is similar.
</comment><comment author="jasontedor" created="2015-08-26T18:16:01Z" id="135128555">I do not think that we should do this. It has the potential to expose un-configured instances of Elasticsearch. We provide the [instructions](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html#_debian_ubuntu) in our setup page and that should be sufficient for those that are ready to configure this.
</comment><comment author="clintongormley" created="2015-08-27T09:46:15Z" id="135366974">I'm with @jasontedor here - I don't think we should make ES autostart ourselves.  Up to the user.
</comment><comment author="nik9000" created="2015-08-27T13:50:51Z" id="135438410">&gt; I'm with @jasontedor here - I don't think we should make ES autostart ourselves. Up to the user.

I'm honestly with him too. I see that things like postgresql and mysql start on install and register themselves to start on restart and I wrinkle my nose. Apparently this is debian policy though. I can't find where in the [policy](https://www.debian.org/doc/debian-policy/) it says that but the internet seems to think its in there somewhere.

I'm fine with saying "We've thought this through. We know we're going against debian policy but we think this should be up to the installer to make the decision about when and if Elasticsearch should start after its installed."
</comment><comment author="clintongormley" created="2015-08-28T09:28:58Z" id="135711682">Discussed in FixItFriday. Agreed that we shouldn't touch the run levels.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging tests should verify that dab and rpm don't register elasticsearch to start on restart or start elasticsearch after install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13122</link><project id="" key="" /><description>We intend for neither the deb nor the rpm to start elasticsearch on install nor register elasticsearch to start on restart. We should test that.

We know this that debian policy is for packages to do both of those things but we think that that would be the wrong choice for elasticsearch, even if it is confusing to debian users.
</description><key id="103302591">13122</key><summary>Packaging tests should verify that dab and rpm don't register elasticsearch to start on restart or start elasticsearch after install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-26T15:46:30Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-09-10T16:33:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Sort thread pools by name in Nodes Stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13121</link><project id="" key="" /><description>This pull request adds a very simple sort of thread pools (by their names) so that the Nodes Stats output is more predictive.
</description><key id="103296867">13121</key><summary>Sort thread pools by name in Nodes Stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-26T15:27:45Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-31T12:31:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-28T19:34:52Z" id="135868075">Left one minor comment
</comment><comment author="nik9000" created="2015-08-28T19:39:03Z" id="135868905">Its ok with me though I think google collection's Lists is banned now.
</comment><comment author="tlrx" created="2015-08-31T12:21:40Z" id="136356868">@bleskes @nik9000 Thanks for your reviews. I updated the code according to your comment, would you like to have a look?
</comment><comment author="bleskes" created="2015-08-31T12:24:57Z" id="136358206">LGTM
</comment><comment author="tlrx" created="2015-08-31T12:26:57Z" id="136358709">Thanks @bleskes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move from oss-parent to nexus-staging-maven-plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13120</link><project id="" key="" /><description>According to sonatype's [docs](http://central.sonatype.org/pages/apache-maven.html#deprecated-oss-parent) the oss-parent module is deprecated and they recommend you use nexus-staging-maven-plugin as documented on that page.
</description><key id="103278390">13120</key><summary>Move from oss-parent to nexus-staging-maven-plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v2.3.0</label></labels><created>2015-08-26T14:07:09Z</created><updated>2016-01-28T16:24:30Z</updated><resolved>2016-01-28T16:24:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-26T14:08:30Z" id="135034173">@rjernst - there are [docs](http://central.sonatype.org/pages/gradle.html) for Gradle too. Just in case. You know.
</comment><comment author="dadoonet" created="2015-08-26T14:58:58Z" id="135050210">++ i was not aware of that. Thanks Nik!
</comment><comment author="nik9000" created="2015-08-26T15:20:47Z" id="135059787">&gt; ++ i was not aware of that. Thanks Nik!

I wasn't either until this morning. It should be reasonably simple to do for anyone that is used to maven's maven-ness. You'll need access to sonatype's elasticsearch oss repository to test this though.
</comment><comment author="nik9000" created="2015-09-21T18:35:47Z" id="142070110">Bumping to 2.1.0 - this isn't really important for releasing 2.0.0.
</comment><comment author="clintongormley" created="2016-01-28T13:06:04Z" id="176175104">@rjernst can this be closed?
</comment><comment author="nik9000" created="2016-01-28T16:24:30Z" id="176261412">We don't need either one in master. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed the `operation_threaded` option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13119</link><project id="" key="" /><description>This low level option isn't worth the complexity and a read/write operation should never happen on the network thread.

This is a spinoff from #12395 that just focusses on removing the `operation_threaded` option.
</description><key id="103245821">13119</key><summary>Removed the `operation_threaded` option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-26T11:47:35Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-26T13:23:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-26T12:22:34Z" id="134982264">LGTM. Awesome stats.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose shards data and state path via ShardStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13118</link><project id="" key="" /><description>Since we now don't stripe shards across data paths we need a way
to access the information on which path a shard is allocated to
eventually do better allocation decisions based on disk usage etc.
This commit exposes the shard paths as part of the shard stats.

Relates to #13106
</description><key id="103226012">13118</key><summary>Expose shards data and state path via ShardStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-26T09:48:55Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-27T07:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-26T16:02:33Z" id="135079845">The LGTM, the only thing I think would be nice to add is a unit test for the `ShardPath` constructor to make sure it throws an exception when `isCustomDataPathEnabled` is true and the data path is not different than the state path.
</comment><comment author="dakrone" created="2015-08-26T16:11:16Z" id="135082011">Oh, also is it worth adding a REST test for this to test the output? It looks good either way, just curious what you think.
</comment><comment author="s1monw" created="2015-08-27T07:26:22Z" id="135319773">&gt; Oh, also is it worth adding a REST test for this to test the output? It looks good either way, just curious what you think.

I added a unittest for the xcontent rendering. I really don't think we need anything else. The test also passes the stats through serialization randomly so we should be covered here and it's an order of a magnitude faster than a rest test.
</comment><comment author="dakrone" created="2015-08-27T17:01:01Z" id="135493277">&gt; I added a unittest for the xcontent rendering.

Cool thanks! I opened https://github.com/elastic/elasticsearch/issues/13157 as maybe a way we can do this in a better way in the future.

I also left a comment about forbidden APIs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>what is default index value for number?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13117</link><project id="" key="" /><description>don't find int in reference doc
</description><key id="103220197">13117</key><summary>what is default index value for number?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-08-26T09:27:05Z</created><updated>2015-08-26T10:31:22Z</updated><resolved>2015-08-26T10:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-26T10:31:22Z" id="134937407">The docs for mapping have been improved in 2.0.  See https://www.elastic.co/guide/en/elasticsearch/reference/2.0/dynamic-field-mapping.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable MLock By Default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13116</link><project id="" key="" /><description>Ticket proposing having bootstrap.mlockall: true in the default configuration distributed with Elasticsearch.  Recommended setting for most production installations.
</description><key id="103206715">13116</key><summary>Enable MLock By Default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2015-08-26T08:21:47Z</created><updated>2015-08-28T09:32:16Z</updated><resolved>2015-08-28T09:32:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-26T10:23:29Z" id="134935666">I don't think we should do this by default, as there are a couple of problems:
1. The ulimit needs to be set by the user to allow Elasticsearch to mlockall, which _could_ be done in the RPM/deb but not in tar.gz/zip
2. We can easily cause an OOM on a box if we lock memory when the user is not expecting it.
</comment><comment author="clintongormley" created="2015-08-28T09:32:16Z" id="135713591">Discussed in FixItFriday.  Setting mlockall by default when a user isn't expecting it could be dangerous.  Rather let them configure it, along with the heap size etc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate `min_score`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13115</link><project id="" key="" /><description>Quoting [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-min-score.html): "Note, most times, this does not make much sense, but is provided for advanced use cases".

Besides, it can be implemented easily with a function_score query, which will be as efficient and more flexible since you can put it anywhere in the query tree.
</description><key id="103193171">13115</key><summary>Deprecate `min_score`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>adoptme</label><label>deprecation</label><label>stalled</label></labels><created>2015-08-26T07:01:42Z</created><updated>2016-03-24T09:53:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-24T09:52:56Z" id="200762654">Actually it is probably not a low hanging fruit because of eg. #14300.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot error while snapshotting to AWS S3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13114</link><project id="" key="" /><description>Hi All,

I am using ES 1.5 version with Cloud-AWS plugin 2.5.1. I am trying to create a snapshot but i see the following error in the logs. 

```
[2015-08-26 00:04:54,216][WARN ][snapshots                ] [prod-01-data01] [[logstash-2015.08.25][3]] [niraj-backup:test] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [logstash-2015.08.25][3] INSTANCE
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:150)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:85)
        at org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:817)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoSuchFieldError: INSTANCE
        at org.apache.http.client.utils.URLEncodedUtils.parse(URLEncodedUtils.java:243)
        at org.apache.http.client.utils.URLEncodedUtils.parse(URLEncodedUtils.java:222)
        at org.apache.http.client.utils.URIBuilder.parseQuery(URIBuilder.java:95)
        at org.apache.http.client.utils.URIBuilder.digestURI(URIBuilder.java:165)
        at org.apache.http.client.utils.URIBuilder.&lt;init&gt;(URIBuilder.java:90)
        at org.apache.http.client.utils.URIUtils.rewriteURI(URIUtils.java:138)
        at org.apache.http.impl.client.DefaultRequestDirector.rewriteRequestURI(DefaultRequestDirector.java:354)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:477)
        at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863)
        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57)
        at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:701)
        at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:462)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:297)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3672)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3621)
        at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:642)
        at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:625)
        at org.elasticsearch.cloud.aws.blobstore.S3BlobContainer.listBlobsByPrefix(S3BlobContainer.java:118)
        at org.elasticsearch.cloud.aws.blobstore.S3BlobContainer.listBlobs(S3BlobContainer.java:136)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:438)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:140)
        ... 5 more
```

Any suggestion would be really appreciated. I am kinda stuck as of now. 

Regards
Niraj
</description><key id="103162394">13114</key><summary>Snapshot error while snapshotting to AWS S3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">niraj8241</reporter><labels><label>:Plugin Cloud AWS</label><label>feedback_needed</label></labels><created>2015-08-26T03:14:47Z</created><updated>2015-09-22T04:38:14Z</updated><resolved>2015-09-15T07:07:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-26T04:35:30Z" id="134825901">Which exact commands you run?
</comment><comment author="niraj8241" created="2015-08-26T04:41:57Z" id="134829042">``` sh
curl -XPUT 'http://localhost:9200/_snapshot/niraj' -d '{
    "type": "s3",
    "settings": {
        "bucket": "xxxxxx",
        "region": "us-east"
    }
}'
```

This was for snapshot 

``` sh
curl -XPUT 'http://localhost:9200/_snapshot/s3/niraj123'
```
</comment><comment author="dadoonet" created="2015-08-26T04:42:00Z" id="134829077">Closing as also opened here: https://discuss.elastic.co/t/nosuchfielderror-instance-while-elasticsearch-backup/28060/2

Will reopen if it's an issue.
</comment><comment author="dadoonet" created="2015-08-26T04:45:26Z" id="134830202">What does your elasticsearch.yml look like?
</comment><comment author="dadoonet" created="2015-08-26T04:47:50Z" id="134831018">Also do you have other plugins or run elasticsearch embedded?
</comment><comment author="niraj8241" created="2015-08-26T05:09:59Z" id="134832829">Below is my `elasticsearch.yml` file

``` yaml
action:
  disable_delete_all_indices: true
bootstrap:
  mlockall: true
cloud:
  aws:
    access_key: xxxxxxxxxxxxx
    secret_key: xxxxxxxxxxxxxxx
cluster:
  name: elasticsearch
discovery:
  zen:
    minimum_master_nodes: 2
    ping:
      multicast:
        enabled: false
      unicast:
        hosts:
             - xxxxxxxx:9300
             - xxxxxxxx:9300
             - xxxxxxxx:9300
gateway:
  expected_nodes: 3
  recover_after_nodes: 2
  recover_after_time: 2m
index:
  indexing:
    slowlog:
      threshold:
        index:
          warn: 10s
  merge:
    scheduler:
      max_thread_count: 1
  refresh_interval: 30s
  search:
    slowlog:
      threshold:
        query:
          warn: 10s
  translog:
    flush_threshold_size: 500mb
indices:
  breaker:
    fielddata:
      limit: 60%
    request:
      limit: 40%
    total:
      limit: 70%
  cache:
    filter:
      size: 15%
  fielddata:
    cache:
      size: 20%
  memory:
    index_buffer_size: 20%
    min_index_buffer_size: 96mb
    min_shard_index_buffer_size: 12mb
node:
  name: prod-01-data03-prod-01-data03
path:
  data: /opt/elasticsearch/data/prod-01-data03
script:
  disable_dynamic: true
```

Yeh i have marvel and watcher installed.
</comment><comment author="dadoonet" created="2015-08-26T05:36:04Z" id="134835932">Reopening.

I'll try to reproduce.

In the meantime, could you uninstall watcher, try again ?
Same with marvel then ?
</comment><comment author="niraj8241" created="2015-08-26T06:35:30Z" id="134858301">Yeah i did. But still the same
</comment><comment author="dadoonet" created="2015-08-26T06:39:11Z" id="134859559">And you restarted the nodes, right?
</comment><comment author="niraj8241" created="2015-08-26T06:47:32Z" id="134863920">yes i did on all the nodes.
</comment><comment author="dadoonet" created="2015-08-26T07:18:38Z" id="134873923">So I checked AWS Plugin 2.5.1. It's bundled with `httpclient:jar:4.3.5` and `httpcore:jar:4.3.2`.

```
[INFO] org.elasticsearch:elasticsearch-cloud-aws:jar:2.5.1
[INFO] +- org.elasticsearch:elasticsearch:jar:1.5.0:provided
[INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-queries:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-memory:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-highlighter:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-queryparser:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-sandbox:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-suggest:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-misc:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-join:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-grouping:jar:4.10.4:provided
[INFO] |  +- org.apache.lucene:lucene-spatial:jar:4.10.4:provided
[INFO] |  |  \- com.spatial4j:spatial4j:jar:0.4.1:provided
[INFO] |  +- org.antlr:antlr-runtime:jar:3.5:provided
[INFO] |  +- org.ow2.asm:asm:jar:4.1:provided
[INFO] |  \- org.ow2.asm:asm-commons:jar:4.1:provided
[INFO] +- org.apache.lucene:lucene-core:jar:4.10.4:provided
[INFO] +- com.amazonaws:aws-java-sdk-ec2:jar:1.9.34:compile
[INFO] |  \- com.amazonaws:aws-java-sdk-core:jar:1.9.34:compile
[INFO] |     +- commons-logging:commons-logging:jar:1.1.3:compile
[INFO] |     +- org.apache.httpcomponents:httpclient:jar:4.3.5:compile
[INFO] |     |  \- org.apache.httpcomponents:httpcore:jar:4.3.2:compile
[INFO] |     +- com.fasterxml.jackson.core:jackson-databind:jar:2.3.2:compile
[INFO] |     |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.3.0:compile
[INFO] |     |  \- com.fasterxml.jackson.core:jackson-core:jar:2.4.2:compile
[INFO] |     \- joda-time:joda-time:jar:2.7:compile
[INFO] +- com.amazonaws:aws-java-sdk-s3:jar:1.9.34:compile
[INFO] |  \- com.amazonaws:aws-java-sdk-kms:jar:1.9.34:compile
[INFO] +- commons-codec:commons-codec:jar:1.4:compile
```

Everything is correct here. I checked the classes and `INSTANCE` really exists in `httpcore`.

Could you run:

``` sh
ls -l plugins/cloud-aws
curl 'localhost:9200/_cat/plugins?v'
```
</comment><comment author="niraj8241" created="2015-08-26T07:30:37Z" id="134876316">```
ls -l plugins/cloud-aws

-rw-r--r-- 1 root root  481641 Aug 26 05:42 aws-java-sdk-core-1.9.23.jar
-rw-r--r-- 1 root root 1838789 Aug 26 05:42 aws-java-sdk-ec2-1.9.23.jar
-rw-r--r-- 1 root root  252285 Aug 26 05:42 aws-java-sdk-kms-1.9.23.jar
-rw-r--r-- 1 root root  540472 Aug 26 05:42 aws-java-sdk-s3-1.9.23.jar
-rw-r--r-- 1 root root   58160 Aug 26 05:42 commons-codec-1.4.jar
-rw-r--r-- 1 root root   62050 Aug 26 05:42 commons-logging-1.1.3.jar
-rw-r--r-- 1 root root   45054 Aug 26 05:42 elasticsearch-cloud-aws-2.5.0.jar
-rw-r--r-- 1 root root  590533 Aug 26 05:42 httpclient-4.3.5.jar
-rw-r--r-- 1 root root  282269 Aug 26 05:42 httpcore-4.3.2.jar
-rw-r--r-- 1 root root   35058 Aug 26 05:42 jackson-annotations-2.3.0.jar
-rw-r--r-- 1 root root  225316 Aug 26 05:42 jackson-core-2.4.2.jar
-rw-r--r-- 1 root root  915096 Aug 26 05:42 jackson-databind-2.3.2.jar
-rw-r--r-- 1 root root  589289 Aug 26 05:42 joda-time-2.7.jar
```

```
name                                    component          version type url
prod-01-data01 cloud-aws          2.5.0   j
prod-01-data03 cloud-aws          2.5.0   j
prod-01-data02 cloud-aws          2.5.0   j
```
</comment><comment author="niraj8241" created="2015-08-26T07:32:42Z" id="134877076">Just to mention. as a part of triaging i downgraded my cloud-aws plugin to 2.5.0 to match the ES version of 1.5.0
</comment><comment author="dadoonet" created="2015-08-26T08:31:23Z" id="134897306">So only 3 nodes in the full cluster and you don't embed elasticsearch in another JVM or so ?
</comment><comment author="niraj8241" created="2015-08-26T15:23:36Z" id="135061216">Sorry I didn't get your question. What does this mean embed elastic search in another JvM?
</comment><comment author="dadoonet" created="2015-08-26T15:30:46Z" id="135064427">If you don't understand that means you probably don't do it ! :)

Well sometimes Java developers embed ES in their Java web application. I guess you don't use Java for programming, right?
</comment><comment author="niraj8241" created="2015-08-26T18:57:54Z" id="135139382">:) yaah i am poor sys admin who doesn't do java programming. 
</comment><comment author="dadoonet" created="2015-09-04T09:15:06Z" id="137685716">Any chance you could upgrade to 1.7 and update aws plugin to 2.7.1?
</comment><comment author="dadoonet" created="2015-09-15T07:07:44Z" id="140300523">No feedback provided. Closing.

Feel free to reopen if you have any more information
</comment><comment author="dadoonet" created="2015-09-21T22:36:25Z" id="142128823">Note that we had a similar report when running AWS plugin and mapper attachments plugin. See https://github.com/elastic/elasticsearch-mapper-attachments/issues/157
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How should I optimize children aggregation usage?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13113</link><project id="" key="" /><description>It appears that the children aggregation is not cached (correct term?) using v1.6, as it is consistently taking the same amount of time for my query.

Here's an example of my aggs:

```
{
    "aggs": {
        "by_user": {
            "terms": {
                "field": "user_id",
                "size": 2000,
                "min_doc_count": 1
            },
            "aggs": {
                "by_date": {
                    "date_histogram": {
                        "field": "timestamp",
                        "interval": "day",
                        "format": "yyyy-MM-dd-HH-mm"
                    },
                    "aggs": {
                        "unique_browsers": {
                            "terms": {
                                "field": "browser",
                                "size": 20,
                                "min_doc_count": 1
                            },
                            "aggs": {
                                "move_to_children": {
                                    "nested": {
                                        "type": "event"
                                    },
                                    "aggs": {
                                        "count_up": {
                                            "sum": "event.my-counter"
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

Now, if I remove the child piece and run it (with no cache), and then rerun - there is an instant speedup. If I add the child piece back, and run it multiple times, it always takes the same amount of time. Is it possible that is missing some form of cache layer? And if so, what are the reasons why/recommendations?

I ask because this aggregation is extremely slow (is this improved in 2.0?), it's taking ~60s with it as above, and if I remove it, the aggregations above take 500ms. I was hoping it'd be a first-hit-slow-second-hit-fast type deal like the other aggregations in use.

Basically, what are the best ways to make children aggregations faster? I've searched docs for about a week with no results on what I've tried, so I was wondering if I'm missing something, or if I should hold on for 2.0, if that does indeed help things.

Thanks in advance, let me know if more info is needed - not sure what is relevant. Hope I was clear :).

It should be noted we're talking about a lot of parents here (in the millions).
</description><key id="103144319">13113</key><summary>How should I optimize children aggregation usage?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitfin</reporter><labels /><created>2015-08-26T00:30:41Z</created><updated>2016-07-24T07:13:21Z</updated><resolved>2015-08-26T07:13:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T07:13:48Z" id="134873138">parent/child is a bit tricky to cache: most of the things we cache are cached per segment but this doesn't work with parent child since the addition of a new document in segment A can change matching documents on segment B. So parent/child operations are never cached in elasticsearch 1.x. Elasticsearch 2.x makes it a bit better since there will be some caching happening if your index does not change too frequently but I wouldn't expect it to bring dramatic speedups.
</comment><comment author="martijnvg" created="2015-08-26T08:04:59Z" id="134890805">@iwhitfield If you aggregation and data is relative static, you may want to check out the request cache, which can cache the entire shard level search response.

The reduce the time the first search request with the `children` agg after each refresh is taking, you can configure eager global ordinals loading on the `_parent` field.

In 2.0 for the `children` agg mainly the on heap memory footprint has significantly improved, because the paren/child data structures is doc values based.
</comment><comment author="whitfin" created="2015-08-26T15:41:20Z" id="135071139">@jpountz @martijnvg thanks for the infos - what's the request cache, is that the query cache? If so, I have rolling time ranges in my filters so that probably wouldn't work, right?

To what @jpountz said, is it possible (because my indices are time based), that I can just merge old segments to have a single segment on a rolling basis? Would that help, either currently or 2.0? Based on what you said, that would fit the criteria for having the children aggregation cacheable, right? Would ES take advantage of that, or is it something that was never implemented due to it being unlikely someone would have their segments in such a way?

If there is any way at all, I'll take it - our model really does not fit with nested (there can be hundreds of children, and we want to be able to just hit the children with queries if needed).
</comment><comment author="adermlx" created="2015-08-26T15:46:44Z" id="135074796">i am fresher
</comment><comment author="jpountz" created="2015-08-26T17:40:49Z" id="135119763">On 1.x there is nothing you can do, we explicitly prevent parent/child queries from being cached. On 2.x this should be possible if the index is (mostly) static, and merging everything down to a single segment will also help.
</comment><comment author="whitfin" created="2015-08-26T18:41:53Z" id="135134699">@jpountz thanks for getting back to me - that's awesome, definitely looking forward to 2.0. Even given the optimum scenario for use of the cache, should I still not be expecting the same level of caching other aggregations receive (sorry for all the questions, I just want to make sure I have the facts :D)?
</comment><comment author="jpountz" created="2015-08-26T18:44:15Z" id="135135290">@iwhitfield sorry my last reply was more targeted at parent/child queries. For aggregations, you should already be able to get caching working if you use the query cache (renamed to request cache in 2.0 to better describe what it does). But this cache only works with search_type=COUNT and is invalidated everyt time there is a refresh operation happening.
</comment><comment author="whitfin" created="2015-08-26T18:47:43Z" id="135136667">@jpountz ah, I can't do that because I've got moving time based filters in my queries sadly, so it would miss when the time changed. 

So... I'm a little confused, because the caching &amp; speed of has_child is fine in my testing, it's purely the children aggregation that's being a pain - is that just the nature of the join, and as good as it's going to get (i.e. never speeds up)?
</comment><comment author="jpountz" created="2015-08-26T19:23:43Z" id="135145468">If it's very slow, any change that you can capture the node hot threads while the aggregation is running?
</comment><comment author="whitfin" created="2015-08-26T19:31:09Z" id="135147041">@jpountz yeah I can try get that info for you, is there anything else which would be helpful?

I mentioned above that that set of aggregations (without the children aggregation) takes a little while to start with, but if I run it again it's milliseconds fast. I'm just trying to achieve the same, but the children aggregation is consistently the same amount of time regardless of how many times it has been run.
</comment><comment author="jpountz" created="2015-08-26T19:35:15Z" id="135148028">&gt; is there anything else which would be helpful?

Not really. This aggregation is known as slow and I suspect there is nothing that can be done about it. Just wanted to see the hot threads to confirm it's not something else.
</comment><comment author="whitfin" created="2015-08-26T20:42:43Z" id="135165004">@jpountz here's a grab of the hot_threads API: https://gist.github.com/iwhitfield/ceefcccf83a4f20aa0de
</comment><comment author="martijnvg" created="2015-08-27T15:15:16Z" id="135465413">@iwhitfield looking at your hot threads dump and it seems that a lot of time is spent at loading  parent/child field data and global ordinals. Have you tried configuring eager global ordinals loading?
(https://www.elastic.co/guide/en/elasticsearch/guide/current/parent-child-performance.html#_global_ordinals_and_latency)
</comment><comment author="whitfin" created="2015-08-27T17:40:50Z" id="135503190">@martijnvg yeah, we have - here's my mapping for my child documents:

```
{
    "_all": {
        "enabled": false
    },
    "_parent": {
        "fielddata": {
            "loading": "eager_global_ordinals"
        },
        "type": "session"
    },
    "_routing": {
        "required": true
    },
    "_size": {
        "enabled": true
    },
    "_timestamp": {
        "enabled": true,
        "path": "ts"
    },
    "dynamic": "false",
    "properties": {
        "event": {
            "doc_values": true,
            "index": "not_analyzed",
            "type": "string"
        },
        "app": {
            "doc_values": true,
            "index": "not_analyzed",
            "type": "string"
        },
        "oid": {
            "doc_values": true,
            "index": "not_analyzed",
            "type": "string"
        },
        "ts": {
            "doc_values": true,
            "format": "dateOptionalTime",
            "type": "date"
        }
    }
}
```

Maybe I did something incorrectly?
</comment><comment author="heiwen" created="2016-07-24T07:13:21Z" id="234762145">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IP fields cause mapping conflicts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13112</link><project id="" key="" /><description>While using the 2.0 branch, @jbudz and I are noticing that `_default_` mappings always cause unresolvable type conflicts.

This is the mapping we are testing with:

``` curl
curl -XPOST localhost:9200/logstash-2015.08.25 -d '{
  "mappings":{
    "_default_": {
      "properties":{
        "ip":{
          "type":"ip"
        }
      }
    }
  }
}'
```

And the documents that we are sending:

```
curl -XPOST localhost:9200/_bulk -d '{"index":{"_index":"logstash-2015.08.25","_type":"apache"}}
{"index":"logstash-2015.08.25","ip":"192.168.1.1"}
{"index":{"_index":"logstash-2015.08.25","_type":"nginx"}}
{"index":"logstash-2015.08.25","ip":"192.168.1.1"}
'
```

But the bulk fails with `"Mapper for [ip] conflicts with existing mapping in other types:\n[mapper [ip] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types.]"`. 

Since we are using `_default_` as the type, it seems like `update_all_types` should not be needed, since we are not actually creating the types until the documents are indexed.
</description><key id="103131473">13112</key><summary>IP fields cause mapping conflicts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">spalger</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-25T22:34:25Z</created><updated>2015-09-14T17:16:37Z</updated><resolved>2015-09-01T18:44:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-26T09:31:14Z" id="134918117">@rjernst could you take a look please
</comment><comment author="clintongormley" created="2015-08-26T13:50:33Z" id="135027052">Actually, this isn't about default mappings, it is a problem with fields of type `ip`.  This request will replicate the same problem:

```
PUT my_index
{
  "mappings": {
    "one": {
      "properties": {
        "field": {
          "type": "ip"
        }
      }
    },
    "two": {
      "properties": {
        "field": {
          "type": "ip"
        }
      }
    }
  }
}
```
</comment><comment author="rjernst" created="2015-08-26T15:34:11Z" id="135066551">@clintongormley Yes, it affects ip field type. The problem is how the search analyzer for ip fields is setup. For other numeric fields, it sets up an index analyzer for the numeric precision, and a search analyzer for the "max" value, and generally has constants for each possible precision. However, ip fields create these (even the "max" on for search analyzer) on the fly every time. This exposed a bug in our comparison of search analyzer when checking compatibility: we are looking at reference equality, instead of the name of the analyzer.

When I found the bug, I realized we had a hole in the unit tests (it was something I was relying on existing mappings tests for, but that was obviously insufficient!). I have a fix ready, and have been working on an improved base test which will exhaustively test compatibility checks for every property, for every field type.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move position_offset_gap default change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13111</link><project id="" key="" /><description>Until a couple of hours ago we expected the position_offset_gap to default
to 0 in 2.0 and 100 in 2.1. We decided it was worth backporting that new
default to 2.0. So now that its backported we need to teach 2.1 that 2.0
also defaults to 100.
</description><key id="103112987">13111</key><summary>Move position_offset_gap default change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v2.1.0</label></labels><created>2015-08-25T20:45:41Z</created><updated>2015-08-26T09:27:21Z</updated><resolved>2015-08-25T22:01:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-25T20:46:39Z" id="134735311">@xuzha would you mind having a look at this? I think it makes master consistent with 2.0.
</comment><comment author="xuzha" created="2015-08-25T21:17:32Z" id="134744488">Left a minor comment. 

LGTM
</comment><comment author="nik9000" created="2015-08-25T22:01:29Z" id="134754962">Squashed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Please add alias creation for snapshot _restore API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13110</link><project id="" key="" /><description>The most frequent scenario for using _restore API is when something is wrong, and index needs to be restored. But in order to restore, we need to delete the old index.

What happens in most cases is the original index is
1. Old index in red state.
2. Old index was wiped out, data is repopulated and an index is created with the same name.

In this case, especially #2, it's actually not OK to delete the index as data will get lost. So what we normally do is restore to a different name and setup an alias with the original index name pointing to the newly restored index.

When we are restoring lots of indices, this is tedious and error prone. I propose to add a alias field for the _restore API, like below:

```
POST /_snapshot/my_backup/snapshot_1/_restore
{
    "indices": "index_1", 
    "rename_pattern": "index_(.+)", 
    "rename_replacement": "restored_index_$1",
    "alias": "$1"            &lt;=== 
}
```
</description><key id="103104195">13110</key><summary>Please add alias creation for snapshot _restore API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkliu</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2015-08-25T20:00:09Z</created><updated>2015-08-28T09:39:23Z</updated><resolved>2015-08-28T09:39:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-26T11:30:17Z" id="134956540">You can't create an alias with the same name as an index, so (if i understand scenario 2 above) you wouldn't be able to use the data from index `foo` plus the data from index `restored_index_foo` via an alias called `foo`.

Instead, you should **always** use aliases to point to the real index name.  That way you can restore your index and update the alias to point to both indices (and not have to change your application).  Of course, that is only one thing you may want to do.  You may also want to update the alias to point to just the restored index.

As soon as there are a number of paths to choose (different people will want to do different things at different times) then the number of options you have to add to support these use cases multiplies and becomes confusing.  

We have APIs to manage aliases which already provide the tools to support all the use cases.  It is pretty easy to write a script to suit your exact use case, certainly easier than trying to add a range of parameters to the snapshot/restore API.

For this reason I'm -1 on adding this.
</comment><comment author="dadoonet" created="2015-08-28T09:39:23Z" id="135717684">Hi,

We discussed this today during our fixit friday meetup and this is something we won't support in elasticsearch itself.
You should always check if your index have been restored correctly before switching an alias.

If you are using curator, may be this is a feature which can be supported there?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More portable extraction of short hostname</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13109</link><project id="" key="" /><description>This commit increases the portability of extracting the short hostname
on a Unix-like system.

Closes #13107
</description><key id="103080554">13109</key><summary>More portable extraction of short hostname</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-25T17:48:37Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-25T18:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-25T18:00:45Z" id="134686600">This probably deserves a comment or some helpful person will revert it back to `hostname -s`. Which unices don't have `-s`, btw?
</comment><comment author="nik9000" created="2015-08-25T18:01:06Z" id="134686679">This probably deserves a comment or some helpful person will revert it back to `hostname -s`. Which unices don't have `-s`, btw?

&gt; This probably deserves a comment or some helpful person will revert it back to hostname -s. Which unices don't have -s, btw?

Ah. Solaris 10.
</comment><comment author="nik9000" created="2015-08-25T18:01:53Z" id="134686873">Otherwise LGTM.
</comment><comment author="jasontedor" created="2015-08-25T18:39:51Z" id="134696971">@nik9000 Also HP-UX.

I've added a comment as you requested. I also discovered in the course of testing this is on Solaris VM that combining the variable definition and export into a single line isn't supported in some shells (namely the Bourne shell which is the default shell on Solaris). I've found a more portable way to do the export as well. I've updated the pull request accordingly.
</comment><comment author="nik9000" created="2015-08-25T18:45:25Z" id="134698407">&gt; I've found a more portable way to do the export as well. I've updated the pull request accordingly.

Bleh. I'm not against just using `#!/bin/bash` instead of `#!/bin/sh`. 

Thanks for adding the comment - that is perfect.
</comment><comment author="jasontedor" created="2015-08-25T18:50:53Z" id="134699755">&gt; I'm not against just using `#!/bin/bash` instead of `#!/bin/sh`.

@nik9000 I've been contemplating that as well but at a minimum it is a separate issue from this one.
</comment><comment author="nik9000" created="2015-08-25T19:11:00Z" id="134704847">&gt; @nik9000 I've been contemplating that as well but at a minimum it is a separate issue from this one.

Yeah, probably.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>VersionConflictEngineException with scrolling search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13108</link><project id="" key="" /><description>We do a scan/scoll search, looking for document to update, with version enabled on search results.
For each hit, we update some fields, then index the document providing the same version as the one we got on the hit. Sometimes we get VersionConflictEngineException because the document was updated in the mean time, but this time it is "stuck" (same document fails for several hours).

```
failure in bulk execution:
[0]: index [index_v2], type [DOCUMENT], id [DOCUMENT_112_153fb1f3ccc7], message [VersionConflictEngineException[[index_v2][3] [DOCUMENT][DOCUMENT_112_153fb1f3ccc7]: 
version conflict, current [1], provided [5]]]
[1]: index [index_v2], type [DOCUMENT], id [DOCUMENT_113_c9b12189c917], message [VersionConflictEngineException[[index_v2][4] [DOCUMENT][DOCUMENT_113_c9b12189c917]: 
version conflict, current [1], provided [4]]]
```

How can the version provided be higher than the current version, when we got the version number from elasticsearch? We get the same version numbers each time.

Between updates we do a refresh to make sure that all the updated documents are visible, in case they need to be updated again.

ES version 1.5.2
</description><key id="103058310">13108</key><summary>VersionConflictEngineException with scrolling search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">henrikno</reporter><labels /><created>2015-08-25T15:58:23Z</created><updated>2015-08-25T16:08:37Z</updated><resolved>2015-08-25T16:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="henrikno" created="2015-08-25T16:08:37Z" id="134652755">Sorry, it was a bug in our code which searched in one index and indexed in another (reindexing using aliases, which was swapped).
Rubber ducking helps.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"hostname -s" not available in at least Solaris 10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13107</link><project id="" key="" /><description>Hi folks,

I just encountered an issue with elasticsearch 1.7.1. I tried to upgrade from ES 1.4.4 to 1.7.1.

ES does run as unprivileged user.

Error output while starting:

&gt; bin/elasticsearch: HOSTNAME=: is not an identifier

It seems that in contrast to 1.4.4 there is line 151 of

&gt; elasticsearch-1.7.1/bin/elasticsearch

which tries to export the environment variable "HOSTNAME". The value is assigned via "hostname -s" which is not available in Solaris.

This feature seems to have been introduced in #8470. Since I have defined my node name via "node.name" this is not quite an issue with exporting the HOSTNAME variable but rather with the switch that is not supported on Solaris.

And yes. I apologize for still being forced to use Solaris, but that's the way it is ;)

Cheers
Thomas
</description><key id="103052834">13107</key><summary>"hostname -s" not available in at least Solaris 10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">TwizzyDizzy</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-25T15:42:57Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-08-25T18:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-25T16:08:16Z" id="134652495">Hi @TwizzyDizzy 

Any ideas what we could use instead on Solaris?
</comment><comment author="nik9000" created="2015-08-25T18:03:33Z" id="134687524">Looks like we have a working patch so I removed feedback_needed. Then I thought better of it. @TwizzyDizzy, can you try `hostname | cut -d. -f1` on a solaris 10 box?
</comment><comment author="TwizzyDizzy" created="2015-08-25T18:09:57Z" id="134689564">Hi @nik9000,

thanks for getting back to me that fast!

&gt; hostname | cut -d. -f1

This should totally work, but let me check it at work tomorrow to be totally sure. I somehow remember some glitches with either the delimiter or the field number, when it comes to the exact syntax.

Will get back to you tomorrow :)

Cheers
Thomas
</comment><comment author="jasontedor" created="2015-08-25T18:27:58Z" id="134694136">@TwizzyDizzy I've tested Elasticsearch with `hostname | cut -d. -f1` on a Solaris VM. You should be able to get by with replacing `hostname -s` with `hostname | cut -d. -f1` in the `elasticsearch` and `plugin` scripts in the `bin` subfolder of the Elasticsearch home. Please see the pull request #13109 I've opened for the full details.

```
-bash-4.3$ uname -a
SunOS shortname.domainname.tld 5.10 Generic_147148-26 i86pc i386 i86pc
-bash-4.3$ ./bin/elasticsearch
[2015-08-25 23:30:59,373][INFO ][org.elasticsearch.node   ] [shortname] started
```
</comment><comment author="TwizzyDizzy" created="2015-08-25T18:32:13Z" id="134695044">Ah well, if you've tested it in a VM then I guess it's fine.

Cheers
Thomas
</comment><comment author="TwizzyDizzy" created="2015-08-26T08:31:34Z" id="134897340">Just for the full picture: the

&gt; | cut -d. -f1

part works on my machine, too.

Cheers
Thomas
</comment><comment author="nik9000" created="2015-08-26T13:38:20Z" id="135024253">&gt; part works on my machine, too.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disk usage should report min/max values for multiple paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13106</link><project id="" key="" /><description>Nodes using multiple paths for the `data/` directory should report the min and max available space across paths.  The min space should be used to decide when to move shards away, while the max space can be used to decide when to move shards to the node.
</description><key id="103047851">13106</key><summary>Disk usage should report min/max values for multiple paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Allocation</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-25T15:25:21Z</created><updated>2015-09-14T17:17:16Z</updated><resolved>2015-08-31T14:25:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Prompted for 'tFormat' username when attempting to remove node from cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13105</link><project id="" key="" /><description>Hi

I'm trying to remove a node from our cluster and have tried running the following curl command via windows powershell with curl installed.

```
  curl  -XPUT http://localhost:9200/_cluster/settings -d { 
       \"transient\":
    { \"cluster.routing.allocation.exclude._ip\":\"10.50.14.120\" }
}
```

I run the command and then get 'Enter host password for user 'tFormat':

This isn't a user that has been setup by us so I'm not sure what this relates to?

Thanks
</description><key id="103037488">13105</key><summary>Prompted for 'tFormat' username when attempting to remove node from cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tomharrisonJG</reporter><labels /><created>2015-08-25T14:41:12Z</created><updated>2015-08-25T16:04:55Z</updated><resolved>2015-08-25T16:04:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-25T16:04:55Z" id="134650569">This isn't anything that we have in Elasticsearch. Either you have some proxy in front of Elasticsearch, or if you're using Shield then I'd suggest contacting your support team.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>log message [favicon.ico] IndexNotFoundException[no such index]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13104</link><project id="" key="" /><description>I get this randomly in my logs with master (bfa3e47):

```
[2015-08-25 16:12:42,693][INFO ][rest.suppressed          ] /favicon.ico Params: {index=favicon.ico}
[favicon.ico] IndexNotFoundException[no such index]
    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:551)
    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:127)
    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:71)
    at org.elasticsearch.action.admin.indices.get.TransportGetIndexAction.checkBlock(TransportGetIndexAction.java:63)
    at org.elasticsearch.action.admin.indices.get.TransportGetIndexAction.checkBlock(TransportGetIndexAction.java:47)
    at org.elasticsearch.action.support.master.TransportMasterNodeAction.innerExecute(TransportMasterNodeAction.java:94)
    at org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:86)
    at org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:48)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
    at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1162)
    at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.getIndex(AbstractClient.java:1262)
    at org.elasticsearch.rest.action.admin.indices.get.RestGetIndicesAction.handleRequest(RestGetIndicesAction.java:83)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:122)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:84)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:348)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

I cannot reproduce this but it seems this happens when the node starts up and I fire a query like `http://localhost:9200/test/_search`. 
</description><key id="103034047">13104</key><summary>log message [favicon.ico] IndexNotFoundException[no such index]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-08-25T14:25:13Z</created><updated>2015-08-25T15:23:02Z</updated><resolved>2015-08-25T15:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-25T14:46:11Z" id="134610067">Sounds like related to this change: 91d16f45c65e07e727d290d30665885a16fe1a39
</comment><comment author="clintongormley" created="2015-08-25T15:17:04Z" id="134619540">Fixed by fb7142cdfb0b1458d12db0c737e4c56443bc407e
</comment><comment author="s1monw" created="2015-08-25T15:23:02Z" id="134623453">this is your browser but it will only do it once - we now log exception on the accepting node if the user doesn't ask for stacktraces...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow validating that settings update doesn't produce invalid combinations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13103</link><project id="" key="" /><description>User can change their (dynamically updatable) cluster settings through the cluster settings API. To make sure the new settings make sense, we have an internal validation infra structure that can reject illegal values (for example, a negative value where a positive is expected). However, the current infrastructure only allows you to validate each entry of the new settings (in the context of the current cluster state) but doesn't not allow to validate it with respect to other settings in the update.

For example, say we have two settings `a` and `b` and we have the requirement that `a` is always smaller than `b` - we currently have now wait of making this request fail:

```
PUT _cluster/settings
{
  "persistent": {
    "a": 5,
    "b": 3
  }
}
```
</description><key id="103011544">13103</key><summary>Allow validating that settings update doesn't produce invalid combinations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-08-25T12:30:03Z</created><updated>2016-01-28T14:18:59Z</updated><resolved>2016-01-28T14:18:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T13:04:35Z" id="176174106">@bleskes Do you have a concrete example of this?  Is there more to do here than has been done with the recent settings rewrite?
</comment><comment author="bleskes" created="2016-01-28T14:18:59Z" id="176203864">Sadly I don't remember exactly anymore. Examples I can come up with now (though i wish I had a better one) is the commit_timeout (how long to wait for nodes to respond for the intial sending of a cluster state change) which has to be less than publishing timeout (total time for nodes to respond to both the initial send and the commit phase).  I'm sure there are better more concrete ones. 

The new infra does allow you to do this but it's non trivial at all. In the old system it was  easier (because we didn't have many of the rigorous checks and infra we have now) so I think it's an easy win. I'm going to close this for now. We can re-evaluate it when we need this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security Manager behave differently between 2.0 and 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13102</link><project id="" key="" /><description>Based on discussion which happened here: https://github.com/elastic/elasticsearch/pull/13001#issuecomment-134245012

I was doing some tests for the mapper attachment plugin which consist basically of reading an encrypted PDF document. This should raise an exception with a message like `The supplied password does not match either the owner or user password in the document`. See disabled test for more info: https://github.com/elastic/elasticsearch-mapper-attachments/commit/7dfaef491621c371605b1e0decd05d9136347aae

But I found the behavior different from 2.0 branch and master branch.
- With 2.1.0, I get the expected exception and the related message.
- With 2.0.0, I get a security manager exception which says that "insertProvider.BC" is forbidden.

Opening this issue so we can try to reproduce this and fix if needed.
</description><key id="103011324">13102</key><summary>Security Manager behave differently between 2.0 and 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>discuss</label></labels><created>2015-08-25T12:28:26Z</created><updated>2015-11-23T12:20:31Z</updated><resolved>2015-11-23T12:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Send response for update request when it timed out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13101</link><project id="" key="" /><description>otherwise the request will hang
</description><key id="103004919">13101</key><summary>Send response for update request when it timed out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v2.0.0</label></labels><created>2015-08-25T11:45:27Z</created><updated>2015-10-07T10:53:22Z</updated><resolved>2015-10-07T10:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-15T22:08:02Z" id="140560419">Left minor comments. This seems important to get into 2.0.0.
</comment><comment author="bleskes" created="2015-09-16T10:01:11Z" id="140693803">Left comments...
</comment><comment author="brwe" created="2015-09-22T13:49:36Z" id="142294604">@nik9000 @bleskes thanks a lot for the review! I addressed all comments. I also found another bug: If `resolveRequest(..)` returned `false` then the request would also hang. Fixed and added a unit test for it.
</comment><comment author="bleskes" created="2015-09-22T15:19:51Z" id="142321677">Thx @brwe - this looks great. Left some comments here and there..
</comment><comment author="brwe" created="2015-09-23T14:15:22Z" id="142614661">@bleskes I changed the tests now so that there is dedicated tests for immediate failure, locally triggered retry, triggered retry from remote etc. I like it better now but am sure you will find more :)
</comment><comment author="bleskes" created="2015-10-06T07:26:02Z" id="145766844">This looks great. Thx @brwe for all the iterations. I left some minor comments. No need for another review imho.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>using default_operator "AND" causes query_strings actually containing AND to return wrong results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13100</link><project id="" key="" /><description>If I have a query string like "one AND two OR three", I expect the results to be the same as one of "(one AND two) OR three" or "one AND (two OR three)" but it seems like they are actually the same as "one OR two OR three".

Also, if the default_operator is AND, actually I expect "one two OR three" to work either like "(one AND two) OR three" or "one AND (two OR three)" -- but instead it works like "one OR two OR three".

Here's an example:

```
# create a test index
curl -XPUT 'localhost:9200/test'

# index a doc with a field containing the text "one"
curl -XPUT 'localhost:9200/test/mytype/1' -d '
{
    "text": "one"
}'

# query the index with no default_operator, for "one AND two OR three" (0 results as expected)
curl -XGET 'localhost:9200/test/mytype/_search?pretty' -d '
{
    "query": {
        "query_string": {
            "default_field": "_all", 
            "query": "text:(one AND two OR three)"
        }
    }
}'

# query with default operator, "(one AND two) OR three" (0 results as expected)
# "one AND (two OR three)" also gives the expected 0 results
curl -XGET 'localhost:9200/test/mytype/_search?pretty' -d '
{
    "query": {
        "query_string": {
            "default_field": "_all", 
            "default_operator": "AND", 
            "query": "text:((one AND two) OR three)"
        }
    }
}'

# query "one AND two OR three" now with default operator, returns one result but I expect 0
curl -XGET 'localhost:9200/test/mytype/_search?pretty' -d '
{
    "query": {
        "query_string": {
            "default_field": "_all", 
            "default_operator": "AND", 
            "query": "text:(one AND two OR three)"
        }
    }
}'
```

Using elasticsearch-1.7.1. 
</description><key id="102999143">13100</key><summary>using default_operator "AND" causes query_strings actually containing AND to return wrong results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magdalene</reporter><labels /><created>2015-08-25T10:56:26Z</created><updated>2015-08-25T12:31:07Z</updated><resolved>2015-08-25T11:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sarwarbhuiyan" created="2015-08-25T11:15:20Z" id="134556929">This may help: 
https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html?q=query_string#_boolean_operators

Especially, "While the + and - only affect the term to the right of the operator, AND and OR can affect the terms to the left and right."

By the way, there are several problems with your specific queries:
1. Since you are prefixing your query with text: already, is there a need for default_field? 
2. Since you're putting using AND and OR in the query, what's the point of the default_operator?

You're probably using the +/- syntax or using the match query DSL though.
</comment><comment author="clintongormley" created="2015-08-25T11:20:15Z" id="134557809">The problem is that the query string does not use pure boolean logic.  It is intended to be used as a query, not as a filter.  Queries have required matches (must) and optional matches (should) which are not required but improve the score if they are present.

Worth reading this blogpost to understand more: https://lucidworks.com/blog/why-not-and-or-and-not/
</comment><comment author="clintongormley" created="2015-08-25T11:21:23Z" id="134558014">As @sarwarbhuiyan said, you're better off using the query DSL if you want real boolean logic.
</comment><comment author="magdalene" created="2015-08-25T11:46:27Z" id="134561843">Thanks! You're probably right it's better to simply always use +/- but at this point that would require us to (further) translate user queries. 

However, I still don't understand the default_operator behavior -- why for the query "x AND y OR z" is the behavior different when the default_operator is "AND" vs. when it's not specified (and from the docs I understand defaults to OR)?

To answer @sarwarbhuiyan -- initially I was trying to understand why "x y OR z" returned results with only x and not y and not z with the default_operator AND. And sorry about the default_field/specifying field in query string redundancy, unfortunately I constructed the test query from various sources. I don't think this is related to the behavior I see though.  My point is actually about the default_operator behavior, not about the AND/OR operators.
</comment><comment author="magdalene" created="2015-08-25T11:51:07Z" id="134562384">Also according to that linked doc page (unless I understood incorrectly, again!) "a AND b OR c" should be equivalent to "(a AND b) OR c" because AND takes precedence -- and in case the default_operator is unspecified, it seems to be, but in case the default_operator is AND, it is not. This is the behavior I am trying to understand/work around.
</comment><comment author="clintongormley" created="2015-08-25T12:03:32Z" id="134564261">The answer is explained in that blog post, to quote:

&gt; Things definitely get very confusing when these &#8220;boolean operators&#8221; are used in ways other then those described above. In some cases this is because the query parser is trying to be forgiving about &#8220;natural language&#8221; style usage of operators that many boolean logic systems would consider a parse error. In other cases, the behavior is bizarrely esoteric:
&gt; - Queries are parsed left to right
&gt; - NOT sets the Occurs flag of the clause to it&#8217;s right to MUST_NOT
&gt; - AND will change the Occurs flag of the clause to it&#8217;s left to MUST unless it has already been set to MUST_NOT
&gt; - AND sets the Occurs flag of the clause to it&#8217;s right to MUST
&gt; - If the default operator of the query parser has been set to &#8220;And&#8221;: **OR will change the Occurs flag of the clause to it&#8217;s left to SHOULD** unless it has already been set to MUST_NOT
&gt; - OR sets the Occurs flag of the clause to it&#8217;s right to SHOULD

Frankly, these rules are just too hard to remember.  This is one of the many reasons I don't like using the `query_string` query at all.  Here's another reason.  Look at these two queries for example:

```
http://foo   # finds an empty regex in field `http` and `foo` in the `_all` field
http://foo/  # throws a malformed regex exception
```

If you want to understand how the query string syntax is being understood, then use the validate-query API:

```
GET _validate/query?explain
{
  "query": {
    "query_string": {
      "query": "x AND y OR z",
      "default_operator": "OR"
    }
  }
}
```
</comment><comment author="magdalene" created="2015-08-25T12:31:07Z" id="134570640">Thanks so much for explaining!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc_values doesn't work with keyword tokenizers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13099</link><project id="" key="" /><description>I've tried enabling doc_values on a keyword tokenized string field (lowercase and icu_collation filter is tried) and it fails with:

```
org.elasticsearch.index.mapper.MapperParsingException: Expected map for property [fields] on field [doc_values] but got a class java.lang.String
        at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:292)
        at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214)
        at org.elasticsearch.index.mapper.object.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:136)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:211)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:192)
        at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:449)
        at org.elasticsearch.cluster.metadata.MetaDataMappingService$4.execute(MetaDataMappingService.java:505)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:374)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:196)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:162)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

I guess this should be a perfectly valid combination.

Examples used:

```
{"tokenizer": "keyword","filter": ["lowercase"]}
```

and 

```
{'subject':{"type": "string", "analyzer": "lc"},"doc_values": true}
```

elasticsearch version 1.7.1, icu plugin: 2.7.0
</description><key id="102989769">13099</key><summary>doc_values doesn't work with keyword tokenizers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bra-fsn</reporter><labels><label>:Mapping</label></labels><created>2015-08-25T09:50:26Z</created><updated>2015-08-25T11:06:35Z</updated><resolved>2015-08-25T10:02:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-08-25T10:02:04Z" id="134543907">Currently, doc values are not supported on analyzed string fields. There is an [open issue](https://github.com/elastic/elasticsearch/issues/12394) to address this situation in a future release of Elasticsearch.

Relates #10061 and #12394.
</comment><comment author="bra-fsn" created="2015-08-25T11:06:35Z" id="134555714">@jasontedor: ah, thanks, I couldn't find it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused code from query_string parser and settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13098</link><project id="" key="" /><description>Remove unused code from query_string parser and settings
</description><key id="102980540">13098</key><summary>Remove unused code from query_string parser and settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-25T08:53:31Z</created><updated>2015-08-25T11:21:36Z</updated><resolved>2015-08-25T11:03:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-25T10:50:50Z" id="134553852">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split cloud-aws into repository-s3 and discovery-ec2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13097</link><project id="" key="" /><description>Until now we had a cloud-aws plugin which is providing 2 disctinct features:
- discovery on EC2
- snapshot/restore on S3

This commit splits the plugin by feature so people can use either one or the other or both features.

Doc is updated accordingly.

If we do this split, we will also need to add new github labels `:Plugin Repository S3` and `:Plugin Discovery EC2`.
</description><key id="102978407">13097</key><summary>Split cloud-aws into repository-s3 and discovery-ec2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>:Plugins</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-08-25T08:47:09Z</created><updated>2016-03-10T18:55:02Z</updated><resolved>2015-09-03T09:14:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-25T09:31:45Z" id="134539073">@rjernst Thanks for the comment. Yeah it makes more sense to me. And BTW it's conform to the documentation titles I added :) 

PR updated for discovery and repository
</comment><comment author="dadoonet" created="2015-08-25T17:20:40Z" id="134676315">@jpountz I addressed your comments (and thanks BTW :) )
</comment><comment author="jpountz" created="2015-08-26T07:25:32Z" id="134875034">LGTM. But this probably deserves a second pair of eyes as I am not familiar with this part of the code, so I could easily miss things.
</comment><comment author="dadoonet" created="2015-08-26T07:27:30Z" id="134875429">Thank you @jpountz. @rjernst wanna give a final look?

Then I'll do the same with gce and move it as a discovery plugin (basically renaming only) and azure which needs to be split as well in 2 plugins.
</comment><comment author="rjernst" created="2015-08-26T15:27:14Z" id="135062675">LGTM, I left a couple minor comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Generate list of built-in  plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13096</link><project id="" key="" /><description>we maintain a list of build-in plugins in multiple places like in the PluginsManager as well as in the `qa/smoke-test-plugins` project. We will soon add a third after #13078 is merged. We should try to generate it somehow from our build files.
</description><key id="102975779">13096</key><summary>Generate list of built-in  plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>adoptme</label><label>test</label></labels><created>2015-08-25T08:29:58Z</created><updated>2017-03-21T18:56:49Z</updated><resolved>2017-03-21T18:56:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-25T15:06:19Z" id="134615597">`qa/vagrant/pom.xml` has a list too. Sadly I don't think there is much we can do about automating the list in maven. @rjernst this is probably more fuel for the burn maven with fire movement.
</comment><comment author="rjernst" created="2017-03-13T00:31:08Z" id="285990942">The static list of plugins has already been removed from the plugin installer a while ago, as well as from the build. The only remaining place is in the smoke tester, so there is one last cleanup needed for this issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RecoveryFailedException: NegativeArraySizeException when marking and sending shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13095</link><project id="" key="" /><description>Elasticsearch version: 1.7.1 - 24 data nodes, 3 masters, 3 clients.

```
Settings (transient)
indices.recovery.translog_size 512kb
indices.recovery.concurrent_streams 3
indices.recovery.translog_ops 1000
indices.recovery.max_bytes_per_sec 40mb
indices.recovery.file_chunk_size 512kb
cluster.routing.allocation.cluster_concurrent_rebalance 2
cluster.routing.allocation.node_concurrent_recoveries 2

Settings (yml)
index.refresh_interval 60s
indices.recovery.max_bytes_per_sec 200mb
```

I get Exceptions when ES moves shards or enables replication. Some replications are stuck in INITIALIZATION. It tries to INIT, then falls back to UNASSIGNED. Some INIt on replicas and some relocations seems to run OK (/_cat/recovery).

I would estiamte this is affected by almost all indices (see many in the log). And it seems not to be only one shard per index. The consistent error is the **Caused by: java.lang.NegativeArraySizeException**. The last Caused by seems to go into recursion and we get a ton of **Suppressed: ..**

The exceptions comes "now and then". If I increase indices.recovery to concurrent_streams 24, max_bytes_per_sec 512mb it doesn't increase the Exception rate. **However**, if I increase node_concurrent_recoveries beyond 2, e.g. to 3, the number of Exceptions explodes - flooding with tens or hundreds per second on each node. 

```
[2015-08-25 07:46:23,207][WARN ][indices.cluster          ] [l03-data-01] [[storm-today][9]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [storm-today][9]: Recovery failed from [l05-data-04][yQg00N7RTzOGHT3UQ-6Sjg]l05][inet[/192.168.1.18:9304]]{rack_id=A25, master=false} into [l03-data-01][7fzOuK7ZRFuhN0a1UlcXIQ][l03][inet[192.168.1.16/192.168.1.16:9301]]{rack_id=A25, master=false}
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
        at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [l05-data-04][inet[/192.168.1.18:9304]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [storm-today][9] Phase[1] Execution failed
        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:883)
        at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
        at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [storm-today][9] Failed to transfer [13] files with total size of [11.2gb]
        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:430)
        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:878)
        ... 10 more
Caused by: java.lang.NegativeArraySizeException
        at org.elasticsearch.indices.recovery.RecoverySourceHandler$3.doRun(RecoverySourceHandler.java:280)
        ... 4 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
        Suppressed: java.lang.NegativeArraySizeException
                ... 5 more
```
</description><key id="102973768">13095</key><summary>RecoveryFailedException: NegativeArraySizeException when marking and sending shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kjelle</reporter><labels><label>:Recovery</label><label>:Settings</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-25T08:17:41Z</created><updated>2016-01-28T13:00:44Z</updated><resolved>2016-01-28T13:00:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-25T10:34:03Z" id="134549668">~~This looks a lot like file corruption, presumably related to the RAID error that you mentioned in~~ #13012

```
final int BUFFER_SIZE = (int) recoverySettings.fileChunkSize().bytes();
final byte[] buf = new byte[BUFFER_SIZE];
```

Hmmm, that's a setting, not the file size...

@mikemccand @bleskes any ideas?
</comment><comment author="kjelle" created="2015-08-25T11:05:12Z" id="134555542">@clintongormley I'v decomissioned our node with the faulty RAID0 and wiped indices and reloading all the data to see if the problem will go away. It seems to be that if an index has resided on the faulty RAID0 all seems well unit this shard is relocated, or I try to generate a replica from it. Let's see if removing one of my nodes reveals anything new.
</comment><comment author="kjelle" created="2015-08-25T11:16:04Z" id="134557030">We'll at the same time remove RAID0 and stripe using multi path in ES yaml.
</comment><comment author="clintongormley" created="2015-08-25T11:25:03Z" id="134558678">@kjelle note that there is a big change in multi-path coming in 2.0.  In 1.x, the files from a single shard are striped across multiple paths, which means that failure on any one path can corrupt all shards on that node.  In 2.0, each shard is written to a single data path only, so failure of that path will only affect the shards on that path.  When first upgrading to 2.0, segment files are moved around to ensure that each shard lives in a single path.
</comment><comment author="kjelle" created="2015-08-25T11:27:33Z" id="134559102">@clintongormley thanks for that info. I decided on RAID0 to be able to utilize the whole IOPS of 6 disks as one, instead one by one (as you mention for 2.x). We'll see how it performance, and switch back if necessary
</comment><comment author="kjelle" created="2015-08-26T11:49:33Z" id="134961215">@clintongormley the problem is with the code in v1.7.1. If you set equal to or higher than 2048mb (2147483648 bytes) you go pass max int for a _32bit Java int_ (2147483647) which results in _-2147483648_.

An interesting observation is that the Exception hits after a while (5minutes+ and after 15ish percent) and not initially

I see that in the [v2.0 branch of RecoverySourceHandler](https://github.com/elastic/elasticsearch/blob/2.0/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java#L296) it has been fixed;

```
final int BUFFER_SIZE = (int) Math.max(1, recoverySettings.fileChunkSize().bytes()); // at least one!
```
</comment><comment author="clintongormley" created="2015-08-26T12:25:20Z" id="134982750">OK, that was changed in https://github.com/elastic/elasticsearch/pull/12919

According to the settings you provided in the description, you'd only set `indices.recovery.file_chunk_size` to `512kb`. Did you try setting it to a different too-high value?

Either way, we should probably add some validation around that setting.
</comment><comment author="kjelle" created="2015-08-26T12:27:56Z" id="134983143">@clintongormley yes, i was setting it up to 2048mb. I guess the reason for error while i had it at 512kb was that some other recovery handler had the 2048mb set from a previous run?

After the clean install it was much clearer that setting this to 2048mb was the cause of the problem.
</comment><comment author="clintongormley" created="2016-01-28T13:00:44Z" id="176171862">Nothing more to do here. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return error for invalid search exists api query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13094</link><project id="" key="" /><description>If you send invalid query like this:

```
POST /someindex/_search/exists
{
  "size": 0,
  "query": {
    "match_all": {}
  }
}
```

it returns `{"exits": false}` while it logs exception like:

```
[2015-08-25 12:29:23,958][DEBUG][action.exists            ] [Stellaris] [someindex][3], node[Qm1-IUusSuioZ_r-T-daSg], [P], s[STARTED]: failed to execute [[[someindex]][], source[{
  "size": 0,
  "query": {
    "match_all": {}
  }
}
]]
org.elasticsearch.index.query.QueryParsingException: [someindex] request does not support [size]
    at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:360)
    at org.elasticsearch.action.exists.TransportExistsAction.shardOperation(TransportExistsAction.java:187)
    at org.elasticsearch.action.exists.TransportExistsAction.shardOperation(TransportExistsAction.java:66)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Shouldn't it return error in this case?
</description><key id="102941825">13094</key><summary>Return error for invalid search exists api query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Search</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-08-25T03:31:30Z</created><updated>2015-10-21T16:48:21Z</updated><resolved>2015-10-21T16:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-25T08:32:23Z" id="134523174">+1
</comment><comment author="harshita1931" created="2015-08-25T13:06:01Z" id="134578894">hello everyone :)
I am new to open source programming and I want to learn the same. I wanted to do some bug fixing so as to get an idea of how things are done. I saw this issue and want to work on it. Kindly help me by telling me in brief how to start on this. I would be greatly thankful to you all :) 
</comment><comment author="johtani" created="2015-09-30T16:53:41Z" id="144474335">I try to execute this invalid query on 2.0.0-beta2, then I can not see an Exception in logs.
</comment><comment author="javanna" created="2015-10-21T16:48:21Z" id="149958410">We decided to remove the search exists api in #13682. It is now deprecated in 2.1 and 2.x branches and removed from master. We can close this as won't fix then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Bootstrap.java startup validation accessible for embedded use</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13093</link><project id="" key="" /><description>Many of the methods are non-public.

Some of the startup validation is mixed with actually starting a node.
</description><key id="102919994">13093</key><summary>Make Bootstrap.java startup validation accessible for embedded use</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-08-25T00:27:50Z</created><updated>2015-08-27T20:27:56Z</updated><resolved>2015-08-27T20:26:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-25T10:19:11Z" id="134547657">Hi @nickminutello 

Currently we don't test embedded Elasticsearch at all, which means that it is not a supported configuration.  We have just gone through a large number of changes to lock Bootstrap down, to simplify the code and configuration, and to make startup more reliable, so any changes that we make now will have to be carefully considered.

Could you be more specific about what you need changed and why? (and I assume you're referring to 2.0?)
</comment><comment author="nickminutello" created="2015-08-26T00:35:23Z" id="134778877">I'm looking at master.

More specifically:
- make Bootstrap class public
- have a method called, say, prepare() which calls initializeNatives(), initializeProbes(), JarHell.checkJarHell(), JVMCheck.check(), clientJvmWarning()**, 
- setup() then just has to call prepare().
- all the startup/init/validation in 1 place.
- but also, those running elastic embedded, can perform the same.

*\* this method doesnt exist - but its the if case checking if running in the client vm.
</comment><comment author="clintongormley" created="2015-08-26T10:06:35Z" id="134931971">thanks @nickminutello 
</comment><comment author="s1monw" created="2015-08-27T20:26:11Z" id="135544892">&gt; Currently we don't test embedded Elasticsearch at all, which means that it is not a supported configuration.

this is actually not true. We test that all over the place. Embedded ES doesn't have Boostrap as it's entry point. If you want to embed ES you should use `NodeBuilder.java` and start from there. Every Integration we run against `InternalTestCluster` test is basically embedding Elasticsearch an it works just fine without opening up bootstrap. Bootstrap should be private as much as possible it's basically our main method.
</comment><comment author="rjernst" created="2015-08-27T20:27:56Z" id="135545604">@nickminutello Bootstrap is an internal implementation detail, and using it was never officially supported. As Clint mentioned, we don't (officially) test or support running Elasticsearch embedded. Can you elaborate on _why_ you need to run it embedded? Is it to run node client? Are you running a single node standalone? Something else?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update joda-time to 2.8.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13092</link><project id="" key="" /><description>## Changes in 2.8.2
- DateTimeZone data updated to version 2015f
## Changes in 2.8.1
- Fixed to handle JDK 8u60 [288, 291]
  Without this fix, formatting a time-zone will print "+00:00" instead of "GMT" for the GMT time-zone
- DateTimeZone data updated to version 2015e

Note that this change is required to make #12859 working. Otherwise, JODA version packaged with elasticsearch is still 2.8.0 so the AWS plugin won't be able to work as expected with change #12859.
</description><key id="102905300">13092</key><summary>Update joda-time to 2.8.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Dates</label><label>blocker</label><label>review</label><label>upgrade</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T22:30:21Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-25T08:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-24T22:58:58Z" id="134405684">Tests passed.
</comment><comment author="bleskes" created="2015-08-25T07:44:46Z" id="134512535">LGTM. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add migration guide notes for multicast moving to a plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13091</link><project id="" key="" /><description>See #13027
</description><key id="102892070">13091</key><summary>Add migration guide notes for multicast moving to a plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T21:09:15Z</created><updated>2015-09-14T17:14:58Z</updated><resolved>2015-08-27T02:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-25T10:09:04Z" id="134545906">LGTM - just add a link to the plugin docs? `{plugins}/discovery-multicast.html`

Could you also add a link to the DBQ plugin in the section just above it? `{plugins}/plugins-delete-by-query.html`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update AWS SDK to 1.10.12</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13090</link><project id="" key="" /><description>Release notes:
- [1.10.11](http://aws.amazon.com/releasenotes/Java/5199590350929641)
- [1.10.12](http://aws.amazon.com/releasenotes/Java/6635368276326731)
</description><key id="102891846">13090</key><summary>Update AWS SDK to 1.10.12</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>upgrade</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T21:07:36Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-24T21:10:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-24T21:08:35Z" id="134380607">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>delete-by-query documentation url incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13089</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/blob/master/plugins/delete-by-query/rest-api-spec/api/delete_by_query.json refers to https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-delete-by-query.html which is a 404.
</description><key id="102887870">13089</key><summary>delete-by-query documentation url incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Delete By Query</label><label>docs</label></labels><created>2015-08-24T20:46:47Z</created><updated>2015-08-25T09:58:58Z</updated><resolved>2015-08-25T09:58:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>matched_queries is empty when filtered query with nested filter is missing must_not clause</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13088</link><project id="" key="" /><description>The matched_queries item in the response is missing when using a filtered query with nested filter that does not have a must_not clause.

ES version 1.5.2

Steps to reproduce:

Create index and mapping

```
PUT /test_index

PUT /test_index/doc_type/_mapping
{
  "doc_type": {
    "dynamic": "strict",
    "properties": {
      "nestedDoc": {
        "type": "nested",
        "dynamic": "strict",
        "properties": {
          "field1": {
            "type": "string"
          }
        }
      }
    }
  }
}
```

Add a document

```
PUT /test_index/doc_type/1
{
  "nestedDoc": [
    {
      "field1": "a"
    },
    {
      "field1": "b"
    }
  ]
}
```

Nested filter query with only a must clause

```
GET /test_index/doc_type/_search
{
  "query" : {
    "filtered" : {
      "query" : {
        "match_all" : { }
      },
      "filter" : {
        "bool" : {
          "should" : {
            "bool" : {
              "must" : {
                "nested" : {
                  "filter" : {
                    "bool" : {
                      "must" : {
                        "term" : {
                          "field1" : "b"
                        }
                      }
                    }
                  },
                  "path" : "nestedDoc"
                }
              },
              "_name" : "QueryName"
            }
          }
        }
      }
    }
  }
}
```

Returns result - no matched_queries item as expected

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test_index",
            "_type": "doc_type",
            "_id": "1",
            "_score": 1,
            "_source": {
               "nestedDoc": [
                  {
                     "field1": "a"
                  },
                  {
                     "field1": "b"
                  }
               ]
            }
         }
      ]
   }
}
```

Nested filter query with an additional must_not clause that does nothing

```
GET /test_index/doc_type/_search
{
  "query" : {
    "filtered" : {
      "query" : {
        "match_all" : { }
      },
      "filter" : {
        "bool" : {
          "should" : {
            "bool" : {
              "must" : {
                "nested" : {
                  "filter" : {
                    "bool" : {
                      "must" : {
                        "term" : {
                          "field1" : "b"
                        }
                      }
                    }
                  },
                  "path" : "nestedDoc"
                }
              },
              "must_not" : {
                "exists" : { "field" : "_does_not_exist_" }
              },
              "_name" : "QueryName"
            }
          }
        }
      }
    }
  }
}
```

Returns result with expected matched_queries item

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test_index",
            "_type": "doc_type",
            "_id": "1",
            "_score": 1,
            "_source": {
               "nestedDoc": [
                  {
                     "field1": "a"
                  },
                  {
                     "field1": "b"
                  }
               ]
            },
            "matched_queries": [
               "QueryName"
            ]
         }
      ]
   }
}
```
</description><key id="102886155">13088</key><summary>matched_queries is empty when filtered query with nested filter is missing must_not clause</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ehclark</reporter><labels><label>:Search</label><label>bug</label></labels><created>2015-08-24T20:37:51Z</created><updated>2015-08-25T12:22:56Z</updated><resolved>2015-08-25T12:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-25T12:04:16Z" id="134564503">@ehclark some bugs were fixed some time ago (#11880) with matched queries and inner docs and this particular issue is fixed in the next 2.0 beta release

One note, the field used in the `term` query is incorrect and should be `nestedDoc.field1` instead of `field1`.
</comment><comment author="clintongormley" created="2015-08-25T12:22:56Z" id="134568744">Closing as duplicate of #11880
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested query should only use bitset cache for parent filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13087</link><project id="" key="" /><description>at the moment the child query that gets passed down to `ToParentBlockJoinQuery` gets wrapped in a boolean query with a the nested child type as filter. The problem here is that a memory heavy bitset filters gets used. A normal filter should be used instead that potentially gets cached by the query cache should be used instead.
</description><key id="102885878">13087</key><summary>Nested query should only use bitset cache for parent filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>bug</label><label>v1.6.3</label><label>v1.7.2</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T20:36:22Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-25T08:13:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-25T07:31:01Z" id="134510357">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Detect duplicate settings keys on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13086</link><project id="" key="" /><description>This commit changes the startup behavior of Elasticsearch to throw an
exception if duplicate settings keys are detected in the Elasticsearch
configuration file.

Closes #13079
</description><key id="102862812">13086</key><summary>Detect duplicate settings keys on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T18:38:34Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-25T00:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-24T18:40:24Z" id="134333478">I didn't really look close enough here but I wonder what happen if I have `foo: bar` defined in the yml file and specify `-Des.foo=bar` on the cmd - will it fail or override it?
</comment><comment author="jasontedor" created="2015-08-24T18:45:29Z" id="134334670">@s1monw I can push a commit that will cause that to fail too.
</comment><comment author="s1monw" created="2015-08-24T19:07:26Z" id="134343099">@jasontedor should it fail? I mean I would expect the -D to override whatever is configured?
</comment><comment author="jasontedor" created="2015-08-24T19:51:41Z" id="134356088">Sorry for the miscommunication here; I misunderstood your first question as you implicitly wanting that change.

To be clear, with this pull request as it currently stands, the only change is that duplicates in the configuration file will cause an exception at startup. Settings can still be specified on the command line and will not cause an exception if settings with the same key are also in the configuration file. Settings on the command line still take precedence.
</comment><comment author="rjernst" created="2015-08-25T00:50:15Z" id="134428062">lgtm
</comment><comment author="jasontedor" created="2015-08-25T00:51:53Z" id="134428235">Thanks for the helpful review @rjernst.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix document parsing to properly ignore entire type when disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13085</link><project id="" key="" /><description>Currently when an entire type is disabled, our document parser will end
parsing on the first field of the document. This blows up the recently
added check that parsing did not silently skip any tokens (ie whether
there was garbage leftover).

This change fixes the parser to correctly skip the entire document when
the type is disabled.

closes #13017
</description><key id="102862256">13085</key><summary>Fix document parsing to properly ignore entire type when disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T18:36:03Z</created><updated>2015-09-14T17:15:39Z</updated><resolved>2015-08-25T15:58:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-25T07:35:31Z" id="134511031">LGTM
</comment><comment author="tlrx" created="2015-08-26T07:28:13Z" id="134875700">@rjernst Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create a new scripting language</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13084</link><project id="" key="" /><description>ElasticSearch needs a scripting language that can be used dynamically while remaining secure.  While Lucene Expressions covers those two points it does not meet the needs of many scripts due the following behavior:

1) Expressions is designed to only work effectively with numerical values. Elasticsearch requires a language that can handle strings, dates, and possibly data structures such as map and list. To add these features to expressions would require a large architectural change within Lucene that doesn't really make sense for the purpose of that language.
2) Expressions is designed to be a single mathematical equation using only one line of code. This does not lend itself well to using things such as a loop to go through multi-valued fields.
3) Expressions is built to be extremely similar to Javascript. This does not lend itself well to having types other than double for translation into Java efficiently. To translate a language like Javascript into Java would require all variables to be Objects that can also track what type they are. This is extremely inefficient.

One of the main goals of this language is any somewhat experienced developer should be able to learn the entirety in about fifteen minutes. For this reason I'm going to keep the control flow simple by allowing only the equivalent of one linearly-run static Java function to be written in total for any given script. No multiple function/method scripts should be allowed, as at this point the users should be writing custom code for their application instead of leaning on scripting.

For the new language I intend to initially have the following:

1) Native types - boolean, byte, short, int, long, float, double, string, date, point, list, and map including the ability to cast when necessary
2) Arithmetic operators - multiplication *, division /, addition +, subtraction -, precedence ( )
3) Comparison operators - less than &lt;, less than or equal to &lt;=, greater than &gt;, greater than or equal to &gt;=, equal to ==, and not equal to !=
4) Boolean operators - not !, and &amp;&amp;, or ||
5) Bitwise operators - shift left &lt;&lt;, shift right &gt;&gt;, unsigned shift &gt;&gt;&gt;, and &amp;, or |, xor ^, not ~
6) A way to call set list of external functions to be defined at a later time (math functions, geo functions)
7) API for strings (possibly a limited api for of regular expressions)
8) API for dates
9) Assignment operations for native types (int x; x = 0;)
10) Control flow - if, else if, else and for, while, do-while using brackets { } and semicolons ; to denote the end of operations/lines
11) Bindings - single-valued and multi-valued field access as available variables along with a way to find out the number of values in a multi-valued field, and a way to access the existing multivaluemode api
12) Shortcuts for map and list access such as (double)map0.item1.0.item2.1 where map0 is the initial map, item1 is an element in the map of type list, 0 is the first element in the list of type map, item2 is an element in the map of type list, and finally 1 is the second element in the list of type double.

This list will be updated as the project moves forward.  To ensure the language does not hang due to an infinite loop or extremely long operational set, the number of instructions will be counted, and an exception will be thrown if a specified limit is reached.

I intend to build the language using ANTLR and ASM as the backbone.  The following steps will be required for the language to be created.

1) Create the ANTLR grammar.
2) Write the code to build the ASM function from the provided script.
3) Write tests.
4) Integrate the language into the ElasticSearch code base.
5) Write more tests.
6) Refine the feature set.
7) Write more tests.
8) Repeat 6 and 7 until completed.
</description><key id="102837312">13084</key><summary>Create a new scripting language</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>feature</label><label>Meta</label></labels><created>2015-08-24T16:30:37Z</created><updated>2016-01-28T12:59:10Z</updated><resolved>2016-01-28T12:59:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T16:52:49Z" id="134297837">w00t
</comment><comment author="uboness" created="2015-08-24T16:54:31Z" id="134298267">w00t indeed
</comment><comment author="uboness" created="2015-08-24T16:58:22Z" id="134299164">&gt; 6) A way to call set list of external functions to be defined at a later time (math functions, geo functions)

this is super important aspect. beyond the basic native operations in the lang, the only other functions that will be available are those that we pre-register with the language (register in code that is). So this mechanism needs to be generic and not hard coded for the math/geo functions. 
</comment><comment author="jdconrad" created="2015-10-07T04:51:22Z" id="146076993">Haven't commented on here in a while, so I thought I would give a quick update.  The majority of the features are implemented as a first pass in a separate project.

What needs to happen before a PR can really be made at this point --

1) A bit better string support. (goal: this week)
2) Bindings for search fields. (goal: week after next -- needs item 6)
3) Tests, lots and lots of tests. (goal: next week)
4) Generally improved stability and bug fixes as the tests reveal them. (goal: next week)
5) Documentation. (goal: week after next)
6) Integration into a plugin. (goal: week after next)
7) General clean up. (goal: week after next)
8) Loop counting to prevent runaway code. (goal: week after next)
9) A name for the language.

This timeline may be a bit ambitious (again), but I'll update in another couple weeks.
</comment><comment author="clintongormley" created="2015-10-07T16:14:35Z" id="146248526">Awesome

&gt; 9) A name for the language.

Well that's going to push the delivery date out to 2019 :)
</comment><comment author="nik9000" created="2015-10-07T16:29:52Z" id="146253499">I wonder if the language can continue to live outside of Elasticsearch? The reason we made it is because there isn't a good, safe, sandboxed scripting language in the JVM. So I imagine it'll be useful to other people. If it were easy to embed in other places that'd be good exposure for us and help the open source community.
</comment><comment author="jdconrad" created="2015-10-07T16:36:33Z" id="146255823">@nik9000 That's a cool thought, and maybe something to consider down the road, but it's beyond the scope of the initial project by quite a bit.  I think the biggest limiter for doing something like that is the language really is designed to only allow scripts that are the equivalent of a single static Java method running on one thread, so for it to be effective outside ES it would certainly need to have it's feature set expanded significantly.
</comment><comment author="jdconrad" created="2015-10-13T18:36:10Z" id="147806349">I wanted to give an update about some of the features and the current state of the language:

The prior list remains except for string features; however, after speaking with @rmuir and @rjernst I would like to take a week to explore the possibility of using invoke dynamic to offer a language that doesn't require casting.  Currently the language has strict typing making it very similar to Java.  However, given the languages that people are used to this may be an issue, longer term.  It's likely I won't solve this in a week, but wanted to explore the possibility as something to do after the initial release with the need to make sure that it's at least possible with the current design.

As it stands the following features exist:

1) Native types - boolean, byte, short, int, long, float, double, object, string, list, and map including the ability to cast when necessary
2) Arithmetic operators - multiplication *, division /, addition +, subtraction -, precedence ( )
3) Comparison operators - less than &lt;, less than or equal to &lt;=, greater than &gt;, greater than or equal to &gt;=, equal to ==, and not equal to !=
4) Boolean operators - not !, and &amp;&amp;, or ||
5) Bitwise operators - shift left &lt;&lt;, shift right &gt;&gt;, unsigned shift &gt;&gt;&gt;, and &amp;, or |, xor ^, not ~
6) A language definition specified in a properties file that allows the ability to add more types (Java classes), and serves as a whitelist for all the things available to create and call.
7) Assignment operations for native types (int x; x = 0;) including increment, decrement, +=, -=, etc.
8) Control flow - if, else if, else and for, while, do-while using brackets { } and semicolons ; to denote the end of operations/lines
9) Shortcuts for maps and lists using brace notation such as (int x = (int)map["test0"][0]["test1"];) where test0 is an index into a map, 0 is an index into a list, test1 is an index into a map, where the value is casted from object to int.
10) A string concatenation operator in the form of '..' instead of '+' because '+' leads to ambiguities such as "string" + 2 + 2, where in java this ends up being string22 as a string, but I believe this may be confusing
11) Promotion for numerics is done in the form of the java style where things are upcast as necessary (int -&gt; double, or long -&gt; float, etc.) or require a cast if promotion cannot be done (long --&gt; int, etc.)
12) Auto-boxing -- since this is using basic types and is written using the JVM, auto boxing is necessary, and will be done when it can be automatically

A small example of the language definition:
class.object  = object   java.lang.Object // define java class Object as the type object
class.string  = string   java.lang.String   // define string class String as the type string
method.object.string = object string string toString() // define java class Object toString method as string for use on the object type
...

A small example of what a script will look like:

list nums = input["inner"]["list"];
int size = nums.size();
double total = 0;
for (int count = 0; count &lt; size; ++count) {
    total += (double)nums[count];
}
return total;

where the automatically generated signature for the script is Object execute(Map&lt;String, Object&gt; input);

Note that this can be thought of as a single static Java method when writing the script.  There is no way to script new functions/methods as if that's necessary, scripting may not be the best choice for the work that needs to be done in most cases.  It may also be possible to add the ability to execute other scripts from the original script to make up for the lack of method calls, but will likely not be included in the initial release.
</comment><comment author="jdconrad" created="2015-10-19T17:44:54Z" id="149294238">Quick update:

@rmuir has added ES plugin logic for the prototype language.  This week will be about adding tests and fixing bugs as they arise.  Not much else to add for now.
</comment><comment author="jdconrad" created="2015-10-21T07:25:41Z" id="149803643">I have removed all shortcuts for now to reduce the amount of debugging necessary for a first iteration.  Shortcuts add a huge amount of complication and ambiguity to the language at this point in time.  A second iteration somewhere down the road will have the goal of shortcuts, plus dynamic method calls, and inferred casting.  The main goal of this project at this time is a simple language that can improve security needs to be safe enough to run dynamic scripts in ES.
</comment><comment author="eskibars" created="2015-10-26T17:27:40Z" id="151216862">I see "+=", what about ".="?
</comment><comment author="jdconrad" created="2015-10-26T17:47:03Z" id="151223695">@eskibars To be clear is .= for string concatenation?  If so, we have decided to use ..= as the (.) operator may end up overloaded with an alternative shortcut for reading through maps/lists at a later time and will be needed for that.  We do not want to use += because that creates some possible ambiguities of it's own and is a math operator.  (While this works in Java, some of the assumptions that need to be made may not be for the best in all situations.)
</comment><comment author="eskibars" created="2015-10-26T20:50:58Z" id="151281119">@jdconrad yes, I was referring to string concatenation
</comment><comment author="damienalexandre" created="2015-11-05T15:35:01Z" id="154095367">Great stuffs! One issue I have with scripting in Elasticsearch is that is really hard to test and debug a script; We need, IMO:
- a way to play a script against any indexed document (maybe an API `index/type/123/_script`?)
- a debug mode, maybe integrated in `?explain` to show:
  - the count of instructions (specially if you limit them)
  - all the available variables
  - the return value, un-edited
- a way to log directly in Elasticsearch logs without playing with Java imports
- better exceptions: if a script fail, often, we get a QueryExec exception but the actual scripting error is hidden in the stack.

Maybe this new (un-named?) scripting language could fix or at least improve those points :) 

Cheers from Elastic{ON} Paris! :beers: 
</comment><comment author="jdconrad" created="2015-11-05T17:16:41Z" id="154126739">@damienalexandre Thanks for the feedback here.  Points 1, 2, and 4 are all solid ideas, and hopefully somewhere down the road we will have time to spec and code some incarnation of them.  For point 3, it's very unlikely that we will ever log anything from this language as we really don't want to write any files because it would mean that we have to open up security to allow this to happen.
</comment><comment author="clintongormley" created="2016-01-28T12:59:10Z" id="176171197">Closed by #15136
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up more bats tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13083</link><project id="" key="" /><description>This cleans up deb, rpm, systemd, and sysvinit tests:
1. Move skip_not_rpm, skip_not_dpkg, etc to the setup() methods for faster
runtime and cleaner code.
2. Removed lots of needless invocations of `run`
3. Created install_package for use in the systemd and sysvinit tests.
4. Removed lots of needless stderr to stdout redirects.

Closes #13075
Related to #13074
</description><key id="102837001">13083</key><summary>Clean up more bats tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T16:29:18Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-09-02T12:45:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-24T16:31:18Z" id="134290993">This won't actually pass right now - the bats tests are broken from the removal of `-u`. #13076 has the fix for that and should be reviewed and (hopefully) merged first. This should be rebased on master once that is in and _then_ it can be properly tested.
</comment><comment author="nik9000" created="2015-08-31T15:24:10Z" id="136405094">I've added the 2.0.0 label to this to make it clear that this is going to the 2.0 branch. I'll need to merge #13223 before I can merge this to the 2.0 branch but I'll get it before closing this pr.
</comment><comment author="tlrx" created="2015-08-31T16:18:46Z" id="136419275">@nik9000 Nice clean up, thanks! Left some comments.

I checked out #13223 and #13076, added this PR into the mix and executed the vagrant tests on wheezy, trusty &amp; centos7 with the command:

```
mvn -Dtests.vagrant -pl qa/vagrant verify -DboxesToTest='wheezy, trusty'
```

To make it works, I had to replace the dependency in `qa/vagrant/pom.xml` file:

``` xml
            &lt;dependencies&gt;
                &lt;dependency&gt;
                    &lt;groupId&gt;org.elasticsearch.distribution&lt;/groupId&gt;
                    &lt;artifactId&gt;elasticsearch-rpm&lt;/artifactId&gt;
                    &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
                    &lt;type&gt;rpm&lt;/type&gt;
                &lt;/dependency&gt;
            &lt;/dependencies&gt;
```

with : 

``` xml
            &lt;dependencies&gt;
                &lt;dependency&gt;
                    &lt;groupId&gt;org.elasticsearch.distribution.rpm&lt;/groupId&gt;
                    &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
                    &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
                    &lt;type&gt;rpm&lt;/type&gt;
                &lt;/dependency&gt;
            &lt;/dependencies&gt;
```

I'm not sure this is the right thing to do but at least I can run the tests. 

Tests succeed on wheezy and trusty but there are some failures in CentOS-7 with status checks you added.
</comment><comment author="nik9000" created="2015-08-31T21:21:25Z" id="136505543">Thanks for the review @tlrx! I'll have a look now.
</comment><comment author="nik9000" created="2015-08-31T21:24:28Z" id="136506104">&gt; To make it works, I had to replace the dependency in qa/vagrant/pom.xml file:

I'm not sure where the original came from - I don't believe this has any changes to the pom.
</comment><comment author="nik9000" created="2015-09-01T18:53:58Z" id="136826923">&gt; I'm not sure where the original came from - I don't believe this has any changes to the pom.

Ok - I figured out the changes to the pom issue and fixed in #13076.

I'll be squashing and rebasing this on master to get #13076. Right now the only open topic on the review are some centos-7 errors that I'll work on after the rebase.
</comment><comment author="nik9000" created="2015-09-01T19:24:43Z" id="136833567">And fixed Centos-7!
</comment><comment author="tlrx" created="2015-09-01T19:28:27Z" id="136834345">Awesome
</comment><comment author="nik9000" created="2015-09-01T23:20:33Z" id="136889840">&gt; Awesome

Great! If it looks good to you I'll squash, rebase, and merge.
</comment><comment author="tlrx" created="2015-09-02T08:33:17Z" id="136975958">LGTM
</comment><comment author="nik9000" created="2015-09-02T12:59:28Z" id="137065361">Merged to 2.0 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Consolidate duplicate logic in RoutingTable all*ShardsGrouped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13082</link><project id="" key="" /><description>This commit consolidates the logic in
RoutingTable#allActiveShardsGrouped and
RoutingTable#allAssignedShardsGrouped into a single method that merely
applies a predicate to each ShardRouting.

Closes #13081
</description><key id="102836647">13082</key><summary>Consolidate duplicate logic in RoutingTable all*ShardsGrouped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-24T16:27:54Z</created><updated>2015-08-25T09:18:08Z</updated><resolved>2015-08-25T09:06:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-25T09:01:34Z" id="134532291">LGTM
</comment><comment author="cbuescher" created="2015-08-25T09:18:08Z" id="134536645">Great, I think this also bothered me a while ago and I opended #10195 for it, will close that one then. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Consolidate duplicate logic in RoutingTable all*ShardsGrouped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13081</link><project id="" key="" /><description>The methods [`RoutingTable#allActiveShardsGrouped`](https://github.com/elastic/elasticsearch/blob/119e9ba138e6f2a8c09e4e9d12ef67eee57b7986/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java#L164-L187) and [`RoutingTable#allAssignedShardsGrouped`](https://github.com/elastic/elasticsearch/blob/119e9ba138e6f2a8c09e4e9d12ef67eee57b7986/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java#L200-L223) contain duplicate logic differing only in the predicate that they apply to a `ShardIterator`. This logic can be consolidated into a single method.
</description><key id="102835879">13081</key><summary>Consolidate duplicate logic in RoutingTable all*ShardsGrouped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-24T16:25:38Z</created><updated>2015-08-25T09:06:26Z</updated><resolved>2015-08-25T09:06:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Document noop behavior of index api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13080</link><project id="" key="" /><description>Relates to #12969
</description><key id="102833367">13080</key><summary>Document noop behavior of index api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T16:20:27Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-25T14:35:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-24T16:20:46Z" id="134282593">I tried to capture @jpountz's reasoning in #12969 in the docs.
</comment><comment author="jpountz" created="2015-08-25T07:40:38Z" id="134511867">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Same setting with different values in elasticsearch.yml should be an error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13079</link><project id="" key="" /><description>I hit this while smoke testing the 2.0.0 beta RC.

Today, if you have the same setting in `config/elasticsearch.yml` more than once, I think the last one silently "wins".

But I think this is too lenient/trappy?  E.g. someone may open the file, insert what they think is a new setting, but then it doesn't "take" because that setting already appears later in the file.

I think we should make this a hard error on startup instead.
</description><key id="102828599">13079</key><summary>Same setting with different values in elasticsearch.yml should be an error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T16:06:42Z</created><updated>2015-09-14T17:17:16Z</updated><resolved>2015-08-25T00:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-24T16:21:41Z" id="134283967">+1
</comment><comment author="rjernst" created="2015-08-24T16:24:04Z" id="134286486">+1

Not only for different values, but the same setting twice should be an error too. We should fail early, before the user tries to change the setting (in just one place) and then hits the proposed error here.
</comment><comment author="mikemccand" created="2015-08-24T16:29:58Z" id="134290659">&gt; Not only for different values, but the same setting twice should be an error too. We should fail early, 

++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add RC smoke tester that checks basic functionality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13078</link><project id="" key="" /><description>This RC smoke tester runs:
1. Downloads the tar.gz &amp; zip file from the staging URL
2. Verifies it's sha1 hashes and GPG signatures against the release key
3. Installs all official plugins
4. Starts one node an checks:
   -- if it runs with Java 1.7
   -- if the build hash given is the one that is returned by the status response
   -- if the build is a release version and not a snapshot version
   -- if all plugins are loaded
   -- if the status reponse returns the correct version
</description><key id="102826697">13078</key><summary>Add RC smoke tester that checks basic functionality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T15:58:38Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-25T08:52:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-24T15:59:23Z" id="134262636">@spinscale I basically put all the things that I do anyway into a script - we can improve it as we go I guess
</comment><comment author="rjernst" created="2015-08-24T23:25:43Z" id="134412020">I left a couple comments, I like this a lot!!
</comment><comment author="rjernst" created="2015-08-24T23:45:18Z" id="134416043">BTW, none of the comments I left are critical. This LGTM regardless of the responses.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more permissions for bouncycastle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13077</link><project id="" key="" /><description>As explained in [bouncy castle doc](http://www.bouncycastle.org/wiki/display/JA1/Using+the+Bouncy+Castle+Provider's+ImplicitlyCA+Facility), we need to have:

```
grant {
   permission java.security.SecurityPermission "putProviderProperty.BC";
   permission java.security.SecurityPermission "insertProvider.BC";
};
```

Mapper attachment plugin needs that to run properly.

Note that it looks like master branch does not require that change which looks strange to me. See https://github.com/elastic/elasticsearch/pull/13001#issuecomment-134245012.

Closes #13001
</description><key id="102820474">13077</key><summary>Add more permissions for bouncycastle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2015-08-24T15:29:40Z</created><updated>2015-08-25T11:59:38Z</updated><resolved>2015-08-25T11:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-25T11:59:14Z" id="134563464">Closing. Same comment as https://github.com/elastic/elasticsearch/pull/13001#issuecomment-134563278
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Install all plugins during bats tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13076</link><project id="" key="" /><description>Related to #12717
</description><key id="102817368">13076</key><summary>Install all plugins during bats tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T15:17:45Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-09-01T17:44:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-24T15:19:06Z" id="134245748">This only installs and removes that plugins using the plugin script - it doesn't try to run elasticsearch with them all installed. I'm not sure vagrant the vagrant tests are the appropriate place to do that since they are ostensibly repeated once per target OS.
</comment><comment author="nik9000" created="2015-08-24T15:19:28Z" id="134245833">Ping @tlrx  for more fun bats code to review. This one has a symlink!
</comment><comment author="nik9000" created="2015-08-25T15:10:36Z" id="134617864">@brwe , this is one of the in flight bats changes.
</comment><comment author="nik9000" created="2015-08-27T15:25:35Z" id="135468596">@spinscale, want to review some packaging tests?
</comment><comment author="drewr" created="2015-08-31T13:47:57Z" id="136376641">Needed to use `VAGRANT_DEFAULT_PROVIDER=virtualbox` to get this to work for me. I suggest we explicitly set `--provider=virtualbox` wherever we call `vagrant up`.
</comment><comment author="nik9000" created="2015-08-31T13:48:33Z" id="136376735">&gt; Needed to use VAGRANT_DEFAULT_PROVIDER=virtualbox to get this to work for me. I suggest we explicitly set --provider=virtualbox wherever we call vagrant up.

I thought that was the default. I can certainly do that though.
</comment><comment author="dakrone" created="2015-08-31T13:48:36Z" id="136376742">&gt; Needed to use VAGRANT_DEFAULT_PROVIDER=virtualbox to get this to work for me.

Same here, otherwise vagrant tries to use libvirt or something.
</comment><comment author="nik9000" created="2015-08-31T13:50:53Z" id="136377166">&gt; Same here, otherwise vagrant tries to use libvirt or something.

wat? The default for vagrant for forever was virtualbox. Anyway, #13217.
</comment><comment author="drewr" created="2015-08-31T13:52:10Z" id="136377406">@nik9000 It should be the default, but somehow because I use lxc in other contexts it seems to be sticking around somewhere. I even ran with `env - mvn ....` and it found it. Maybe just Vagrant :hankey:.
</comment><comment author="drewr" created="2015-08-31T14:23:48Z" id="136385977">`env VAGRANT_DEFAULT_PROVIDER=virtualbox mvn -Dtests.vagrant clean verify` built successfully :bowtie: 
</comment><comment author="nik9000" created="2015-08-31T15:24:06Z" id="136405074">I've added the 2.0.0 label to this to make it clear that this is going to the 2.0 branch. I'll need to merge #13223 before I can merge this to the 2.0 branch but I'll get it before closing this pr.
</comment><comment author="nik9000" created="2015-09-01T13:25:53Z" id="136721391">@tlrx, would you mind reviewing this on its own? I think it might be ok and its worth getting in if we can.
</comment><comment author="nik9000" created="2015-09-01T14:20:25Z" id="136737294">&gt; @tlrx, would you mind reviewing this on its own? I think it might be ok and its worth getting in if we can.

Thanks! I'll read these and fix.
</comment><comment author="tlrx" created="2015-09-01T14:55:43Z" id="136750977">I like these changes, great work @nik9000 ! I read carefully the bats files and I made some minor comments. 

Tests succeed on my laptop for precise, trusty and centos-7 (still have to remove the elasticsearch-rpm dependency to get them work). Please let me know if you want me to test a specific VM.

The last and most important thing I see is that for now the test installs a plugin, check for files and remove the plugin. That would be nice to start elasticsearch with the installed plugin and checks that it started correctly and that the plugin is referenced in Nodes Info API... What do you think?
</comment><comment author="nik9000" created="2015-09-01T15:03:24Z" id="136753478">&gt; Tests succeed on my laptop for precise, trusty and centos-7 (still have to remove the elasticsearch-rpm dependency to get them work). Please let me know if you want me to test a specific VM.

Those vms are fine.

&gt; The last and most important thing I see is that for now the test installs a plugin, check for files and remove the plugin. That would be nice to start elasticsearch with the installed plugin and checks that it started correctly and that the plugin is referenced in Nodes Info API... What do you think?

Yeah - we probably should do it. I think that should be in another pr though.
</comment><comment author="tlrx" created="2015-09-01T15:09:15Z" id="136754978">&gt; Yeah - we probably should do it. I think that should be in another pr though.

Sounds good. Can you create an issue please?
</comment><comment author="tlrx" created="2015-09-01T15:09:32Z" id="136755061">LGTM then
</comment><comment author="nik9000" created="2015-09-01T17:41:57Z" id="136808323">&gt; Yeah - we probably should do it. I think that should be in another pr though.

#13254

&gt; LGTM then

In that case I'll squash, rebase, and merge.
</comment><comment author="nik9000" created="2015-09-01T18:14:47Z" id="136816174">Merged to master. Tried to merge it to 2.0 and found some unbackported stuff:
391ea379e2799037944741b8c626702434e931c0
0b650ed203939364bd9133041b6939deaa72b9d7

Trying to backport those before merging this into 2.0.
</comment><comment author="nik9000" created="2015-09-02T12:39:59Z" id="137061225">&gt; Trying to backport those before merging this into 2.0.

Did that in #13255.
</comment><comment author="nik9000" created="2015-09-02T12:45:07Z" id="137062397">And merged to 2.0! Yay! All done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up skip_not_* in bats tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13075</link><project id="" key="" /><description>Right now we liberally sprinkle things like `skip_not_rpm` and `skip_not_sysvinit` in our tests. This isn't a problem but we could save a lot of copy and paste if we move these skips to the `setup` functions for the tests.
</description><key id="102800350">13075</key><summary>Clean up skip_not_* in bats tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-24T13:59:40Z</created><updated>2015-09-02T12:45:15Z</updated><resolved>2015-09-02T12:45:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Clean the use of `run` in bats tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13074</link><project id="" key="" /><description>In the bats tests we do a lot of

``` bash
run rm -rf /tmp/foo/bar
[ "$status" -eq 0 ]
```

but that is kind of an abuse of the Bats' `run` feature and we should clean these up.

The idea behind `run` is that it'll store `$?` in `$status` and stdout in `$output`. Its super useful if you want to assert something about what a command logs to stdout or you expect some non-zero return status. But if you don't care about stdout beyond logging the command and you want to fail if the command returns a non-zero status you should really just run the command like a normal bash script would:

``` bash
rm -rf /tmp/foo/bar
```

That way the stdout is logged and the bats script fails if the command exits non-zero.
</description><key id="102799631">13074</key><summary>Clean the use of `run` in bats tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-24T13:56:38Z</created><updated>2015-09-04T20:56:33Z</updated><resolved>2015-09-04T20:56:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-25T15:10:55Z" id="134617949">@brwe, this is the other in flight bats change.
</comment><comment author="nik9000" created="2015-09-04T20:56:33Z" id="137850597">This is mostly cleaned by now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unsupported `rewrite` from multi_match query builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13073</link><project id="" key="" /><description>The `rewrite` option has been removed from the parser with ommit da5fa6c4 and won't parse anymore, however we still have a setter for it in the builder that gets rendered out when used and potentially leads to parsing errors. This PR removes the setter for the unsupported `rewrite` option.

Follow up PR to #13069 
</description><key id="102797669">13073</key><summary>Remove unsupported `rewrite` from multi_match query builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-24T13:50:40Z</created><updated>2015-08-24T15:40:24Z</updated><resolved>2015-08-24T14:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-24T13:52:05Z" id="134209706">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update the plugin install CLI help to the new short name for commercial plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13072</link><project id="" key="" /><description /><key id="102796308">13072</key><summary>Update the plugin install CLI help to the new short name for commercial plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>docs</label><label>review</label></labels><created>2015-08-24T13:44:55Z</created><updated>2015-08-24T16:16:00Z</updated><resolved>2015-08-24T16:16:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T15:01:12Z" id="134235164">@cbuescher I thought the help could do with more of a cleanup - it's very long.  How about this instead (removing the EXAMPLES) section:

```
DESCRIPTION

    This command installs an elasticsearch plugin. It can be used as follows:

    Officially supported or commercial plugins require just the plugin name:

        plugin install analysis-icu
        plugin install shield

    Plugins from GitHub require 'username/repository' or 'username/repository/version':

        plugin install lmenezes/elasticsearch-kopf
        plugin install lmenezes/elasticsearch-kopf/1.5.7

    Plugins from Maven Central or Sonatype require 'groupId/artifactId/version':

        plugin install org.elasticsearch/elasticsearch-mapper-attachments/2.6.0

    Plugins can be installed from a custom URL or file location as follows:

        plugin install http://some.domain.name//my-plugin-1.0.0.zip
        plugin install file:/path/to/my-plugin-1.0.0.zip
```
</comment><comment author="cbuescher" created="2015-08-24T15:21:44Z" id="134246372">@clintongormley Great, thanks for the input, will update this PR.
</comment><comment author="cbuescher" created="2015-08-24T16:12:07Z" id="134272422">@clintongormley updated the PR, if there's nothing else to add I'll merge this and backport it to the 2.0 branch.
</comment><comment author="clintongormley" created="2015-08-24T16:13:58Z" id="134273862">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't use complex params with lang=expression scripts?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13071</link><project id="" key="" /><description>I'm trying to port the following simple inline function_score / script_score script from groovy to expression to avoid the need for dynamic scripting with potential security holes:

```
{ "script": "indirect_score.get(doc['somefield'].value, 0.0)",
  "lang": "groovy",
  "params": {
    "indirect_score": {
      "hello": 1,
      "world": 2
    }
  }
}
```

However when ported to expression:

```
{ "script": "indirect_score[doc['somefield'].value] || 0.0",
  "lang": "expression",
  "params": {
    "indirect_score": {
      "hello": 1,
      "world": 2
    }
  }
}
```

I get a surprising

```
ExpressionScriptCompilationException[Unknown variable [indirect_score] in expression];
```

error whever I try to access the map, even with hardcoded `indirect_score['hello']` script. Primitive params seem to recognized just fine however, but are not flexible enough for the indirect lookup we need.

Does this mean dicts/maps or other complex variables can not be used as expression params, or what am I doing wrong?
</description><key id="102795852">13071</key><summary>Can't use complex params with lang=expression scripts?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tko</reporter><labels /><created>2015-08-24T13:42:40Z</created><updated>2015-08-24T20:06:58Z</updated><resolved>2015-08-24T14:45:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T14:45:58Z" id="134230627">Hi @tko 

This is a limitation of the expressions language, which is very fast, but very limited.  You can save your groovy scripts as script files instead.
</comment><comment author="tko" created="2015-08-24T15:29:43Z" id="134251303">Yeah, I was trying to avoid having to change to our deployment process as well. Oh well.

FWIW without knowing anything about the internals it's rather surprising that dictionary-like access sometimes works (`doc['field']`) and other times doesnt (`someparam['field']`) -- it seems like all the pieces should be there, though I can easily imagine many reasons they haven't been wired up equally.
</comment><comment author="rjernst" created="2015-08-24T19:17:52Z" id="134345596">What you are trying to do is a hash lookup, which is slow when doing per document.  In this case, I assume your "somefield" field is a string? `doc` is setup as a special variable which binds to field accessors at compile time, so that looking up for each doc is like saying "lookup this field number in docvalues/fielddata". But your example is trying to get the value of a string field (already something slow not supported in expressions) and use that to do a lookup in a map.

It might be possible for us to tweak the inputs and language to allow array params of numbers and indirect lookups based on number field values, but there is already ongoing work to expand the capabilities of scripts with #13084.
</comment><comment author="tko" created="2015-08-24T20:06:58Z" id="134361253">Strings, yes, though could just as easily be numbers.

I just meant that syntactically the `doc` access looks _very much_ like hash lookup which (to someone with zero knowledge of expressions limitations) rather implies that hash lookup is supported in general. I may very well have missed the relevant documentation and made too many assumptions, but now that I know it simply won't work I'm good and can stop banging my head on the keyboard.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix some issues developing Elasticsearch with Eclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13070</link><project id="" key="" /><description>1. Fix importing Elasticsearch into eclipse with `mvn eclipse:clean &amp;&amp; mvn eclipse:eclipse`. It was creating projects with the same name which Eclipse was less than happy with.
2. Fix the launch script. It was failing on the security manager among other things. This disables the security manager when launching Elasticsearch using the pre-packaged launch configuration. We can re-enable it later. I just wanted to get it working again because it can really speed development in some areas.
</description><key id="102789041">13070</key><summary>Fix some issues developing Elasticsearch with Eclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T13:07:19Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-24T19:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-24T13:10:12Z" id="134196600">LGTM
</comment><comment author="dadoonet" created="2015-08-24T13:10:35Z" id="134196673">Don't you think you should backport it in 2.0 BTW?
</comment><comment author="nik9000" created="2015-08-24T13:12:03Z" id="134197077">&gt; Don't you think you should backport it in 2.0 BTW?

That is probably a good idea, yeah. The issue comes up in 2.0, certainly.
</comment><comment author="nik9000" created="2015-08-24T19:40:11Z" id="134351211">Merged into 2.0 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unsupported `rewrite` option from match query builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13069</link><project id="" key="" /><description>The `rewrite` option has been removed from the parser with commit da5fa6c4 and won't parse anymore, however we still have a setter for it in the builder that gets rendered out when used and potentially leads to parsing errors. This PR removes the setter for the unsupported `rewrite` option.
</description><key id="102769007">13069</key><summary>Remove unsupported `rewrite` option from match query builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-24T11:07:52Z</created><updated>2015-08-24T14:35:58Z</updated><resolved>2015-08-24T12:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-24T11:25:24Z" id="134158100">good catch, LGTM I wonder if we should do the same with multi_match too.
</comment><comment author="jpountz" created="2015-08-24T11:51:44Z" id="134164738">LGTM
</comment><comment author="cbuescher" created="2015-08-24T12:05:36Z" id="134166949">Thanks, will look into multi_match as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make refresh a replicated action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13068</link><project id="" key="" /><description>prerequisite to #9421
see also #12600
</description><key id="102750097">13068</key><summary>Make refresh a replicated action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-24T09:41:01Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-09-01T07:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-25T09:19:32Z" id="134536848">I love this change looks very good. I left some comments around unittesting 
</comment><comment author="bleskes" created="2015-08-28T14:40:36Z" id="135792711">I agree with @s1monw that this looks great. I left minor comments here and there.
</comment><comment author="s1monw" created="2015-08-31T12:30:47Z" id="136359275">LGTM 
</comment><comment author="brwe" created="2015-08-31T13:34:20Z" id="136373024">@s1monw thanks for the review! 

addressed all comments. @bleskes want to have one more look?
</comment><comment author="bleskes" created="2015-08-31T14:15:22Z" id="136383163">Looks awesome. I only miss a test for the aggregation of results from multiple shard level responses.
</comment><comment author="brwe" created="2015-08-31T15:37:06Z" id="136408965">@bleskes addressed all comments and added test here: https://github.com/elastic/elasticsearch/pull/13068/files#diff-6030559b5ed4d55d9a754523f5c6ce6dR137 
</comment><comment author="bleskes" created="2015-08-31T15:58:39Z" id="136414713">LGTM! (minor comments, no need for another review)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error NoMethodError: undefined method scoped'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13067</link><project id="" key="" /><description>Dear elastic community,

I tried to install elasticsearch with the plugin for Redmine some time ago. I retried yesterday following the instruction from https://github.com/Undev/redmine_elasticsearch1
Point 6. of the instruction is not working at my machinge. I get the following message:

C:\Bitnami\redmine-3.0.2-0\apps\redmine\htdocs&gt;bundle exec rake redmine_elastics
earch:reindex_all RAILS_ENV=production --trace
Your Gemfile lists the gem kaminari (&gt;= 0) more than once.
You should probably keep only one of them.
While it's not a problem now, it could cause errors if you change the version of
just one of them later.
DL is deprecated, please use Fiddle
ansi: 'gem install win32console' to use color on Windows
Loading Rails environment for Resque
*\* Invoke redmine_elasticsearch:reindex_all (first_time)
*\* Invoke redmine_elasticsearch:logged (first_time)
*\* Invoke environment (first_time)
*\* Execute environment
*\* Execute redmine_elasticsearch:logged
*\* Execute redmine_elasticsearch:reindex_all
Recreate index for all available search types
rake aborted!
NoMethodError: undefined method scoped' for # C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/activerecord-4.2.1/lib/ active_record/dynamic_matchers.rb:26:inmethod_missing'
C:/Bitnami/redmine-3.0.2-0/apps/redmine/htdocs/plugins/redmine_elasticsearch/app
/models/parent_project.rb:57:in searching_scope' C:/Bitnami/redmine-3.0.2-0/apps/redmine/htdocs/plugins/redmine_elasticsearch/lib /redmine_elasticsearch/indexer_service.rb:45:infor_each_parent_project'
C:/Bitnami/redmine-3.0.2-0/apps/redmine/htdocs/plugins/redmine_elasticsearch/lib
/redmine_elasticsearch/indexer_service.rb:18:in reindex_all' C:/Bitnami/redmine-3.0.2-0/apps/redmine/htdocs/plugins/redmine_elasticsearch/lib /tasks/index.rake:18:inblock (2 levels) in '
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ta
sk.rb:240:in call' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ta sk.rb:240:inblock in execute'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ta
sk.rb:235:in each' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ta sk.rb:235:inexecute'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ta
sk.rb:179:in block in invoke_with_call_chain' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/2.0.0/monitor.rb:211:inmon_synchroniz
e'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ta
sk.rb:172:in invoke_with_call_chain' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ta sk.rb:165:ininvoke'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap
plication.rb:150:in invoke_task' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap plication.rb:106:inblock (2 levels) in top_level'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap
plication.rb:106:in each' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap plication.rb:106:inblock in top_level'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap
plication.rb:115:in run_with_threads' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap plication.rb:100:intop_level'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap
plication.rb:78:in block in run' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap plication.rb:176:instandard_exception_handling'
C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/lib/rake/ap
plication.rb:75:in run' C:/Bitnami/redmine-3.0.2-0/ruby/lib/ruby/gems/2.0.0/gems/rake-10.4.2/bin/rake:33 :in'
C:/Bitnami/redmine-3.0.2-0/ruby/bin/rake:23:in load' C:/Bitnami/redmine-3.0.2-0/ruby/bin/rake:23:in

'
Tasks: TOP =&gt; redmine_elasticsearch:reindex_all
Please any help!?!?

Or is everything working because the plugin is already in the plugin list of Redmine? How can I know if it is working or not?

Thank you for your time and help!!
</description><key id="102749702">13067</key><summary>Error NoMethodError: undefined method scoped'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexander-st</reporter><labels /><created>2015-08-24T09:39:05Z</created><updated>2015-08-25T06:10:12Z</updated><resolved>2015-08-24T14:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-24T14:04:41Z" id="134216398">This error is not related to elasticsearch. You should ask this question to the elasticsearch-redmine project.
</comment><comment author="alexander-st" created="2015-08-25T06:10:12Z" id="134491601">Thank you for your response!
I will try to get an answer from the plugin developers.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Prevent running whole lifecycle twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13066</link><project id="" key="" /><description /><key id="102736147">13066</key><summary>Release: Prevent running whole lifecycle twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-24T08:27:24Z</created><updated>2015-08-24T08:30:43Z</updated><resolved>2015-08-24T08:30:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Explicitly mention applied dynamic mapping changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13065</link><project id="" key="" /><description>When mapping is updated dynamically we get something like this;

```
[2015-08-24 11:50:03,383][INFO ][cluster.metadata         ] [mark] [index-2015.01.15] update_mapping [data] (dynamic)
```

It'd be super awesome if we provided a bit more information for the user to be aware of what is updated/had changed in their field structure. I am thinking something like this;

```
[2015-08-24 11:50:03,383][INFO ][cluster.metadata         ] [mark] [index-2015.01.15] update_mapping [data] (dynamic - field9 added =&gt; string, field2 updated =&gt; int)
```
</description><key id="102685165">13065</key><summary>Explicitly mention applied dynamic mapping changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Mapping</label><label>discuss</label><label>enhancement</label></labels><created>2015-08-24T01:57:42Z</created><updated>2016-01-28T12:57:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T09:44:58Z" id="135720693">There are people who add thousands of fields in a single document, which could make the logging a bit overwhelming.  Also, we don't currently have the infrastructure to do this - not a low hanging fruit.  We'd probably need to log all mapping changes.  This would need to be TRACE level as it could be way too much.

What is the problem that would be solved by adding this logging?  The log message doesn't identify a rogue document, and in fact it'd be easier to find rogue docs via the API (get mapping and field-exists query) than from the logs?
</comment><comment author="markwalkom" created="2015-08-28T09:54:16Z" id="135723447">It's just an informational thing, keeping users aware of what's changed rather than having to figure out some other way of tracking this (eg VCS).
</comment><comment author="bleskes" created="2015-08-28T10:30:05Z" id="135729655">I think this is a good request and we have had ways to deal with verbosity in the past (log the first X fields in debug, all in trace). The tricky part is the implementation as we don&#8217;t have the right information available at the right place at the moment (the mapping merge produces a new mapping without telling you what changed). 

&gt; On 28 Aug 2015, at 11:54, markwalkom notifications@github.com wrote:
&gt; 
&gt; It's just an informational thing, keeping users aware of what's changed rather than having to figure out some other way of tracking this (eg VCS).
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi level nested objects `inner_hits` not working (only root level working)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13064</link><project id="" key="" /><description>Hi,

Here is the mapping:  

```
{
    "mappings": {
        "people": {
            "properties": {
                "cars": {
                    "type": "nested",
                    "properties": {
                        "manufacturers": {
                            "type": "nested",
                            "properties": {
                                "country": {
                                    "type": "string"
                                },
                                "name": {
                                    "type": "string"
                                }
                            }
                        },
                        "model": {
                            "type": "string"
                        },
                        "make": {
                            "type": "string"
                        }
                    }
                },
                "last_name": {
                    "type": "string"
                },
                "first_name": {
                    "type": "string"
                }
            }
        }
    }
}
```

The test document is: 

```
{
    "first_name": "Zach",
    "last_name": "Foobar",
    "cars": [{
        "make": "Saturn",
        "model": "SL",
        "manufacturers": [{
            "name": "Saturn",
            "country": "USA"
        }, {
            "name": "Honda",
            "country": "Canada"
        }]
    }, {
        "make": "Subaru",
        "model": "Imprezza",
        "manufacturers": [{
            "name": "Subaru",
            "country": "Japan"
        }, {
            "name": "Daimler",
            "country": "Germany"
        }]
    }]
}
```

and the query is:

```
{
    "query": {
        "nested": {
            "path": "cars",
            "query": {
                "nested" : {
                    "path" :  "cars.manufacturers",
                    "query" :  {
                       "match": {
                           "cars.manufacturers.country": "Japan"
                        }
                    },
                    "inner_hits": {}
                }
            },
            "inner_hits": {}
        }
    }
}
```

ElasticSearch gives the search result:

```
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 2.252763,
    "hits": [
      {
        "_index": "abcindex3",
        "_type": "people",
        "_id": "Zach",
        "_score": 2.252763,
        "_source": {
          "first_name": "Zach",
          "last_name": "Foobar",
          "cars": [
            {
              "make": "Saturn",
              "model": "SL",
              "manufacturers": [
                {
                  "name": "Saturn",
                  "country": "USA"
                },
                {
                  "name": "Honda",
                  "country": "Canada"
                }
              ]
            },
            {
              "make": "Subaru",
              "model": "Imprezza",
              "manufacturers": [
                {
                  "name": "Subaru",
                  "country": "Japan"
                },
                {
                  "name": "Daimler",
                  "country": "Germany"
                }
              ]
            }
          ]
        },
        "inner_hits": {
          "cars": {
            "hits": {
              "total": 1,
              "max_score": 2.252763,
              "hits": [
                {
                  "_index": "metaportal3",
                  "_type": "people",
                  "_id": "Zach",
                  "_nested": {
                    "field": "cars",
                    "offset": 1
                  },
                  "_score": 2.252763,
                  "_source": {
                    "make": "Subaru",
                    "model": "Imprezza",
                    "manufacturers": [
                      {
                        "name": "Subaru",
                        "country": "Japan"
                      },
                      {
                        "name": "Daimler",
                        "country": "Germany"
                      }
                    ]
                  }
                }
              ]
            }
          },
          "cars.manufacturers": {
            "hits": {
              "total": 0,
              "max_score": null,
              "hits": []
            }
          }
        }
      }
    ]
  }
}
```

The root level `inner_hits` is working, but the multi-level nested objects `inner_hits` are not working. The `cars.manufacturers` inner hits is empty. Am I doing something wrong or is this a bug? From [this doc](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html) it says that multi level `inner_hits` are supported and should return non-root level inner_hits.
</description><key id="102683749">13064</key><summary>Multi level nested objects `inner_hits` not working (only root level working)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">RanadeepPolavarapu</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2015-08-24T01:44:31Z</created><updated>2017-04-27T06:48:31Z</updated><resolved>2015-08-25T10:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-24T20:50:36Z" id="134376166">@RanadeepPolavarapu the `inner_hits` syntax in the query dsl doesn't support proper nested. For now you should use the top level `inner_hits` syntax: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html#top-level-inner-hits

This issue is related to this ticket: #11118
Although this ticket is about parent/child inner hits, the the nested inner_hits in the query dsl doesn't work there too and this should be fixed
</comment><comment author="clintongormley" created="2015-08-25T10:00:00Z" id="134543253">Closing as duplicate of #11118
</comment><comment author="RanadeepPolavarapu" created="2015-08-27T13:48:30Z" id="135437348">@martijnvg How could I replicate that query correctly using top level `inner_hits`? 
</comment><comment author="martijnvg" created="2015-08-27T19:04:22Z" id="135524830">@RanadeepPolavarapu the following query should work:

```
curl -XGET "http://localhost:9200/_search" -d'
{
  "query": {
    "nested": {
      "path": "cars",
      "query": {
        "nested": {
          "path": "cars.manufacturers",
          "query": {
            "match": {
              "cars.manufacturers.country": "Japan"
            }
          }
        }
      }
    }
  },
  "inner_hits": {
    "cars": {
      "path": {
        "cars": {
          "query": {
            "nested": {
              "path": "cars.manufacturers",
              "query": {
                "match": {
                  "cars.manufacturers.country": "Japan"
                }
              }
            }
          },
          "inner_hits": {
            "manufacturers": {
              "path": {
                "cars.manufacturers": {
                  "query": {
                    "match": {
                      "cars.manufacturers.country": "Japan"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}'
```

Downside that It is more verbose, because the query needs to be repeated on each nested level. When the embedded query dsl `inner_hits` syntax also properly supports multiple level of nested object fields and parent child relations then your query should work too.
</comment><comment author="abulhol" created="2016-02-01T13:28:06Z" id="177971934">+1 
I am wondering why the reference includes this paragraph:
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html#hierarchical-nested-inner-hits
which seems exactly the part that is not working
</comment><comment author="black-square" created="2016-10-03T14:57:43Z" id="251129440">This bug is closed as well as #11118, but the original example from @RanadeepPolavarapu still doesn't work on 2.4. Should it be reopen again?
</comment><comment author="martijnvg" created="2016-10-03T21:02:36Z" id="251226171">@black-square The bug was only fixed in 5.0.x. Fixing it required a refactoring that didn't make its way back in 2.4
</comment><comment author="abhishek5678" created="2017-04-27T06:16:38Z" id="297621949">Hi Guys,
In this data when i am trying with one level nested data then it's working fine but when i am adding one more nested data then it's not working  if anybody will be knows about that  then please reply me.I am sending my document data and mapping file also.

POST /test_word14/doc/1
{
    "name": "Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism (D50-D89)",
    "depth": 1,
	"children": [{
		"name": "Nutritional anemias (D50-D53)",
		"depth": 2,
		"children": [{
			"code": "D50",
			"name": "Iron deficiency anemia",
			"depth": 3,
			"children": [{
				"code": "D50.0",
				"name": "Iron deficiency anemia secondary to blood loss (chronic)",
				"depth": 4
			}, {
				"code": "D50.1",
				"name": "Sideropenic dysphagia",
				"depth": 4
			}, {
				"code": "D50.8",
				"name": "Other iron deficiency anemias",
				"depth": 4
			}, {
				"code": "D50.9",
				"name": "Iron deficiency anemia, unspecified",
				"depth": 4
			}]
		}]
	}]
}

(mapping file)
PUT /test_word16
{
  "mappings": {
    "doc": { 
      "properties": {
        "name":            { "type": "string" },
        "depth":           {"type":"long"},
        "children": { 
          "type":             "nested",
          "properties": {
            "name":       { "type": "string" },
            "depth":          { "type": "long"   },
        "children": { 
          "type":             "nested",
          "properties": {
            "code":           { "type": "string" },
            "name":       { "type": "string" },
            "depth":          { "type": "long"   },
            "children":   { 
              "type":         "nested",
              "properties": {
                "code":     { "type": "string" },
                "name":    { "type": "string" },
                "depth":     { "type": "long" },
          }
        }
      }
    }
  }
}
}
}
}
}


please tell me what is the query i am applying here.i am applying the query like this:
GET /icd10_codes/doc/_search
{
  "_source": false,
  "query": {
    "nested": {
      "path": "children",
      "query": {
    "nested": {
      "path": "children.children",
 "query": {
    "nested": {
      "path": "children.children.children",
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "children.children.name": "Iron"
              }
            }
          ]
        }
      },
       "inner_hits": {        
        "_source": {
          "excludes":["name"]
        }
      }
    }
  }
}
}
}
}</comment><comment author="abhishek5678" created="2017-04-27T06:48:31Z" id="297627064">Hi @RanadeepPolavarapu

when i am running your example then it is not working it's showing like that:Unknown key for a START_OBJECT in [inner_hits]</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The queue_size value should be shown as an integer.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13063</link><project id="" key="" /><description>closes #10404
</description><key id="102678686">13063</key><summary>The queue_size value should be shown as an integer.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Stats</label><label>breaking</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-24T00:51:28Z</created><updated>2015-08-24T20:03:44Z</updated><resolved>2015-08-24T08:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-24T07:36:24Z" id="134073174">LGTM. Thx. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add two phased commit to Cluster State publishing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13062</link><project id="" key="" /><description>When publishing a new cluster state, the master will send it to all the node of the cluster, noting down how many _master_ nodes responded successfully. The nodes do not yet process the new cluster state, but rather park it in memory. As soon as at least minimum master nodes have ack-ed the cluster state change, it is committed and a commit request is sent to all the node that responded so far (and will respond in the future). Once receiving the commit requests the nodes continue to process the cluster state change as they did before this change.

A few notable comments:
1. For this change to have effect, min master nodes must be configured.
2. All basic cluster state validation is done in the first phase of publish and is thus now part of `ShardOperationResult`
3. A new `COMMIT_TIMEOUT` settings is introduced, dictating how long a master should wait for nodes to ack the first phase. Unlike `PUBLISH_TIMEOUT`, if waiting for a commit times out, the cluster state change will be rejected.
4. Failing to achieve a min master node of acks, will cause the master to step down as it clearly doesn't have enough active followers.
5. Previously there was a short window between the moment a master lost it's followers and it stepping down because of node fault detection failures. In this short window, the master could process any change (but fail to publish it). This PR closes this gap to 0.

I still have one no commit and some docs to add but I think we can start the review cycles.

@brwe @imotov and @jasontedor  - can you have a **careful** look when you have time?
</description><key id="102660632">13062</key><summary>Add two phased commit to Cluster State publishing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>feature</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-08-23T20:58:07Z</created><updated>2015-09-15T08:50:09Z</updated><resolved>2015-09-14T08:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-24T15:25:09Z" id="134248031">@brwe @imotov @jasontedor FYI I removed the no commit.
</comment><comment author="bleskes" created="2015-08-25T13:05:10Z" id="134578658">I update the docs. @clintongormley @jasontedor - I would love a native English speaker to review it - can you take a look?
</comment><comment author="jasontedor" created="2015-08-25T13:35:48Z" id="134587129">@bleskes I left a review of the [updated docs](https://github.com/bleskes/elasticsearch/commit/ec5675887629881cf69de2bb350f42e504cf8d3e) but I'm still in progress on a review of the code.
</comment><comment author="clintongormley" created="2015-08-25T13:56:24Z" id="134593812">left some minor docs suggestions
</comment><comment author="bleskes" created="2015-08-26T14:10:19Z" id="135034656">@imotov @brwe pushed another commit addressing comments so far
</comment><comment author="bleskes" created="2015-08-27T15:16:09Z" id="135465626">@imotov @brwe I pushed another commit. I think I addressed all feedback so far.
</comment><comment author="imotov" created="2015-08-27T15:17:55Z" id="135466112">LGTM
</comment><comment author="brwe" created="2015-08-28T10:08:13Z" id="135726444">Just https://github.com/elastic/elasticsearch/pull/13062#discussion_r38091772 left and I am not too passionate about it. LGTM too otherwise. 
</comment><comment author="bleskes" created="2015-08-28T11:12:42Z" id="135742909">@brwe thx. I update the PR based on your last comment (and rebased to latest master)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add plugin modules before (almost all) others</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13061</link><project id="" key="" /><description>This change makes modules added by plugins come before others, as it was
before #12783. The order of configuration, and thereby binding, happens
in the order modules are received, and without this change, some plugins
can get _insane_ guice errors (500mb stack trace).
</description><key id="102643064">13061</key><summary>Add plugin modules before (almost all) others</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-23T18:11:27Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-23T18:16:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-23T18:14:30Z" id="133889133">LGTM thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make FunctionScore work on unmapped field with `missing` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13060</link><project id="" key="" /><description>#10948: FunctionScore wont raise an exception if the field is not mapped and the user provided an 'missing' value.

P.S: Will greatly appreciate any feedback regarding my code, today is my first day trying to help the elasticsearch community. Thanks!

Closes #10948
</description><key id="102582720">13060</key><summary>Make FunctionScore work on unmapped field with `missing` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">andrestc</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-23T02:43:22Z</created><updated>2015-09-01T09:05:40Z</updated><resolved>2015-09-01T09:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-24T11:42:08Z" id="134163227">@andrestc I left a minor comment but otherwise I like the change.
</comment><comment author="andrestc" created="2015-08-24T12:21:00Z" id="134170484">@jpountz There it is! Thanks for the feedback.
</comment><comment author="andrestc" created="2015-08-24T13:37:07Z" id="134203074">@jpountz Just pushed the change. Thanks
</comment><comment author="jpountz" created="2015-08-24T13:42:20Z" id="134205012">Thanks @andrestc. I will merge next week when I'm back from traveling unless someone beats me to it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rethink plugin modules and services</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13059</link><project id="" key="" /><description>We currently allow creating modules per node, per index, and per shard. It is unclear if any plugins actually use this, or conceivably need it (see https://github.com/elastic/elasticsearch/pull/13034#discussion_r37699770).

There are two reasons for modules:
- Some extension points require constructing a class which gets an injected existing service to add custom functionality (for example, adding custom mapping types). This is being fixed by moving all extension points to be handled through existing modules.
- To create a new service, the new service must be bound in a module, and then returned with `nodeServices()`, `indexServices()` or `shardServices()`.

We should simplify the latter, and in general think how we could provide the ability to add services without injection. IMO this would mean clearly defining what services each type of extension has access to. For example, discovery plugins probably need access to NetworkService, but do they really need to be allowed to grab RepositoryService in their ctor?
</description><key id="102570030">13059</key><summary>Rethink plugin modules and services</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>discuss</label></labels><created>2015-08-22T22:01:14Z</created><updated>2016-10-06T00:57:15Z</updated><resolved>2016-10-06T00:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-10-06T00:57:14Z" id="251840990">We have a good handle now on how plugin extension points are designed in 5.0, so I am closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warmers delete _all should not throw exception when no warmers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13058</link><project id="" key="" /><description>I think this fixes #8991. It will not throw exceptions when the request is to delete "_all" warmers but log it.
</description><key id="102563035">13058</key><summary>Warmers delete _all should not throw exception when no warmers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">andrestc</reporter><labels><label>:Warmers</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-22T19:54:34Z</created><updated>2015-11-22T10:14:27Z</updated><resolved>2015-08-27T10:25:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-24T12:45:17Z" id="134182211">Thanks for the PR @andrestc I reviewed it and left two comments, would it be possible also to write a test around this change please?
</comment><comment author="andrestc" created="2015-08-24T15:34:48Z" id="134253733">@javanna Thanks for the review. Will work on it!
</comment><comment author="andrestc" created="2015-08-25T21:06:52Z" id="134742190">@javanna I've updated the PR with your observations.
Thanks!
</comment><comment author="javanna" created="2015-08-26T06:59:24Z" id="134868689">Thanks @andrestc I did another round and left a couple of minor comments, looks good though.
</comment><comment author="andrestc" created="2015-08-26T13:26:35Z" id="135019212">@javanna thanks! I will work on them today and submit it again later.
</comment><comment author="andrestc" created="2015-08-26T15:06:24Z" id="135053776">@javanna I've updated the PR.
Is there any difference in adding a random test case and doing what I did?

Thanks!
</comment><comment author="javanna" created="2015-08-26T15:08:25Z" id="135054610">LGTM thanks @andrestc I will merge this soon.

&gt; Is there any difference in adding a random test case and doing what I did?

you test is fine, thanks!
</comment><comment author="javanna" created="2015-08-27T10:25:30Z" id="135374246">merged https://github.com/elastic/elasticsearch/commit/793fcb699807f3099bd064fcab77e0b8aadfc567 thanks @andrestc !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make mlockall configuration easier.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13057</link><project id="" key="" /><description>If the user tries to enable mlockall, chances are it will not work without some additional OS-level configuration. Today we try help show you how to get it working, but instead of using `esuser` in the example, we should just use the current username (user.name is always available from java). This means if they just blindly copy/paste it will work.

```
[2015-08-22 09:54:24,332][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=12,reason=Cannot allocate memory
[2015-08-22 09:54:24,332][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
[2015-08-22 09:54:24,332][WARN ][org.elasticsearch.bootstrap] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536
[2015-08-22 09:54:24,332][WARN ][org.elasticsearch.bootstrap] These can be adjusted by modifying /etc/security/limits.conf, for example: 
    # allow user 'rmuir' mlockall
    rmuir soft memlock unlimited
    rmuir hard memlock unlimited
[2015-08-22 09:54:24,332][WARN ][org.elasticsearch.bootstrap] If you are logged in interactively, you will have to re-login for the new limits to take effect.
```
</description><key id="102538962">13057</key><summary>Make mlockall configuration easier.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-22T13:58:26Z</created><updated>2015-11-22T10:14:26Z</updated><resolved>2015-08-24T12:49:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-22T15:32:42Z" id="133715656">Awesome! 
</comment><comment author="s1monw" created="2015-08-22T21:11:37Z" id="133755152">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename position_offset_gap to position_increment_gap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13056</link><project id="" key="" /><description>Rename position_offset_gap to position_increment_gap

closes #12562
</description><key id="102517010">13056</key><summary>Rename position_offset_gap to position_increment_gap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta2</label></labels><created>2015-08-22T08:47:10Z</created><updated>2015-11-22T10:14:26Z</updated><resolved>2015-08-26T22:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-24T22:42:41Z" id="134402361">Updated and rebased. I think this is ready for another round of review. Thanks a lot.
</comment><comment author="nik9000" created="2015-08-25T18:07:14Z" id="134688714">Left a small comment otherwise LGTM.
</comment><comment author="nik9000" created="2015-08-25T18:13:44Z" id="134690704">I think we will have a race between this and #12544. I think it's about to win so you might have some rebasing to do, unfortunately.
</comment><comment author="xuzha" created="2015-08-25T18:16:16Z" id="134691338">Thanks so much for the review, @nik9000 and @rjernst 

I will do the rebase :-)
</comment><comment author="nik9000" created="2015-08-25T18:24:34Z" id="134693165">&gt; I will do the rebase :-)

And now master is ready for you. I'm going to try backporting #12544 to 2.0 as requested in the pr which might complicate things more. It shouldn't be too bad.
</comment><comment author="xuzha" created="2015-08-25T22:34:39Z" id="134761219">Rebase, all tests passed on my machine. @nik9000 please, would you like to do a final check?
</comment><comment author="nik9000" created="2015-08-25T22:37:02Z" id="134761607">Looks good. I left two minor comments that you can fix without a re-review I think. Other than that it looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Removed plugin.types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13055</link><project id="" key="" /><description>The setting `plugin.types` is currently used to load plugins from the
classpath. This is necessary in tests, as well as the transport client.

This change removes the setting, and replaces it with the ability to
directly add plugins when building a transport client, as well as
infrastructure in the integration tests to specify which plugin classes
should be loaded on each node.

Note: I have nocommit in the docs as I'm not sure how to properly create a link to another page.
</description><key id="102516373">13055</key><summary>Plugins: Removed plugin.types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-22T08:41:51Z</created><updated>2015-12-26T08:47:14Z</updated><resolved>2015-08-30T23:40:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T14:05:57Z" id="134216930">The link can be written as:

```
read more in {ref}/integration-tests.html#changing-node-configuration[Changing Node Configuration].
```
</comment><comment author="rjernst" created="2015-08-24T19:05:39Z" id="134342460">I've updated the link and synced with master.
</comment><comment author="dakrone" created="2015-08-25T23:23:51Z" id="134768970">@rjernst the way this is specified for the MockNode seems a little crazy, can we do a var-args version of `MockNode` so we can do:

``` java
public MockNode(Settings settings, boolean loadConfigSettings, Version version, Class&lt;? extends Plugin&gt;... classpathPlugins) {
    ...
}
```

And then call it with:

``` java
MockNode m = new MockNode(settings, false, Version.CURRENT, NativeScriptPlugin.class, MyOtherPlugin.class);
```
</comment><comment author="rjernst" created="2015-08-26T00:48:25Z" id="134780234">@dakrone There are only 3 callers really: test cluster start, test restart node, and transport client. They all already have a collection (because they've been building it up in a list). The calls that you seem to be referring to are just for those 3 script benchmarks...so I don't think we should switch to an array because it would actually be more work in the "real" callers.
</comment><comment author="dakrone" created="2015-08-26T16:14:57Z" id="135083162">Okay, the 3 callers thing makes sense, I think this looks good and makes the tests with custom plugins look much nicer!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add favicon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13054</link><project id="" key="" /><description>Looks like this. Let the bikeshedding begin :)

![favicon](https://cloud.githubusercontent.com/assets/504194/9422484/2b32d65c-4863-11e5-96b3-707660d7e4f6.png)
</description><key id="102502015">13054</key><summary>Add favicon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:REST</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-22T04:18:18Z</created><updated>2015-11-22T10:14:26Z</updated><resolved>2015-08-24T12:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-22T04:49:49Z" id="133634992">LGTM I raised one little concern :8ball: 
</comment><comment author="tlrx" created="2015-08-24T08:31:10Z" id="134093058">Nice... Looks like you were bored with unicast/multicast things :)
</comment><comment author="jpountz" created="2015-08-24T11:53:44Z" id="134164985">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup bootstrap package.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13053</link><project id="" key="" /><description>Cleanup the bootstrap package a bit:
- makes most classes final and package private
- removes duplicate and confusing multiple entry points
- adds javadocs to some classes like JarHell,Security
- adds a public class BootStrapInfo that exposes any stats
  needed by outside code.

Closes #13044
</description><key id="102500016">13053</key><summary>Cleanup bootstrap package.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-22T03:31:29Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-22T04:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-22T03:51:01Z" id="133632596">Looks good to me
</comment><comment author="vinhn" created="2015-08-22T07:44:19Z" id="133649310">Privatizing these kinds of things may make it harder for users to extend/customize the ES code, or to support other creative use cases like ES embedded inside another app/container.
</comment><comment author="rmuir" created="2015-08-22T12:22:36Z" id="133685972">This code is not extendable. Its not intended for extension.
Embedding inside a container is not supported.
</comment><comment author="bleskes" created="2015-08-24T08:29:22Z" id="134091856">@vinhn if you want to embed an ES node, `org.elasticsearch.node.NodeBuilder` is the way to go.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting java.lang.NoSuchFieldError run time error on field JRE_IS_64BIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13052</link><project id="" key="" /><description>My environment: I am using ES 1.7.1, with jdk1.7.0_72, on a Linux x86-64 bit platform, and in its lib directory, there is this jar called lucene-core-4.10.4.jar.

A simple ES transport client, using its JAVA API, produces this error stack trace:
java.lang.NoSuchFieldError: JRE_IS_64BIT
        at org.apache.lucene.util.RamUsageEstimator.&lt;clinit&gt;(RamUsageEstimator.java:145)
        at org.elasticsearch.common.util.BigArrays.&lt;clinit&gt;(BigArrays.java:51)
        at org.elasticsearch.common.io.stream.BytesStreamOutput.&lt;init&gt;(BytesStreamOutput.java:55)
        at org.elasticsearch.common.io.stream.BytesStreamOutput.&lt;init&gt;(BytesStreamOutput.java:45)
        at org.elasticsearch.common.xcontent.XContentBuilder.builder(XContentBuilder.java:77)
        at org.elasticsearch.common.xcontent.json.JsonXContent.contentBuilder(JsonXContent.java:40)
        at org.elasticsearch.common.xcontent.XContentFactory.contentBuilder(XContentFactory.java:122)
        at org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder(XContentFactory.java:49)

When i de-compile lucene-core-4.10.4.jar, i do see the field JRE_IS_64BIT though.

Is it possible that an out of version JAR is packaged up inside ES 1.7.1?
</description><key id="102495282">13052</key><summary>Getting java.lang.NoSuchFieldError run time error on field JRE_IS_64BIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kvramana</reporter><labels /><created>2015-08-22T01:55:39Z</created><updated>2015-08-22T04:10:57Z</updated><resolved>2015-08-22T04:10:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kvramana" created="2015-08-22T04:10:57Z" id="133633395">Closing it, i figured that an older version of lucene-core.jar got ahead into classpath, removed it and it's working as it should.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene SPI support for plugins.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13051</link><project id="" key="" /><description>When we create a plugin's classloader, we should allow it to register things in
the lucene SPI (registry of Tokenizers, TokenFilters, CharFilters, Codec,
PostingsFormat, DocValuesFormat).

Plugins should be able to do this so they can extend Lucene.
</description><key id="102484042">13051</key><summary>Lucene SPI support for plugins.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-22T00:14:56Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-22T00:46:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-22T00:16:41Z" id="133594253">This PR works, but it would be nice to test: maybe it should be a followup.

We should probably add an API somewhere (it can be internal or whatever) to dump out the contents of these registries for debugging anyway...

If we did this, we could add integration tests to our current analysis plugins that they are registered correctly, and that would do the trick.
</comment><comment author="rjernst" created="2015-08-22T00:21:02Z" id="133594703">LGTM
</comment><comment author="uschindler" created="2015-08-22T00:22:24Z" id="133595210">+1. We should at some point add an API to Lucene that does this in one call (Lucene should know on its own which SPIs are there)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve startup exceptions (especially file permissions etc)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13050</link><project id="" key="" /><description>Its probably likely users will configure paths and have issues (file not found, bad file permissions, etc).
Today the failure is pitiful:

```
Exception in thread "main" /path
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:767)
        at org.elasticsearch.bootstrap.Security.ensureDirectoryExists(Security.java:171)
        at org.elasticsearch.bootstrap.Security.addPath(Security.java:151)
        at org.elasticsearch.bootstrap.Security.createPermissions(Security.java:133)
        at org.elasticsearch.bootstrap.Security.configure(Security.java:57)
        at org.elasticsearch.bootstrap.Bootstrap.setupSecurity(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:166)
        at org.elasticsearch.bootstrap.Bootstrap.doMain(Bootstrap.java:281)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:228)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
Refer to the log for complete error details.
```

We need more context here, like this:

```
Exception in thread "main" java.lang.IllegalStateException: Unable to access 'path.data' (/path/to/data)
Likely root cause: java.nio.file.AccessDeniedException: /path
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:767)
        at org.elasticsearch.bootstrap.Security.ensureDirectoryExists(Security.java:181)
        at org.elasticsearch.bootstrap.Security.addPath(Security.java:158)
        at org.elasticsearch.bootstrap.Security.createPermissions(Security.java:133)
        at org.elasticsearch.bootstrap.Security.configure(Security.java:57)
        at org.elasticsearch.bootstrap.Bootstrap.setupSecurity(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:166)
        at org.elasticsearch.bootstrap.Bootstrap.doMain(Bootstrap.java:281)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:228)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
Refer to the log for complete error details.
```
</description><key id="102471848">13050</key><summary>Improve startup exceptions (especially file permissions etc)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T22:11:49Z</created><updated>2015-08-24T14:00:09Z</updated><resolved>2015-08-21T22:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-21T22:14:14Z" id="133577828">LGTM
</comment><comment author="rjernst" created="2015-08-21T22:14:36Z" id="133577879">LGTM too!
</comment><comment author="rmuir" created="2015-08-21T22:24:51Z" id="133579310">Closed via https://github.com/elastic/elasticsearch/commit/d96af934db7a9ab25e8e76f01a2b5fe4c26f6ec1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't check if directory is present to prevent races</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13049</link><project id="" key="" /><description>We do check if a directory is present and then open a dir stream on
it. Yet the file can be concurrrently deleted which is OK but we fail
with a hard exception. This change tries to open the dir directly (listing via stream)
and catches NoSuchFileEx | FNFEx.

here is a CI failure that shows the problem: http://build-us-00.elastic.co/job/es_core_master_medium/2121/testReport/junit/org.elasticsearch.gateway/MetaDataWriteDataNodesIT/testMetaIsRemovedIfAllShardsFromIndexRemoved/

this test does this check potentially concurrent to the directory deletion.
</description><key id="102451348">13049</key><summary>Don't check if directory is present to prevent races</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-21T19:53:46Z</created><updated>2015-11-22T10:14:26Z</updated><resolved>2015-08-24T12:47:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-21T20:05:00Z" id="133546603">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>need clarification for mmapfs?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13048</link><project id="" key="" /><description>How can we increase virtual memory on linux?
But if use mmapfs, how it works? In case we have 100 indexes and each of size like 2-3 GB.
In case we have memory up to like 20 GB and in such case will elasticsearch handle, how many will stay in memory and how memory will remain on disk and in this case, mmapfs is not an option?
</description><key id="102444668">13048</key><summary>need clarification for mmapfs?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lokeshhctm</reporter><labels /><created>2015-08-21T19:07:08Z</created><updated>2015-08-24T13:59:17Z</updated><resolved>2015-08-24T13:59:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T13:59:17Z" id="134214158">Hi @lokeshhctm 

Please ask questions like these on the forum instead: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>On disk replicas not used after node restart if hit low disk watermark</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13047</link><project id="" key="" /><description>Today I needed to restart a node in my Elasticsearch 1.7.1 cluster for a firmware update. It's used with time based indices. The disks are over 85% full, but they won't fill for a while so was planning on adding more storage next week. However when I restarted the node, it wouldn't serve any shards as the disk was over the 85% threshold, yet the shards were already on disk and all but the latest time based index was exactly the same as the primary elsewhere in the cluster.

It's not a major issue, just forced me to sort out my disk space situation before being able to restart any other nodes, and it might have been a pain to recover the data if the disk crashed on another node.

```
[node2] low disk watermark [85%] exceeded on [xxxxxx][node3] free: 50.2gb[12.4%], replicas will not be assigned to this node
```
</description><key id="102444554">13047</key><summary>On disk replicas not used after node restart if hit low disk watermark</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-21T19:06:14Z</created><updated>2015-08-28T09:58:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-28T09:56:13Z" id="135723753">The problem here is that, while a replica may exist, we have no idea whether the replica is the same as the primary or not (unless the replica is sync flushed).  So we may end up having to copy over the whole primary (while still keeping the replica around just in case) which could push you over the limit.

I think what should happen here is
- existing primaries should be recovered automatically (needs checking)
- sync-flushed replicas should be recovered automatically (probably needs adding)
- once the index is green, unused shard copies should be deleted (which is what happens now)

It is still possible to get into a deadlock situation, eg:
- two nodes, 1 replica, both nodes over the low watermark
- the primary recovers automatically
- the replica is not sync-flushed, but the node is over the low watermark and so the shard will not recover

In this case, the low watermark can be changed dynamically (and temporarily) to allow recovery.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Turn DestructiveOperations.java into a Guice module.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13046</link><project id="" key="" /><description>Closes https://github.com/elastic/elasticsearch/issues/4665

Inject DestructiveOperations object rather than use new.

Use constant rather than hard-coded string
</description><key id="102441255">13046</key><summary>Turn DestructiveOperations.java into a Guice module.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimhooker2002</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-21T18:46:33Z</created><updated>2015-08-31T19:22:55Z</updated><resolved>2015-08-31T19:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-23T19:12:33Z" id="133900480">@jimhooker2002 left one comment, other than that this looks good.
</comment><comment author="jimhooker2002" created="2015-08-29T06:45:48Z" id="135948458">Thanks for the review Martijn.  I've now extended AbstractComponent and removed the logger.  Code tested/clean/pushed and ready for pull.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide case insensitive option for mapping char_filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13045</link><project id="" key="" /><description>mapping char_filter is currently case-sensitive, can be helpful to provide a configurable option for it to perform the replace action in a case-insensitive manner.

https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-mapping-charfilter.html?q=char%20filter
</description><key id="102434077">13045</key><summary>Provide case insensitive option for mapping char_filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Analysis</label><label>enhancement</label></labels><created>2015-08-21T18:05:48Z</created><updated>2016-10-08T11:06:24Z</updated><resolved>2015-08-21T18:58:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-21T18:10:26Z" id="133516712">What is the use case? The only purpose of a charfilter is to "hack" the text before the tokenizer, and most tokenizers don't care about case.
</comment><comment author="ppf2" created="2015-08-21T18:58:21Z" id="133530707">Thanks for the feedback, sounds like this is not what the mapping char_filter is intended for so I will close this one out :)  For this particular use case, the end user is trying to use a keyword tokenizer and have some of the terms have synonyms (which doesn't work with keyword), so they are using char_filter as an alternate option.  Workaround here can be to do a -&gt; z, A-&gt; z in the char_filter, or add another char_filter that lowercase everything first.
</comment><comment author="tomfotherby" created="2016-09-27T10:22:15Z" id="249825765">My use case for wanting a case-insensitive mapping `char_filter` is that I have a custom tokeniser which splits on many characters including the forwardslash. However I have some domain-specific words, for example `i/o` that I want to keep as one. So I have a mapping char_filter to do `'i/o =&gt; i_fs_o'` and then a `pattern_replace` filter to put it back again (`'pattern'     =&gt; '(.*)_fs_(.*)','replacement' =&gt; '$1/$2',`).  The problem is that I need to specify every case variate of i/o in the mapping ('I/o','I/O', etc). Since I have 50 or so such keywords, the variations of case bloat the mapping considerably.
</comment><comment author="clintongormley" created="2016-10-08T11:06:24Z" id="252418700">@tomfotherby Use the [pattern-replace character filter](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/analysis-pattern-replace-charfilter.html) instead
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make Bootstrap package private, Elasticsearch.main() calls Bootstrap.doMain()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13044</link><project id="" key="" /><description>Today its Elasticsearch.main() -&gt; Bootstrap.main() -&gt; Bootstrap.doMain(), and they are all public.

Its too much and adds a unnecessary stack frame to any exceptions to the console. We can also reduce exposed surface area of the scary stuff in Bootstrap by making the whole class package private.

And we should only have one entry point to start ES.
</description><key id="102406494">13044</key><summary>make Bootstrap package private, Elasticsearch.main() calls Bootstrap.doMain()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label></labels><created>2015-08-21T15:40:21Z</created><updated>2015-08-22T04:21:20Z</updated><resolved>2015-08-22T04:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Do not permit multiple settings files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13043</link><project id="" key="" /><description>This commit enforces that at most a single settings file is found. If
multiple settings files are found, a SettingsException will be thrown

Closes #13042
</description><key id="102406125">13043</key><summary>Do not permit multiple settings files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>breaking</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T15:38:40Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-21T16:28:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-21T15:51:38Z" id="133469839">Similar concerns as Robert for tests, but otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not permit multiple settings files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13042</link><project id="" key="" /><description>In #13039 it was discussed that we should not permit multiple settings files. Allowing multiple settings files just [adds complexity](https://github.com/elastic/elasticsearch/pull/13039#discussion_r37642154) for the user to reason where a particular setting came from.
</description><key id="102404686">13042</key><summary>Do not permit multiple settings files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>breaking</label></labels><created>2015-08-21T15:32:05Z</created><updated>2015-08-24T13:56:23Z</updated><resolved>2015-08-21T16:28:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use StartupError to format all exceptions hitting the console</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13041</link><project id="" key="" /><description>Closes #13040
</description><key id="102399938">13041</key><summary>Use StartupError to format all exceptions hitting the console</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Exceptions</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T15:06:24Z</created><updated>2015-08-24T13:55:12Z</updated><resolved>2015-08-21T15:09:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-21T15:07:33Z" id="133456848">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use StartupException more consistently in Bootstrap for consistent console error formatting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13040</link><project id="" key="" /><description>I noticed this when testing https://github.com/elastic/elasticsearch/pull/13039

We only use this exception today after we startup, but we should probably try to use it during startup too.
</description><key id="102397855">13040</key><summary>Use StartupException more consistently in Bootstrap for consistent console error formatting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Exceptions</label><label>adoptme</label><label>bug</label></labels><created>2015-08-21T14:56:53Z</created><updated>2015-08-24T13:55:26Z</updated><resolved>2015-08-24T13:55:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T13:55:26Z" id="134212257">Closed by #13041
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not swallow exceptions thrown while parsing settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13039</link><project id="" key="" /><description>This commit fixes an issue that was causing Elasticsearch to silently
ignore settings files that contain garbage. The underlying issue was
swallowing an `SettingsException` under the assumption that the only
reason that an exception could be throw was due to the settings file
not existing (in this case the `IOException` would be the cause of the
swallowed `SettingsException`). This assumption is mistaken as an
`IOException` could also be thrown due to an access error or a read
error. Additionally, a `SettingsException` could be thrown exactly
because garbage was found in the settings file. We should instead
explicitly check that the settings file exists, and bomb on an
exception thrown for any reason.

Closes #13028
</description><key id="102395081">13039</key><summary>Do not swallow exceptions thrown while parsing settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>blocker</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T14:44:45Z</created><updated>2015-08-24T13:54:07Z</updated><resolved>2015-08-21T15:07:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-21T14:55:09Z" id="133452646">+1, I think this is a good step.
</comment><comment author="jasontedor" created="2015-08-21T15:46:26Z" id="133468355">Thanks for helpful review and comments @jpountz, @dakrone and @rmuir. Pushed to master and back ported to 2.0. I've opened a separate issue #13042 to address multiple settings files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Port fix for making extended bounds recognize timezones to v1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13038</link><project id="" key="" /><description>Retrofit the fix for elastic#12278 to v1.7.  This also required retrofitting elastic#12531.
</description><key id="102389992">13038</key><summary>Port fix for making extended bounds recognize timezones to v1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">csquire</reporter><labels><label>:Aggregations</label><label>review</label></labels><created>2015-08-21T14:23:59Z</created><updated>2016-04-06T20:54:45Z</updated><resolved>2016-04-06T20:54:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T20:54:45Z" id="206563309">Hi @csquire we have no plans to release any more 1.7 releases except in critical security bug fixes, so I'm going to close this for now. Thanks for the submission though!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize counts on simple queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13037</link><project id="" key="" /><description>Today we always collect in order to compute counts, but some queries can be
easily optimized by using pre-computed index statistics. This is especially
true in the case that there are no deletions, which should be common for the
time-based data use-case.

Counts on match_all queries can always be optimized, so requests like

```
GET index/_search?size=0

GET index/_search
{
  "size": 0,
  "query" : {
    "match_all": {}
  }
}
```

should now return almost instantly. Additionally, when there are no deletions,
term queries are also optimized, so the below queries which all boil down to a
single term query would also return almost immediately:

```
GET index/type/_search?size=0

GET index/_search
{
  "size": 0,
  "query" : {
    "match": {
      "foo": "bar"
    }
  }
}

GET index/_search
{
  "size": 0,
  "query" : {
    "constant_score": {
      "filter": {
        "exists": {
          "field": "foo"
        }
      }
    }
  }
}
```
</description><key id="102363875">13037</key><summary>Optimize counts on simple queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-21T12:04:06Z</created><updated>2015-09-03T07:47:49Z</updated><resolved>2015-09-03T07:47:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-02T07:14:50Z" id="136958886">@nik9000 Thanks for having a look. I addressed feedback and rewrote the optimization in a way that should be more robust.
</comment><comment author="nik9000" created="2015-09-02T12:44:08Z" id="137062237">&gt; @nik9000 Thanks for having a look. I addressed feedback and rewrote the optimization in a way that should be more robust.
&gt;  Show all checks

Thanks! Its very readable.
</comment><comment author="nik9000" created="2015-09-02T12:44:10Z" id="137062245">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit type name length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13036</link><project id="" key="" /><description>This commit will limit type name length to be at most 255 characters.

Closes #13021
</description><key id="102362748">13036</key><summary>Limit type name length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Index APIs</label><label>breaking</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T11:58:06Z</created><updated>2015-11-22T11:31:46Z</updated><resolved>2015-08-21T13:19:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-21T12:02:46Z" id="133391981">Looks good.

May be tell people how much long it is for now? 

`mapping type name [" + mapper.type() + "] is " + mapper.type().length() + " characters long; limit is length 255`
</comment><comment author="s1monw" created="2015-08-21T13:16:56Z" id="133423229">LGTM
</comment><comment author="jasontedor" created="2015-08-21T13:17:28Z" id="133423332">Good idea @dadoonet. I've [added](https://github.com/jasontedor/elasticsearch/commit/8bfabb14bc546895e8eb6515856c393267a40006) that.
</comment><comment author="jasontedor" created="2015-08-21T13:35:18Z" id="133426401">Thanks @dadoonet and @s1monw for reviewing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation: Fix AggregationPath.subPath() to not throw ArrayStoreException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13035</link><project id="" key="" /><description>Aggregation.subPath() always threw an ArrayStoreException because we were trying to pass a List into System.arraycopy(). This change fixes that bug and adds a test to prevent regression
</description><key id="102359556">13035</key><summary>Aggregation: Fix AggregationPath.subPath() to not throw ArrayStoreException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T11:42:28Z</created><updated>2015-08-21T11:52:24Z</updated><resolved>2015-08-21T11:50:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-21T11:44:47Z" id="133384946">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove SpawnModules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13034</link><project id="" key="" /><description>The last use case of spawn modules was for plugins. This change handles
plugin modules directly, and removes SpawnModules.

closes #12783
</description><key id="102353555">13034</key><summary>Remove SpawnModules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-21T11:03:45Z</created><updated>2015-11-22T10:14:26Z</updated><resolved>2015-08-22T22:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-22T16:45:11Z" id="133725424">I synced up with master, and also removed PreProcessModule as it is no longer used.
</comment><comment author="s1monw" created="2015-08-22T21:10:34Z" id="133755023">LGTM
</comment><comment author="clintongormley" created="2015-08-25T12:36:43Z" id="134571523">@rjernst Did you mean to merge this in to 2.0 as well? 
</comment><comment author="s1monw" created="2015-08-25T15:13:24Z" id="134618553">back-ported to 2.0 branch as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoDistance Aggregation now prints field name when it finds an unexpected token.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13033</link><project id="" key="" /><description>closes #12391 
</description><key id="102324153">13033</key><summary>GeoDistance Aggregation now prints field name when it finds an unexpected token.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-21T08:18:19Z</created><updated>2015-11-22T10:15:21Z</updated><resolved>2015-08-26T12:41:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Never cache match_all queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13032</link><project id="" key="" /><description>This commit backports https://issues.apache.org/jira/browse/LUCENE-6748 to make
sure that we never cache trivial queries like MatchAllDocsQuery.
</description><key id="102317178">13032</key><summary>Never cache match_all queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T07:34:21Z</created><updated>2015-08-24T13:35:14Z</updated><resolved>2015-08-21T07:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-21T07:35:15Z" id="133315889">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java native client, date range query and got exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13031</link><project id="" key="" /><description>my mapping:

```
.startObject("endTime")
     .field("type", "date")
     .field("format", "yyyy-MM-dd HH:mm:ss")
.endObject()
```

my source data to index:

``` java
String json = "{" +
            "\"endTime\":\"2015-08-19 09:00:00\"," +
                "\"message\":\"trying out Elasticsearch\"" +
            "}";
```

my query:

``` java
DateTimeFormatter formatter = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss");
LocalDate fromDateTime = formatter.parseLocalDate("2015-08-20 12:27:11");
LocalDate toDateTime = formatter.parseLocalDate("2015-08-29 14:00:00");
QueryBuilder qb = QueryBuilders.rangeQuery(termName).from(fromDateTime).to(toDateTime);
                SearchResponse sResponse = client.prepareSearch(indexName)
                        .setTypes(typeName)
                        .setSearchType(SearchType.QUERY_THEN_FETCH)
                        .setQuery(qb)
                        .setFrom(0).setSize(60).execute().actionGet();
```

my exception:

```
org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query_fetch], all shards failed; shardFailures {[nSf0KsETQUi3J0Sb2i_tTg][twitter][0]: SearchParseException[[twitter][0]: from[0],size[60]: Parse Failure [Failed to parse source [{"from":0,"size":60,"query":{"range":{"endTime":{"from":"2015-08-20","to":"2015-08-29","include_lower":true,"include_upper":true}}},"sort":[{"endTime":{"order":"desc"}}]}]]]; nested: ElasticsearchParseException[failed to parse date field [2015-08-20], tried both date format [yyyy-MM-dd HH:mm:ss], and timestamp number]; nested: IllegalArgumentException[Invalid format: "2015-08-20" is too short]; }
```

can anyone help me out of this?
</description><key id="102306646">13031</key><summary>java native client, date range query and got exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-08-21T06:01:14Z</created><updated>2015-08-21T09:22:40Z</updated><resolved>2015-08-21T06:58:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-21T06:58:00Z" id="133308542">Hey. Please join us on discuss.elastic.co for questions like this.

I think you should simply do this:

``` java
QueryBuilder qb = QueryBuilders.rangeQuery(termName).from("2015-08-20 12:27:11").to("2015-08-29 14:00:00");
SearchResponse sResponse = client.prepareSearch(indexName)
    .setTypes(typeName)
    .setQuery(qb)
    .get();
```
</comment><comment author="makeyang" created="2015-08-21T09:09:00Z" id="133343712">@dadoonet 
first of all, I can't visit discuss.elastic.co. I don't know why. sorry for bother.
And still I have question about this situation.
same mapping, same source data to index and if I got this kind of date format string: 2015-08-21T08:35:13.890Z, how do I do date range query?
</comment><comment author="dadoonet" created="2015-08-21T09:22:40Z" id="133346283">&gt; I can't visit discuss.elastic.co. 

@makeyang Where are you located? I mean are you aware of any firewall within your company?

&gt; I got this kind of date format string: 2015-08-21T08:35:13.890Z

Not sure exactly about the use case here but you can define multiple date formats. Elasticsearch will try the first one, then the second, then will fail... Read this: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>create unit tests for StartupException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13030</link><project id="" key="" /><description>Followup to #13029 . We can probably at least test parts of this, maybe refactor some of it out to static methods, or whatever. we can print to bytearray and assert stuff... it would be an improvement.
</description><key id="102301291">13030</key><summary>create unit tests for StartupException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-08-21T05:18:39Z</created><updated>2016-01-28T12:53:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix formatting of startup/configuration errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13029</link><project id="" key="" /><description>Previous versions of ES wouldn't give you a stacktrace at all, but just print messages, which left you wondering where that `Nested: NullPointerException;` came from.

On the other hand, currently in 2.0 if you misconfigure something and piss guice off enough, you will get _2MB_ of exceptions to the console. Sorry, that is unusable. It may even OOM your terminal window.

This PR attempts to make a compromise, so users arent horrified, but things can be debugged too:
- It tries to present message and root cause with stacktrace
- Stacktraces are limited to 30 frames, otherwise truncated, with a message they were truncated
- Runs of guice frames are replaced with a marker.
- The user is notified to check the logs for the full error details.

Example:

```
Exception in thread "main" Failed to resolve address for [_bogushost]
Likely root cause: java.net.UnknownHostException: _bogushost: unknown error
    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:907)
    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1302)
    at java.net.InetAddress.getAllByName0(InetAddress.java:1255)
    at java.net.InetAddress.getAllByName(InetAddress.java:1171)
    at java.net.InetAddress.getAllByName(InetAddress.java:1105)
    at org.elasticsearch.transport.netty.NettyTransport.parse(NettyTransport.java:642)
    at org.elasticsearch.transport.netty.NettyTransport.addressesFromString(NettyTransport.java:594)
    at org.elasticsearch.transport.TransportService.addressesFromString(TransportService.java:388)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.&lt;init&gt;(UnicastZenPing.java:138)
    at org.elasticsearch.discovery.zen.ping.ZenPingService.&lt;init&gt;(ZenPingService.java:60)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:190)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
Refer to the log for complete error details.
```
</description><key id="102285713">13029</key><summary>Fix formatting of startup/configuration errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T03:03:50Z</created><updated>2015-08-24T13:20:59Z</updated><resolved>2015-08-21T05:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-21T04:58:31Z" id="133285844">WOW man! what a great idea and how simple!! LGTM go push this to 2.0 as well please
</comment><comment author="rjernst" created="2015-08-21T05:02:47Z" id="133286816">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>straight up garbage in elasticsearch.yml does not cause error or even warning.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13028</link><project id="" key="" /><description>I added this:

```
...
# action.destructive_requires_name: true

SKDFLK@$#L%@KL#%L#@$#@L$ #L$@$ #L@K$#L $L $K#L#@L $#L
!!@!@$(#%#)(@)% #(%)
#(%#@)%@#)% (@#%()
()#%@#% (@ )%@%(@#)% @( %)@ %(@)
)(%)@()(%)()(#%)@#
rmuir@beast:~/workspace/
```

Nothing happens.
</description><key id="102283964">13028</key><summary>straight up garbage in elasticsearch.yml does not cause error or even warning.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-21T02:53:11Z</created><updated>2015-08-21T15:07:17Z</updated><resolved>2015-08-21T15:07:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-21T13:38:45Z" id="133427072">it also causes all configuration to be silently ignored.

How could be done any worse?
</comment><comment author="rmuir" created="2015-08-21T13:42:42Z" id="133427807">There is a storm coming. I will break backwards compatibility, whatever it takes to fix this.
</comment><comment author="jasontedor" created="2015-08-21T13:47:52Z" id="133429047">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move multicast discovery to a plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13027</link><project id="" key="" /><description>Multicast has known issues (see #12999 and #12993). This change moves
multicast into a plugin, and deprecates it in the docs.  It also allows
for plugging in multiple zen ping implementations.

I think we should also state in the docs this is deprecated, but I didn't know how to word that or how to mark something as deprecated in asciidoc.

closes #13019
</description><key id="102262465">13027</key><summary>Move multicast discovery to a plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Network</label><label>breaking</label><label>PITA</label><label>v2.0.0-beta1</label></labels><created>2015-08-20T23:52:14Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-21T06:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-21T03:27:40Z" id="133267521">The plugin works, I installed it and set the boolean and things joined over the network.
The plugin needs to be added to the help file with the other plugins right?
Also, I think you have to add it to qa/smoke-test-plugins or it should fail.
</comment><comment author="rjernst" created="2015-08-21T05:33:57Z" id="133293812">I pushed new commits. mvn verify passes.
</comment><comment author="rjernst" created="2015-08-21T06:05:30Z" id="133298222">I also ran verify with -Des.node.mode=network
</comment><comment author="rmuir" created="2015-08-21T06:11:17Z" id="133298727">me too. +1
</comment><comment author="dadoonet" created="2015-08-21T06:32:41Z" id="133303372">Missing change in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/PluginManager.java and in plugin manager tests.
</comment><comment author="bleskes" created="2015-08-21T06:35:28Z" id="133305003">LGTM2, apart from what @dadoonet said (which I wouldn't have come up with my self ... )
</comment><comment author="dadoonet" created="2015-08-21T06:40:12Z" id="133305835">Yeah we need to find a way to automatically fail when we omit such a change.
May be a qa thing for each plugin listed in plugins?
</comment><comment author="dadoonet" created="2015-08-21T07:07:50Z" id="133312061">@rjernst Also the documentation build is failing. Could you check?
</comment><comment author="dadoonet" created="2015-08-21T07:40:36Z" id="133316537">&gt; Also the documentation build is failing. Could you check?

Thank you @jpountz for fixing it: c54a4c4f8995fea61f0e02586faf6101661231e4
</comment><comment author="brwe" created="2015-08-21T09:14:02Z" id="133344723">@rjernst I think this should this be backported to 2.0 branch?
</comment><comment author="rjernst" created="2015-08-21T11:04:28Z" id="133373455">Yes, I wanted to give CI a little time to chew. I'll backport now.
</comment><comment author="dadoonet" created="2015-08-21T11:32:19Z" id="133379429">@rjernst Could you also add `discovery-multicast` to [PluginManagerIT](https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java#L551-L574)?
</comment><comment author="s1monw" created="2015-08-21T11:47:25Z" id="133386029">&gt; @rjernst Could you also add discovery-multicast to PluginManagerIT?

done on master and 2.0
</comment><comment author="clintongormley" created="2015-08-24T08:49:46Z" id="134099231">@rjernst Please could you add this to the migration docs as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't print lots of noise on IPv4 only hosts.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13026</link><project id="" key="" /><description>If the machine doesn't support IPv6, or if the user disabled it
with -Djava.net.preferIPv4Stack, or if the user disabled it with
ES_USE_IPV4, we will still try to bind a socket to ::1 out of box,
and we will see lots of exceptions logged to the console.

It is harmless noise but not a great experience.
</description><key id="102237756">13026</key><summary>Don't print lots of noise on IPv4 only hosts.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-20T21:08:05Z</created><updated>2015-08-24T13:15:51Z</updated><resolved>2015-08-20T21:11:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-20T21:10:24Z" id="133176157">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] typos in derivative aggregation documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13025</link><project id="" key="" /><description>Using "bucket_paths" makes the server return a 400 with "Unknown key for a VALUE_STRING in [aggregation-name]: [buckets_paths]."
</description><key id="102237258">13025</key><summary>[Docs] typos in derivative aggregation documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mpereira</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-08-20T21:06:03Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-24T12:17:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-24T11:59:11Z" id="134165955">@mpereira thanks for pointing this out. Could I ask you to sign the Contributor Licence Agreement (https://www.elastic.co/contributor-agreement/) so I can merge this fix?
</comment><comment author="mpereira" created="2015-08-24T12:10:53Z" id="134168325">@colings86 done.
</comment><comment author="colings86" created="2015-08-24T12:23:46Z" id="134170916">@mpereira thanks very much. This has been merged.
</comment><comment author="mpereira" created="2015-08-24T12:31:01Z" id="134173793">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Couple of typos - various misspellings of `buckets-path`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13024</link><project id="" key="" /><description /><key id="102235499">13024</key><summary>Couple of typos - various misspellings of `buckets-path`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iantruslove</reporter><labels><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-08-20T20:57:23Z</created><updated>2015-11-22T10:15:21Z</updated><resolved>2015-08-24T13:37:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-24T12:00:21Z" id="134166122">@iantruslove thanks for pointing this out. Could I ask you to sign the Contributor Licence Agreement (https://www.elastic.co/contributor-agreement/) so I can merge this fix?
</comment><comment author="iantruslove" created="2015-08-24T13:34:12Z" id="134202256">I signed it right after seeing the note on the PR - I should be good to go
now.

On Mon, Aug 24, 2015 at 6:01 AM, Colin Goodheart-Smithe &lt;
notifications@github.com&gt; wrote:

&gt; @iantruslove https://github.com/iantruslove thanks for pointing this
&gt; out. Could I ask you to sign the Contributor Licence Agreement (
&gt; https://www.elastic.co/contributor-agreement/) so I can merge this fix?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13024#issuecomment-134166122
&gt; .
</comment><comment author="colings86" created="2015-08-24T13:36:11Z" id="134202848">Ah so you did. sorry, I missed that
</comment><comment author="colings86" created="2015-08-24T13:43:23Z" id="134205254">Thanks again for the contribution, this has now been merged in :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce supported Maven versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13023</link><project id="" key="" /><description>Our builds are currently not compatible with Maven versions `&lt; 3.1.0` and
`&gt;= 3.3.0`. This commit will enforce Maven 3 versions that our builds are
known to be compatible with. We will consider these our supported Maven
versions.

Closes #13022
</description><key id="102227093">13023</key><summary>Enforce supported Maven versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v2.0.0-beta2</label></labels><created>2015-08-20T20:14:12Z</created><updated>2015-11-22T10:15:21Z</updated><resolved>2015-09-01T10:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-20T20:37:37Z" id="133163535">See https://github.com/elastic/elasticsearch/pull/13013
</comment><comment author="jasontedor" created="2015-08-20T20:40:26Z" id="133164080">Closing as #13013 was already opened earlier to achieve the same.
</comment><comment author="jasontedor" created="2015-08-21T01:45:49Z" id="133242632">Reopened as #13013 was only aimed at the distribution module.
</comment><comment author="rmuir" created="2015-08-21T03:39:52Z" id="133269401">I agree, this is needed, otherwise we will get strange questions or wacky artifacts being built. 

Also we need it going forward too, this stuff changes over time, we want build servers to fail hard if they are misconfigured and not leave us debugging something complex.
</comment><comment author="dadoonet" created="2015-08-21T07:06:01Z" id="133311846">I'm -1 on this change sorry for all the reasons I gave there: https://github.com/elastic/elasticsearch/pull/13013#issuecomment-133205344

The most important to my heart is this one:

&gt; I think that external contributors could become annoyed if we don't allow them to contribute a patch on core or a plugin because they are running maven 3.3 although mvn install will just work fine...

I'm perfectly fine of failing (or warning only is even better) a single "minor" module such as a rpm one. Basically it will be only a constraint for people having `rpmbuild` installed (most likely Ubuntu users). 
So forcing for example windows users to downgrade their maven version does not make sense to me.

Most likely they won't run QA tests when contributing to the project. So they won't hit any maven issue.
</comment><comment author="jasontedor" created="2015-08-26T18:06:00Z" id="135126124">&gt; I think that external contributors could become annoyed if we don't allow them to contribute a patch on core or a plugin because they are running maven 3.3 although mvn install will just work fine...

I'm not sure that I see the issue with that. Analogously, we have upper bounds on Java language versions (right now Java 7 even though JDK 7 is past end of life).

&gt; I'm perfectly fine of failing (or warning only is even better) a single "minor" module such as a rpm one. Basically it will be only a constraint for people having rpmbuild installed (most likely Ubuntu users). So forcing for example windows users to downgrade their maven version does not make sense to me.

People do build and package elasticsearch from source. We get [questions](https://github.com/elastic/elasticsearch/issues/13126) it. We even call out the instructions for doing so in our [README](https://github.com/elastic/elasticsearch/blob/68307aa9f3636bdccd1f1ca90f1d0fd640a019a5/README.textile#building-from-source) that is displayed on our [project page](https://github.com/elastic/elasticsearch). If we are going to be supporting and answering issues regarding such topics, we should enforce versions that we know our build works with top to bottom with. Those will be our supported versions.
</comment><comment author="dakrone" created="2015-08-26T18:23:41Z" id="135130359">I agree with @jasontedor for this, Maven versions should be bounded just like JDK versions are. Users can and will build their own distribution packages and we should ensure they use a non-broken version of Maven to do that.
</comment><comment author="nik9000" created="2015-08-26T18:44:43Z" id="135135407">&gt; I'm not sure that I see the issue with that. Analogously, we have upper bounds on Java language versions (right now Java 7 even though JDK 7 is past end of life).

Just because Oracle has end-of-life-ed 1.7 doesn't mean its dead. RedHat will continue to support the OpenJDK 7 for a long time. But we've wandered off the point pretty far.

&gt; People do build and package elasticsearch from source. We get questions it. We even call out the instructions for doing so in our README that is displayed on our project page. If we are going to be supporting and answering issues regarding such topics, we should enforce versions that we know our build works with top to bottom with. Those will be our supported versions.

I'm all for this but in that case we have to work harder to get the build working with maven 3.3 because its what you get when you `brew install maven`. I suspect its what you get with `apt-get install maven` and `yum install maven` pretty frequently as well.
</comment><comment author="jasontedor" created="2015-08-26T18:57:33Z" id="135139309">&gt; Just because Oracle has end-of-life-ed 1.7 doesn't mean its dead. RedHat will continue to support the OpenJDK 7 for a long time. But we've wandered off the point pretty far.

That is not germane to this discussion.

&gt; I'm all for this but in that case we have to work harder to get the build working with maven 3.3 because its what you get when you brew install maven. 

That's easily solved with `brew install homebrew/versions/maven32`.

&gt; I suspect its what you get with `apt-get install maven`

It's not. You actually get 3.0.5.

&gt; and `yum install maven` pretty frequently as well.

Out of the box this won't do anything. You have to set up a repo manually. Done using a [common repo](http://stackoverflow.com/a/26115127) gives version 3.2.5. To the best of my knowledge, there is no official repo.

As a last point, I don't think the defaults much matter.
</comment><comment author="dakrone" created="2015-08-26T19:01:12Z" id="135140325">Also, `brew`, `apt-get`, `yum`, or `dnf` installing **anything** Java-related is not necessarily great for production systems. Remember when install java on older Linux systems used to symlink `java` to `gcj`? :) It's the whole reason for the `JAVA_HOME` confusion. I don't think we should rely on what package managers are providing.

Regardless, the whole point is that if someone does `apt-get install maven` and gets a _bad_ version of Maven that we will prevent them from building ES with a version that doesn't work, so I still think this is necessary.
</comment><comment author="jasontedor" created="2015-08-26T19:01:49Z" id="135140519">&gt; Regardless, the whole point is that if someone does `apt-get install maven` and gets a bad version of Maven that we will prevent them from building ES with a version that doesn't work, so I still think this is necessary.

+1
</comment><comment author="dadoonet" created="2015-08-26T19:28:47Z" id="135146478">I'm not totally sure that contributors will appreciate having to change their maven version unless they work **only** on elasticsearch which I doubt.

Again, I'm totally +1 to limit the lower value to at least `3.1.0`. I'm -1 to limit the upper value.
If you do it, then please exclude explicitly 3.3.0 to 3.3.3. But not 3.4 or 3.3.4 please.

And what is wrong with 3.3? Only QA multinode module fails if you run it from root dir. 
Contributors who really really want to run the full tests can run until we fix the issue we have seen:

``` sh
mvn clean install -pl !smoke-test-multinode
mvn clean install -pl smoke-test-multinode
```

My cents.
</comment><comment author="nik9000" created="2015-08-26T19:38:53Z" id="135148720">&gt; As a last point, I don't think the defaults much matter.

Given your points above about ubuntu and rhel's default versions I think we can get away with just failing the build on Maven 3.3. I think defaults matter very much as they are the most likely thing to be on a new contributor's laptop. But if 3.0.5 and 3.2.5 are what you normally end up with in deb and rpm land that'll cover at least half of the folks and we aren't leaving too many people having to downgrade.
</comment><comment author="jasontedor" created="2015-08-26T20:22:36Z" id="135159745">&gt;  I think defaults matter very much as they are the most likely thing to be on a new contributor's laptop. 

To add another data point, as recently as OS X 10.8, Macs shipped with Maven 3.0.3 out of the box, no install required (although you did have to accept the Java 6 download).

The defaults vary wildly from platform to platform which is why they should only play a small role in determining which versions we support. 
</comment><comment author="rmuir" created="2015-09-01T10:29:44Z" id="136667497">This is a blocker. Being lenient here is not helpful. Asking to widen the support beyond what works already is irrelevant. If its a pain in the ass to get the versions we support that is irrelevant, it is what it is. 

When I see stuff like https://discuss.elastic.co/t/error-building-rpm-against-2-0-branch/, then I know we need this PR like, yesterday.

Repeat after me: we will start making software that works, we will start making software that works...
</comment><comment author="jasontedor" created="2015-09-01T11:41:51Z" id="136684686">Removing the [blocker](https://github.com/elastic/elasticsearch/labels/blocker) label since this has been integrated into master and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce supported Maven versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13022</link><project id="" key="" /><description>We should enforce specific Maven versions that our builds are known to be compatible with. Currently we are known to have issues with the Maven 3.0.x line (during RPM packaging) and the Maven 3.3.x line (in the shaded smoke test). We will enforce a Maven version in [3.1.0,3.3.0) and consider these to be our supported Maven versions.
</description><key id="102226611">13022</key><summary>Enforce supported Maven versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v2.0.0-beta2</label></labels><created>2015-08-20T20:11:28Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-09-01T10:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Put a limit on type name length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13021</link><project id="" key="" /><description>Index names already have a character limit of 255 characters or less. This should also be applied to types.

``` http
PUT /my-index/type-0airn7cyol1jfddmrzax5xzw8nue8qkcetnhux2apco8rpi1iztwwq1vp65skvnolmal7ruoabkqtl6xurr0gwivkpmpu6kmmarjeugijzvfugv8ly0nbtleytjeruistrtmthmqyiuvrtwwzzt8gburbpiu1jpqprktjf7swesbjjenxnmjmypygwktgifyo6gigbv8wgkjained5pbikd5whybj22tas60rncmefbwb4xuampjvxbo3lpndlsoy8AFSADtsywxgkxawdfsl9maue1nfqysepuokgrybvhskhwn/1
{
  "field" : "value"
}
```

I cannot think of a legitimate use case for obscenely large type names and using them as keys is a mistake. If a user were to actually want this, then they can add a `not_analyzed` string field (perhaps `keyword` field in the future!) to support this behavior without abusing the type mappings.
</description><key id="102220697">13021</key><summary>Put a limit on type name length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Index APIs</label><label>discuss</label></labels><created>2015-08-20T19:43:20Z</created><updated>2015-08-25T13:58:34Z</updated><resolved>2015-08-21T13:19:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-20T19:50:07Z" id="133151862">+1
</comment><comment author="s1monw" created="2015-08-20T19:50:42Z" id="133151981">`throw new InsanityException("what do you expect?")`
</comment><comment author="rjernst" created="2015-08-20T19:50:55Z" id="133152022">There was a PR last year some time to limit lots of names in length and characters. @dakrone what ever happened to that?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for address resolving in InetSocketTransportAddress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13020</link><project id="" key="" /><description>this commit removes all support for reverse host name resolving from
InetSocketTransportAddress. This class now only returns IP addresses.

Closes #13014
</description><key id="102214085">13020</key><summary>Remove support for address resolving in InetSocketTransportAddress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-20T19:18:40Z</created><updated>2015-08-24T13:15:09Z</updated><resolved>2015-08-20T21:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-20T19:27:34Z" id="133139839">+1 for this. 
</comment><comment author="s1monw" created="2015-08-20T19:44:02Z" id="133150396">pushed a new commit @rmuir 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make multicast zen discovery a plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13019</link><project id="" key="" /><description>Followup to #12999 which explains the rationale.
</description><key id="102188962">13019</key><summary>make multicast zen discovery a plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Discovery Multicast</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-08-20T17:28:55Z</created><updated>2015-08-24T13:23:00Z</updated><resolved>2015-08-21T06:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>adds grammer correction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13018</link><project id="" key="" /><description>adds you
</description><key id="102186836">13018</key><summary>adds grammer correction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dylan8902</reporter><labels><label>docs</label></labels><created>2015-08-20T17:18:24Z</created><updated>2015-08-24T12:59:04Z</updated><resolved>2015-08-24T12:59:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Using enabled:false on document type throws exception in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13017</link><project id="" key="" /><description>In integration tests, using `enabled: false` on a root document type makes document indexing fails.

Here is the mapping I use:

```
"my_doc_type": {
  "enabled": false
}
```

But then indexing a document throws a MapperParsingException with the following stack:

``` java
MapperParsingException[failed to parse]; nested: AssertionError;
    at __randomizedtesting.SeedInfo.seed([1B10B6999E316CD7:4AED1D00C8EC9444]:0)
    at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:155)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:317)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:313)
    ...
```

The assertion is located in the `DocumentParser` class:

``` java
            // try to parse the next token, this should be null if the object is ended properly
            // but will throw a JSON exception if the extra tokens is not valid JSON (this will be handled by the catch)
            if (Version.indexCreated(indexSettings).onOrAfter(Version.V_2_0_0_beta1)
                &amp;&amp; source.parser() == null &amp;&amp; parser != null) {
                // only check for end of tokens if we created the parser here
                token = parser.nextToken();
                assert token == null; // double check, in tests, that we didn't end parsing early
            }
```

When assertion are disabled, everything is OK. I did a simple test to reproduce the issue here:
https://github.com/tlrx/elasticsearch/commit/94712121dd524c48f2dc40c787efd1eb6375d799

This assertion has been added recently and I don't know much about this part of code, maybe @rjernst can help here?
</description><key id="102171560">13017</key><summary>Using enabled:false on document type throws exception in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-20T15:54:58Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-08-30T19:14:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-20T16:29:23Z" id="133067020">This seems like a bug maybe? why are we not consuming the token there?
</comment><comment author="tlrx" created="2015-08-20T16:39:42Z" id="133070901">Looks like it has been introduced by #11414
</comment><comment author="rjernst" created="2015-08-20T17:16:58Z" id="133080276">I think you misunderstood. The assertion code is correct, it means there was leftover stuff at the end of parsing. But _why_ is there leftover stuff in this case. what is the document you are sending, I only see the mapping in the original description? My hunch is, we need to fully consume the parser for mappings that are disabled.
</comment><comment author="tlrx" created="2015-08-20T17:51:03Z" id="133096765">What I don't get is why I can create this mapping and index documents in elasticsearch but cannot do exactly the same thing in integration tests.

Here is the document I index: https://github.com/tlrx/elasticsearch/commit/94712121dd524c48f2dc40c787efd1eb6375d799#diff-82784da2187574da5398ae215c8a6795R276

This test fails on my computer and I don't get why.
</comment><comment author="rjernst" created="2015-08-20T17:57:41Z" id="133098493">Assertions are enabled in tests, but not when running with bin/elasticsearch.
</comment><comment author="rmuir" created="2015-08-20T18:00:50Z" id="133099329">Why isn't this a real check
</comment><comment author="tlrx" created="2015-08-20T18:03:11Z" id="133099891">@rjernst Yes I know, but the test I linked to seems good to me and I don't see why this assertion throws up. Can you please have a quick look to the code and/or try to reproduce?
</comment><comment author="clintongormley" created="2015-08-24T12:27:35Z" id="134171816">this is a bug.  The following fails with assertions enabled:

```
PUT my_index
{
  "mappings": {
    "my_type": {
      "enabled": false
    }
  }
}

PUT my_index/my_type/1
{
  "foo": "bar"
}
```

The whole type is disabled, which means the _source should be stored but nothing should be indexed.  It looks like we're just skipping parsing completely instead of consuming the tokens.
</comment><comment author="rjernst" created="2015-08-24T16:25:16Z" id="134287940">I have a test reproducing the issue and am investigating.
</comment><comment author="tlrx" created="2015-08-26T08:11:25Z" id="134892860">@rjernst Thanks for the work you have done :) Unfortunately I reopen this bug because indexing a document (like @clinton suggested) now fails with a `NullPointerException` (I tested on latest snapshot - build e8834cc78c13a507e2851908f8d51489e7888570).

As far as I understand the code it seems that your fix skips the document parsing when the type is disabled. So field mappers like `UidFieldMapper` are not used, resulting in a null `uid` thrown [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java#L515). 

I'm not familiar with this part of the code but if you don't have time to look further please let me know and I'll try to have a look.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing support for escape to QueryStringQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13016</link><project id="" key="" /><description>QueryStringQueryParser parses `escape`, but java api users had no chance to actually set it.
</description><key id="102151642">13016</key><summary>Add missing support for escape to QueryStringQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-20T14:27:35Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-21T13:11:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-08-20T15:08:26Z" id="133043750">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move LocalTransport under `src/test` </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13015</link><project id="" key="" /><description>I am not sure why we have this sitting here in production code. We should be able to disable clustering without this etc. I think this stuff belongs under `src/test`
</description><key id="102151343">13015</key><summary>Move LocalTransport under `src/test` </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-20T14:26:04Z</created><updated>2017-02-14T14:31:47Z</updated><resolved>2017-02-14T14:31:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-20T17:06:20Z" id="133077952">+1
</comment><comment author="jpountz" created="2015-08-26T20:19:16Z" id="135159024">+1
</comment><comment author="s1monw" created="2015-08-27T07:24:44Z" id="135319516">I think the transport part is harder than expected since I think for the embedded case where you don't want anything to connect to ES you need this kind of transport. Yet, the discovery part I think should just go into src/test or maybe even should be deleted altogether. It's there today to speed things up but it would be awesome if we could make our default (zen) fast enough instead.
</comment><comment author="nik9000" created="2017-02-14T14:31:43Z" id="279722283">I believe @s1monw got to this when he replaced `LocalTransport` with `MockTransport`.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for address resolving in InetSocketTransportAddress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13014</link><project id="" key="" /><description>We shouldn't do any hostname resolving inside ES even though it's disabled by default we should't do it at all. it's simpler and more reliable to just use IPs. This will essentially remove `network.address.serialization.resolve` which is false by default.

this is a followup from #12999 
</description><key id="102149828">13014</key><summary>Remove support for address resolving in InetSocketTransportAddress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-20T14:17:44Z</created><updated>2015-09-16T13:33:17Z</updated><resolved>2015-08-20T21:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kenden" created="2015-08-25T16:55:42Z" id="134669397">Hi @s1monw,
Could you explain why we shouldn't do any hostname resolving inside ES?
What if we use for example AWS Route53 to point to the IP of an Elasticsearch cluster?
(Ex: "myaddress.myserver. -&gt; 10.0.10.51)
If the IP address is changed to, say 10.0.10.59, AWS clients will still be trying to access the old IP address and get a NoNodeAvailableException.
</comment><comment author="s1monw" created="2015-08-25T18:37:20Z" id="134696236">&gt; Could you explain why we shouldn't do any hostname resolving inside ES?
&gt; What if we use for example AWS Route53 to point to the IP of an Elasticsearch cluster?
&gt; (Ex: "myaddress.myserver. -&gt; 10.0.10.51)
&gt; If the IP address is changed to, say 10.0.10.59, AWS clients will still be trying to access the old IP address and get a NoNodeAvailableException.

sorry I don't understand what you are referring to. This change disables the potential reverse lookup for IP -&gt; hostname that we do to render nodestats etc. I don't think this will affect transport clients?
</comment><comment author="kenden" created="2015-08-25T21:47:23Z" id="134751301">But don't we use InetSocketTransportAddress to set up a TransportClient?
https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html
Sorry if I'm getting this wrong.  This seems related to this issue:
https://github.com/elastic/elasticsearch/issues/10337
</comment><comment author="s1monw" created="2015-08-27T21:14:24Z" id="135556564">&gt; But don't we use InetSocketTransportAddress to set up a TransportClient?

so to be clear the change here is what is default in previous version of elasticsearch. We never lookup the hostname from the IP. for the transport client you should be all fine here but nodes in the cluster might be different. the node will likely drop out of the cluster and then rejoin with the new IP? to enable that host name resolution you had to enable an undocumented setting so I don't think it's really changing anything for you?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] enforce maven version for rpm module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13013</link><project id="" key="" /><description>When RPM module is supposed to be built, we need to have at least maven 3.2.3 to build the package.

Otherwise the finalName we get is `elasticsearch-2.0.0-beta1_SNAPSHOT20150820131523.noarch.rpm` instead of `elasticsearch-2.0.0-beta1-SNAPSHOT.rpm`.
</description><key id="102140120">13013</key><summary>[build] enforce maven version for rpm module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>review</label></labels><created>2015-08-20T13:35:31Z</created><updated>2015-09-03T08:25:48Z</updated><resolved>2015-09-01T11:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-20T14:34:31Z" id="133031616">sounds good to me? what do others think? I guess this should be a blocker for 2.0.0-beta1?
</comment><comment author="dadoonet" created="2015-08-20T14:45:05Z" id="133037044">Well. Not really a blocker as the workaround is to simply update your maven version.
Just a safeguard...
</comment><comment author="rjernst" created="2015-08-20T17:08:19Z" id="133078376">+1
</comment><comment author="nik9000" created="2015-08-20T18:15:28Z" id="133105359">Lgtm. I don't see this as a big problem for the 2.0beta release.
On Aug 20, 2015 10:08 AM, "Ryan Ernst" notifications@github.com wrote:

&gt; +1
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13013#issuecomment-133078376
&gt; .
</comment><comment author="jasontedor" created="2015-08-20T20:46:59Z" id="133165819">I think that we should do this but I left some comments. One is very minor, but I think that we should specify a version range up to but exclusive of 3.3.0.
</comment><comment author="dadoonet" created="2015-08-20T21:18:19Z" id="133178064">AFAIK the issue with 3.3 is only in QA when running it from root dir. No issue when you skip the shaded distribution module. 
Not sure we need to exclude 3.3 Maven users (like me).
</comment><comment author="rmuir" created="2015-08-20T21:37:33Z" id="133185995">Why must it be 3.2.3? 3.2.1 works fine.
</comment><comment author="rmuir" created="2015-08-20T22:01:14Z" id="133191038">Here is the way i see it, lets not put the version check here. Lets figure out what we support in our build (whether thats an exact version or a range) and just put it in /pom.xml, and fail otherwise so there are no surprises or bugs.
</comment><comment author="dadoonet" created="2015-08-20T22:18:05Z" id="133196139">&gt; Why must it be 3.2.3? 3.2.1 works fine.

Because we did not check all versions from 3.0.3 to see exactly where it was fixed.
We saw that 3.2.3 (minimal version we tested) works fine so we ended up with that version.

Happy to hear that 3.2.1 works fine. I guess from 3.2 it's fine. I'll update to 3.2.1 though.

&gt; Here is the way i see it, lets not put the version check here. Lets figure out what we support in our build (whether thats an exact version or a range) and just put it in /pom.xml, and fail otherwise so there are no surprises or bugs.

If we know in advance that if you want to build the RPM package using 3.0.x will fail, then why not just failing at the very beginning? This PR only concerns people who have a `rpm` machine and people who installed `rpm` with `brew` on a MacOS. For all others, using maven 3.0 just works fine.

I would not put a high limit, so people can freely test 3.4 when out. 

Is that what you meant @rmuir? Or are you `-1` for this PR (even for the low limit)?
</comment><comment author="rmuir" created="2015-08-20T22:20:05Z" id="133197058">&gt; If we know in advance that if you want to build the RPM package using 3.0.x will fail, then why not just failing at the very beginning? This PR only concerns people who have a rpm machine and people who installed rpm with brew on a MacOS. For all others, using maven 3.0 just works fine.

I just mean, we have other problems in the build like stuff with shading doesnt work correctly. We cannot support _all_ maven versions, I think we should pick some very small range [3.2.3-3.3), support that, and fail hard and clear on everything else. Otherwise results could be surprising.
</comment><comment author="rmuir" created="2015-08-20T22:37:35Z" id="133203575">I edited mine to 3.2.3, whatever value it needs to be. But we have to have some minimal value, so we can pay attention that we don't use any stuff introduced after it, and we know 3.+ does not work at the moment. just like java versions and what not
</comment><comment author="dadoonet" created="2015-08-20T22:51:27Z" id="133205344">Note that I created this enforcer check only within the rpm module. It's not a global one whatever module a developer would like to build.

I mean that limiting issues in modules which are affected by maven bugs (or supposed maven bugs) is fine.
I can add for example a check within qa/shaded module not to run specifically maven 3.3. to 3.3.3 which we know are buggy. But on the other end, you can still run the tests with maven 3.3.3 by doing `cd qa; mvn install`. And I would really like to be able to test maven 3.4 when out without changing any `pom.xml` file. 

I think that external contributors could become annoyed if we don't allow them to contribute a patch on core or a plugin because they are running maven 3.3 although mvn install will just work fine...
And yes, I think that most contributors won't run all the maven modules (the qa and distribution ones will be skipped most likely). 

I started to invest some time trying to reproduce the "issue" with maven 3.3 on a sample tiny project so I could then open an issue in Maven but for now I did not succeed in reproducing the issue. Which makes me think that we may be doing something wrong. And we should fix that instead of giving a constraint on maven 3.3.
</comment><comment author="jasontedor" created="2015-08-21T00:35:33Z" id="133227457">&gt; Note that I created this enforcer check only within the rpm module. 

I missed that this was only on the RPM module so I've reopened #13023 (we can always re-close it).

&gt; It's not a global one whatever module a developer would like to build.

I'm with @rmuir here. We can not support all versions of Maven. We have "incompatibility" issues currently with the 3.0.x line and the 3.3.x line. Thus #13023.

&gt; I can add for example a check within qa/shaded module not to run specifically maven 3.3. to 3.3.3 which we know are buggy. But on the other end, you can still run the tests with maven 3.3.3 by doing cd qa; mvn install.

There should exist versions of Maven that we support that we know that we can run `mvn verify` on the root and have it succeed on all modules. Any other versions should be considered unsupported and enforced in the POM.

&gt; And I would really like to be able to test maven 3.4 when out without changing any pom.xml file.

You can add `-Denforcer.skip=true` to your Maven command parameters.

&gt; I think that external contributors could become annoyed if we don't allow them to contribute a patch on core or a plugin because they are running maven 3.3 although mvn install will just work fine...

Our community and our community contributions are immensely important. But there are platforms / tools / language versions etc. that we do not support. That's okay.

&gt; And we should fix that instead of giving a constraint on maven 3.3.

But until we fix it (or find a bug in Maven) we should not support development with the impacted versions of Maven (whether or not the issue is on our side or the Maven side).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>optimize one shard failed - caused by org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13012</link><project id="" key="" /><description>Hello.

Im running ES v1.7.1 on ubuntu 14.04 servers. I write data to an alias for "today's index" (storm-YYYY-MM-DD). I have 24 nodes, and today's index have 24 shards and 0 replicas. At midnight I update my aliases, and issue a "curator optimize --max_num_segments 1" on "yesterday's index" and enable 3 replicas. I update "yesterday's index" _settings with setting "index.store.throttle.type" to none before starting the optimize (running at the night - almost no searches going).

When I call the optimize I see a long list of segments (last night 1127 segments). When it finished I noticed an exception in my master node's log and shard0 was not optimized. The other 23 shards, 1-23, had exactly 1 segment. To check the list of segments i call "_cat/segments/$YESTERDAY_INDEX?pretty&amp;v". This list was long and ugly before the optimize. Afterwards, it only displayed shard 1-23 (shard 0 is absent). When I call the same API a few seconds later, shard0 i present with it's previous 45ish segment count.

```
[DEBUG][action.admin.indices.optimize] [s01-master-01] [storm-2015-08-18][0], node[hsfByw1LT46uvyHKjImkiA], [P], s[STARTED]: failed to execute [OptimizeRequest{maxNumSegments=1, onlyExpungeDeletes=false, flush=true, force=false}]
org.elasticsearch.transport.RemoteTransportException: [l06-data-02][inet[/192.168.1.19:9302]][indices:admin/optimize[s]]
Caused by: org.elasticsearch.index.engine.OptimizeFailedEngineException: [storm-2015-08-18][0] force merge failed
        at org.elasticsearch.index.engine.InternalEngine.forceMerge(InternalEngine.java:829)
        at org.elasticsearch.index.shard.IndexShard.optimize(IndexShard.java:734)
        at org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:110)
        at org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:49)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:338)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:324)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: background merge hit exception: _fm(4.10.4):C7401808 _7z6(4.10.4):c7051806 _96x(4.10.4):c7006519 _9kz(4.10.4):c7033966 _6t7(4.10.4):c7038676 _4r0(4.10.4):c7040730 _8do(4.10.4):c7012751 _3v7(4.10.4):c7061041 _16c(4.10.4):C6950686 _9li(4.10.4):c263395 _9ky(4.10.4):c231955 _9j2(4.10.4):c187371 _9lg(4.10.4):c137054 _9jt(4.10.4):c89542 _9lh(4.10.4):c86325 _9j3(4.10.4):c59804 _9kq(4.10.4):c34919 _9jy(4.10.4):c33735 _9lj(4.10.4):c32217 _9ka(4.10.4):c31557 _9js(4.10.4):c18749 _9ke(4.10.4):c17671 _9lb(4.10.4):c14342 _9la(4.10.4):c13163 _9l5(4.10.4):c12237 _9l4(4.10.4):c11045 _9le(4.10.4):c10496 _9l8(4.10.4):c6735 _9l2(4.10.4):c6319 _9kv(4.10.4):c40 into _9lk [maxNumSegments=1]
        at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1829)
        at org.elasticsearch.index.engine.InternalEngine.forceMerge(InternalEngine.java:817)
        ... 10 more
Caused by: java.lang.IllegalStateException: class org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards
        at org.apache.lucene.store.ChecksumIndexInput.seek(ChecksumIndexInput.java:49)
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader$ChunkIterator.next(CompressingStoredFieldsReader.java:442)
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.merge(CompressingStoredFieldsWriter.java:368)
        at org.apache.lucene.index.SegmentMerger.mergeFields(SegmentMerger.java:332)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:100)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4223)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3811)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:409)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:486)
```
</description><key id="102078816">13012</key><summary>optimize one shard failed - caused by org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">kjelle</reporter><labels><label>:Core</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-08-20T07:24:53Z</created><updated>2016-06-21T11:54:06Z</updated><resolved>2016-01-28T12:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-08-24T13:05:04Z" id="134193701">This is a spooky exception; I haven't seen it before.  It means that when reading from a segment in order to merged stored fields, we somehow attempted to seek backwards, which should never happen because merging only iterates through the docs "in order" 0 .. maxDoc.

I've committed more verbosity to this particular exception message in Lucene so we'll know "how far backwards" in the future, but for today:

It might be a Lucene bug, maybe a corner case somehow (the code in this area is quite complex, but I can't see any bug after staring at it for a while)...

It could be a JVM bug: which Java version are you using on the node with the affected shard?

It could also be as yet undetected index corruption: can you run CheckIndex on this shard?  Use index.shard.check_on_startup and restart that node: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html

Or, can you upload the shard somewhere where I can download?
</comment><comment author="kjelle" created="2015-08-25T08:18:16Z" id="134518885">To allow the cluster to function properly I excluded that node from allocation so no data resides on the RAID0 for this node atm.

I coincidentally checked dmesg on the server and it seemed to be a disk problem for the RAID0 partition used by _l06-data-02_ (sdc1) at the same time as the Exception appeared.

```
[47263.954825] sd 0:2:2:0: [sdc]
[47263.954833] Result: hostbyte=DID_ERROR driverbyte=DRIVER_OK
[47263.954835] sd 0:2:2:0: [sdc] CDB:
[47263.954837] Read(16): 88 00 00 00 00 00 7a c0 00 7a 00 00 00 08 00 00
[47263.954847] end_request: I/O error, dev sdc, sector 2059403386
[47263.960688] EXT4-fs error (device sdc1): ext4_wait_block_bitmap:476: comm java: Cannot read block bitmap - block_group = 7867, block_bitmap = 257425419
[47264.010809] sd 0:2:2:0: [sdc]
[47264.010823] Result: hostbyte=DID_ERROR driverbyte=DRIVER_OK
[47264.010826] sd 0:2:2:0: [sdc] CDB:
[47264.010828] Read(16): 88 00 00 00 00 00 7a c0 00 82 00 00 00 08 00 00
[47264.010838] end_request: I/O error, dev sdc, sector 2059403394
[47264.016754] EXT4-fs error (device sdc1): ext4_wait_block_bitmap:476: comm java: Cannot read block bitmap - block_group = 7868, block_bitmap = 257425420
```

Why it tries to seek backwards I cannot say.

~~But - after this Exception, and moving the shards off this node, I have begun to experience other Exceptions which I cannot resolve (google, forums, github issues). See [#13095]~~
</comment><comment author="kjelle" created="2015-08-26T07:21:45Z" id="134874429">@mikemccand: We have reconfigured the entire cluster. We are now running 24 RAID0's and assign 6 partitions to each ES node (4 ES nodes per server). Waiting to see if/when the bug comes back.
</comment><comment author="jpountz" created="2015-09-27T22:04:54Z" id="143597874">@kjelle Have you seen the error happen again?
</comment><comment author="clintongormley" created="2016-01-28T12:52:00Z" id="176166747">No more feedback. Closing
</comment><comment author="Leryan" created="2016-06-21T11:37:07Z" id="227414291">Hi,

Running on NFS can trigger this error. You must add `nolock` to the fstab mountpoint.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong in 'search-suggesters-completion' doc?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13011</link><project id="" key="" /><description>Link: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html

The `length` in response should be 1, not 4?

```
curl -X POST 'localhost:9200/music/_suggest?pretty' -d '{
    "song-suggest" : {
        "text" : "n",
        "completion" : {
            "field" : "suggest"
        }
    }
}'

{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "song-suggest" : [ {
    "text" : "n",
    "offset" : 0,
    "length" : 4,
    "options" : [ {
      "text" : "Nirvana - Nevermind",
      "score" : 34.0, "payload" : {"artistId":2321}
    } ]
  } ]
}
```
</description><key id="102066827">13011</key><summary>Wrong in 'search-suggesters-completion' doc?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thihy</reporter><labels><label>docs</label></labels><created>2015-08-20T05:39:43Z</created><updated>2015-08-24T11:33:22Z</updated><resolved>2015-08-24T11:33:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve java version comparison and explicitly enforce a version format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13010</link><project id="" key="" /><description>This pull request addresses two issues #12441 and #13009.

The first issue relates to improving the method of Java version comparison in `JarHell#checkVersion`. 

The first improvement here is to add explicit verification of the version format to be of the form specified in the [Java Product Versioning documentation](https://docs.oracle.com/javase/8/docs/technotes/guides/versioning/spec/versioning2.html). Specifically:

&gt; Specification version numbers use a Dewey decimal notation consisting of numbers seperated by periods.

This format is also specified in the documentation for [`java.lang.Package#getSpecificationVersion`](https://docs.oracle.com/javase/8/docs/api/java/lang/Package.html#getSpecificationVersion--):

&gt; This version string must be a sequence of nonnegative decimal integers separated by "."'s and may have leading zeros. When version strings are compared the most significant numbers are compared.

The second improvement here is to improve the Java version comparison in `JarHell#checkJavaVersion`. We can rely on the [`java.lang.Package#isCompatibleWith`](https://docs.oracle.com/javase/8/docs/api/java/lang/Package.html#isCompatibleWith-java.lang.String-) method as this uses the `java.specification.version` from the JVM and performs the lexicographic comparison on the above version format for us. Moreover, by utilizing this method we are safe against all possible Java specification versions whereas the current method can only parse those of the form `[0-9](.[0-9]+)?`.

The second issue relates to explicitly enforcing a version format on plugins. Because of the version check in `JarHell#checkJavaVersion` we implicitly require the above version format. We should be explicit about this and add it to the documentation.

Closes #12441, #13009.
</description><key id="102044301">13010</key><summary>Improve java version comparison and explicitly enforce a version format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-20T02:37:11Z</created><updated>2015-08-21T13:48:00Z</updated><resolved>2015-08-21T12:30:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-20T02:52:50Z" id="132865015">I added some comments. I really like the idea of this change, I'm not a good reviewer on whether the "java.lang".isCompatibleWith is good here, only because... man is java version confusing, and I'm heads down on another PR. 

I do know the comparator used is ok, we used the one from harmony for a long time in lucene. If we test this change with java 9 I am happy.
</comment><comment author="jasontedor" created="2015-08-20T02:58:15Z" id="132866988">Thanks for reviewing @rmuir!

I will test against Java 9. I totally get the potential for issues there because of the Jigsaw changes. I'll let you know what I find and be sure to sync up with you before taking this to master.
</comment><comment author="rmuir" created="2015-08-20T03:01:23Z" id="132867705">My concern is not jigsaw actually, more this one: http://openjdk.java.net/jeps/223

And I haven't kept up, I'm not sure what if anything is committed or what. See thread `RFR: JDK-8085822 JEP 223: New Version-String Scheme (initial integration)`
</comment><comment author="jasontedor" created="2015-08-20T03:03:19Z" id="132868432">Okay, thanks for pointing me to that one! I'll catch myself up on that JEP and see where it is and the impact here.
</comment><comment author="rmuir" created="2015-08-20T03:11:59Z" id="132871155">Yeah my concerns extend beyond your change, we should double-check the logic for jar manifests (see /pom.xml manifest stuff for the jar plugin) too.
</comment><comment author="jasontedor" created="2015-08-21T01:14:09Z" id="133234676">[Jigsaw](http://openjdk.java.net/projects/jigsaw/) does in fact present a problem causing the logic here to fail. The underlying issue is the [removal of the system jar rt.jar](http://openjdk.java.net/jeps/220) in favor of implementation-specific libraries. As such, the system classes no longer have an associated manifest so the trick of loading the system package (from `Runtime.class`) to utilize `Package#isCompatibleWith` will not work post Jigsaw.

Instead, we're going to have load the system property "java.specification.version" (as you were doing) and then compare lexicographically point by point with the target version. As such, I've implemented `JavaVersion` to encapsulate this parsing and comparison logic. I've pushed a new [commit](https://github.com/jasontedor/elasticsearch/commit/8a3930976c29bcf08fefb6d32f9765389998e777) with the necessary changes.

Related, I looked at the JEP-223 proposal. Here's a summary of my findings regarding [JEP 223](http://openjdk.java.net/jeps/223) and its impact on the version checking logic discussed here:
- the proposal is to establish a revised version-string scheme for JDK versions
- the proposal has a lexical format similar to the current versioning scheme with different semantics yet version comparison can still be performed as done today
- in particular, the proposal defines _version numbers_ as strings that consists of a sequence of non-negative integers **without** leading or trailing zeros (matches `^[1-9][0-9]*(((\.0)*\.[1-9][0-9]*)*)*$`); this is similar to the format used today with the removal of allowing leading zeros
- the major semantical difference is that the JDK major version number will now occupy the leading position in the version number (i.e., 9 instead of 1.9).
- the proposal explicitly states that "existing code that compares version numbers by parsing their elements and comparing them numerically will continue to work"
- consequently, the proposal as currently written implies that the approach outlined in this pull request will continue to work
- if JEP-223 is accepted into mainline JDK 9, there will be a `Version` class that implements the necessary parsing and comparison logic and we will be able to replace the above `JavaVersion` class with the JDK 9 `Version` class.

I've tested this pull request with the updated [commit](https://github.com/jasontedor/elasticsearch/commit/8a3930976c29bcf08fefb6d32f9765389998e777) on current Java 7u80 and Java 8u60 GA releases, current mainline JDK 9b77, and a custom build of the JDK 9 JEP-223 branch and all are good.
</comment><comment author="rmuir" created="2015-08-21T03:36:28Z" id="133268207">Thanks for the investigation! it looks good to me. I added a few questions, but its just minor stuff.
</comment><comment author="jasontedor" created="2015-08-21T12:32:26Z" id="133410833">Thanks a lot for your helpful comments and thoughts on this @rmuir!
</comment><comment author="rmuir" created="2015-08-21T13:48:00Z" id="133429096">Thank you for removing this leniency!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce version format for java.version for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13009</link><project id="" key="" /><description>Currently we implicitly enforce a version format on the `java.version` property for plugins via `JarHell.checkJavaVersion`. We should explicitly enforce this version format and specify it in the documentation.
</description><key id="102042512">13009</key><summary>Enforce version format for java.version for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-20T02:25:24Z</created><updated>2015-08-21T12:31:05Z</updated><resolved>2015-08-21T12:31:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>documentation error in snapshots docs - repo.path should be path.repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13008</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html

repo.path should be path.repo
</description><key id="102035570">13008</key><summary>documentation error in snapshots docs - repo.path should be path.repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">sarwarbhuiyan</reporter><labels><label>:Snapshot/Restore</label><label>docs</label><label>v1.6.0</label><label>v1.7.0</label><label>v2.0.0-beta2</label></labels><created>2015-08-20T01:20:06Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-08-20T01:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add packages to the 'Use xyz instead' comments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13007</link><project id="" key="" /><description>This makes it easier to see how to fix your mistake without having to dig/guess where those utilities may be coming from.
</description><key id="102023926">13007</key><summary>Add packages to the 'Use xyz instead' comments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>build</label><label>v2.0.0-rc1</label></labels><created>2015-08-19T23:38:52Z</created><updated>2016-03-10T18:13:19Z</updated><resolved>2015-09-22T15:00:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-20T02:49:55Z" id="132863271">LGTM
</comment><comment author="jpountz" created="2015-08-20T07:19:01Z" id="132915728">LGTM
</comment><comment author="nik9000" created="2015-08-20T19:31:05Z" id="133142100">Lgtm too!
On Aug 20, 2015 12:19 AM, "Adrien Grand" notifications@github.com wrote:

&gt; LGTM
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13007#issuecomment-132915728
&gt; .
</comment><comment author="nik9000" created="2015-09-22T14:29:19Z" id="142305670">I'm going to merge this for @pickypg because he's busy.
</comment><comment author="nik9000" created="2015-09-22T15:00:32Z" id="142314475">Rebased to resolve conflicts and then merged to master and cherry-picked to 2.x and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some URLs in the reference docs prevent selecting the version of the docs you're viewing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13006</link><project id="" key="" /><description>If you search for "elasticsearch api" on Google, one of the hits you get back is this one:

![screen shot 2015-08-20 at 9 26 47 am](https://cloud.githubusercontent.com/assets/43236/9372068/b94d06e8-471d-11e5-8516-d9819b661987.png)

So I went there: https://www.elastic.co/guide/en/elasticsearch/reference/1.4//index.html

Then I tried to view the docs for 2.0 by changing the combo box. It reloads the page, but doesn't actually go to the 2.0 version of the docs.

This occurs on both Chrome and Firefox.
</description><key id="102022782">13006</key><summary>Some URLs in the reference docs prevent selecting the version of the docs you're viewing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trejkaz</reporter><labels /><created>2015-08-19T23:30:02Z</created><updated>2015-08-24T11:30:19Z</updated><resolved>2015-08-24T11:30:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-20T04:59:53Z" id="132890882">Agreed. Sounds like it's caused by the double `//` before `index.html`.
If you remove it, then you can change the version using the dropdown menu.
</comment><comment author="trejkaz" created="2015-08-20T05:16:07Z" id="132896209">Yeah, figured that out after comparing with a colleague who said the problem wasn't happening for him. Sent him the link - happened for him. Removed the extra slash on mine, started working properly.

Still, the double-slash version is in the Google index now, and perhaps a 404 or a redirect would be a better treatment of it, rather than the page loading but then failing to work correctly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show build hash/timestamp for plugins in verbose mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13005</link><project id="" key="" /><description>Based on @dadoonet's comment in #13004

&gt; Do we want also to use it? May be we could display that info when installing a plugin in verbose mode?
</description><key id="101980512">13005</key><summary>Show build hash/timestamp for plugins in verbose mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Plugins</label><label>low hanging fruit</label></labels><created>2015-08-19T19:39:28Z</created><updated>2016-01-28T17:08:08Z</updated><resolved>2016-01-28T12:51:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T12:51:40Z" id="176166437">Given that our plugins are built and released at the same time as Elasticsearch, I think this is no longer useful.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding build hash and build timestamp to the plugin-descriptor.properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13004</link><project id="" key="" /><description>Adds something along the lines of this to each plugin:

``` properties
# 'hash': the commit hash relative to the code for systems that support it (e.g., git)
hash=f2f95ea115aa3fa2020d034ae4e10d2ef8837d62
#
# 'timestamp': the build time of the plugin
timestamp=1440012506349
```

Closes #13002
</description><key id="101979256">13004</key><summary>Adding build hash and build timestamp to the plugin-descriptor.properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels /><created>2015-08-19T19:31:22Z</created><updated>2015-08-21T09:17:00Z</updated><resolved>2015-08-19T22:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-08-19T19:31:35Z" id="132752564">Should this be backported to `v2.0.0`?
</comment><comment author="dadoonet" created="2015-08-19T19:35:27Z" id="132753358">Looks good. Do we want also to use it? May be we could display that info when installing a plugin in verbose mode?
</comment><comment author="pickypg" created="2015-08-19T19:37:03Z" id="132753668">I'm all for doing it, if we think it will be helpful. It could help to find mistakes for those with a keen eye.
</comment><comment author="rmuir" created="2015-08-19T19:40:48Z" id="132754520">What are we using this for?

For the record this information is recorded in all the actual jar files we produce (for debugging purposes), already, as jar metadata.
</comment><comment author="pickypg" created="2015-08-19T22:36:48Z" id="132811042">Closed in favor of @rmuir's current approach where this data is already written to each jar's manifest. We should update #13005 to pull the info from the manifest.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API to show the number of index fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13003</link><project id="" key="" /><description>I have seen a few issues where having a count of fields across indices would have been helpful to have for troubleshooting indexing performance, OOME, etc. Although this information can be gathered from mappings, it could be useful to show a count of index fields akin to what Lucene's CheckIndex tool displays per segment.

```
Segments file=segments_c numSegments=12 version=4.10.4 format= userData={translog_id=1434119476444, sync_id=AU9BzZeqTGo-78cobmuv}
  1 of 12: name=_v docCount=9873
    version=4.10.4
    codec=Lucene410
    compound=false
    numFiles=12
    size (MB)=16.177
    diagnostics = {os=Mac OS X, java.vendor=Oracle Corporation, java.version=1.8.0_25, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=x86_64, source=merge, mergeFactor=10, os.version=10.10.1, timestamp=1434119509593}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
&gt;&gt;&gt; test: fields..............OK [27 fields] &lt;&lt;&lt;
    test: field norms.........OK [6 fields]
    test: terms, freq, prox...OK [282439 terms; 2040942 terms/docs pairs; 2958473 tokens]
    test: stored fields.......OK [30542 total field count; avg 3.093 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]
```

Does this functionality exist in another API?
</description><key id="101978811">13003</key><summary>API to show the number of index fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">inqueue</reporter><labels><label>discuss</label><label>enhancement</label></labels><created>2015-08-19T19:28:18Z</created><updated>2016-01-28T12:47:42Z</updated><resolved>2016-01-28T12:47:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T10:51:37Z" id="134148227">Hi @inqueue 

Given that this is a one-off (and seldom used) diagnostic which can be answered already via the mapping API, I'm not sure it is worth adding a dedicated API.  Probably something we could add to https://github.com/elastic/elasticsearch-support-diagnostics instead, no?
</comment><comment author="inqueue" created="2015-08-24T15:41:17Z" id="134255746">Hi @clintongormley,

I suppose I am expecting to find a field count somewhere in the _cat API, maybe in _cat/indices or perhaps _cat/segments|shards. I could not find a clean way retrieve this information from mappings without parsing its output, which we can do with es-support-diagnostics.

To gather this information, would you suggest properties or something like `GET */_mapping/field/*`? Other suggestions?

Thank you
</comment><comment author="clintongormley" created="2016-01-28T12:47:42Z" id="176163873">Yes, the field mapping API would be the appropriate one.  Also, we're planning on adding a soft limit on the number of fields you can have an index, which will help here.  See https://github.com/elastic/elasticsearch/issues/11443
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Build info to the plugin properties by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13002</link><project id="" key="" /><description>When debugging, it's often nice to see things like the build time and commit hash of the current build to find accidental differences. With the new plugin properties, we can add that information by default:

``` properties
# 'hash' : the commit hash relative to the code for systems that support it (e.g., git)
hash=${buildNumber}
# 'timestamp' : the build time of the plugin
timestamp=${timestamp}
```

This can be added to the build without requiring any additional properties.
</description><key id="101977824">13002</key><summary>Add Build info to the plugin properties by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Plugins</label><label>build</label><label>low hanging fruit</label><label>v2.1.0</label></labels><created>2015-08-19T19:23:43Z</created><updated>2015-08-19T22:37:10Z</updated><resolved>2015-08-19T22:37:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-08-19T22:37:10Z" id="132811139">Closed in favor of content already existing in the `MANIFEST.MF` file.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more permissions for bouncycastle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13001</link><project id="" key="" /><description>As explained in [bouncy castle doc](http://www.bouncycastle.org/wiki/display/JA1/Using+the+Bouncy+Castle+Provider's+ImplicitlyCA+Facility), we need to have:

```
grant {
   permission java.security.SecurityPermission "putProviderProperty.BC";
   permission java.security.SecurityPermission "insertProvider.BC";
   permission org.bouncycastle.jce.ProviderConfigurationPermission "BC", "ecImplicitlyCA, threadLocalEcImplicitlyCA";
};
```

Mapper attachment plugin needs them to run properly.
</description><key id="101975178">13001</key><summary>Add more permissions for bouncycastle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2015-08-19T19:09:32Z</created><updated>2015-08-25T11:58:56Z</updated><resolved>2015-08-25T11:57:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-19T19:10:05Z" id="132746076">@jaymode Could you review please?
</comment><comment author="jaymode" created="2015-08-19T19:27:14Z" id="132751712">@dadoonet can you see if it works with `threadLocalEcImplicitlyCA` only instead of `ecImplicitlyCA, threadLocalEcImplicitlyCA`? It would be great if we can limit it to the thread local permission instead of the global one.
</comment><comment author="dadoonet" created="2015-08-24T15:17:36Z" id="134245012">@jaymode Thanks for your comment. I did some more testing and something really strange is happening.

I tried this:
- Build elasticsearch 2.1.0-SNAPSHOT without the current patch.
- Update my mapper attachment plugin branch to elasticsearch 2.1.0-SNAPSHOT: worked
- Update my mapper attachment plugin branch to elasticsearch 2.0.0-beta1-SNAPSHOT: failed
- Update my local 2.0.0-beta1-SNAPSHOT with this patch: worked

I was expecting this to fail also in 2.1.0-SNAPSHOT. Does it mean Security Manager is not working in the same way between 2.0.0 and 2.1.0?

BTW I can confirm that I only need to add:

```
  permission java.security.SecurityPermission "insertProvider.BC";
```

to make it work.
</comment><comment author="dadoonet" created="2015-08-25T11:57:50Z" id="134563278">I discussed with @s1monw about this and actually this was only needed for a mapper attachments test so for now we [disable the test](https://github.com/elastic/elasticsearch-mapper-attachments/commit/7dfaef491621c371605b1e0decd05d9136347aae).

Closing this PR.

That said, I'd really like to understand why I found the security manager [behave differently from 2.0 to 2.1](https://github.com/elastic/elasticsearch/pull/13001#issuecomment-134245012). @rmuir @jaymode if you guys have any idea...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Question: Tribe node and search.queue?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13000</link><project id="" key="" /><description>I have a question... Let's say we have 3 tribe nodes, 10 data nodes and # of replicas is 1 for each shard. How tribe nodes balance requests? Or even does it balance requests? Sometimes I see that while one node over loaded by search queries, the other node (which has replica of that shard) just sits and do nothing. Would be using only one tribe node help in this case? Or they are not related at all?

ES v1.4.4
</description><key id="101973026">13000</key><summary>Question: Tribe node and search.queue?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devfacet</reporter><labels /><created>2015-08-19T18:58:17Z</created><updated>2015-08-24T10:48:28Z</updated><resolved>2015-08-24T10:48:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T10:48:28Z" id="134147253">Hi @cmfatih 

Please use the forums for questions like this http://discuss.elastic.co/.  The tribe nodes behave like any other client nodes: they round-robin requests around all copies (primary and replicas) of a shard.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default to unicast discovery, with default host list of 127.0.0.1, [::1]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12999</link><project id="" key="" /><description>Multicast discovery has serious issues such as https://github.com/elastic/elasticsearch/issues/12993
It is never going to work correctly and securely with a "default to localhost" scenario. It also has numerous issues with operating system and JDK support.

Instead we should encourage unicast discovery by default, since thats what we encourage for production, and configure it to look at "localhost" by default which will give the same experience for startup/development.

This PR changes the default unicast host list (if not specified) to be "127.0.0.1, [::1]", enables unicast by default, disables multicast by default, fixes the parsing of unicast hosts to be strict, fixes it to allow IPV6 addresses unambiguously, and fixes it to work with hostnames that resolve to multiple addresses.

Things that should be deferred to followups (trust me, this is a huge change on its own, and i know a bunch of tests are angry, which i need help with, but the change works):
- Move multicast zen discovery out to a plugin, possibly deprecate it.
- Review all uses + ban InetSocketAddress(java.lang.String, int), which is lenient and bad to use. I did some cleanups here already.

If anyone wants to help, let me know I will give commit access to my branch here.
</description><key id="101965710">12999</key><summary>Default to unicast discovery, with default host list of 127.0.0.1, [::1]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Discovery</label><label>blocker</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T18:26:33Z</created><updated>2015-08-24T12:56:29Z</updated><resolved>2015-08-20T18:28:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-19T22:38:31Z" id="132811782">ok we are down to 1 awaits fix and all other tests pass - there is some cleanup work needed here for sure but this is a great start!!
</comment><comment author="s1monw" created="2015-08-20T08:26:32Z" id="132934744">I fixed all the remaining tests. Everything now runs on unicast and passes with network and local. We still have some forbiddenAPI:

```
[ERROR] Forbidden method invocation: java.net.InetSocketAddress#getHostName() [Use getHostString() instead, which avoids a DNS lookup]
[ERROR]   in org.elasticsearch.common.transport.InetSocketTransportAddress (InetSocketTransportAddress.java:96)
[ERROR] Forbidden method invocation: java.net.InetSocketAddress#getHostName() [Use getHostString() instead, which avoids a DNS lookup]
[ERROR]   in org.elasticsearch.common.transport.InetSocketTransportAddress (InetSocketTransportAddress.java:132)
[ERROR] Scanned 5737 (and 1524 related) class file(s) for forbidden API invocations (in 1.90s), 2 error(s).
```

but those are the last remaining issues. We can `SuppressForbidden` them if we wanna push the decision out to a different issue? 
</comment><comment author="s1monw" created="2015-08-20T14:45:25Z" id="133037130">I am +1 on pushing this - it looks good and has alll the fixes needed. We can do all otehr things in folllowups
</comment><comment author="rmuir" created="2015-08-20T14:51:51Z" id="133039025">Summarizing the current tradeoffs (versus multicast) so we are all aware:
- out of box `bin/elasticsearch` will discover nodes on the local machine by default just like multicast did.
- adding "localhost" as a default unicast discovery host has some advantages over multicast: for example it works with ipv6-only configurations.
- however, unlike multicast, as soon as you start messing with configuration (changing port numbers, etc), you are going to need to deal with discovery. in most cases, multicast would continue to work all the way into production, regardless.
</comment><comment author="jaymode" created="2015-08-20T17:06:36Z" id="133077999">+1, this looks good to me
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to set default geo_point value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12998</link><project id="" key="" /><description>Spoke with @nknize about the possibility of setting a default geo_point lat/lon value, and he said it would be possible to add this feature. 
</description><key id="101961534">12998</key><summary>Ability to set default geo_point value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">kurtado</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2015-08-19T18:07:59Z</created><updated>2015-08-24T11:34:08Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-20T06:20:06Z" id="132904825">What do you mean by default? `null_value` like we have for other field types?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix documentation: scrolls are not closed automatically.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12997</link><project id="" key="" /><description>The documentation states that scrolls are automatically closed when all
documents are consumed, but this is not the case. I first tried to fix
the code to close scrolls automatically but this made REST tests fail
because clearing a scroll that is already closed returned a 4xx error
instead of a 2xx code, so this has probably been this way for a very long
time.
</description><key id="101953576">12997</key><summary>Fix documentation: scrolls are not closed automatically.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T17:34:42Z</created><updated>2015-08-21T09:18:47Z</updated><resolved>2015-08-20T07:24:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-08-19T17:36:22Z" id="132710574">I noticed a small typo but otherwise LGTM.
</comment><comment author="jpountz" created="2015-08-19T17:36:43Z" id="132710830">On the contrary to regular scrolls, scan/scrolls are automatically closed, but since they are on the way out, I think it's better to make the documentation document what regular scrolls do.
</comment><comment author="martijnvg" created="2015-08-19T18:13:40Z" id="132727799">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ordering Significant Terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12996</link><project id="" key="" /><description>Hello,

from a discussion on Elastic forum it seems impossible to order results from a Significant Terms aggregation, the 'order' parameter is not handled.

Is it possible to add 'order' as in the Terms aggregation ?

Best regards,
Arnaud
</description><key id="101947951">12996</key><summary>Ordering Significant Terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acassaignemondeca</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-08-19T17:08:50Z</created><updated>2015-08-25T07:07:41Z</updated><resolved>2015-08-24T15:47:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T10:44:04Z" id="134145388">@markharwood could you comment on this?
</comment><comment author="markharwood" created="2015-08-24T14:52:14Z" id="134232400">There are typically a huge number of "candidate" terms produced by significant terms. What determines the final set is the statistical significance of their frequencies. If we introduced another sort order then they would no longer be "significant" terms in the intended sense. 
It is possible to plug-in custom significance heuristics which alter the score but we [deliberately limited the scope](https://github.com/elastic/elasticsearch/issues/10613#issuecomment-93948953) of how they compute scores to just dealing with the supplied frequencies.
</comment><comment author="clintongormley" created="2015-08-24T15:47:09Z" id="134256937">thanks @markharwood  - makes sense. closing
</comment><comment author="acassaignemondeca" created="2015-08-24T16:29:09Z" id="134290396">Thank you for your responses and sorry not to have responded earlier.

Actually I am using Kibana and the Significant Terms table is not sorted by descending number of occurrences. Without changing anything in the result and the way it is computed, my request is to have the result sorted by count of occurrences as a post-treatment step.

Here is my use case:

Kibana Significant Terms table:
![signterms](https://cloud.githubusercontent.com/assets/13748482/9445574/1a65bb2e-4a8d-11e5-9fa1-89e4579f16d2.png)

I see the doc_count field in the ES result, ideally result would be ordered by this field for the 10 result items. May be it is something that should be done at Kibana level.

Arnaud  
</comment><comment author="markharwood" created="2015-08-25T07:07:41Z" id="134505873">This indeed is a [Kibana issue](https://github.com/elastic/kibana/issues/2018)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deduplicate addresses from resolver.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12995</link><project id="" key="" /><description>In some cases this may contain duplicates, although its a misconfiguration,
lets not bind to multiple ports. Its no problem for us to dedup, this code
doesn't need to be huper-duper fast since its used only for logic around bind/publish.

I found references to situations like this and added as link in the code:
https://bugzilla.redhat.com/show_bug.cgi?id=496300

I think @bleskes may know of a machine with that or some similar problem. Lets just be defensive since this is addresses for binding.
</description><key id="101935644">12995</key><summary>Deduplicate addresses from resolver.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T16:19:59Z</created><updated>2015-08-21T09:06:35Z</updated><resolved>2015-08-19T20:17:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-19T19:08:05Z" id="132745642">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate the `scan` search type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12994</link><project id="" key="" /><description>This commit deprecates the `scan` search type in favour of regular scroll
requests sorted by `_doc`.

Related to #12983
</description><key id="101912231">12994</key><summary>Deprecate the `scan` search type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>deprecation</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-19T14:52:49Z</created><updated>2016-04-05T08:51:16Z</updated><resolved>2015-08-20T10:47:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-19T16:34:19Z" id="132687380">LGTM
</comment><comment author="jasontedor" created="2015-08-19T17:16:08Z" id="132699679">LGTM.
</comment><comment author="martijnvg" created="2015-08-19T18:12:46Z" id="132727278">LGTM3
</comment><comment author="dadoonet" created="2016-04-05T08:51:15Z" id="205719757">Sounds like we forgot to update the Java Guide as well: https://discuss.elastic.co/t/scroll-api-for-java-api-2-3-0/46342
I'll send a PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch listens on udp port 54328 on all network interfaces by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12993</link><project id="" key="" /><description>This is unrelated to sending multicast packets "too far" (#12914), which is an OS-X specific bug.

This is the client side bind (See MulticastSocket ctor), and happens on linux.

Unfortunately, because linux `lo0` address does not support multicast, this is the only reason discovery works out of box today. You cant listen on `lo0` only.
</description><key id="101911521">12993</key><summary>elasticsearch listens on udp port 54328 on all network interfaces by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label></labels><created>2015-08-19T14:49:15Z</created><updated>2015-08-20T18:28:40Z</updated><resolved>2015-08-20T18:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Log slow queries as json, not binary.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12992</link><project id="" key="" /><description>In the Java client, you can compose the query either by adding String source chunks or by using another builder, like the Aggregation builder and adding the aggregation to the query. There is a quirk, though: if you use a builder, the only way to contribute it to the query is to first turn it into a byte[]. When you do this, the query ultimately gets logged as half-json, half-binary-string in the slow query log.

That isn't a _problem_, per se, but it's annoying when you're trying to analyze the logs.

I have no opinion about how the query objects are stored in memory, but it would be nice if the slow query log (and full query log if #9172 comes to pass) would log a human readable query.
</description><key id="101907273">12992</key><summary>Log slow queries as json, not binary.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vvcephei</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-08-19T14:29:12Z</created><updated>2016-01-28T12:38:01Z</updated><resolved>2016-01-28T12:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-19T21:51:43Z" id="132799414">This seems like a fair request to me.
</comment><comment author="colings86" created="2016-01-28T12:38:00Z" id="176158451">This is fixed by https://github.com/elastic/elasticsearch/pull/13752 in the master branch. The search source is now stored as an object instead of as bytes and so SearchSlowLog will now print the json search request in the slow log rather than the binary bytes. 

Note that until https://github.com/elastic/elasticsearch/issues/10217 is fully complete there will still be some elements of the search request (e.g. sort parameters) which will be rendered as binary bytes since until these are refactored they are still stored in the Search request as bytes.

Closing in favour of the mentioned PRs/issues
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suppress rest exceptions by default and log them instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12991</link><project id="" key="" /><description>Today we are very verbose when rendering exceptions on the rest layer.
Yet, this isn't necessarily very easy to read and way too much infromation most
of the time. This change suppresses the stacktrace rendering by default but instead
adds a `rest.suppressed` logger that logs the suppressed stacktrace or rather the entire
exception on the node that renderes the exception
The log message looks like this:

```
[2015-08-19 16:21:58,427][INFO ][rest.suppressed          ] /test/_search/ Params: {index=test}
[test] IndexNotFoundException[no such index]
    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:551)
```
</description><key id="101907174">12991</key><summary>Suppress rest exceptions by default and log them instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T14:28:47Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-21T13:18:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-19T14:29:05Z" id="132619030">@jaymode @uboness can you take a look 
</comment><comment author="s1monw" created="2015-08-21T11:02:33Z" id="133373216">@uboness @jaymode can you take another look
</comment><comment author="uboness" created="2015-08-21T11:23:34Z" id="133376460">left one comment
</comment><comment author="drewr" created="2015-10-08T20:53:21Z" id="146681995">These are really painful on `INFO`. Things that aren't really exceptional, like `409`s from a `_create` or `_update` can be quite numerous in normal operation. Should we consider `DEBUG` or `TRACE` for these?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] rpm module should be build on machine with /usr/bin/rpmbuild</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12990</link><project id="" key="" /><description>We build the rpm module automatically if you have either:
- `/usr/bin/rpmbuild`
- `/usr/local/bin/rpmbuild`

available.

If your `rpmbuild` is in another location and available in your path, then run maven with `rpm` profile:

``` sh
mvn deploy -Prpm
```

Closes #12984.
</description><key id="101904867">12990</key><summary>[build] rpm module should be build on machine with /usr/bin/rpmbuild</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T14:19:02Z</created><updated>2015-08-29T21:04:39Z</updated><resolved>2015-08-20T07:46:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-19T21:48:01Z" id="132798684">LGTM. I've bounced into this maven profile activation comes before parents are resolved issue as well. :cry:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw error if cardinality aggregator has sub aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12989</link><project id="" key="" /><description>The cardinality aggregation is a metric aggregation and therefore cannot accept sub-aggregations. It was previously possible to create a rest request with a cardinality aggregation that had sub-aggregations. Now such a request will throw an error in the response.

Close #12988
</description><key id="101894697">12989</key><summary>Throw error if cardinality aggregator has sub aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T13:33:05Z</created><updated>2015-08-21T09:25:10Z</updated><resolved>2015-08-19T13:37:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-19T13:33:53Z" id="132599460">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cardinality Agg wrongly accepts sub aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12988</link><project id="" key="" /><description>On the REST API you can construct a request for a cardinality aggregation which has a sub-aggregation.  This should not be allowed since the Cardinality aggregation is a metrics aggregation.

``` js
POST test/doc/1
{
  "l":1
}

POST test/doc/2
{
  "l":2
}

POST test/doc/3
{
  "l":3
}

POST test/doc/4
{
  "l":4
}

GET test/_search
{
  "size": 0, 
  "aggs": {
    "card": {
      "cardinality": {
        "field": "l"
      },
      "aggs": {
        "terms": {
          "terms": {
            "field": "l",
            "size": 10
          }
        }
      }
    }
  }
}
```

This returns 

```
{
   "took": 5,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 4,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "card": {
         "value": 4
      }
   }
}
```

It should instead throw an error response back.
</description><key id="101893330">12988</key><summary>Cardinality Agg wrongly accepts sub aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T13:26:47Z</created><updated>2015-09-16T13:34:44Z</updated><resolved>2015-08-19T13:37:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Opening an index with a missing analyzer stopword file result in a node unavailability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12987</link><project id="" key="" /><description>Is it ok that opening an index with a missing analyzer stopword file returns {"acknowledged":true} and enter in a loop searching/waiting for the missing file, resulting in a global node unavailability ?
I think it should just return an error, keep the index closed and the node healthy.

Test :

```
    echo "le\nla\nles\n" &gt; /home/quentin/stopwords_fr.txt

    read -d '' POST &lt;&lt;EOF
    {"settings": {
       "analysis":{
         "filter":{
           "stop_francais":{
             "type":"stop",
             "stopwords_path": "/home/quentin/stopwords_fr.txt"
            }
          }
        }
      }
    }
    EOF
    curl -s -XPOST http://localhost:9200/test-crash -d "$POST";echo
```

Then delete file and try to close/open 

```
    rm /home/quentin/stopwords_fr.txt
    curl -XPOST "localhost:9200/test-crash/_close"
    curl -XPOST "localhost:9200/test-crash/_open"
```

Error in the logs

```
Caused by: org.elasticsearch.env.FailedToResolveConfigException: Failed to resolve config path [/home/quentin/stopwords_fr.txt], tried file path [....], and classpath
```
</description><key id="101892585">12987</key><summary>Opening an index with a missing analyzer stopword file result in a node unavailability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">QuentinAmbard</reporter><labels /><created>2015-08-19T13:23:50Z</created><updated>2015-08-24T10:29:44Z</updated><resolved>2015-08-24T10:29:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-24T10:29:44Z" id="134138415">Hi @Avricot 

Agreed - I've added a note to #9126 as i think this should be part of that change.  Closing in favour of #9126
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only resolve host if explicitly allowed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12986</link><project id="" key="" /><description>We have some settings that prevent host name resolution which should
be repected by InetSocketTransportAddress#getHost() to only resolve if
allowed or desired.
</description><key id="101889649">12986</key><summary>Only resolve host if explicitly allowed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>regression</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T13:12:41Z</created><updated>2015-08-24T10:30:28Z</updated><resolved>2015-08-19T13:25:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-19T13:13:47Z" id="132587511">looks good.
</comment><comment author="jaymode" created="2015-08-19T13:14:17Z" id="132587731">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor script for RC creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12985</link><project id="" key="" /><description>The script now allows to run all required steps at once and alternatively
prints out manual instructions to run the steps individually. It also has
flags and options to run debug builds from a local checkout.

@spinscale can you take a look
</description><key id="101888767">12985</key><summary>Refactor script for RC creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T13:07:47Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-21T12:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-19T14:37:05Z" id="132622171">great work! Left one minor thing to have working repos with incremental releases! We can almost ditch `build_release.py` as well
</comment><comment author="s1monw" created="2015-08-21T06:18:03Z" id="133300869">@spinscale can you take another look
</comment><comment author="s1monw" created="2015-08-21T11:14:12Z" id="133374619">@spinscale I addressed all your comments!
</comment><comment author="spinscale" created="2015-08-21T12:32:56Z" id="133410916">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.7 and 2.0 missing snapshot rpm packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12984</link><project id="" key="" /><description>1.7 and 2.0 branches are missing the rpm snapshot packages.
</description><key id="101884364">12984</key><summary>1.7 and 2.0 missing snapshot rpm packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electrical</reporter><labels /><created>2015-08-19T12:44:44Z</created><updated>2015-08-20T07:46:36Z</updated><resolved>2015-08-20T07:46:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-19T13:10:20Z" id="132585973">can you provide example URLs for this please? It's not clear which misses what, thanks
</comment><comment author="electrical" created="2015-08-19T13:14:58Z" id="132588166">https://s3-eu-west-1.amazonaws.com/build.eu-west-1.elastic.co/origin/1.7/nightly/JDK7/elasticsearch-latest-SNAPSHOT.deb exists for example for the 1.7 branch but there is no rpm equivalent.
For 2.0 branch

https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/origin/2.0/nightly/JDK7/elasticsearch-latest-SNAPSHOT.noarch.rpm is empty because there rpm is not generated and the symlink we create in jenkins generates an empty file then.
</comment><comment author="dadoonet" created="2015-08-19T13:23:44Z" id="132594466">It's a bug. Maven profiles are executed first then inner properties are evaluated.

We defined:

``` xml
    &lt;properties&gt;
                &lt;packaging.rpm.rpmbuild&gt;/usr/bin/rpmbuild&lt;/packaging.rpm.rpmbuild&gt;
    &lt;/properties&gt;
```

And 

``` xml
    &lt;profiles&gt;
        &lt;profile&gt;
            &lt;id&gt;rpm&lt;/id&gt;
            &lt;activation&gt;
                &lt;file&gt;
                    &lt;!-- Folks having /usr/bin/rpmbuild available will be able to build the rpm module --&gt;
                    &lt;exists&gt;${packaging.rpm.rpmbuild}&lt;/exists&gt;
                &lt;/file&gt;
            &lt;/activation&gt;
            &lt;modules&gt;
                &lt;module&gt;rpm&lt;/module&gt;
            &lt;/modules&gt;
        &lt;/profile&gt;
    &lt;/profiles&gt;
```

When the `profile` part is evaluated, `packaging.rpm.rpmbuild` has not been set yet.
So we end up to evaluate `&lt;exists&gt;&lt;/exists&gt;`.

That's why this profile is never executed, and the rpm module is not added to the reactor.

I'll come with a fix.

In the meantime, running `mvn deploy -Dpackaging.rpm.rpmbuild=/usr/bin/rpmbuild` or `mvn deploy -Prpm` might solve the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize sorted scroll when sorting by `_doc`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12983</link><project id="" key="" /><description>This change means that we will be able to remove the `SCAN` search type in 3.0
and recommend users to use sorted scrolls instead.
</description><key id="101872357">12983</key><summary>Optimize sorted scroll when sorting by `_doc`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scroll</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-19T11:42:22Z</created><updated>2016-02-15T16:47:23Z</updated><resolved>2015-08-20T10:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-19T16:26:55Z" id="132681959">LGTM
</comment><comment author="martijnvg" created="2015-08-19T18:09:57Z" id="132725830">This looks great! Left some minor comments and a question, but otherwise LGTM2
</comment><comment author="clintongormley" created="2015-08-24T12:09:11Z" id="134167578">@jpountz please can you document this change
</comment><comment author="clintongormley" created="2015-08-24T12:10:28Z" id="134168102">ie there is no mention of sorting by `_doc` in the sorting documentation
</comment><comment author="jpountz" created="2015-08-24T12:15:10Z" id="134169635">I added some in the PR that deprecates scan (#12994). See https://github.com/elastic/elasticsearch/pull/12994/files#diff-a9ef1c25ab00ab4680de0b901976e2cfR93 for instance.
</comment><comment author="jpountz" created="2015-08-24T13:47:42Z" id="134207251">OK I understand what you mean now, there is no single mention of `_doc` in the section about sorting. I pushed the following commit https://github.com/elastic/elasticsearch/commit/7b878b5b5c67e5684dd58e398dc0a30f4bfbd241 
</comment><comment author="OsamaAbbas" created="2016-02-07T18:33:46Z" id="181074944">Sorry for replying to this old page.

I'm trying to use regular `scroll` and sort in `_doc` order. However, using `scan` search type is much faster. I've tried both, in two different languages (javascript and python).

Here is a snippet in Node.js:

``` javascript
es.client.search({
  index: 'library',
  type: 'page',
  scroll: '2m',
  sort: '_doc',
  search_type: 'scan',
  body: {"query":{"filtered":{"filter":{"term":{"book_id":1681}}}}}
}, ...);
```

And in Python:

``` python
es.search(index='library',
    doc_type='page',
    scroll='2m',
    sort='_doc',
    search_type='scan',
    body='{"query":{"filtered":{"filter":{"term":{"book_id":1681}}}}}')
```

By removing `search_type`, it takes three to four times longer to get all documents using subsequent `.scroll` calls.
</comment><comment author="jpountz" created="2016-02-07T21:09:38Z" id="181118167">How many shards do you have? I'm asking because `search_type='scan'` retrieves `$size * $num_shards` docs per page, so the following would be a better comparison (assuming 5 shards):

``` python
es.search(index='library',
    doc_type='page',
    scroll='2m',
    search_type='scan',
    size=10,
    body='{"query":{"term":{"book_id":1681}}}')
```

vs.

``` python
es.search(index='library',
    doc_type='page',
    scroll='2m',
    size=50,
    body='{"query":{"term":{"book_id":1681}},"sort":["_doc"]}')
```
</comment><comment author="OsamaAbbas" created="2016-02-07T21:33:10Z" id="181122187">Thank you! It worked. I guess you'd better add this notice to documentation.
</comment><comment author="l15k4" created="2016-02-15T16:42:17Z" id="184292174">@jpountz what happens if sorting by `_doc` is not specified ?

```
es.search(index='library',
    doc_type='page',
    scroll='2m',
    size=50,
    body='{"query":{"term":{"book_id":1681}}}')
``
```
</comment><comment author="jpountz" created="2016-02-15T16:47:23Z" id="184295019">This will return documents sorted by _score. The benefit of sonting by `_doc` is that elasticsearch can efficiently skip to the next matching document when moving to the next page (it will simply ignore all docs that have a smaller doc id than the last retured document). When sorting by another criterion, elasticearch will need to visit all matching documents in order to make sure it does not miss any match.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Propagate Headers and Context through to ScriptService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12982</link><project id="" key="" /><description>At the moment if an index script is used in a request, the spawned request to get the indexed script from the `.scripts` index does not get the headers and context copied to it from the original request. This change makes the calls to the `ScriptService` pass in a `HasContextAndHeaders` object that can provide the headers and context. For the `search()` method the context and headers are retrieved from `SearchContext.current()`.

Closes #12891
</description><key id="101843568">12982</key><summary>Propagate Headers and Context through to ScriptService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-19T09:15:36Z</created><updated>2015-11-22T10:15:21Z</updated><resolved>2015-09-02T12:50:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-20T11:08:21Z" id="132975678">can we add tests to `ContextAndHeaderTransportIT` for testing this?
</comment><comment author="colings86" created="2015-08-21T09:10:37Z" id="133344260">@spinscale I added a couple of tests to check that the headers and context are propagated for the TemplateQuery and for the reduce phase of aggregations.
</comment><comment author="spinscale" created="2015-08-31T10:58:05Z" id="136335169">overall this looks good to me, but
- I couldnt run `mvn test` due to forbidden API errors, is it me?
- Can we add a test for the phrase suggesters as it has been changed?
- Thinking if we can reuse the `DelegatingHasContextAndHeaders` in our `SearchContext` (might be tricky with the inheritance though)
</comment><comment author="colings86" created="2015-09-01T10:40:17Z" id="136669545">@spinscale I have implemented the suggested changes and rebased onto master (without squashing commits), are you able to take another look?
</comment><comment author="spinscale" created="2015-09-01T15:50:13Z" id="136767089">left one last comment, I am not sure if you can remove all the methods (having the asserts in the percolate context seems useful to me), but maybe you can remove some code...

apart from that LGTM
</comment><comment author="colings86" created="2015-09-02T12:30:10Z" id="137057546">I was able to remove the methods from FilteredSearchContext but I kept them in PercolateContext as I think the asserts are important to have.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add serialization support for InterruptedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12981</link><project id="" key="" /><description>it's an important exception to serialize and we see it often in tests
etc. but then it's wrapped in NotSerializableExceptionWrapper which is
odd. This commit adds native support for this exception.
</description><key id="101834100">12981</key><summary>Add serialization support for InterruptedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T08:24:38Z</created><updated>2015-08-24T10:24:44Z</updated><resolved>2015-08-19T09:26:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-19T08:56:44Z" id="132498798">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge pull request #1 from elastic/master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12980</link><project id="" key="" /><description>Merging to head
</description><key id="101814556">12980</key><summary>Merge pull request #1 from elastic/master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsohil</reporter><labels /><created>2015-08-19T06:04:20Z</created><updated>2015-08-19T08:13:33Z</updated><resolved>2015-08-19T08:13:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-19T08:13:33Z" id="132486073">Probably open by mistake?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log network configuration at debug level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12979</link><project id="" key="" /><description>In 2.0, a lot of users are probably going to have to deal with this to get stuff to work, since we only bind to loopback by default, they are going to have to configure interfaces, deal with addresses, and so on.

I hate loggers :) but if things go wrong, it would be good to have an easy way to debug what java sees on the system, similar to running `ifconfig -a`:

```
2015-08-19 00:25:40,933][DEBUG][org.elasticsearch.common.network] configuration:

lo
        Software Loopback Interface 1
        inet 127.0.0.1 netmask:255.0.0.0 broadcast:127.255.255.255 scope:host
        inet6 0:0:0:0:0:0:0:1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:-1 index:1

eth0
        Microsoft Kernel Debug Network Adapter
        MULTICAST mtu:-1 index:2

eth1
        Intel(R) PRO/1000 MT Desktop Adapter
        inet 10.0.2.15 netmask:255.255.255.0 broadcast:10.0.2.255 scope:site
        inet6 fe80:0:0:0:e5ff:af5:8ad0:d2d1%eth1 prefixlen:64 scope:link
        hardware 08:00:27:65:7C:DF
        UP MULTICAST mtu:1500 index:3

net0
        Microsoft ISATAP Adapter
        inet6 fe80:0:0:0:0:5efe:a00:20f%net0 prefixlen:128 scope:link
        hardware 00:00:00:00:00:00:00:E0
        POINTOPOINT mtu:1280 index:4

net1
        Teredo Tunneling Pseudo-Interface
        inet6 2001:0:5ef5:79fd:1048:33e8:b7ac:c1f9 prefixlen:0
        inet6 fe80:0:0:0:1048:33e8:b7ac:c1f9%net1 prefixlen:32 scope:link
        hardware 00:00:00:00:00:00:00:E0
        UP POINTOPOINT mtu:1280 index:5
...
```
</description><key id="101813001">12979</key><summary>Log network configuration at debug level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T05:51:52Z</created><updated>2015-08-24T10:25:44Z</updated><resolved>2015-08-19T11:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-19T07:36:43Z" id="132478215">huge +1 LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `cloud.account` and `cloud.key` settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12978</link><project id="" key="" /><description>Looks like we are going to remove it, put a quick PR

closes #12809
</description><key id="101795750">12978</key><summary>Remove `cloud.account` and `cloud.key` settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud AWS</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-19T02:55:39Z</created><updated>2015-08-24T13:36:27Z</updated><resolved>2015-08-21T08:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-19T06:31:27Z" id="132461888">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add millisecond parser for dynamic date fields mapped from "yyyy/MM/dd"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12977</link><project id="" key="" /><description>Dynamic date fields mapped from dates of the form "yyyy-MM-dd"
automatically receive the millisecond paresr epoch_millis as an
alternative parsing format. However, dynamic date fields mapped from
dates of the form "yyyy/MM/dd" do not. This is a bug since the migration
documentation currently specifies that a dynamically added date field,
by default, includes the epoch_millis format. This commit adds
epoch_millis as an alternative parser to dynamic date fields mapped from
dates of the form "yyyy/MM/dd".

Closes #12873
</description><key id="101794338">12977</key><summary>Add millisecond parser for dynamic date fields mapped from "yyyy/MM/dd"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Dates</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T02:40:51Z</created><updated>2015-11-03T04:11:20Z</updated><resolved>2015-08-19T09:30:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-19T05:11:54Z" id="132449169">LGTM
</comment><comment author="RicherdLee" created="2015-11-03T04:11:20Z" id="153234577">I use 1.7.2
I set mapping like this
"mappings": {
        "enlink_query_record": {
            "properties": {
                "ordertime": {
                    "type": "date",
                    "format": "yyyy/MM/dd HH:mm:ss"
                }
            }
        }
    }

when i syn data,the ordertime still display
![image](https://cloud.githubusercontent.com/assets/8063459/10901091/b80af8bc-8223-11e5-9354-8ae654d90297.png)
I'm not sure whether it's millisecond parser for dynamic date fields problem&#12290;
shoud id change 2.0.0
                                             Thinks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve console logging on startup exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12976</link><project id="" key="" /><description>Today we show the exception twice: once by the logger and then again
by the JVM. This is too noisy, and easy to avoid.
</description><key id="101793066">12976</key><summary>Improve console logging on startup exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Exceptions</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-19T02:29:52Z</created><updated>2015-08-24T10:22:01Z</updated><resolved>2015-08-19T03:25:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-19T02:36:23Z" id="132425909">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add /_cat/clusters for tribe nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12975</link><project id="" key="" /><description>This is an enhancement request.  It would be nice to have something like a /_cat/clusters API for tribe nodes to return the list of clusters the tribe node know about.  Also good to know those that are connected and healthy vs those that are not connected/not healthy.

I am currently using /_cat/nodes?h='h,i,n' and look in node name for node name that has a '/' in it to figure out the tribe cluster it is in.

For example: 'lva1-app1234.prod_tribe/galene-tribe_ela4' mean that the tribe put it in the galene-tribe_ela4 cluster.
</description><key id="101785665">12975</key><summary>Add /_cat/clusters for tribe nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TinLe</reporter><labels><label>:Tribe Node</label><label>discuss</label><label>enhancement</label></labels><created>2015-08-19T01:30:36Z</created><updated>2017-05-05T14:41:50Z</updated><resolved>2017-05-05T14:41:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-19T01:32:31Z" id="132410828">Great idea!
</comment><comment author="javanna" created="2017-05-05T14:41:50Z" id="299483397">We did something similar (although not a cat api) for the newly introduced cross cluster search functionality (which will replace the tribe node): https://github.com/elastic/elasticsearch/issues/23925 . </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Add query profiler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12974</link><project id="" key="" /><description>Only about a year late, here is the followup to #6699. :)  

This PR adds a query profiler to time the various components of a query.  This PR differs from #6699 mainly in implementation details (and superficially, some of the response syntax).  
#### How the old PR worked

The old method basically walked the query tree after it was processed and wrapped everything in a special `ProfileQuery`.  This class then delegated to the wrapped query/filter and timed the execution.

This approach was problematic for one main reason:  the query-walking dispatcher needed many special cases, since special-snowflake queries introduced edge cases that needed handling.  This meant that any time queries were altered in ES/Lucene, the walker would likely need to be updated.

Another problem was how timings were stored:  each `ProfileQuery` maintained it's own "local" timing.  When the query was finished, timings had to be recursively merged upwards from the leaf nodes to find a total time, then merged back down to derive a relative time.  This whole process required a second "profile walker" which would traverse the profiled query, calculate the timings and spit out a tree of Profiled components.

Finally, it made book-keeping very tricky due to rewrites.  Some rewrites will change the query structure and you end up with "dangling" ProfileQueries that were no longer in the tree.  Some optimizations in Lucene, such as collapsing multiple boolean queries into a single bool, could really mess up the tree.
#### How the new PR works

The new method basically injects logic in the ContextIndexSearcher and overrides a few key methods (rewrite, createWeight, createNormalizedWeight).  If profiling is enabled, weights are wrapped in a `ProfileWeight`, which then further wraps scorers in a `ProfileScorer`.  

Timings are then stored in a centralized, thread-local `InternalProfiler`, which also maintains a dependency graph.  Conveniently, createWeight() is called once per node in tree, so we can use that to maintain a stack of tree depth and generate the dependency graph on the fly, instead of pre-walking the entire thing.

This is generally less invasive and more tolerant to rewrite changes (weights are generated after the rewrite is finished, so all of our wrapped weights/scores are done post-rewrite).  The downside is that profiling logic is now baked into ContextIndexSearcher and toggled with a flag.  We looked into wrapping the searcher with a ProfileIndexSearcher, but the current architecture won't allow that to work for technical reasons.  So the current approach works, but isn't entirely non-invasive...definitely room for improvement.
#### Syntax

Sample query:

``` json
GET /test/test/_search
{
   "profile": true,
   "query": {
      "bool": {
         "should": [
            {
               "match": {
                  "abc": "xyz"
               }
            },
            {
               "match": {
                  "abc": "abc"
               }
            },
            {
               "match": {
                  "abc": "123"
               }
            }
         ]
      }
   }
}
```

And sample response (truncated):

``` json
{
   "hits": {...},
   "profile": {
      "query": {
         "shards": [
            {
               "shard_id": "o73tK_SGR9GDofdy9zg0Gw",
               "timings": [
                  {
                     "query_type": "BooleanQuery",
                     "lucene": "+(abc:xyz abc:abc abc:123) #ConstantScore(_type:test)",
                     "time": "40.78110900ms",
                     "relative_time": "100.0000000%",
                     "breakdown": {
                        "rewrite": 69067,
                        "weight": 20363221,
                        "score": 279524,
                        "cost": 0,
                        "normalize": 153166,
                        "build_scorer": 19916131
                     },
                     "children": [
                        {
                           "query_type": "BooleanQuery",
                           "lucene": "abc:xyz abc:abc abc:123",
                           "time": "30.67931900ms",
                           "relative_time": "75.22924156%",
                           "breakdown": {
                              "rewrite": 0,
                              "weight": 13726549,
                              "score": 92738,
                              "cost": 0,
                              "normalize": 58733,
                              "build_scorer": 16801299
                           },
                           "children": [
                              {
                                 "query_type": "TermQuery",
                                 "lucene": "abc:xyz",
                                 "time": "14.07877100ms",
                                 "relative_time": "34.52277622%",
                                 "breakdown": {
                                    "rewrite": 0,
                                    "weight": 11665755,
                                    "score": 0,
                                    "cost": 0,
                                    "normalize": 21939,
                                    "build_scorer": 2391077
                                 },
                                 "children": []
                              },
                              {
                                 "query_type": "TermQuery",
                                 "lucene": "abc:abc",
                                 "time": "3.957625000ms",
                                 "relative_time": "9.704554626%",
                                 "breakdown": {
                                    "rewrite": 0,
                                    "weight": 409593,
                                    "score": 45739,
                                    "cost": 40510,
                                    "normalize": 4256,
                                    "build_scorer": 3457527
                                 },
                                 "children": []
                              },
                              {
                                 "query_type": "TermQuery",
                                 "lucene": "abc:123",
                                 "time": "0.3979560000ms",
                                 "relative_time": "0.9758341785%",
                                 "breakdown": {
                                    "rewrite": 0,
                                    "weight": 197993,
                                    "score": 4931,
                                    "cost": 12881,
                                    "normalize": 3858,
                                    "build_scorer": 178293
                                 },
                                 "children": []
                              }
                           ]
                        },
           ...
}
```
#### Known issues
- I haven't run the full test suite yet, so this may break some stuff :s
- Does not work with DFS_query_then_fetch, since this adds logic to directly ContextIndexSearcher instead of wrapping
- Untested against nested / parent-child.
- A few locations where the profile results are serialized may need version checks?  They are annotated with `//nocommit`
- I wasn't entirely sure what utility methods to include for consumers of the Java API.  Currently, the ProfileResults interface provides a way to get a map ( Shard -&gt; ProfileResults), an EntrySet and a Collection.  This was all I really needed to build the tests, but I'm open to suggestions for more user-friendly API
- The profiled times are in nanoseconds...should we change this to milliseconds? 

/cc @jpountz 

Edit:
![itshappening](https://cloud.githubusercontent.com/assets/1224228/9344854/c62c54f8-45d9-11e5-8ee3-a597052a413f.gif)
</description><key id="101766418">12974</key><summary>Search: Add query profiler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Search</label><label>feature</label></labels><created>2015-08-18T22:48:29Z</created><updated>2015-08-24T13:53:10Z</updated><resolved>2015-08-21T14:30:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-19T06:34:23Z" id="132462186">Thank you so much Zach!
</comment><comment author="jpountz" created="2015-08-19T09:16:54Z" id="132504730">I think the way this PR does the wrapping is indeed more robust than the previous PR. Regarding the DFS issue, we have had a similar issue in other pull requests which boils down to the fact that IndexSearcher is hard (impossible?) to wrap correctly, so maybe it would need some refactoring...

We should try to explore profiling collectors, which are a common source of slowness eg. if you use heavy aggregations. Profiling the reduce phase would be nice too but I don't think it's required for the first iteration?

Maybe we should move this PR to a public branch to make it easier to iterate on?
</comment><comment author="polyfractal" created="2015-08-21T14:30:49Z" id="133443549">For those that want to follow along, a [public branch has been pushed.](https://github.com/elastic/elasticsearch/tree/query_profiler).

Closing this PR, we'll re-open a new one once the shared branch is ready to go (third time's a charm) :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove CachedDfSource</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12973</link><project id="" key="" /><description>and move the dfs logic to the ContextIndexSearcher.

This PR relates to #12864
</description><key id="101750380">12973</key><summary>Remove CachedDfSource</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-18T21:13:01Z</created><updated>2015-08-24T10:46:33Z</updated><resolved>2015-08-19T18:54:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-19T08:20:16Z" id="132488194">LGTM, just left a minor suggestion
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop commons-lang dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12972</link><project id="" key="" /><description>commons-lang really is only used by some core classes to join strings or modiy arrays.
It's not worth carrying the dependency. This commit removes the dependency on commons-lang
entirely.
</description><key id="101734699">12972</key><summary>Drop commons-lang dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T19:47:46Z</created><updated>2015-08-25T14:00:14Z</updated><resolved>2015-08-18T21:01:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-18T19:56:57Z" id="132332504">LGTM
</comment><comment author="martijnvg" created="2015-08-18T20:05:31Z" id="132334384">LGTM2
</comment><comment author="rmuir" created="2015-08-18T20:10:22Z" id="132335409">this looks great
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cryptic error message when mis-spelling a field in geo-distance aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12971</link><project id="" key="" /><description>issue-id - 12391
issue-url - https://github.com/elastic/elasticsearch/issues/12391
# Problem statement:

when sometimes the fields in the Geo distance aggregator query is misspelt., it
results in throwing out an error message. But is is highly crytic
# Approach:

The error messages at present in GeoDistanceParser which is responsible
for parsing the fields within an aggregator of type geo_distance has error message like this

``` java

throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "]."

```

in the above it relies on "token" variable which holds the Parser util's representation of what
1) close bracket is
2) open bracket is
3) array start bracket is
4) field is
5) field value is

and so on ....

as it is a variable that is used by the util for its own understanding, it is highly cryptic.
So it is not advicable to use this.
Instead directly point to "fieldname" whose parsing resulted in error (here it is in variable "currentFieldName")
and to aggregator name so that people can use it to find in which aggregator the field lies (in varible "aggregationName")

so remove token and use aggregationName and currentFieldName to narrow down on the location of error.
# Results:

new message

---

   Failed when parsing field with name 'orgin' found inside aggregation with name 'per_ring'. Few Possible Causes for the error:1)Misspelling the field name.  2)Giving a field name which is not recognized by Geo Distance Aggregator.3)Assigning the field to a value with unallowed object type.

old message

---

   Unexpected token START_OBJECT in [per_ring].
# Files Changed:

modified:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java

here made the error message more readable.
The same error message is applicable at a number of places so made it into a function.
# TODO for FUTURE :

The same type of problem exists in most of the parsers that we have in elasticsearch.
so make this fix common one to all and make the message to be constructed at one common place
</description><key id="101732105">12971</key><summary>Cryptic error message when mis-spelling a field in geo-distance aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HarishAtGitHub</reporter><labels /><created>2015-08-18T19:33:11Z</created><updated>2015-08-26T13:13:07Z</updated><resolved>2015-08-26T12:34:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-26T12:34:50Z" id="134985927">Hi @HarishAtGitHub 

Thanks for working on this, but I think I prefer the simpler solution in https://github.com/elastic/elasticsearch/pull/13033.  It is less invasive, and doesn't change the exception text from "Unknown key" to "Unexpected token" for all of the other exceptions.

I think we'll just go ahead and merge #13033 instead. Thank you for contributing and don't be discouraged.   
</comment><comment author="HarishAtGitHub" created="2015-08-26T13:12:39Z" id="135009617">Hi @clintongormley , Np, these are learnings (Simpler is better). I can take it . Thanks ....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Workaround JDK bug 8034057</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12970</link><project id="" key="" /><description>See https://bugs.openjdk.java.net/browse/JDK-8034057, where two filesystem methods are broken for SUBST'd drive letters (this is basically where you map a local drive to another letter, like c:\foo -&gt; d:).

These bugs won't be fixed until java 9, but its a configuration some people use on windows.
</description><key id="101732061">12970</key><summary>Workaround JDK bug 8034057</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>jvm bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T19:32:56Z</created><updated>2015-08-25T14:00:48Z</updated><resolved>2015-08-18T21:05:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-18T19:46:10Z" id="132330082">left some minor comments LGTM otherwise
</comment><comment author="rjernst" created="2015-08-18T19:50:15Z" id="132331073">LGTM too
</comment><comment author="s1monw" created="2015-08-18T19:51:12Z" id="132331320">LGTM
</comment><comment author="rmuir" created="2015-08-18T19:57:00Z" id="132332518">This is pretty ugly code, but the bug is even uglier, ES won't start at all, plugin installation won't work, etc.

So I don't know what versions we want this in, someone other than me decide.

FYI I also sent an email to nio-dev, asking about the real fix...
</comment><comment author="s1monw" created="2015-08-18T19:57:44Z" id="132332655">I am ok with this going into 2.0
</comment><comment author="rmuir" created="2015-08-18T21:05:16Z" id="132350555">Fixed in https://github.com/elastic/elasticsearch/commit/e07f0396598d322a53efb1c8a7d2abd46f2d05ea
</comment><comment author="pickypg" created="2015-08-18T22:23:34Z" id="132374313">JDK backport link: https://bugs.openjdk.java.net/browse/JDK-8133891
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>detect_noop on index api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12969</link><project id="" key="" /><description>Currently you can send the same data over and over again to the index api and the document will be reindexed even if it hasn't changed. Maybe we should not reindex it? We're already looking to default detect_noop to true for the update api (#11282) so this isn't that big a change conceptually.
</description><key id="101724987">12969</key><summary>detect_noop on index api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>breaking</label><label>discuss</label></labels><created>2015-08-18T18:50:59Z</created><updated>2015-08-24T15:52:26Z</updated><resolved>2015-08-24T15:45:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-24T14:27:21Z" id="134225563">I think the trade-off is more complicated than on the update API, since the index API never needs to fetch the source which is a costly operation (random seek) while the update API needs to do it anyway in order to patch the document.
</comment><comment author="clintongormley" created="2015-08-24T15:45:10Z" id="134256442">I agree with @jpountz on this.  If you want this functionality you should use the update API instead
</comment><comment author="nik9000" created="2015-08-24T15:52:26Z" id="134258497">&gt; I think the trade-off is more complicated than on the update API, since the index API never needs to fetch the source which is a costly operation (random seek) while the update API needs to do it anyway in order to patch the document.

Fair enough.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix verbose logging on plugin install and tidy up log message.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12968</link><project id="" key="" /><description>Closes https://github.com/elastic/elasticsearch/issues/12907
</description><key id="101716966">12968</key><summary>Fix verbose logging on plugin install and tidy up log message.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">jimhooker2002</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label></labels><created>2015-08-18T18:12:46Z</created><updated>2015-09-19T11:09:57Z</updated><resolved>2015-09-15T13:07:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimhooker2002" created="2015-08-21T07:03:44Z" id="133311599">Also added the fix for https://github.com/elastic/elasticsearch/issues/4665 to this pull request as well.
</comment><comment author="dadoonet" created="2015-08-21T07:10:55Z" id="133312425">Why not 2 PR instead of one? I think they are not related, right?
</comment><comment author="jimhooker2002" created="2015-08-21T07:15:22Z" id="133312976">Correct, not related. Happy to submit two separate if that's how it's done.  Sorry for the mess - new to Github and associated workflows.
</comment><comment author="dadoonet" created="2015-08-21T07:29:35Z" id="133315184">:) No problem! 

Yes please remove the commit 66fe490 from this branch and force push it again.

Add the commit in another branch and send a new PR with it so we can review both.
</comment><comment author="jimhooker2002" created="2015-08-21T18:33:24Z" id="133525562">https://github.com/elastic/elasticsearch/commit/66fe490d98a8bb5f15ba595b692100df674d4cae now removed from this pull request - will submit separately.  This pull request should now be clean and ready.
</comment><comment author="dadoonet" created="2015-09-15T13:07:42Z" id="140381975">Was actually already fixed in 2.x (59571749a7e1a544a12a86d919c122285eea76b1) and master (7d6acde) branches with another commit. I just cherry picked for 2.0: 4f7c6a9
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure additionalSettings() do not conflict</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12967</link><project id="" key="" /><description>Plugins can preovide additional settings to be added to the settings
provided in elasticsearch.yml. However, if two different plugins supply
the same setting key, the last one to be loaded wins, which is
indeterminate. This change enforces plugins cannot have conflicting
settings, at startup time. As a followup, we should do this when
installing plugins as well, to give earlier errors when two plugins
collide.
</description><key id="101706289">12967</key><summary>Ensure additionalSettings() do not conflict</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T17:21:39Z</created><updated>2015-08-25T13:59:36Z</updated><resolved>2015-08-18T17:36:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-08-18T17:30:29Z" id="132289298">I like it! left a small comment, OTT LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: deprecate _name and boost in short variants of queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12966</link><project id="" key="" /><description>As discussed in #11744 this is the last step to unify parsing of boost and _name. Those fields are supported only in long version of queries, while we sometimes parse them when wwe shouldn't, inconsistently.

Closes #11744
</description><key id="101700806">12966</key><summary>Query DSL: deprecate _name and boost in short variants of queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-18T16:55:36Z</created><updated>2015-11-22T10:15:21Z</updated><resolved>2015-08-27T10:25:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-19T09:18:14Z" id="132504952">LGTM
</comment><comment author="javanna" created="2015-08-19T10:34:02Z" id="132533847">Turns out that using `ParseField` against `_name` causes problems, since we internally convert it to its camelCase variant, which becomes `name` and then the term query cannot query anymore a field called `name`, which is not good.

I made a little adjustment to `Strings#toCamelCase` so that if the first character is `_` it leaves it untouched e.g. `_name` stays as-is while `_invalid_name` becomes `_invalidName`. To be honest I am a bit afraid of what this change might break... other opinions are more than welcome.
</comment><comment author="jpountz" created="2015-08-21T10:02:40Z" id="133361838">Can you add unit tests for toCamelCase given the changes you made?
</comment><comment author="javanna" created="2015-08-21T10:47:41Z" id="133371487">&gt; Can you add unit tests for toCamelCase given the changes you made?

done! are we sure this change is ok? I am a bit afraid of the consequences it might have, that we may not realize at the moment. 
</comment><comment author="jpountz" created="2015-08-24T13:02:06Z" id="134191143">This might only have bad impact on camel case, which I'm not too scared to break as none of our examples promotes it and it is on the way out (https://github.com/elastic/elasticsearch/issues/8988).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fail with better error message if elasticsearch was started already</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12965</link><project id="" key="" /><description>Before it failed with
.../es/qa/smoke-test-plugins/target/integ-tests/es.pid doesn't exist
which was confusing.
</description><key id="101699729">12965</key><summary>fail with better error message if elasticsearch was started already</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T16:52:01Z</created><updated>2015-08-21T09:35:36Z</updated><resolved>2015-08-19T15:56:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-18T17:12:55Z" id="132283020">Left only wording comments. It looks good to me.
</comment><comment author="brwe" created="2015-08-19T10:05:53Z" id="132524109">addressed all comments
</comment><comment author="dadoonet" created="2015-08-19T10:46:56Z" id="132536640">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] simplify ant script for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12964</link><project id="" key="" /><description>Now we are using short names for artifactId (see #12879) so we don't need anymore to transform long names `elasticsearch-pluginname` to short names `pluginname` in ant script when we install a plugin.
</description><key id="101693989">12964</key><summary>[build] simplify ant script for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-18T16:22:52Z</created><updated>2015-08-18T17:43:55Z</updated><resolved>2015-08-18T17:43:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-18T16:25:10Z" id="132266096">We should also nuke the elasticsearch- stripping in `convert-plugin-name`
</comment><comment author="dadoonet" created="2015-08-18T16:25:46Z" id="132266226">ha! Missed that one... Adding a new commit soonish...
</comment><comment author="rmuir" created="2015-08-18T16:27:43Z" id="132266641">(I just did `fgrep -r "elasticsearch-"` to find that instance. there are others in the build, like in vagrant tests that seem to be bugs?)
</comment><comment author="dadoonet" created="2015-08-18T16:28:32Z" id="132266809">Interesting. Sounds like I forgot to run the vagrant tests!
Checking...
</comment><comment author="dadoonet" created="2015-08-18T16:52:14Z" id="132274173">@rmuir I added a commit.
</comment><comment author="rmuir" created="2015-08-18T17:15:20Z" id="132283549">+1, thanks for these cleanups
</comment><comment author="dadoonet" created="2015-08-18T17:43:31Z" id="132292674">Merged in master with 0599f85d2d993cdeb76cb919d510ddd5ea5a5d3e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replicate Closed Indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12963</link><project id="" key="" /><description>Closing an index is a good way to save memory and to remove unneeded indices from your cluster without deleting the data. It can also help to improve performance if you have an extremely long retention period where you rarely query the oldest info. This allows indices to be opened on-demand, then closed when you are done.

However, there's a small problem with closed indices: they aren't replicated in the case of data loss. This makes sense because the cluster gives up managing them, but it also can create surprise situations in normal shard loss scenarios (e.g., lost drives).

Without opening the closed indices, there needs to be some way to force them to be [re-]replicated or to somehow warn that they are not replicated.
</description><key id="101693747">12963</key><summary>Replicate Closed Indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Cluster</label><label>adoptme</label><label>feature</label></labels><created>2015-08-18T16:21:26Z</created><updated>2017-01-19T17:05:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-18T17:38:12Z" id="132291109">Isn't the way to do this to take a snapshot?
</comment><comment author="pickypg" created="2015-08-18T17:44:38Z" id="132293010">Snapshots are always the ultimate disaster recovery scenario and they are absolutely the best solution, but you don't even know that you need to restore from the snapshot until opening the index fails due to missing shards for closed indices.
</comment><comment author="bleskes" created="2015-08-18T18:09:21Z" id="132302269">I think there is merit to this issue (although it requires a big but doable conceptual change - closed shards). The main use case for closed indices is an easy-to-access archive. It&#8217;s good for the cluster to react if one the archived copies is lost (server blows up) and make sure we create a new copy of the data.

&gt; On 18 Aug 2015, at 19:44, Chris Earle notifications@github.com wrote:
&gt; 
&gt; Snapshots are always the ultimate disaster recovery scenario and they are absolutely the best solution, but you don't even know that you need to restore from the snapshot until opening the index fails due to missing shards for closed indices.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="rtoma" created="2015-12-10T14:48:31Z" id="163646082">Maybe its worth updating docs for Elasticsearch and Curator (offering an easy way of closing indices), so users will know what risks are involved?
</comment><comment author="smford" created="2016-01-20T11:11:02Z" id="173174127">I second this request, we have worked around the issue by writing a script that re-opens an indices from which ES then replicates it across.  Once replicated the script then closes it again.
It would be nice if there was the ability to replicate the closed indices without this bodge.
</comment><comment author="clintongormley" created="2016-01-28T12:23:53Z" id="176154316">Perhaps related to #12847 
</comment><comment author="prog8" created="2016-07-06T08:53:48Z" id="230715545">Are there any conclusions related to replicating and relocating closed indices? What kind of changes this requires in ES code? Maybe the first step could be a plugin which does something @smford has mentioned?
</comment><comment author="bleskes" created="2016-07-06T13:07:39Z" id="230765640">@prog8 you are more than welcome to try - but the amount of changes are considerate - this is rooted deep in our infra strcuture and is definitely not a low hanging fruit.
</comment><comment author="jasontedor" created="2016-07-07T03:10:40Z" id="230967977">&gt; Maybe the first step could be a plugin which does something @smford has mentioned?

To be clear to what @bleskes said, a plugin implementation will not be possible. This requires fundamental changes inside of Elasticsearch.
</comment><comment author="yackushevas" created="2016-12-27T16:07:49Z" id="269345579">Any news?</comment><comment author="chandlermelton" created="2017-01-10T22:17:16Z" id="271715804">Curious about this as well. @smford, would you be willing to link to/describe your script?</comment><comment author="smford" created="2017-01-11T12:09:03Z" id="271852806">@chandlermelton : Getting approval from work and the author to release the script. Will get back to you soon.  @jinnko</comment><comment author="jinnko" created="2017-01-19T16:19:45Z" id="273821665">This is the script we've used.  It's nothing special, simply a loop to get a list of indices, open the closed ones and wait for them to complete being verified or re-routed, then close them again.

https://gist.github.com/jinnko/78072a77c8302390ff0830963492e9f4</comment><comment author="chandlermelton" created="2017-01-19T17:05:02Z" id="273835174">Thanks guys!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchSourceBuilder.field() method is not accessible through TopHitsBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12962</link><project id="" key="" /><description>The class TopHitsBuilder provides addFieldDataField() method which calls directly SearchSourceBuilder.fieldDataField(), but there is no way to access to SearchSourceBuilder.field() through TopHitsBuilder.

``` java
public TopHitsBuilder addFieldDataField(String name) {
        sourceBuilder().fieldDataField(name);
        return this;
    }
```

Could it be possible for TopHitsBuilder to have a new public method addField() that calls SearchSourceBuilder.field()  ?
</description><key id="101686721">12962</key><summary>SearchSourceBuilder.field() method is not accessible through TopHitsBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jbghoul</reporter><labels><label>:Java API</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2015-08-18T15:45:42Z</created><updated>2015-11-09T10:40:13Z</updated><resolved>2015-11-09T10:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>check with jps that the pid file contains a pid that actually is an e&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12961</link><project id="" key="" /><description>&#8230;lasticsearch process

relates to #12063
</description><key id="101679230">12961</key><summary>check with jps that the pid file contains a pid that actually is an e&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-18T15:10:27Z</created><updated>2015-11-22T10:15:21Z</updated><resolved>2015-08-26T12:29:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-18T15:32:01Z" id="132251495">Looks great. This extra check is really needed.
</comment><comment author="brwe" created="2015-08-19T08:55:11Z" id="132498526">addressed all comments. I cannot use `${java.home}/bin/jps` because `java.home` seems to point to the jre and not jdk but I can use `${environment.JAVA_HOME}/bin/jps`. Let me know if this is OK or if should go back to just calling `jps`.
</comment><comment author="s1monw" created="2015-08-25T08:59:26Z" id="134531393">LGTM
</comment><comment author="s1monw" created="2015-08-27T11:27:36Z" id="135393616">my build now fails with this:

```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (integ-teardown) on project elasticsearch: An Ant BuildException has occured: The following error occurred while executing this line:
[ERROR] /media/benchmark/elasticsearch/distribution/tar/target/dev-tools/ant/integration-tests.xml:299: The following error occurred while executing this line:
[ERROR] /media/benchmark/elasticsearch/distribution/tar/target/dev-tools/ant/integration-tests.xml:229: Execute failed: java.io.IOException: Cannot run program "/media/work/jdk1.7.0_55/bin/java/bin/jps" (in directory "/media/benchmark/elasticsearch/distribution/tar"): error=20, Not a directory
[ERROR] around Ant part ...&lt;ant antfile="/media/benchmark/elasticsearch/distribution/tar/target/dev-tools/ant/integration-tests.xml" target="stop-external-cluster"/&gt;... @ 4:141 in /media/benchmark/elasticsearch/distribution/tar/target/antrun/build-main.xml
```

I guess it's related to this, not sure if it's on my end or a configuration problem?
</comment><comment author="brwe" created="2015-08-27T11:56:41Z" id="135399296">@s1monw I assume JAVA_HOME is set to /media/work/jdk1.7.0_55/bin/java/ on your machine? Is that supposed to be so? It would work if you set it to /media/work/jdk1.7.0_55/ I guess. Can you try? Or is jps actually located at /media/work/jdk1.7.0_55/bin/java/bin/jps ?

In any way, you are already the second that reported problems with JAVA_HOME not be set to the expected path. I opened already https://github.com/elastic/elasticsearch/pull/13124 to make sure tests fail fast in case JAVA_HOME is not set at all. So, I do wonder if we should use JAVA_HOME at all. I am not sure where to get the path to jps from though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>parent/child   no matches found error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12960</link><project id="" key="" /><description>got a error no matches found error

```
$ curl -XPUT localhost:9200/scenic_index/person/1233  -d '{"names":"wang"}'
{"_index":"scenic_index","_type":"person","_id":"1233","_version":1,"created":true}%   


```

then put child doc

```
curl -XPUT localhost:9200/scenic_index/homes/11?parent=1233  -d '{"homes":"wang"}'
zsh: no matches found:localhost:9200/scenic_index/homes/11?parent=1233
```

then got error:
no matches found: localhost:9200/scenic_index/homes/11?parent=1233

mapping:

``` java

{
    "scenic_index": {
        "mappings": {
            "person": {
                "properties": {
                    "names": {
                        "type": "string"
                    }
                }
            },
            "homes": {
                "_parent": {
                    "type": "person"
                },
                "_routing": {
                    "required": true
                },
                "properties": {
                    "home": {
                        "type": "string"
                    }
                }
            }
        }
    }
}

```

gratefully
</description><key id="101676871">12960</key><summary>parent/child   no matches found error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mouzt</reporter><labels /><created>2015-08-18T15:00:13Z</created><updated>2015-08-18T16:22:01Z</updated><resolved>2015-08-18T16:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-18T16:22:01Z" id="132265327">Please ask questions like these on the forum: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove usage of `InetAddress#getLocalHost`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12959</link><project id="" key="" /><description>this method is very confusing and if it's used it's likely the wrong thing
with respect to the actual bound / published address. This change discourages
it's use and removes all useage. It's replaced with the actual published address
most of the time.

This can cause very confusing output for instance from `http://localhost:9200/_cat/shards`

```
index shard prirep state   docs  store ip            node    
foo   1     r      STARTED    5  5.5kb 192.168.2.123 Jester  
foo   1     p      STARTED    5 10.7kb 192.168.2.123 Sunfire 
foo   4     p      STARTED    4  8.1kb 192.168.2.123 Jester  
foo   4     r      STARTED    4 10.6kb 192.168.2.123 Sunfire 
foo   2     p      STARTED    9 10.9kb 192.168.2.123 Jester  
foo   2     r      STARTED    9 13.5kb 192.168.2.123 Sunfire 
foo   3     r      STARTED    7 15.9kb 192.168.2.123 Jester  
foo   3     p      STARTED    7 15.9kb 192.168.2.123 Sunfire 
foo   0     p      STARTED    5 13.3kb 192.168.2.123 Jester  
foo   0     r      STARTED    5 10.7kb 192.168.2.123 Sunfire 
```

it show `192.168.2.123` which is simply wrong here we bound to `127.0.0.1` and published on `127.0.0.1` as well.
</description><key id="101671808">12959</key><summary>Remove usage of `InetAddress#getLocalHost`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T14:37:27Z</created><updated>2015-08-21T09:25:33Z</updated><resolved>2015-08-18T15:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-18T14:38:58Z" id="132234384">awesome, +1, thank you for cleaning this up and removing the confusion
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't specify parent if no parent field has been configured</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12958</link><project id="" key="" /><description>type=scenic_quote  mapping:

``` java
{
  "scenic_quote": {
    "_all": {
      "enabled": false
    },
    "dynamic_templates": [
      {
        "code": {
          "match": "*_code",
          "mapping": {
            "type": "string",
            "index": "not_analyzed"
          },
          "match_mapping_type": "string"
        }
      },
      {
        "no": {
          "match": "*_no",
          "mapping": {
            "type": "string",
            "index": "not_analyzed"
          },
          "match_mapping_type": "string"
        }
      }
    ],
    "properties": {
    }

  }
}
```

type=scenic_schedule  mapping:

```
{
  "scenic_schedule": {
    "_parent": {
      "type": "scenic_quote"
    },
    "dynamic_templates": [
      {
        "code": {
          "match": "*_code",
          "mapping": {
            "type": "string",
            "index": "not_analyzed"
          },
          "match_mapping_type": "string"
        }
      },
      {
        "no": {
          "match": "*_no",
          "mapping": {
            "type": "string",
            "index": "not_analyzed"
          },
          "match_mapping_type": "string"
        }
      }
    ]
  }
}
```

got error 
Can't specify parent if no parent field has been configured

how can I resolve this question?
gratefully
</description><key id="101661845">12958</key><summary>Can't specify parent if no parent field has been configured</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mouzt</reporter><labels /><created>2015-08-18T13:57:52Z</created><updated>2015-08-18T16:21:38Z</updated><resolved>2015-08-18T16:21:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-18T16:21:37Z" id="132265222">@mouzt what exception? what version of ES? what were you trying to do?  

Please ask questions like these on the forum http://discuss.elastic.co/ (but still provide the details above so that people have enough information to understand what the actual problem is)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] fix proposal after plugins doc review</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12957</link><project id="" key="" /><description>@clintongormley could you review this please?
</description><key id="101633104">12957</key><summary>[doc] fix proposal after plugins doc review</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T11:20:07Z</created><updated>2015-08-21T09:25:45Z</updated><resolved>2015-08-18T17:00:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-18T15:53:29Z" id="132257235">LGTM, thanks for reviewing the docs!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: IndicesQueriesRegistry back to being created only once</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12956</link><project id="" key="" /><description>With #12921 we refactored IndicesModule but we forgot to make sure we create IndicesQueriesRegistry once. IndicesQueriesModule used to do `bind(IndicesQueriesRegistry.class).asEagerSingleton();` otherwise we get multiple instances of the registry. This needs to be ported do the IndicesModule.
</description><key id="101631224">12956</key><summary>Internal: IndicesQueriesRegistry back to being created only once</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T11:06:13Z</created><updated>2015-08-21T09:33:45Z</updated><resolved>2015-08-18T11:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T11:09:46Z" id="132173725">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explicitly disabled the query cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12955</link><project id="" key="" /><description>It was already disabled, but during tests the test framework enabled the query cache by setting the static default query cache. The caching behaviour should be the same in production and in tests.
</description><key id="101628249">12955</key><summary>Explicitly disabled the query cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T10:48:17Z</created><updated>2015-08-21T09:20:30Z</updated><resolved>2015-08-18T10:59:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T10:54:13Z" id="132171046">LGTM
</comment><comment author="martijnvg" created="2015-08-18T10:59:34Z" id="132171792">pushed: https://github.com/elastic/elasticsearch/commit/e74f559fd4cb5b80a62c769517b53bddb91be53e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add integrations tests for installing plugins with custom options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12954</link><project id="" key="" /><description>This adds one test for each these options

-Des.path.plugins
-Des.path.conf
-Des.path.logs

Each of the new tests installs each plugin with the option and tests if
elasticsearch is able to start with all plugins
In addition, this adds checks that
- plugins can be uninstalled
- config path is actually used
- log path is actually used
- plugins can be listed with 'plugins list'

I am unsure about several things:
- would this be better to implement as a bats test like in [50_plugins.bats](https://github.com/elastic/elasticsearch/blob/master/qa/vagrant/src/test/resources/packaging/scripts/50_plugins.bats)? Writing this with ant felt rather clumsy.
- I made one test per option but this adds about 3 minutes because each test loads all plugins. Maybe we should have one test that checks the defaults and one that tests all options instead? And maybe we don't need to load all modules?
- do we actually need to test plugin installation with a  custom log folder? What do we actually want to test there?
- I made the smoke-test-plugins a parent project that the other tests inherit from to avoid pom duplication. Is that OK?

Let me know what you think. There is also some TODOS, all pointers appreciated.

closes #12712
</description><key id="101619880">12954</key><summary>Add integrations tests for installing plugins with custom options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Plugins</label><label>review</label><label>test</label></labels><created>2015-08-18T10:05:37Z</created><updated>2015-10-06T18:01:57Z</updated><resolved>2015-10-06T16:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-25T15:58:08Z" id="134647191">The ant is a bit beyond me. I understand why you created all those maven modules to do this but... ew. I don't think there is a better way than this though.

I wonder if someone better with ant can have a look at this - @rjernst would love it because its yet more maven backflips.
</comment><comment author="brwe" created="2015-09-15T16:48:42Z" id="140459772">Talked to @nik9000 - apparently I waited long enough and the vagrant tests have matured to a stage where they might replace what is in this pr. I will try and see how much we would have to change there.
</comment><comment author="brwe" created="2015-09-17T10:25:03Z" id="141037776">I took a look at the basts tests and it seems to me only few things are missing:
- check that 'plugins list' works - I added that in https://github.com/elastic/elasticsearch/pull/13617
- for custom config folder, we test that we can install and uninstall the jvm-example already here: https://github.com/elastic/elasticsearch/blob/master/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash#L99 , we only need to check that the custom setting is actually picked up when elasticsearch is started and config file removed when we uninstall
- custom plugin folder, we test already with jvm-example here https://github.com/elastic/elasticsearch/blob/master/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash#L84 We should add a test for when we start elasticsearch all works as expected.
- custom log folder: I do not see what we should test so I would just test nothing

I do not think anymore it makes sense to test this for all plugins. Adding these checks for jvm-example should be enough. I will do that and close this pr when I am done.
</comment><comment author="brwe" created="2015-09-22T13:54:28Z" id="142296000">I made a pr for the second bullet point here: #13687
Last bullet point is now implemented by c9cd70d4191741baf08bbb594680bd6bafddc456
</comment><comment author="nik9000" created="2015-10-06T16:18:42Z" id="145917145">@brwe is this pr still a thing or have you covered it entirely in the other prs?
</comment><comment author="brwe" created="2015-10-06T16:19:32Z" id="145917330">entirely covered by other prs now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed section name and api name in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12953</link><project id="" key="" /><description>I fixed section name, like image bellow.
- https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html

![es-docs](https://cloud.githubusercontent.com/assets/1573290/9323841/b06d50fa-45bf-11e5-99c1-8594c9691626.png)

Thank you so much.
</description><key id="101583420">12953</key><summary>Fixed section name and api name in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kakakakakku</reporter><labels><label>docs</label></labels><created>2015-08-18T06:44:06Z</created><updated>2015-08-18T15:51:00Z</updated><resolved>2015-08-18T15:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T10:04:46Z" id="132153561">Actually I think it is at the right level, but maybe the right fix would be to get this section a better title. @nik9000 what do you think?
</comment><comment author="kakakakakku" created="2015-08-18T10:17:57Z" id="132157441">@jpountz 
Oh sorry, I understand.

So, how about these title, or any better idea?
- Detect noop
- Detecting noop

&gt; but maybe the right fix would be to get this section a better title.
</comment><comment author="jpountz" created="2015-08-18T10:21:50Z" id="132158062">I like "Detecting noop".
</comment><comment author="kakakakakku" created="2015-08-18T10:28:19Z" id="132159111">@jpountz 
Thank you for your advise.
I fixed it, and I changed commit message.

Section level is staying `===` now.
Thank you!
</comment><comment author="jpountz" created="2015-08-18T10:53:07Z" id="132170901">I'll wait for @nik9000 to give his opinion but this looks good to me.
</comment><comment author="kakakakakku" created="2015-08-18T10:58:42Z" id="132171685">Of course :grinning:
</comment><comment author="nik9000" created="2015-08-18T14:58:47Z" id="132240197">Sure! Its fine with me.
</comment><comment author="kakakakakku" created="2015-08-18T15:07:20Z" id="132242705">:+1: 
</comment><comment author="clintongormley" created="2015-08-18T15:51:00Z" id="132256632">thanks @Kakakakakku - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify Plugin API for constructing modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12952</link><project id="" key="" /><description>The Plugin interface currently contains 6 different methods for
adding modules. Elasticsearch has 3 different levels of injectors,
and for each of those, there are two methods. The first takes no
arguments and returns a collection of class objects to construct. The
second takes a Settings object and returns a collection of module
objects already constructed. The settings argument is unecessary because
the plugin can already get the settings from its constructor. Removing
that, the only difference between the two versions is returning an
already constructed Module, or a module Class, and there is no reason
the plugin can't construct all their modules themselves.

This change reduces the plugin api down to just 3 methods for adding
modules. Each returns a Collection&lt;Module&gt;. It also removes the
processModule method, which was unnecessary since onModule
implementations fullfill the same requirement. And finally, it renames
the modules() method to nodeModules() so it is clear these are created
once for each node.
</description><key id="101564389">12952</key><summary>Simplify Plugin API for constructing modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T03:40:04Z</created><updated>2015-08-19T23:04:05Z</updated><resolved>2015-08-18T21:28:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-18T08:20:14Z" id="132117795">I really like the change. Something I'm just wondering. 
Does this PR change anything in the way some components are loaded? 

I mean that before the change plugin components were loaded only when used. For example, `AwsEc2Service` is loaded only if the user set discovery to `ec2`. Is it still the case with this PR? Components are loaded only on demand?
</comment><comment author="rjernst" created="2015-08-18T08:41:47Z" id="132121369">@dadoonet This does not change anything about component loading. Components are still returned as classes and constructed with the injector. This is simply about modules, whose sole purpose is to bind classes the injector will use when necessary.
</comment><comment author="dadoonet" created="2015-08-18T08:48:56Z" id="132122616">@rjernst Thanks for confirming!
The change looks good to me but I think another reviewer would be great.
</comment><comment author="kimchy" created="2015-08-18T08:50:51Z" id="132123003">I really like this change!. Two notes:
- If we are already changing the `Plugin` interface, I would just merge `AbstractPlugin` into `Plugin` and remove the interface aspect.
- Why did you remove passing Settings to the different modules constructions? Those modules might need the Settings to properly construct? How will they do it now?
</comment><comment author="rjernst" created="2015-08-18T08:54:12Z" id="132124202">@kimchy Plugins have the option of a ctor that takes Settings. They can store this, and pass to any of their modules that need settings.
</comment><comment author="rjernst" created="2015-08-18T09:03:25Z" id="132125960">Ok, settings are needed for the shard/index modules, since those settings are the index settings, not the node settings. I'll add that parameter back.
</comment><comment author="kimchy" created="2015-08-18T09:03:45Z" id="132126034">Yea, it is the shard/index level Settings that are important... . I would still think how to clean up the duplication of which Settings to use, thinking that maybe cleanest is to pass the right Settings to all callbacks (modules/service) and not rely on black magic constructor based Settings injection, which one needs to remember are node based settings. But that is a different change.
</comment><comment author="rjernst" created="2015-08-18T16:51:39Z" id="132273956">@kimchy I merged AbstractPlugin and Plugin, and added back Settings for the index/shard modules methods.
</comment><comment author="s1monw" created="2015-08-18T20:09:36Z" id="132335155">added one comment other than that LGTM
</comment><comment author="rjernst" created="2015-08-18T21:29:10Z" id="132357358">I will backport to the 2.0 branch once the beta is out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use preferIPv6Addresses for sort order, not preferIPv4Stack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12951</link><project id="" key="" /><description>java.net.preferIPv6Addresses is a better choice. preferIPv4Stack is a nuclear option
and you just won't even bind to any IPv6 addresses. This reduces confusion.
</description><key id="101558007">12951</key><summary>Use preferIPv6Addresses for sort order, not preferIPv4Stack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-18T02:55:36Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-18T03:30:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-18T03:16:21Z" id="132054342">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can we move in-memory index to disk dynamically?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12950</link><project id="" key="" /><description /><key id="101544626">12950</key><summary>Can we move in-memory index to disk dynamically?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lokeshhctm</reporter><labels /><created>2015-08-18T01:20:19Z</created><updated>2015-08-18T10:05:50Z</updated><resolved>2015-08-18T10:05:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T10:05:50Z" id="132153905">No. Besides, the memory store will be removed in 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't i find children of parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12949</link><project id="" key="" /><description>```
PUT /test
{

      "mappings": {
         "client": {
            "properties": {
               "name": {
                  "type": "string"
               }
            }
         },
         "comment": {
           "_parent": {
             "type": "post"
           },
            "properties": {
               "body": {
                  "type": "string"
               }
            }
         },
         "post": {
           "_parent":{
             "type":"client"
           },
            "properties": {
               "name": {
                  "type": "string"
               }
            }
         }
      }

}


POST /test/client/_bulk
{index:{"_id":"1"}}
{"name":"evil corp"}
{index:{"_id":"2"}}
{"name":"good corp"}

POST /test/post/_bulk
{index:{"_id":"1","parent":"1"}}
{"name":"test post1"}
{index:{"_id":"2","parent":"1"}}
{"name":"test post2"}

POST /test/comment/_bulk
{index:{"_id":"1","parent":"1"}}
{"body":"test comment1"}
{index:{"_id":"2","parent":"2"}}
{"body":"test comment2"}
{index:{"_id":"3","parent":"1"}}
{"body":"test comment3"}
{index:{"_id":"4","parent":"1"}}
{"body":"test comment4"}
{index:{"_id":"5","parent":"1"}}
{"body":"test comment5"}

GET /test/_mapping

GET /test/comment/2?parent=2

GET /test/comment/_search
{
  "query": {
    "has_parent": {
      "type": "post",
      "query": {
        "term": {
          "_id": "1"
        }
      }
    }
  }
}

# why doesnt this work ?
GET /test/comment/_search
{
  "query": {
    "has_parent": {
      "type": "post",
      "query": {
        "term": {
          "_id": "2"
        }
      }
    }
  }
}
```

Why doesn't this work ? I can find the children of post with id = 1 but not 2
</description><key id="101525775">12949</key><summary>Can't i find children of parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jb-san</reporter><labels /><created>2015-08-17T22:47:11Z</created><updated>2015-08-17T23:27:13Z</updated><resolved>2015-08-17T23:27:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jb-san" created="2015-08-17T23:27:13Z" id="131991273">never mind found the documentation about the routing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify custom repository type setup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12948</link><project id="" key="" /><description>Custom repository types are registered through the RepositoriesModule.
Later, when a specific repository type is used, the RespositoryModule
is installed, which in turn would spawn the module that was
registered for that repository type. However, a module is not needed
here. Each repository type has two associated classes, a Repository and
an IndexShardRepository.

This change makes the registration method for custom repository
types take both of these classes, instead of a module.

See #12783.
</description><key id="101525082">12948</key><summary>Simplify custom repository type setup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T22:41:30Z</created><updated>2015-08-19T23:04:18Z</updated><resolved>2015-08-18T17:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T08:12:31Z" id="132115418">LGTM. I like how this removes one level of indirection in the modules.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `expectedShardSize` to ShardRouting and use it in path.data allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12947</link><project id="" key="" /><description>Today we only guess how big the shard will be that we are allocating on a node.
Yet, we have this information on the master but it's not available on the data nodes
when we pick a data path for the shard. We use some rather simple heuristic based on
existing shard sizes on this node which might be complete bogus. This change adds
the expected shard size to the ShardRouting for RELOCATING and INITIALIZING shards
to be used on the actual node to find the best data path for the shard.

Closes #11271
</description><key id="101495528">12947</key><summary>Add `expectedShardSize` to ShardRouting and use it in path.data allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T19:44:16Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-21T06:34:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-08-17T20:19:13Z" id="131948027">I left a couple comments, and I don't understand all the places where you had to punch expectedShardSize through, but big +1 to sending expectedShardSize to the node so it can make a more informed decision when it has multiple path.data.
</comment><comment author="clintongormley" created="2015-08-18T11:24:13Z" id="132179668">@s1monw what happens in this scenario: A node has three data paths and one shard, located on the data path with least capacity. The shard grows to fill 99% of its data path...
</comment><comment author="s1monw" created="2015-08-18T11:26:47Z" id="132180690">&gt; @s1monw what happens in this scenario: A node has three data paths and one shard, located on the data path with least capacity. The shard grows to fill 99% of its data path...

it blows up - this is out of scope in this PR that's a problem of the disk-allocation decider and needs to be fixed in a different PR
</comment><comment author="s1monw" created="2015-08-19T07:17:39Z" id="132471395">if nobody objects I will push this in the next 72 hours 
</comment><comment author="jpountz" created="2015-08-21T09:26:50Z" id="133347885">@s1monw I can only find this commit in master, not in the 2.0 branch, is it expected?
</comment><comment author="s1monw" created="2015-08-21T11:45:24Z" id="133385216">@jpountz I ported it just now... wanted to give it some CI time first
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] Provide List of Expected Exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12946</link><project id="" key="" /><description>With the introduction of structured exceptions, it would be great if we could start to list all _expected_ exceptions and what they mean to the catcher (e.g., can they retry? should they retry?).

I think it should start simple with Search exceptions and Indexing exceptions, then build out from there. Perhaps we should even have an error code system to make looking them up easier?
</description><key id="101493880">12946</key><summary>[Docs] Provide List of Expected Exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Exceptions</label><label>docs</label></labels><created>2015-08-17T19:33:22Z</created><updated>2017-05-05T15:45:51Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yazaddaruvala" created="2016-09-20T02:41:17Z" id="248186524">May I ask why this isn't getting prioritized?

Its really frustrating not knowing which exceptions can be caused, if they should be retried, or not. A list with explanations would be tremendously useful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multicast ping should work over ipv6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12945</link><project id="" key="" /><description>followup to #12942

Currently this is only going to work over ipv4, due to the address used. We should support a pure v6 environment: maybe it works today if you use a -D to change the address, but it sorta defeats the purpose of not requiring configuration:)

However, I think its best to get #12914 straightened out on the OS X first, to eliminate any confusion.
</description><key id="101492196">12945</key><summary>multicast ping should work over ipv6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>bug</label></labels><created>2015-08-17T19:23:17Z</created><updated>2016-02-03T13:08:15Z</updated><resolved>2016-02-03T13:08:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T12:23:20Z" id="176154223">Multicast support has been removed. Closing
</comment><comment author="rmuir" created="2016-01-28T12:25:44Z" id="176154873">Multicast has been removed? Why do i still see the plugin in master?
</comment><comment author="clintongormley" created="2016-01-29T09:15:21Z" id="176655576">I was under the mistaken impression that it was deprecated when it was moved to a plugin.
</comment><comment author="clintongormley" created="2016-01-29T14:22:19Z" id="176779642">See #16310 
</comment><comment author="clintongormley" created="2016-02-03T13:08:15Z" id="179220300">Multicast has been removed by #16326
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add mechanism for transporting shard-level actions by node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12944</link><project id="" key="" /><description>Currently, many shard-level operations are transported with a request
per shard via TransportBroadcastAction. These shard-level requests are
then submitted to unbounded execution queues for asynchronous execution
on the receiving node. This transport mechanism and stuffing of the
execution queues can be problematic on large clusters. A better
mechanism would be to aggregate the shard-level requests, transport
them via a single request per node, and execute the shard-level
operations serially on the receiving node.

This commit introduces TransportNodeBroadcastAction which is the
high-level mechanism for transporting the shard-level operations in a
single request per node. The shard-level operations are executed
serially on the receiving node and per-node shard-level results are
aggregated into a single response per node. These node-level results
are then aggregated into a single response to the initial request.

One item of note is a new mechanism for registering request handlers.
This mechanism enables registrants to provide a callback for
instantiating new instances of the request class. Doing this enables
the inner class to be instantiated with the context of its outer class.
This is done so that a single NodeRequest class can be defined rather
than defining a class per operation.

Closes #7990
</description><key id="101490952">12944</key><summary>Add mechanism for transporting shard-level actions by node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-17T19:15:30Z</created><updated>2015-11-22T10:15:21Z</updated><resolved>2015-08-29T20:45:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T09:36:06Z" id="132143456">I'm slightly worried that we are optimizing for a use-case (too many shards/indices) that we also happen to recommend against? Besides I'm wondering how much it will help: even if we transport a single request instead of N, I don't see it helping save memory (https://github.com/elastic/elasticsearch/issues/9130) or make stats calls faster (https://github.com/elastic/elasticsearch/issues/7385) which seem to be motivations for #7990?
</comment><comment author="nik9000" created="2015-08-21T19:45:14Z" id="133540555">&gt; I'm slightly worried that we are optimizing for a use-case (too many shards/indices) that we also happen to recommend against?

We recommend against oversubscription as a way of optimizing write throughput but we recommend a little oversubscription if you expect to add more nodes. We also commonly recommend time based indexes which can lead to lots of situations like this.

&gt; Besides I'm wondering how much it will help: even if we transport a single request instead of N, I don't see it helping save memory (#9130) or make stats calls faster (#7385) which seem to be motivations for #7990?

I haven't done enough research to refute or confirm these claims.
</comment><comment author="jasontedor" created="2015-08-22T13:15:45Z" id="133702709">@jpountz The initial version of this pull request contained only the change to send a single request to each node rather than a request per shard. A subsequent [commit](https://github.com/jasontedor/elasticsearch/commit/0d2c91663243abfffff6bc5a0c11761f6714f035) has updated this pull request to also execute the shard operations in serial on the receiving node. The advantage of this is to avoid filling up unbounded execution queues (e.g., the management and optimize queues) with many shard-level operations on large clusters.
</comment><comment author="bleskes" created="2015-08-23T20:26:34Z" id="133923413">@jasontedor I did a review and I like it. Left a bunch of comments. It would also be great if we can manage to have unit testing for the new transport action like we have for TransportReplicationAction.
</comment><comment author="jasontedor" created="2015-08-26T13:09:59Z" id="135007381">@bleskes Thanks for the very helpful and thorough review. I've addressed all of your review comments. Of note, I added unit tests for `TransportBroadcastByNodeAction`. In the course of writing those tests, I [altered `TransportBroadcastByNodeAction`](https://github.com/jasontedor/elasticsearch/blob/9b9d859198f4cc43615f43036b210453ca0b2e43/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java#L104-L110) to count all shards on a failing node as contributing to failures that accumulate in the final response. Good?
</comment><comment author="jasontedor" created="2015-08-26T16:51:52Z" id="135106159">@bleskes I've pushed commits in response to your latest round of comments. Thanks again for reviewing.
</comment><comment author="jasontedor" created="2015-08-27T17:33:49Z" id="135501565">@bleskes I think that this is ready for another round of review.
</comment><comment author="bleskes" created="2015-08-28T18:59:32Z" id="135860786">LGTM. Left some minor comments - no need for another review - feel free to push once addressed. I think this has turned out pretty nice.
</comment><comment author="jasontedor" created="2015-08-28T19:08:57Z" id="135862537">Thanks @bleskes for another round of helpful reviews, especially the base class request/response simplification. I'll get this integrated into master soon.
</comment><comment author="bleskes" created="2015-08-28T19:15:18Z" id="135863725">oh - one more thing - after you squash / rebase etc. Can you make sure to add a note to the commit message and this PR that we added an option to register transport handlers and why? In case someone wonders in the future why it was done... 
</comment><comment author="jasontedor" created="2015-08-29T21:01:38Z" id="136050602">This pull request has been merged to master in commit 5e2efcfe091c62ef215016e2981d05339ea945a3 and integrated into the 2.0 branch in commit b6287bb37355a5d84639e7cb015810b6c112e088.
</comment><comment author="bleskes" created="2015-08-30T07:38:21Z" id="136095164">@jasontedor great! I just realized that TransportUpgradeAction should probably be migrated as well. Can you take care of it?
</comment><comment author="jasontedor" created="2015-08-31T00:44:49Z" id="136223275">@bleskes I've opened #13205 to address `TransportUpgradeAction`.
</comment><comment author="mikemccand" created="2015-09-02T11:55:58Z" id="137046559">In the nightly benchmarks (https://benchmarks.elastic.co/index.html ) it looks like this change caused an increase in indices stats time (see annotation B on the Stats request time graph).

I guess this is expected, since now the stats computation for the 5 shards is done in serial on the one node, instead of concurrently?
</comment><comment author="jasontedor" created="2015-09-02T13:14:40Z" id="137073254">&gt; In the nightly benchmarks (https://benchmarks.elastic.co/index.html ) it looks like this change caused an increase in indices stats time (see annotation B on the Stats request time graph).

Thanks for bringing this to my attention @mikemccand.

&gt; I guess this is expected, since now the stats computation for the 5 shards is done in serial on the one node, instead of concurrently?

Precisely. It looks the regression here is 2x, from ~20ms to ~40ms. If this turns out to be a major issue, we have some options to address.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] added tests for alternate queries formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12943</link><project id="" key="" /><description>We currently test that our query parsers can parse the format that our query builder outputs in XContent format, but in some cases the parser supports more than that, hence we need more specific tests otherwise we have no coverage for alternate formats.
</description><key id="101467984">12943</key><summary>[TEST] added tests for alternate queries formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>test</label></labels><created>2015-08-17T17:18:08Z</created><updated>2015-08-21T14:18:52Z</updated><resolved>2015-08-21T14:18:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-20T10:01:41Z" id="132962656">Took a look and LGTM.  I think it's good that we extend the tests for the parsers with alternative query strings.
Just as an afterthought, if in the future we include more additional tests for the query parsers `fromXContent()`, maybe we should introduce separate query parser test classes for that? I imagine the base class could be simpler since we won't need all the services we now use in `toQuery()` and could start to test more irregularities in the parsers.
</comment><comment author="javanna" created="2015-08-20T11:01:40Z" id="132974463">Your comment makes sense @cbuescher , also cause the tests are called *QueryBuilderTest, while these are query parser tests. Maybe we should look into splitting the tests into two for every query? Seems like a scary change though at this point :)
</comment><comment author="cbuescher" created="2015-08-20T12:08:54Z" id="132987520">@javanna I wouldn't do this now, just if we have to make even bigger adjustments to the base query test infra in the future  to test parsers, I would start thinking about moving those into their own tests. I think that we test toXContent() roundtrip in the query builder test right now makes sense, only if we inlcude more complex things we might move them out to somewhere else.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix network binding for ipv4/ipv6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12942</link><project id="" key="" /><description>Elasticsearch doesn't work well today with modern machines that might support both ipv4 and ipv6. The worst problem here that `curl http://localhost...` does not work e.g. on mac (#12906), because we aren't bound to any v6 address.

Equally bad, we only bind to loopback by default for 2.0, so if you want to connect to the network "for real" you might provide an interface name, but that always tends to do the wrong thing too (#12915), e.g. pick link local or some other useless address.

This is a compromise fix that tries to keep things simple:
- we bind to multiple addresses when a specified host/interface name has multiple addresses. If you dont like this, then specify a single address.
- we still only _publish_ by default to one (this would require more work).
- no changes for ipv6 multicast or anything like that yet.

The default for which address to publish when bound to multiple addresses is pretty simple, we prefer ipv4 by default (java.net.preferIPv4Stack) and I think we should until things like multicast are fixed to work correct over ipv6. Otherwise we prefer "real addresses" &gt; site local &gt; link local and so on.

Some things are still a bit messy, because real cleanups need to not be right before a beta release. 

Closes #12906
Closes #12915
</description><key id="101458135">12942</key><summary>Fix network binding for ipv4/ipv6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T16:20:39Z</created><updated>2015-08-21T09:14:40Z</updated><resolved>2015-08-18T11:16:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-17T17:42:41Z" id="131904349">So, an interesting byproduct of this is that if I have an ES node bind to the public interface with `bin/elasticsearch -Des.network.host=_en0_` and then start up another with no options (so it binds to loopback interfaces only), the loopback one detects and joins a cluster with the public facing one.

I'm not sure if this is desired or not, I guess I would expect the `loopback` one only to be able to form clusters with other nodes bound to the loopback interface. What do you think?
</comment><comment author="dakrone" created="2015-08-17T17:46:14Z" id="131905831">Also, this is an OSX-only issue, Linux doesn't have this loopback &lt;-&gt; non-loopback communication weirdness.
</comment><comment author="rmuir" created="2015-08-17T17:47:38Z" id="131906412">That is not related to this change! That is https://github.com/elastic/elasticsearch/issues/12914 and I deferred it because its Mac OS X specific, and because its not related to these changes.
</comment><comment author="dakrone" created="2015-08-17T17:50:20Z" id="131907614">&gt; That is not related to this change!

Okay, I totally agree about the deferring it. Just wanted to make sure it was something we are aware of (which of course there is the other issue open for it).
</comment><comment author="dakrone" created="2015-08-17T18:00:18Z" id="131910245">Left a few comments but generally LGTM. I tested this locally on OSX and Linux. Would be great to have someone manually test on Windows as well just for a sanity check.
</comment><comment author="s1monw" created="2015-08-17T18:31:25Z" id="131918833">Left mostly minor comments! This looks great to me! I will run tests with node.mode=network quickly as well on that.
</comment><comment author="s1monw" created="2015-08-17T19:11:17Z" id="131933041">I ran tests with `-Des.node.mode=network` and they pass +1 to push here is my LGTM

thanks rob!
</comment><comment author="clintongormley" created="2015-08-18T11:16:24Z" id="132174711">Merged in b3a9abb7265e7f5cf2f13c17597c5f269ea46a85 and 68307aa9f3636bdccd1f1ca90f1d0fd640a019a5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch doesn't want to stop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12941</link><project id="" key="" /><description>I did a large-ish bulk load on my laptop over the weekend of about five million documents into Elasticsearch 1.7.1. They were large-ish so I set the refresh_interval to `-1` during the load process and to `1s` after it was finished. I also sent a `_flush` to the index after all the documents were loaded so I could send a `_count` and get accurate numbers.

Anyway, after that I tried to stop Elasticsearch so that I could shut it down and make a copy of the load for posterity. I hit `ctrl-c` in its window. It didn't die. I worked for an hour and came back. Still not dead. The only runnable threads are ConcurrentMergeScheduler [doing its thing](https://gist.github.com/nik9000/3dc4df2b97ced8c7d60b). It also had three threads that looked [parked](https://gist.github.com/nik9000/96d3ddc5408fbbda03fd) but in the middle of something.

These wait times seem to be a bit much. I get waiting for a minute to try to finish a merge but an hour is going to have people `kill -9`ing Elasticsearch pretty consistently.
</description><key id="101454331">12941</key><summary>Elasticsearch doesn't want to stop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2015-08-17T15:58:12Z</created><updated>2015-08-18T11:26:25Z</updated><resolved>2015-08-17T21:32:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-17T18:37:21Z" id="131920228">do you have a stack dump from this?
</comment><comment author="nik9000" created="2015-08-17T18:39:07Z" id="131920614">&gt; do you have a stack dump from this?

Those two links above are stack dumps of the threads that I think are the trouble. I can take a stack dump of the whole thing though.
</comment><comment author="nik9000" created="2015-08-17T18:40:39Z" id="131920951">https://gist.github.com/nik9000/85b3172585a7a47e2ee5
</comment><comment author="s1monw" created="2015-08-17T18:46:05Z" id="131923079">this helps I will update you with ideas later!
</comment><comment author="s1monw" created="2015-08-17T19:22:00Z" id="131935415">I acutally think we always had that problem but since https://github.com/elastic/elasticsearch/pull/10833 it's showing. We forced -9 basically before. In master / 2.0 this issue is gone since we don't call maybeMerge
</comment><comment author="mikemccand" created="2015-08-17T19:24:02Z" id="131935981">Thanks @nik9000, what's happening is merges are falling way behind (laptop hard drive), and in ES 1.x we let Lucene's CMS do hard throttling of new merges in this case.

But in 2.0 this should be fixed with the Lucene upgrade just before https://github.com/elastic/elasticsearch/pull/8643 ... if you can re-run your test with 2.0 beta that would be great :)
</comment><comment author="nik9000" created="2015-08-17T19:28:10Z" id="131936748">&gt; But in 2.0 this should be fixed with the Lucene upgrade just before #8643 ... if you can re-run your test with 2.0 beta that would be great :)

I'll do that.

I have two questions then:
1. Is it ok that 1.7-ish can get into a state where their init/systemd scripts won't kill them? I imagine its helpful to have this issue be google-able but is that enough?
2. In the case of my load going on right now can I safely `kill -9` elasticsearch? I suspect so, but it took a long time and I just want to make sure.
</comment><comment author="s1monw" created="2015-08-17T19:29:25Z" id="131937018">&gt; 1. In the case of my load going on right now can I safely kill -9 elasticsearch? I suspect so, but it took a long time and I just want to make sure.

by that time yes

&gt; 1. Is it ok that 1.7-ish can get into a state where their init/systemd scripts won't kill them? I imagine its helpful to have this issue be google-able but is that enough?

it was always the case just hidden
</comment><comment author="nik9000" created="2015-08-17T21:32:43Z" id="131968440">&gt; But in 2.0 this should be fixed with the Lucene upgrade just before #8643 ... if you can re-run your test with 2.0 beta that would be great :)

Indeed - this problem doesn't happen in the 2.0 branch. Elasticsearch sits there for a few seconds after being killed, obviously trying to finish something off, and then dutifully dies. Sometimes it logs a nice error about jobs being submitted to killed thread pools. It still dies. Which is great.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make a test less flakey</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12940</link><project id="" key="" /><description>EsExecutorsTests had a test that was failing spuriously due to threadpools
being threadpools. This weakens the assertions that the test makes to what
should always be true.
</description><key id="101446178">12940</key><summary>Make a test less flakey</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T15:22:56Z</created><updated>2015-08-21T09:08:45Z</updated><resolved>2015-08-17T15:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-17T15:23:57Z" id="131860282">@s1monw - this should fix test that has gotten jenkins up in arms.
</comment><comment author="s1monw" created="2015-08-17T15:24:55Z" id="131860708">LGTM
</comment><comment author="nik9000" created="2015-08-17T15:28:48Z" id="131862496">Merged into 2.1 and cherry-picked into 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Share thread pools that have similar purposes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12939</link><project id="" key="" /><description>Because we have thread pools for almost everything, even if each of them has a
reasonable size, the total number of threads that elasticsearch creates is
high-ish. For instance, with 8 processors, elasticsearch creates between 58
(only fixed thread pools) and 111 threads (including fixed and scaling pools).
With this change, the numbers go down to 33/59.

Ideally the SEARCH and GET thread pools should be the same, but I couldn't do
it now given that some SEARCH requests block on GET requests in order to
retrieve indexed scripts or geo shapes. So they are still separate pools for
now.

However, the INDEX, BULK, REFRESH and FLUSH thread pools have been merged into
a single WRITE thread pool, the SEARCH, PERCOLATE and SUGGEST have been merged
into a single READ thread pool and FETCH_SHARD_STARTED and FETCH_SHARD_STORE
have been merged into FETCH_SHARD. Also the WARMER pool has been removed: it
was useful to parallelize fielddata loading but now that we have doc values by
default, we can make things simpler by just loading them in the current thread.

Close #12666
</description><key id="101445006">12939</key><summary>Share thread pools that have similar purposes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2015-08-17T15:16:01Z</created><updated>2016-01-26T17:59:22Z</updated><resolved>2016-01-26T17:57:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-17T15:26:26Z" id="131861317">&gt; default, we can make things simpler by just loading them in the current thread.

That'd be on the merge or WRITE thread, right?
</comment><comment author="jpountz" created="2015-08-17T15:31:48Z" id="131863619">Exactly.
</comment><comment author="nik9000" created="2015-08-17T16:12:43Z" id="131875772">LGTM
</comment><comment author="bleskes" created="2015-08-18T19:02:10Z" id="132319537">I'm +1 on most of these merges (because of the simplification,  I would personally like to understand more about the concerns due to the overhead of threads, especially for the scaling thread pools).

However, I am concerned that not having a dedicated BULK threadpool will cause operations to stall due to heavy indexing load. I would suggest leaving BULK as a separate Thread Pool and have WRITE be used for all "lite" write operations.
</comment><comment author="jpountz" created="2015-08-19T08:13:03Z" id="132485893">Thanks for taking a look @bleskes. There are several reasons for reducing the number of threads:
- reducing context switching
- reducing memory usage (see #9135)
- speeding up elasticsearch startup

These pools are only one part of the threads that elasticsearch creates, we also have transport threads, merge threads, a scheduling thread pool, ...

I understand your concerns about BULK vs. WRITE but the same could be said about SUGGEST vs. SEARCH or REFRESH vs. INDEX, or even long-running low-value search requests vs. short-running high-value search requests. If we want to try to give better latency to some operations, I think we should rather use priority queues than set up new thread pools?
</comment><comment author="bleskes" created="2015-08-19T09:48:29Z" id="132518378">&gt; I understand your concerns about BULK vs. WRITE but the same could be said about SUGGEST vs. SEARCH or REFRESH vs. INDEX, or even long-running low-value search requests vs. short-running high-value search requests.

Agreed that these are the same tensions and it&#8217;s all about a good balance. I think the bulk thread pool is much more likely to be tasked with have load than the other ones. That dedicated bulk pool was added to users actually running into this starvation issue. We have similar protections in other places (like a dedicate recovery channel for small files). I&#8217;d hate to see a regression here&#8230;

&gt;  If we want to try to give better latency to some operations, I think we should rather use priority queues than set up new thread pools?

Agreed that might a good idea in general, but it still wouldn&#8217;t solve the case were all threads of the pool are busy with a heavy task where a light one comes.

&gt; On 19 Aug 2015, at 10:13, Adrien Grand notifications@github.com wrote:
&gt; 
&gt; Thanks for taking a look @bleskes. There are several reasons for reducing the number of threads:
&gt; 
&gt;   &#8226; reducing context switching
&gt;   &#8226; reducing memory usage (see #9135)
&gt;   &#8226; speeding up elasticsearch startup
&gt; These pools are only one part of the threads that elasticsearch creates, we also have transport threads, merge threads, a scheduling thread pool, ...
&gt; 
&gt; I understand your concerns about BULK vs. WRITE but the same could be said about SUGGEST vs. SEARCH or REFRESH vs. INDEX, or even long-running low-value search requests vs. short-running high-value search requests. If we want to try to give better latency to some operations, I think we should rather use priority queues than set up new thread pools?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="jpountz" created="2015-08-19T12:38:49Z" id="132576706">I added the INDEX threadpool back.
</comment><comment author="kimchy" created="2015-08-19T12:46:47Z" id="132578353">I am hesitant with this change to be honest. Few examples:
- Systems that rely on fast id based GET API, but also execute heavy search requests. This simple supposedly fast get by id be blocked and compete with search requests.
- Fetch thread pools, they were separated intentionally (and they are scaling, they mostly matter on cluster restarts or node join/leave). Fetching store is typically slow, but fetching states is not. And fetching state is what makes a primary be allocated. We can end up with primaries not being allocates since they are waiting on fetching store of replicas.
- Same reasoning from Boaz around index and bulk, though I see it was added back, but I am confused, since it is called INDEX and it is used for BULK?
</comment><comment author="jpountz" created="2015-08-19T13:32:20Z" id="132598598">But then how do we reduce the number of threads that elasticsearch starts? For instance, I started elasticsearch on my 8-cores machine (single-node cluster) and even with moderate activity, I have:
- 16 http_server_worker
- 16 transport_client_worker
- 16 transport_server_worker
- 13 search
- 8 get
- 8 index
- 8 bulk
- 8 suggest
- 8 percolate
- 5 management
- 5 warmer
- 4 refresh
- 4 flush
- 4 listener
- as well as ~10 more threads for varous purposes (http_server_boss, transport_client_timer, ttl_expire, master_mapping_updater, timer, scheduler, transport_server_boss, transport_client_boss, discovery#multicast#receiver, clusterService#updateTask)

Overall, this is more than 16 times the number of cores I have on my machine yet not all threadpools are active (eg. fetch_shard_started, optimize).

&gt; Same reasoning from Boaz around index and bulk, though I see it was added back, but I am confused, since it is called INDEX and it is used for BULK?

The "write" pool is still used for bulk in the PR, I just revived the "index" threadpool so that index/delete/update operations are not delayed byheavy bulk requests, which I think addresses Boaz's concerns?
</comment><comment author="kimchy" created="2015-08-19T13:43:57Z" id="132603864">First, I am not sure if it is a problem? Many of our operations are IO heavy, like refresh or flush, the cost of a thread in today OS is light, compared to doing blocking IO for the actual operation. Having bulk operations compete with refresh doesn't sound right. Another example is completion suggester, that is supposed to provide results extremely fast, should it compete with "regular" search requests?

If we do think it is a problem, then we should come up with a better solution compared to folding all those thread pools. I am not sure what a better solution is compared to what we have today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release script: Set versions for non inherited projects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12938</link><project id="" key="" /><description>rest-api-spec and dev-tools dont have the elasticsearch-parent
set as a parent and thus need a separate mvn run to change the
plugin version.
</description><key id="101435349">12938</key><summary>Release script: Set versions for non inherited projects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T14:36:41Z</created><updated>2015-08-17T15:11:31Z</updated><resolved>2015-08-17T15:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-17T14:45:22Z" id="131846895">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] inject a random index to TestClusterService in BaseQueryTestCase#init</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12937</link><project id="" key="" /><description>Some of our next queries to refactor rely on some state taken from the cluster state. That is why we need to mock cluster service and inject an index to it, the index that we simulate the execution of the queries against. The best would be to have multiple indices actually, but that would make our setup a lot more complicated, especially given that IndexQueryParseService is still per index. We might be able to improve that in the future though, for now this is as good as it gets.
</description><key id="101433672">12937</key><summary>[TEST] inject a random index to TestClusterService in BaseQueryTestCase#init</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>test</label></labels><created>2015-08-17T14:29:31Z</created><updated>2015-08-18T07:43:36Z</updated><resolved>2015-08-18T07:30:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-08-17T16:34:21Z" id="131882616">This looks good. I would also add a method to get the name of the index so we can use it in say Indices Query.
</comment><comment author="javanna" created="2015-08-17T16:37:36Z" id="131883944">&gt; I would also add a method to get the name of the index so we can use it in say Indices Query.

Makes sense, I had forgotten about that, I pushed an update.
</comment><comment author="cbuescher" created="2015-08-17T17:13:42Z" id="131893185">Also did a quick look, only one suggestion, otherwise LGTM
</comment><comment author="javanna" created="2015-08-17T17:26:21Z" id="131896450">@cbuescher I pushed a new commit, hopefully sorted out index settings now. please have a look.
</comment><comment author="cbuescher" created="2015-08-17T17:55:58Z" id="131909348">LGTM
</comment><comment author="alexksikes" created="2015-08-17T18:12:17Z" id="131912947">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add build short hash to the download manager headers to identify staging builds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12936</link><project id="" key="" /><description>It might turn out to be useful to have the actual commit hash of the version we are
looking for if our download manager can just redirect to the right staging repository.
</description><key id="101419393">12936</key><summary>Add build short hash to the download manager headers to identify staging builds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T13:20:10Z</created><updated>2015-08-21T09:35:43Z</updated><resolved>2015-08-19T13:10:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-17T13:20:19Z" id="131816459">@spinscale FYI
</comment><comment author="spinscale" created="2015-08-19T13:05:15Z" id="132583917">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[snapshot]&#160;support multiple credentials in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12935</link><project id="" key="" /><description>Based on https://github.com/elastic/elasticsearch-cloud-azure/pull/93#issuecomment-131810017

We need to support in  `elasticsearch.yml` multiple credentials.
We could imagine that as a generic feature whatever repository type you want to use.

Let say that we can now create something like:

``` yml
cloud:
    azure:
        storage:
            azure1:
              account: your_azure_storage_account1
              key: your_azure_storage_key1
              default: true
            azure2:
              account: your_azure_storage_account2
              key: your_azure_storage_key2
            azure3:
              account: your_azure_storage_account3
              key: your_azure_storage_key3
```

Then when we create the repo, we can specify which credentials we want to use:

```
# use credentials 2
PUT _snapshot/my_backup2
{
  "type": "azure",
  "settings": {
      "creadentials": "azure2",
      "container": "backup_container",
      "base_path": "backups"
  }
}

# This one will use the one marked as "default"
PUT _snapshot/my_backup3
{
  "type": "azure"
}
```
</description><key id="101416166">12935</key><summary>[snapshot]&#160;support multiple credentials in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2015-08-17T13:03:03Z</created><updated>2015-12-18T14:03:03Z</updated><resolved>2015-12-18T14:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-18T14:03:03Z" id="165784780">This has been fixed by #13779
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManagerUnitTests.testSimplifiedNaming fails due to an unexpected url</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12934</link><project id="" key="" /><description>See: http://build-us-00.elastic.co/job/es_g1gc_master_metal/15392/testReport/junit/org.elasticsearch.plugins/PluginManagerUnitTests/testSimplifiedNaming/

```
java.lang.AssertionError: 
Expected: is &lt;http://download.elastic.co/elasticsearch/staging/c3b3b0e/org/elasticsearch/plugin/elasticsearch-USdnUtkzOv/2.1.0/elasticsearch-USdnUtkzOv-2.1.0.zip&gt;
     but: was &lt;http://download.elastic.co/elasticsearch/staging/elasticsearch-2.1.0-c3b3b0e/org/elasticsearch/plugin/elasticsearch-USdnUtkzOv/2.1.0/elasticsearch-USdnUtkzOv-2.1.0.zip&gt;
    at __randomizedtesting.SeedInfo.seed([4F64EE4CD37244CE:251CB915160223E7]:0)
```

My guess this relates to the latest refactoring work on the build system.
</description><key id="101405576">12934</key><summary>PluginManagerUnitTests.testSimplifiedNaming fails due to an unexpected url</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>blocker</label><label>jenkins</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T12:01:25Z</created><updated>2015-08-17T12:35:17Z</updated><resolved>2015-08-17T12:35:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-17T12:35:17Z" id="131800444">This was fixed already but the commit (https://github.com/elastic/elasticsearch/commit/ee4bdf45423c52b2baf2e5b610122d97c9ce2687 ) wasn't cherry-picked to master. Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch bootstrap help shouldn't mention plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12933</link><project id="" key="" /><description>We have a dedicated entry point for that.
</description><key id="101403713">12933</key><summary>Elasticsearch bootstrap help shouldn't mention plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T11:48:33Z</created><updated>2015-08-18T16:57:14Z</updated><resolved>2015-08-17T12:37:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-17T11:48:53Z" id="131792675">@spinscale  can you take a look?
</comment><comment author="dadoonet" created="2015-08-17T11:55:35Z" id="131793670">+1
</comment><comment author="spinscale" created="2015-08-17T12:23:11Z" id="131798513">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>agg question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12932</link><project id="" key="" /><description>I create `two type` in a `index`, mapping as follows:

`userinfo` mapping

```
{
    "userinfo": {
        "type": "object",
        "properties": {
            "city": {
                "type": "string",
                "index": "not_analyzed",
                "store": true
            },
            "country": {
                "type": "string",
                "index": "not_analyzed",
                "store": true
            }
        }
    }
}
```

`wx` mapping

```
{
    "wx": {
        "properties": {
            "c": {
                "type": "integer",
                "store": true
            },
            "userinfo": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "index": "not_analyzed",
                        "store": true
                    },
                    "country": {
                        "type": "string",
                        "index": "not_analyzed",
                        "store": true
                    }
                }
            }
        }
    }
}
```

`type wx` field `userinfo` and `type userinfo` use the `same name`, then I use `aggs` in `wx` field `userinfo.country`, but don't get results

why??
</description><key id="101400597">12932</key><summary>agg question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feifeiiiiiiiiiii</reporter><labels /><created>2015-08-17T11:28:08Z</created><updated>2015-08-17T11:41:11Z</updated><resolved>2015-08-17T11:41:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-17T11:41:11Z" id="131791600">I suspect this is due to the fact elasticsearch allows types to be part of paths, which introduces ambiguities. For instance with "userinfo.country", it's not clear whether it is the country field of the userinfo type or the userinfo.country field of the wx type. In 2.x providing the type will not be supported anymore: https://github.com/elastic/elasticsearch/issues/8872
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move the `murmur3` field to a plugin and fix defaults.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12931</link><project id="" key="" /><description>This move the `murmur3` field to the `mapper-murmur3` plugin and fixes its
defaults so that values will not be indexed by default, as the only purpose
of this field is to speed up `cardinality` aggregations on high-cardinality
string fields, which only requires doc values.

I also removed the `rehash` option from the `cardinality` aggregation as it
doesn't bring much value (rehashing is cheap) and allowed to remove the
coupling between the `cardinality` aggregation and the `murmur3` field.

Close #12874
</description><key id="101399126">12931</key><summary>Move the `murmur3` field to a plugin and fix defaults.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T11:18:19Z</created><updated>2015-08-21T09:37:41Z</updated><resolved>2015-08-18T10:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-17T11:51:20Z" id="131793026">Can you add a test in `PluginManagerIT#testOfficialPluginName_ThrowsException()`. I'm adding ATM one for `mapper-size` which is missing.
</comment><comment author="clintongormley" created="2015-08-17T12:30:02Z" id="131799472">Could you add a link to this plugin to https://raw.githubusercontent.com/elastic/elasticsearch/master/docs/reference/mapping/types.asciidoc as well please
</comment><comment author="jpountz" created="2015-08-17T15:43:10Z" id="131867672">@dadoonet @clintongormley I just pushed a new commit to address your comments.
</comment><comment author="rjernst" created="2015-08-17T18:58:50Z" id="131929853">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Preregistered templates ignores custom es.config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12930</link><project id="" key="" /><description>While [the docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#pre-registered-templates) refers simply to `config/scripts`, it doesn't actually use `es.config` if you pass that in yourself, and instead relies on `$ES_HOME/config/scripts`.

Here's an example, even with a valid script (ie not an empty one) it just ignores the config path;

```
~/Temp$ ll config/scripts/
total 0
-rw-r--r--  1 markw  staff     0B 17 Aug 20:42 test.mustache

$ elasticsearch-1.7.1/bin/elasticsearch -Des.config=/Users/markw/Temp/blank.yml -Xmx2g -Xms2g
[2015-08-17 20:42:04,643][INFO ][node                     ] [new] version[1.7.1], pid[91275], build[b88f43f/2015-07-29T09:54:16Z]
[2015-08-17 20:42:04,644][INFO ][node                     ] [new] initializing ...
[2015-08-17 20:42:04,730][INFO ][plugins                  ] [new] loaded [], sites []
[2015-08-17 20:42:04,780][INFO ][env                      ] [new] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [215.8gb], net total_space [464.8gb], types [hfs]
[2015-08-17 20:42:07,378][INFO ][node                     ] [new] initialized
[2015-08-17 20:42:07,378][INFO ][node                     ] [new] starting ...
[2015-08-17 20:42:07,540][INFO ][transport                ] [new] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.69.27:9300]}
[2015-08-17 20:42:07,563][INFO ][discovery                ] [new] new/mBoQzAkHT_OGnkghoCAeUg
[2015-08-17 20:42:10,584][INFO ][cluster.service          ] [new] new_master [new][mBoQzAkHT_OGnkghoCAeUg][bender.local][inet[/192.168.69.27:9300]], reason: zen-disco-join (elected_as_master)
[2015-08-17 20:42:10,611][INFO ][http                     ] [new] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.69.27:9200]}
[2015-08-17 20:42:10,611][INFO ][node                     ] [new] started
[2015-08-17 20:42:10,625][INFO ][gateway                  ] [new] recovered [0] indices into cluster_state
```

You can see it ignores the scripts

Whereas if I put the script into `$ES_HOME/config/scripts`;

```
$ ll elasticsearch-1.7.1/config/scripts/
total 0
-rw-r--r--  1 markw  staff     0B 17 Aug 20:43 test.mustache
$ elasticsearch-1.7.1/bin/elasticsearch -Des.config=/Users/markw/Temp/blank.yml -Xmx2g -Xms2g
[2015-08-17 20:44:12,556][INFO ][node                     ] [new] version[1.7.1], pid[91325], build[b88f43f/2015-07-29T09:54:16Z]
[2015-08-17 20:44:12,557][INFO ][node                     ] [new] initializing ...
[2015-08-17 20:44:12,634][INFO ][plugins                  ] [new] loaded [], sites []
[2015-08-17 20:44:12,685][INFO ][env                      ] [new] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [215.8gb], net total_space [464.8gb], types [hfs]
[2015-08-17 20:44:14,611][INFO ][script                   ] [new] compiling script file [/Users/markw/Workspace/elastic/elasticsearch-1.7.1/config/scripts/test.mustache]
[2015-08-17 20:44:14,877][INFO ][node                     ] [new] initialized
[2015-08-17 20:44:14,877][INFO ][node                     ] [new] starting ...
[2015-08-17 20:44:15,026][INFO ][transport                ] [new] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.69.27:9300]}
[2015-08-17 20:44:15,042][INFO ][discovery                ] [new] new/B13XeJjYSoaOMhRqwlb5Ug
[2015-08-17 20:44:18,066][INFO ][cluster.service          ] [new] new_master [new][B13XeJjYSoaOMhRqwlb5Ug][bender.local][inet[/192.168.69.27:9300]], reason: zen-disco-join (elected_as_master)
[2015-08-17 20:44:18,090][INFO ][http                     ] [new] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.69.27:9200]}
[2015-08-17 20:44:18,090][INFO ][node                     ] [new] started
[2015-08-17 20:44:18,107][INFO ][gateway                  ] [new] recovered [0] indices into cluster_state
^C[2015-08-17 20:44:19,251][INFO ][node                     ] [new] stopping ...
[2015-08-17 20:44:19,265][INFO ][node                     ] [new] stopped
[2015-08-17 20:44:19,265][INFO ][node                     ] [new] closing ...
[2015-08-17 20:44:19,270][INFO ][node                     ] [new] closed
```

But should we be assuming that es.config is where the scripts should live, or should we maybe have a `path.scripts` setting?

I'll raise another ticket to get the docs updated to add in the `$ES_HOME` as that should clarify things in the meantime.
</description><key id="101395161">12930</key><summary>Preregistered templates ignores custom es.config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2015-08-17T10:50:37Z</created><updated>2015-11-17T14:21:23Z</updated><resolved>2015-11-17T14:21:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-17T12:26:05Z" id="131798922">I think we should remove the `-Des.config` option.  It points just to the config file itself, not the config directory.  If the specified path is relative then it looks for it under `path.config`.

Setting `--path.config` is the correct way to specify a custom config dir
</comment><comment author="dadoonet" created="2015-11-12T15:59:34Z" id="156148092">`-Des.config` has been removed and is not supported anymore. If you define it, then elasticsearch just stops with:

```
[2015-11-12 16:58:50,307][INFO ][bootstrap                ] es.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.
```

Should we close this issue?
</comment><comment author="clintongormley" created="2015-11-17T14:21:23Z" id="157383518">Yes, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make platform specific assumptions in OS &amp; Process probes  tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12929</link><project id="" key="" /><description>This pull request changes the tests assumptions in `OSProbeTests` and `ProcessProbeTests` in order to be more platform specific (at least for Windows) when checking the stats results.

There is good chance that some tests fail on some platforms/JVM but it's a good thing since it will help us to know were stats are available.

It also fixed the test  &#768;ProcessProbeTests.testProcessStats` [that failed on CI](http://build-us-00.elastic.co/job/elasticsearch-20-win2012r2/14/console) with the following error:

```
  &gt; Expected: a value equal to or greater than &lt;0s&gt;
   &gt;      but: &lt;-1s&gt; was less than &lt;0s&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([2C939BD6917275D4:72B58150CBA0DEB7]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.monitor.process.ProcessProbeTests.testProcessStats(ProcessProbeTests.java:49)
```

Because CPU percent can be negative if the system recent CPU usage is not available and the test checks that it was greater or equal to 0. This commit fix this.
</description><key id="101391931">12929</key><summary>Make platform specific assumptions in OS &amp; Process probes  tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T10:37:06Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-17T13:32:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-17T10:50:59Z" id="131778773">LGTM thanks
</comment><comment author="s1monw" created="2015-08-17T10:51:15Z" id="131778861">this can go into 2.0?
</comment><comment author="tlrx" created="2015-08-17T11:43:03Z" id="131791880">@s1monw thanks for your review. Yes it can go in 2.0, we just have to know that some tests might fail and might require some test code adaptation. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to install license shield</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12928</link><project id="" key="" /><description>I want to update my shield because time it's expiry
when I run 
`
curl -XPUT -u admin 'http://localhost:9200/_licenses' -d @license.json
`
I have message like this
`{"error":"ElasticsearchParseException[Failed to derive xcontent from (offset=0, length=0): []]","status":400}`
</description><key id="101388165">12928</key><summary>How to install license shield</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tommoholmes10</reporter><labels /><created>2015-08-17T10:17:47Z</created><updated>2015-08-17T11:47:23Z</updated><resolved>2015-08-17T11:47:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-17T10:19:20Z" id="131767688">You should really chat to the Elastic support team about this, because assistance for installing licensed plugins is covered with your subscription :)

I'll see if any Shield devs can assist though.
</comment><comment author="tommoholmes10" created="2015-08-17T10:25:37Z" id="131768988">Okee thanks
</comment><comment author="markwalkom" created="2015-08-17T10:31:04Z" id="131771383">I've asked someone from the Shield team to drop in, however I would encourage you to contact the Support team ASAP to get this resolved, especially if your license is expired.
</comment><comment author="jaymode" created="2015-08-17T10:51:16Z" id="131778883">@tommoholmes10 is your license file empty? I was able to reproduce the exception you got with an empty license file, which is to be expected.
</comment><comment author="markwalkom" created="2015-08-17T11:47:23Z" id="131792467">It looks like you have also raised this on our forums - https://discuss.elastic.co/t/license-shield/27493/3

I'll close this as Github is for bugs or feature requests, not support.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: remove aws-maven plugin/add docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12927</link><project id="" key="" /><description>In order to have consistent deploys across several repositories,
we should deploy to sonatype first, then mirror those contents,
and then upload to s3.

This means, the aws wagon is not needed anymore.
</description><key id="101381618">12927</key><summary>Release: remove aws-maven plugin/add docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T09:46:12Z</created><updated>2015-08-17T13:39:59Z</updated><resolved>2015-08-17T13:39:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-17T13:29:34Z" id="131818417">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards stuck in INITIALIZING</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12926</link><project id="" key="" /><description>I have a fairly large index (~3.5 million documents, with nested fields, some of the docs are quite large) where the documents are being updated a lot.

A while ago the cluster went into the yellow state and I'm seeing a number of shards stuck in the "INITIALIZING" state:

```
joint_user_summary_v1                   0 r INITIALIZING                   127.0.1.1 cubitsearch-1
joint_user_summary_v1                   7 r INITIALIZING                   127.0.1.1 cubitsearch-5
joint_user_summary_v1                   7 r INITIALIZING                   127.0.1.1 cubitsearch-2
joint_user_summary_v1                   3 r INITIALIZING                   127.0.1.1 cubitsearch-1
joint_user_summary_v1                   1 r INITIALIZING                   127.0.1.1 cubitsearch-4
joint_user_summary_v1                   1 r INITIALIZING                   127.0.1.1 cubitsearch-3
joint_user_summary_v1                   5 r INITIALIZING                   127.0.1.1 cubitsearch-2
joint_user_summary_v1                   6 r INITIALIZING                   127.0.1.1 cubitsearch-3
joint_user_summary_v1                   6 r INITIALIZING                   127.0.1.1 cubitsearch-5
```

Naturally, I can't optimize this index anymore:

```
{"_shards":{"total":30,"successful":5,"failed":6,"failures":[{"index":"joint_user_summary_v1","shard":0,"status":500,"reason":"BroadcastShardOperationFailedException[[joint_user_summary_v1][0] ]; nested: RemoteTransportException[[cubitsearch-2][inet[/10.0.1.236:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][0] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][0] recovery is in progress, flush is not allowed]; "},{"index":"joint_user_summary_v1","shard":1,"status":500,"reason":"BroadcastShardOperationFailedException[[joint_user_summary_v1][1] ]; nested: RemoteTransportException[[cubitsearch-2][inet[/10.0.1.236:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][1] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][1] recovery is in progress, flush is not allowed]; "},{"index":"joint_user_summary_v1","shard":3,"status":500,"reason":"BroadcastShardOperationFailedException[[joint_user_summary_v1][3] ]; nested: RemoteTransportException[[cubitsearch-3][inet[/10.0.1.237:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][3] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][3] recovery is in progress, flush is not allowed]; "},{"index":"joint_user_summary_v1","shard":5,"status":500,"reason":"BroadcastShardOperationFailedException[[joint_user_summary_v1][5] ]; nested: RemoteTransportException[[cubitsearch-1][inet[/10.0.1.235:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][5] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][5] recovery is in progress, flush is not allowed]; "},{"index":"joint_user_summary_v1","shard":6,"status":500,"reason":"BroadcastShardOperationFailedException[[joint_user_summary_v1][6] ]; nested: RemoteTransportException[[cubitsearch-1][inet[/10.0.1.235:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][6] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][6] recovery is in progress, flush is not allowed]; "},{"index":"joint_user_summary_v1","shard":7,"status":500,"reason":"BroadcastShardOperationFailedException[[joint_user_summary_v1][7] ]; nested: RemoteTransportException[[cubitsearch-3][inet[/10.0.1.237:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][7] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][7] recovery is in progress, flush is not allowed]; "}]}}
```

Here's an excerpt from the log file that may shed some light on the issue:

```
Aug 17 09:06:31 cubitsearch-3 elasticsearch: [2015-08-17 09:06:31,525][DEBUG][action.bulk              ] [cubitsearch-3] observer: timeout notification from cluster service. timeout setting [60ms], time since start [60ms]
Aug 17 09:06:31 cubitsearch-3 elasticsearch: [2015-08-17 09:06:31,525][DEBUG][action.bulk              ] [cubitsearch-3] observer: timeout notification from cluster service. timeout setting [60ms], time since start [60ms]
Aug 17 09:06:31 cubitsearch-3 elasticsearch: [2015-08-17 09:06:31,939][WARN ][indices.cluster          ] [cubitsearch-3] [[joint_user_summary_v1][1]] marking and sending shard failed due to [failed recovery]
Aug 17 09:06:31 cubitsearch-3 elasticsearch: org.elasticsearch.indices.recovery.RecoveryFailedException: [joint_user_summary_v1][1]: Recovery failed from [cubitsearch-2][U2eMGyQ7Rhic1lHqzIoLXA][cubitsearch-2][inet[/10.0.1.236:9300]]{max_local_storage_nodes=1, master=false} into [cubitsearch-3][IEP7xQczReKinA78HfLC3Q][cubitsearch-3][inet[/10.0.1.237:9300]]{max_local_storage_nodes=1, master=false}
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: Caused by: org.elasticsearch.transport.RemoteTransportException: [cubitsearch-2][inet[/10.0.1.236:9300]][internal:index/shard/recovery/start_recovery]
Aug 17 09:06:31 cubitsearch-3 elasticsearch: Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [joint_user_summary_v1][1] Phase[2] Execution failed
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:902)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: Caused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [cubitsearch-3][inet[/10.0.1.237:9300]][internal:index/shard/recovery/prepare_translog] request_id [14280416] timed out after [900000ms]
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
Aug 17 09:06:31 cubitsearch-3 elasticsearch: #011... 3 more
```

I was hoping that upgrading to the latest version (1.7.1) would fix the problem, but unfortunately it persists.
</description><key id="101375198">12926</key><summary>Shards stuck in INITIALIZING</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">krisb78</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2015-08-17T09:11:46Z</created><updated>2015-08-17T13:02:04Z</updated><resolved>2015-08-17T12:54:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-17T10:59:57Z" id="131781507">Are you setting the cluster timeout somewhere? I'm seeing `timeout setting [60ms]` while the timeout in the code is 60 seconds.  Also, what logs are you seeing in the other nodes (eg `cubitsearch-2`) about why recovery is failing?
</comment><comment author="krisb78" created="2015-08-17T11:49:17Z" id="131792732">I'm setting the `timeout` param on bulk requests to "60", I'm assuming it's coming from there.

Here's what I see in the logs on cubitsearch-2:

```
Aug 17 11:36:12 cubitsearch-2 elasticsearch: [2015-08-17 11:36:12,191][WARN ][indices.cluster          ] [cubitsearch-2] [[joint_user_summary_v1][5]] marking and sending shard failed due to [failed recovery]
Aug 17 11:36:12 cubitsearch-2 elasticsearch: org.elasticsearch.indices.recovery.RecoveryFailedException: [joint_user_summary_v1][5]: Recovery failed from [cubitsearch-1][-nlEhC5-SliQ_REVJm1FWA][cubitsearch-1][inet[/10.0.1.235:9300]]{max_local_storage_nodes=1, master=false} into [cubitsearch-2][U2eMGyQ7Rhic1lHqzIoLXA][cubitsearch-2][inet[/10.0.1.236:9300]]{max_local_storage_nodes=1, master=false}
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: Caused by: org.elasticsearch.transport.RemoteTransportException: [cubitsearch-1][inet[/10.0.1.235:9300]][internal:index/shard/recovery/start_recovery]
Aug 17 11:36:12 cubitsearch-2 elasticsearch: Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [joint_user_summary_v1][5] Phase[2] Execution failed
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:902)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: Caused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [cubitsearch-2][inet[/10.0.1.236:9300]][internal:index/shard/recovery/prepare_translog] request_id [13236725] timed out after [900000ms]
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
Aug 17 11:36:12 cubitsearch-2 elasticsearch: #011... 3 more
Aug 17 11:36:19 cubitsearch-2 elasticsearch: [2015-08-17 11:36:19,768][WARN ][indices.cluster          ] [cubitsearch-2] [[joint_user_summary_v1][5]] marking and sending shard failed due to [failed to create shard]
Aug 17 11:36:19 cubitsearch-2 elasticsearch: org.elasticsearch.index.shard.IndexShardCreationException: [joint_user_summary_v1][5] failed to create shard
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:196)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:162)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1][5], timed out after 5000ms
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
Aug 17 11:36:19 cubitsearch-2 elasticsearch: #011... 9 more
```
</comment><comment author="krisb78" created="2015-08-17T12:00:17Z" id="131794796">Also, I changed the timeout on these bulk requests to `60 * 1000` and am now seeing:

```
Aug 17 11:59:15 cubitsearch-2 elasticsearch: [2015-08-17 11:59:15,210][DEBUG][action.bulk              ] [cubitsearch-2] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
Aug 17 11:59:19 cubitsearch-2 elasticsearch: [2015-08-17 11:59:19,214][DEBUG][action.bulk              ] [cubitsearch-2] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
Aug 17 11:59:19 cubitsearch-2 elasticsearch: [2015-08-17 11:59:19,214][DEBUG][action.bulk              ] [cubitsearch-2] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
```
</comment><comment author="krisb78" created="2015-08-17T12:03:57Z" id="131795301">Every now and then I'm also seeing things like:

```
Aug 17 12:02:10 cubitsearch-2 elasticsearch: [2015-08-17 12:02:10,262][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1799364ms] ago, timed out [899363ms] ago, action [internal:index/shard/recovery/prepare_translog], node [[cubitsearch-3][IEP7xQczReKinA78HfLC3Q][cubitsearch-3][inet[/10.0.1.237:9300]]{max_local_storage_nodes=1, master=false}], id [15135853]
```

```
Aug 17 12:01:45 cubitsearch-2 elasticsearch: [2015-08-17 12:01:45,397][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1374518ms] ago, timed out [474517ms] ago, action [internal:index/shard/recovery/prepare_translog], node [[cubitsearch-1][-nlEhC5-SliQ_REVJm1FWA][cubitsearch-1][inet[/10.0.1.235:9300]]{max_local_storage_nodes=1, master=false}], id [15166776]
```
</comment><comment author="clintongormley" created="2015-08-17T12:33:00Z" id="131800079">You're setting it to 60 milliseconds, not 60 seconds. Try `60s`.
</comment><comment author="krisb78" created="2015-08-17T12:50:02Z" id="131806213">Yep, done that, please see above.
</comment><comment author="clintongormley" created="2015-08-17T12:54:37Z" id="131807239">ok, looks like the root of the problem is here:

```
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1][5], timed out after 5000ms
```

This looks like a duplicate of #12011.  I think the only thing you can do here is to restart your nodes (one by one).  We're still trying to figure out what is causing this issue.
</comment><comment author="krisb78" created="2015-08-17T13:02:04Z" id="131810371">I did restart all of them a couple of times, but it didn't help...

Thanks for your prompt responses though, I'll be keeping an eye on https://github.com/elastic/elasticsearch/issues/12011
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>common.network failed to resolve localhost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12925</link><project id="" key="" /><description>in linux:

[root@Search bin]# ./elasticsearch
[2015-08-17 16:39:06,294][INFO ][node                     ] [Vivisector] version[1.7.1], pid[8548], build[b88f43f/2015-07-29T09:54:16Z]
[2015-08-17 16:39:06,300][INFO ][node                     ] [Vivisector] initializing ...
[2015-08-17 16:39:06,451][INFO ][plugins                  ] [Vivisector] loaded [analysis-ik], sites [head, kopf]
[2015-08-17 16:39:06,484][INFO ][env                      ] [Vivisector] using [1] data paths, mounts [[/ (/dev/sda3)]], net usable_space [4.6gb], net total_space [8.6gb], types [ext4]
[2015-08-17 16:39:08,434][WARN ][common.network           ] failed to resolve local host, fallback to loopback
java.net.UnknownHostException: Search: Search: unknown error
    at java.net.InetAddress.getLocalHost(InetAddress.java:1505)
    at org.elasticsearch.common.network.NetworkUtils.&lt;clinit&gt;(NetworkUtils.java:55)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.&lt;init&gt;(NettyHttpServerTransport.java:165)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:210)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:77)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:245)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.net.UnknownHostException: Search: unknown error
</description><key id="101371539">12925</key><summary>common.network failed to resolve localhost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wadekun</reporter><labels /><created>2015-08-17T08:48:38Z</created><updated>2015-08-17T09:14:33Z</updated><resolved>2015-08-17T09:14:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wadekun" created="2015-08-17T08:55:33Z" id="131735739">i forget to modify the hosts............
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Replace python search/replace with mvn versions:set plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12924</link><project id="" key="" /><description>mvn has a versions:set plugin, that can be easily invoked and does not
require the python script to parse the files and hope that there are no
other snapshot mentions.
</description><key id="101354717">12924</key><summary>Release: Replace python search/replace with mvn versions:set plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T07:16:35Z</created><updated>2015-08-18T16:55:12Z</updated><resolved>2015-08-17T09:48:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-17T07:32:27Z" id="131706078">LGTM
</comment><comment author="dadoonet" created="2015-08-17T07:33:12Z" id="131706346">Fantastic! I thought it was a feature built in release plugin only!
+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The differences between filter and query?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12923</link><project id="" key="" /><description>There are some same points between filter and query when I use Java api. What are the differences between them? When we send the same request, which one can return the response faster?
</description><key id="101343227">12923</key><summary>The differences between filter and query?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KennFalcon</reporter><labels /><created>2015-08-17T05:38:40Z</created><updated>2015-08-17T05:40:57Z</updated><resolved>2015-08-17T05:40:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-17T05:40:57Z" id="131685678">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor, remove _node/network and _node/stats/network. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12922</link><project id="" key="" /><description>Closes #12889
</description><key id="101332007">12922</key><summary>Refactor, remove _node/network and _node/stats/network. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T03:24:35Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-17T12:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-08-17T12:44:14Z" id="131803577">LGTM. Thx. I'll merge it in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flatten IndicesModule and add tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12921</link><project id="" key="" /><description>The IndicesModule was made up of two submodules, one which handled registering queries, and the other for registering hunspell dictionaries. This change moves those into IndicesModule. It also adds a new extension point type, InstanceMap. This is simply a Map&lt;K,V&gt;, where K and V are actual objects, not classes like most other extension points. I also added a test method to help testing instance map extensions. This was particularly painful because of how guice binds the key and value as separate bindings, and then reconstitutes them into a Map at injection time. In order to gain access to the object which links the key and value, I had to tweak our guice copy to not use an anonymous inner class for the Provider.

Note that I also renamed the existing extension point types, since they were very redundant. For example, ExtensionPoint.MapExtensionPoint is now ExtensionPoint.ClassMap.

See #12783.
</description><key id="101321365">12921</key><summary>Flatten IndicesModule and add tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-17T02:07:30Z</created><updated>2015-08-21T09:08:02Z</updated><resolved>2015-08-17T19:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-17T07:34:18Z" id="131706684">LGTM
</comment><comment author="rjernst" created="2015-08-17T19:08:34Z" id="131932325">I will backport to the 2.0 branch once the beta is out.
</comment><comment author="javanna" created="2015-08-18T12:11:33Z" id="132188489">@rjernst when you backport don't forget about #12956 ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2 nodes using same configuration file.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12920</link><project id="" key="" /><description>Hi, I'm learning elasticsearch and trying to run multiple nodes or 2 services of elasticsearch. the both work smoothly but the both use the same config file even I already point the another one to use different config file.

```
{
"name": "Node client balancer",
"transport_address": "inet[/127.0.0.1:9301]",
"host": "mockie-notebook",
"ip": "127.0.1.1",
"version": "1.6.0",
"build": "cdd3ac4",
"http_address": "inet[/127.0.0.1:9201]",
"attributes": {
"master": "true"
},
"settings": {
"pidfile": "/var/run/elasticsearch1.pid",
"path": {
"conf": "/etc/elasticsearch",
"data": "/var/lib/elasticsearch1",
"logs": "/var/log/elasticsearch1",
"work": "/tmp/elasticsearch1",
"home": "/usr/share/elasticsearch"
},
"cluster": {
"name": "elasticsearch"
},
"node": {
"name": "Node client balancer",
"data": "true",
"master": "true"
},
"name": "Node client balancer",
"http": {
"enabled": "true"
},
"client": {
"type": "node"
},
"config.ignore_system_properties": "true",
"config": "/etc/elasticsearch/elasticsearch1.yml",
"network": {
"host": "127.0.0.1",
"bind_host": "127.0.0.1",
"publish_host": "127.0.0.1"
}
},
"os": {
"refresh_interval_in_millis": 1000,
"available_processors": 4,
"cpu": {
"vendor": "Intel",
"model": "Core(TM) i3-4030U CPU @ 1.90GHz",
"mhz": 1900,
"total_cores": 4,
"total_sockets": 4,
"cores_per_socket": 16,
"cache_size_in_bytes": 3072
},
"mem": {
"total_in_bytes": 4023095296
},
"swap": {
"total_in_bytes": 2998923264
}
},
"process": {
"refresh_interval_in_millis": 1000,
"id": 5200,
"max_file_descriptors": 65535,
"mlockall": false
},
"jvm": {
"pid": 5200,
"version": "1.8.0_45",
"vm_name": "Java HotSpot(TM) 64-Bit Server VM",
"vm_version": "25.45-b02",
"vm_vendor": "Oracle Corporation",
"start_time_in_millis": 1439766605548,
"mem": {
"heap_init_in_bytes": 268435456,
"heap_max_in_bytes": 1038876672,
"non_heap_init_in_bytes": 2555904,
"non_heap_max_in_bytes": 0,
"direct_max_in_bytes": 1038876672
},
"gc_collectors": [
"ParNew"
,
"ConcurrentMarkSweep"
],
"memory_pools": [
"Code Cache"
,
"Metaspace"
,
"Compressed Class Space"
,
"Par Eden Space"
,
"Par Survivor Space"
,
"CMS Old Gen"
]
},
"thread_pool": {
"percolate": {
"type": "fixed",
"min": 4,
"max": 4,
"queue_size": "1k"
},
"fetch_shard_started": {
"type": "scaling",
"min": 1,
"max": 8,
"keep_alive": "5m",
"queue_size": -1
},
"listener": {
"type": "fixed",
"min": 2,
"max": 2,
"queue_size": -1
},
"index": {
"type": "fixed",
"min": 4,
"max": 4,
"queue_size": "200"
},
"refresh": {
"type": "scaling",
"min": 1,
"max": 2,
"keep_alive": "5m",
"queue_size": -1
},
"suggest": {
"type": "fixed",
"min": 4,
"max": 4,
"queue_size": "1k"
},
"generic": {
"type": "cached",
"keep_alive": "30s",
"queue_size": -1
},
"warmer": {
"type": "scaling",
"min": 1,
"max": 2,
"keep_alive": "5m",
"queue_size": -1
},
"search": {
"type": "fixed",
"min": 7,
"max": 7,
"queue_size": "1k"
},
"flush": {
"type": "scaling",
"min": 1,
"max": 2,
"keep_alive": "5m",
"queue_size": -1
},
"optimize": {
"type": "fixed",
"min": 1,
"max": 1,
"queue_size": -1
},
"fetch_shard_store": {
"type": "scaling",
"min": 1,
"max": 8,
"keep_alive": "5m",
"queue_size": -1
},
"management": {
"type": "scaling",
"min": 1,
"max": 5,
"keep_alive": "5m",
"queue_size": -1
},
"get": {
"type": "fixed",
"min": 4,
"max": 4,
"queue_size": "1k"
},
"merge": {
"type": "scaling",
"min": 1,
"max": 2,
"keep_alive": "5m",
"queue_size": -1
},
"bulk": {
"type": "fixed",
"min": 4,
"max": 4,
"queue_size": "50"
},
"snapshot": {
"type": "scaling",
"min": 1,
"max": 2,
"keep_alive": "5m",
"queue_size": -1
}
},
"network": {
"refresh_interval_in_millis": 5000,
"primary_interface": {
"address": "192.168.1.112",
"name": "wlan0",
"mac_address": "5C:93:A2:BE:EC:60"
}
},
"transport": {
"bound_address": "inet[/127.0.0.1:9301]",
"publish_address": "inet[/127.0.0.1:9301]",
"profiles": { }
},
"http": {
"bound_address": "inet[/127.0.0.1:9201]",
"publish_address": "inet[/127.0.0.1:9201]",
"max_content_length_in_bytes": 104857600
},
"plugins": [
{
"name": "head",
"version": "NA",
"description": "No description found.",
"url": "/_plugin/head/",
"jvm": false,
"site": true
}
]
}
```

as you can see above : 

```
"config.ignore_system_properties": "true",
"config": "/etc/elasticsearch/elasticsearch1.yml",
```

but it seems the node ignore `/etc/elasticsearch/elasticsearch1.yml` file and keep using `/etc/elasticsearch/elasticsearch.yml`. I'm using ubuntu and this is my service file https://gist.github.com/anonymous/0b8e21b46909b46f2cab
</description><key id="101307945">12920</key><summary>2 nodes using same configuration file.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mockiemockiz</reporter><labels /><created>2015-08-16T23:17:35Z</created><updated>2015-08-16T23:18:57Z</updated><resolved>2015-08-16T23:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-16T23:18:34Z" id="131638501">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Endless recovery loop with `indices.recovery.file_chunk_size=0Bytes`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12919</link><project id="" key="" /><description>This is caused by sending the same file to the chunk handler with offset
`0` which in-turn opens a new outputstream and waits for bytes. But the next round
will send 0 bytes again with offset 0. This commit adds some checks / validators that those
settings are positive byte values and fixes the RecoveryStatus to throw an IAE if the same file
is opened twice.
</description><key id="101267062">12919</key><summary>Endless recovery loop with `indices.recovery.file_chunk_size=0Bytes`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-16T14:04:15Z</created><updated>2015-08-18T16:54:14Z</updated><resolved>2015-08-17T07:59:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-16T14:05:14Z" id="131554250">here are two test that triggered that behavior:
- http://build-us-00.elastic.co/job/es_core_master_centos/6581/
- http://build-us-00.elastic.co/job/elasticsearch-20-metal/82/
</comment><comment author="jpountz" created="2015-08-16T14:54:12Z" id="131569408">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Small code Refactor - shard variable dependency from processFirstPhaseResults as shard is no more needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12918</link><project id="" key="" /><description>this is a small commit to remove the
shard variable dependency from processFirstPhaseResults as shard is no more
needed here . it only deals with the results obtained from the synchronous search on each shard.
It is [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java#L361)
</description><key id="101255316">12918</key><summary>Small code Refactor - shard variable dependency from processFirstPhaseResults as shard is no more needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HarishAtGitHub</reporter><labels /><created>2015-08-16T11:43:46Z</created><updated>2015-08-18T11:39:07Z</updated><resolved>2015-08-18T11:39:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HarishAtGitHub" created="2015-08-16T11:44:35Z" id="131530701">pull request : https://github.com/elastic/elasticsearch/pull/12917
</comment><comment author="jpountz" created="2015-08-18T11:39:07Z" id="132182676">Fixed via #12917
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor - shard variable dependency from processFirstPhaseResults as shard is no more needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12917</link><project id="" key="" /><description>this is a small commit to remove the
shard variable dependency from processFirstPhaseResults as shard is no more
needed here . it only deals with the results obtained from the synchronous search on each shard.
It is [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java#L361)
</description><key id="101255270">12917</key><summary>Refactor - shard variable dependency from processFirstPhaseResults as shard is no more needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HarishAtGitHub</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-08-16T11:42:33Z</created><updated>2015-08-18T16:53:36Z</updated><resolved>2015-08-16T14:21:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-16T14:21:15Z" id="131555748">merged thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flatten ClusterModule and add more tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12916</link><project id="" key="" /><description>The ClusterModule contained a couple submodules. This moves the
functionality from those modules into ClusterModule. Two of those
had to do with DynamicSettings. This change also cleans up
how DynamicSettings are built, and enforces they are added, with
validators, in ClusterModule.

See #12783
</description><key id="101242613">12916</key><summary>Flatten ClusterModule and add more tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-16T08:23:00Z</created><updated>2015-08-19T23:04:49Z</updated><resolved>2015-08-16T17:08:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-16T14:16:15Z" id="131554871">LGTM
</comment><comment author="rjernst" created="2015-08-16T17:09:26Z" id="131583893">I will backport to 2.0 branch after the beta is released.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>-Des.network.host=_eth0_ binds to ipv6 link-local address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12915</link><project id="" key="" /><description>Try this on two linux machines:

bin/elasticsearch -Des.network.host=_eth0_

You will see that it binds to a link-local ipv6 address: 

[2015-08-15 22:59:25,266][INFO ][org.elasticsearch.http   ] [Timberius] bound_address {inet[/fe80:0:0:0:f66d:4ff:fe90:ce0c%2:9200]}, publish_address {inet[/fe80:0:0:0:f66d:4ff:fe90:ce0c%eth0:9200]}

Nodes do get multicast packets from each other, but the address isn't going to work, because its link-local:

[2015-08-15 23:00:19,016][WARN ][org.elasticsearch.discovery.zen.ping.multicast] [Shamrock] failed to connect to requesting node [Bloodstorm][RVsIyNniTq6LYEQmTPRCEA][mac2][inet[/fe80:0:0:0:3e15:c2ff:fee5:d26c%4:9300]]
ConnectTransportException[[Bloodstorm][inet[/fe80:0:0:0:3e15:c2ff:fee5:d26c%4:9300]] connect_timeout[30s]]; nested: SocketException[Network is unreachable];

This makes it tricky to get things working since most machines are dual-stack and we are picking an address that won't go anywhere. Of course you can do -Des.network.host=_eth0:ipv4_ to workaround it. 
</description><key id="101228684">12915</key><summary>-Des.network.host=_eth0_ binds to ipv6 link-local address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>bug</label></labels><created>2015-08-16T03:02:18Z</created><updated>2015-08-18T11:18:02Z</updated><resolved>2015-08-17T19:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-16T17:33:25Z" id="131588135">Confirmed this locally, when I do `bin/elasticsearch -Des.network.host=_wlp3s0_` it binds to the link-local IPv6 address (`bound_address {inet[/fe80:0:0:0:8638:35ff:fe5e:93ce%2:9300]}`) even though this is not a loopback interface. Interestingly enough, it sends multicast **from** the IPv4 address for the `wlp3s0` interface, see:

```
&#187; sudo tcpdump -n -s0 -A -i wlp3s0 udp port 54328                                                                                                                                                            
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on wlp3s0, link-type EN10MB (Ethernet), capture size 262144 bytes
11:21:58.375184 IP 192.168.0.4.54328 &gt; 224.2.2.4.54328: UDP, length 116
E....}@...E-.........8.8.|...   ....z.....elasticsearch Protector.3oLX8kvNSb2wMCxlijXI_Q.Xanadu.domain.192.168.0.4.............85..^........$V...z
```

Since this affects 2.0, what do you think about defaulting `ES_USE_IPV4` to true for the 2.0.0 beta/ga (which fixes this issue for me) and addressing IPv6 fully going forward? I'm concerned about some things:
- People don't think about IPv6 much with firewalls yet, so if they bind to `eth0` and only protect IPv4, they could be in a bad situation (we should carefully test and document using IPv6 with ES)
- Connectivity problems due to a lack of testing coverage since every OS under the sun seems to do IPv6/multicast slightly different (as well as DNS issues with IPv6 also...)
- IPv6 with privacy extensions enabled causing development problems (haven't tested to see how it would affect this yet)
- Current lack of our test coverage for IPv6 in general

I think we could address the last one through the Vagrant testing that @nik9000 has been working on. We should make sure each machine has both the v4 and v6 stacks and we can bind/discover things correctly.

Thoughts?
</comment><comment author="nik9000" created="2015-08-16T17:38:49Z" id="131589338">We could absolutely add more tests around this with vagrant. Spin up a
couple of machines with a private network and have more bats tests. I don't
know if we can do osx vms though.
On Aug 16, 2015 10:33 AM, "Lee Hinman" notifications@github.com wrote:

&gt; Confirmed this locally, when I do bin/elasticsearch
&gt; -Des.network.host=_wlp3s0_ it binds to the link-local IPv6 address (bound_address
&gt; {inet[/fe80:0:0:0:8638:35ff:fe5e:93ce%2:9300]}) even though this is not a
&gt; loopback interface. Interestingly enough, it sends multicast _from_ the
&gt; IPv4 address for the wlp3s0 interface, see:
&gt; 
&gt; &#187; sudo tcpdump -n -s0 -A -i wlp3s0 udp port 54328
&gt; tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
&gt; listening on wlp3s0, link-type EN10MB (Ethernet), capture size 262144 bytes
&gt; 11:21:58.375184 IP 192.168.0.4.54328 &gt; 224.2.2.4.54328: UDP, length 116
&gt; E....}@...E-.........8.8.|...   ....z.....elasticsearch Protector.3oLX8kvNSb2wMCxlijXI_Q.Xanadu.domain.192.168.0.4.............85..^........$V...z
&gt; 
&gt; Since this affects 2.0, what do you think about defaulting ES_USE_IPV4 to
&gt; true for the 2.0.0 beta/ga (which fixes this issue for me) and addressing
&gt; IPv6 fully going forward? I'm concerned about some things:
&gt; - People don't think about IPv6 much with firewalls yet, so if they
&gt;   bind to eth0 and only protect IPv4, they could be in a bad situation
&gt;   (we should carefully test and document using IPv6 with ES)
&gt; - Connectivity problems due to a lack of testing coverage since every
&gt;   OS under the sun seems to do IPv6/multicast slightly different (as well as
&gt;   DNS issues with IPv6 also...)
&gt; - IPv6 with privacy extensions enabled causing development problems
&gt;   (haven't tested to see how it would affect this yet)
&gt; - Current lack of our test coverage for IPv6
&gt; 
&gt; I think we could address the last one through the Vagrant testing that
&gt; @nik9000 https://github.com/nik9000 has been working on. We should make
&gt; sure each machine has both the v4 and v6 stacks and we can bind/discover
&gt; things correctly.
&gt; 
&gt; Thoughts?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12915#issuecomment-131588135
&gt; .
</comment><comment author="dakrone" created="2015-08-16T17:40:45Z" id="131589781">We should also test IPv6-**only** machines to ensure we have no hard requirement on IPv4-specific behavior.
</comment><comment author="jasontedor" created="2015-08-16T17:48:51Z" id="131591616">We can technically do OS X VMs but we can't legally do OS X VMs until we provision some Mac hardware for our CI infrastructure and run OS X as the host OS. Tests utilizing these VMs can run legally only on this hardware.

From the [OS X 10.10 SLA](http://images.apple.com/legal/sla/docs/OSX1010.pdf):

&gt; B. Mac App Store License. If you obtained a license for the Apple Software from the Mac App Store, then subject to the terms and conditions of this License and as permitted by the Mac App Store Usage Rules set forth in the App Store Terms and Conditions (http://www.apple.com/legal/internet-services/itunes/ww/) (&#8220;Usage Rules&#8221;), you are granted a limited, non-transferable, non-exclusive license:
&gt; 
&gt; [...]
&gt; 
&gt; (iii) to install, use and run up to two (2) additional copies or instances of the Apple Software within virtual operating system environments on each Mac Computer you own or control that is already running the Apple Software, for purposes of: (a) software development; (b) testing during software development; (c) using OS X Server; or (d) personal, non-commercial use.
</comment><comment author="nik9000" created="2015-08-16T17:54:00Z" id="131593778">It looks like there are a couple of OSX boxes in vagrant atlas too. So that
should be possible.

Nik

On Sun, Aug 16, 2015 at 10:48 AM, Jason Tedor notifications@github.com
wrote:

&gt; We can technically do OS X VMs but we can't legally do OS X VMs until we
&gt; provision some Mac hardware for our CI infrastructure and run OS X as the
&gt; host OS. Tests utilizing these VMs could only run on this hardware.
&gt; 
&gt; From the OS X 10.10 EULA
&gt; http://images.apple.com/legal/sla/docs/OSX1010.pdf:
&gt; 
&gt; B. Mac App Store License. If you obtained a license for the Apple Software
&gt; from the Mac App Store, then subject to the terms and conditions of this
&gt; License and as permitted by the Mac App Store Usage Rules set forth in the
&gt; App Store Terms and Conditions (
&gt; http://www.apple.com/legal/internet-services/ itunes/ww/) (&#8220;Usage
&gt; Rules&#8221;), you are granted a limited, non-transferable, non-exclusive license:
&gt; 
&gt; [...]
&gt; 
&gt; (iii) to install, use and run up to two (2) additional copies or instances
&gt; of the Apple Software within virtual operating system environments on each
&gt; Mac Computer you own or control that is already running the Apple Software,
&gt; for purposes of: (a) software development; (b) testing during software
&gt; development; (c) using OS X Server; or (d) personal, non-commercial use.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12915#issuecomment-131591616
&gt; .
</comment><comment author="rmuir" created="2015-08-16T19:48:22Z" id="131612923">&gt; Since this affects 2.0, what do you think about defaulting ES_USE_IPV4 to true for the 2.0.0 beta/ga (which fixes this issue for me) and addressing IPv6 fully going forward?

One problem with that i simple stuff like `localhost` isn't going to work. Stuff like curl is simply not going to care. When I played with this issue, i ended out with all kinds of interesting situations: advertising 127.0.0.1 on ethernet multicast, advertising ipv6 over ipv4, etc. 

I think it was buggy all along but hidden by the fact that things were binding to all interfaces or whatever before?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multicast is sent over all interfaces by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12914</link><project id="" key="" /><description>Migration guide says:

Elasticsearch now binds to the loopback interface by default (usually
`127.0.0.1` or `::1`). The `network.host` setting can be specified to change
this behavior.

But this is a little confusing, because it still sends multicast packets over all interfaces (224.2.2.4). Instead, when we are only bound to loopback interface, I think we should use a link-local scoped address (e.g. 224.0.0.x). Ideally we'd use node-local if ipv4 had it (nodes bound to different loopback interfaces won't see each other). ipv6 can do node-local though.

Just startup another node on the same network with -Des.network.host=&lt;realip&gt; and you will see that it gets multicast packets from the other machine (of course things don't work, since its advertising localhost):

[2015-08-15 22:19:45,835][WARN ][org.elasticsearch.discovery.zen.ping.multicast] [Lorna Dane] failed to connect to requesting node [Locksmith][ywu92vT0Q0OYRNo-EmK01w][mac2][inet[/127.0.0.1:9300]]
ConnectTransportException[[Locksmith][inet[/127.0.0.1:9300]] connect_timeout[30s]]; nested: ConnectException[Connection refused: /127.0.0.1:9300];
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:827)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:760)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:733)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:234)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver$1.run(MulticastZenPing.java:537)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: /127.0.0.1:9300
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    ... 3 more

Try the same scenario with `bin/elasticsearch -Ddiscovery.zen.ping.multicast.group=224.0.0.200` and you can bring up multiple instances on one machine, but you won't see these pings from another machine. With mac at least there is an issue, and even changing the address here does not work...
</description><key id="101226869">12914</key><summary>multicast is sent over all interfaces by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>bug</label><label>jvm bug</label></labels><created>2015-08-16T02:46:25Z</created><updated>2015-08-24T10:26:06Z</updated><resolved>2015-08-19T11:58:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-16T03:45:10Z" id="131487614">Hmm i'm not sure its as simple as the address, and thats not working from OS X to OS X. 

Linux seems ok.

This problem seems to be OS X specific. I'm not sure the current logic around `discovery.zen.ping.multicast.defer_group_to_set_interface` to try to coerce OS x to do the right thing is working. Setting it to false doesn't change anything.
</comment><comment author="dakrone" created="2015-08-16T15:27:26Z" id="131572328">When I originally added `discovery.zen.ping.multicast.defer_group_to_set_interface` it was because though OSX would bind to `lo0`, it wouldn't send the multicast packet throw that interface (using `en0` instead) so it couldn't discover other nodes bound to 127.0.0.1. I verified the behavior with tcpdump, but perhaps it has been since fixed.
</comment><comment author="rmuir" created="2015-08-16T15:39:46Z" id="131575648">Yeah, its not behaving the same as linux machines: those seem fine. Keep in mind its tricky to verify with tcpdump, in most cases I think you need to join the multicast group for tcpdump to show the packets. Its easier to just bring up an ES node on another machine in the ethernet network and see if its getting multicast packets.
</comment><comment author="rmuir" created="2015-08-18T00:37:24Z" id="132005510">There is a JDK bug here.
When the address is a loopback address, we should set TTL=0, so that no packets leave the machine.
However, this does not work on OS X.

Its the exact same issue as https://bugs.openjdk.java.net/browse/JDK-6250763, just that was only fixed on Linux, not fixed on OS X, even with recent java 9 builds.

if i set TTL to 0, and try to send to an IPV4 address, I still send multicast packets out of my ethernet to other machines, which should not happen!

If I specify -Djava.net.preferIPv4Stack=true, then it works and TTL is correctly respected (but thats hardly a viable workaround).

You can test this easily with a small java or C program like this. Run an ES node on linux or something on the network because it will print errors if packets are sent:

```
public static void main(String args[]) throws Exception {
  MulticastSocket socket = new MulticastSocket();
  socket.setTimeToLive(0); // has no impact unless jvm flag -Djava.net.preferIPv4Stack=true 
  SocketAddress destination = new InetSocketAddress(InetAddress.getByName("224.2.2.4"),   54328);
  socket.send(new DatagramPacket("abcd".getBytes(), 4, destination));
  socket.close();
}
```
</comment><comment author="rmuir" created="2015-08-19T11:58:57Z" id="132561410">I'm closing this: nothing can be done except to fix the JVM. it must make the correct system calls.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate class before cast.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12913</link><project id="" key="" /><description>Let stats aggregation returns 400 error when performed over an invalid field

closes #12842
</description><key id="101215563">12913</key><summary>Validate class before cast.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">xuzha</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-08-15T22:44:32Z</created><updated>2015-08-17T10:44:33Z</updated><resolved>2015-08-17T07:38:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-15T22:45:15Z" id="131461042">Adding  class check for numeric and geo-point, probably not enough.
</comment><comment author="jpountz" created="2015-08-17T07:40:27Z" id="131709635">Thanks @xuzha I think this is a good start.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Querying on a date range</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12912</link><project id="" key="" /><description>I'm quite new to Elastic Search, and trying to store and visualize time-series data. I've removed Kibana from the equation, and I'm down with the following behavior that I can't explain.

A document is indexed with what seems to be a proper timestamp:

```
# curl elasticsearch:9200/[redacted]/[redacted]/[redacted]?fields=_timestamp | jq .
{
  "_index": "[redacted]",
  "_type": "[redacted]",
  "_id": "[redacted]",
  "_version": 1,
  "found": true,
  "fields": {
    "_timestamp": 1439673273000
  }
}
```

Querying with a date range which does include this document:

```
# cat query
{
  "size": 0,
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "analyze_wildcard": true,
          "query": "*"
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "_timestamp": {
                  "gte": 1439660028476,
                  "lte": 1439674428476
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  }
}
```

Gives me no result:

```
# curl -XGET elasticsearch:9200/[redacted]/[redacted]/_search -d @query | jq .
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": 0,
    "hits": []
  }
}
```

Relevant timestamps:
- Query lower bound `1439660028476` (Sat, 15 Aug 2015 17:33:48 GMT)
- Query upper bound `1439674428476` (Sat, 15 Aug 2015 21:33:48 GMT)
- Document timestamp `1439673273000`  (Sat, 15 Aug 2015 21:14:33 GMT)

Elastic Search is 1.7.1, running in PST timezone.
</description><key id="101213944">12912</key><summary>Querying on a date range</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">icecrime</reporter><labels /><created>2015-08-15T22:22:26Z</created><updated>2015-08-15T23:48:23Z</updated><resolved>2015-08-15T23:48:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="steeve" created="2015-08-15T22:26:09Z" id="131456733">does it work when you replace `query_string` with a `match_all` query ?

and/or when you remove the `range` filter ?
</comment><comment author="icecrime" created="2015-08-15T22:27:23Z" id="131456780">@steeve Mmm good point but no, same result.
</comment><comment author="icecrime" created="2015-08-15T22:30:05Z" id="131456863">Also something that might be relevant here: my mapping has

```
    "_timestamp": {
          "enabled": true,
          "store": true
        },
```
</comment><comment author="steeve" created="2015-08-15T22:31:33Z" id="131456914">Try and remove `store` from your mapping
</comment><comment author="dadoonet" created="2015-08-15T23:48:23Z" id="131469945">Please use discuss.elastic.co for questions. We'll be happy to help there.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Give maven modules nicer names, so they are more readable in the reactor summary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12911</link><project id="" key="" /><description>The reactor summary now looks like this:

```
[INFO] Build Resources ................................... SUCCESS [  0.490 s]
[INFO] Rest API Spec ..................................... SUCCESS [  0.214 s]
[INFO] Parent POM ........................................ SUCCESS [  2.899 s]
[INFO] Core .............................................. SUCCESS [ 18.169 s]
[INFO] Distribution: Parent POM .......................... SUCCESS [  0.447 s]
[INFO] Distribution: With all optional dependencies ...... SUCCESS [  0.363 s]
[INFO] Distribution: Shaded .............................. SUCCESS [  0.327 s]
[INFO] Distribution: TAR ................................. SUCCESS [  0.352 s]
[INFO] Distribution: ZIP ................................. SUCCESS [  0.341 s]
[INFO] Distribution: DEB ................................. SUCCESS [  0.930 s]
[INFO] Plugins: Parent POM ............................... SUCCESS [  0.596 s]
[INFO] Plugins: Analysis - Japanese (Kuromoji) ........... SUCCESS [  0.918 s]
[INFO] Plugins: Analysis - Smart Chinese ................. SUCCESS [  0.807 s]
[INFO] Plugins: Analysis - Polish (Stempel) .............. SUCCESS [  0.834 s]
[INFO] Plugins: Analysis - Phonetic ...................... SUCCESS [  1.031 s]
[INFO] Plugins: Analysis - ICU ........................... SUCCESS [  1.127 s]
[INFO] Plugins: Cloud - GCE .............................. SUCCESS [  1.068 s]
[INFO] Plugins: Cloud - Azure ............................ SUCCESS [  1.490 s]
[INFO] Plugins: Cloud - AWS .............................. SUCCESS [  1.225 s]
[INFO] Plugins: Script - Python .......................... SUCCESS [  0.912 s]
[INFO] Plugins: Script - Javascript ...................... SUCCESS [  0.997 s]
[INFO] Plugins: Mapper - _size ........................... SUCCESS [  0.870 s]
[INFO] Plugins: Delete By Query .......................... SUCCESS [  1.208 s]
[INFO] Plugins: Example - JVM ............................ SUCCESS [  0.863 s]
[INFO] Plugins: Example - Site ........................... SUCCESS [  0.169 s]
[INFO] QA: Parent POM .................................... SUCCESS [  0.155 s]
[INFO] QA: Smoke Test Plugins ............................ SUCCESS [  0.565 s]
[INFO] QA: Smoke Test Shaded Jar ......................... SUCCESS [  0.320 s]
[INFO] QA: Smoke Test Multi-Node IT ...................... SUCCESS [  0.202 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
```
</description><key id="101208625">12911</key><summary>Give maven modules nicer names, so they are more readable in the reactor summary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label></labels><created>2015-08-15T20:41:04Z</created><updated>2015-08-17T10:25:09Z</updated><resolved>2015-08-15T21:07:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-15T20:44:58Z" id="131446833">@rjernst I did the same within this PR: https://github.com/elastic/elasticsearch/pull/12879
</comment><comment author="rjernst" created="2015-08-15T20:47:23Z" id="131446922">@dadoonet It looks like that PR only has it renamed for plugins? I think this could go in first, we don't need to wait on artifact renames?
</comment><comment author="dadoonet" created="2015-08-15T21:04:20Z" id="131447591">I renamed distribution, plugins, dev tools...
</comment><comment author="rjernst" created="2015-08-15T21:07:47Z" id="131447836">Ok, I dont' care, I had only seen the first reactor summary there. I would make them more uniform though...like "Plugins: Parent POM" and the rest still starting with "Plugins"...
</comment><comment author="dadoonet" created="2015-08-15T21:16:20Z" id="131448731">Feel free to comment my PR for better names. I'll be happy to fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cmd /C needs to be quoted as a whole when starting integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12910</link><project id="" key="" /><description>To support spaces in both the command as well as its arguments `cmd /C` needs
be called like this:

```
cmd /C ""c:\a b\c.bat" "argument 1" "argument2""
```

Note the double quotes around the whole command line.

ant was running:

```
cmd /C "c:\a b\c.bat" "argument 1" "argument2"
```

which triggers `cmd /C`  to preprocess the command line to

```
cmd /C c:\a b\c.bat" "argument 1" "argument2
```

Which would make it appear as though ant was not properly quoting, which it was but just not according to `cmd /C`'s rules.

Closes #12848
</description><key id="101202653">12910</key><summary>cmd /C needs to be quoted as a whole when starting integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>build</label><label>v2.0.0-beta2</label></labels><created>2015-08-15T19:35:51Z</created><updated>2015-11-22T10:15:20Z</updated><resolved>2015-08-31T11:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-15T20:03:29Z" id="131438947">It makes sense and looks good to me.
</comment><comment author="s1monw" created="2015-08-16T14:16:45Z" id="131554888">LGTM too
</comment><comment author="clintongormley" created="2015-08-25T08:36:42Z" id="134523806">@Mpdreamz want to merge this and reenable tests with spaces (https://github.com/elastic/elasticsearch/issues/12848)?
</comment><comment author="rjernst" created="2015-08-25T08:55:31Z" id="134530750">IIRC, this get's the ant part working, but there were still integ test failures using spaces. But we should get this in so we can iterate on fixing the spaces problems in the windows bat.
</comment><comment author="Mpdreamz" created="2015-08-31T11:37:38Z" id="136344202">merged c18093d5a428e19c7d6545017e1dfce3a9b5418f in master and 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix variable substitution for OS's using systemd </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12909</link><project id="" key="" /><description>Systemd looks to be a bit less tolerant about $VAR than bash is. Replace
$VAR with ${VAR} in places in the systemd configuration file to get the
substitutions working.
</description><key id="101186484">12909</key><summary>Fix variable substitution for OS's using systemd </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-15T15:55:32Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-17T14:14:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-15T15:56:45Z" id="131395805">@tlrx - I believe centos-7 wasn't working because of this. Rather - making this change got the vagrant tests passing in centos-7. I'm reasonably sure that means we're not working properly on any systemd flavored distro without it.
</comment><comment author="nik9000" created="2015-08-15T15:58:20Z" id="131395858">I _think_ this is broken on 2.0.0 beta....
</comment><comment author="nik9000" created="2015-08-15T16:15:06Z" id="131396884">I'm now sure its broken in the 2.0 branch.
</comment><comment author="nik9000" created="2015-08-15T23:29:59Z" id="131468847">With this patch the bats tests pass on precise, trusty, vivid, wheezy, jessie, centos-6, centos-7, fedora-22, and Oracle Enterprise Linux 7.
</comment><comment author="s1monw" created="2015-08-16T14:17:13Z" id="131554908">LGTM
</comment><comment author="tlrx" created="2015-08-17T12:29:33Z" id="131799410">LGTM

I did some manual testing on centos7 &amp; jessie because I did not succeed in running the Vagrant tests.
</comment><comment author="nik9000" created="2015-08-17T14:19:56Z" id="131835084">OK - merged to 2.0. I'll try and figure out why the vagrant tests aren't running and work on that too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Output plugin info only in verbose mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12908</link><project id="" key="" /><description>Hi

Moved output to verbose level and additionally changed the plugin info output format

Before:

``` shell
PluginInfo{name='cloud-aws', description='The Amazon Web Service (AWS) Cloud plugin allows to use AWS API for the unicast discovery mechanism and add S3 repositories.', site=false, jvm=true, classname=org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin, isolated=true, version='2.1.0-SNAPSHOT'}
```

After:

``` shell
- Plugin information:
Name: cloud-aws
Description: The Amazon Web Service (AWS) Cloud plugin allows to use AWS API for the unicast discovery mechanism and add S3 repositories.
Site: false
Version: 2.1.0-SNAPSHOT
JVM: true
* Classname: org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin
* Isolated: true
```

Fixes #12907

What do you think @clintongormley?
</description><key id="101177510">12908</key><summary>Output plugin info only in verbose mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">kubum</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-08-15T13:44:10Z</created><updated>2015-09-14T20:22:59Z</updated><resolved>2015-09-14T19:47:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-15T14:34:52Z" id="131387969">LGTM. I'm a little weary of making toString's multline line but I think its fine for this.
</comment><comment author="kubum" created="2015-08-15T14:41:06Z" id="131388744">@nik9000 I also thought about this, maybe better to extract put multiline info to some kind of `displayInfo` method and leave `toString` as it is?
</comment><comment author="nik9000" created="2015-08-15T14:50:51Z" id="131390161">&gt; @nik9000 I also thought about this, maybe better to extract put multiline info to some kind of displayInfo method and leave toString as it is?

The toString was already super long so it'd be hard to use in debugging. I'm fine with the pr as it stands. If someone is debugging and the toString is a problem they can change it.

That gets me to thinking - can you think of any way to test this? We can certainly add something to the bats tests that we run in the qa/vagrant repository. Those are undergoing major surgery right now so it might be simpler to just create a followup issue to test this with bats.
</comment><comment author="kubum" created="2015-08-17T21:55:15Z" id="131975120">Hi @nik9000 

Ah, I didn't know that there are bash tests. Is this file [50_plugins.bats](https://github.com/elastic/elasticsearch/blob/master/qa/vagrant/src/test/resources/packaging/scripts/50_plugins.bats) would be a good place for output test? I think I can figure out how to do add test for it.

However, if you are sure we can just do it in a separate issue.
</comment><comment author="nik9000" created="2015-08-17T22:58:08Z" id="131986936">&gt; However, if you are sure we can just do it in a separate issue.

Go ahead and add them - I'm in the middle of refactoring them now but we can handle a merge if we have to. Just try to keep your change confined so its easier to merge.

TESTING.asciidoc has some information on the best ways to run those tests. They are reasonably destructive so its best to run them in a VM which TESTING.asciidoc describes how to do.
</comment><comment author="kubum" created="2015-08-23T17:30:09Z" id="133880841">Hi @nik9000 

I tried to run tests today from master and unfortunately they are failing.

Could you please advice something on this?

First of all, all tests use -u param and fail with:

```
ERROR: Unrecognized option: -u
```

I removed it and tried to run again:

``` shell
 &#10007; [PLUGINS] install jvm-example plugin
   (in test file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/50_plugins.bats, line 69)
     `[ "$status" -eq 0 ]' failed
   -&gt; Installing jvm-example... Trying http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/jvm-example/2.1.0/jvm-example-2.1.0.zip ... Failed: IOException[Can't get http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/jvm-example/2.1.0/jvm-example-2.1.0.zip to /tmp/jvm-example8389380809329658167.zip]; nested: FileNotFoundException[http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/jvm-example/2.1.0/jvm-example-2.1.0.zip]; nested: FileNotFoundException[http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/jvm-example/2.1.0/jvm-example-2.1.0.zip]; ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
```

Apparently resource is not available anymore. Is there a plan to move it? or use from somewhere else?

I just wanted to add a regexp to check the verbose and non-verbose output of the jvm plugin install :) 
</comment><comment author="nik9000" created="2015-08-24T13:24:34Z" id="134199806">&gt; Could you please advice something on this?

I'll have a look. Right now those tests aren't yet run as part of everyone's development process so they might not be working properly.
</comment><comment author="nik9000" created="2015-08-25T14:44:57Z" id="134609769">&gt; I'll have a look. Right now those tests aren't yet run as part of everyone's development process so they might not be working properly.

Bleh - ok I have a fix proposed for this in #13076 but its a larger change than just fixing the `-u` issue. Bleh. Sorry. Lets not hold this up for that.

I think I still need another review if I can get it - @tlrx, do you have an opinion on this?
</comment><comment author="nik9000" created="2015-08-25T14:48:11Z" id="134610541">Swapped the v2.0.0 out for v2.1.0 because we're trying to stabilize 2.0 and don't want to push too much to it.
</comment><comment author="nik9000" created="2015-08-25T15:34:12Z" id="134629800">Verified that it looks as good as @kubum says it does:

``` bash
$ ./bin/plugin install jvm-example -v -u file:///elasticsearch/qa/vagrant/target/testroot/elasticsearch-jvm-example-2.1.0-SNAPSHOT.zip
-&gt; Installing jvm-example...
Trying file:/elasticsearch/qa/vagrant/target/testroot/elasticsearch-jvm-example-2.1.0-SNAPSHOT.zip ...
Downloading .DONE
Verifying file:/elasticsearch/qa/vagrant/target/testroot/elasticsearch-jvm-example-2.1.0-SNAPSHOT.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
- Plugin information:
Name: jvm-example
Description: Demonstrates all the pluggable Java entry points in Elasticsearch
Site: false
Version: 2.1.0-SNAPSHOT
JVM: true
 * Classname: org
```
</comment><comment author="nik9000" created="2015-09-02T19:50:35Z" id="137225219">&gt; Verified that it looks as good as @kubum says it does:

Ok - I just forgot about this for a week. Sorry! Right now the vagrant testing is much more under control. If you rebase on elastic's master branch you should be able to work on tests for this if you want. If you don't want to try your hand at bats testing then I think this is fine to merge and I'll write the tests in a followup pull request.
</comment><comment author="kubum" created="2015-09-07T22:37:21Z" id="138387456">Hi @nik9000 

I tried to run master bats tests and got

``` shell
vagrant@vagrant-ubuntu-trusty-64:/elasticsearch/qa/vagrant/target/testroot$ sudo bats $BATS/*.bats
bats: /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/25_tar_plugins.bats does not exist
/usr/libexec/bats-exec-suite: line 20: let: count+=: syntax error: operand expected (error token is "+=")
vagrant@vagrant-ubuntu-trusty-64:/elasticsearch/qa/vagrant/target/testroot$ ls -la /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/
total 76
drwxr-xr-x 1 vagrant vagrant   510 Sep  7  2015 .
drwxr-xr-x 1 vagrant vagrant   102 Aug 11 20:50 ..
-rw-r--r-- 1 vagrant vagrant  2590 Sep  7 22:18 20_tar_package.bats
lrwxr-xr-x 1 vagrant vagrant    22 Sep  7  2015 25_tar_plugins.bats -&gt; plugin_test_cases.bash
-rw-r--r-- 1 vagrant vagrant  4625 Sep  7 22:18 30_deb_package.bats
-rw-r--r-- 1 vagrant vagrant  3506 Sep  7 22:18 40_rpm_package.bats
lrwxr-xr-x 1 vagrant vagrant    22 Sep  7  2015 50_plugins.bats -&gt; plugin_test_cases.bash
-rw-r--r-- 1 vagrant vagrant  5850 Sep  7 22:29 50_plugins_.bats
-rw-r--r-- 1 vagrant vagrant  3692 Sep  7 22:18 60_systemd.bats
-rw-r--r-- 1 vagrant vagrant  2704 Sep  7 22:18 70_sysv_initd.bats
-rw-r--r-- 1 vagrant vagrant  3480 Sep  7 22:18 80_upgrade.bats
-rw-r--r-- 1 vagrant vagrant 14479 Sep  7 22:18 packaging_test_utils.bash
-rw-r--r-- 1 vagrant vagrant  5850 Sep  7 22:18 plugin_test_cases.bash
-rw-r--r-- 1 vagrant vagrant  3541 Sep  7 22:18 plugins.bash
-rw-r--r-- 1 vagrant vagrant  2545 Sep  7 22:18 tar.bash
```

Apparently it's not really working well with symlinks? I was following the TESTING.asciidoc. Do you have any ideas where is the issue?
</comment><comment author="nik9000" created="2015-09-09T12:43:37Z" id="138897493">&gt; Apparently it's not really working well with symlinks? I was following the TESTING.asciidoc. Do you have any ideas where is the issue?

Hmmm - I'm not sure.

What is your host OS?

What happens when you `sudo bats $BATS/25_tar_plugins.bats`?

Master isn't going to work properly without #13422 because master has jumped to Java 8.

I'd be fine merging as is and filing a second issue for the bats test if you are fine with it.
</comment><comment author="kubum" created="2015-09-09T20:50:46Z" id="139041822">Hi @nik9000 

I'm totally happy if it can be merged as it is. Just wanted to make it better :) but probably better to do it as part of the second issue.

I did it in vagrant trusty box.
</comment><comment author="nik9000" created="2015-09-10T17:24:20Z" id="139317155">&gt; I did it in vagrant trusty box.

What is the _host_ OS? Like, what OS is running vagrant/virtuabox? I'm wondering what might be up with the symlinks.
</comment><comment author="kubum" created="2015-09-13T13:08:23Z" id="139872879">Ah, sorry.

It's Mac OS X Yosemite 10.10.3. 
Vagrant 1.7.4. 
VirtualBox 5.0.2r102096

Hope this helps
</comment><comment author="nik9000" created="2015-09-14T13:41:07Z" id="140078687">&gt; Hope this helps

Its what I asked for but it doesn't help! Its pretty much the same as what I have. As much as I'd love to get this working for you I think the right thing here is to just merge as is and create a followup issue for checking the output. I'll try and do both of those today.

As far as why the tests don't work for you - I'm not sure. They seem to work fine for other folks that tried them. The symlink is happy which is wonderful. I'm not sure what's up.
</comment><comment author="nik9000" created="2015-09-14T19:48:27Z" id="140187552">Merged to master. I'm backporting it to 2.x right now.

I have no idea why github used the red closed symbol rather than the merged symbol. Its merged! Well, half way.
</comment><comment author="nik9000" created="2015-09-14T19:50:53Z" id="140188155">Thanks for writing this @kubum !
</comment><comment author="nik9000" created="2015-09-14T20:22:59Z" id="140195459">And merged to 2.x. All done!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Output PluginInfo only when in verbose mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12907</link><project id="" key="" /><description>When installing a plugin, the plugin manager outputs PluginInfo by default, which I think it should only do in verbose mode.  Extra points for formatting the info nicely:

```
.........................................................................................................DONE
PluginInfo{name='cloud-aws', description='The Amazon Web Service (AWS) Cloud plugin allows to use AWS API for the unicast discovery mechanism and add S3 repositories.', site=false, jvm=true, classname=org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin, isolated=true, version='2.0.0-beta1'}
```
</description><key id="101152202">12907</key><summary>Output PluginInfo only when in verbose mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-08-15T08:33:13Z</created><updated>2015-10-07T10:38:59Z</updated><resolved>2015-09-14T19:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimhooker2002" created="2015-08-18T07:02:20Z" id="132096228">Got a fix for this.  I'll submit the pull request later today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure `localhost` works on IPv4 and IPv6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12906</link><project id="" key="" /><description>Today we use `localhost` in all of our curl examples, but Elasticsearch is binding to either `127.0.0.1` or `::1`, and `localhost` may not work.

We should try to bind to both.
</description><key id="101150614">12906</key><summary>Ensure `localhost` works on IPv4 and IPv6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Network</label><label>adoptme</label><label>bug</label></labels><created>2015-08-15T07:52:24Z</created><updated>2015-08-18T11:18:25Z</updated><resolved>2015-08-17T19:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-16T18:00:27Z" id="131595895">Related to https://github.com/elastic/elasticsearch/issues/12914 and https://github.com/elastic/elasticsearch/issues/12915
</comment><comment author="rmuir" created="2015-08-17T13:37:18Z" id="131819936">I'm working on this: I've made progress but need a few more hours to have something to show.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>InnerHitsIT#testRandomNested fails randomly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12905</link><project id="" key="" /><description>```
 mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=FCF41B5CE51F5C52 -Dtests.class=org.elasticsearch.search.innerhits.InnerHitsIT -Dtests.method="testRandomNested" -Des.logger.level=DEBUG -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.locale=ar_TN -Dtests.timezone=Antarctica/Casey
```

http://build-us-00.elastic.co/job/es_core_master_debian/6915/consoleFull
http://build-us-00.elastic.co/job/es_core_master_suse/1424/consoleFull
http://build-us-00.elastic.co/job/es_core_master_oracle_6/1756/

I will mute the test
</description><key id="101147860">12905</key><summary>InnerHitsIT#testRandomNested fails randomly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Inner Hits</label><label>jenkins</label></labels><created>2015-08-15T06:40:07Z</created><updated>2015-08-16T21:59:19Z</updated><resolved>2015-08-16T21:59:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-16T14:09:52Z" id="131554389">I muted the entire InnerHitsIT test I suspect this is related to https://github.com/elastic/elasticsearch/pull/12261 which changed lately
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ES_CLEAN_BEFORE_TEST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12904</link><project id="" key="" /><description>In the bats test ES_CLEAN_BEFORE_TEST was used to clean the environment
before running the tests. Unfortunately the tests don't work unless you
specify it every time. This removes that option and always runs the clean.
</description><key id="101146543">12904</key><summary>Remove ES_CLEAN_BEFORE_TEST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-15T06:12:22Z</created><updated>2015-08-15T14:30:20Z</updated><resolved>2015-08-15T14:30:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-15T06:12:34Z" id="131308112">@tlrx, and another bats change.
</comment><comment author="tlrx" created="2015-08-15T08:34:33Z" id="131316121">LGTM

@nik9000 thanks for taking care of this, I want to do it for a loooong time :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up the tar tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12903</link><project id="" key="" /><description>1. Move `clean_before_test` to the first test so its more explicit.
2. Move `skip_not_tar_gz` to setup because it was run first in every test.
3. Remove calls to `run` that only check the status. Its simpler to just
   execute the command. Its better because std-out will be captured and replayed
   on error.
4. Switch from `su` to `sudo` because `su` was breaking `bats`'s error
   reporting.
</description><key id="101146512">12903</key><summary>Clean up the tar tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-15T06:10:59Z</created><updated>2015-08-24T14:00:21Z</updated><resolved>2015-08-24T14:00:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-15T06:11:11Z" id="131308083">@tlrx, more bats changes for review....
</comment><comment author="tlrx" created="2015-08-17T09:00:15Z" id="131736440">&gt; Move clean_before_test to the first test so its more explicit.

I'm not a fan of his because a) that's why the `setup` method is for and b) I'm always afraid that someone, one day, inserts a new test at the first place.

&gt; Move skip_not_tar_gz to setup because it was run first in every test.

I'm afraid we should do this for all bats tests.

&gt; Remove calls to run that only check the status. Its simpler to just execute the command. Its better because std-out will be captured and replayed on error.

Same here.

&gt; Switch from su to sudo because su was breaking bats's error reporting.

Nice!
</comment><comment author="nik9000" created="2015-08-17T16:37:49Z" id="131883990">&gt; I'm not a fan of his because a) that's why the setup method is for and b) I'm always afraid that someone, one day, inserts a new test at the first place.

Meh - I see the setup method as something that is for running common code before each test case. I think its funky to have code in setup that intentionally only runs on the first test case.

&gt; I'm afraid we should do this for all bats tests.

Probably. I wanted to write something that wouldn't conflict with #12895.
</comment><comment author="nik9000" created="2015-08-17T17:33:40Z" id="131899216">Ahk - I got my diffs confused and thought the review was done on this and rebased/squashed it. That hid the still open point between @tlrx and I about `/tmp/elasticsearch/elasticsearch.pid`. Its up there - but it needs to be expanded.
</comment><comment author="nik9000" created="2015-08-24T13:36:45Z" id="134202953">@tlrx any chance you can leave some more comments? I can undo the `clean_before_test` change if it'd make you feel better about this. I have some more changes to do in this area and I'd love to get some of this in and rebase on it.
</comment><comment author="tlrx" created="2015-08-24T13:39:50Z" id="134204098">@nik9000 I don't have a strong feeling for the `clean_before_test` change so feel free to push your change. Can you please create an issue for points 2. and 3. so that it won't be lost? Thanks a lot.
</comment><comment author="nik9000" created="2015-08-24T14:00:07Z" id="134214543">&gt; @nik9000 I don't have a strong feeling for the clean_before_test change so feel free to push your change. Can you please create an issue for points 2. and 3. so that it won't be lost? Thanks a lot.

#13074 and #13075
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>total_indices in slowlog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12902</link><project id="" key="" /><description>Take the following example with 2 indices ([test] is a 5 shard index, [test_1] is a 1 shard index).

When running a query against both indices, eg. `GET test,test_1/_search`, the following returns:

```
[2015-08-14 17:26:48,182][DEBUG][index.search.slowlog.query] [Neophyte] [test][0] took[65micros], took_millis[0], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[6], source[], extra_source[], 
[2015-08-14 17:26:48,182][DEBUG][index.search.slowlog.query] [Neophyte] [test_1][0] took[88micros], took_millis[0], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[6], source[], extra_source[], 
[2015-08-14 17:26:48,182][DEBUG][index.search.slowlog.query] [Neophyte] [test][2] took[67micros], took_millis[0], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[6], source[], extra_source[], 
[2015-08-14 17:26:48,182][DEBUG][index.search.slowlog.query] [Neophyte] [test][3] took[37micros], took_millis[0], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[6], source[], extra_source[], 
[2015-08-14 17:26:48,182][DEBUG][index.search.slowlog.query] [Neophyte] [test][1] took[49micros], took_millis[0], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[6], source[], extra_source[], 
[2015-08-14 17:26:48,183][DEBUG][index.search.slowlog.query] [Neophyte] [test][4] took[40micros], took_millis[0], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[6], source[], extra_source[], 
```

If the users look at `[test_1][0]` entry, they will wonder why the query is hitting 6 shards when test_1 only has a single shard.  This is because total_shards is reporting the total number of shards hit by the "full" request which is against 2 different indices.

It will be helpful to add a "total_indices[2]" metric in the output as well.
</description><key id="101126388">12902</key><summary>total_indices in slowlog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-08-15T00:33:33Z</created><updated>2017-05-05T15:51:45Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-12-30T23:20:30Z" id="168093821">+1 I am often wondering how many indices get touched while debugging these kind of issues.
</comment><comment author="GlenRSmith" created="2017-01-23T22:49:54Z" id="274643260">&#128077; </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Accumulate validation errors when validating index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12901</link><project id="" key="" /><description>This commit changes the behavior when validating index templates to
accumulate all validation errors before reporting failure to the user.
This addresses a usability issue when creating index templates.

Closes #12900
</description><key id="101108286">12901</key><summary>Accumulate validation errors when validating index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Index Templates</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-14T21:46:56Z</created><updated>2015-08-21T12:33:03Z</updated><resolved>2015-08-21T12:08:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-20T15:07:22Z" id="133043461">I had a look at this, left some comments. Can we also add a test around this?
</comment><comment author="jasontedor" created="2015-08-20T17:26:04Z" id="133082981">Thanks @javanna. I've pushed a commit that incorporates your suggestions.
</comment><comment author="javanna" created="2015-08-21T07:56:41Z" id="133319130">left a few very minor comments, LGTM though besides those
</comment><comment author="jasontedor" created="2015-08-21T12:33:03Z" id="133410930">Thanks for your helpful review @javanna.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Accumulate validation errors when validating index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12900</link><project id="" key="" /><description>Currently when validating index templates we do not accumulate multiple validation errors. This is mostly a usability issue as in the case of multiple validation errors, we force the user to resubmit their template over and over to iterate through the validation errors. A better approach would be to accumulate these validation errors and report them all to the user.

This issue comes at the [suggestion](https://github.com/elastic/elasticsearch/pull/12892#issuecomment-131234553) of @dakrone.
</description><key id="101107745">12900</key><summary>Accumulate validation errors when validating index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Index Templates</label><label>enhancement</label></labels><created>2015-08-14T21:42:32Z</created><updated>2015-08-21T12:08:50Z</updated><resolved>2015-08-21T12:08:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add elasticsearch version as a prefix for the staging URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12899</link><project id="" key="" /><description>This is purely for maintainance reasons since it easier to see if we can drop
certain stageing urls if we have the version next to the hash.
I also removed the gpg passphrase from the example URL since it's better to get prompted?

@spinscale can you take a look?
</description><key id="101106018">12899</key><summary>Add elasticsearch version as a prefix for the staging URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>review</label></labels><created>2015-08-14T21:28:27Z</created><updated>2015-08-17T09:06:32Z</updated><resolved>2015-08-17T09:06:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-14T21:41:41Z" id="131244563">It makes sense and looks good to me.
</comment><comment author="rjernst" created="2015-08-14T22:00:16Z" id="131250362">LGTM
</comment><comment author="spinscale" created="2015-08-15T06:57:30Z" id="131311157">left one comment, LGTM otherwise
</comment><comment author="spinscale" created="2015-08-17T09:04:43Z" id="131737235">one last nitpicking comment, LGTM, push it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change qa/vagrant artifactId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12898</link><project id="" key="" /><description>Related to #12651
</description><key id="101096164">12898</key><summary>Change qa/vagrant artifactId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v2.1.0</label></labels><created>2015-08-14T20:19:34Z</created><updated>2015-08-17T19:22:54Z</updated><resolved>2015-08-16T17:11:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-14T20:58:24Z" id="131237529">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add bin to jvm-example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12897</link><project id="" key="" /><description>This will be useful in testing the plugin installer.

Relates to #12651
</description><key id="101093313">12897</key><summary>Add bin to jvm-example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-14T20:06:29Z</created><updated>2015-08-16T17:11:23Z</updated><resolved>2015-08-16T17:11:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-14T20:07:55Z" id="131224680">Ping @tlrx or @rjernst for review. This one doesn't try to actually use the plugin or anything fancy - just make something in the bin directory that we can make sure works properly with vagrant.
</comment><comment author="dadoonet" created="2015-08-14T21:00:26Z" id="131238197">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove URL from PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12896</link><project id="" key="" /><description>We should be using `URI` instead.

It'd be great to eventually get `new URL(...)` in the forbidden APIs also.
</description><key id="101090982">12896</key><summary>Remove URL from PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-08-14T19:53:35Z</created><updated>2017-03-13T00:32:38Z</updated><resolved>2017-03-13T00:28:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-14T21:41:20Z" id="131244523">+1
</comment><comment author="rjernst" created="2017-03-13T00:28:44Z" id="285990785">The plugin manager was completely rewritten since creating this issue. It still uses `new URL`, but only in the case that a URL is passed in (in which case the plugin id is not a builtin plugin or a maven identifier).  We need URL because we need to open a connection; URI only has information about the URI, but you still need to call toURL() to get something that can open a connection.</comment><comment author="dakrone" created="2017-03-13T00:32:38Z" id="285991047">+1, thanks for closing this Ryan</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use jvm-example for testing bin/plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12895</link><project id="" key="" /><description>Related to #12651
</description><key id="101090498">12895</key><summary>Use jvm-example for testing bin/plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-14T19:50:38Z</created><updated>2015-08-17T18:13:02Z</updated><resolved>2015-08-17T18:13:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-14T19:51:23Z" id="131218898">Ping @tlrx for review - I'll be adding another pr to handle the TODO.
</comment><comment author="rjernst" created="2015-08-14T21:40:22Z" id="131244389">LGTM, I would just remove all the commented out file checks for shield.
</comment><comment author="nik9000" created="2015-08-16T17:58:05Z" id="131594958">Rebased, added tests for plugins with bin directory, and removed all mention of shield. Sorry for the rebase - I had to do it to get access to a plugin with a bin directory.
</comment><comment author="tlrx" created="2015-08-17T08:18:18Z" id="131726962">The changes LGTM but bats tests are failing at various stages on my laptop. I guess this is due to recent changes and not this PR specifically.
</comment><comment author="nik9000" created="2015-08-17T14:57:34Z" id="131853083">&gt; The changes LGTM but bats tests are failing at various stages on my laptop. I guess this is due to recent changes and not this PR specifically.

Hmm - they work for me:

```
vagrant destory -f
mvn clean install -DskipTests
mvn -pl qa/vagrant -Dtests.vagrant verify
```

and it all passes. That first line isn't usually required but if things are flakey its best to just rebuild the VMs....
</comment><comment author="nik9000" created="2015-08-17T18:12:59Z" id="131913121">Ok - this has enough LGTMs that I've rebased it and will merge it to master. I'll help @tlrx work through vagrant issues when our days overlap better.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix reproduction line to include project filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12894</link><project id="" key="" /><description>Today on a failure the reproduce line printed out by the test framework
will build all projects and might fail if the test class is not present.
This commit adds a reactor filter to the reproduction line to ensure
unrelated projects are skipped.

Closes #12838
</description><key id="101088877">12894</key><summary>Fix reproduction line to include project filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T19:40:27Z</created><updated>2015-08-18T16:53:06Z</updated><resolved>2015-08-14T20:52:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-14T20:25:04Z" id="131231210">+1, tested and it works great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose points_only option through geo_shape field mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12893</link><project id="" key="" /><description>This PR adds a `points_only` option (defaults to `false`) to the GeoShapeFieldMapper that exposes the pointsOnly optimization in lucene's RecursivePrefixTreeStrategy. This optimization short-circuits DFS traversal when it is known that the field contains points only. To enforce this option the `GeoShapeFieldMapper` has been updated to throw a MapperParsingException for any `Shape` that is not of type `Point` when the option is set to `true`.  Randomized testing is added and docs are updated. ShapeCollection has also been updated to eventually support MultiPoint. Limitations of S4j are currently the only blocker. 

closes #12856
</description><key id="101075933">12893</key><summary>Expose points_only option through geo_shape field mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v2.0.0-rc1</label></labels><created>2015-08-14T18:38:46Z</created><updated>2016-03-10T18:13:19Z</updated><resolved>2015-09-08T21:23:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-01T22:58:48Z" id="136886596">Left two very minor comments, other than that looks good to me!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate settings specified in index templates at template creation time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12892</link><project id="" key="" /><description>Previously settings specified in index templates were not validated upon
template creation. Creating an index from an index template with invalid
settings could lead to cluster stability issues because creation of such
indexes would bypass index settings validation.

This commit adds validation of settings specified in index templates at
template creation time. This works by routing the index template
settings through the index settings validation mechanism.

Closes #12865
</description><key id="101075658">12892</key><summary>Validate settings specified in index templates at template creation time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Index Templates</label><label>bug</label><label>v2.1.0</label></labels><created>2015-08-14T18:37:32Z</created><updated>2015-08-16T03:10:35Z</updated><resolved>2015-08-14T21:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-14T20:43:01Z" id="131234553">Generally this looks good to me, but I wonder if we should move towards the validation style that the `*Request` classes use, where they create an `ActionRequestValidationException` and use `validationException = addValidationError("&lt;message&gt;", validationException)` multiple times to collect multiple validations at once?
</comment><comment author="jasontedor" created="2015-08-14T21:42:58Z" id="131244790">@dakrone Completely agree; I will address that in a [separate issue](https://github.com/elastic/elasticsearch/issues/12900).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Requests do not always propagate headers/context when retrieving indexed scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12891</link><project id="" key="" /><description>In #11060 (issue #10979), the request headers and context of the originating request were copied to all sub-requests. In doing some testing on master, I noticed that in the `ScriptService#getScriptFromIndex` the headers and context were not being copied due to the lack of a `SearchContext`. To see what calls were affected, I added an assert to check if the context was null. When doing so the following tests trip the assertion:

```
- org.elasticsearch.search.aggregations.pipeline.BucketScriptIT.indexedScript
- org.elasticsearch.search.aggregations.metrics.ScriptedMetricIT.testInitMapCombineReduce_withParams_Indexed
- org.elasticsearch.search.aggregations.pipeline.BucketSelectorIT.indexedScript
- org.elasticsearch.validate.RenderSearchTemplateIT.indexedTemplate
```

There were also two rest test failures for the Render Search Template API. For the render search template api, we need a way to pass in the headers/context in a way that isn't a search context because it is not a search request.

For the other failures, it appears that the subsequent requests happen after the SearchContext has been cleared/released.
</description><key id="101063359">12891</key><summary>Requests do not always propagate headers/context when retrieving indexed scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Aggregations</label><label>:Internal</label><label>:Scripting</label><label>:Search</label><label>blocker</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-14T17:43:38Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-09-02T12:50:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-15T08:26:57Z" id="131314992">@colings86 please could you take this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove attemped (not working) support for array in not query parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12890</link><project id="" key="" /><description>Query DSL: remove attemped (not working) support for array in not query parser.

Looking at this parser I don't see how it can parse an inner query wrapped in an array, although it makes little sense.
</description><key id="101058927">12890</key><summary>Remove attemped (not working) support for array in not query parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T17:24:50Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-18T13:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T09:37:30Z" id="132143991">LGTM
</comment><comment author="jpountz" created="2015-08-21T09:13:58Z" id="133344710">@javanna This is labeled as 2.0 but I can't find a commit on the 2.0 branch, did you forget to backport or should it be only labeled as a 2.1 change?
</comment><comment author="javanna" created="2015-08-21T09:14:41Z" id="133344798">hey @jpountz I am waiting for beta1  to go out so I can backport.
</comment><comment author="javanna" created="2015-08-21T13:47:07Z" id="133428868">it is now backported to 2.0 as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the `network` option from nodes info/stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12889</link><project id="" key="" /><description>As of https://github.com/elastic/elasticsearch/pull/12054, there is no longer a `network` section in the nodes info and stats requests. The following should no longer work:

```
GET _nodes/network
GET _nodes/stats/network
```
</description><key id="101056348">12889</key><summary>Remove the `network` option from nodes info/stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Stats</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0-beta1</label><label>v2.0.0-beta2</label></labels><created>2015-08-14T17:12:57Z</created><updated>2015-09-17T09:23:39Z</updated><resolved>2015-08-17T12:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-17T03:25:27Z" id="131669412">Here's a quick fix. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate checksums for plugins if available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12888</link><project id="" key="" /><description>When a plugin is downloaded, this change additionally tries to download
`${pluginurl}.sha1` and verify the SHA1 checksum for the file. If no
.sha1 file is found, it tries `${pluginurl}.md5`.

Note that if neither checksum file is found, a notice is printed but the
plugin can still be installed. If the checksum check fails, the plugin
install is aborted.

Example output if no checksums are available:

```
bin/plugin install elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT
-&gt; Installing elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT...
Trying http://download.elastic.co/elasticsearch/elasticsearch-analysis-icu/elasticsearch-analysis-icu-2.6.0-SNAPSHOT.zip ...
Trying http://search.maven.org/remotecontent?filepath=elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT/elasticsearch-analysis-icu-2.6.0-SNAPSHOT.zip ...
Trying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT/elasticsearch-analysis-icu-2.6.0-SNAPSHOT.zip ...
Trying https://github.com/elasticsearch/elasticsearch-analysis-icu/archive/2.6.0-SNAPSHOT.zip ...
Trying https://github.com/elasticsearch/elasticsearch-analysis-icu/archive/master.zip ...
Downloading .....................................DONE
Verifying https://github.com/elasticsearch/elasticsearch-analysis-icu/archive/master.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
```

Example output if checksums are available:

```
bin/plugin install elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT
-&gt; Installing elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT...
Trying http://download.elastic.co/elasticsearch/elasticsearch-analysis-icu/elasticsearch-analysis-icu-2.6.0-SNAPSHOT.zip ...
Trying http://search.maven.org/remotecontent?filepath=elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT/elasticsearch-analysis-icu-2.6.0-SNAPSHOT.zip ...
Trying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/elasticsearch-analysis-icu/2.6.0-SNAPSHOT/elasticsearch-analysis-icu-2.6.0-SNAPSHOT.zip ...
Trying https://github.com/elasticsearch/elasticsearch-analysis-icu/archive/2.6.0-SNAPSHOT.zip ...
Trying https://github.com/elasticsearch/elasticsearch-analysis-icu/archive/master.zip ...
Downloading .....................................DONE
Verifying https://github.com/elasticsearch/elasticsearch-analysis-icu/archive/master.zip checksums if available ...
Downloading .DONE
```

Example output if checksums fail:

```
bin/plugin install elasticsearch/elasticsearch-analysis-kuromoji/2.5.0 -url http://localhost:8000/elasticsearch-analysis-kuromoji-2.5.0.zip
-&gt; Installing elasticsearch/elasticsearch-analysis-kuromoji/2.5.0...
Trying http://localhost:8000/elasticsearch-analysis-kuromoji-2.5.0.zip ...
Downloading .............................................DONE
Verifying http://localhost:8000/elasticsearch-analysis-kuromoji-2.5.0.zip checksums if available ...
Downloading .DONE
ERROR: incorrect hash, file hash: [dbdc9c2cd32782054497a21fbdcae3ca1ff23c80], expected: [dbdc9c2cd32782054497a21fbdcae3ca1ff23c80-bad]
```

Resolves #12750
</description><key id="101051572">12888</key><summary>Validate checksums for plugins if available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T16:48:21Z</created><updated>2016-02-29T16:38:16Z</updated><resolved>2015-08-14T20:30:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-14T17:36:13Z" id="131190850">I was wondering if we can do the MD5/SHA1 analysis on the fly while downloading the zip file.
Like what we did here: https://github.com/elastic/elasticsearch/blob/master/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java#L132-L150

Would this make sense in such a case?
</comment><comment author="dakrone" created="2015-08-14T18:21:28Z" id="131200572">&gt; I was wondering if we can do the MD5/SHA1 analysis on the fly while downloading the zip file.

I'm not sure what that would buy us? It would mean that we don't have to
read the entire zip into memory, but is the extra complexity worth it?
Do we expect plugins to be gigantic?
</comment><comment author="dadoonet" created="2015-08-14T18:34:17Z" id="131204670">Not a big deal. Just that I found that way of doing it on the fly is super nice.

The PluginManager runs in its own JVM so memory constraint is less important here I guess.

As for the biggest plugin I've seen it's probably the mapper attachment which is more than 20mb IIRC. Not that big indeed.
</comment><comment author="s1monw" created="2015-08-14T19:45:26Z" id="131217713">left minor comments otherwise LGTM
</comment><comment author="s1monw" created="2015-08-14T19:56:53Z" id="131219812">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: prevent NPE from being thrown in ParseField when providing a null field name as an argument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12887</link><project id="" key="" /><description>Internal: prevent NPE from being thrown in ParseField when providing a null field name as an argument
</description><key id="101050859">12887</key><summary>Internal: prevent NPE from being thrown in ParseField when providing a null field name as an argument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2015-08-14T16:44:47Z</created><updated>2016-08-02T13:58:10Z</updated><resolved>2016-02-11T17:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-14T21:31:23Z" id="131243070">When is null a valid field name?
</comment><comment author="javanna" created="2015-08-17T08:19:14Z" id="131727356">&gt; When is null a valid field name?

it is not a valid field name but due to the way we parse it might happen that the currentFieldName hasn't been set yet, hence we end up passing in null. This can happen with any of our parsers as we never check for null before using parse field.
</comment><comment author="rjernst" created="2015-08-17T19:07:22Z" id="131932046">Can you elaborate a path that causes currentFieldName to be null? Why isn't the caller checking in this rare case? I don't think we should add leniency at such a low level: it will hide bugs. It sounds like how we parse queries is broken if we are trying to match a field name before we've parsed the field name. Can we delay the match check until after all the query parameters have been parsed?
</comment><comment author="javanna" created="2015-08-18T08:59:41Z" id="132125356">&gt; Can you elaborate a path that causes currentFieldName to be null? Why isn't the caller checking in this rare case? 

Think of some broken json for instance, all of our parsers work under the assumption that currentFieldName was assigned in some previous loop iteration, but what if it wasn't? I saw this happen in the query-refactoring branch while testing things out. I agree that the fix is a bit sneaky...the problem is that without parse field we don't care much cause we do e.g. `"analyzer".equals(currentFieldName)` which will never throw NPE. But once we introduce `ParseField` there we do run into an NPE while I think returning false is ok. Otherwise we would need to add null checks whenever we use `ParseField`. That is why I think this fix is reasonable.
</comment><comment author="javanna" created="2015-08-27T07:28:33Z" id="135320210">@rjernst I explained above the rationale behind this change. What do we do? Shall we get it in or not?
</comment><comment author="rjernst" created="2015-08-30T19:34:29Z" id="136177374">@javanna I don't think we should decrease safety checks because of a construct used for (silly IMO) backcompat. If something is trying to use the fieldname before we've actually found one in the query DSL, then that code is broken IMO. If something is calling match() before the field name was actually parsed, then woudln't just changing the order of the query elements cause incorrect matching?
</comment><comment author="rjernst" created="2015-08-30T19:35:05Z" id="136177407">I also think we should have an assert or real check that the field name is not null, instead of what is proposed here.
</comment><comment author="javanna" created="2015-09-02T19:13:40Z" id="137217416">I do understand where your concerns come from and why this PR looks fishy, agreed. But I am not sure the alternatives we have are feasible. Lemme try and explain again why...

Not accepting `null` means that when somebody sends a broken json things will get even worse. It is not nice to accept `null`, I agree, but part of it comes with the nature of pull parsing. The reason why we always do `FIELD.equals(currentFieldName)` and not the other way around in all of our parsers is that we know the field name might be null. 

Especially looking at #8964 , when we'll move to `ParseField` everywhere this problem will get bigger and bigger, cause we'll end up potentially passing in `null` to `ParseField#match` all over the place. 

At the end of the day my proposed change does the same as the existing equality checks when providing the nullable value as argument... or do we really want to add null checks in every single field name based conditional, in every single parser? That seems conceptually better but I am not sure it is feasible in reality.

The other tricky aspect is properly testing broken json and reproducing all the horrible things that can happen while pull parsing the way we do it. For instance how do we make sure that the assert trips once we forgot a null check somewhere?
</comment><comment author="rjernst" created="2015-09-03T06:41:03Z" id="137351700">Let's look at an example. I just pulled up the first query parser I found, QueryStringQueryParser. The beginning of the parsing loop looks like this:

```
String currentFieldName = null;
        XContentParser.Token token;
        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
            if (token == XContentParser.Token.FIELD_NAME) {
                currentFieldName = parser.currentName();
            } else if (token == XContentParser.Token.START_ARRAY) {
                if ("fields".equals(currentFieldName)) {
                   ...
                } else {
                    throw new QueryParsingException(parseContext, "[query_string] query does not support [" + currentFieldName
                            + "]");
                }
```

if `currentFieldName` is null when the START_ARRAY branch is found, this is totally broken. And worse, the exception would have "does not support [null]". How would this be ok?

Why does the json parser we have not throw exception on malformed json (I thought it did)?
</comment><comment author="javanna" created="2015-09-03T08:10:55Z" id="137373165">I will come up with examples of when this happens.... which is the reason why I initially opened this PR... note that with your example, if we have null it is fine with the current parser, but as soon as we move to using `ParseField` (#8964) we get NPE rather than "does not support [null]". My change was exactly to keep the original "does not support [null]" response.
</comment><comment author="rjernst" created="2015-09-03T08:13:24Z" id="137373487">&gt; note that with your example, if we have null it is fine with the current parser

How? If it is null, it means we have broken json right?
</comment><comment author="javanna" created="2015-09-03T08:16:34Z" id="137373975">we do pull parsing, what I'm trying to say is that broken json doesn't necessarily mean the parser is out of the picture, in a lot of cases we go ahead parsing anyway. That is the hard part to test and predict, cause it depends on how the json is broken. I did see this happen myself... as I said as soon as I have time I will post some example of when this happens just to prove it...
</comment><comment author="rjernst" created="2015-09-03T08:18:26Z" id="137374270">&gt; what I'm trying to say is that broken json doesn't necessarily mean the parser is out of the picture, in a lot of cases we go ahead parsing anyway

Why?? If we have broken json, we should throw an exception!
</comment><comment author="javanna" created="2015-09-03T08:22:53Z" id="137375628">&gt; Why?? If we have broken json, we should throw an exception!

Sure, if we can detect it, but I've seen cases where we can't, or we don't. This is the nature of pull parsing, we essentially try to read what we find and understand...
</comment><comment author="rjernst" created="2015-09-03T08:25:15Z" id="137376802">JSON is a spec. The json parser should throw an exception if there is anything misaligned. Can you find that example where the json parser does not throw an exception on malformed input?
</comment><comment author="javanna" created="2015-09-10T16:48:20Z" id="139307199">I did some digging, but I am not able to reproduce this anymore. I regret not having saved the json that caused this issue in the first place back when I saw it. I guess we can close till we see it happen again then, I guess it will at some point.
</comment><comment author="javanna" created="2016-02-11T17:58:45Z" id="182979892">Closing this. Will reopen if it comes up again.
</comment><comment author="javanna" created="2016-08-02T13:57:48Z" id="236912237">After more than a year, the same NPE came up again. I was able to trace what caused it, and open up a PR with hopefully a better fix than this one. See #19750 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ValueParser.DateMath aware of timezone setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12886</link><project id="" key="" /><description>This PR adds a timezone field to ValueParser.DateMath that is
set to UTC by default but can be set using the existing constructors.
This makes it possible for extended bounds setting in DateHistogram
to also use date math expressions that e.g. round by day and apply
this rounding in the time zone specified in the date histogram
aggregation request.

Closes #12278
</description><key id="101050175">12886</key><summary>Make ValueParser.DateMath aware of timezone setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T16:40:21Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-19T16:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-18T09:51:29Z" id="132148764">LGTM
</comment><comment author="feltnerm" created="2015-08-19T01:28:30Z" id="132410406">:100: ! :beers: @cbuescher 
</comment><comment author="feltnerm" created="2015-08-19T14:10:07Z" id="132611563">Any chance this will make it into the latest 1.x? Would be great!
</comment><comment author="cbuescher" created="2015-08-19T14:52:17Z" id="132626022">@feltnerm Sadly, I wouldn't count on it. Currently working towards 2.0, only major bugfixes get backported to the 1.x branches.
</comment><comment author="cbuescher" created="2015-08-19T16:43:01Z" id="132689939">@jpountz @clintongormley any opinion if this should be backported to 2.0?
</comment><comment author="jpountz" created="2015-08-19T16:43:57Z" id="132690315">It's a bug fix and risks look low to me so I would say yes.
</comment><comment author="cbuescher" created="2015-08-21T14:23:30Z" id="133441196">Also backported to 2.0 branch with 8ba2c9ba
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix automatically generated URLs for official plugins in PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12885</link><project id="" key="" /><description>In order to match the paths of official plugins, we need to fix
the broken test by removing the elasticsearch prefix from the official
plugin names before testing.
</description><key id="101046106">12885</key><summary>Fix automatically generated URLs for official plugins in PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T16:22:54Z</created><updated>2015-08-15T08:24:14Z</updated><resolved>2015-08-14T16:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-14T16:25:59Z" id="131168332">It looks good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java api: remove execution from TermsQueryBuilder as it has no effect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12884</link><project id="" key="" /><description>Also introduced ParseField for execution in TermsQueryParser so proper deprecation warnings get printed out when requested.
</description><key id="101044862">12884</key><summary>Java api: remove execution from TermsQueryBuilder as it has no effect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T16:14:55Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-18T13:17:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-08-14T16:18:30Z" id="131167000">LGTM, perhaps mention it as well in migration doc. I'm putting a refers to #12042 so we know where this comes from.
</comment><comment author="jpountz" created="2015-08-18T09:37:06Z" id="132143843">LGTM
</comment><comment author="javanna" created="2015-08-21T09:20:56Z" id="133345953">This is labelled `2.0` but it hasn't been backported to `2.0` yet, will just have to clarify when I can go ahead and backport.
</comment><comment author="s1monw" created="2015-08-21T11:53:57Z" id="133388191">it's baked enough in master please go ahead and cherry-pick it
</comment><comment author="javanna" created="2015-08-21T13:46:54Z" id="133428825">backported, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pass down the EngineConfig to IndexSearcherWrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12883</link><project id="" key="" /><description>If a new IndexSearcher gets created in an `IndexSearcherWrapper` implementation then the engine config can be used to get the query cache and query cache policy from. Otherwise the new `IndexSearcher` being created uses the incorrect query cache and query cache policy.
</description><key id="101037313">12883</key><summary>Pass down the EngineConfig to IndexSearcherWrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-14T15:36:47Z</created><updated>2015-11-22T10:15:20Z</updated><resolved>2015-08-18T09:56:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-08-18T08:33:31Z" id="132119922">LGTM, I just left a minor comment about the javadocs...
</comment><comment author="jpountz" created="2015-08-21T09:22:19Z" id="133346200">@martijnvg This is labeled as 2.0 but I can only see it in the master branch? Should it be backported or should we fix the version labels on this PR?
</comment><comment author="martijnvg" created="2015-08-23T18:58:08Z" id="133897839">@jpountz I think we should back port it. I'll do it once the beta release is out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Adding aws-maven extension to dev-tools and rest-api-spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12882</link><project id="" key="" /><description /><key id="101035898">12882</key><summary>Build: Adding aws-maven extension to dev-tools and rest-api-spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T15:30:30Z</created><updated>2015-08-14T15:32:40Z</updated><resolved>2015-08-14T15:32:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-14T15:30:47Z" id="131150273">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow a plugin to supply its own query cache implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12881</link><project id="" key="" /><description /><key id="101034006">12881</key><summary>Allow a plugin to supply its own query cache implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T15:23:04Z</created><updated>2015-08-18T16:55:27Z</updated><resolved>2015-08-17T09:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-16T22:08:45Z" id="131632822">@uboness @rjernst I've updated the PR use `ExtensionPoint` to register query cache implementations.

I think the fact that a plugin can fail initialization if another query cache implementation has been configured via node/index settings than the implementation a plugin wishes to use, is sufficient for now.
</comment><comment author="rjernst" created="2015-08-16T22:19:22Z" id="131633478">Looks good, but can you add a test? See for example SearchModuleTests. Check a custom one can be registered and set, and that registering a duplicate name fails.
</comment><comment author="martijnvg" created="2015-08-16T22:34:39Z" id="131636108">@rjernst I've  added a test.
</comment><comment author="rjernst" created="2015-08-16T22:36:21Z" id="131636171">LGTM, thanks!
</comment><comment author="s1monw" created="2015-08-17T07:57:06Z" id="131719375">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Google Cloud Storage Repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12880</link><project id="" key="" /><description>_From @dadoonet on January 28, 2014 21:18_

We want to have Snapshot and Restore available for Google Storage Repository.

_Copied from original issue: elastic/elasticsearch-cloud-gce#11_
</description><key id="101028760">12880</key><summary>Add Google Cloud Storage Repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>:Snapshot/Restore</label><label>enhancement</label></labels><created>2015-08-14T14:58:41Z</created><updated>2016-05-19T12:11:43Z</updated><resolved>2016-05-19T12:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-14T14:58:42Z" id="131137333">_From @merrellb on April 17, 2014 17:42_

Is this the same as Google Cloud-Datastore (https://cloud.google.com/products/cloud-datastore/)?  Definitely would be a great addition (and add feature parity with the AWS plugin)
</comment><comment author="dadoonet" created="2015-08-14T14:58:42Z" id="131137338">_From @dspiteself on June 7, 2014 17:29_

+1
</comment><comment author="dadoonet" created="2015-08-14T14:58:42Z" id="131137340">_From @benjamin-smith on July 16, 2014 16:6_

+1 !!!
</comment><comment author="dadoonet" created="2015-08-14T14:58:43Z" id="131137343">_From @danielschonfeld on July 21, 2014 0:2_

Is this in the works at all?
</comment><comment author="dadoonet" created="2015-08-14T14:58:43Z" id="131137346">@tlrx Wanna take it?
</comment><comment author="dadoonet" created="2015-08-14T14:58:43Z" id="131137352">_From @tlrx on October 24, 2014 13:21_

@merrellb this issue concerns Google Cloud Storage, which is a cloud based object storage service.

Google Datastore is more a NoSQL data storage service.
</comment><comment author="dadoonet" created="2015-08-14T14:58:43Z" id="131137355">_From @tlrx on October 31, 2014 16:45_

@dadoonet @imotov @s1monw I'll be glad if you can review this
</comment><comment author="dadoonet" created="2015-08-14T14:58:44Z" id="131137358">_From @vvasanth86 on February 18, 2015 17:28_

Would this be available soon? Thanks!
</comment><comment author="dadoonet" created="2015-08-14T14:58:44Z" id="131137360">_From @pires on March 6, 2015 16:3_

+1
</comment><comment author="dadoonet" created="2015-08-14T14:58:44Z" id="131137365">_From @thekantian on March 30, 2015 12:55_

What's blocking getting this in? Just spun up an ES cluster in GCE and right now I have 2 choices for backup (I think):

1) rsync files to GCS (the old way of backups, don't get any of the snapshot/restore goodies)
2) set up NFS shared folder on all machines and use as a target (tons of overhead that I don't want to deal with).

Given that GCS exists, would be a much better option... What's the roadmap for getting this merged? - 6 months seems like a long time to review...I'm happy to help with whatever work needs to be done!
</comment><comment author="dadoonet" created="2015-08-14T14:58:45Z" id="131137367">_From @tlrx on March 30, 2015 13:0_

@thekantian I apologize for the time it takes but be sure it's still on the road. Some internal classes in elasticsearch have changed multiple times the last few months, making this difficult to test and merge. Now things are stabilized it should not take too much time to be merged.

As soon as it is merged we would be happy to have testers :)
</comment><comment author="dadoonet" created="2015-08-14T14:58:45Z" id="131137372">_From @thekantian on March 30, 2015 13:5_

No apology necessary! Thanks for the update - I was just wondering if there was anything I could do to help move it along since I'm sure it will improve the Google Cloud platform experience for ES a lot for us all!
</comment><comment author="dadoonet" created="2015-08-14T14:58:45Z" id="131137374">_From @pires on March 30, 2015 14:6_

Great news :+1: 
</comment><comment author="dadoonet" created="2015-08-14T14:58:46Z" id="131137379">_From @hussein-vastani on April 17, 2015 13:40_

+1 
Hoping to see this land soon! My current GCE project has a dependency on this. Thank you.
</comment><comment author="dadoonet" created="2015-08-14T14:58:46Z" id="131137382">_From @soundofjw on May 5, 2015 23:26_

Who do we buy :beers: for to see this completed? @tlrx ? :)
</comment><comment author="dadoonet" created="2015-08-14T14:58:47Z" id="131137385">_From @simonmorley on May 6, 2015 0:49_

+1
</comment><comment author="dadoonet" created="2015-08-14T14:58:47Z" id="131137388">_From @ghost on July 1, 2015 19:18_

+1 !
</comment><comment author="dadoonet" created="2015-08-14T14:58:47Z" id="131137391">_From @akleiman on July 1, 2015 19:20_

+1
</comment><comment author="dadoonet" created="2015-08-14T14:58:48Z" id="131137396">_From @fvantin on July 1, 2015 19:57_

+1
</comment><comment author="dadoonet" created="2015-08-14T14:58:48Z" id="131137399">_From @MaxDaten on July 11, 2015 12:39_

+1
</comment><comment author="dadoonet" created="2015-08-14T14:58:48Z" id="131137403">_From @psychonaut on August 13, 2015 9:27_

+1
</comment><comment author="dadoonet" created="2015-08-14T14:58:49Z" id="131137408">_From @vishalshah-org on August 14, 2015 12:12_

any updates on this, we are paying big time $$ to backup to S3 from our google cloud infrastructure :( any help appreciated!
thanks.
</comment><comment author="dadoonet" created="2015-09-10T09:48:12Z" id="139188602">Note that there is a pending PR available here: https://github.com/elastic/elasticsearch-cloud-gce/pull/39
</comment><comment author="omriiluz" created="2015-10-28T09:19:10Z" id="151776854">+1
</comment><comment author="24601" created="2015-11-23T22:14:34Z" id="159083901">+1 to @espeed's comment. 

Is there an issue in using Google Cloud Storage's s3 compatibility? We are testing it as our repository for snapshots without an issue so far, turned on S3/HMAC compatibility, generated a key, set endpoint to the google storage API end point, and are simply using the s3 support. It works perfectly. 

Is there a long term/production use concern with this? Our testing reveals it works fine, would like to know if we are missing some issue here? 
</comment><comment author="dadoonet" created="2015-11-24T08:14:23Z" id="159191425">Using Google Storage with `cloud-aws` plugin is not part of our integration tests. So we can't say if it's safe for production usage. 
You could open a discussion on discuss.elastic.co to see if other users have already tested it?
</comment><comment author="chrislovecnm" created="2015-12-17T21:48:56Z" id="165591986">It is my understand that Google Storage is not compatible with multipart uploads - so can anyone verify that the AWS plugin works?

Who do I need to bribe to get this fixed? Google is a partner with elastic...
</comment><comment author="tlrx" created="2015-12-17T22:41:11Z" id="165605183">&gt; Who do I need to bribe to get this fixed? Google is a partner with elastic...

We're talking about adding a new feature, not _fixing_ a bug, and partnerships do not influence anything here: there's good chance that elasticsearch 2.x has support for Google Storage. The latest pull request is https://github.com/elastic/elasticsearch/pull/13578 and we are in final review stages.
</comment><comment author="chrislovecnm" created="2015-12-18T01:01:22Z" id="165628290">@tlrx I understand that this is a new feature ;) good news, thanks

@cmod AWS s3 plugin -&gt; google storage? Which version of the plugin?
</comment><comment author="cmoad" created="2015-12-18T03:32:12Z" id="165660536">@chrislovecnm I deleted my comment. I misread the question.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[maven] rename artifactIds from `elasticsearch-something` to `something`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12879</link><project id="" key="" /><description>In plugins, we are using non consistent naming. We use `elasticsearch-cloud-aws` as the artifactId, which generates a jar file called `elasticsearch-cloud-aws-VERSION.jar`.

But when you want to install the plugin, you will end up with a shorter name for the plugin `cloud-aws`.

```
bin/plugin install cloud-aws
```

This commit changes that and use consistent names for `artifactId`, so `finalName`.

Also changed the plugin maven names to respect the `QA`naming we have as I found it fine. 
Same for Distribution modules.
Same for parent, dev-tools and rest-spec.

Now, it gives:

```
[INFO] Reactor Summary:
[INFO] 
[INFO] Build Tools and Resources .......................... SUCCESS [  0.133 s]
[INFO] Rest API Specification ............................. SUCCESS [  0.003 s]
[INFO] Elasticsearch: Parent POM .......................... SUCCESS [  0.104 s]
[INFO] Elasticsearch: Core ................................ SUCCESS [  0.008 s]
[INFO] Distribution: Parent POM ........................... SUCCESS [  0.004 s]
[INFO] Distribution: with all optional dependencies ....... SUCCESS [  0.005 s]
[INFO] Distribution: Shaded ............................... SUCCESS [  0.006 s]
[INFO] Distribution: TAR .................................. SUCCESS [  0.006 s]
[INFO] Distribution: ZIP .................................. SUCCESS [  0.007 s]
[INFO] Distribution: Debian ............................... SUCCESS [  0.048 s]
[INFO] Distribution: RPM .................................. SUCCESS [  0.090 s]
[INFO] Plugin: Parent POM ................................. SUCCESS [  0.010 s]
[INFO] Plugin: Analysis: Japanese (kuromoji) .............. SUCCESS [  0.011 s]
[INFO] Plugin: Analysis: Smart Chinese (smartcn) .......... SUCCESS [  0.011 s]
[INFO] Plugin: Analysis: Polish (stempel) ................. SUCCESS [  0.012 s]
[INFO] Plugin: Analysis: Phonetic ......................... SUCCESS [  0.010 s]
[INFO] Plugin: Analysis: ICU .............................. SUCCESS [  0.010 s]
[INFO] Plugin: Cloud: Google Compute Engine ............... SUCCESS [  0.012 s]
[INFO] Plugin: Cloud: Azure ............................... SUCCESS [  0.012 s]
[INFO] Plugin: Cloud: AWS ................................. SUCCESS [  0.009 s]
[INFO] Plugin: Delete By Query ............................ SUCCESS [  0.023 s]
[INFO] Plugin: Language: Python ........................... SUCCESS [  0.011 s]
[INFO] Plugin: Language: JavaScript ....................... SUCCESS [  0.011 s]
[INFO] Plugin: Mapper: Size ............................... SUCCESS [  0.009 s]
[INFO] Plugin: JVM example ................................ SUCCESS [  0.020 s]
[INFO] Plugin: Example site ............................... SUCCESS [  0.010 s]
[INFO] QA: Parent POM ..................................... SUCCESS [  0.004 s]
[INFO] QA: Smoke Test Plugins ............................. SUCCESS [  0.009 s]
[INFO] QA: Smoke Test Shaded Jar .......................... SUCCESS [  0.017 s]
[INFO] QA: Smoke Test Multi-Node IT ....................... SUCCESS [  0.031 s]
```

Related to https://github.com/elastic/elasticsearch/pull/12775/files#r37074563

Also added a message when running license checker (cc @clintongormley).
</description><key id="101027817">12879</key><summary>[maven] rename artifactIds from `elasticsearch-something` to `something`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T14:55:37Z</created><updated>2015-08-21T09:36:18Z</updated><resolved>2015-08-18T13:26:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-14T15:46:58Z" id="131156535">Can the code in PluginManager that looks for elasticsearch- and renames it be removed? Also integration-tests.xml has logic to deal with this renaming when looking to see that the plugin was installed correctly, that should be removed too.

This way tests are picky about the change.
</comment><comment author="dadoonet" created="2015-08-14T15:55:07Z" id="131159612">@rmuir That's the plan indeed. I'm waiting for #12775 to be merged first as it changes a lot of things like this.
</comment><comment author="nik9000" created="2015-08-14T18:13:51Z" id="131198479">I like the idea behind this change - so long as all the plugins keep working.

In theory I hate that we're moving artifacts around. People depend on these artifacts and you really aren't supposed to rename them every couple of versions. But these are better names.
</comment><comment author="rjernst" created="2015-08-14T21:35:02Z" id="131243627">+1

&gt;  People depend on these artifacts

How so? Plugins should be installed via `bin/plugin`, and in this case, they are currently, and will be in the future, using the name without the prefix.
</comment><comment author="s1monw" created="2015-08-14T21:37:21Z" id="131243979">I think this change for GA is fine we should make it as simple as possible and maybe can make it work with and without hte prefix?
</comment><comment author="rjernst" created="2015-08-14T21:45:08Z" id="131245100">&gt; maybe can make it work with and without the prefix?

With an explicit list for ease of migration (assuming someone had been installing iwth the full name before)? I think new plugins we make (pulling more stuff out of core) should only use the short name.
</comment><comment author="s1monw" created="2015-08-14T21:45:50Z" id="131245190">&gt; With an explicit list for ease of migration (assuming someone had been installing iwth the full name before)? I think new plugins we make (pulling more stuff out of core) should only use the short name.

yeah maybe
</comment><comment author="nik9000" created="2015-08-14T22:02:16Z" id="131250601">Maven builds depend on plugins sometimes. I've done it myself. The list of
people who care is small in this case but in general its a pain when people
rename their artifacts.
On Aug 14, 2015 2:35 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; +1
&gt; 
&gt; People depend on these artifacts
&gt; 
&gt; How so? Plugins should be installed via bin/plugin, and in this case,
&gt; they are currently, and will be in the future, using the name without the
&gt; prefix.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12879#issuecomment-131243627
&gt; .
</comment><comment author="clintongormley" created="2015-08-15T14:30:49Z" id="131387674">It has just occurred to me that almost all the community plugins are called `elasticsearch-something`, eg `elasticsearch-kopf`, which then get installed as `kopf`.  Are we keeping this behaviour?
</comment><comment author="dadoonet" created="2015-08-15T14:47:49Z" id="131389622">Developers can now choose the plugin name within the property file.
Even if the repo name is elasticsearch-something they can use something as the plugin name.
</comment><comment author="rjernst" created="2015-08-15T20:46:19Z" id="131446879">We shoudln't be renaming things. If a plugin from a user is `elasticsearch-foo`, then that should be the directory name, and what we find in the descriptor file. If users want to rename their plugins (to remove the `elasticsearch-` prefix), they are free to do so.
</comment><comment author="cbuescher" created="2015-08-17T09:47:46Z" id="131755350">Should the OFFICIAL_PLUGINS list in PluginManager and the corresponding entries in `plugin-install.help` also be updated with this PR?
</comment><comment author="dadoonet" created="2015-08-17T12:53:42Z" id="131806785">@cbuescher I applied some changes based on your comment. Wanna review?
</comment><comment author="dadoonet" created="2015-08-17T13:39:48Z" id="131820402">I rebased all the code on master as there have been some changes lately.
@s1monw @spinscale Do you think you can give a look at it?
</comment><comment author="nik9000" created="2015-08-17T14:30:26Z" id="131840435">Left minor naming complaints but LGTM.
</comment><comment author="cbuescher" created="2015-08-17T16:36:12Z" id="131883345">@dadoonet I looked at your changes, left two small questions. I think someone else should have a closer look as well since I'm not sure how renaming of the artifacts affect anything in the test infra, staging etc...
</comment><comment author="dadoonet" created="2015-08-18T08:38:12Z" id="132120730">I added a new commit to also change the artifactId for dev-tools and rest-api-spec.
I did not change `elasticsearch-parent` though as `parent` does not look to me as a good name.

@rmuir @rjernst @cbuescher thoughts? 
</comment><comment author="rjernst" created="2015-08-18T08:50:36Z" id="132122966">@dadoonet This LGTM. I would personally still rename to just `parent` because it is still unique because of the groupId.
</comment><comment author="dadoonet" created="2015-08-18T09:03:40Z" id="132126020">@rjernst done. I also fixed the wrong `scm` links we had. Wanna check the latest commit?
</comment><comment author="cbuescher" created="2015-08-18T09:08:45Z" id="132127637">@dadoonet @rjernst I'm not sure if shortening everything is always a good idea. `parent` might be okay, since its a pom, but when the `artifactId` also determines artifact name for jars etc. I think common prefixes might be useful sometimes. But that's just my two cents.
</comment><comment author="dadoonet" created="2015-08-18T11:46:15Z" id="132184516">@jpountz I added a new commit for murmur3 plugin here. 
</comment><comment author="jpountz" created="2015-08-18T12:47:14Z" id="132195817">Thanks
</comment><comment author="dadoonet" created="2015-08-18T13:35:34Z" id="132207837">Needs to bake a bit in Jenkins and see if we can backport that in 2.0 safely.
</comment><comment author="dadoonet" created="2015-08-19T20:32:41Z" id="132773000">For the last day, I did not see issues in Jenkins with that change.
Can I backport it to 2.0 branch so it will be available for the next release?

@clintongormley WDYT?
</comment><comment author="dadoonet" created="2015-08-19T23:34:20Z" id="132825755">Also pushed to 2.0 branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release scripts: Split prepare_release into two scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12878</link><project id="" key="" /><description>In order to reflect our RC release process, we need to separate
the prepare_release script into two separate scripts.

One script now updates the documentation. That one can be executed
anytime and needs to be pushed after that.

The other script updates the version in Version.java and all pom.xml
files, but does not commit anymore. This allows to create a non snapshot
version locally, run mvn deploy, push the artifacts into S3 and, upon
successful tests, simply release them on sonatype. This also allows for
updates, because the S3 snapshot will include the commitId in their repo
as already pushed before.
</description><key id="101020264">12878</key><summary>Release scripts: Split prepare_release into two scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T14:22:10Z</created><updated>2015-08-14T14:45:44Z</updated><resolved>2015-08-14T14:45:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-14T14:36:04Z" id="131127041">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Change staging URL to reflect S3 bucket</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12877</link><project id="" key="" /><description>In order to create releases without actually changing the version
as part of a commit, we also need to reflect the path of the potentially
changing S3 repo.
</description><key id="101010269">12877</key><summary>PluginManager: Change staging URL to reflect S3 bucket</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T13:25:22Z</created><updated>2015-08-15T08:15:53Z</updated><resolved>2015-08-14T13:29:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-14T13:28:39Z" id="131106592">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Also deploy top level artifacts to S3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12876</link><project id="" key="" /><description>This commit adds the S3 wagon release profile also to dev-tools
and rest-api-spec and makes the actual repository path / bucket
configurable.
</description><key id="101009948">12876</key><summary>Also deploy top level artifacts to S3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T13:23:42Z</created><updated>2015-08-14T13:28:54Z</updated><resolved>2015-08-14T13:28:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-14T13:28:45Z" id="131106617">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify ContextIndexSearcher.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12875</link><project id="" key="" /><description>In particular this commit moves collector wrapping logic from
ContextIndexSearcher to QueryPhase.
</description><key id="101006793">12875</key><summary>Simplify ContextIndexSearcher.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>non-issue</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-14T13:05:42Z</created><updated>2015-08-21T09:32:29Z</updated><resolved>2015-08-14T13:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-14T13:33:42Z" id="131107451">LGTM this is a good step forward. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Murmur3 fields should not be indexed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12874</link><project id="" key="" /><description>```
PUT my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "foo": {
          "type": "murmur3"
        }
      }
    }
  }
}

GET my_index/_mapping/*/field/foo?include_defaults
```

return: `{"index": "not_analyzed"}`
</description><key id="100981739">12874</key><summary>Murmur3 fields should not be indexed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T10:18:21Z</created><updated>2015-09-16T13:37:06Z</updated><resolved>2015-08-18T10:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add epoch_ms to dynamic dates with format `yyyy/MM/dd`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12873</link><project id="" key="" /><description>When creating date fields dynamically:

```
PUT my_index/my_type/1
{
  "date_one": "2015-01-01", 
  "date_two": "2015/01/01"  
}
```

the date matching `strict_date_optional_time` adds the `||epoch_millis` format as an alternative, but the date matching `yyyy/MM/ss` doesn't:

```
{
   "my_index": {
      "mappings": {
         "my_type": {
            "properties": {
               "date_one": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "date_two": {
                  "type": "date",
                  "format": "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
               }
            }
         }
      }
   }
}
```

I think it should.
</description><key id="100980895">12873</key><summary>Add epoch_ms to dynamic dates with format `yyyy/MM/dd`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Dates</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T10:11:43Z</created><updated>2015-09-16T13:35:20Z</updated><resolved>2015-08-19T09:30:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove Environment.resolveConfig</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12872</link><project id="" key="" /><description>This method has multiple modes of resolving config files by
first looking in the config directory, then on the classpath,
and finally by prefixing with "config/" on the classpath.

Most of the places taking advantage of this were tests, so they
did not have to setup a real home dir with config. The only place
that was really relying on it was the code which loads names.txt
to randomly choose a node name.

This change fixes test to setup fake home dirs with their config
files. It also makes the logic for finding names.txt explicit:
look in config dir, and if it doesn't exist, load /config/names.txt
from the classpath.
</description><key id="100980486">12872</key><summary>Remove Environment.resolveConfig</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T10:07:55Z</created><updated>2015-08-18T16:52:52Z</updated><resolved>2015-08-14T20:56:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-14T12:49:51Z" id="131096232">LGTM
</comment><comment author="s1monw" created="2015-08-14T20:47:54Z" id="131235378">LGTM this really should go into beta2
</comment><comment author="nik9000" created="2015-08-14T20:52:07Z" id="131236273">LGTM
</comment><comment author="rjernst" created="2015-08-14T21:05:59Z" id="131239118">Pushed to 2.0 branch as well: bf0fe57
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to find a field mapper for field in nested query using field_value_factor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12871</link><project id="" key="" /><description>Here's the mapping:

``` http
PUT books-index
{
  "mappings": {
    "books": {
      "properties": {
        "tags": {
          "type": "nested",
          "fields": {
            "name": {
              "type": "string"
            },
            "weight": {
              "type": "float"
            }
          }
        }
      }
    }
  }
}
```

Then doing a nested query using a field_value_factor fails with an error

``` http
GET books-index/books/_search
{
  "query": {
    "nested": {
      "path": "tags",
      "score_mode": "sum",
      "query": {
        "function_score": {
          "query": {
            "match": {
              "tags.name": "world"
            }
          },
        "field_value_factor": {
            "field": "weight"
         }
        }
      }
    }
  }
}
```

The error: **"nested: ElasticsearchException[Unable to find a field mapper for field [weight]]"**

Interestingly, if there's one book in the index with tags - there's no error and the query works well.

**Why is this happening? how can I prevent the error when there are no books with tags in the index?**

Any ideas?

Thank you!
</description><key id="100968101">12871</key><summary>Unable to find a field mapper for field in nested query using field_value_factor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vrepsys</reporter><labels /><created>2015-08-14T09:11:35Z</created><updated>2015-08-15T08:29:50Z</updated><resolved>2015-08-14T13:12:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-14T13:12:34Z" id="131100977">I suspect this is because you gave `weight` instead of `tags.weight` as a field name. For such questions however, please use the [forums](https://discuss.elastic.co/) instead of creating a Github issue. We aim at using Github issues only for confirmed bugs and feature requests.
</comment><comment author="vrepsys" created="2015-08-14T13:32:12Z" id="131107220">There's a related question on stackoverflow since a long time ago that gets upvotes, but nobody can answer: http://stackoverflow.com/questions/27274240/function-score-query-with-field-value-factor-on-not-yet-existing-field

Also the issue very similar to this: https://github.com/elastic/elasticsearch/issues/10948
But it is not clear if and how it's been resolved.

Are you sure this is not an issue with elastic?

P.S. "tags.weight" instead of "tags" makes no difference.
</comment><comment author="clintongormley" created="2015-08-15T08:18:34Z" id="131314599">@vrepsys your mapping is incorrect.  You used `fields` instead of `properties`. 

```
PUT books-index
{
  "mappings": {
    "books": {
      "properties": {
        "tags": {
          "type": "nested",
          "properties": {
            "name": {
              "type": "string"
            },
            "weight": {
              "type": "float"
            }
          }
        }
      }
    }
  }
}
```

At least in 2.0 (not sure about before), your mapping fails with an exception
</comment><comment author="vrepsys" created="2015-08-15T08:29:50Z" id="131315862">Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhancement/terms lookup fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12870</link><project id="" key="" /><description>Relates to https://github.com/elastic/elasticsearch/pull/12042#issuecomment-131024096 .
- Java api: remove support for lookup cache in TermsLookupBuilder: TermsQueryParser doesn't support the cache field anymore, so if it gets set through java api, the subsequent parsing of that query will throw error
- Java api: restore support for minimumShouldMatch and disableCoord in TermsQueryBuilder: TermsQueryParser still parses those values although deprecated. These need to be present in the java api as well to get ready for the query refactoring, where the builders are the intermediate query format that we parse our json queries into. Whatever the parser supports need to be supported by the builder as well.
</description><key id="100965348">12870</key><summary>Enhancement/terms lookup fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T08:53:51Z</created><updated>2015-08-14T16:13:43Z</updated><resolved>2015-08-14T09:31:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-14T09:10:06Z" id="131033341">LGTM
</comment><comment author="javanna" created="2015-08-14T16:13:40Z" id="131165623">Marking this as breaking as the removal of the setter for `lookupCache` from `TermsLookupBuilder` breaks the java api.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Socket appender logging to unavailable Logstash node kills Elasticsearch node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12869</link><project id="" key="" /><description>We experienced an issue today in our production cluster, after a cluster upgrade from 1.6.x to 1.7.1.

(In all of the below, our "Elasticsearch cluster" is a 3 node setup running production data for our e-commerce platform, whereas our "Logstash cluster" is a different setup with other Elasticsearch nodes and the Logstash service)

Our whole Elasticsearch cluster is set to have a socket appender log option, to our Logstash cluster. Previously in 1.6.x when the Logstash service was unavailable on the cluster, nothing bad would happen (except missing log entries obviously), but after our upgrade to 1.7.x we yesterday experienced some weird behaviour.

It is as if about 5-10 minutes pass if the Logstash service dies, after which exactly one node in our Elasticsearch cluster will start to exhibit the following:
- Most of the times it will act as it's still part of the whole cluster, and will give cluster status and correct master response
- Some times, above _cat operations will give the 30sec timeout
- All queries, GETs, deleted, index operations and even a _cat/indices will result in a request that seems to never terminate

All other nodes in the cluster _seems_ to work. Not entirely sure.

Before we identified the missing Logstash service as the culprit, we attempted a full cluster restart. Bringing node 1 and node 2 up, those worked for a while, but when about 5 minutes passed and we brought up node 3 (and it started recovery), node 1 started it's weird behaviour again -- resulting in the recovery on node 3 to hang, because it attempted to read data from node 1.

We also have file logging in place, and nothing seems to be in the logs around this issue, on either node.

I'm not entirely sure about all of this, but wanted to report it just in case. It seems weird that a missing log source should be able to bring the whole cluster down - and since we didn't see the same behaviour when we were running 1.6.x, it's tempting to believe this is something that is caused by the upgrade to 1.7.1, made only a few days ahead of all of this.
</description><key id="100953454">12869</key><summary>Socket appender logging to unavailable Logstash node kills Elasticsearch node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HenrikOssipoff</reporter><labels><label>:Logging</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-08-14T07:39:04Z</created><updated>2015-10-16T09:40:28Z</updated><resolved>2015-10-16T09:40:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-15T08:06:20Z" id="131314250">Hi @HenrikOssipoff 

Thanks for reporting.  Could you add the details about how you set up the socket appender logging? Wondering if this is related to #12542
</comment><comment author="clintongormley" created="2015-10-16T09:40:28Z" id="148668442">We've just discussed this in FixItFriday and agree that logging across the network is a really bad idea for a busy Elasticsearch server, and not a configuration that we want to support.

There are plenty of tools available which can tail log files and ship them to a central server, including Logstash and [Filebeat](https://github.com/elastic/filebeat)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ClassLoader from Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12868</link><project id="" key="" /><description>Settings currently has a classloader member, which any user (plugin
or core ES code) can access to load classes/resources. This is extremely
error prone as setting the classloder on the Settings instance is a
public method. Furthermore, it is not really necessary. Classes that
need resources should load resources using normal means
(getClass().getResourceAsStream). Those that need classes
should use Class.forName, which will load the class with the
same classloader as the calling class. This means, in the few
places where classes are loaded by string name, they will use
the appropriate loader: either the default classloader which loads
core ES code, or a child classloader for each plugin.

This change removes the classloader member from Settings, as
well as other classloader related uses (except for a handful
of cases which must use a classloader, at least for now).
</description><key id="100948484">12868</key><summary>Remove ClassLoader from Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-14T06:56:33Z</created><updated>2015-08-19T23:05:05Z</updated><resolved>2015-08-14T09:15:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-14T09:04:45Z" id="131032312">There is one test which I don't understand why it got removed, but otherwise LGTM
</comment><comment author="jpountz" created="2015-08-14T09:09:18Z" id="131033245">LGTM
</comment><comment author="s1monw" created="2015-08-14T09:13:54Z" id="131035490">LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't create repository </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12867</link><project id="" key="" /><description>hi all, 

i have a problem like this :  https://github.com/elastic/elasticsearch/issues/11463
but first there is not  have a user 'elasticsearch'
i just operate es with root . so I think it is not a problem with permission.
so i come here and commit this question.

my es version 1.6.0.

curl -XGET 'http://localhost:9200'
{
  "status" : 200,
  "name" : "Stingray",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.6.0",
    "build_hash" : "cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0",
    "build_timestamp" : "2015-06-09T13:36:34Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}

---

i am just solve this problem
set  path.repo in elasticsearch.yml

like this:
path.repo: ["/mount/backups", "/mount/longterm_backups"]
</description><key id="100919960">12867</key><summary>Can't create repository </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaozhengmao</reporter><labels /><created>2015-08-14T02:58:38Z</created><updated>2015-08-14T06:56:57Z</updated><resolved>2015-08-14T06:56:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Terms aggregation unexpected results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12866</link><project id="" key="" /><description>We are facing an unexpected behavior with a term aggregation.
We are getting different results whether we use a term facet or its term aggregation equivalent.

Steps to reproduce:
#### Test case 1

**_Add  some test data:**_

```
cat &lt;&lt; EOF | curl -X POST http://localhost:9200/test/mention --data-binary @-
{
  "mention": {
    "hasMedia" : true,
    "original": {
      "hasMedia": false
    }
  }
}
EOF
```

**_Term facet and aggregation for the same field:**_

```
cat &lt;&lt; EOF | curl -X POST --data-binary @- 'http://localhost:9200/test/mention/_search?search_type=count&amp;pretty'
{
  "facets" : {
    "mention.hasMedia" : {
      "terms" : {
        "field" : "mention.hasMedia"
      }
    }
  },
  "aggregations" : {
    "mention.hasMedia" : {
      "terms" : {
        "field" : "mention.hasMedia"
      }
    }
  }
}
EOF
```

**_Results:**_*

```
{
  "aggregations": {
    "mention.hasMedia": {
      "buckets": [
        {
          "doc_count": 1,
          "key": "F"
        }
      ],
      "sum_other_doc_count": 0,
      "doc_count_error_upper_bound": 0
    }
  },
  "facets": {
    "mention.hasMedia": {
      "terms": [
        {
          "count": 1,
          "term": "T"
        }
      ],
      "other": 0,
      "total": 1,
      "missing": 0,
      "_type": "terms"
    }
  },
  "hits": {
    "hits": [],
    "max_score": 0,
    "total": 1
  },
  "_shards": {
    "failed": 0,
    "successful": 5,
    "total": 5
  },
  "timed_out": false,
  "took": 6
}
```

We would expect both terms facet and aggregation results to show count/doc_count: 1 and key/term: "T".

What I found more peculiar is that if you change the "hasMedia" field name for lets say "hasLinks" the results are the expected ones. Take a look at this next test case.
#### Test case 2

**_Sample data with changed field name (from hasMedia to hasLinks):**_

```
cat &lt;&lt; EOF | curl -X POST http://localhost:9200/test/mentionlinks --data-binary @-
{
  "mention": {
    "hasLinks" : true,
    "original": {
      "hasLinks": false
    }
  }
}
EOF
```

**_Same facet and aggregation for the new field name:**_

```
cat &lt;&lt; EOF | curl -X POST -d @- 'http://localhost:9200/test/mentionlinks/_search?search_type=count&amp;pretty'
{
  "facets" : {
    "mention.hasLinks" : {
      "terms" : {
        "field" : "mention.hasLinks"
      }
    }
  },
  "aggregations" : {
    "mention.hasLinks" : {
      "terms" : {
        "field" : "mention.hasLinks"
      }
    }
  }
}
EOF
```

**_New results:**_

```
{
  "aggregations": {
    "mention.hasLinks": {
      "buckets": [
        {
          "doc_count": 1,
          "key": "T"
        }
      ],
      "sum_other_doc_count": 0,
      "doc_count_error_upper_bound": 0
    }
  },
  "facets": {
    "mention.hasLinks": {
      "terms": [
        {
          "count": 1,
          "term": "T"
        }
      ],
      "other": 0,
      "total": 1,
      "missing": 0,
      "_type": "terms"
    }
  },
  "hits": {
    "hits": [],
    "max_score": 0,
    "total": 1
  },
  "_shards": {
    "failed": 0,
    "successful": 5,
    "total": 5
  },
  "timed_out": false,
  "took": 3
}
```

As you can see the results are now the expected ones. Showing count/doc_count: 1 and key/field: "T" in both facet and aggregation. So the issue seems to be related to field names in the doc?.

Im running 1.7.1 out of the box.

Any help will be greatly appreciated.
</description><key id="100899354">12866</key><summary>Terms aggregation unexpected results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pablod</reporter><labels /><created>2015-08-14T00:17:41Z</created><updated>2015-08-14T13:23:10Z</updated><resolved>2015-08-14T13:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-14T13:23:10Z" id="131104822">Indeed in 1.x we have issues because 1.7 tries to allow paths to be prefixed by the name of the path. But then it leads to inconsistencies when a document has the same type as a top-level property, like `mention` in your case. See https://github.com/elastic/elasticsearch/issues/8872 for more information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Template allows creation of index with 0 primary shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12865</link><project id="" key="" /><description>If you create a template with `number_of_shards` set to 0, then it accepts it and the eventual index fails. Attempted in ES 1.7.1. (If you try to do this directly with the index, then it will appropriately block the attempt.)

``` http
# Create the template
PUT /_template/test_shards
{
  "template": "test_shards*",
  "settings": {
    "number_of_shards" : 0
  }
}

# Create the index
PUT /test_shards
```

Once created, the cluster is in a red state because no primaries are allocated, which is kind of odd on its own because no primaries are missing.

``` http
DELETE /test_shards
```

When trying to delete the index, an exception is logged:

```
[2015-08-13 18:49:11,042][WARN ][cluster.action.index     ] [WallE] [test_shards]failed to ack index store deleted for index
java.lang.IllegalArgumentException: settings must contain a non-null &gt; 0 number of shards
    at org.elasticsearch.env.NodeEnvironment.lockAllForIndex(NodeEnvironment.java:445)
    at org.elasticsearch.indices.IndicesService.processPendingDeletes(IndicesService.java:733)
    at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction.lockIndexAndAck(NodeIndexDeletedAction.java:125)
    at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction.access$500(NodeIndexDeletedAction.java:49)
    at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction$1.doRun(NodeIndexDeletedAction.java:94)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

However, after an usually long delay, it eventually responds with success. Looking on disk, the index's directory still exists, but it is appropriately empty.
### Workaround

You can fix the issue by updating the template to fix the issue, creating the index, and then deleting it. (You can also just specify the `number_of_shards` directly at index time to override them template, but fixing the template is the appropriate fix if you run into this issue!)

``` http
# Recreate the index
PUT /test_shards
{
  "settings" : {
    "number_of_shards" : 1
  }
}

# Clean it up
DELETE /test_shards
```

This will appropriately cleanup the directory.
</description><key id="100888686">12865</key><summary>Template allows creation of index with 0 primary shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Index Templates</label><label>bug</label></labels><created>2015-08-13T23:09:37Z</created><updated>2015-08-14T21:43:16Z</updated><resolved>2015-08-14T21:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove unnecessary usage of extra index searchers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12864</link><project id="" key="" /><description>and use the IndexSeacher directly from the engine searcher.
</description><key id="100885844">12864</key><summary>Remove unnecessary usage of extra index searchers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-13T22:46:19Z</created><updated>2015-08-26T13:04:08Z</updated><resolved>2015-08-26T13:04:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-14T10:18:52Z" id="131059780">Changed the version to `2.0.0`. The second commit is larger than I want to it be for beta1.
</comment><comment author="martijnvg" created="2015-08-14T14:05:37Z" id="131114980">@jpountz I updated the PR based on the just pushed changes in ContextIndexSearcher.
</comment><comment author="jpountz" created="2015-08-14T14:12:06Z" id="131117106">Isn't the fact that ContextIndexSearcher calls `searchContext.clearReleasables(Lifetime.COLLECTION)` going to be an issue for p/c queries as these queries build recyclable data-structures in createWeight that they later reuse in their scorer?
</comment><comment author="martijnvg" created="2015-08-14T14:18:14Z" id="131119515">I don't think that is an issue, because by the time the ContextIndexSearcher is going to clear the releasables the p/c query has already executed completely. (the search in #doCreateWeight() and the scores created by the weight have then already done their jobs)
</comment><comment author="jpountz" created="2015-08-17T13:09:48Z" id="131813173">I'm not sure I understand why it is safe yet. For instance, ParentQuery.createWeight calls IndexSearcher.search, so clearReleasables will be called both in ParentQuery.createWeight and after execution of the main search request terminates?
</comment><comment author="martijnvg" created="2015-08-17T13:35:29Z" id="131819621">right now I  see, it is weird that `mvn verify` didn't fail last friday...

What I tried to get around this if add an extra search method to ContextIndexSearcher that search features which use releasable data structures and that run an extra search during the main search should use. This search method doesn't release the data structures, so that this can be done after the main search has been completed.
</comment><comment author="jpountz" created="2015-08-17T13:40:44Z" id="131820582">Ideally, I think the best option would be to push back the clearReleasables calls from ContextIndexSearcher to the callers. In case there are many callers, an ok-ish in-between might be to have the logic on Engine.Searcher?
</comment><comment author="martijnvg" created="2015-08-17T21:28:30Z" id="131967704">@jpountz I moved the clearReleasables part from the ContexIndexSearcher to SearchContext#search(...), with the reason the on places the ContextIndexSearcher is used, the search context is always available. Whereas if the logic would be added to `Searcher.Engine` class the engine searcher impl isn't always as type `ContexIndexSearcher`.
</comment><comment author="martijnvg" created="2015-08-18T13:16:30Z" id="132201762">@jpountz I updated the PR and removed all `clearReleasables()` calls from ContextIndexSearcher.

I wasn't able to `s/sc.searcher()/searcher/` in p/c queries, because  in the case of a dfs_\* search the provided searcher can be of type CachedDfSource and that implementation can't be used for searching.

Maybe as a followup PR we can merge CachedDfSource into ContextIndexSearcher? 
</comment><comment author="martijnvg" created="2015-08-19T20:56:38Z" id="132781231">@jpountz I updated the PR so that it works based on the changes in #12973
 (now the `s/sc.searcher()/searcher/` change is possible to make)
</comment><comment author="jpountz" created="2015-08-26T11:56:53Z" id="134963412">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor classes only plugged in by tests to use package private extension points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12863</link><project id="" key="" /><description>We previous used something like Class.forName to load mock classes,
where tests would set a setting that was _supposed_ to only be used by
tests. This change make these impls package private so that only tests
can change out these implementations, through test plugins.

closes #12784
</description><key id="100868695">12863</key><summary>Refactor classes only plugged in by tests to use package private extension points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-13T21:01:25Z</created><updated>2015-08-19T22:39:41Z</updated><resolved>2015-08-13T21:36:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-13T21:31:54Z" id="130850427">awesome LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't cache percolator query on loading percolators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12862</link><project id="" key="" /><description>No need to load catch this query since it's cheap and not reused.
If we cache it, it can cause assertions to be tripped since this
method is executed during postRecovery phase and might still run while
nodes are shutdown in tests.

here are related test failures:

http://build-us-00.elastic.co/job/elasticsearch-20-oracle-jdk6/11
http://build-us-00.elastic.co/job/elasticsearch-20-suse/5/
</description><key id="100860598">12862</key><summary>Don't cache percolator query on loading percolators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-13T20:24:44Z</created><updated>2015-08-15T07:58:30Z</updated><resolved>2015-08-13T20:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-13T20:25:15Z" id="130829726">LGTM
</comment><comment author="martijnvg" created="2015-08-13T20:29:00Z" id="130830528">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test for equals and hashcode to BaseQueryTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12861</link><project id="" key="" /><description>Add test for equals and hashcode to BaseQueryTestCase and modified AbstractQueryBuilder default hashcode impl
</description><key id="100827730">12861</key><summary>Add test for equals and hashcode to BaseQueryTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>test</label></labels><created>2015-08-13T17:43:29Z</created><updated>2015-08-14T12:06:20Z</updated><resolved>2015-08-14T12:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-14T10:46:54Z" id="131070379">@javanna left two small questions, otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Create pre release script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12860</link><project id="" key="" /><description>Refactored a part out of the release script, so the user can
change the version locally as well as move the documentation
and change the Version.java

The background of this change is to have a very simple release
process that puts stuff into a staging environment, so the beta
release can be tested, before it is officially released.

This means the build_release script can be removed soon.
</description><key id="100813065">12860</key><summary>Release: Create pre release script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>blocker</label><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-13T16:26:58Z</created><updated>2015-08-14T09:05:48Z</updated><resolved>2015-08-14T09:05:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-13T16:27:52Z" id="130750982">@s1monw would be awesome if you could take a look, but I basically copied most of the existing release..

you can easily run the script in your local branch, as it does not push, it just adds two commits to your local branch, which can easily be reverted again
</comment><comment author="s1monw" created="2015-08-13T19:34:50Z" id="130814077">I left some commetns - looks good though!
</comment><comment author="s1monw" created="2015-08-14T09:02:43Z" id="131031984">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update AWS SDK to 1.10.10 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12859</link><project id="" key="" /><description>cloned PR from https://github.com/elastic/elasticsearch-cloud-aws/pull/232/files
</description><key id="100783222">12859</key><summary>Update AWS SDK to 1.10.10 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">Fsero</reporter><labels><label>:Plugin Cloud AWS</label><label>blocker</label><label>review</label><label>upgrade</label><label>v2.0.0-beta2</label></labels><created>2015-08-13T14:31:29Z</created><updated>2015-11-22T10:15:20Z</updated><resolved>2015-08-24T21:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-13T14:59:24Z" id="130721684">The `pom.xml` change is missing, right?
</comment><comment author="Fsero" created="2015-08-13T15:15:14Z" id="130726027">is already on 1.10.10 thats because i didnt change it see on https://github.com/elastic/elasticsearch/blob/master/plugins/cloud-aws/pom.xml#L19
</comment><comment author="dadoonet" created="2015-08-13T15:35:54Z" id="130731961">It's `1.10.0` right?
</comment><comment author="Fsero" created="2015-08-13T15:45:33Z" id="130734279">Ohh!! totally sorry about that :)
</comment><comment author="Fsero" created="2015-08-13T16:00:14Z" id="130738897">done. 
</comment><comment author="daviddyball" created="2015-08-18T13:10:10Z" id="132200512">Will this change get back-ported to the 1.4 Docker containers or will it only come to 1.7+ versions of ES? Currently the errors in elastic/elasticsearch-cloud-aws#233 are preventing us from implementing correct backup/restore functionality with an S3 backend and we can't progress until this is fixed.
</comment><comment author="FestivalBobcats" created="2015-08-19T23:40:27Z" id="132827131">I'm curious as well how to patch ES 1.4.x - 1.7.0. I've applied the changes here in the 1.4 branch but tests are failing.
</comment><comment author="jhansen-tt" created="2015-08-24T16:29:16Z" id="134290426">Please release 1.7.2 ASAP with this fix.  This is a major issue.
</comment><comment author="dadoonet" created="2015-08-24T16:33:03Z" id="134291587">@jhansen-tt feel Free to test the SNAPSHOT version and report in this PR https://github.com/elastic/elasticsearch-cloud-aws/pull/234#issuecomment-134171132
</comment><comment author="jhansen-tt" created="2015-08-24T16:38:29Z" id="134293592">Is there a SNAPSHOT PPA?
</comment><comment author="dadoonet" created="2015-08-24T17:07:29Z" id="134303105">Just follow the instructions in the PR comment.
Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[allocation] change watermark low and high defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12858</link><project id="" key="" /><description> As for now, we have the current defaults:
- `cluster.routing.allocation.disk.watermark.low:85%`
- `cluster.routing.allocation.disk.watermark.high:90%`

But, even if you have plenty of free space on you 1Tb disk, you could end up not allocating any replica if you have less than 15gb free disk space.

This change propose to set:
- `cluster.routing.allocation.disk.watermark.low:1gb`
- `cluster.routing.allocation.disk.watermark.high:500mb`

as the new defaults.

Related to https://github.com/elastic/elasticsearch/pull/12853#issuecomment-130614764
Closes #12852.
</description><key id="100773019">12858</key><summary>[allocation] change watermark low and high defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Allocation</label><label>:Settings</label></labels><created>2015-08-13T13:48:00Z</created><updated>2015-08-18T11:25:35Z</updated><resolved>2015-08-13T16:29:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-13T13:49:00Z" id="130678425">-1 , ES uses relative defaults for a good reason. This would prevent a lot of users from running ES on embedded systems, or testing with ram drives. It is also not nearly enough safety for merges.
</comment><comment author="nik9000" created="2015-08-13T14:27:21Z" id="130695244">-1. Lee's second point is more valid than the first I think. Systems with
smaller hard drives are an exception rather than the rule so I don't think
we need to cater the default to them. But having room for big merges is
important.
On Aug 13, 2015 6:49 AM, "Lee Hinman" notifications@github.com wrote:

&gt; -1 , ES uses relative defaults for a good reason. This would prevent a lot
&gt; of users from running ES on embedded systems, or testing with ram drives.
&gt; It is also not nearly enough safety for merges.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12858#issuecomment-130678425
&gt; .
</comment><comment author="dadoonet" created="2015-08-13T16:29:33Z" id="130751333">Closing as won't be merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>internal: Never wrap IndexSearcher</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12857</link><project id="" key="" /><description>Wrapping of an IndexSearcher is tricky and we shouldn't do it.
- CreateContextIndexSearcher is now the IndexSearcher used on all places.
- The `IndexSearcherWrappingService` has been changed into `CreateContextIndexSearcher` and is now also creates `ContextIndexSearcher` for all shard operations.
- Moved the dfs logic into ContextIndexSearcher and remove CachedDfSource
- In tests removed the usage of AssertingIndexSearcher in MockSupport and replaced it with AssertingContextIndexSearcher in AssertingContextIndexSearcher. Not we properly assert all calls to IndexSearcher during tests (whereas before not all calls where properly asserted). The downside is that the logic in AssertingIndexSearcher had to be forked in AssertingContextIndexSearcher, but I think that shouldn't be too bad. 

This PR also fixes a bug with the `IndexSearchWrapper` extension point, in the case the dfs_query_\* search_type was enabled the ContextIndexSearcher would bypass the IndexSearcher created by the IndexSearcherWrapper. By never wrapping an IndexSearcher this bug can't manifest anymore.
</description><key id="100771866">12857</key><summary>internal: Never wrap IndexSearcher</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2015-08-13T13:41:58Z</created><updated>2015-08-18T15:11:15Z</updated><resolved>2015-08-18T15:11:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-13T13:52:58Z" id="130680667">I understand what you are trying to fix, but this change makes IndexSearcher management more complex and I'm a bit unhappy about it. Maybe we were wrong to allow IndexSearcher to be wrapped and should work on making IndexSearcher more wrapper-friendly first. Otherwise I'm afraid we'll always end up having to fix things due to method calls not being propagated to the wrapped instance as expected.
</comment><comment author="martijnvg" created="2015-08-14T08:15:08Z" id="131014889">@jpountz Totally agree to make IndexSearcher wrapper friendly. Unfortunately that may just take a while before it is available for ES. 

I think this can be seen as my best effort to at least remove complexity in ContextIndexSearcher (regarding dfs and the wrapped index searcher that exist today for testing purposes), which I think has worked, but unfortunately that adds extra management when we acquire a searcher and the extra places where need to manipulate ContextIndexSearcher.
</comment><comment author="martijnvg" created="2015-08-18T15:11:15Z" id="132244192">this is no longer relevant
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose points_only option through geo_shape field mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12856</link><project id="" key="" /><description>`SpatialPrefixTreeStrategy` includes an option for optimizing `geo_shape` types when the user expects to index points only. With a slight decrease in performance and increase in index size, when compared its `geo_point`, counterpart this option will treat a `geo_shape` structure similar to a `geo_point` with the added benefit of being compatible with other `geo_shape` types (polygons, multipolygons, etc.).

This provides an initial solution to a question often asked in training (and most recently raised by @polyfractal), "will geo_points be compatible with geo_shapes". For those use cases this enhancement will allow users to separate `geo_point` fields from `geo_shape` fields, achieve a performance boost for point only queries, yet still be able to use `geo_shape` queries on both `geo_point` and `geo_shape` fields.
</description><key id="100771236">12856</key><summary>Expose points_only option through geo_shape field mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2015-08-13T13:38:10Z</created><updated>2015-09-08T21:23:53Z</updated><resolved>2015-09-08T21:23:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>SharedClusterSnapshotRestoreIT#renameOnRestoreTest fails randomly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12855</link><project id="" key="" /><description>here is an example: http://build-us-00.elastic.co/job/es_core_master_oracle_6/1706/consoleFull

I will mute the guy now
</description><key id="100757560">12855</key><summary>SharedClusterSnapshotRestoreIT#renameOnRestoreTest fails randomly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-08-13T12:32:27Z</created><updated>2015-08-18T07:32:20Z</updated><resolved>2015-08-18T02:34:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-14T15:46:57Z" id="131156528">here is another one http://build-us-00.elastic.co/job/es_core_master_centos/6515/consoleFull
@imotov can you take a look?
</comment><comment author="s1monw" created="2015-08-18T07:32:20Z" id="132102790">thanks igor!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index "creation_date" not accurate when created with settings from another index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12854</link><project id="" key="" /><description>issue_topic - Index "creation_date" not accurate when created with settings from another index
issue_id - 12790
issue_url - https://github.com/elastic/elasticsearch/issues/12790
# PROBLEM STATEMENT:

Create Index operation API is capable of taking the settings as input
and create an index. The problem is that it is also using the
input creation_date and creating new index with same date
# APPROACH:

Find where it accepts the creating_date from the request object and
change it so that it is always generated at the time of index creation.
(it should effectively behave like uuid)
# Files Changed:

```
    modified:   core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
modified:   core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java
```

&gt; modified:   core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java

Here the part where it looks for creation_date from request object is removed.

&gt; modified:   core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java

Removed test case that tests that the date given in settings is same as the creation_date of the
newly created index.
# RESULT AFTER THE CHANGE:

input settings

``` javascript
{
     'settings':
          {  'index':
                       {'number_of_replicas': '1',
                         'version': {'created': '2000010'},
                         'creation_date': '1439384094544'
                         'uuid': 'ayzHgwB3Sgey-Pk_Okgwyg',
                         'number_of_shards': '5',
                       }
          }
}
```

new index settings

``` javascript
{
    'settings':
          {'index':
                     {'number_of_replicas': '1',
                      'version': {'created': '2000010'},
                      'creation_date': '1439456346424',
                      'uuid': 'fddpIVxNTTKPzqDakLAL4A',
                      'number_of_shards': '5'
                     }
         }
}
```

Please give feedback.
# Test case to check result

https://github.com/HarishAtGitHub/elasticsearch-tester/blob/master/12790.py
</description><key id="100752416">12854</key><summary>Index "creation_date" not accurate when created with settings from another index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">HarishAtGitHub</reporter><labels /><created>2015-08-13T12:01:12Z</created><updated>2016-01-19T11:26:46Z</updated><resolved>2016-01-19T11:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HarishAtGitHub" created="2015-08-13T12:02:08Z" id="130642837">pull request for the date replication reported in https://github.com/elastic/elasticsearch/issues/12790.
</comment><comment author="HarishAtGitHub" created="2015-08-13T12:04:22Z" id="130643959">I have just commented out the if loop as so that this change can be easily tracked in case it results in some other bug .(doing this as getting from input settings code has lived for such a long time)
</comment><comment author="HarishAtGitHub" created="2015-08-18T08:48:34Z" id="132122483">In the changes made to fix the version.created problem , I went with the "filtering params" in the starting place though this would not be ideal in most of the cases. But in our case I thought it might fit because  fixing in the MetaDataCreateIndexService(core) might not be appropriate because at this point we will not know who set the settings "is he the user" or "from internal code" ...  Fixing in MetaDataCreateIndexService(core) was ok for created_date feature removal as the feature is to be removed for all .But Our aim is to block external users and not internal usage. so the logic would be to filter it at the doorsteps of various inlets.

But this too has a drawback. the drawback is we should find all the possible doors through which users can enter the MetaDataCreateIndexService . one is the fixed one in CreateIndexRequest which is used by RestCreateIndexAction .
Are there any other routes through which users can set settings in CreateIndexRequest and feed it to MetaDataCreateIndexService ? Is CreateIndexRequestBuilder as possible route ?

Also another problem is what if users directly set settings as CreateIndexRequest too has 

``` java
/**
     * Constructs a new request to create an index with the specified name and settings.
     */
    public CreateIndexRequest(String index, Settings settings) {
        this.index = index;
        this.settings = settings;
    }
```
</comment><comment author="s1monw" created="2015-08-21T12:17:32Z" id="133403818">there are more problems like this, you can also fake the `index.version.created` etc. yet, I think we need this ability in our tests so I guess there should be a way to by-pass this by the test infrastructure. I think it would make sense to have some switch on `MetaDataCreateIndexService` to allow overriding these? 
The two I think of are:
- `index.creation_date` 
- `index.version.created`

I can take care of this if you want but I wanted to give you a heads-up first
</comment><comment author="HarishAtGitHub" created="2015-08-25T08:09:57Z" id="134517137">@s1monw  thanks for the review .

sorry for the delay @s1monw .... I did not notice that you reviewed (pls add the review tag if possible in the issue, I was waiting for review but left it as as there was no tag) ....
I will work on it today nd tmrw (2 days) .... If I cannot find a way out in 2 days I will let you know and you can take it....
</comment><comment author="HarishAtGitHub" created="2015-08-26T16:55:51Z" id="135107031">Hi @s1monw, I cannot think of a good acceptable solution for this problem now. Ya may be you or someone else can take it up ... I can look into some other issue ... eager to see the solution ...
</comment><comment author="rjernst" created="2015-08-26T22:55:37Z" id="135199812">I'm going to work on this soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[qa] multinode tests fails when you run low on disk space (85%)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12853</link><project id="" key="" /><description>Indeed, we check within the test suite that we have not unassigned shards.

But when the test starts on my machine I get:

```
[elasticsearch] [2015-08-13 12:03:18,801][INFO ][org.elasticsearch.cluster.routing.allocation.decider] [Kehl of Tauran] low disk watermark [85%] exceeded on [eLujVjWAQ8OHdhscmaf0AQ][Jackhammer] free: 59.8gb[12.8%], replicas will not be assigned to this node
```

```
  2&gt; REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests -Dtests.seed=2AE3A3B7B13CE3D6 -Dtests.class=org.elasticsearch.smoketest.SmokeTestMultiIT -Dtests.method="test {yaml=smoke_test_multinode/10_basic/cluster health basic test, one index}" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=ar_YE -Dtests.timezone=Asia/Hong_Kong -Dtests.rest.suite=smoke_test_multinode
FAILURE 38.5s | SmokeTestMultiIT.test {yaml=smoke_test_multinode/10_basic/cluster health basic test, one index} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{"cluster_name":"prepare_release","status":"yellow","timed_out":true,"number_of_nodes":2,"number_of_data_nodes":2,"active_primary_shards":3,"active_shards":3,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":3,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":50.0}]
```

I propose here to define for all integration tests:
- `cluster.routing.allocation.disk.watermark.low:200mb`
- `cluster.routing.allocation.disk.watermark.high:100mb`

Closes #12852.
</description><key id="100737009">12853</key><summary>[qa] multinode tests fails when you run low on disk space (85%)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-13T10:48:11Z</created><updated>2015-08-21T09:42:47Z</updated><resolved>2015-08-18T11:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-13T10:48:43Z" id="130610621">@rmuir Could you tell me what you think about this?
</comment><comment author="rmuir" created="2015-08-13T10:54:12Z" id="130614764">Sounds like elasticsearch has bad defaults to me. This will just hide those bad defaults.
</comment><comment author="dadoonet" created="2015-08-13T10:56:13Z" id="130615453">Makes sense. @dakrone WDYT? Should we change Elasticsearch defaults to absolute values?
</comment><comment author="rmuir" created="2015-08-13T10:57:32Z" id="130615836">Keep in mind every -D here makes the tests less realistic: there are already far too many -D's in the integration tests IMO.

Unless users start elasticsearch with 57 -D's, then we should not either.
</comment><comment author="dakrone" created="2015-08-13T13:50:55Z" id="130679612">&gt; Sounds like elasticsearch has bad defaults to me.

@rmuir explain why you think these are bad? I could see changing to 90/95% perhaps, but there isn't a good way to support the wide array of disk sizes people run ES on other than relative values.

&gt; Should we change Elasticsearch defaults to absolute values?

No. I think relative values are the only way to support as many disk sizes as we can by default. And if not, that's why they are dynamically configurable.
</comment><comment author="jpountz" created="2015-08-13T16:09:54Z" id="130744265">I see it as a test bug: the test makes incorrect assumptions about allocation rules.
</comment><comment author="rmuir" created="2015-08-13T16:12:55Z" id="130745554">integration tests should test our defaults. if you are annoyed that tests fail because they don't work as expected when you are "low" on disk space, then users will be equally annoyed when they are in the same situation.
</comment><comment author="rmuir" created="2015-08-13T16:13:10Z" id="130745662">please don't add the -D's. Fix the defaults.
</comment><comment author="dakrone" created="2015-08-13T16:16:02Z" id="130746740">&gt; please don't add the -D's. Fix the defaults.

"fixing" the defaults to run QA tests is not a good solution. This is the same as saying we should "fix" the JVM checker to allow running old JVMs because the tests fail if you try to run them using an old version.

&gt; users will be equally annoyed when they are in the same situation

I'll take "equally annoyed" versus "out of disk space and with corrupted indices or translogs" any day.
</comment><comment author="jpountz" created="2015-08-13T16:23:25Z" id="130749869">I agree we shouldn't add a`-D`, however I think we should fix the test to not expect all shards to be allocated instead of fixing our defaults (which look reasonable to me).
</comment><comment author="dadoonet" created="2015-08-13T16:24:27Z" id="130750189">&gt; I think we should fix the test to not expect all shards to be allocated instead of fixing our defaults (which look reasonable to me).

Agreed. Will come with an update.
</comment><comment author="rmuir" created="2015-08-13T16:43:24Z" id="130754302">&gt; I'll take "equally annoyed" versus "out of disk space and with corrupted indices or translogs" any day.

This has nothing to do with that. If elasticsearch has problems on disk full like that, then its because elasticsearch is broken.

Lucene does not have such problems.
</comment><comment author="dadoonet" created="2015-08-18T09:34:26Z" id="132142820">@jpountz I added a new commit. It now does not check anymore if we have unassigned shards and wait for yellow instead of green.

We still check that we have 2 nodes running which is I think the first goal for this qa test.
</comment><comment author="jpountz" created="2015-08-18T09:46:38Z" id="132146712">LGTM
</comment><comment author="colings86" created="2015-08-21T09:39:04Z" id="133351912">@dadoonet this doesn't seem to have been backported to the 2.0 branch. Should it be backported?
</comment><comment author="colings86" created="2015-08-21T09:42:47Z" id="133353887">@dadoonet sorry, my bad, it is in 2.0. Ignore the above
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[qa] multinode tests fails when you run low on disk space (85%)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12852</link><project id="" key="" /><description>Indeed, we check within the test suite that we have not unassigned shards.

But when the test starts on my machine I get:

```
[elasticsearch] [2015-08-13 12:03:18,801][INFO ][org.elasticsearch.cluster.routing.allocation.decider] [Kehl of Tauran] low disk watermark [85%] exceeded on [eLujVjWAQ8OHdhscmaf0AQ][Jackhammer] free: 59.8gb[12.8%], replicas will not be assigned to this node
```

```
  2&gt; REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests -Dtests.seed=2AE3A3B7B13CE3D6 -Dtests.class=org.elasticsearch.smoketest.SmokeTestMultiIT -Dtests.method="test {yaml=smoke_test_multinode/10_basic/cluster health basic test, one index}" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=ar_YE -Dtests.timezone=Asia/Hong_Kong -Dtests.rest.suite=smoke_test_multinode
FAILURE 38.5s | SmokeTestMultiIT.test {yaml=smoke_test_multinode/10_basic/cluster health basic test, one index} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{"cluster_name":"prepare_release","status":"yellow","timed_out":true,"number_of_nodes":2,"number_of_data_nodes":2,"active_primary_shards":3,"active_shards":3,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":3,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":50.0}] 
```

I think that for those tests we need to disable low watermark.
</description><key id="100731397">12852</key><summary>[qa] multinode tests fails when you run low on disk space (85%)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>test</label></labels><created>2015-08-13T10:12:09Z</created><updated>2015-08-18T11:23:57Z</updated><resolved>2015-08-18T11:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Dont leave leftover files on unsuccessful installs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12851</link><project id="" key="" /><description>If the plugin manager cannot successfully install a plugin, ensure
that every directory is cleaned up again. This includes

plugins/foo
config/foo
bin/foo

Closes #12749
</description><key id="100717625">12851</key><summary>PluginManager: Dont leave leftover files on unsuccessful installs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>blocker</label><label>bug</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-08-13T08:53:39Z</created><updated>2016-03-10T18:15:34Z</updated><resolved>2015-10-08T08:15:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-13T09:27:41Z" id="130590118">Left 2 small comments. It looks good to me.
</comment><comment author="spinscale" created="2015-09-01T12:33:29Z" id="136698419">fixed the review comments, rebased against master with a fair share of changes and only run the logic to copy `bin` and `config` dirs if actually needed, otherwise exit early

please give it another look
</comment><comment author="dadoonet" created="2015-09-03T11:26:24Z" id="137412022">@spinscale I left some comments.
</comment><comment author="nik9000" created="2015-09-15T13:25:51Z" id="140392467">@spinscale, there are a few comments around more you could do here but I think its an improvement as is. Its worth merging, I think.

I love the tests. Very descriptive method names help a ton.

If you want to add the tests for sourceConfigPath being a file instead of a dir I'd cause that to be an "abort fast" thing. Its a pretty broken plugin that does it.
</comment><comment author="spinscale" created="2015-10-07T11:27:27Z" id="146160920">@dadoonet updated your last comment and rebased against master, mind to take another view?
</comment><comment author="nik9000" created="2015-10-07T16:34:23Z" id="146254907">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed unused factor parameter in DateHistogramBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12850</link><project id="" key="" /><description>DateHistogramParser does not recognise this parameter and therefore setting it would throw an exception as noted in https://github.com/elastic/elasticsearch/issues/6490. It is also not documented.
</description><key id="100716770">12850</key><summary>Removed unused factor parameter in DateHistogramBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>breaking</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-13T08:50:17Z</created><updated>2015-08-21T09:37:48Z</updated><resolved>2015-08-13T09:18:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-13T08:50:46Z" id="130580103">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests fail when building ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12849</link><project id="" key="" /><description>I've checked out ES source with v1.7.2 tag and when I try to build tests fail with the following tests:

This happens on win7 with Java 8 + Maven 3.3.3

Tests with failures (first 3 out of 12):
- org.elasticsearch.index.codec.postingformat.ElasticsearchPostingsFormatTest.testDocsAndFreqsAndPositionsAndOffsetsAndPayloads
- org.elasticsearch.index.codec.postingformat.ElasticsearchPostingsFormatTest.testDocsOnly
- org.elasticsearch.index.codec.postingformat.ElasticsearchPostingsFormatTest.testRandom

[INFO] JVM J0:     1.68 ..  1811.50 =  1809.81s
[INFO] JVM J1:     1.68 ..  1811.46 =  1809.78s
[INFO] JVM J2:     1.68 ..  1811.46 =  1809.78s
[INFO] Execution time total: 30 minutes 11 seconds
[INFO] Tests summary: 758 suites, 5083 tests, 12 errors, 69 ignored (60 assumptions)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 30:30 min
[INFO] Finished at: 2015-08-13T16:10:10+08:00
[INFO] Final Memory: 23M/597M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.2:junit4 (tests) on project elasticsearch: Execution tests of goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.2:junit4 failed: D:\work\es\elasticsearch1.7.1\target\junit4-ant-3382632566736328075.xml:16: There were test failures: 758 suites, 5083 tests, 12 errors, 69 ignored (60 assumptions) -&gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
</description><key id="100713756">12849</key><summary>Tests fail when building ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">focusme</reporter><labels /><created>2015-08-13T08:27:53Z</created><updated>2015-08-13T08:32:25Z</updated><resolved>2015-08-13T08:32:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-13T08:32:25Z" id="130574293">please try running tests on master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure elasticsearch.bat works with spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12848</link><project id="" key="" /><description>When we added spaces to integration test paths, the distribution integ tests started failed on windows. I changed the paths back to not contain spaces (ie quieted the test), until we can fix the windows batch file.

The first hurdle is just getting the batch file to run when spaces are in the _path_ to the batch file. I have not yet found the magic ant incantation. My latest attempt looks like this:
https://gist.github.com/rjernst/9c9dcda71b1ac9ee1f85
</description><key id="100708753">12848</key><summary>Make sure elasticsearch.bat works with spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-13T07:46:20Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-09-11T08:35:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-13T07:48:31Z" id="130567236">To see the failures, you can change `integ.scratch` and `integ.deps` to contain spaces in pom.xml.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show more details of closed indices in _cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12847</link><project id="" key="" /><description>From [this forum post](https://discuss.elastic.co/t/size-of-closed-index/27302).

While we let users know an index is closed using `_cat/indices`, it'd be handy if we could show them things like the number of shards, docs and store size.
</description><key id="100707294">12847</key><summary>Show more details of closed indices in _cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-08-13T07:32:52Z</created><updated>2016-02-10T20:49:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-13T10:42:27Z" id="130609540">I don't think it is feasible. The only way to get this information is to open the index. About the only thing we could change is to include the primary and replica settings for closed indices (because those come from the cluster state). However, there is no guarantee that they reflect what is on the disk.  You could delete the closed index and add another with the same name and different settings, and we would be none the wiser until trying to open the index.

I think I've just convinced myself that we shouldn't change this.
</comment><comment author="markwalkom" created="2015-08-13T10:45:01Z" id="130609909">There's no chance we can keep some sort of data in cluster state on this?
I think it'd be very useful, even if not just for _cat.

And yeah, I can appreciate that someone could delete the files etc etc, but the likelihood of that versus the ability to be able to see what's on a node - to the best of our understanding - is important.
</comment><comment author="bleskes" created="2016-01-28T14:40:26Z" id="176212482">a side note for whoever ends up here looking for info (the feature is still good imho) - a current alternative is to use the [shard stores](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shards-stores.html)
</comment><comment author="seang-es" created="2016-02-10T20:49:56Z" id="182577613">_shard_stores does not seem to return data for closed indices
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bubbling up the mapping exception msg.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12846</link><project id="" key="" /><description>Closes #12839.
</description><key id="100703974">12846</key><summary>Bubbling up the mapping exception msg.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels /><created>2015-08-13T07:05:11Z</created><updated>2015-08-29T07:52:01Z</updated><resolved>2015-08-29T07:51:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-13T07:27:51Z" id="130564314">We should not do this. The exception message is already there. Structured exceptions must be not following the cause chain all the way?
</comment><comment author="xuzha" created="2015-08-13T07:44:06Z" id="130566625">Yea, pretty duplicated. 
</comment><comment author="xuzha" created="2015-08-13T08:07:47Z" id="130570163">OK, the code follows the cause chain down to the last ES exception. In this case the root cause is MapperParsingException, which caused by IllegalArgumentException. We have full description in IllegalArgumentException rather than in MapperParsingException.

When print root cause, it just get MapperParsingException detail message and print it out. 
We should print caused_by block for MapperParsingException in side of the root cause block, but skipped that [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/ElasticsearchException.java#L295)
</comment><comment author="xuzha" created="2015-08-29T07:51:43Z" id="135957794">Close this, we should not do this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't show access_key and filter_key in S3 repository settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12845</link><project id="" key="" /><description>In #11265 we added an ability to filter out sensitive repository settings. This commit uses this change to filter out access_key and filter_key in S3 repository settings.

Closes elastic/elasticsearch-cloud-aws#184
</description><key id="100669083">12845</key><summary>Don't show access_key and filter_key in S3 repository settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Plugin Cloud AWS</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-13T01:13:06Z</created><updated>2015-08-24T10:53:51Z</updated><resolved>2015-08-19T20:36:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-13T14:14:36Z" id="130691455">LGTM, left a couple of questions
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cloud-aws] Move integration tests to unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12844</link><project id="" key="" /><description>We now use a `AwsEc2ServiceMock` which simulates responses from AWS API.

We can test with this that settings are really used as expected.

I just started to implement some tests. More to come!

I also removed Ec2DiscoveryITest as the goal of this class was only to check that when we start elasticsearch with this plugin, elasticsearch works fine.
We don't need that anymore as we now have RestIT which do that right (and better)!

Unsure if we want it in 2.0.0.Beta1 or later...
</description><key id="100666187">12844</key><summary>[cloud-aws] Move integration tests to unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>review</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-13T00:46:13Z</created><updated>2015-09-10T13:48:13Z</updated><resolved>2015-09-10T13:22:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-25T14:16:56Z" id="134600355">@s1monw wanna review this change?
</comment><comment author="dadoonet" created="2015-08-28T07:18:53Z" id="135659467">@bleskes Could you help me on this?
</comment><comment author="bleskes" created="2015-08-28T19:23:07Z" id="135865969">I took a look and left some minor comments. I'm not expert in this code nor what it is expected to do, so I'm not sure how much it's worth but LGTM. I did expect to find some kind of integration test with ES's ZenDiscovery . Maybe use the fact we can mock the AWS services and try to form a cluster?
</comment><comment author="dadoonet" created="2015-09-07T15:10:02Z" id="138321215">@bleskes I adressed your comments. Thanks!
I also rebased on latest 2.0 branch.

@rjernst Do you think you can also give a look to this?
</comment><comment author="dadoonet" created="2015-09-10T13:22:24Z" id="139232926">Closed by fe74219 in 2.x branch.
Will follow up with another PR for master branch because of the split between s3 and ec2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move free disk log entries to TRACE level when there is plenty of free disk space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12843</link><project id="" key="" /><description>For messages like the following where it is nowhere near the disk threshold, it will be nice to log them at the TRACE level.  Even at the DEBUG level, it is too noisy and can drown other important debug messages.  For example, I am looking at this single day log file that has 53K+ log entries like the one below:

```
[2015-08-11 11:41:54,600][DEBUG][cluster.routing.allocation.decider] [node_name] Node [Sa-w1xe8Q4Si4V30Tb_Vrg] has 94.62963991540137% free disk (2072797175808 bytes)
```
</description><key id="100655173">12843</key><summary>Move free disk log entries to TRACE level when there is plenty of free disk space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-08-12T23:15:23Z</created><updated>2015-10-30T21:07:21Z</updated><resolved>2015-10-30T21:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="navneet83" created="2015-08-15T15:59:35Z" id="131395909">I'll take this one and work on it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>stats aggregation returns 500 error when performed over an invalid field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12842</link><project id="" key="" /><description>Performing a stats aggregation over a string field throws 500 error with an exception as opposed to a simplified error stating the the field specified is not valid for a stats aggregation.

I would expect a 400 for example since the client to fix their request before retrying it. Additionally it would help if the error message was more indicative of the problem as opposed to a SearchPhaseExecutionException. I would hope we should be able to detect this prior to performing the actual search.

```
GET /stack/_search?search_type=count
{
  "aggs": {
    "dateAggTest": {
      "stats": {
        "field": "title"
      }
    }
  }
}
```

```
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[hjvyPMaiTgewDbpd9j67_A][stack][0]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[hjvyPMaiTgewDbpd9j67_A][stack][1]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[hjvyPMaiTgewDbpd9j67_A][stack][2]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[hjvyPMaiTgewDbpd9j67_A][stack][3]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[hjvyPMaiTgewDbpd9j67_A][stack][4]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}]",
   "status": 500
}
```
</description><key id="100642739">12842</key><summary>stats aggregation returns 500 error when performed over an invalid field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Aggregations</label><label>:Exceptions</label><label>adoptme</label><label>bug</label></labels><created>2015-08-12T21:47:14Z</created><updated>2015-08-17T07:38:59Z</updated><resolved>2015-08-17T07:38:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-13T10:12:31Z" id="130602468">Agreed, should be a 400 - probably need a wider review of exceptions too
</comment><comment author="xuzha" created="2015-08-14T07:58:13Z" id="131011234">We should have a better error message here.

And should we just treat every ClassCastException is a 400 rather than 500?  If user send correct request and get 400, OK, you find a bug we are going to fix it.
</comment><comment author="jpountz" created="2015-08-14T13:26:41Z" id="131105700">&gt; And should we just treat every ClassCastException is a 400 rather than 500? If user send correct request and get 400, OK, you find a bug we are going to fix it.

I don't think we can generalize that class-cast exceptions are user-input errors rather than internal errors. We should just better validate that fields have the expected type.

It might be harder with scripts as scripts can do pretty much anything, but I assume that if we can make it work well on fields, that would be a good start already.
</comment><comment author="djschny" created="2015-08-14T16:30:04Z" id="131170742">+1 @jpountz, I would assume scripts to be out of scope. Not enough gain for the amount of work it would take to handle all cases, etc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting and sub documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12841</link><project id="" key="" /><description>I need a tip, please! We append several attachments to our elastic document. Alle attachments are stored in an array of sub documents called "ANY_BLOB". This is mapped to the "attachment" type. Per mapping we add a Field "Reference" to the ANY_BLOB sub document mapping. This is stored and retrieved well. But if we use the "highlight"-feature, we got the highlighting of some of the "ANY_BLOB"s, no idea to which subdocument it belongs. Is there any way to assign the "Reference"-Field to the highlighting? In fact we have to know, which of the ANY_BLOB-Attachments caused the highlighting.
</description><key id="100622325">12841</key><summary>Highlighting and sub documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreasTurowski</reporter><labels /><created>2015-08-12T19:57:14Z</created><updated>2015-08-13T10:27:38Z</updated><resolved>2015-08-13T10:27:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-13T04:52:09Z" id="130535171">If this is parent/child then I don't know the answer. If it is just that
you have a list of documents then you can look at the experimental
highlighter. It has a feature that will return sibling fields but there are
tons of caveats to how it works. Try it but be very careful to read the
docs.
On Aug 12, 2015 12:57 PM, "AndreasTurowski" notifications@github.com
wrote:

&gt; I need a tip, please! We append several attachments to our elastic
&gt; document. Alle attachments are stored in an array of sub documents called
&gt; "ANY_BLOB". This is mapped to the "attachment" type. Per mapping we add a
&gt; Field "Reference" to the ANY_BLOB sub document mapping. This is stored and
&gt; retrieved well. But if we use the "highlight"-feature, we got the
&gt; highlighting of some of the "ANY_BLOB"s, no idea to which subdocument it
&gt; belongs. Is there any way to assign the "Reference"-Field to the
&gt; highlighting? In fact we have to know, which of the ANY_BLOB-Attachments
&gt; caused the highlighting.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12841.
</comment><comment author="AndreasTurowski" created="2015-08-13T05:12:50Z" id="130537630">THX! You mean this one?
https://github.com/wikimedia/search-highlighter https://github.com/wikimedia/search-highlighter

&gt; Am 13.08.2015 um 06:53 schrieb Nik Everett notifications@github.com:
&gt; 
&gt; If this is parent/child then I don't know the answer. If it is just that
&gt; you have a list of documents then you can look at the experimental
&gt; highlighter. It has a feature that will return sibling fields but there are
&gt; tons of caveats to how it works. Try it but be very careful to read the
&gt; docs.
&gt; On Aug 12, 2015 12:57 PM, "AndreasTurowski" notifications@github.com
&gt; wrote:
&gt; 
&gt; &gt; I need a tip, please! We append several attachments to our elastic
&gt; &gt; document. Alle attachments are stored in an array of sub documents called
&gt; &gt; "ANY_BLOB". This is mapped to the "attachment" type. Per mapping we add a
&gt; &gt; Field "Reference" to the ANY_BLOB sub document mapping. This is stored and
&gt; &gt; retrieved well. But if we use the "highlight"-feature, we got the
&gt; &gt; highlighting of some of the "ANY_BLOB"s, no idea to which subdocument it
&gt; &gt; belongs. Is there any way to assign the "Reference"-Field to the
&gt; &gt; highlighting? In fact we have to know, which of the ANY_BLOB-Attachments
&gt; &gt; caused the highlighting.
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; https://github.com/elastic/elasticsearch/issues/12841.
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/12841#issuecomment-130535171.
</comment><comment author="nik9000" created="2015-08-13T05:31:21Z" id="130540685">Yup. The docs I was describing are
https://github.com/wikimedia/search-highlighter/blob/master/docs/fetch_fields.md

On Thu, Aug 13, 2015 at 1:12 AM, AndreasTurowski notifications@github.com
wrote:

&gt; THX! You mean this one?
&gt; https://github.com/wikimedia/search-highlighter &lt;
&gt; https://github.com/wikimedia/search-highlighter&gt;
&gt; 
&gt; &gt; Am 13.08.2015 um 06:53 schrieb Nik Everett notifications@github.com:
&gt; &gt; 
&gt; &gt; If this is parent/child then I don't know the answer. If it is just that
&gt; &gt; you have a list of documents then you can look at the experimental
&gt; &gt; highlighter. It has a feature that will return sibling fields but there
&gt; &gt; are
&gt; &gt; tons of caveats to how it works. Try it but be very careful to read the
&gt; &gt; docs.
&gt; &gt; On Aug 12, 2015 12:57 PM, "AndreasTurowski" notifications@github.com
&gt; &gt; wrote:
&gt; &gt; 
&gt; &gt; &gt; I need a tip, please! We append several attachments to our elastic
&gt; &gt; &gt; document. Alle attachments are stored in an array of sub documents
&gt; &gt; &gt; called
&gt; &gt; &gt; "ANY_BLOB". This is mapped to the "attachment" type. Per mapping we
&gt; &gt; &gt; add a
&gt; &gt; &gt; Field "Reference" to the ANY_BLOB sub document mapping. This is stored
&gt; &gt; &gt; and
&gt; &gt; &gt; retrieved well. But if we use the "highlight"-feature, we got the
&gt; &gt; &gt; highlighting of some of the "ANY_BLOB"s, no idea to which subdocument
&gt; &gt; &gt; it
&gt; &gt; &gt; belongs. Is there any way to assign the "Reference"-Field to the
&gt; &gt; &gt; highlighting? In fact we have to know, which of the
&gt; &gt; &gt; ANY_BLOB-Attachments
&gt; &gt; &gt; caused the highlighting.
&gt; &gt; &gt; 
&gt; &gt; &gt; &#8212;
&gt; &gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; &gt; https://github.com/elastic/elasticsearch/issues/12841.
&gt; &gt; &gt; 
&gt; &gt; &gt; &#8212;
&gt; &gt; &gt; Reply to this email directly or view it on GitHub &lt;
&gt; &gt; &gt; https://github.com/elastic/elasticsearch/issues/12841#issuecomment-130535171
&gt; &gt; &gt; .
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12841#issuecomment-130537630
&gt; .
</comment><comment author="clintongormley" created="2015-08-13T10:27:38Z" id="130605227">This question is better suited to the forums: https://discuss.elastic.co/

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`update_all_types` missing from REST spec and tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12840</link><project id="" key="" /><description>The `update_all_types`  parameter needs to be added to the REST test api spec, and should have tests added otherwise clients won't know about it.
</description><key id="100603104">12840</key><summary>`update_all_types` missing from REST spec and tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>:REST</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-08-12T18:15:59Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-08-26T13:33:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping conflict exception not bubbling up to root cause</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12839</link><project id="" key="" /><description>A mapping conflict such as:

```
PUT my_index
{
  "mappings": {
    "type_one": {
      "properties": {
        "text": {
          "type": "string",
          "analyzer": "standard",
          "search_analyzer": "whitespace"
        }
      }
    },
    "type_two": {
      "properties": {
        "text": {
          "type": "string"
        }
      }
    }
  }
}
```

throws an exception like:

```
    {
       "error": {
          "root_cause": [
             {
                "type": "mapper_parsing_exception",
                "reason": "mapping [type_two]",
                "stack_trace": "MapperParsingException[mapping [type_two]]; ... 6 more\n"
             }
          ],
          "type": "mapper_parsing_exception",
          "reason": "mapping [type_two]",
          "caused_by": {
             "type": "illegal_argument_exception",
             "reason": "Mapper for [text] conflicts with existing mapping in other types:\n[mapper [text] has different analyzer, mapper [text] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types.]",
             "stack_trace": "java.lang....\n"
          },
          "stack_trace": "MapperParsingException[mapping [type_two]]; nested: IllegalArgumentException[Mapper for [text] conflicts with existing mapping in other types:\n[mapper [text] has different analyzer, mapper [text] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types.]];...\n"
       },
       "status": 400
    }
```

The `error.root_cause.reason` should contain the text from `error.type.caused_by.reason`
</description><key id="100600389">12839</key><summary>Mapping conflict exception not bubbling up to root cause</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Exceptions</label><label>bug</label><label>low hanging fruit</label><label>v2.1.0</label></labels><created>2015-08-12T18:02:22Z</created><updated>2015-10-19T15:08:35Z</updated><resolved>2015-10-19T15:08:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-14T05:52:13Z" id="130981384">I'm not sure if this is a bug.
The root causes here block looks like correct. Inside of it, there could be a caused_by key-value pair like this:

```
          "root_cause": [
             {
                "type": "mapper_parsing_exception",
                "reason": "mapping [type_two]",
                "stack_trace": "MapperParsingException[mapping [type_two]]; ... 6 more\n"
                "caused_by": {.......} // skipped
             }

```

We skip it on purpose, [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/BytesRestResponse.java#L130)
</comment><comment author="rjernst" created="2015-08-14T06:01:45Z" id="130983255">I don't think it is a bug, but a choice that was made. The root_cause stops at a subclass of ElasticsearchException, I believe. Perhaps @s1monw can elaborate on why we don't go down to the original root cause (the IAE here)?
</comment><comment author="jpountz" created="2015-08-14T13:27:46Z" id="131106205">+1 on improving exception rendering to try to better expose the actual root cause of the issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mvn test reproduction lines do not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12838</link><project id="" key="" /><description>When a test fails we print out the reproduction line, with the recent refactoring of the maven build logic, the line no longer works.

For example:

```
&#955; mvn verify -Pdev -Dskip.unit.tests -Dtests.seed=B8E9EE4B240CA1F3 -Dtests.class=org.elasticsearch.index.mapper.size.SizeMappingIT -Dtests.method="testBasic" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=th_TH_TH_#u-nu-thai -Dtests.timezone=Asia/Istanbul

[INFO] Executed tasks
[INFO] 
[INFO] --- maven-failsafe-plugin:2.18.1:verify (verify) @ elasticsearch ---
[INFO] Failsafe report directory: /home/hinmanm/src/elasticsearch/core/target/failsafe-reports
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Elasticsearch Build Resources ...................... SUCCESS [  0.471 s]
[INFO] Elasticsearch Rest API Spec ........................ SUCCESS [  0.195 s]
[INFO] Elasticsearch Parent POM ........................... SUCCESS [  0.648 s]
[INFO] Elasticsearch Core ................................. FAILURE [  5.412 s]
[INFO] Elasticsearch Distribution ......................... SKIPPED
[INFO] Elasticsearch with all optional dependencies ....... SKIPPED
[INFO] Elasticsearch Shaded Distribution .................. SKIPPED
[INFO] Elasticsearch TAR Distribution ..................... SKIPPED
[INFO] Elasticsearch ZIP Distribution ..................... SKIPPED
[INFO] Elasticsearch DEB Distribution ..................... SKIPPED
[INFO] Elasticsearch RPM Distribution ..................... SKIPPED
[INFO] Elasticsearch Plugin POM ........................... SKIPPED
[INFO] Elasticsearch Japanese (kuromoji) Analysis plugin .. SKIPPED
[INFO] Elasticsearch Smart Chinese Analysis plugin ........ SKIPPED
[INFO] Elasticsearch Stempel (Polish) Analysis plugin ..... SKIPPED
[INFO] Elasticsearch Phonetic Analysis plugin ............. SKIPPED
[INFO] Elasticsearch ICU Analysis plugin .................. SKIPPED
[INFO] Elasticsearch Google Compute Engine cloud plugin ... SKIPPED
[INFO] Elasticsearch Azure cloud plugin ................... SKIPPED
[INFO] Elasticsearch AWS cloud plugin ..................... SKIPPED
[INFO] Elasticsearch Delete By Query plugin ............... SKIPPED
[INFO] Elasticsearch Python language plugin ............... SKIPPED
[INFO] Elasticsearch JavaScript language plugin ........... SKIPPED
[INFO] Elasticsearch Mapper size plugin ................... SKIPPED
[INFO] Elasticsearch example JVM plugin ................... SKIPPED
[INFO] Elasticsearch Example site plugin .................. SKIPPED
[INFO] QA: Parent POM ..................................... SKIPPED
[INFO] QA: Smoke Test Plugins ............................. SKIPPED
[INFO] QA: Smoke Test Shaded Jar .......................... SKIPPED
[INFO] QA: Smoke Test Multi-Node IT ....................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 7.450 s
[INFO] Finished at: 2015-08-12T11:38:29-06:00
[INFO] Final Memory: 60M/956M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:2.18.1:verify (verify) on project elasticsearch: No tests were executed!  (Set -DfailIfNoTests=false to ignore this error.) -&gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &lt;goals&gt; -rf :elasticsearch
```

Specifically: `Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:2.18.1:verify (verify) on project elasticsearch: No tests were executed!`

Setting `-DfailIfNoTests=false` in the line does not work either.
</description><key id="100596418">12838</key><summary>mvn test reproduction lines do not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>build</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-12T17:43:23Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-08-14T20:52:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-12T17:50:04Z" id="130388841">This issue is killing me because in order to test a specific failure I need to run the **whole** `mvn verify` and wait for it to fail (or pass)
</comment><comment author="jasontedor" created="2015-08-14T18:56:52Z" id="131209352">It's worse than the test reproduction commands not working; at the moment even running a single integration test class is broken.
</comment><comment author="s1monw" created="2015-08-14T18:58:29Z" id="131209662">hmm this is only broken from top level right? can't you just go into the project and run mvn verify from there?
</comment><comment author="dakrone" created="2015-08-14T19:01:15Z" id="131210198">If you make changes to the `core` project and want to test a plugin however, this won't work because it pulls a snapshot of ES instead of using the local copy?
</comment><comment author="jasontedor" created="2015-08-14T19:03:48Z" id="131210635">@s1monw No, it looks like it no longer supports running integration test classes:

`12:57:34 &#9178; [jason:~/src/elastic/elasticsearch] master &#177; mvn clean verify -Dtests.class=org.elasticsearch.search.query.SearchQueryIT`

`[INFO] Elasticsearch Core ................................. FAILURE [ 36.993 s]`

`[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.16:junit4 (tests) on project elasticsearch: There were no executed tests: 0 suites, 0 tests -&gt; [Help 1]`

This is failing in core.
</comment><comment author="s1monw" created="2015-08-14T19:11:43Z" id="131212050">&gt; If you make changes to the core project and want to test a plugin however, this won't work because it pulls a snapshot of ES instead of using the local copy?

I ran `mvn install` which also puts stuff int he local repo so it won't download it. not ideal but it works well for me and not really an overhead.  -Dskip.unit.tests=true

&gt; This is failing in core.

try `mvn clean verify -Dtests.class=org.elasticsearch.search.query.SearchQueryIT  -Dskip.unit.tests=true`
I will look into making this simpler
</comment><comment author="jasontedor" created="2015-08-14T19:31:54Z" id="131215493">@s1monw Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the `node.enable_custom_paths` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12837</link><project id="" key="" /><description>This setting is useless now that we have the `path.shared_data` setting.

Resolves #12776
</description><key id="100593868">12837</key><summary>Remove the `node.enable_custom_paths` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>breaking</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T17:27:08Z</created><updated>2016-03-03T19:12:19Z</updated><resolved>2015-08-12T18:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-12T17:27:46Z" id="130383614">@rjernst can you take a look?
</comment><comment author="rjernst" created="2015-08-12T17:30:54Z" id="130384321">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES Cluster heap usage spikes up after index rolls over</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12836</link><project id="" key="" /><description>ES cluster heap usage spikes up and cluster health is red.  Theres nothing obvious in the the logs except for GC messages when the spike occurs.

![screen shot 2015-08-12 at 8 44 16 am](https://cloud.githubusercontent.com/assets/5067728/9229647/ffbf9ede-40d2-11e5-82c5-036a366115c0.png)

[2015-08-11 16:53:48,533][WARN ][monitor.jvm              ] [es101a] [gc][old][1653310][18240] duration [13s], collections [1]/[14.2s], total [13s]/[34.5m], memory [28.8gb]-&gt;[26.6gb]/[29.6gb], all_pools {[young] [2.1gb]-&gt;[1.7mb]/[2.4gb]}{[survivor] [316.1mb]-&gt;[0b]/[316.1mb]}{[old] [26.3gb]-&gt;[26.6gb]/[26.9gb]}
[2015-08-11 16:54:10,249][WARN ][monitor.jvm              ] [es101a] [gc][old][1653320][18242] duration [11.5s], collections [1]/[12.7s], total [11.5s]/[34.7m], memory [29.3gb]-&gt;[26.9gb]/[29.6gb], all_pools {[young] [2.3gb]-&gt;[82.3mb]/[2.4gb]}{[survivor] [316.1mb]-&gt;[0b]/[316.1mb]}{[old] [26.6gb]-&gt;[26.9gb]/[26.9gb]}
[2015-08-11 16:54:28,559][WARN ][monitor.jvm              ] [es101a] [gc][old][1653328][18244] duration [11.1s], collections [1]/[11.2s], total [11.1s]/[34.9m], memory [29.5gb]-&gt;[27gb]/[29.6gb], all_pools {[young] [2.4gb]-&gt;[175.7mb]/[2.4gb]}{[survivor] [253.7mb]-&gt;[0b]/[316.1mb]}{[old] [26.8gb]-&gt;[26.9gb]/[26.9gb]}
[2015-08-11 16:54:46,680][INFO ][monitor.jvm              ] [es101a] [gc][old][1653336][18247] duration [10.8s], collections [2]/[11.1s], total [10.8s]/[35.1m], memory [29.6gb]-&gt;[27.1gb]/[29.6gb], all_pools {[young] [2.4gb]-&gt;[257.5mb]/[2.4gb]}{[survivor] [250.3mb]-&gt;[0b]/[316.1mb]}{[old] [26.9gb]-&gt;[26.9gb]/[26.9gb]}

I'm currently running 1.5.0-1
elasticsearch-1.5.0-1.noarch
CentOS 6.5
56 Core
64GB Memory
20TB storage
</description><key id="100582731">12836</key><summary>ES Cluster heap usage spikes up after index rolls over</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rsomcio</reporter><labels /><created>2015-08-12T16:29:20Z</created><updated>2015-08-12T16:51:19Z</updated><resolved>2015-08-12T16:44:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-12T16:44:48Z" id="130366248">Hi @rsomcio 

This sounds very much like you are loading a whole lot of fielddata when your index rolls over.  Whatever it is, your heap is full.  You either need to use less heap or add more nodes.  It doesn't sound like a bug. I suggest looking for advice in the forums: https://discuss.elastic.co/
</comment><comment author="rsomcio" created="2015-08-12T16:51:19Z" id="130367835">@clintongormley Great.  Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to GCE API v1-rev71-1.20.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12835</link><project id="" key="" /><description /><key id="100571089">12835</key><summary>Update to GCE API v1-rev71-1.20.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T15:34:17Z</created><updated>2015-08-17T16:06:06Z</updated><resolved>2015-08-12T16:05:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>save result in variable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12834</link><project id="" key="" /><description>Hi,
       i just started using your PAMI. i can see Events and Actions results on screen. how can i save events and actions result in variable and save them in DB. is there any discussion form of PAMI?

Thanks,
</description><key id="100556084">12834</key><summary>save result in variable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qasimkhans</reporter><labels /><created>2015-08-12T14:28:57Z</created><updated>2015-08-12T14:56:45Z</updated><resolved>2015-08-12T14:56:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-12T14:56:45Z" id="130333165">Sorry @qasimkhans but I've never heard of PAMI.  I think you've got the wrong project.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Throw Exception for missing settings file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12833</link><project id="" key="" /><description>An Exception will be thrown when the given YAML-file doesn't exist.

Applied changed to the current sources to get rid of merge problems

Closes #11510
</description><key id="100541083">12833</key><summary> Throw Exception for missing settings file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">saschamarkus</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T13:19:39Z</created><updated>2015-08-18T08:01:01Z</updated><resolved>2015-08-13T12:26:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-13T12:26:06Z" id="130648772">This sounds reasonable to me and tests passed. Thanks @saschamarkus
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Min Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12832</link><project id="" key="" /><description>This is currently a WIP as the tests are not complete. It is up for review mostly to show how #12830 would work in practice. the min agg specific changes are in https://github.com/elastic/elasticsearch/commit/3ab5dc309c9615b2a1cef0ec7eab5a70bebde96c
</description><key id="100526246">12832</key><summary>Aggregations Refactor: Refactor Min Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-08-12T11:57:12Z</created><updated>2015-11-16T13:07:31Z</updated><resolved>2015-11-16T13:07:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-14T14:51:46Z" id="131134316">LGTM
</comment><comment author="colings86" created="2015-11-11T15:06:24Z" id="155806404">@jpountz could you re-review this? It has changed a bit since you LGTM'ed it
</comment><comment author="jpountz" created="2015-11-11T21:19:08Z" id="155913446">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Min Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12831</link><project id="" key="" /><description>This is currently a WIP as the tests are not complete. It is up for review mostly to show how https://github.com/elastic/elasticsearch/pull/12830 would work in practice
</description><key id="100526079">12831</key><summary>Aggregations Refactor: Refactor Min Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>review</label><label>WIP</label></labels><created>2015-08-12T11:55:47Z</created><updated>2015-08-12T11:56:10Z</updated><resolved>2015-08-12T11:56:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aggregation refactor: make aggregationFactory implement NamedWritable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12830</link><project id="" key="" /><description>Also makes AggregatorFactories implement Writable
</description><key id="100525805">12830</key><summary>Aggregation refactor: make aggregationFactory implement NamedWritable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>review</label></labels><created>2015-08-12T11:53:45Z</created><updated>2015-08-18T12:12:08Z</updated><resolved>2015-08-18T12:11:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-13T12:17:04Z" id="130646263">I gave it a quick look and I think the fact that most aggregation objects track their parent is going to make the refactoring a bit less straightforward than for queries (see comments about serialization and checking for parents in equals)
</comment><comment author="colings86" created="2015-08-14T11:46:14Z" id="131085042">@jpountz I've pushed a commit addressing your comments. If you could especially check the equals and hashCode methods in AggregatoryFactory, ValuesSourceAggregatorFactory, PipelineAggregatorFactory and AggregatorFactories, that would be great
</comment><comment author="javanna" created="2015-08-14T13:17:01Z" id="131102245">I had a look and this looks good to me, I think the main concern around equality checks has been addressed, all good besides that
</comment><comment author="jpountz" created="2015-08-14T14:45:58Z" id="131131879">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Investigate moving from Joda to java.time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12829</link><project id="" key="" /><description>Today we use a forked version of Joda for performance reasons.  This can lead to problems with class loading (eg #10290, #3557, #4660), and requires us to have hacks to ensure that our version of Joda is loaded first.  

We should investigate the possibility of replacing Joda with java.time.  One roadblock would be the need to reimplement the dateOptionalTimeParser, which doesn't have an equivalent java.time. See http://sourceforge.net/p/joda/mailman/message/34327973/ for more
</description><key id="100524438">12829</key><summary>Investigate moving from Joda to java.time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Dates</label><label>adoptme</label><label>high hanging fruit</label></labels><created>2015-08-12T11:44:51Z</created><updated>2017-03-30T06:42:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-08-14T21:34:26Z" id="131243529">Strong +1
</comment><comment author="jack-pappas" created="2015-09-13T17:40:22Z" id="139898886">Another alternative might be to use [date4j](http://www.date4j.net/) instead of Joda Time. It's supposedly faster than Joda and should also fix the timestamp-precision issues (Joda only maintains millisecond-level precision and truncates any timestamps more precise than that).
</comment><comment author="dadoonet" created="2015-09-13T17:54:38Z" id="139899510">Note that date4j uses a BSD license.
</comment><comment author="s1monw" created="2015-09-13T18:15:47Z" id="139900864">In general I think the less dependencies we have the better ie. I think cutting over to java.time would be ideal from a maintenance perspective
</comment><comment author="uschindler" created="2015-09-13T21:50:22Z" id="139921603">In addition, as java.time (which is a fork of joda anyways) is part of the JDK, it should get more attention by Hotspot deveopers. We may get performance improvements...

BTW: forbidden-apis 2.0 has new signatures around java.time, but they are also part of Lucene's signature files. If you want to switch, I would recommend to use those (or update to 2.0, which comes earlier).
</comment><comment author="uschindler" created="2015-09-13T21:52:06Z" id="139921687">See: https://github.com/policeman-tools/forbidden-apis/commit/c34a02afcd7856478e9adfd32be2fc5bf82ca268
</comment><comment author="jordansissel" created="2017-03-30T06:42:57Z" id="290317277">Slightly related -- Logstash will also be moving its date filter to use java.time. We will share any lessons learned if we get to this first.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose zen ElectMasterService as a Discovery extension point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12828</link><project id="" key="" /><description>Some users need to override ElectMasterService from zen to add functionality
inside their infrastructure. This commit allows to extend it.
</description><key id="100519297">12828</key><summary>Expose zen ElectMasterService as a Discovery extension point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T11:02:45Z</created><updated>2015-08-12T11:40:13Z</updated><resolved>2015-08-12T11:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Split SearchModule.configure() into separate methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12827</link><project id="" key="" /><description>This change splits SearchModule.configure() contents into separate methods so that sub-class of it (e.g. tests) can use only the parts they require
</description><key id="100519217">12827</key><summary>Split SearchModule.configure() into separate methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T11:02:09Z</created><updated>2015-08-12T11:10:37Z</updated><resolved>2015-08-12T11:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-12T11:03:48Z" id="130262118">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce a formal ExtensionPoint class to stream line extensions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12826</link><project id="" key="" /><description>This commit tries to add some infrastructure to streamline how extension
points should be strucutred. It's a simple approache with 4 implementations
for `highlighter`, `suggester`, `allocation_decider` and `shards_allocator`.
It simplifies adding new extension points and forces to register classes instead
of strings.
</description><key id="100510337">12826</key><summary>Introduce a formal ExtensionPoint class to stream line extensions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T10:07:46Z</created><updated>2015-08-13T14:43:13Z</updated><resolved>2015-08-13T12:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-12T15:33:33Z" id="130344162">One thought here is to split ExtensionPoint into two different variants, one for a Set and one for a Map. Right now it looks like we have a switch for this, with if/else in the impl. We just have those two types of extension points, either we keep a set of class impls, or we keep a map from name to impl. I think separating them would make the module code a little clearer, and less error prone when adding these (must choose the type of extension when defining, instead of accidentally calling the wrong register method).
</comment><comment author="rjernst" created="2015-08-12T16:04:04Z" id="130356266">This looks great in general, I think it will simplify a lot where we have extension points, so we get the same logic/exception checking. I left a few comments/questions.
</comment><comment author="rjernst" created="2015-08-12T16:49:43Z" id="130367463">LGTM. I left two more questions but those can be follow ups or ignored.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow plugins to add custom stats to Node Stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12825</link><project id="" key="" /><description>Work in progress pull request for #12526.

This pull request allows plugins to add their own custom stats to the Node Stats. Plugins can use the `PluginsModule.registerStatsService()` to register a new `PluginStatsService`: 

``` java
public static class MyPlugin extends AbstractPlugin {
        ...
        public void onModule(PluginsModule pluginsModule) {
            pluginsModule.registerStatsService(MyCustomStatsService.class);
        }
}
```

A PluginStatsService provides a set of plugin stat objects which are basically a key/category associated to a XContent. This XContent is useful because it can be easily streamed between nodes (even if the master node does not have the plugin installed) and it allows to build complex stats objects on plugin side. 

Here is a simple example of how a plugin can implement a PluginStatsService:

``` java
public static class MyCustomStatsService implements PluginStatsService {
        @Override
        public Collection&lt;PluginStat&gt; stats() {
            List&lt;PluginStat&gt; stats = new ArrayList&lt;&gt;();
            try {
                PluginStat stat = new PluginStat("foo", jsonBuilder()
                                                            .startObject()
                                                                .field("version", "1.0.3")
                                                                .field("foo_2", 1437580442979L)
                                                            .endObject()
                                                        .bytes());
                stats.add(stat);
            } catch (IOException e) {
                // We don't care
            }
            return stats;
        }
    }
```

Internally, the NodeService calls the PluginService.stats() method which iterates over all registered PluginStatsService and compute their stats. Note: plugins stats services are passed to the plugin service instance using Guice providers.

A plugin can return multiple stats with multiple categories, and the category name is used as namespace in the Node Stats response.

The REST API can accept a `plugins` flag to retrieve all custom plugins stats:

```
GET /_nodes/stats/plugins?pretty
{
  "cluster_name" : "elasticsearch",
  "nodes" : {
    "4_YaEiKeS-2_JLNLbdYBbA" : {
      "timestamp" : 1439371328976,
      "name" : "Tri-Man",
      "transport_address" : "inet[/127.0.0.1:9300]",
      "host" : "portable",
      "ip" : [ "inet[/127.0.0.1:9300]", "NONE" ],
      "foo" : {
        "version" : "1.0.3",
        "properties" : {
          "chunk_batch" : 123
        }
      },
      "session" : {
        "uptime" : 1439371328977,
        "uids" : [ 1, 5, 9, 7, 3 ]
      }
    }
  }
}
```

One can also use a custom flag to retrieve a given plugin stats. Here it retrieves the stat `session` that it provides by a plugin installed on node Tri-man only:

```
GET  /_nodes/stats/session?pretty
{
  "cluster_name" : "elasticsearch",
  "nodes" : {
    "4_YaEiKeS-2_JLNLbdYBbA" : {
      "timestamp" : 1439371356029,
      "name" : "Tri-Man",
      "transport_address" : "inet[/127.0.0.1:9300]",
      "host" : "portable",
      "ip" : [ "inet[/127.0.0.1:9300]", "NONE" ],
      "session" : {
        "uptime" : 1439371356029,
        "uids" : [ 1, 5, 9, 7, 3 ]
      }
    },
    "2JupIqE8QpChAHdtT-f2lQ" : {
      "timestamp" : 1439371356028,
      "name" : "Widget",
      "transport_address" : "inet[/127.0.0.1:9301]",
      "host" : "portable",
      "ip" : [ "inet[/127.0.0.1:9301]", "NONE" ]
    }
  }
}
```

Response filtering also works, which is cool:

```
GET 'localhost:9200/_nodes/stats?pretty&amp;filter_path=**.azure.**.c*'
{
  "nodes" : {
    "4_YaEiKeS-2_JLNLbdYBbA" : {
      "azure" : {
        "properties" : {
          "chunk_batch" : 123
        }
      }
    }
  }
}
```

There are some questions left:
- how plugin stats are shown and computed in Cluster Stats
- what to do in case of categery/namespace conflicts (2 different plugins returning a `foo` stat)
- maybe propagate request params to the plugin stats services

I personally like how it currently works but I'd be glad to have some feedback.
</description><key id="100507340">12825</key><summary>Allow plugins to add custom stats to Node Stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>:Stats</label><label>stalled</label></labels><created>2015-08-12T09:51:09Z</created><updated>2016-10-10T08:27:18Z</updated><resolved>2016-03-10T11:35:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-12T09:59:37Z" id="130243773">cant you circumvent name clashes, by simply grouping the stats by the plugin name, which should be unique or would that convulute the JSON?

also, should plugins only be able to register stats or also static information?
</comment><comment author="clintongormley" created="2016-03-10T11:35:35Z" id="194802083">This PR is out of date and probably no longer needed. Closing
</comment><comment author="tlrx" created="2016-10-10T08:27:18Z" id="252559840">This subject has revived in #20824.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace HTTP urls with HTTPS in PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12824</link><project id="" key="" /><description>Switch to use HTTPS by default for all hardcoded plugin URLs.
If users want to install via HTTP they can still specify a HTTP
URL manually.

Closes #12748
</description><key id="100505442">12824</key><summary>Replace HTTP urls with HTTPS in PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-08-12T09:43:07Z</created><updated>2015-11-22T10:15:20Z</updated><resolved>2015-08-31T09:53:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-12T10:14:34Z" id="130248889">LGTM
</comment><comment author="nik9000" created="2015-08-24T20:25:11Z" id="134366609">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Allow REST tests to run over https.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12823</link><project id="" key="" /><description>This adds new settings to the REST client, so that tests can be run over SSL
and optionally use an alternate truststore.
</description><key id="100500682">12823</key><summary>Tests: Allow REST tests to run over https.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T09:14:25Z</created><updated>2015-08-12T09:48:21Z</updated><resolved>2015-08-12T09:48:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-12T09:27:59Z" id="130235321">LGTM
</comment><comment author="jpountz" created="2015-08-12T09:43:09Z" id="130238279">@jaymode I pushed a new commit
</comment><comment author="jaymode" created="2015-08-12T09:46:08Z" id="130240269">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchQueryIT#testIndicesQuerySkipParsing fails until we refactor nested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12822</link><project id="" key="" /><description>This is reminder issue to that SearchQueryIT#testIndicesQuerySkipParsing cannot be ran successfully until we have refactored nested queries. The test is marked as AwaitsFix for now.
</description><key id="100492699">12822</key><summary>SearchQueryIT#testIndicesQuerySkipParsing fails until we refactor nested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-08-12T08:27:28Z</created><updated>2015-09-14T22:01:53Z</updated><resolved>2015-09-14T22:01:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-14T22:01:53Z" id="140217559">This seems to work now that the HasChild/HasParent and Nested query are done. Re-enabled the test with 01ea426.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Improve error message of ClassCastExceptions </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12821</link><project id="" key="" /><description>Closes #12135 
# What is the problem we are trying to solve ?

When we are doing aggregations against a field name as shown in
https://github.com/HarishAtGitHub/elasticsearch-tester/blob/master/12135.py#L37-L46

``` javascript
search = {
           "aggs": {
             "NAME": {
               "terms": {
                 "field": "ip_str",
                 "size": 10
               }
             }
           }
         }
```

and when the field "ip_str" has values of different types in different indices
. say one is of type StringTerms type and other is of IP(LongTerms type) then
the aggregation fails as the types do not match(incompatible).
The failure throws a class cast exception as follows:

``` javascript
{
   "error": {
      "root_cause": [],
      "type": "reduce_search_phase_exception",
      "reason": "[reduce] ",
      "phase": "query",
      "grouped": true,
      "failed_shards": [],
      "caused_by": {
         "type": "class_cast_exception",
         "reason": "org.elasticsearch.search.aggregations.bucket.terms.LongTerms$Bucket cannot be cast to org.elasticsearch.search.aggregations.bucket.terms.StringTerms$Bucket"
      }
   },
   "status": 503
}
```

which is hard to understand . User cannot infer anything about the cause of the problem and what he should do from seeing the
class cast exception.
# What can be the possible solution ?

Make the exception more readable by showing him the root cause of the problem so that he can
understand which area actually caused the problem, so that he can take necessary steps further.
# Code Analysis

Debugging code shows that:
the query /{indices}/_search?search_type=count involves two phases
1) search phase

---

```
 searchService.sendExecuteQuery(...) [Ref: TransportSearchCountAction]

 what happens here ?
    the phase 1, which is the search phase goes without error.
    In this phase the shards for the given indexes are collected and the search is done on all asynchronously
    and finally collected in the variable "firstResults" and given to meger phase.

    [Flow: .... -&gt; TransportSearchTypeAction -&gt; method performFirstPhase]
```

2) merge phase

---

```
 searchPhaseController.merge(...firstResults...) [Ref: TransportSearchCountAction]

 what happens here ?
    the "firstresults" QuerySearchResults are now to be aggregated and combined.

    [Flow: SearchPhaseController.merge(...) -&gt; ..... -&gt; InternalTerms.doReduce(...)]
```

the phase 1, which is the search phase goes without error.
The problem comes in phase 2, which is merge phase.
Now the individual term buckets are available.
As per the test case , there are two indices cast and cast2, so by default 10 shards.
cast has ip_str of type StringTerms
cast2 has ip_str of type ip which is actually LongTerms

so here two types of Buckets exist. StringTerms_Bucket and LongTerms_Bucket.
Now the aggregation is to be put inside the BucketPriorityQueue(size 2: as out of 10, 2 has hits) finally.
(docs of PriorityQueue: https://lucene.apache.org/core/4_4_0/core/org/apache/lucene/util/PriorityQueue.html#insertWithOverflow(T))

Now first the LongTerms$Bucket is put inside.
then the StringTerms$Bucket is to be put in.
This is the area where exception is thrown. What happens is when adding the StringTerms$Bucket now it has to
goes through the code "lessThan(element, heap[1])"
which finally calls

&lt;code&gt;
     StringTerms$Bucket.compareTerms(other)  &lt;---------------- Area of exception  
&lt;/code&gt;

where when comparing one to other a type cast is done and it fails as StringTerms$Bucket and LongTerms$Bucket are
incompatible.
# Approach to solve:

The best way is to make user understand that the problem is when reducing/merging/aggregating the buckets which came as a result of
querying different shards, so that this will make them infer that the problem is because the values of the fields are of different types.
The message is also user friendly and much better than the indecipherable classcastexception.

The only place to infer correctly that the aggregation has failed is in the place where aggregations take place.
so

at InternalTerms.java -&gt; (BucketPriorityQueue)ordered.insertWithOverflow(b);

so here I can throw AggregationExecutionException saying it is because the buckets are of different
types.

&lt;b&gt;But when can I infer at this point that the failure is due to mismatch of types of buckets ???&lt;/b&gt;
it can be possible only if at this point it is informed that the problem which occurred deep inside
is due to buckets that were incomparable.
so from just a classCastException we cannot make such a pointed exact inference, because
as class cast exception can be due to a number of scenarios and at a number of places.

so unless we inform the exact problem to InternalTerms it will not be able to infer properly.
so infer the classCastException at the compareTerms function itself that it is a IncomparableTermBucktesTypeException.
This is the best place to infer classCastException as this the place which generated the exception.
Best inference of exceptions can be done only at the source/origin of the exception.

so IncomparableTermBucktesTypeException to InternalTerms-&gt; will make it infer and conclude on why
aggregation failed and give best information to user.
# File changes:

```
new file:   core/src/main/java/org/elasticsearch/IncomparableException.java
new file:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/IncomparableTermBucketsException.java
modified:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
modified:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java
modified:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
```

Details of changes

```
new file:   core/src/main/java/org/elasticsearch/IncomparableException.java
```

   This is the base class for all incomparable exception type. If the exceptions can be infered just with this
   base class he can very well use this without subclassing.

```
new file:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/IncomparableTermBucketsException.java
```

   This is sub class of IncomparableException.java . This is just to enhance readability as it is same
   as IncomparableException AND inference can be better. because from IncomparableException he can infer
   only that some quantities are incomparable. but if we want to say exactly Bucktes are incomparable this can be
   used. For eg. This resulted in better inference in InternalTerms and we concluded that the buckets differed in types.

```
modified:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
```

   throw AggregationExecutionException on receiveing IncomparableTermBucketsException

```
modified:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java
```

   infer class cast exception as IncomparableTermBucketsException

```
modified:   core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
```

   make the abstract method throw IncomparableTermBucketsException so that the implementors can follow the convention.
# Result after the change:

{

```
"error":{

    "root_cause":[

    ],

    "type":"reduce_search_phase_exception",

    "reason":"[reduce] ",

    "phase":"query",

    "grouped":true,

    "failed_shards":[

    ],

    "caused_by":{

        "type":"aggregation_execution_exception",

        "reason":"Merging/Reducing the aggregations failed because the buckets differ in type and they are incomparable",

        "caused_by":{

            "type":"incomparable_term_buckets_exception",

            "reason":"The Buckets cannot be compared as one is of type LongTerms and the other is of type StringTerms",

            "caused_by":{

                "type":"class_cast_exception",

                "reason":"org.elasticsearch.search.aggregations.bucket.terms.LongTerms$Bucket cannot be cast to org.elasticsearch.search.aggregations.bucket.terms.StringTerms$Bucket"

            }

        }

    }

},

"status":503
```

}
</description><key id="100490734">12821</key><summary> Improve error message of ClassCastExceptions </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">HarishAtGitHub</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-12T08:10:58Z</created><updated>2015-08-17T10:44:08Z</updated><resolved>2015-08-17T07:36:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HarishAtGitHub" created="2015-08-12T08:26:15Z" id="130214832">Doubts:
 1)   All tests passed except " Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.16:junit4 (tests) on project elasticsearch" . Why am I getting this ? To check if my code brought in this , I had a same fork without my changes and there too it failed ? what am I to infer from this ?

Note: this is just the basic patch . please comment and give feedback and I can make changes.
I should make changes like making other terms also make this sort of check in compare and throw exception . Is this needed now or what should I do ?

If this approach is ok I can try to solve this similar issue too https://github.com/elastic/elasticsearch/issues/12675 .

Thanks in advance.
</comment><comment author="jpountz" created="2015-08-12T12:53:50Z" id="130290856">Thanks @HarishAtGitHub. This approach works, but I would rather proactively check that the Terms instances are compatibles instead of catching class-cast exceptions. Maybe we could just loop over the aggregations instances in `InternalTerms.doReduce` and raise an error if we find two different impls of InternalTerms (with the exception of UnmappedTerms, which can be merged with any other Terms instance)?

&gt; All tests passed except " Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.16:junit4 (tests) on project elasticsearch" . Why am I getting this ? To check if my code brought in this , I had a same fork without my changes and there too it failed ? what am I to infer from this ?

I suspect there is the actual error message right before or after this message, can you create a gist of the whole build log?
</comment><comment author="HarishAtGitHub" created="2015-08-12T13:37:14Z" id="130305630">Hi @jpountz  ,
      First of all thanks for the review (I was worried if people will be bored reading my huge commit message. I personally believe that the entire thought process should be in commit message. Thats y this hugeness.sorry if it bothered. ).Thank you very much .  It was great help.

Ok now coming back. ...I thought about your way of checking terms prior ... But I was not confident because of these questions that arose in my mind(may be because I am a beginner for elasticsearch concepts)
1) what if there are many shards on which search occurred . For eg . say I am searching on 10 indices. and people used the default 10 shards per index. in that loop we might end up with 10 \* 10 = 100 entries . 
2) what if the 100 entries above had entries with all possible terms .. today may be there are some 5 terms(different) . may tomorrow it might go up to 10 or more (I don't know ..but)...

What I worried was that there are more possibilities of checking the compatibility between terms.
I was worried that this proactive check approach might end up doing more work .
But if we leave it for the normal loop itself to check , it is only one loop or one breakage.
But in proactive check approach will we not end up with (loopcheck + existinglogic) or (loopcheck &amp; breakage) .
The tradeoff made me lean towards the present approach .

And also that these kinds of incompatibility breakage seems to be in a number of places. For eg . I saw this similar one https://github.com/elastic/elasticsearch/issues/12675.

If we follow the present approach all these places can be fixed without any effort(or looping proactively ). I thought that it might be performance efficient as well as we don't bring in any additional looping to check incompatibility and the normal business logic takes care of checking incompatibility.

Please share your thoughts .
</comment><comment author="jpountz" created="2015-08-12T13:56:37Z" id="130314215">&gt; 1) what if there are many shards on which search occurred . For eg . say I am searching on 10 indices. and people used the default 10 shards per index. in that loop we might end up with 10 \* 10 = 100 entries .
&gt; 2) what if the 100 entries above had entries with all possible terms .. today may be there are some 5 terms(different) . may tomorrow it might go up to 10 or more (I don't know ..but)...

1 is right, but I don't see it as an issue, we need to iterate over all aggs anyway if we want to reduce results. Regarding 2, I was not suggesting to iterate over the actual terms, but the aggregation object (which should be a sub-class or the `Terms` class).

I don't think performance would be an issue at all.
</comment><comment author="HarishAtGitHub" created="2015-08-12T17:01:21Z" id="130370413">Ok to get clarity to know if what I am thinking is correct..... 
we already have a loop like

``` java
for (InternalAggregation aggregation : aggregations) {  ----&gt; loop 1
  ..... N aggregations takes N loop
}
for (Collection&lt;Bucket&gt; l : buckets.asMap().values()) { ----&gt; loop 2
  ...... M times ...Far less than N as it includes only matches' buckets (in our case with 10 shards only 2 qualifies)
}
```

&gt; 1 is right, but I don't see it as an issue, we need to iterate over all aggs anyway
&gt; Maybe we could just loop over the aggregations instances in InternalTerms.doReduce and raise an error if we find two different impls of InternalTerms

So you are asking me to do the buckets mismatch check in loop-1 itself. Right ?

But to do this I will have to iterate the bucket's again and check for each if there is mismatch.My doubt is why should we do it there when we are already doing it in the M-count loop(loop2).(but I agree that it is an indirect approach)

Another doubt is if there is no mismatch of buckets there will be two iterations of buckets .
one inside loop-1 another in loop-2 .
so net would be something like loop1 + [M_(M+1)/2]_inner bucket loops  + loop2(which already again does this by means of comparison inside BucketPriorityQueue)

But if we do it in the present way it will always be just
loop1 + loop2(which already again does this by means of comparison inside BucketPriorityQueue.)

I can surely understand your point that if we do it your way we can directly interepret the mismatch here itself inside InternalTerms.
But what I cannot understand is just for this exception interpretation(which is done with the present approach in the patch already without extra computation) why should we bring in so many additional loops ?
</comment><comment author="HarishAtGitHub" created="2015-08-12T19:55:32Z" id="130426455">&gt; I suspect there is the actual error message right before or after this message, can you create a gist of the whole build log?

Yes. Now I fixed it. It was a org.elasticsearch.ExceptionSerializationTests.testExceptionRegistration test that was failing as my exception was not registered . Now fixed it and pushed it .Thanks.
Unfortunately I ran the mvn clean install at night 1 AM nearly to check any failure, hoping it will get over soon...but  ahh it took lot of time nearly 40 minutes.
</comment><comment author="rjernst" created="2015-08-12T22:00:33Z" id="130460530">Do we really need a new exception? I think `IllegalArgumentException` or `IllegalStateException` would suffice?
</comment><comment author="HarishAtGitHub" created="2015-08-13T05:16:52Z" id="130537891">&gt; Do we really need a new exception? I think IllegalArgumentException or IllegalStateException would suffice?

@rjernst, I understand your concerns.... But the following was my thought process ....

I want to make normal user(who does not know much about coding and exceptions) to even interpret from the exception. I am just targetting users who know just about elasticsearch, that is elasticsearch users . May be users who have just read his [elasticsearch documentation link](https://www.elastic.co/guide/en/elasticsearch/reference/1.6/search-aggregations-bucket-terms-aggregation.html) . IllegalArgumentException would be good if it is just going to be in the logs inside the box which is not accessible to common user.
I want to have something that is not too broad and also conveys the meaning to normal elasticsearch user. But at the same time as you say we cannot create exceptions(for every usecase) and this would flood the code base with exception classes.
I want to stand somewhere in the middle satisfying both.
Thats y I chose to introduce new exception.

It is like this. We already have [IndexOutOfBoundsException](http://docs.oracle.com/javase/7/docs/api/java/lang/IndexOutOfBoundsException.html) . But despite having this, we have [ArrayIndexOutOfBoundsException](http://docs.oracle.com/javase/7/docs/api/java/lang/ArrayIndexOutOfBoundsException.html) and [StringIndexOutOfBoundsException](http://docs.oracle.com/javase/7/docs/api/java/lang/StringIndexOutOfBoundsException.html) . This is there just to convey the exact sense. 

I am not saying that we should follow what java community has done . But just that this approach they have taken made sense to me.

And that this exception is to be shown to the user on api failures so I felt it is not a bad choice to be precise and exact and convey the sense and hence introduce a new one as none of the exceptions at purpose satisfied my need.
Just by seeing the returned error json I should be able to approach the problem.

&gt; Harish says : Just by seeing the returned error json I should be able to approach the problem.

but again if you feel that  the statement I made above is not the truth achieved with my fix approach, please let me know . I can just change the fix to make the error more readable and easily decipherable . I am fine with that . I can look for another way. My aim is to make user easily parse error message and interpret it the best possible right way.
</comment><comment author="rjernst" created="2015-08-13T06:03:57Z" id="130548829">I personally think the actual exception type is less meaningful (hence just tend to use builtin exceptions; we use `IllegalArgumentException` all over elasticsearch). The message is what is important. The user will quickly look at "reason" in structured exceptions. 
</comment><comment author="HarishAtGitHub" created="2015-08-13T06:23:34Z" id="130551332">@rjernst , Ya.. Fine agreed ... you are right ... Your argument makes a lot of sense ... message within exception itself is enough to convey the meaning to common user.

ok I will do it like this now ?

``` java
StringTerms$Bucket
try {
     compareTerm(.... typecast ...)
} catch (TypecastException e) {
    IllegalAgrgumentException("the buckets type could not be compared as they were incompatible", e)
}
```

``` java
InternalTerms
try{
  ordered.insertWithOverflow(b);
catch(RuntimeException e) {
  throw AggregationExecutionException("merging/reducing the buckets failed", e);
}
```

does the above makes sense ..
Now as you said we just used existing exceptions but still conveyed the meaning with the help of messages inside it.
so what ever runtimeexception is thrown within insertWithOverflow will result in throwing AggregationExecutionException as aggregation failed .
and this AggregationExecutionException will have the cause exception within it for more info ..
Ya this makes sense for me.
you are fine with this @rjernst ?

NOTE: I will make the messages better . but the above is just to convey the idea..
</comment><comment author="jpountz" created="2015-08-13T12:49:56Z" id="130657980">&gt; for (InternalAggregation aggregation : aggregations) {  ----&gt; loop 1
&gt;   ..... N aggregations takes N loop
&gt; }
&gt; for (Collection&lt;Bucket&gt; l : buckets.asMap().values()) { ----&gt; loop 2
&gt;   ...... M times ...Far less than N as it includes only matches' buckets (in our case with 10 shards only 2 qualifies)
&gt; }

My point was that you don't need loop2, since an aggregation has all buckets of the same type, you could just do loop1.
</comment><comment author="HarishAtGitHub" created="2015-08-13T13:51:03Z" id="130679685">ok @jpountz , I will give it a try now ...Thanks ...
</comment><comment author="HarishAtGitHub" created="2015-08-16T11:10:23Z" id="131525346">Hi @jpountz , did it as per review comments . 
Please give your review comments .
this is the final diff .. https://github.com/elastic/elasticsearch/pull/12821/files
(one commit I made to revert the line change I made in my first commit .)
(0 Test failures ..)
</comment><comment author="HarishAtGitHub" created="2015-08-16T11:18:35Z" id="131527394">@jpountz ,
new error message is like this 

&lt;b&gt; Merging/Reducing the aggregations failed when computing the aggregation [ Name: NAME, Type: terms ] because: the field you gave in the aggregation query existed as two different types in two different indices &lt;/b&gt;

``` javascript
{  
    "error":{  
        "root_cause":[  

        ],
        "type":"reduce_search_phase_exception",
        "reason":"[reduce] ",
        "phase":"query",
        "grouped":true,
        "failed_shards":[  

        ],
        "caused_by":{  
            "type":"aggregation_execution_exception",
            "reason":"Merging/Reducing the aggregations failed when computing the aggregation [ Name: NAME, Type: terms ] because: the field you gave in the aggregation query existed as two different types in two different indices"
        }
    },
    "status":503
}
```
</comment><comment author="HarishAtGitHub" created="2015-08-17T02:45:20Z" id="131662694">@jpountz also a doubt, will there be cases when the aggregation and type will not be mandatory ?
I just tried the query and on leaving out either of these it said ""type":"search_parse_exception","reason":"Expected [START_OBJECT] under [field], but got a [VALUE_STRING] in [NAME]","line":1,"col":20}" .
so can I assume it will work ? . or should I have a null pointer check on the params (for safety) I am introducing in aggregation_execution_exception ?
(but may be we can assume it will not be null as: only because it is terms agg's it came inside internalterms, and the previous step is aggregation separation by name so that name can also not be null .. can  I take this freedom of conclusion ?) seems like [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html#_structuring_aggregations) confirms we need not have null check. what do u say @jpountz ?
</comment><comment author="jpountz" created="2015-08-17T07:37:08Z" id="131707631">Thank you @HarishAtGitHub 
</comment><comment author="HarishAtGitHub" created="2015-08-17T08:25:03Z" id="131728335">May I know what was wrong in the patch ? . just for my learning @jpountz ?
</comment><comment author="jpountz" created="2015-08-17T08:26:15Z" id="131728749">Nothing was wrong, I just squashed the commits.
</comment><comment author="HarishAtGitHub" created="2015-08-17T08:27:28Z" id="131728982">the comments said "closed with UNMERGED commits" thats y I was worried ...
</comment><comment author="HarishAtGitHub" created="2015-08-17T08:33:51Z" id="131730046">Thank you very much for the guidance ... your approach was easier and direct and exactly solved the issue.... learning from u ...Thanks...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move note about update API not supporting external versioning up to introduction to make it more prominent.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12820</link><project id="" key="" /><description>This documentation patch moves the note about the update API being incompatible with external versioning from the very last paragraph up to the introductory text at the start, to make it absolutely clear from the outset that the update API cannot be used when external versioning is in play.
</description><key id="100475350">12820</key><summary>Move note about update API not supporting external versioning up to introduction to make it more prominent.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>docs</label></labels><created>2015-08-12T06:17:51Z</created><updated>2016-03-10T11:34:15Z</updated><resolved>2016-03-10T11:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-12T10:05:43Z" id="130246695">Hi @joshuar 

I think the first para is the wrong place to put it.  Remember that somebody reading these docs is probably starting from scratch and so listing all the exceptions in the opening para will just be confusing.  Instead, I'd add an IMPORTANT block, something like:

```
[IMPORTANT]
.External versioning cannot be used with the `update` API
==========================
The reason why.....
==========================
```
</comment><comment author="clintongormley" created="2015-08-12T10:06:07Z" id="130246903">... and I'd add it under the version/version_type docs
</comment><comment author="joshuar" created="2015-08-12T10:14:40Z" id="130248901">Good point. I did struggle with considering rewording that bit. I can
rework this PR as per your suggestion instead of you if you want?
On 12 Aug 2015 8:07 pm, "Clinton Gormley" notifications@github.com wrote:

&gt; ... and I'd add it under the version/version_type docs
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12820#issuecomment-130246903
&gt; .
</comment><comment author="clintongormley" created="2015-08-12T10:16:33Z" id="130249178">++
</comment><comment author="joshuar" created="2015-08-12T10:18:40Z" id="130249484">Expect update tomorrow &#128513;
On 12 Aug 2015 8:17 pm, "Clinton Gormley" notifications@github.com wrote:

&gt; ++
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12820#issuecomment-130249178
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Are these ES bugs or just wrong implementations by me?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12819</link><project id="" key="" /><description>I have set the maximum shingle size as 3 for my index. When I am done with indexing I find that the shingles : 
1. `yield`
2. `_ yield`
3. `_ _ yield`

have the same start and ending offsets. So, if I would need the original sub-string I will always get `yield`!! Why is that and can it be corrected?

Another thing that I noticed is that the offsets of the shingles like:
1. `_ yield _`
2. `yield _`
3. `yield _ _`
have length such that the string has an extra space that the end. e.g. for `_ yield _` if there is a string `"not yield until"` then the offsets, is used on the original text, return `"yield until "` (notice trailing space). I actually would want `"not yield until"`.

Finally, is there a way we can avoid stop words while creating shingles? In my application the stop words don't play any role.

Following is the settings that I am using for the index:

```
{
   "test_index": {
      "settings": {
         "index": {
            "refresh_interval": "60s",
            "number_of_shards": "5",
            "store": {
               "type": "default"
            },
            "creation_date": "1439324341816",
            "analysis": {
               "filter": {
                  "snowball_stop_words_en_EN": {
                     "type": "stop",
                     "stopwords_path": "snowball.stop"
                  },
                  "smart_stop_words_en_EN": {
                     "type": "stop",
                     "stopwords_path": "smart.stop"
                  },
                  "porter_stemmer_en_EN": {
                     "name": "porter",
                     "type": "stemmer"
                  },
                  "word_delimiter_en_EN": {
                     "type": "word_delimiter",
                     "stem_english_possessive": "true"
                  },
                  "default_stop_name_en_EN": {
                     "name": "_english_",
                     "type": "stop"
                  },
                  "preserve_original_en_EN": {
                     "type": "word_delimiter",
                     "preserve_original": "true"
                  },
                  "apos_replace_en_EN": {
                     "pattern": ".*\\'$",
                     "type": "pattern_replace",
                     "replacement": ""
                  },
                  "shingle_filter_en_EN": {
                     "max_shingle_size": "3",
                     "min_shingle_size": "2",
                     "type": "shingle",
                     "output_unigrams": "true"
                  }
               },
               "analyzer": {
                  "test_analyzer": {
                     "filter": [
                        "lowercase",
                        "smart_stop_words_en_EN",
                        "preserve_original_en_EN",
                        "porter_stemmer_en_EN",
                        "asciifolding",
                        "apos_replace_en_EN",
                        "shingle_filter_en_EN"
                     ],
                     "type": "custom",
                     "tokenizer": "standard"
                  }
               }
            },
            "number_of_replicas": "1",
            "version": {
               "created": "1060099"
            },
            "uuid": "urrFTCEoThyuPjjBYCBgYQ"
         }
      }
   }
}
```

I use the `test_analyzer` on the documents while indexing.

I am using ElasticSearch 1.6.0.

As an example I have even attached [link](https://www.dropbox.com/s/dipb0quha3wke1f/fft.txt?dl=0) to a text file that I was using.

Thanks.
Any suggestions are welcome.
</description><key id="100471485">12819</key><summary>Are these ES bugs or just wrong implementations by me?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apanimesh061</reporter><labels /><created>2015-08-12T05:39:40Z</created><updated>2016-01-28T12:21:03Z</updated><resolved>2016-01-28T12:21:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-12T08:05:47Z" id="130211309">This question has already been posted on the forums here: https://discuss.elastic.co/t/are-these-es-bugs-or-just-wrong-implementations-by-me/27228

Please keep the discussion in one place as it will avoid the conversation getting split between two threads which makes it hard to follow both for current participants and for people looking for the same information in the future.
</comment><comment author="clintongormley" created="2015-08-12T09:58:31Z" id="130243525">@colings86 actually i think this is worth discussing here.  there may be things we can improve.  Reopening
</comment><comment author="clintongormley" created="2015-08-12T10:02:39Z" id="130244552">@apanimesh061 As a start, try getting rid of the word delimiter filter (first time i've seen it used twice???) but it is known to mess up the analysis chain.  Don't try to put every analysis variation into a single field - you just end up with garbage.  Instead break it down into a few fields used for different purposes, eg one with standard analyzer, one with stopwords and stemmer, etc

I'd be interested in seeing a full JSON replication (without word delimiter), ie something that I can run easily without having to build my own example :)
</comment><comment author="apanimesh061" created="2015-08-12T12:23:36Z" id="130281448">@clintongormley I am pretty sure issue [#4307](https://github.com/elastic/elasticsearch/issues/4307#issuecomment-130239252) is related to this.
I have updated the settings as asked:

```
{
   "test_index": {
      "settings": {
         "index": {
            "refresh_interval": "60s",
            "number_of_shards": "5",
            "store": {
               "type": "default"
            },
            "creation_date": "1439381862716",
            "analysis": {
               "filter": {
                  "default_stop_name_en_EN": {
                     "name": "_english_",
                     "type": "stop"
                  },
                  "snowball_stop_words_en_EN": {
                     "type": "stop",
                     "stopwords_path": "snowball.stop"
                  },
                  "smart_stop_words_en_EN": {
                     "type": "stop",
                     "stopwords_path": "smart.stop"
                  },
                  "apos_replace_en_EN": {
                     "pattern": ".*\\'$",
                     "type": "pattern_replace",
                     "replacement": ""
                  },
                  "shingle_filter_en_EN": {
                     "max_shingle_size": "3",
                     "min_shingle_size": "2",
                     "type": "shingle",
                     "output_unigrams": "true"
                  },
                  "porter_stemmer_en_EN": {
                     "name": "porter",
                     "type": "stemmer"
                  }
               },
               "analyzer": {
                  "test_analyzer": {
                     "type": "custom",
                     "filter": [
                        "lowercase",
                        "smart_stop_words_en_EN",
                        "porter_stemmer_en_EN",
                        "asciifolding",
                        "apos_replace_en_EN",
                        "shingle_filter_en_EN"
                     ],
                     "tokenizer": "standard"
                  }
               }
            },
            "number_of_replicas": "1",
            "version": {
               "created": "1060099"
            },
            "uuid": "uvmOqkayTOKy8H3FpFH6zA"
         }
      }
   }
}
```
</comment><comment author="clintongormley" created="2015-08-13T09:29:48Z" id="130590652">@apanimesh061 when I said "full JSON replication" I meant: a series of curl commands that i can copy and paste, along with "this request produces this output, while i think it should be this instead".  otherwise I have to spend time trying to guess which parts you think are wrong.
</comment><comment author="clintongormley" created="2016-01-28T12:21:03Z" id="176153019">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flatten Allocation modules and add back ability to plugin ShardsAllocators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12818</link><project id="" key="" /><description>There were two submodules of AllocationModule. This combines them into a
single module, adds a base test case for module testing, and adds back
the ability for plugins to provide custom ShardsAllocators.

closes #12781
</description><key id="100456948">12818</key><summary>Flatten Allocation modules and add back ability to plugin ShardsAllocators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-12T03:26:44Z</created><updated>2015-08-13T14:01:08Z</updated><resolved>2015-08-12T04:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-12T04:55:36Z" id="130161734">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Normalized/Shared Mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12817</link><project id="" key="" /><description>### Problem

Currently, for anyone that uses time-based indices, they will run into repeating their mapping--ideally using a template--for each split of the index.

This can be wasteful and it's hard to manage from ORM tools and tools like Kibana where a mapping of 300 days with 100 fields becomes 30,000 fields. To display content generated from these kind of indices requires somehow looping across them and combining them.
### Proposed Solution

I think there are two different solutions to this problem where the first is easier than the second:
1. Create a Normalized Mapping API that returns a single, combined index mapping that represents the merged fields from every index. If there are any additions, then they are appended. If there are any differences/conflicts, then it should simply fail. This simply avoids the network hop for any service that is already doing this now.
2. Enable indices to use a new type of shared mapping. It could behave similarly to the snapshot/restore API where the mapping is pointed too by the index. If any change is ever made to the shared mapping, then it creates a new version of it and only new indices point to it. Naturally the shards will have to store the mapping, but any operation against them could use the single, shared mapping. This should also help to reduce the cluster state size by avoiding even adding a new mapping with any new index that shares the same mapping.
</description><key id="100435181">12817</key><summary>Normalized/Shared Mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-08-11T23:39:06Z</created><updated>2016-03-01T10:39:43Z</updated><resolved>2016-03-01T10:39:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2016-01-14T15:23:46Z" id="171671824">Just popping in to say this would be crazy useful for us. We're planning to deprecate our storage of a pre-normalized copy of the mapping as it was simply a work around for not having structured exceptions in Elasticsearch and caused a ton of big problems for users that update their field lists.

Having this done in elasticsearch would be a huge performance and reliability boost!
</comment><comment author="rjernst" created="2016-01-18T20:36:41Z" id="172647282">Where would the performance, or reliability boost come in? The cluster state is already compressed, so similarities in mappings across indexes shouldn't take up extra space there. As for sharing in memory, I think that might be a very large change (we don't have really anything shared _across_ indexes right now, afaik), and I'm not sure the memory savings really add up to anything significant, compared to the cost of having that many indexes (in which case, if that is a bottleneck, the user should probably create new indexes less often). Instead, something like @s1monw has suggested before, having the ability to collapse multiple indices into one for archiving, would be much better on performance.
</comment><comment author="clintongormley" created="2016-01-19T12:49:46Z" id="172843680">Another thought that came up in FixItFriday: HTTP compression should greatly reduce the amount of data being sent over the wire (given that there is so much repetition in the mappings).  Unfortunately, HTTP compression is disabled by default (see https://github.com/elastic/elasticsearch/issues/1482) . @kimchy can you remember the details?

I tested this out with 10 indices containing the same mapping of twenty fields, and it reduced a GET _mapping from 5589 bytes to 209 bytes...  Sounds like this could be worth doing. 
</comment><comment author="clintongormley" created="2016-03-01T10:39:43Z" id="190656705">Closing in favour of https://github.com/elastic/elasticsearch/issues/15728
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java SDK: LongTerms.Bucket#getKeyAsText should return the string value for date aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12816</link><project id="" key="" /><description>`TL;DR;` For example, when querying for the following aggregation on field `createDate` (which is a Date field): 

```
{
  "size": 0,
  "query": { "match_all": {} },
  "aggregations": {
    "DATE_TERMS": {
      "terms": {
        "field": "createDate",
        "size": 10
      }
    }
  }
}
```

The response in JSON is:

```
"DATE_TERMS": {
    "doc_count_error_upper_bound": 0,
    "sum_other_doc_count": 0,
    "buckets": [
      {
        "key": 592444800000,
        "key_as_string": "10/10/88",
        "doc_count": 12
      },
      {
        "key": 646876800000,
        "key_as_string": "07/02/90",
        "doc_count": 7
      },
      {
        "key": 744940800000,
        "key_as_string": "08/10/93",
        "doc_count": 3
      }
    ]
  }
```

When I loop through the buckets in Java code, 

```
Terms terms = response.getAggregations().get("DATE_TERMS");
terms.getBuckets().stream.forEach( bucket -&gt; {
  String keyAsString = bucket.getKeyAsText().string(); 
  System.out.println(keyAsString);
})
```

The output is

```
592444800000
646876800000
744940800000
```

I'm expecting `bucket.getKeyAsText().string()` would give me the actual date in string format (e.g. `10/10/88`, `07/02/90`, etc..), but currently it gives me the long value of the date (e.g. `592444800000`, `646876800000`, ...). This behavior in the SDK is inconsistent with the JSON response, where there is a field called `key_as_string`.
# 

The workaround (as shown in the following code) would be first to check if `terms` is a `LongTerms` type, and if it is, during the loop, use `ValueFormatter.DateTime.DEFAULT.format()` to convert the long value to the actually date string, which is tedious. Moreover the default formatter doesn't give me the same string as in the JSON response either.

```
Terms terms = response.getAggregations().get("DATE_TERMS");
terms.getBuckets().stream.forEach( bucket -&gt; {
  String keyAsString
  if (terms instanceof LongTerms)
    keyAsString = ValueFormatter.DateTime.DEFAULT.format(bucket.getKeyAsNumber().longValue());
  else
    keyAsString = bucket.getKeyAsText().string(); 
  System.out.println(keyAsString);
})
```

The output is

```
1988-10-10T00:00:00.000Z
1990-07-02T00:00:00.000Z
1993-08-10T00:00:00.000Z
```

which is fine, but I would expect the same output as in the JSON response:

```
10/10/88
07/02/90
08/10/93
```

Moreover, it is not generally applicable. If the aggregation term is a number type (long, integer), the conversion to date will be wrong.

The SDK version I'm using is `1.7.1`.

I'm not sure whether I describe the problem clearly, but let me know if you need more elaboration.

Thanks!
</description><key id="100420616">12816</key><summary>Java SDK: LongTerms.Bucket#getKeyAsText should return the string value for date aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cooniur</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-08-11T21:55:18Z</created><updated>2016-01-28T12:20:18Z</updated><resolved>2016-01-28T12:20:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cooniur" created="2015-08-25T18:27:56Z" id="134694126">@colings86 Is anyone interested in this topic?
</comment><comment author="clintongormley" created="2016-01-28T12:20:18Z" id="176152505">Closing in favour of #15146
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add weight to AllocationDecider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12815</link><project id="" key="" /><description>AllocationDeciders (plural) currently take all the AllocationDecider instances in a set. While the order does not matter for a "yes" decision (because it must be unanimous), a "no" or "throttle" decision could be optimized if cheaper deciders were allowed to run first.

Adding a weight, with a default of 1.0 (easy since AllocationDecider is an abstract class), would allow expensive deciders to increase their weight and run later.
</description><key id="100416301">12815</key><summary>Add weight to AllocationDecider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Allocation</label><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-11T21:27:00Z</created><updated>2016-10-04T12:36:44Z</updated><resolved>2016-10-04T12:36:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-11T21:28:19Z" id="130083525">@rjernst we do optimize the order of the AllocationDeciders to have the cheapest ones run first and short-circuit on "NO" answers, are you saying we should have the allocation deciders decide the weight themselves?
</comment><comment author="s1monw" created="2015-08-11T21:29:18Z" id="130083733">@dakrone I think that is not true - it's a set so it order is undefined?
</comment><comment author="rjernst" created="2015-08-11T21:33:37Z" id="130084535">Yeah, the injected AllocationDeciders ctor takes a set of AllocationDecider instances, so no order that I see. @dakrone Yes, I think the deciders should give their own weight. They are should be self aware of their expensiveness.
</comment><comment author="dakrone" created="2015-08-11T21:33:55Z" id="130084597">Ahh okay, I thought I remembered a commit changing that. Frustrating that we can't inject an ordered list by default with Guice. +1 to weight then.
</comment><comment author="dakrone" created="2016-09-27T15:21:01Z" id="249897713">Now that allocation deciders are de-guiced, I think we can do this with regular ordering instead of adding a weight, and it should be less complex to implement
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hierarchical Tribe Nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12814</link><project id="" key="" /><description>We are hoping hierarchical tribe nodes to solve the following issues:
1. Tribe start up is slow, especially for the "top level" tribe, which talk with all 30 clusters cross a number of data centers. It would take up to 10 minutes before a tribe node is fully up and ready to answer queries.  The idea is each data center would form its own tribe in order to buffer some of the slowness.  
2.  Query slowness. Queries are slow when talking to the "top level" tribe nodes but may be faster to each data center based tribe.
3.  When one or more downstream clusters have issues, it affects the whole tribe.  With hierarchical tribe nodes, it will affect some of the tribes but not others.
</description><key id="100416006">12814</key><summary>Hierarchical Tribe Nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sherry-ger</reporter><labels><label>:Tribe Node</label><label>discuss</label><label>enhancement</label></labels><created>2015-08-11T21:25:25Z</created><updated>2015-09-28T10:04:16Z</updated><resolved>2015-09-27T22:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TinLe" created="2015-08-11T22:19:24Z" id="130094549">Adding more information.   The "clusters" consist of independent ELK cluster per data center.  Currently we have three main data centers (that's 30 x 3 = 90 ELK clusters).

A cluster is an ELK cluster that consumed log data for a specific set of services/applications.  These services/application run in all data centers.   Per security policy, log data generated in each data center must stay within that data center.   For each cluster, there is tribe setup to aggregate data across all data centers for that particular cluster.

Separate from each cluster having their own tribe, there is a top level set of tribe nodes (1 per data center) that talk to all 3 DCs and all 30 clusters (x the 3 DCs).

The model is distributed, loosely coupled.   Each cluster is independently managed/own by the team that own the service/application.   Only the top level tribe cluster that talk to all clusters is own by me.

I would like to avoid Single Point of Failure as much as possible.   Downstream clusters having issue should not affect the performance, availability of the top level Tribe cluster.   The Tribe node should be robust enough to deal with downstream clusters dropping out, rejoining, having issues.

Another issue that we've seen and reported here: https://github.com/elastic/elasticsearch/issues/12804
</comment><comment author="TinLe" created="2015-08-27T22:15:46Z" id="135570880">This is becoming very urgent.   Our tribe nodes are crashing on an hourly basis now due to GC.   Essentially they become unresponsive, nothing in log until we bounce them, the while dying, there will be a few lines in log that say they are hitting GC.

We are already using maxheap of 31GB on 64GB dedicated tribe nodes.   I have also tested on 256GB nodes, with various maxheap from 64GB, 96GB, 100GB, 120GB, 128GB.   Best time the node stay alive/responsive was ~5hrs.   I've also tested with CMS vs G1GC.   None of them make much of a difference.
</comment><comment author="clintongormley" created="2015-08-28T12:17:46Z" id="135757221">Honestly, I don't see a clear path to making this work.  The tribe nodes need to have the cluster state from all clusters in order to be able to do their job.  The tribe joins each cluster as a client node. Query slowness is basically down to network latency and the slowest node across all clusters.

Sorry, but I can't give you a magic solution here.
</comment><comment author="TinLe" created="2015-08-28T16:02:15Z" id="135814941">I agree that it is a hard problem.   The Galene team solved similar problem,and it took some effort.

https://engineering.linkedin.com/search/did-you-mean-galene

Federation/aggregation of tribe is a hard problem and require internal re-architecting.
</comment><comment author="jpountz" created="2015-09-27T22:13:21Z" id="143598217">Maybe there are ways we can improve the tribe nodes to improve the situation, but I don't think hierarchical tribe nodes are a solution. It's also not as easy as it seems since some parts of the code are assuming that the reduce phase is happening only once (eg. for pipeline aggs or the ways terms aggs can over-request shards).
</comment><comment author="clintongormley" created="2015-09-28T10:04:16Z" id="143698950">https://github.com/elastic/elasticsearch/pull/13627 will also help with this extreme use of tribe nodes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes #11571 - update "Cluster Stats" documentation with valid example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12813</link><project id="" key="" /><description>It should fix the docs mentioned in #11571 

I have doubts about `id_cache` since it was removed in ece18f1, but it is still valid for current 1.7 docs.

What do you think @clintongormley @spapin?
</description><key id="100414361">12813</key><summary>Fixes #11571 - update "Cluster Stats" documentation with valid example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kubum</reporter><labels><label>docs</label></labels><created>2015-08-11T21:16:32Z</created><updated>2015-09-22T14:13:18Z</updated><resolved>2015-09-22T14:13:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spapin" created="2015-08-11T21:42:03Z" id="130086261">Great! thanks for the update. I believe cache_id is better removed as the committed doc will be up to date with the code in the git repo, letting @clintongormley confirm that.
</comment><comment author="clintongormley" created="2015-08-12T09:40:24Z" id="130237833">Hi @kubum 

Thanks for the PR.  This should go only into 1.7 as the stats have changed quite a lot in master.  Also, please could you sign the CLA so that I can merge this in? http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="kubum" created="2015-08-12T20:01:11Z" id="130427611">Hi @clintongormley 

Just signed it. I don't think github allows me to change the target branch. Do you want me recreate PR to 1.7 or something else?
</comment><comment author="clintongormley" created="2015-08-13T10:10:21Z" id="130602185">Thanks @kubum - merged
</comment><comment author="nik9000" created="2015-09-22T14:13:18Z" id="142300691">Since @clintongormley's said he's merged this I'll close it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Synced flush missing from Java client API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12812</link><project id="" key="" /><description /><key id="100410398">12812</key><summary>Synced flush missing from Java client API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">shikhar</reporter><labels><label>:Index APIs</label><label>v2.2.0</label></labels><created>2015-08-11T21:00:04Z</created><updated>2015-12-16T16:18:45Z</updated><resolved>2015-12-16T16:18:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-11T21:12:45Z" id="130079338">@shikhar we did this on purpose since it's really a last resort API that should only be run by an admin manually ie. via rest. I think we should keep it like this, what's your usecase?
</comment><comment author="shikhar" created="2015-08-12T02:07:11Z" id="130136055">@s1monw some of our indexing activity is periodical rather than NRT (e.g. every minute or every hour). We tend to bounce our clusters quite often so it'd be great to be synced as much as possible, and for the indexers that are running every minute the 5 min auto-sync is too long. For now I am thinking of adding a curl to perform the synced flush in the indexer script (they're cronned short-lived processes), but being able to use the Java client API would have met our needs better.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warn users when their bulk requests are too large</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12811</link><project id="" key="" /><description>Its not super clear how many documents is the right number to send in a bulk request. I imagine its because it depends on lots of things - mapping, document size, lots of stuff. I imagine sometimes Elasticsearch can _tell_ when the user's sent too many documents. In those times we should log a warning or send a warning back to the user or something.

I'm not super clear on how this would work because I'm not super clear on what the problems are with sending too many documents in one bulk request.
</description><key id="100405891">12811</key><summary>Warn users when their bulk requests are too large</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2015-08-11T20:47:11Z</created><updated>2016-01-26T19:39:12Z</updated><resolved>2016-01-26T19:39:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T19:39:12Z" id="175194074">Closing in favour of #16011
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Brings Lucene query assertions back to QB test.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12810</link><project id="" key="" /><description>This brings the Lucene query assertions back to the querybuilder test that were
removed in the ancient past when we tested Lucene queries through their
inherent equals method. As we no longer do that it makes sense to do at least
coarse sanity checking on the generated Lucene query. More such checks are
being added as part of this commit.

Note: This goes against the feature/query-refactoring feature branch.

Relates to #10217
</description><key id="100399297">12810</key><summary>Brings Lucene query assertions back to QB test.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label><label>test</label></labels><created>2015-08-11T20:18:02Z</created><updated>2015-08-20T08:21:56Z</updated><resolved>2015-08-20T08:21:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-13T09:51:06Z" id="130595262">I am conflicted on whether the `doAssertLuceneQuery` goes too deep now, and whether this is maintainable on the long run. That said let's give it a try and see. LGTM
</comment><comment author="MaineC" created="2015-08-18T18:08:50Z" id="132302085">@cbuescher Thanks for your comment - I changed the if in question slightly, feel free to take another look.
</comment><comment author="cbuescher" created="2015-08-18T19:03:45Z" id="132319897">@MaineC thanks, now I see the different cases much clearer. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `cloud.account` and `cloud.key` settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12809</link><project id="" key="" /><description>_From @dadoonet on February 27, 2015 15:21_

`cloud.account` and `cloud.key` settings are not documented as we use `cloud.aws.access_key` and `cloud.aws.secret_key` settings instead.

We can deprecated those settings in `es-1.4` branch and remove them in `master`.

_Copied from original issue: elastic/elasticsearch-cloud-aws#183_
</description><key id="100369470">12809</key><summary>Remove `cloud.account` and `cloud.key` settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Settings</label><label>adoptme</label></labels><created>2015-08-11T17:55:48Z</created><updated>2015-08-21T08:07:03Z</updated><resolved>2015-08-21T08:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-11T17:55:49Z" id="129992830">@imotov WDYT?
</comment><comment author="dadoonet" created="2015-08-11T17:55:49Z" id="129992833">_From @imotov on February 27, 2015 15:35_

Personally I like `cloud.account` and `cloud.key` more (mostly because they are shorter). But I can see how this can cause some confusion. Not sure.
</comment><comment author="dadoonet" created="2015-08-11T17:55:49Z" id="129992836">Yeah. If we want to mix within a node azure settings and aws settings (let say backup in two places), I think it's better to prefix setting name.
</comment><comment author="dadoonet" created="2015-08-11T17:55:50Z" id="129992839">_From @kimchy on February 27, 2015 15:43_

we do support both, right? I mean, sure, we can simplify and not provide too many options to configure things and be more explicit around the actual aws settings
</comment><comment author="dadoonet" created="2015-08-11T17:55:50Z" id="129992843">we do support both today. But the simplest form `cloud.account` is not documented AFAIK.
</comment><comment author="dadoonet" created="2015-08-11T17:55:50Z" id="129992847">_From @nikolay on February 27, 2015 20:17_

@imotov With AWS, "account" and "access key" are two different things, so, `cloud.account` will be misleading.
</comment><comment author="xuzha" created="2015-08-19T22:39:38Z" id="132812351">Do we still need to discuss about this? If nobody objects,  I'm going to merge this change soon. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove duplicate code in AndQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12808</link><project id="" key="" /><description>When parsing inner array of filters, AndQueryParser seems to check for correct "filters" field name but then does the same kind of operation in the `else` branch of the stament. This PR removes the leniency of allowing array with fieldname other than `filters`. Top level array with nested queries is still allowed.

This PR is against the feature refactoring branch.
</description><key id="100342172">12808</key><summary>Remove duplicate code in AndQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-08-11T15:43:48Z</created><updated>2015-08-13T10:41:31Z</updated><resolved>2015-08-13T10:41:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-11T15:45:56Z" id="129937813">@javanna Closed the corresponding PR against master (https://github.com/elastic/elasticsearch/pull/12576). This one adds tests for the parser.
</comment><comment author="javanna" created="2015-08-13T09:44:08Z" id="130593161">left two minor comments
</comment><comment author="cbuescher" created="2015-08-13T10:31:11Z" id="130605761">@javanna rebased with one minor modifications
</comment><comment author="javanna" created="2015-08-13T10:38:57Z" id="130608595">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flatten SearchService and clean up build-in registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12807</link><project id="" key="" /><description>Relates to #12783
</description><key id="100334337">12807</key><summary>Flatten SearchService and clean up build-in registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-11T15:07:22Z</created><updated>2015-08-12T10:08:32Z</updated><resolved>2015-08-12T07:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-11T15:08:17Z" id="129922679">this is basically a proof of concept for #12783 - it's not just possible this is also much cleaner than the nested module mess we had before. Classes are registered where they belong etc. without unnecessary indirections.
</comment><comment author="rjernst" created="2015-08-11T16:02:34Z" id="129947855">This looks great! +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds a setting to control source output in indexing slowlog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12806</link><project id="" key="" /><description>Instead of logging the entire _source in the indexing slowlog we log by default just the first 1000 characters - this is controlled by the `index.indexing.slowlog.source` settings and can be set to `true` to log the whole _source, `false` to log none of it, and a number to log at most that many characters.

Closes #4485
</description><key id="100326920">12806</key><summary>Adds a setting to control source output in indexing slowlog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Logging</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-11T14:35:18Z</created><updated>2015-08-12T09:37:14Z</updated><resolved>2015-08-11T20:57:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-11T14:40:02Z" id="129910610">@jpountz, this could use a review.
</comment><comment author="jpountz" created="2015-08-11T16:00:30Z" id="129946850">LGTM
</comment><comment author="nik9000" created="2015-08-11T20:15:43Z" id="130050808">Ok - rebased and squashed. I reset the author to myself - this was based on @metadave's work in #12655 but little of it remains. Some. But not much. At this point I'm the better person to contact if something goes wrong with this code.

I'll rerun the tests one last time and merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to create Time-based index pattern with wildcard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12805</link><project id="" key="" /><description> `[logs-*-]YYYY.MM.DD` doesn't match logs-syslog-20150811.

This comes from https://github.com/elastic/kibana/blob/1e555c5961686c1d51693506e97fb2da8e483a1a/src/ui/public/index_patterns/_mapper.js#L104:

```
          var matches = all.filter(function (existingIndex) {
            var parsed = moment(existingIndex, indexPattern.id);
            return existingIndex === parsed.format(indexPattern.id);
          });
```

Workaround is to use `return true`
</description><key id="100319954">12805</key><summary>Unable to create Time-based index pattern with wildcard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sathieu</reporter><labels /><created>2015-08-11T14:07:19Z</created><updated>2015-08-11T14:30:31Z</updated><resolved>2015-08-11T14:30:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sathieu" created="2015-08-11T14:08:58Z" id="129893846">NB: Here we have:

```
existingIndex == 'logs-syslog-20150811'
indexPattern.id == '[logs-*-]YYYY.MM.DD'
parsed.format(indexPattern.id) == 'logs-*-2015.08.11'
```
</comment><comment author="sathieu" created="2015-08-11T14:12:01Z" id="129896088">A bit better:

```
          var matches = all.filter(function (existingIndex) {
            var parsed = moment(existingIndex, indexPattern.id);
            var formatted = parsed.format(indexPattern.id);
            return (existingIndex === formatted) || (formatted.indexOf('*') != -1);
          });
```
</comment><comment author="sathieu" created="2015-08-11T14:21:48Z" id="129901461">NB: Consider the above patch public-domain. you can merge it (I won't sign the CLA).
</comment><comment author="sathieu" created="2015-08-11T14:28:07Z" id="129904840">Oups, this should be in kibana instead.
</comment><comment author="sathieu" created="2015-08-11T14:30:31Z" id="129906181">Moved to https://github.com/elastic/kibana/issues/4633
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node does not retry connection to clusters after startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12804</link><project id="" key="" /><description>This issue was raised in a discuss.elastic.co topic: https://discuss.elastic.co/t/child-node-never-connected-if-offline-at-startup-of-tribe/27193

If a cluster is not started when the tribe node starts, it will fail to connect, log a warning and then never try to connect to that cluster again. Other clusters in the tribe work as expected but when the first cluster comes back online the tribe node will not try to connect to it
</description><key id="100313343">12804</key><summary>Tribe node does not retry connection to clusters after startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Tribe Node</label><label>adoptme</label><label>bug</label></labels><created>2015-08-11T13:40:57Z</created><updated>2016-01-26T19:37:58Z</updated><resolved>2016-01-26T19:37:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="espenwa" created="2015-08-17T11:11:36Z" id="131783486">Would it be possible to re-evaluate the 2.1.0-scheduling of this issue? A very simple timed retry every x minutes would do wonders for the usability of the tribe-feature. 
</comment><comment author="sherry-ger" created="2015-09-03T17:16:28Z" id="137516958">I could not reproduce this with both multicast and unicast on ES version 1.7.1
</comment><comment author="TinLe" created="2015-09-03T17:40:52Z" id="137523539">I saw this issue on ES 1.4.5 and earlier.   I am in the process of
qualifying 1.7.1 in our environment.   It may have been fixed after 1.4.5.

Tin

On Thu, Sep 3, 2015 at 10:16 AM, sherry-ger notifications@github.com
wrote:

&gt; I could not reproduce this with both multicast and unicast on ES version
&gt; 1.7.1
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12804#issuecomment-137516958
&gt; .
</comment><comment author="clintongormley" created="2016-01-26T19:37:58Z" id="175193753">This appears to be fixed at least in 2.2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build of QA: Smoke Test Shaded Jar fails under maven 3.3.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12803</link><project id="" key="" /><description>Build fails with maven 3.3.1 and 3.3.3. To reproduce, install one of the 3.3.x versions of maven and run `mvn clean verify` in the root directory of the project. The build will fail in the QA: Smoke Test Shaded Jar module with the following error:

```
Started J0 PID(99979@flea.local).
Suite: org.elasticsearch.shaded.test.ShadedIT
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testJodaIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.06s | ShadedIT.testJodaIsNotOnTheCP &lt;&lt;&lt;
  &gt; Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException
  &gt; at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:3A9404F1F69FD80]:0)
  &gt; at junit.framework.Assert.fail(Assert.java:57)
  &gt; at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testGuavaIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.01s | ShadedIT.testGuavaIsNotOnTheCP &lt;&lt;&lt;
  &gt; Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException
  &gt; at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:C2502FD54D83433D]:0)
  &gt; at junit.framework.Assert.fail(Assert.java:57)
  &gt; at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testjsr166eIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.01s | ShadedIT.testjsr166eIsNotOnTheCP &lt;&lt;&lt;
  &gt; Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException
  &gt; at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:35593286F4269392]:0)
  &gt; at junit.framework.Assert.fail(Assert.java:57)
  &gt; at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /Users/Shared/Jenkins/Home/workspace/elasticsearch-master/qa/smoke-test-shaded/target/J0/temp/org.elasticsearch.shaded.test.ShadedIT_2F4D23A7462CF921-001
  2&gt; NOTE: test params are: codec=CheapBastard, sim=DefaultSimilarity, locale=, timezone=Asia/Baku
  2&gt; NOTE: Mac OS X 10.10.4 x86_64/Oracle Corporation 1.8.0_25 (64-bit)/cpus=8,threads=1,free=482137936,total=514850816
  2&gt; NOTE: All tests run in this JVM: [ShadedIT]
Completed [1/1] in 6.61s, 5 tests, 3 failures &lt;&lt;&lt; FAILURES!

Tests with failures:
  - org.elasticsearch.shaded.test.ShadedIT.testJodaIsNotOnTheCP
  - org.elasticsearch.shaded.test.ShadedIT.testGuavaIsNotOnTheCP
  - org.elasticsearch.shaded.test.ShadedIT.testjsr166eIsNotOnTheCP
```

Please note that build doesn't fail with maven 3.2.x and it doesn't fail if mvn command is executed inside the qa/smoke-test-shaded directory. Only when the build is started from the root directory the error above can be observed.

The reason is because of the shaded version which depends on elasticsearch core.
When Maven build the module only, then elasticsearch core is not added to the dependency tree.

``` sh
mvn dependency:tree -pl :smoke-test-shaded
```

```
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ smoke-test-shaded ---
[INFO] org.elasticsearch.qa:smoke-test-shaded:jar:2.0.0-beta1-SNAPSHOT
[INFO] +- org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
[INFO] |  +- org.apache.lucene:lucene-core:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-queries:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-memory:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-misc:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-join:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-grouping:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile
[INFO] |  \- com.spatial4j:spatial4j:jar:0.4.1:compile
[INFO] +- org.hamcrest:hamcrest-all:jar:1.3:test
[INFO] \- org.apache.lucene:lucene-test-framework:jar:5.2.1:test
[INFO]    +- org.apache.lucene:lucene-codecs:jar:5.2.1:test
[INFO]    +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test
[INFO]    +- junit:junit:jar:4.11:test
[INFO]    \- org.apache.ant:ant:jar:1.8.2:test
```

But if shaded plugin is involved during the build, it modifies the `projectArtifactMap`:

``` sh
mvn dependency:tree -pl org.elasticsearch.distribution.shaded:elasticsearch,:smoke-test-shaded
```

```
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ smoke-test-shaded ---
[INFO] org.elasticsearch.qa:smoke-test-shaded:jar:2.0.0-beta1-SNAPSHOT
[INFO] +- org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
[INFO] |  \- org.elasticsearch:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
[INFO] |     +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-queries:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-memory:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
[INFO] |     |  \- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile
[INFO] |     |  \- org.apache.lucene:lucene-misc:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-join:jar:5.2.1:compile
[INFO] |     |  \- org.apache.lucene:lucene-grouping:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile
[INFO] |     |  \- com.spatial4j:spatial4j:jar:0.4.1:compile
[INFO] |     +- com.google.guava:guava:jar:18.0:compile
[INFO] |     +- com.carrotsearch:hppc:jar:0.7.1:compile
[INFO] |     +- joda-time:joda-time:jar:2.8:compile
[INFO] |     +- org.joda:joda-convert:jar:1.2:compile
[INFO] |     +- com.fasterxml.jackson.core:jackson-core:jar:2.5.3:compile
[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.5.3:compile
[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.5.3:compile
[INFO] |     |  \- org.yaml:snakeyaml:jar:1.12:compile
[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.5.3:compile
[INFO] |     +- io.netty:netty:jar:3.10.3.Final:compile
[INFO] |     +- com.ning:compress-lzf:jar:1.0.2:compile
[INFO] |     +- com.tdunning:t-digest:jar:3.0:compile
[INFO] |     +- org.hdrhistogram:HdrHistogram:jar:2.1.6:compile
[INFO] |     +- org.apache.commons:commons-lang3:jar:3.3.2:compile
[INFO] |     +- commons-cli:commons-cli:jar:1.3.1:compile
[INFO] |     \- com.twitter:jsr166e:jar:1.1.0:compile
[INFO] +- org.hamcrest:hamcrest-all:jar:1.3:test
[INFO] \- org.apache.lucene:lucene-test-framework:jar:5.2.1:test
[INFO]    +- org.apache.lucene:lucene-codecs:jar:5.2.1:test
[INFO]    +- org.apache.lucene:lucene-core:jar:5.2.1:compile
[INFO]    +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test
[INFO]    +- junit:junit:jar:4.11:test
[INFO]    \- org.apache.ant:ant:jar:1.8.2:test
```

A fix could consist of fixing something on Maven side. Probably something changed in a recent version and introduced this "issue" but it might be not really an issue. More a fix.

There are two workarounds:

1) exclude manually elasticsearch core from shaded version in smoke-test-shaded module and add manually each lucene lib needed by elasticsearch

2) add a new `elasticsearch-lucene` (lucene) POM module which simply declares all needed lucene libs in subprojects (such as the smoke tester one).

I choose in the commit the later but I'm open to suggestions!
</description><key id="100310374">12803</key><summary>Build of QA: Smoke Test Shaded Jar fails under maven 3.3.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>test</label></labels><created>2015-08-11T13:24:19Z</created><updated>2015-08-13T13:26:53Z</updated><resolved>2015-08-13T10:19:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-11T13:39:56Z" id="129876701">Related Maven Shade Plugin issue: https://issues.apache.org/jira/browse/MSHADE-202
</comment><comment author="dadoonet" created="2015-08-11T13:58:36Z" id="129886117">For information, I tried without this patch to change maven-shade-plugin to 2.3 and 2.2 and it did not change anything. I'm afraid the root cause is Maven 3.3 itself.
</comment><comment author="dadoonet" created="2015-08-13T08:35:35Z" id="130574778">Related to #12791 
</comment><comment author="s1monw" created="2015-08-13T08:48:34Z" id="130578831">LGTM
</comment><comment author="dadoonet" created="2015-08-13T10:14:04Z" id="130602753">Works pretty much as expected:

```
[INFO] Elasticsearch Build Resources ...................... SUCCESS [  2.792 s]
[INFO] Elasticsearch Rest API Spec ........................ SUCCESS [  1.426 s]
[INFO] Lucene dependencies for elasticsearch .............. SUCCESS [  9.003 s]
[INFO] Elasticsearch Parent POM ........................... SUCCESS [ 16.848 s]
[INFO] Elasticsearch Core ................................. SUCCESS [31:34 min]
[INFO] Elasticsearch Distribution ......................... SUCCESS [  5.845 s]
[INFO] Elasticsearch with all optional dependencies ....... SUCCESS [  2.438 s]
[INFO] Elasticsearch Shaded Distribution .................. SUCCESS [ 40.974 s]
[INFO] Elasticsearch TAR Distribution ..................... SUCCESS [02:15 min]
[INFO] Elasticsearch ZIP Distribution ..................... SUCCESS [01:53 min]
[INFO] Elasticsearch DEB Distribution ..................... SUCCESS [ 15.692 s]
[INFO] Elasticsearch RPM Distribution ..................... SUCCESS [02:52 min]
[INFO] Elasticsearch Plugin POM ........................... SUCCESS [  5.143 s]
[INFO] Elasticsearch Japanese (kuromoji) Analysis plugin .. SUCCESS [ 52.320 s]
[INFO] Elasticsearch Smart Chinese Analysis plugin ........ SUCCESS [ 48.423 s]
[INFO] Elasticsearch Stempel (Polish) Analysis plugin ..... SUCCESS [ 53.240 s]
[INFO] Elasticsearch Phonetic Analysis plugin ............. SUCCESS [ 58.673 s]
[INFO] Elasticsearch ICU Analysis plugin .................. SUCCESS [01:08 min]
[INFO] Elasticsearch Google Compute Engine cloud plugin ... SUCCESS [ 49.755 s]
[INFO] Elasticsearch Azure cloud plugin ................... SUCCESS [01:44 min]
[INFO] Elasticsearch AWS cloud plugin ..................... SUCCESS [01:02 min]
[INFO] Elasticsearch Delete By Query plugin ............... SUCCESS [01:30 min]
[INFO] Elasticsearch Python language plugin ............... SUCCESS [01:35 min]
[INFO] Elasticsearch JavaScript language plugin ........... SUCCESS [01:03 min]
[INFO] Elasticsearch Mapper size plugin ................... SUCCESS [ 52.873 s]
[INFO] Elasticsearch example JVM plugin ................... SUCCESS [ 31.734 s]
[INFO] Elasticsearch Example site plugin .................. SUCCESS [ 26.972 s]
[INFO] QA: Parent POM ..................................... SUCCESS [  2.241 s]
[INFO] QA: Smoke Test Plugins ............................. SUCCESS [01:21 min]
[INFO] QA: Smoke Test Shaded Jar .......................... SUCCESS [ 25.150 s]
[INFO] QA: Smoke Test Multi-Node IT ....................... FAILURE [01:23 min]
```

The last failure was only caused by #12852.
</comment><comment author="dadoonet" created="2015-08-13T10:19:34Z" id="130604022">Merged with fbd8f6927380aae4557166fe43795f49b774eb34
</comment><comment author="dadoonet" created="2015-08-13T11:09:39Z" id="130620087">It sounds like this PR is now causing issues when running old maven versions (3.0.5 here for example):

```
[es_core_master_small] $ /usr/local/jenkins/tools/hudson.tasks.Maven_MavenInstallation/mvn_3.0.5/bin/mvn -s /usr/local/jenkins/workspace/es_core_master_small/master/ci/pom/es_random_setting.xml clean test -U
```

It also seems to confuse IntelliJ.

I think I should revert this change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>allow specifying additional arguments in startup-elasticsearch macrodef</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12802</link><project id="" key="" /><description>Previously we had additional.args as a argument to the startup-elasticsearch macrodef and this was
being used to set some additional elasticsearch settings. This adds the ability to specify additional
arguments back using a element called additional-args.
</description><key id="100308079">12802</key><summary>allow specifying additional arguments in startup-elasticsearch macrodef</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-11T13:13:39Z</created><updated>2015-08-11T15:07:47Z</updated><resolved>2015-08-11T15:07:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-08-11T13:17:31Z" id="129868964">@s1monw or @rmuir can you take a look?
</comment><comment author="jpountz" created="2015-08-11T13:57:07Z" id="129885086">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused java opts/es java opts from plugin manager call</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12801</link><project id="" key="" /><description>When calling the plugin manager on java 7 with additional JAVA_OPTS
that change heap configuration compared to what is set at the plugin
manager shell script. This resulted in errors.

This commit removes the JAVA_OPTS and ES_JAVA_OPTS from the plugin
manager call to prevent those settings.

Closes #12479
</description><key id="100304792">12801</key><summary>Remove unused java opts/es java opts from plugin manager call</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>bug</label><label>review</label><label>v2.0.0-rc1</label></labels><created>2015-08-11T12:55:59Z</created><updated>2016-03-10T18:13:19Z</updated><resolved>2015-09-15T15:06:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-11T12:57:47Z" id="129861897">I tested the deb/rpm manually, and ran the vagrant tests as well as regular `mvn verify`. However there might still be cases, that we are missing here, like the puppet module or something.

@electrical can you run your tests against this branch to see if this breaks the plugin manager somewhere in your test suite?

Also, whoever is reviewing this, might want to test this with some more custom esoteric plugin paths...
</comment><comment author="electrical" created="2015-08-11T13:40:18Z" id="129876806">@spinscale Im afraid i can't directly test of a branch yet or custom build snapshot packages. I still need to work on that. That said I don't directly see any issues with the proposed change.
</comment><comment author="spinscale" created="2015-08-12T08:56:28Z" id="130221647">tested further, also `bin/plugin install lmenezes/elasticsearch-kopf -Des.path.plugins=/tmp/foo/` works as expected.

Also works as expected: Changing `path.plugins` in `config/elasticsearch.yml` and calling `bin/plugin install lmenezes/elasticsearch-kopf`

Also works as expected: Using `CONF_DIR` environment variables: `CONF_DIR=/tmp bin/plugin install lmenezes/elasticsearch-kopf`
</comment><comment author="spinscale" created="2015-08-12T08:56:47Z" id="130221689">ping @nik9000 or @tlrx for review
</comment><comment author="tlrx" created="2015-08-17T08:36:32Z" id="131730527">Shouldn't we udpate `plugin.bat` too? Otherwise LGTM and it also fixes issues like #11623
</comment><comment author="spinscale" created="2015-08-31T06:13:25Z" id="136277467">changed the `plugin.bat`file as well, can you test that as well, just want to make sure the `-D` syntax is still working
</comment><comment author="s1monw" created="2015-08-31T07:51:00Z" id="136291849">while we are at it can we remove the `Xmx and Xms` settings and force a `-client` jvm? it would just make way more sense to me? We can totally do that in a different PR
</comment><comment author="spinscale" created="2015-09-03T07:50:53Z" id="137367733">+1 for the `-client` parameter, will fix. I printed out `Runtime.getMemory()` stats (free, total, max) at the beginning and end of a plugin manager run. When we omit `Xms/Xms` we get far higher numbers here. Are we still good that change then?

```
With current default (or with -client set, numbers are the same)

# bin/plugin list
total/free/max: 15.5mb/12.2mb/57mb
total/free/max: 19.5mb/10.5mb/57mb
# bin/plugin install file:///Users/alr/devel/elasticsearch/plugins/analysis-icu/target/releases/analysis-icu-2.1.0-SNAPSHOT.zip
total/free/max: 15.5mb/12.2mb/57mb
total/free/max: 27.5mb/5.7mb/57mb
# bin/plugin list
total/free/max: 15.5mb/12.2mb/57mb
total/free/max: 19.5mb/10.4mb/57mb
# bin/plugin remove analysis-icu
total/free/max: 15.5mb/12.2mb/57mb
total/free/max: 19.5mb/10.4mb/57mb

With -client and no -Xmx and -Xms
# bin/plugin list
total/free/max: 245.5mb/239mb/3.5gb
total/free/max: 245.5mb/227.5mb/3.5gb
# bin/plugin install file:///Users/alr/devel/elasticsearch/plugins/analysis-icu/target/releases/analysis-icu-2.1.0-SNAPSHOT.zip
total/free/max: 245.5mb/239mb/3.5gb
total/free/max: 245.5mb/200.6mb/3.5gb
# bin/plugin remove analysis-icu
total/free/max: 245.5mb/239mb/3.5gb
total/free/max: 245.5mb/227.5mb/3.5gb
```
</comment><comment author="spinscale" created="2015-09-07T14:34:00Z" id="138314246">as @clintongormley has had problems installing a bigger plugin in beta1, I now switched to use the client VM and removed the heap size configuration... @s1monw can you take another look?

Ran `mvn clean verify` with all the integration tests locally and it passed
</comment><comment author="nik9000" created="2015-09-15T13:28:47Z" id="140393463">LGTM. I can run the vagrant tests with this applied as super double extra check but I think its fine.
</comment><comment author="nik9000" created="2015-09-15T13:43:30Z" id="140397500">Ran vagrant tests with this. Everything is good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: better naming consistency for getters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12800</link><project id="" key="" /><description>In the query refactoring branch we've been introducing getter methods for every bit that you can set to each query. The naming is not every consistent at the moment. The applied naming convention are the following:
- `innerQuery()` for any inner query, when there's only one of them
- when there's more than one inner query, use a prefix that identifies which query it is, and the `query` suffix (e.g. `positiveQuery` or `littleQuery`)
- `fieldName()` for the name of the field to be queried
- `value()` for the actual query

These changes don't break bw comp given that these getters were all introduced with the query refactoring which hasn't been released yet. Also we are modifying getters that don't have a corresponding setter, as the fields are final, hence we are not breaking consistency between getter and setter.
</description><key id="100301460">12800</key><summary>Query refactoring: better naming consistency for getters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>enhancement</label><label>review</label></labels><created>2015-08-11T12:32:09Z</created><updated>2015-08-11T14:03:07Z</updated><resolved>2015-08-11T14:03:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-11T13:40:11Z" id="129876770">LGTM
</comment><comment author="kimchy" created="2015-08-11T13:42:01Z" id="129877281">btw, @javanna, since we are slowly getting away from non "getXXX" method, i might make sense to have all the methods here start with "get". (`getValue`, ...). I have been slowly moving to getters where I can, for example, in the stats API
</comment><comment author="javanna" created="2015-08-11T13:43:41Z" id="129877765">yea been thinking about that @kimchy I think we decided to wait a bit and move getters and setters at the same time, otherwise there are inconsistencies between them. Will do that at some point ;)
</comment><comment author="kimchy" created="2015-08-11T13:44:09Z" id="129877904">@javanna ++
</comment><comment author="alexksikes" created="2015-08-11T13:47:26Z" id="129878936">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] failures in MovAvgUnitTests with seed 571CCDD579600679</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12799</link><project id="" key="" /><description>I just ran on the latest master branch `mvn clean verify` from the root dir and it failed in core module with those tests:

```
Suite: org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=571CCDD579600679 -Dtests.class=org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests -Dtests.method="testHoltWintersMultiplicativePadModel" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=en_NZ -Dtests.timezone=Asia/Manila
FAILURE 0.02s J0 | MovAvgUnitTests.testHoltWintersMultiplicativePadModel &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;0&gt;
   &gt;      but: was &lt;1&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([571CCDD579600679:B423C11EAE0C0175]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests.testHoltWintersMultiplicativePadModel(MovAvgUnitTests.java:383)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=571CCDD579600679 -Dtests.class=org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests -Dtests.method="testHoltWintersAdditiveModel" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=en_NZ -Dtests.timezone=Asia/Manila
FAILURE 0.01s J0 | MovAvgUnitTests.testHoltWintersAdditiveModel &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;0&gt;
   &gt;      but: was &lt;1&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([571CCDD579600679:F3BB0CD1D2ABDC22]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests.testHoltWintersAdditiveModel(MovAvgUnitTests.java:523)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=571CCDD579600679 -Dtests.class=org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests -Dtests.method="testHoltWintersMultiplicativePadPredictionModel" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=en_NZ -Dtests.timezone=Asia/Manila
FAILURE 0.01s J0 | MovAvgUnitTests.testHoltWintersMultiplicativePadPredictionModel &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;0&gt;
   &gt;      but: was &lt;-1&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([571CCDD579600679:777379F9A211757]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests.testHoltWintersMultiplicativePadPredictionModel(MovAvgUnitTests.java:454)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=571CCDD579600679 -Dtests.class=org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests -Dtests.method="testHoltWintersAdditivePredictionModel" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=en_NZ -Dtests.timezone=Asia/Manila
FAILURE 0.01s J0 | MovAvgUnitTests.testHoltWintersAdditivePredictionModel &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;0&gt;
   &gt;      but: was &lt;1&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([571CCDD579600679:4AC23A92C590A297]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests.testHoltWintersAdditivePredictionModel(MovAvgUnitTests.java:594)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/core/target/J0/temp/org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests_571CCDD579600679-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene50): {}, docValues:{}, sim=DefaultSimilarity, locale=en_NZ, timezone=Asia/Manila
  2&gt; NOTE: Mac OS X 10.10.4 x86_64/Oracle Corporation 1.7.0_60 (64-bit)/cpus=4,threads=1,free=334469960,total=525860864
  2&gt; NOTE: All tests run in this JVM: [ElasticsearchPostingsFormatTest, TribeUnitTests, IndexShardModuleTests, DeflateXContentTests, Rest2Tests, NodeClientTests, StoreTest, ArrayUtilsTests, RatioValueTests, FuzzinessTests, DoubleFieldDataTests, StopTokenFilterTests, AnalysisFactoryTests, GetTermVectorsTests, IndexLookupTests, TermsShardMinDocCountTests, SimpleAllTests, IndicesLifecycleListenerSingleNodeTests, MultiPercolatorTests, MissingTests, CompletionTokenStreamTest, CreateIndexRequestBuilderTest, PathTests, GeoDistanceTests, KeepTypesFilterFactoryTests, PatternCaptureTokenFilterTests, BuilderRawFieldTests, ShardInfoTests, TermVectorsUnitTests, SearchFieldsTests, MultiDataPathUpgraderTests, VersionsTests, DynamicMappingTests, IndicesRequestTests, SearchWithRejectionsTests, MinTests, SumTests, LoggingListenerTests, GeoJSONShapeParserTests, RoutingBackwardCompatibilityUponUpgradeTests, MoreLikeThisTests, HppcMapsTests, RecoveriesCollectionTests, Murmur3FieldMapperTests, HttpPublishPortTests, DelayedAllocationTests, FieldStatsIntegrationTests, HistogramTests, JNANativesTests, ClusterHealthResponsesTests, IndicesLeaksTests, ParentIdAggTests, SimpleDynamicTemplatesTests, NoneRecyclerTests, UpdateNumberOfReplicasTests, ParentFieldTypeTests, ByteUtilsTests, SyncedFlushSingleNodeTest, FilteringAllocationTests, ByteSizeUnitTests, JarHellTests, NodeVersionAllocationDeciderTests, UpgradeReallyOldIndexTest, EnvironmentTests, ShardRoutingTests, ClusterHealthTests, ZenFaultDetectionTests, PercentileRanksTests, GeoShapeFieldTypeTests, FilterRoutingTests, IndexWithShadowReplicasTests, TransportMessageTests, SuggestStatsTests, ExternalValuesMapperIntegrationTests, UnicastZenPingTests, BasicAnalysisBackwardCompatibilityTests, SimpleBlocksTests, CompletionFieldMapperTests, UnassignedInfoTests, ListenerActionTests, IndexLifecycleActionTests, GetIndexBackwardsCompatibilityTests, Rest6Tests, IdFieldTypeTests, MathUtilsTests, IndicesStoreIntegrationTests, RecoveryStateTest, SimpleIndexTemplateTests, SimpleExistsTests, TransportClientTests, StoredNumericValuesTest, FileWatcherTest, TranslogVersionTests, ActionNamesTests, ConcurrentPercolatorTests, BlobStoreTest, ClusterStatsTests, GeoFilterTests, SettingsValidatorTests, IndicesSegmentsRequestTests, SimpleMultiSearchTests, MultiPercolatorRequestTests, CircuitBreakerNoopTests, ParentFieldLoadingTest, GeoLocationContextMappingTest, JsonSettingsLoaderTests, IndexRequestBuilderTests, ParseMappingTypeLevelTests, TimeZoneRoundingTests, XContentFactoryTests, ChildrenTests, TenShardsOneReplicaRoutingTests, IndexShardTests, StatsTests, PreBuiltAnalyzerIntegrationTests, SnapshotUtilsTests, JacksonLocationTests, SearchWhileCreatingIndexTests, CJKFilterFactoryTests, UpdateNoopTests, RejectionActionTests, EnableAllocationDeciderIntegrationTest, SearchTimeoutTests, GeoUtilsTests, Rest5Tests, ByteSizeValueTests, RegexTests, ShardReduceTests, SloppyMathTests, StemmerTokenFilterFactoryTests, OptimizeBlocksTests, DoubleIndexingDocTest, IndexAliasesServiceTests, NestedAggregatorTest, SortParserTests, ListenableActionFutureTests, TTLPercolatorTests, DedicatedAggregationTests, ExplainableScriptTests, DiscoveryWithServiceDisruptionsTests, ScanContextTests, LegacyVerificationTests, CborFilteringGeneratorTests, FunctionScorePluginTests, MustacheScriptEngineTest, NativeScriptTests, UpdateMappingIntegrationTests, CorruptedFileTest, CorruptedCompressorTests, IndicesRequestCacheTests, UpdateThreadPoolSettingsTests, ShardReplicationTests, MultiValueModeTests, KeyedLockTests, RoutingTypeMapperTests, IndexTypeMapperIntegrationTests, ScoreTypeTests, SearchWithRandomExceptionsTests, ClusterStateRequestTest, RebalanceAfterActiveTests, SearchServiceTests, GeoPointParsingTests, SearchScrollWithFailingNodesTests, PathTrieTests, TokenCountFieldMapperTests, IndexCacheableQueryTests, DecayFunctionScoreTests, AggregationsBinaryTests, UnicastBackwardsCompatibilityTest, AckClusterUpdateSettingsTests, SimpleDateMappingTests, CreateIndexTests, FunctionScoreTests, ScriptedMetricTests, BulkProcessorTests, DirectoryUtilsTest, SearchSourceBuilderTest, NGramTokenizerFactoryTests, OpenCloseIndexTests, DateHistogramOffsetTests, StopAnalyzerTests, IdMappingTests, DeflateCompressedStreamTests, MissingValuesTests, FileSystemUtilsTests, IndexQueryParserFilterDateRangeTimezoneTests, ElasticsearchExceptionTests, MetaDataTests, SmileFilteringGeneratorTests, RoutingServiceTests, DateFieldTypeTests, CircuitBreakerUnitTests, IndexRequestBuilderTest, SignificantTermsBackwardCompatibilityTests, SourceFetchingTests, XContentBuilderTests, TransportSearchFailuresTests, ChannelsTests, SimpleThreadPoolTests, SimpleSortTests, SnapshotBlocksTests, PercolatorTests, Rest0Tests, DoubleTermsTests, DuelScrollTests, GetActionTests, GeoDistanceTests, LZFCompressedStreamTests, ContextAndHeaderTransportTests, BucketScriptTests, CircuitBreakerServiceTests, TokenCountFieldMapperIntegrationTests, SearchWhileRelocatingTests, MappingMetaDataParserTests, MockDiskUsagesTests, MinBucketTests, AliasRoutingTests, TransformOnIndexMapperIntegrationTest, LongHashTests, ClusterStateDiffPublishingTests, CustomQueryParserTests, NaNSortingTests, TTLMappingTests, TypesExistsTests, FloatFieldDataTests, SimpleStringMappingTests, GeoShapeFieldMapperTests, InternalTestClusterTests, InternalEngineSettingsTest, ScriptQuerySearchTests, BinaryDVFieldDataTests, MissingValueTests, DeleteIndexBlocksTests, CompoundAnalysisTests, ResponseHeaderPluginTests, TransportClientRetryTests, CommonGramsTokenFilterFactoryTests, BitSetFilterCacheTest, MovAvgUnitTests]
Completed [733/803] on J0 in 0.12s, 13 tests, 4 failures &lt;&lt;&lt; FAILURES!
```
</description><key id="100276510">12799</key><summary>[test] failures in MovAvgUnitTests with seed 571CCDD579600679</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>test</label></labels><created>2015-08-11T10:07:22Z</created><updated>2015-08-11T10:15:36Z</updated><resolved>2015-08-11T10:13:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-11T10:13:12Z" id="129809579">Probably a local issue. Closing...
</comment><comment author="dadoonet" created="2015-08-11T10:15:36Z" id="129810449">Argh... This moment you discover you were working in an outdated dir containing an outdated version of the project! What a shame... :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple query string: remove (not working) support for alternate formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12798</link><project id="" key="" /><description>Removed attempt of parsing of `field` rather than `fields` and attempted support of the following syntax:

```
{
  "simple_query_string": {
    "body" : {
      "query": "foo bar"
    }
  }
}
```

Both these two syntaxes were undocumented, untested and not working.

Added test for case when `fields` is not specified, then the default field is queried.

Closes #12794
</description><key id="100273696">12798</key><summary>simple query string: remove (not working) support for alternate formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-11T09:55:57Z</created><updated>2015-08-21T09:22:55Z</updated><resolved>2015-08-11T10:55:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-11T10:11:42Z" id="129808875">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add multi-node IT infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12797</link><project id="" key="" /><description>This adds the infrastrucutre to run integration tests with more than one node.
- it adds relevant macros and targets to integration-tests.xml to start unicast nodes
- there is a qa/smoke-test-multinode project that simulates such a setup

this commit is soely the infrastructure and doesn't hook up any projects to use this.
For reliability and stability reasons this should be used with care and only if it's really
needed.

Closes #12718
</description><key id="100270376">12797</key><summary>Add multi-node IT infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>review</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-11T09:37:03Z</created><updated>2015-08-13T14:27:02Z</updated><resolved>2015-08-11T10:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-11T10:13:05Z" id="129809520">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apply additional plugin settings only if settings are not explicit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12796</link><project id="" key="" /><description>We have a way to allow a plugin to specify additional settings. These
settings should only be applied if they are not already existing in the
node settings.
</description><key id="100251918">12796</key><summary>Apply additional plugin settings only if settings are not explicit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-11T08:07:43Z</created><updated>2015-08-11T14:11:48Z</updated><resolved>2015-08-11T08:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-11T08:10:49Z" id="129758657">LGTM
</comment><comment author="rjernst" created="2015-08-11T08:21:33Z" id="129764973">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy script update causes dynamic mapping update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12795</link><project id="" key="" /><description>Hey!

My mapping is set as `float` but when I retrieve my document it is returned as a string and not a float. I have seen this on `1.7.1` and `1.5.2`.

I have a groovy script which updates a document field like:

``` groovy
ctx._source.clicks += click;
ctx._source.loads += load;
ctx._source.ctr = ctx._source.clicks / ctx._source.loads;
```

My template type is set to float:

``` json
{"ctr":{"type":"float"},"clicks":{"type":"integer"},"loads":{"type":"integer"}}
```

I have updated the document using my script through curl:

```
curl -XPOST 'localhost:9200/docs/stats/a43ae07c6be014d843f77d07c587b1f926b31f7eb/_update?pretty' -d '{"script_id":"update","params":{"click":0,"load":2}}'
curl -XPOST 'localhost:9200/docs/stats/a43ae07c6be014d843f77d07c587b1f926b31f7eb/_update?pretty' -d '{"script_id":"update","params":{"click":1,"load":0}}'
```

I see a log message from Elasticsearch on the first update:

```
[2015-08-11 09:06:04,171][INFO ][cluster.metadata         ] [Scream] [ads] update_mapping [data] (dynamic)
```

The document is always created with `ctr=0`. But after updating with the script it looks like:

``` json
{"ctr":"0.5","clicks":1,"loads":2}
```

So I thought I'd try to do `Float.parseFloat` since it is returned as a string but that just gives me an  exception, because it isn't actually a string:

``` json
{
  "error" : "ElasticsearchIllegalArgumentException[failed to execute script]; nested: GroovyScriptExecutionException[MissingMethodException[No signature of method: static java.lang.Float.parseFloat() is applicable for argument types: (java.math.BigDecimal) values: [0.3333333333]\nPossible solutions: parseFloat(java.lang.String)]]; ",
  "status" : 400
}
```

Seeing as other arithmetic operations seem to return a numeric type and not a string, I am guessing this should too?

It works fine when I use `BigDecimal.floatValue()`:

``` groovy
ctx._source.ctr = (ctx._source.clicks / ctx._source.loads).floatValue();
```
</description><key id="100247640">12795</key><summary>Groovy script update causes dynamic mapping update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bondza</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-08-11T07:43:52Z</created><updated>2015-08-14T07:55:22Z</updated><resolved>2015-08-13T14:58:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-11T14:04:01Z" id="129890496">Hi @Bondza 

Groovy uses BigDecimal for these operations, which are cast to strings by Elasticsearch.  This will work too:

```
ctx._source.ctr = (double) ctx._source.clicks / ctx._source.loads;
```

@dakrone Was #6609 supposed to fix this? if not, is it fixable?
</comment><comment author="dakrone" created="2015-08-11T14:08:38Z" id="129893565">#6609 was only to fix constants (like `1.3`) being compiled as BigDecimals by Groovy, I think this is the Groovy math itself doing this on division.

I'm not sure if it's fixable, I can do some research to see if it is.
</comment><comment author="Bondza" created="2015-08-11T16:26:23Z" id="129955671">I thought that it might have something to do with how Elasticsearch handles BigDecimal. Unfortunately it made my query scripts cause an exception later on.

It is a bit confusing though, since the field which it assigns is not of type string. It thought that would cause an exception since there is a mapping set for that field?

It would be awesome if it was fixable! :)
</comment><comment author="masaruh" created="2015-08-13T08:07:51Z" id="130570176">Groovy returns `BigDecimal` for integer division (http://docs.groovy-lang.org/latest/html/documentation/core-syntax.html#integer_division) and it's serialized as `String` as described in #12385.

Best bet would be to cast/parse yourself...
</comment><comment author="dakrone" created="2015-08-13T14:11:44Z" id="130690027">Yes, I tried to find a way where we could automatically change Groovy's integer division, but unfortunately it doesn't look like there's a programmatic way to do it from a Transformer (unlike the constant expressions where we can).

I think right now best bet is to cast to `(double)` as Clint mentioned.
</comment><comment author="clintongormley" created="2015-08-13T14:58:31Z" id="130721447">thanks @dakrone . Closing
</comment><comment author="Bondza" created="2015-08-14T07:55:22Z" id="131010847">Alright, casting works fine so I'll remember this the next time I need to do some scripting in Elasticsearch.

Thanks for the help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple query string: remove (not working) support for alternate formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12794</link><project id="" key="" /><description>The simple_query_string supports the following main format properly:

```
{
  "simple_query_string": {
    "query": "foo bar",
    "fields": ["body^5","_all"]
  }
}
```

I noticed in the parser also an attempt of supporting the following, undocumented, untested and not working:

```
{
  "simple_query_string": {
    "query": "foo bar",
    "field": "body"
  }
}
```

What happens with the above query is that we query the `_all` field as `fields` is not present.

There's also another attempt of supporting the following, undocumented, untested and not working:

```
{
  "simple_query_string": {
    "body" : {
      "query": "foo bar"
    }
  }
}
```

What happens with the above query is again that we query the `_all` field as `fields` is not present.

Given that the last two variants never worked and are not documented I am wondering if it even makes sense to fix them. I personally don't like having so many different ways of specifying the same thing, so I would stick to the `fields` version that always worked and everybody is aware of, and remove the other attempts in the parser. What do others think?
</description><key id="100243564">12794</key><summary>simple query string: remove (not working) support for alternate formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>enhancement</label></labels><created>2015-08-11T07:13:15Z</created><updated>2015-08-11T10:55:00Z</updated><resolved>2015-08-11T10:55:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-11T07:15:40Z" id="129736722">+1 to remove these broken variants
</comment><comment author="s1monw" created="2015-08-11T07:29:21Z" id="129739456">++
</comment><comment author="clintongormley" created="2015-08-11T09:44:10Z" id="129793054">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove node.local and node.mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12793</link><project id="" key="" /><description>These settings parallel each other and are also confusing with `transport.type`. This causes problems in tests sometimes since we randomize the first two settings. So if a test wants to rely on, eg always using netty, it has to remember to set _two_ settings, to make sure randomization doesn't mess this up. And if the test forgets one, it can have weird consequences, like the wrong transport address instance being created, causing class cast exceptions like in #12788.

I think we should minimize these to as single setting (preferably `transport.type` since that is the pluggable setting).
</description><key id="100221190">12793</key><summary>Remove node.local and node.mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>discuss</label></labels><created>2015-08-11T04:04:56Z</created><updated>2016-01-26T18:37:32Z</updated><resolved>2016-01-26T18:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T18:37:32Z" id="175164262">Duplicate of https://github.com/elastic/elasticsearch/issues/16234
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get rid of http.enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12792</link><project id="" key="" /><description>Do we really need this?

To me it seems like a bad setting as it stops simple interactions with the cluster and also destroys any ability to do monitoring (especially with Marvel).
</description><key id="100215775">12792</key><summary>Get rid of http.enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Network</label><label>adoptme</label><label>v6.0.0</label></labels><created>2015-08-11T03:09:00Z</created><updated>2017-05-24T20:38:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-11T05:53:59Z" id="129713930">I know it's used in wares plugin.
Also It could make sense to disable this on production nodes and only activate it on monitoring nodes.

disabling http does not stop Marvel collecting data and sending it to the monitoring cluster, right?
</comment><comment author="colings86" created="2015-08-11T08:06:40Z" id="129753921">It also may be useful for people who only want to interact with Elasticsearch using the Java API and therefore have no need for the HTTP protocol
</comment><comment author="markwalkom" created="2015-08-11T08:14:47Z" id="129763186">To me, allowing this is akin to keeping _shutdown.
Sure it is useful for a few people, but disabling it breaks too much stuff to be really worth it.
</comment><comment author="dadoonet" created="2015-08-11T08:17:59Z" id="129764311">@markwalkom I think you should ask also the question on discuss.elastic.co to see how many users actually are disabling http, for which use case and invite them to comment on this issue.
</comment><comment author="jprante" created="2015-08-15T11:57:51Z" id="131360396">Beside of using Elasticsearch by Java API only, I like the feature of disabling HTTP port to protect data nodes in a private network from tampering with them by sending spurious HTTP requests. The disabling of HTTP also assures that HTTP-based overhead (including HTTP client traffic and workload) is not present on that nodes, which may be of some importance in a team environment with distributed tasks of searching, indexing, monitoring, operating.

Otherwise, I would have to set up operating system layer TCP/IP filtering services (e.g. RHEL 7 firewalld) on all ports in the range 9200-9299, which is of course possible, but is another thing that can be easily forgotten when adding nodes (and should be documented in case).

Maybe I'm wrong, but I sincerely hope that optional commercial add-ons are not driving Elasticsearch features. As an alternative, I suggest to improve the Marvel product by using Java API only.
</comment><comment author="dakrone" created="2016-09-27T15:30:24Z" id="249901063">I think with our current work towards a Java HTTP client, along with moving as much as possible to APIs, and with not being able to access those APIs hampers any kind of debugging ability, we should remove the setting and enable HTTP everywhere. It's been a while since we've discussed this, does anyone else have thoughts?
</comment><comment author="dadoonet" created="2016-09-27T15:40:15Z" id="249904305">If we have an architecture with data nodes and client nodes, would that make sense to still allow disabling http on data nodes?

That being said may be client nodes could be seen/replaced by federated search/index nodes (well tribe nodes nowadays) which I believe would use the REST layer?

IMO that sounds ok to remove `http.enabled` setting. Would love also to hear from others though.
</comment><comment author="dakrone" created="2016-09-27T15:41:14Z" id="249904615">&gt; If we have an architecture with data nodes and client nodes, would that make sense to still allow disabling http on data nodes?

I still think it's valuable, since you may want to retrieve something like `GET /_cluster/state?local=true` for diagnosis, but you then cannot since HTTP is disabled.
</comment><comment author="jaymode" created="2017-01-26T22:48:39Z" id="275538758">+1 to remove the option of disabling HTTP (I was just about to open a new issue for this). With our continued work on making a REST client be the official Java client, I think this is the way to go.</comment><comment author="s1monw" created="2017-01-27T09:52:00Z" id="275627771">I am +1 on this too. If somebody doesn't wanna expose stuff to the outer world they can just bind to localhost and they are good?</comment><comment author="javanna" created="2017-05-05T15:49:14Z" id="299501732">There seems to be consensus on this and nothing to discuss further. I labelled this 6.0.0 and adoptme thinking that we could start with deprecating the setting in 6.0.0 and remove in 7.0.0. </comment><comment author="ppf2" created="2017-05-05T19:07:22Z" id="299550445">Before we move forward with this, let's review where this is used across our stack.  For example, Logstash depends on this for the [sniffing](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-sniffing) feature in the ES output and we have users out there who wants sniffing but at the same time would like the ability to have LS send events only to a subset of sniffed nodes (by disabling HTTP on specific nodes).</comment><comment author="jordansissel" created="2017-05-05T19:58:07Z" id="299561275">&gt; Logstash depends on this for the sniffing feature in the ES output 

Logstash (code) does not depend on `http.enabled` as far as I can tell. We do mention `http.enabled` it in the documentation to help users select which Elasticsearch nodes they want to talk to (sniffing all nodes, subtract nodes with http disabled). It is not a required feature, though, nor is it a out-of-the-box dependency. For the future, we want to improve our selection criteria (by node tag, etc) for Logstash's sniffing, so we won't need "running http?" as criteria.

However ... 
Logstash does depend on [the `/_nodes/http` api ](https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/master/lib/logstash/outputs/elasticsearch/http_client/pool.rb#L37 )in order to perform sniffing. If this API is going away, I cannot tell from the discussion in this ticket.

Can someone tell us if `/_nodes/http` is being removed as a part of this effort?</comment><comment author="jasontedor" created="2017-05-05T20:01:25Z" id="299562091">&gt; Can someone tell us if `/_nodes/http` is being removed as a part of this effort?

We would not, or at least, I would be opposed to removing it.</comment><comment author="javanna" created="2017-05-05T20:05:05Z" id="299562966">All our language clients depend on `/_nodes/http` for sniffing. That will stay. This issue is about removing the ability to disable http on a node, through the `http.enabled` setting.</comment><comment author="andrewvc" created="2017-05-23T18:24:24Z" id="303489205">I've been thinking that the best thing for most clients to do would be to use node attrs for sniffing. All of the other automatic criteria are problematic. It's best just to let users label nodes how they like and use that info to sniff.

That's probably a larger discussion that should be had among client library authors however.</comment><comment author="andrewvc" created="2017-05-24T20:38:36Z" id="303844655">I've created an issue to discuss some proposed changes to clients to help with this here: https://github.com/elastic/elasticsearch/issues/24871</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build of QA: Smoke Test Shaded Jar fails under maven 3.3.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12791</link><project id="" key="" /><description>Build fails with maven 3.3.1 and 3.3.3. To reproduce, install one of the 3.3.x versions of maven and run `mvn clean verify` in the root directory of the project. The build will fail in the QA: Smoke Test Shaded Jar module with the following error:

```
Started J0 PID(99979@flea.local).
Suite: org.elasticsearch.shaded.test.ShadedIT
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testJodaIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.06s | ShadedIT.testJodaIsNotOnTheCP &lt;&lt;&lt;
  &gt; Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException
  &gt; at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:3A9404F1F69FD80]:0)
  &gt; at junit.framework.Assert.fail(Assert.java:57)
  &gt; at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testGuavaIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.01s | ShadedIT.testGuavaIsNotOnTheCP &lt;&lt;&lt;
  &gt; Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException
  &gt; at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:C2502FD54D83433D]:0)
  &gt; at junit.framework.Assert.fail(Assert.java:57)
  &gt; at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testjsr166eIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.01s | ShadedIT.testjsr166eIsNotOnTheCP &lt;&lt;&lt;
  &gt; Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException
  &gt; at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:35593286F4269392]:0)
  &gt; at junit.framework.Assert.fail(Assert.java:57)
  &gt; at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /Users/Shared/Jenkins/Home/workspace/elasticsearch-master/qa/smoke-test-shaded/target/J0/temp/org.elasticsearch.shaded.test.ShadedIT_2F4D23A7462CF921-001
  2&gt; NOTE: test params are: codec=CheapBastard, sim=DefaultSimilarity, locale=, timezone=Asia/Baku
  2&gt; NOTE: Mac OS X 10.10.4 x86_64/Oracle Corporation 1.8.0_25 (64-bit)/cpus=8,threads=1,free=482137936,total=514850816
  2&gt; NOTE: All tests run in this JVM: [ShadedIT]
Completed [1/1] in 6.61s, 5 tests, 3 failures &lt;&lt;&lt; FAILURES!


Tests with failures:
  - org.elasticsearch.shaded.test.ShadedIT.testJodaIsNotOnTheCP
  - org.elasticsearch.shaded.test.ShadedIT.testGuavaIsNotOnTheCP
  - org.elasticsearch.shaded.test.ShadedIT.testjsr166eIsNotOnTheCP
```

Please note that build doesn't fail with maven 3.2.x and it doesn't fail if mvn command is executed inside the qa/smoke-test-sharded directory. Only when the build is started from the root directory the error above can be observed. 
</description><key id="100198732">12791</key><summary>Build of QA: Smoke Test Shaded Jar fails under maven 3.3.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>build</label><label>test</label></labels><created>2015-08-11T00:38:50Z</created><updated>2015-09-08T19:28:43Z</updated><resolved>2015-09-08T19:28:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-11T10:52:55Z" id="129834405">I can confirm and reproduce the issue. 

Funny thing: when you do that again from root dir but "resume from" smoke-test-shaded module, you don't hit the issue...

`mvn clean verify -rf :smoke-test-shaded` gives `BUILD SUCCESS`
</comment><comment author="dadoonet" created="2015-08-11T11:29:16Z" id="129844836">I found a way to quickly reproduce the issue.

```
mvn verify -pl org.elasticsearch.distribution.shaded:elasticsearch,:smoke-test-shaded
```

It makes obvious that when shaded is added to the reactor, it fails then smoke tests.
Working on it...
</comment><comment author="dadoonet" created="2015-08-11T12:07:33Z" id="129851681">So the difference between both execution plan is obviously the `projectArtifactMap`:

The wrong one when you run `org.elasticsearch.distribution.shaded:elasticsearch,:smoke-test-shaded`

```
[DEBUG]   (f) projectArtifactMap = 
* org.elasticsearch.distribution.shaded:elasticsearch=org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
* org.elasticsearch:elasticsearch=org.elasticsearch:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
* org.apache.lucene:lucene-backward-codecs=org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
* org.apache.lucene:lucene-analyzers-common=org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
* org.apache.lucene:lucene-queries=org.apache.lucene:lucene-queries:jar:5.2.1:compile
* org.apache.lucene:lucene-memory=org.apache.lucene:lucene-memory:jar:5.2.1:compile
* org.apache.lucene:lucene-highlighter=org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
* org.apache.lucene:lucene-queryparser=org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
* org.apache.lucene:lucene-sandbox=org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
* org.apache.lucene:lucene-suggest=org.apache.lucene:lucene-suggest:jar:5.2.1:compile
* org.apache.lucene:lucene-misc=org.apache.lucene:lucene-misc:jar:5.2.1:compile
* org.apache.lucene:lucene-join=org.apache.lucene:lucene-join:jar:5.2.1:compile
* org.apache.lucene:lucene-grouping=org.apache.lucene:lucene-grouping:jar:5.2.1:compile
* org.apache.lucene:lucene-spatial=org.apache.lucene:lucene-spatial:jar:5.2.1:compile
* com.spatial4j:spatial4j=com.spatial4j:spatial4j:jar:0.4.1:compile
* com.google.guava:guava=com.google.guava:guava:jar:18.0:compile
* com.carrotsearch:hppc=com.carrotsearch:hppc:jar:0.7.1:compile
* joda-time:joda-time=joda-time:joda-time:jar:2.8:compile
* org.joda:joda-convert=org.joda:joda-convert:jar:1.2:compile
* com.fasterxml.jackson.core:jackson-core=com.fasterxml.jackson.core:jackson-core:jar:2.5.3:compile
* com.fasterxml.jackson.dataformat:jackson-dataformat-smile=com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.5.3:compile
* com.fasterxml.jackson.dataformat:jackson-dataformat-yaml=com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.5.3:compile
* org.yaml:snakeyaml=org.yaml:snakeyaml:jar:1.12:compile
* com.fasterxml.jackson.dataformat:jackson-dataformat-cbor=com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.5.3:compile
* io.netty:netty=io.netty:netty:jar:3.10.3.Final:compile
* com.ning:compress-lzf=com.ning:compress-lzf:jar:1.0.2:compile
* com.tdunning:t-digest=com.tdunning:t-digest:jar:3.0:compile
* org.hdrhistogram:HdrHistogram=org.hdrhistogram:HdrHistogram:jar:2.1.6:compile
* org.apache.commons:commons-lang3=org.apache.commons:commons-lang3:jar:3.3.2:compile
* commons-cli:commons-cli=commons-cli:commons-cli:jar:1.3.1:compile
* com.twitter:jsr166e=com.twitter:jsr166e:jar:1.1.0:compile
* org.hamcrest:hamcrest-all=org.hamcrest:hamcrest-all:jar:1.3:test
* org.apache.lucene:lucene-test-framework=org.apache.lucene:lucene-test-framework:jar:5.2.1:test
* org.apache.lucene:lucene-codecs=org.apache.lucene:lucene-codecs:jar:5.2.1:test
* org.apache.lucene:lucene-core=org.apache.lucene:lucene-core:jar:5.2.1:compile
* com.carrotsearch.randomizedtesting:randomizedtesting-runner=com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test
* junit:junit=junit:junit:jar:4.11:test
* org.apache.ant:ant=org.apache.ant:ant:jar:1.8.2:test
```

The right one:

```
[DEBUG]   (f) projectArtifactMap = 
* org.elasticsearch.distribution.shaded:elasticsearch=org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
* org.apache.lucene:lucene-core=org.apache.lucene:lucene-core:jar:5.2.1:compile
* org.apache.lucene:lucene-backward-codecs=org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
* org.apache.lucene:lucene-analyzers-common=org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
* org.apache.lucene:lucene-queries=org.apache.lucene:lucene-queries:jar:5.2.1:compile
* org.apache.lucene:lucene-memory=org.apache.lucene:lucene-memory:jar:5.2.1:compile
* org.apache.lucene:lucene-highlighter=org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
* org.apache.lucene:lucene-queryparser=org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
* org.apache.lucene:lucene-sandbox=org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
* org.apache.lucene:lucene-suggest=org.apache.lucene:lucene-suggest:jar:5.2.1:compile
* org.apache.lucene:lucene-misc=org.apache.lucene:lucene-misc:jar:5.2.1:compile
* org.apache.lucene:lucene-join=org.apache.lucene:lucene-join:jar:5.2.1:compile
* org.apache.lucene:lucene-grouping=org.apache.lucene:lucene-grouping:jar:5.2.1:compile
* org.apache.lucene:lucene-spatial=org.apache.lucene:lucene-spatial:jar:5.2.1:compile
* com.spatial4j:spatial4j=com.spatial4j:spatial4j:jar:0.4.1:compile
* org.hamcrest:hamcrest-all=org.hamcrest:hamcrest-all:jar:1.3:test
* org.apache.lucene:lucene-test-framework=org.apache.lucene:lucene-test-framework:jar:5.2.1:test
* org.apache.lucene:lucene-codecs=org.apache.lucene:lucene-codecs:jar:5.2.1:test
* com.carrotsearch.randomizedtesting:randomizedtesting-runner=com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test
* junit:junit=junit:junit:jar:4.11:test
* org.apache.ant:ant=org.apache.ant:ant:jar:1.8.2:test
```

Trying to find a fix now...
</comment><comment author="dadoonet" created="2015-08-11T12:15:14Z" id="129853954">Adding here some notes on how to quickly debug that (might help for the future).

Just run:

``` sh
mvn dependency:tree -pl :smoke-test-shaded
```

```
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ smoke-test-shaded ---
[INFO] org.elasticsearch.qa:smoke-test-shaded:jar:2.0.0-beta1-SNAPSHOT
[INFO] +- org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
[INFO] |  +- org.apache.lucene:lucene-core:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-queries:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-memory:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-misc:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-join:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-grouping:jar:5.2.1:compile
[INFO] |  +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile
[INFO] |  \- com.spatial4j:spatial4j:jar:0.4.1:compile
[INFO] +- org.hamcrest:hamcrest-all:jar:1.3:test
[INFO] \- org.apache.lucene:lucene-test-framework:jar:5.2.1:test
[INFO]    +- org.apache.lucene:lucene-codecs:jar:5.2.1:test
[INFO]    +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test
[INFO]    +- junit:junit:jar:4.11:test
[INFO]    \- org.apache.ant:ant:jar:1.8.2:test
```

and 

``` sh
mvn dependency:tree -pl org.elasticsearch.distribution.shaded:elasticsearch,:smoke-test-shaded
```

```
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ smoke-test-shaded ---
[INFO] org.elasticsearch.qa:smoke-test-shaded:jar:2.0.0-beta1-SNAPSHOT
[INFO] +- org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
[INFO] |  \- org.elasticsearch:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile
[INFO] |     +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-queries:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-memory:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
[INFO] |     |  \- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile
[INFO] |     |  \- org.apache.lucene:lucene-misc:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-join:jar:5.2.1:compile
[INFO] |     |  \- org.apache.lucene:lucene-grouping:jar:5.2.1:compile
[INFO] |     +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile
[INFO] |     |  \- com.spatial4j:spatial4j:jar:0.4.1:compile
[INFO] |     +- com.google.guava:guava:jar:18.0:compile
[INFO] |     +- com.carrotsearch:hppc:jar:0.7.1:compile
[INFO] |     +- joda-time:joda-time:jar:2.8:compile
[INFO] |     +- org.joda:joda-convert:jar:1.2:compile
[INFO] |     +- com.fasterxml.jackson.core:jackson-core:jar:2.5.3:compile
[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.5.3:compile
[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.5.3:compile
[INFO] |     |  \- org.yaml:snakeyaml:jar:1.12:compile
[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.5.3:compile
[INFO] |     +- io.netty:netty:jar:3.10.3.Final:compile
[INFO] |     +- com.ning:compress-lzf:jar:1.0.2:compile
[INFO] |     +- com.tdunning:t-digest:jar:3.0:compile
[INFO] |     +- org.hdrhistogram:HdrHistogram:jar:2.1.6:compile
[INFO] |     +- org.apache.commons:commons-lang3:jar:3.3.2:compile
[INFO] |     +- commons-cli:commons-cli:jar:1.3.1:compile
[INFO] |     \- com.twitter:jsr166e:jar:1.1.0:compile
[INFO] +- org.hamcrest:hamcrest-all:jar:1.3:test
[INFO] \- org.apache.lucene:lucene-test-framework:jar:5.2.1:test
[INFO]    +- org.apache.lucene:lucene-codecs:jar:5.2.1:test
[INFO]    +- org.apache.lucene:lucene-core:jar:5.2.1:compile
[INFO]    +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test
[INFO]    +- junit:junit:jar:4.11:test
[INFO]    \- org.apache.ant:ant:jar:1.8.2:test
```
</comment><comment author="dadoonet" created="2015-08-13T16:52:43Z" id="130758591">Reopening as the change has been reverted
</comment><comment author="dadoonet" created="2015-09-08T19:28:43Z" id="138676850">We removed the shaded module so it's not an issue anymore... :p 

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index "creation_date" not accurate when created with settings from another index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12790</link><project id="" key="" /><description>When creating a new index using settings from an old index, the "creation_date" of new index is same as that of old index. Steps - 
1. Get settings of existing index using GetIndexRequestBuilder()
2. Create new index using CreateIndexRequestBuilder() by doing - 
a. copying the mappings getIndexResponse.getMappings()
b. copying the settings getIndexResponse.getSettings()
c. copying headers getIndexResponse.getHeaders()

Now the new index has same "creation_date" timestamp as old index. Version - 1.4
</description><key id="100196499">12790</key><summary>Index "creation_date" not accurate when created with settings from another index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajeis</reporter><labels><label>:Settings</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha1</label></labels><created>2015-08-11T00:19:31Z</created><updated>2016-01-19T11:27:32Z</updated><resolved>2016-01-19T11:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-11T13:40:23Z" id="129876821">Hi @ajeis 

Thanks for reporting.  This is indeed a bug.  Confirmed on master with the REST api.  Also `version.created` should not be settable either:

```
PUT x
{
  "settings": {
    "index": {
      "creation_date": "1439300111000",
      "version": {
        "created": "2000123"
      }
    }
  }
}

GET x/_settings
```

Returns:

```
{
   "x": {
      "settings": {
         "index": {
            "creation_date": "1439300111000",
            "uuid": "plte5zsHRwSTYzolfP537w",
            "number_of_replicas": "1",
            "number_of_shards": "5",
            "version": {
               "created": "2000123"
            }
         }
      }
   }
}
```
</comment><comment author="rjernst" created="2015-08-11T17:49:54Z" id="129989979">`version.created` is set in tests (to check backcompat behavior) _a lot_ (at least within mapping tests).
</comment><comment author="ajeis" created="2015-08-13T06:51:24Z" id="130557581">Where would the fix for this be - IndexSettingsModule, InternalIndicesService, IndexSettingsService or somewhere else? I am still fairly new to the codebase.
</comment><comment author="HarishAtGitHub" created="2015-08-13T07:50:16Z" id="130567469">Hi , I already started working on this issue. I am also a beginner (let's join hands and work together)..
I think I spotted the issue ... 
I thinks it's [here](https://github.com/elastic/elasticsearch/blob/2.0/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java#L331-L339) ...

Investigating why they are fetching it from settings
not sure if it was done intentionally for some purpose like migration where they need to maintain such information. I am just guessing ... Elasticsearch developers or experts can throw more light on this ...
I am almost there with the fix ...

``` java
if (indexSettingsBuilder.get(SETTING_VERSION_CREATED) == null) {
                        DiscoveryNodes nodes = currentState.nodes();
                        final Version createdVersion = Version.smallest(version, nodes.smallestNonClientNodeVersion());
                        indexSettingsBuilder.put(SETTING_VERSION_CREATED, createdVersion);
                    }

                    if (indexSettingsBuilder.get(SETTING_CREATION_DATE) == null) {
                        indexSettingsBuilder.put(SETTING_CREATION_DATE, new DateTime(DateTimeZone.UTC).getMillis());
                    }
```
</comment><comment author="HarishAtGitHub" created="2015-08-13T09:10:01Z" id="130586740"> I fixed it and it started working ..
input settings 

``` javascript
{
     'settings':  
          {  'index': 
                       {'number_of_replicas': '1', 
                         'version': {'created': '2000010'}, 
                         'creation_date': '1439384094544'
                         'uuid': 'ayzHgwB3Sgey-Pk_Okgwyg',
                         'number_of_shards': '5', 
                       }
          }
}
```

new index settings 

``` javascript
{
    'settings': 
          {'index': 
                     {'number_of_replicas': '1', 
                      'version': {'created': '2000001'}, 
                      'creation_date': '1439456346424', 
                      'uuid': 'fddpIVxNTTKPzqDakLAL4A', 
                      'number_of_shards': '5'
                     }
         }
}
```

But the ramifications are something to be worried about ..... Now I understand what @rjernst
is talking about in https://github.com/elastic/elasticsearch/issues/12790#issuecomment-129989979
The fix caused 

&gt; 474 suites, 2648 tests, &lt;b&gt; 43 errors, 22 failures &lt;/b&gt;

(...tears in my eyes ...)
</comment><comment author="rjernst" created="2015-08-13T09:30:01Z" id="130590738">Well it is not just for tests, that setting is inserted so we know when the index was created for things like backcompat checks, and also feature behavior. For example, when the `_field_names` was added back in 1.3, that setting helped determine for which indexes the new field should be added (those created on or after 1.3).
</comment><comment author="HarishAtGitHub" created="2015-08-13T10:44:08Z" id="130609798">This version.created seems like a bit dangerous entity to play with .
I just was playing with the version number and unfortunately I gave version. created as 1 ..
it accepted without any problem.. but then when I stopped it and restarted ... &lt;b&gt;it could not start&lt;/b&gt; . It was throwing [errors](https://gist.github.com/HarishAtGitHub/6072670467d7d768ebf3) that I "The index [demo6] was created before v0.90.0 and wasn't upgraded. This index should be open using a version before 2.0.0-beta1-SNAPSHOT and upgraded using the upgrade API." ...
I cleared the data dir then to fix ..

Hmm.. so dangerous ....
</comment><comment author="HarishAtGitHub" created="2015-08-13T12:03:33Z" id="130643578">Pull request for date replication : https://github.com/elastic/elasticsearch/pull/12854
</comment><comment author="HarishAtGitHub" created="2015-08-13T12:24:14Z" id="130648390">Ok .. what is the final take on version.created ?

apart from that: Is my comment https://github.com/elastic/elasticsearch/issues/12790#issuecomment-130609798 a valid one ?
Is it good to allow such a dangerous property out .
Why I am asking this is .. anything that can easily Fool/Trick/Ruin the system is dangerous and should be avoided ? So by that logic the comment seems valid ...

or can it be ignored as index creation is mostly carried out by admins ? (I saw most of the index creation,deletion api's inside admin) ? and you have left it knowingly based on the belief that admins are supposed to perform such an operation with discretion ?
</comment><comment author="rjernst" created="2015-08-14T23:06:53Z" id="131264860">@HarishAtGitHub `version.created` is an internal setting. It is not meant to be set by a user (and thus not documented). In an ideal world, there would be some separate section of the settings (probably outside the `settings` key) which would be used for these internal settings. However, I think this is currently difficult to change. In the meantime, the answer may be something like a "clone" index metadata action? I don't know how this would look, but it could ignore the known internal settings. I'm curious to here what @jpountz @clintongormley and others think about either of these ideas.
</comment><comment author="clintongormley" created="2015-08-15T08:56:03Z" id="131318073">I'd definitely like to see a list of "banned" settings, things that can only be set internally.
</comment><comment author="HarishAtGitHub" created="2015-08-16T09:02:50Z" id="131505050">Ok ... seems like a critical thing ...
so why should the creation_date replication patch wait for this ?
can we review that patch and pull if it deserves a pull ?
and track the version.created separately ? 
</comment><comment author="HarishAtGitHub" created="2015-08-17T11:36:34Z" id="131791072">FYI: I just started to work on this on "version.created" part . I will update my progress in a day ....
what is the timeline for this issue completion(as I see 2.0 tag in this thats y asking this Q) ? 
Is it ok for a person(me) who has has a week experience in code base do this ?

If you want this to be done by experts(for sooner completion for 2.0 release) I am fine , as I don't want to be a blocker . No worries ...
I am fine with any decision u take ... @rjernst , @clintongormley  ???
</comment><comment author="HarishAtGitHub" created="2015-08-17T12:23:38Z" id="131798572">May be input creation_date feature information should also be removed from the docs [here](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/indices-create-index.html#_creation_date)
</comment><comment author="HarishAtGitHub" created="2015-08-18T07:47:45Z" id="132108874">Hi,
    made a commit to the same PR for handling version.created problem also: https://github.com/elastic/elasticsearch/pull/12854

Please give feedback and I can refactor the patch.

Note about Test case results:

---

I built and all tests PASSED except the "Smoke test shaded jar"
see build log here: https://gist.github.com/HarishAtGitHub/aa8539a5c56cb4ad48b9
. and I hope this has nothing to do with my code(as all related tests passed).
and suspecting some other problem I resumed the build and I landed in https://gist.github.com/HarishAtGitHub/fcc519bd9863ab8a95af

which is reported in https://github.com/elastic/elasticsearch/issues/12791 .
</comment><comment author="s1monw" created="2015-10-08T18:23:39Z" id="146644941">we decided to move this out to 2.1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a warning that memory-based indices are going to be deprecated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12789</link><project id="" key="" /><description>Enhancement for documentation https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-store.html#store-memory
</description><key id="100195030">12789</key><summary>Add a warning that memory-based indices are going to be deprecated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">gmoskovicz</reporter><labels><label>docs</label><label>enhancement</label></labels><created>2015-08-11T00:08:40Z</created><updated>2016-01-26T18:39:01Z</updated><resolved>2016-01-26T18:36:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T18:36:22Z" id="175163710">Closing - already removed
</comment><comment author="gmoskovicz" created="2016-01-26T18:39:01Z" id="175165015">Thanks @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NettyTransportMultiPortIntegrationIT failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12788</link><project id="" key="" /><description>This seems to happen often in jenkins, but I have not reproduced on my local box. The logs seem to indicate the test's forcing the transport to netty does not always take.

http://build-us-00.elastic.co/job/es_core_master_metal/10930/console
</description><key id="100193487">12788</key><summary>NettyTransportMultiPortIntegrationIT failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-08-10T23:54:31Z</created><updated>2015-08-11T03:58:19Z</updated><resolved>2015-08-11T03:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>QA: Create a test plugin with configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12787</link><project id="" key="" /><description>In order to test the way plugins with configuration are installed and removed
we need a plugin with configuration in the repository. The simplest way to
get one is to make an "example" plugin.

In the process of making this example it became aparent that cat actions were
difficult to create outside of the org.elasticsearch.rest.action.cat package
because key methods in AbstractCatAction were package private. This makes them
protected and uses them to create the example configured plugin.

Relates to #12717 but is only one step of many to close it.
</description><key id="100177852">12787</key><summary>QA: Create a test plugin with configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label></labels><created>2015-08-10T21:53:48Z</created><updated>2015-08-12T14:36:35Z</updated><resolved>2015-08-12T14:36:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-10T21:55:28Z" id="129625121">This is step 0 in removing shield from the integration tests - it creates a shield lookalike that we can use to test the configuration installation and removal.
</comment><comment author="rjernst" created="2015-08-10T22:13:58Z" id="129631301">@nik9000 Can you merge this "configured-plugin" into the "jvm-example" plugin I added in #12744? That is essentially a blank slate right now, and has a similar intended purpose to what is here.
</comment><comment author="nik9000" created="2015-08-10T22:14:56Z" id="129631472">&gt; @nik9000 Can you merge this "configured-plugin" into the "jvm-example" plugin I added in #12744? That is essentially a blank slate right now, and has a similar intended purpose to what is here.

Sure - I guess I should have been reviewing that pull request....
</comment><comment author="nik9000" created="2015-08-11T14:38:10Z" id="129909814">@rjernst so merged.
</comment><comment author="rjernst" created="2015-08-11T17:32:19Z" id="129980881">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cloud-gce] Move integration tests to unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12786</link><project id="" key="" /><description>Basically this PR:
- Update GCE API to latest version (1.71)
- Replace IT by unit tests
- Use Google Transport Mock to simulate JSON Response which helps to **really** test the implementation classes. That was not the case previously.
- Remove GceSimpleITest as we now have RestIT (same goal for boths)

Would love to get feedback from reviewers. Thanks!
</description><key id="100176206">12786</key><summary>[cloud-gce] Move integration tests to unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>blocker</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T21:45:48Z</created><updated>2015-08-12T16:12:38Z</updated><resolved>2015-08-12T16:05:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T21:59:31Z" id="129627239">Tests are failing with:

```
Started J0 PID(13396@MacBook-Air-de-David.local).
Suite: org.elasticsearch.discovery.gce.GceDiscoveryTest
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=31458D3312839D7E -Dtests.class=org.elasticsearch.discovery.gce.GceDiscoveryTest -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fr_FR -Dtests.timezone=Europe/Paris
  2&gt; NOTE: test params are: codec=Asserting(Lucene50): {}, docValues:{}, sim=RandomSimilarityProvider(queryNorm=false,coord=crazy): {}, locale=vi_VN, timezone=Australia/Sydney
  2&gt; NOTE: Mac OS X 10.10.4 x86_64/Oracle Corporation 1.7.0_60 (64-bit)/cpus=4,threads=1,free=463145336,total=515375104
  2&gt; NOTE: All tests run in this JVM: [GceDiscoveryTest]
ERROR   0.00s | GceDiscoveryTest (suite) &lt;&lt;&lt;
   &gt; Throwable #1: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "accessClassInPackage.sun.misc")
   &gt;    at __randomizedtesting.SeedInfo.seed([31458D3312839D7E]:0)
   &gt;    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)
   &gt;    at java.security.AccessController.checkPermission(AccessController.java:559)
   &gt;    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
   &gt;    at java.lang.SecurityManager.checkPackageAccess(SecurityManager.java:1525)
   &gt;    at java.lang.Class.checkPackageAccess(Class.java:2304)
   &gt;    at java.lang.Class.checkMemberAccess(Class.java:2284)
   &gt;    at java.lang.Class.getDeclaredFields(Class.java:1805)
   &gt;    at java.lang.Thread.run(Thread.java:745)
Completed [1/2] in 1.78s, 9 tests, 1 error &lt;&lt;&lt; FAILURES!
```

Not sure what I should look at from this? Any idea @rjernst @rmuir ?
</comment><comment author="dadoonet" created="2015-08-11T09:12:47Z" id="129784373">Thanks to @rmuir comment, I found my issue (and learned from that BTW) and fixed it. It's now running fine. Also fixed comments left in the first review.

Let me know!

I consider this PR as a first try on GCE. I'd like to do next the same cleanup on azure and aws if possible.
</comment><comment author="dadoonet" created="2015-08-12T08:19:29Z" id="130213897">I marked it as a blocker as we have a `AwaitsFix` in it.
</comment><comment author="s1monw" created="2015-08-12T08:28:09Z" id="130215214">left some comments - this looks awesome
</comment><comment author="dadoonet" created="2015-08-12T09:32:20Z" id="130236237">@s1monw Updated with your comments.
</comment><comment author="s1monw" created="2015-08-12T15:26:08Z" id="130342031">left one comment LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix rpm -e removing /etc/elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12785</link><project id="" key="" /><description>When we fixed rpm creating the /etc/elasticsearch/scripts directory we
broke removing the rpm - it lef the /etc/elasticsearch directory behind.
This fixes that.
</description><key id="100173969">12785</key><summary>Fix rpm -e removing /etc/elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T21:34:19Z</created><updated>2015-08-11T14:19:40Z</updated><resolved>2015-08-11T12:08:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-10T21:34:24Z" id="129616549">Ping @spinscale 
</comment><comment author="spinscale" created="2015-08-11T11:18:04Z" id="129841537">ok, tested with a file lying around in `/etc/elasticsearch/scripts` - which means, the directory remains on deletion, removed the file after and reinstalled/removed the RPM - the directory was gone.

tested on centos-7

LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests should use pkg private statics on modules to plugin mock implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12784</link><project id="" key="" /><description>As part of #12744, some tests were changed to plug in mock implementations of classes with regular ES plugins. We should continue this work until all current setting-based-string-classname-loading is removed.
</description><key id="100170565">12784</key><summary>Tests should use pkg private statics on modules to plugin mock implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>test</label></labels><created>2015-08-10T21:18:40Z</created><updated>2015-08-13T21:36:36Z</updated><resolved>2015-08-13T21:36:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove SpawnModules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12783</link><project id="" key="" /><description>This is a spinoff from #12744. SpawnModules is a way to have modules produce sub modules. However, one way they have been used in the past is broken since we now have plugin classloader isolation. Some extension points used to allow plugging in an entire module "implementation". This is far too crazy for us to maintain in ES. It also leads to having lots of tiny modules that bind just a handful of classes.

We should remove this interface, and flatten the modules we have. All extension points should be class implementation based, by registering custom implementations with the module responsible for binding a given class. This has a bunch of benefits, like:
- Making it easier to find where to add new classes to bind
- Making it easier to document and write plugins since there will be less modules to possibly register custom extensions with
- Reducing spaghetti dependencies because it is clear when we have _tons_ of bound classes (a lot of which can probably be removed when we can clearly see what they are instead of being spread across tons of modules)
</description><key id="100168705">12783</key><summary>Remove SpawnModules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>:Plugins</label><label>blocker</label><label>breaking</label><label>v2.0.0-beta2</label></labels><created>2015-08-10T21:07:18Z</created><updated>2015-09-14T17:17:15Z</updated><resolved>2015-08-22T22:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-08-10T21:15:48Z" id="129608479">+1
</comment><comment author="s1monw" created="2015-08-10T21:18:46Z" id="129609600">++
</comment><comment author="uboness" created="2015-08-10T22:59:30Z" id="129640879">personally I find having a hierarchy of modules helpful to keep things clean, more modular and more testable:
- a module is a unit responsible for X
- the module itself is a sub-system within a system. Breaking the system to modules is good, and I believe the same holds for modules themselves. Specially when it comes to complex modules.
- one can argue that this can also be fixed by collapsing a hierarchy to a single module - I believe this will lead to a spaghetti code on the module level. Instead, split responsibilities to higher/lower level functions and work your way down/up.
- applied properly, sub-modules actually reduce spaghetti dependencies, simply because higher modules are not (and should not) be aware of lower ones. When flattening everything, you risk modules in the system knowing/depending on other modules that they should not even know about. A hierarchy mandates strict narrow dependency rules:

```
A
|-- B
|    |-- C
|    |-- D
|-- E
|    |-- F
|    |-- G
```

A should only know about B &amp; E
B and E only know about each other 
C and D only know about each other 
F and G only know about each other 

each module knows the minimum it needs to know in order to function. With a flatten model the dependency rules are quite weak:

```
A
B
C
D
E
F
G
```

everyone knows about everyone... no clear dependency rules... everything is allowed.

One thing I like about the hierarchy model is that if you follow the rules it forces you to think. That is, if you see that for some reason dependencies leak out of their boundaries - you probably did something wrong.

We've had plenty of discussion on this topic already in #12744, so not going to repeat everything again. If most agree to remove this.... it's fine... as long as the end result doesn't turn out to be worse than where we started.
</comment><comment author="s1monw" created="2015-08-11T07:46:08Z" id="129743779">@uboness I totally understand where you are coming from. The last couple of months elasticsearch has undergone some massive changes since we are basically not able to cope with the amount of classes and abstractions, number of packages etc. It's largely grown over the years and taking several steps back here is a reasonable thing to do. So what does this mean, we mainly folded classes into larger and more general classes, removed packages containing a single class or like a handful, removed wiring services via guice and use good old ctors.
These refactorings are similar to this, in theory the abstractions where ok and the right thing todo if you look at smaller and meant to be extendable projects. In elasticsearch the complexity of the system alone is hard to cope with, we should try to make our interfaces simple. Lemme explain using an example:

We have a class `SearchModule` it spawns a bunch of modules:

``` Java
    @Override
    public Iterable&lt;? extends Module&gt; spawnModules() {
        return ImmutableList.of(
                new SearchServiceModule(settings),
                new TransportSearchModule(),
                new HighlightModule(),
                new SuggestModule(),
                new FunctionScoreModule(),
                new AggregationModule(),
                new FetchSubPhaseModule());
    }
```

almost all of them follow a simple pattern:

``` Java
public class FooModule extends AbstractModule {

  public void registerFoo(Foo foo) { ... }

  @Override
  protected void configure() { ... }
}
```

I think stuff would be way simpler if we would merge them all into `SearchModule` like this:

``` Java
public class  SearchModule extends AbstractModule  {

  public void registerHighlighter(Class&lt;? extends Highlighter&gt; clazz) { ... } 
  public void registerSuggester(Class&lt;? extends Suggester&gt; suggester) { ... }
  public void registerFetchSubPhase(Class&lt;? extends FetchSubPhase&gt; subPhase)  { ... }
  public void registerParser(Class&lt;? extends ScoreFunctionParser&gt; parser) { .. }
  public void registerStream(SignificanceHeuristicStreams.Stream stream) { ... } 
  public void registerStream(MovAvgModelStreams.Stream stream) { ... }

}
```

This is simple to use, clear and reduces bloat. Abstractions are added via the individual classes you can register that is a clear interface. The fact that `SearchModule` spawns `TransportSearchModule` to spawn `TransportAggregationModule` to spawn `[TransportSignificantTermsHeuristicModule, TransportMovAvgModelModule]` is just confusing and not needed. There is no gain here only complexity. I am 100% convinced we should discourage and prevent this overdesign pattern.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set prompts to be more consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12782</link><project id="" key="" /><description>Also a small amount of cleanup in the way we create VMs - just a bit less
repetition.

Prompts are always of the form "box:cwd$ ". Even for root. Which is ok because
you don't have to be that careful with root because these are VMs that you
can destroy and recreate quickly.
</description><key id="100165958">12782</key><summary>Set prompts to be more consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.1.0</label></labels><created>2015-08-10T20:51:40Z</created><updated>2015-08-21T18:43:17Z</updated><resolved>2015-08-17T18:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-14T20:35:12Z" id="131233079">Ping @tlrx or @dakrone for review - just a tiny bit of vagrant cleanup.
</comment><comment author="dakrone" created="2015-08-14T21:29:56Z" id="131242866">@nik9000 you're just trying to get our Ruby percentage up over 0.1% so we can be truly polyglot right? Let me try to remember all the ruby I used to do for reviewing this... :)
</comment><comment author="nik9000" created="2015-08-14T21:32:55Z" id="131243302">&gt; @nik9000 you're just trying to get our Ruby percentage up over 0.1% so we can be truly polyglot right? Let me try to remember all the ruby I used to do for reviewing this... :)

Thanks! Its mostly shell in heredocs in ruby though. Which probably confuses github.
</comment><comment author="tlrx" created="2015-08-17T08:26:12Z" id="131728746">LGTM
</comment><comment author="dakrone" created="2015-08-17T18:29:08Z" id="131918154">Prompt stuff looks good to me too, however, I do get Vagrant failures for RPM removal on centos-7 - https://gist.github.com/dakrone/b499a9de41e76008a48b
</comment><comment author="nik9000" created="2015-08-17T18:32:45Z" id="131919245">&gt; Prompt stuff looks good to me too, however, I do get Vagrant failures for RPM removal on centos-7 - https://gist.github.com/dakrone/b499a9de41e76008a48b

I'm reasonably sure that is fixed in master and we'd have to rebase to get that passing on this pr. Or I could just merge it and master should just pass. Rather, master passes for me right now and should continue to do so with this merged in.
</comment><comment author="dakrone" created="2015-08-17T18:33:14Z" id="131919347">+1, that's fine, this is only to change prompts
</comment><comment author="nik9000" created="2015-08-17T18:33:42Z" id="131919448">Merged.
</comment><comment author="colings86" created="2015-08-21T09:32:32Z" id="133349833">@nik9000 this doesn't seem to have been backported to the 2.0 branch. Should it be backported?
</comment><comment author="nik9000" created="2015-08-21T18:42:32Z" id="133527502">Ahk. No it was supposed to be 2.1 only. I have no idea why I labeled it that way. There isn't really any advantage to pulling this into 2.0. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back ability to register custom allocators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12781</link><project id="" key="" /><description>This was temporarily removed in #12744. We need to have an extension point to register custom allocators (before they were loaded from the classpath).
</description><key id="100153930">12781</key><summary>Add back ability to register custom allocators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>regression</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T19:53:57Z</created><updated>2015-08-12T09:57:26Z</updated><resolved>2015-08-12T04:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Return `408 REQUEST_TIMEOUT` if `_cluster/health` times out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12780</link><project id="" key="" /><description>Today we return `200 OK` which is misleading since we really didn't
produce a valid response / didn't wait long enough.
</description><key id="100153012">12780</key><summary>Return `408 REQUEST_TIMEOUT` if `_cluster/health` times out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T19:49:49Z</created><updated>2015-08-12T16:18:09Z</updated><resolved>2015-08-12T16:18:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-10T19:49:59Z" id="129582480">@clintongormley can you take a look?
</comment><comment author="clintongormley" created="2015-08-11T11:50:07Z" id="129848553">@s1monw LGTM.  Here's a REST test which should be added as `rest-api-spec/src/main/resources/rest-api-spec/test/cluster.health/20_request_timeout.yaml`:

```
---
"cluster health request timeout":
  - do:
      catch: request_timeout
      cluster.health:
        wait_for_nodes: 10
        timeout: 1s

  - is_true:   cluster_name
  - is_true:   timed_out
  - gte:       { number_of_nodes:         1 }
  - gte:       { number_of_data_nodes:    1 }
  - match:     { active_primary_shards:   0 }
  - match:     { active_shards:           0 }
  - match:     { relocating_shards:       0 }
  - match:     { initializing_shards:     0 }
  - match:     { unassigned_shards:       0 }
  - gte:       { number_of_pending_tasks: 0 }
```
</comment><comment author="s1monw" created="2015-08-11T15:54:52Z" id="129942876">I added your tests...
</comment><comment author="martijnvg" created="2015-08-12T08:38:36Z" id="130217763">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix field type compatiblity check to work when only one previous type exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12779</link><project id="" key="" /><description>This was a straight up bug found in #12753. If only one type existed,
the compatibility check for a new type was not strict, so changes to
an updateable setting like search_analyzer got through (but only
partially). This change fixes the check and adds tests (which were
previously a TODO). 

This also fixes a bug in dynamic field creation which woudln't copy
fielddata settings when duplicating a pre-existing field with the
same name.

closes #12753
</description><key id="100151649">12779</key><summary>Fix field type compatiblity check to work when only one previous type exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T19:43:14Z</created><updated>2015-08-11T17:26:09Z</updated><resolved>2015-08-11T17:24:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-11T14:13:08Z" id="129896735">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>designate master node through cluster api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12778</link><project id="" key="" /><description>Feature request:
I would like the ability to be able to manually designate a master node for a cluster.

Use case:
I want to have a dedicated master for performance reasons, but I don't want to keep 2 additional dedicated masters as wasted resources waiting for a disaster to happen.
The solution I would like to use is to have a single dedicated master, and make the data notes master-eligible so that one of them can take over if the master fails.  Then, when the master rejoins the cluster, I need a way to make it the master again.  The only way I can think of doing this now is setting each of the data nodes to be non-master-eligible, and restarting the data node that became master when the master node died.  This is suboptimal.
</description><key id="100133141">12778</key><summary>designate master node through cluster api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">charlesmims</reporter><labels /><created>2015-08-10T18:09:54Z</created><updated>2015-12-22T21:33:48Z</updated><resolved>2015-08-11T12:36:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-11T12:36:25Z" id="129858168">Hi @charlesmims 

I think this change would be a mistake.  Master nodes, like data nodes, should all be of similar quality, otherwise if one "better" master node fails, then another "worse" master node tries to take over, you could end up just crashing your cluster regardless.

Dedicated master nodes make sense once your cluster is big enough.  They can use cheaper, smaller hardware than the data nodes.  Alternatively, you can also run two instances of elasticsearch on each box: one as dedicated data, and one as dedicated master, with a much smaller heap.
</comment><comment author="charlesmims" created="2015-09-09T19:45:40Z" id="139025550">I disagree with this assessment.  For situations where data nodes are huge beefy boxes, adding the duty of temporarily being master in addition to being a data node shouldn't cause instability unless the data node is already running close to its limits.  At any rate there are countless design decisions like this that elasticsearch admins need to account for already.  I'm probably not the only admin who wants a way to have a dedicated master and be able to survive failure of that master without dedicated backup masters and without having to restart a node to get the cluster back to the original master.

How is running two instances on one machine, one as a dedicated master and one as dedicated data preferable to allowing a data node to temporarily take over as master in the event that a master node goes down?  If you're suggesting that it should be done this way because you would implement it with enough memory set aside for a master instance and reduced memory for the data instance, this isn't any better than having dedicated backup master machines.

The guys in elasticsearch irc chat seemed to think this would be a good feature, can we put it to a vote or something rather than just closing it?
</comment><comment author="charlesmims" created="2015-09-09T19:50:30Z" id="139026682">Another benefit of being able to specify a master is eliminating downtime while a new master is elected, when doing rolling restarts to upgrade, for example.
</comment><comment author="bleskes" created="2015-09-19T17:22:31Z" id="141690575">&gt; How is running two instances on one machine, one as a dedicated master and one as dedicated data preferable to allowing a data node to temporarily take over as master in the event that a master node goes down? 

While it's true that the master and data nodes share the risk of server failures, using two separate processes protects the master from any data/search related pressure (mostly memory and GC). We try to keep things as simple as possible because as you say, there is a lot an admin needs to worry about it. We feel this is a simpler and less error prone solution.

&gt; Another benefit of being able to specify a master is eliminating downtime while a new master is elected, when doing rolling restarts to upgrade, for example.

I a fully agree that a clean master step down API will be handy to have a planned master shutdown go without any service hick up. It's own are radar but at the moment, there are other things higher up on the list - for example, shorten the 3s master election so if a master goes down (planned or not) a new master will be more quickly elected.
</comment><comment author="charlesmims" created="2015-12-22T21:33:48Z" id="166735226">Another use-case for this feature:
Upgrading masters and rolling restarts could be less harrowing than shutting down the current master and hoping master election works.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change RPM package to include "noarch"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12777</link><project id="" key="" /><description>In master branch we are missing the .noarch part in the resulting rpm name.

$ find . -name *.rpm
./distribution/rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm
./distribution/rpm/target/rpm/elasticsearch/RPMS/noarch/elasticsearch-2.0.0-beta1_SNAPSHOT20150810154300.noarch.rpm
</description><key id="100106314">12777</key><summary>Change RPM package to include "noarch"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electrical</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-08-10T16:17:25Z</created><updated>2016-06-13T17:22:53Z</updated><resolved>2016-06-13T17:22:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T16:40:37Z" id="129521836">Do we agree that the final name in `target/releases` dir should be `elasticsearch-2.0.0-beta1_SNAPSHOT20150810154300.noarch.rpm`? 

It's what RPM maven plugin produces by default, which looks good to me but note the `underscore` in `_SNAPSHOTxxx`. I think I saw also that kind of substitution (using `~` IIRC) in `rpm` or `deb` packages. We should probably try to stick with all maven plugins defaults here?
</comment><comment author="jordansissel" created="2016-06-09T04:51:22Z" id="224799422">With some of the recent unified build work, I think we can close this. All projects are converging on a consistent artifact naming scheme. That scheme is, if I recall correctly, what ES has been using for some time -- `project-version.rpm` and `project_version.deb` for platform-independent projects.
</comment><comment author="clintongormley" created="2016-06-13T17:22:49Z" id="225649421">thanks @jordansissel 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `node.enable_custom_paths` after `path.shared_data` is added</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12776</link><project id="" key="" /><description>Follow-up to comments from https://github.com/elastic/elasticsearch/pull/12729
</description><key id="100103822">12776</key><summary>Remove `node.enable_custom_paths` after `path.shared_data` is added</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>deprecation</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T16:06:41Z</created><updated>2015-09-16T13:32:21Z</updated><resolved>2015-08-12T18:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use 'name' from plugin descriptor file to determine plugin name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12775</link><project id="" key="" /><description>At the moment, when installing from an url, a user provides a plugin name on the command line like:
- bin/plugin install [plugin-name] --url [url]

This can lead to confusion when picking an already existing name from another plugin, which might even overwrite plugins already installed with that name.

In order to remedy this, this PR introduces an additional mandatory `name` property to the plugin descriptor file which overwrites the name provided by the user after the plugin was loaded and the descriptor file is unpacked.

Relates to #12715
</description><key id="100095884">12775</key><summary>Use 'name' from plugin descriptor file to determine plugin name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Plugins</label><label>breaking</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T15:35:06Z</created><updated>2015-11-22T10:10:52Z</updated><resolved>2015-08-19T10:33:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-10T15:40:09Z" id="129499433">Some open questions I have with the original issue which are not addressed by this PR so far:
- What should happen with the `[plugin-name]` CLI argument. We can't remove it, since it is used and needed when no url is provided. As far as I understand, in this case the name is going to be resolved from the CLI argument depending on various rules.
- When the name is specified in the descriptor file, I think that also that name needs to be used to remove the plugin. Not sure how much this is in conflict with existing behaviour
</comment><comment author="dadoonet" created="2015-08-10T16:34:01Z" id="129519080">Some thoughts that come to mind. 

Now we know that if the user set a `url` like:

```
bin/plugin install foo --url http://link.to/myplugin.zip
```

And that `myplugin.zip` contains in its descriptor a `plugin.name` equal to `bar`, we will end up installing `bar` plugin.

So it does not make sense anymore to support this `--url` option IMO.

We should have:

``` sh
# download from elastic.co, maven central or github
bin/plugin install foo
# download from a URI (URL)
bin/plugin install http://link.to/foo.zip
# download from a URI (file)
bin/plugin install file:.target/releases/foo.zip
```

@spinscale @clintongormley WDYT?
</comment><comment author="rjernst" created="2015-08-10T18:29:36Z" id="129557932">Hrm, @dadoonet Can we be explicit instead of having to infer _what_ was passed as the argument to install? What you suggest would require even more heuristics. Instead, why don't we remove the fallback to trying as a url, change http:// to require using --url, and add a --file that requires a local path?
</comment><comment author="rjernst" created="2015-08-10T18:35:18Z" id="129559165">Also, when we download a plugin, if it was downloaded by name, we should verify the name matches?
</comment><comment author="clintongormley" created="2015-08-11T15:46:04Z" id="129937866">I agree with being explicit:

```
# download from elastic.co, maven central or github
bin/plugin install foo
# download from a URI (URL)
bin/plugin install --url http://link.to/foo.zip
# download from a URI (file)
bin/plugin install --file .target/releases/foo.zip
```
</comment><comment author="dadoonet" created="2015-08-11T16:04:37Z" id="129949053">@spinscale Does CLI allows that?

I think we might need to do something like:

``` sh
# download from elastic.co, maven central or github
bin/plugin install foo
# download from a URI (URL)
bin/plugin install http://link.to/foo.zip --url 
# download from a URI (file)
bin/plugin install .target/releases/foo.zip --file
```
</comment><comment author="spinscale" created="2015-08-11T16:06:30Z" id="129949736">the last two syntaxes are really confusing IMO, one can change to allow for that, but it doesnt feel nice
</comment><comment author="clintongormley" created="2015-08-11T16:18:50Z" id="129952815">If we can't easily support the version in https://github.com/elastic/elasticsearch/pull/12775#issuecomment-129937866 then let's use the version in https://github.com/elastic/elasticsearch/pull/12775#issuecomment-129519080

The heuristic would simply be: anything matching this regex is a URL `^\w+:`
</comment><comment author="rjernst" created="2015-08-11T17:19:17Z" id="129976957">Wasn't @spinscale talking about the odd syntax of adding options after the thing the option is pointing to? Surely having command line parameter that takes an argument is supported by the CliTool? We should simply not use heuristics; they are not needed, and only lead to confusing errors.
</comment><comment author="clintongormley" created="2015-08-11T17:22:13Z" id="129977814">@rjernst yes he was.  @dadoonet was not sure if the CLI parser we use would support that style of argument parsing.  I'm saying that, if it doesn't (and is not easy to fix) then we should rather use the simple heuristic than the confusing syntax.

But I agree, I can't see why the CLI parser wouldn't support it.
</comment><comment author="spinscale" created="2015-08-12T08:26:00Z" id="130214795">Looks good me to regarding support... One last thing though:

```
# downloads official plugin
bin/plugin install analysis-kuromoji 
# downloads github plugin
bin/plugin install lmenezes/elasticsearch-kopf
# URL
bin/plugin install http://link.to/foo.zip
bin/plugin install file://.target/releases/foo.zip
# local
bin/plugin install --file .target/releases/foo.zip
```

Do we really need a `--url` and `--file` switch? How about just providing an URL by default, which also works if you provide `file://`. As we try to remove leniency, rather than add it at the moment, we could still have the `--file` switch, which automatically prepends `file://` to the specified path to create an URI instead of trying to be smart and fall back to do this automatically if parsing as an URL fails.
</comment><comment author="cbuescher" created="2015-08-12T08:42:57Z" id="130219560">@spinscale Thanks for that suggestions, looking at the implementation details and given that `-u` currently also covers installation from file I'd also prefer not adding yet another switch. Will try this first without the `--file` switch.
</comment><comment author="rjernst" created="2015-08-12T22:06:07Z" id="130461489">@spinscale I like this approach. In this case we shoudln't provide `--file` at all (we didn't before right? we only had `--url`?).

So to confirm, the logic would be:

```
if (argument-parses-as-uri) {
  ...use URI
} else {
  ...use as name with download service
}
```
</comment><comment author="rjernst" created="2015-08-12T22:07:01Z" id="130461646">But that means we should remove `--url` too, otherwise we still would have the issue of the user passing a name with a url.
</comment><comment author="spinscale" created="2015-08-13T06:35:43Z" id="130555075">@rjernst depening on our impl, logic would be

```
if (argument-parses-as-uri) {
  ...use URI
} else {
   if (argument-parses-as-uri-with-file://-prefix) {
  } else { 
    ...use as name with download service
  }
}
```

and yes, we could totally drop the --url in that case and just have a single argument, the identifier, being a name, local path or URL
</comment><comment author="rjernst" created="2015-08-13T06:37:54Z" id="130555733">Why would the inner if be necessary? `file://` should parse as a uri right?
</comment><comment author="spinscale" created="2015-08-13T07:21:09Z" id="130563165">@rjernst sure, but you might want to use paths like `/my/path/to/my/plugin.zip` without specifying file:// in front
</comment><comment author="rjernst" created="2015-08-13T07:24:32Z" id="130563855">We should not do that. That is leniency. Using a local file is expert, let's just stick to a uri, and not have to deal with `is-this-thing-a-file-or-should-we-try-the-download-service`
</comment><comment author="rjernst" created="2015-08-13T07:25:44Z" id="130564043">It's also no different than what users already had to do, using --url. We can just remove --url, since we can always clearly tell if something is a uri.
</comment><comment author="spinscale" created="2015-08-13T07:26:06Z" id="130564091">valid argument, +1
</comment><comment author="cbuescher" created="2015-08-13T18:19:03Z" id="130787494">@rjernst @spinscale updated the PR with changes to the CLI as mentioned in https://github.com/elastic/elasticsearch/pull/12775#issuecomment-130214795, only without the `--file` option, also removing the `--url` option. I choose not to try to parse to an URI since also valid plugin names parse to URIs, only without a scheme, let me know if you think this is okay. Also, if only url/file is supllied, I seem to need a temporary name for the download, not sure what to do here. I mentioned this in a comment.
</comment><comment author="rjernst" created="2015-08-14T08:38:25Z" id="131026830">&gt; Also, if only url/file is supllied, I seem to need a temporary name for the download

How about using part of the URL in this case? Maybe the path?
</comment><comment author="cbuescher" created="2015-08-14T12:52:02Z" id="131096524">Rebased and added some randomness to the temporary plugin name that is used when url is provided but descriptor file hasn'r been retrieved yet. I tried getting around this but this would have meant larger rewrites of the plugin manager code and as far I understand @spinscale this will happen at a later point in time anyway.
</comment><comment author="dadoonet" created="2015-08-18T09:42:15Z" id="132145128">@cbuescher I think this file needs also to be updated with your change: https://github.com/elastic/elasticsearch/blob/master/docs/plugins/plugin-script.asciidoc#custom-url-or-file-system
</comment><comment author="cbuescher" created="2015-08-18T13:01:55Z" id="132199024">Updated the PR, rebased and added comments, changes in logging suggested by @nik9000 and changed docs.
</comment><comment author="dadoonet" created="2015-08-18T13:15:10Z" id="132201481">I left a small comment but the change looks good to me.
</comment><comment author="dadoonet" created="2015-08-18T13:38:01Z" id="132208919">@cbuescher I merged #12879 so you can now change pom.xml files to use maven artifactId instead of setting manually `elasticsearch.plugin.name`.
</comment><comment author="cbuescher" created="2015-08-18T17:04:49Z" id="132281172">@dadoonet thanks, I updated the poms using the artifactId instead of hardcoding the name, added some documentation and rebased. I'll sqash this again before merging with master.
</comment><comment author="dadoonet" created="2015-08-18T17:08:55Z" id="132282112">I left one small comment. 

I think we are really close now!
</comment><comment author="cbuescher" created="2015-08-18T17:47:54Z" id="132293714">@dadoonet thanks for the review, didn't know `artifactId` would be resolved in the submodules if placeholder is defined in the parent pom, but seems to work. I updated the PR accordingly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't run shaded tests as unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12774</link><project id="" key="" /><description>If we run the tests as a reactor build we reference the dependencies
before they are shaded. This causes problems since we verify that unshaded versions
of a transitive dependency is not present. This commit moves the verification tests
into the integration test that always runs with the shaded version of the jar.
</description><key id="100081131">12774</key><summary>Don't run shaded tests as unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T14:31:15Z</created><updated>2015-08-12T07:17:29Z</updated><resolved>2015-08-10T14:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T14:42:06Z" id="129477931">It makes sense to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add type safety for QueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12773</link><project id="" key="" /><description>QueryParser now explicitly declares the type of `QueryBuilder` that it creates. Properly applied to already refactored queries, not applicable to queries that are not refactored yet as their fromXContent returns QueryWrappingQueryBuilder but the return type of getBuilderPrototype is different.
</description><key id="100069455">12773</key><summary>add type safety for QueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-08-10T13:43:04Z</created><updated>2015-08-11T13:55:01Z</updated><resolved>2015-08-11T13:55:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-10T13:44:00Z" id="129459300">@cbuescher what do you think?
</comment><comment author="alexksikes" created="2015-08-10T15:22:25Z" id="129493068">LGTM
</comment><comment author="cbuescher" created="2015-08-11T13:37:42Z" id="129876063">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamic_templates not used when calling analyze API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12772</link><project id="" key="" /><description>When defining dynamic templates, it is somewhat irritating to notice that any request to the analyze API will only consider said dynamic mappings when a document with the specific field has already been posted.

Example:

```
{
"mappings": {
    "assets": {
      "dynamic_templates": [
        {
          "de": {
            "path_match": "localizations.de-*.*",
            "match_mapping_type": "string",
            "mapping": {
              "type": "string",
              "analyzer": "ger"
            }
          }
        }
      ]
    }
  }
}
```

```
GET http://localhost:9002/assets/_analyze?field=localizations.de-DE.text?text=Dies%20ist%20ein%20beispielhafter%20Text
```

will only use the correct analyzer when a document with the exact same field 'localizations.de-DE.text' had already been posted. For me, this behaviour caused some irritations. A note in the documentation would have helped a lot. 

Thanks!
</description><key id="100055569">12772</key><summary>dynamic_templates not used when calling analyze API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">Calardan</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-08-10T12:50:57Z</created><updated>2016-05-24T10:30:06Z</updated><resolved>2016-05-24T10:30:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-10T13:39:25Z" id="129458405">While I'm all for improving the documentation, cluttering it with too many notes actually makes it harder to read.  Honestly, i'm surprised that you thought it would work in the first place. If you saw something in the docs that implied it would work, I'd be happy to change that. You want to send a PR with a suggested edit?

btw, you can just specify `?analyzer=ger` in the analyze API and you'll see the effect it has.
</comment><comment author="Calardan" created="2015-08-10T13:46:52Z" id="129459830">I knew it works with `?analyzer=ger`. All I wanted to do is test my configuration if it works for the specified field.
I have to admit that afterwards it makes sense that any dynamic mapping would only work when it has actually been applied. But it was still very confusing before realizing this.
</comment><comment author="clintongormley" created="2016-05-24T10:30:06Z" id="221229466">I don't think there's anything left to do here. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Makes sure all POMs contain a description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12771</link><project id="" key="" /><description>Also adds an explicit description the RPM package so it doesn't inherit the description from the POM.

Closes #12550
</description><key id="100050516">12771</key><summary>Makes sure all POMs contain a description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T12:20:25Z</created><updated>2015-08-11T09:05:43Z</updated><resolved>2015-08-10T14:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-10T12:30:31Z" id="129427109">Debian package:

```
# dpkg --info deb/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.deb
...
Description: Open Source, Distributed, RESTful Search Engine
  Elasticsearch is a distributed RESTful search engine built for the cloud.
  .
  Features include:
  .
  + Distributed and Highly Available Search Engine.
    - Each index is fully sharded with a configurable number of shards.
    - Each shard can have one or more replicas.
    - Read / Search operations performed on either one of the replica shard.
  + Multi Tenant with Multi Types.
    - Support for more than one index.
    - Support for more than one type per index.
    - Index level configuration (number of shards, index storage, ...).
  + Various set of APIs
    - HTTP RESTful API
    - Native Java API.
    - All APIs perform automatic node operation rerouting.
  + Document oriented
    - No need for upfront schema definition.
    - Schema can be defined per type for customization of the indexing process.
  + Reliable, Asynchronous Write Behind for long term persistency.
  + (Near) Real Time Search.
  + Built on top of Lucene
    - Each shard is a fully functional Lucene index
    - All the power of Lucene easily exposed through simple
      configuration/plugins.
  + Per operation consistency
    - Single document level operations are atomic, consistent, isolated and
      durable.
  + Open Source under the Apache License, version 2 ("ALv2").
 Homepage: https://www.elastic.co/
```

and the RPM

```
# rpm -pqi rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm
Name        : elasticsearch                Relocations: /usr
Version     : 2.0.0                             Vendor: (none)
Release     : beta1_SNAPSHOT20150810122129   Build Date: Mon Aug 10 14:22:02 2015
Install Date: (not installed)            Build Host: orca.fritz.box
Group       : Application/Internet          Source RPM: elasticsearch-2.0.0-beta1_SNAPSHOT20150810122129.src.rpm
Size        : 32332940                         License: (c) 2009
Signature   : RSA/SHA1, Mon Aug 10 14:22:02 2015, Key ID c4afd48faa37d761
Packager    : Elasticsearch
Summary     : Elasticsearch RPM Distribution
Architecture: noarch
Description :
Open Source, Distributed, RESTful Search Engine Elasticsearch is a distributed RESTful search engine built for the cloud.
```

The RPM description is kind of repeating itself in the two sentences? 

minor thing: maybe unify the description between both packages, and have the RPM summary return s/Distribution/Package?

LGTM other than that
</comment><comment author="spinscale" created="2015-08-10T13:29:35Z" id="129453065">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] updated migrate guide with info for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12770</link><project id="" key="" /><description /><key id="100046277">12770</key><summary>[DOCS] updated migrate guide with info for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>docs</label><label>review</label></labels><created>2015-08-10T11:53:20Z</created><updated>2015-08-10T15:20:04Z</updated><resolved>2015-08-10T13:53:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-10T13:44:11Z" id="129459344">@cbuescher can you have a quick look please?
</comment><comment author="cbuescher" created="2015-08-10T13:47:06Z" id="129459868">Looks good, maybe add a short note about builders implementation of validate() and what should go there?
</comment><comment author="javanna" created="2015-08-10T13:50:02Z" id="129460472">good point will add that, thanks!
</comment><comment author="alexksikes" created="2015-08-10T15:20:04Z" id="129491927">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move vagrant activation to a parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12769</link><project id="" key="" /><description>Closes #12611
</description><key id="100033021">12769</key><summary>Move vagrant activation to a parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label></labels><created>2015-08-10T10:41:34Z</created><updated>2015-08-12T14:40:54Z</updated><resolved>2015-08-12T14:40:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-10T10:42:49Z" id="129404144">This moves the vagrant profile activation to a parameter like @dadoonet  and @rjernst asked for in #12646.
</comment><comment author="nik9000" created="2015-08-11T14:38:54Z" id="129910153">@dadoonet, renamed the parameter.
</comment><comment author="dadoonet" created="2015-08-11T17:36:56Z" id="129983288">It looks good to me.
</comment><comment author="nik9000" created="2015-08-12T14:40:54Z" id="130325144">Squashed and merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing and packaging changes for 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12768</link><project id="" key="" /><description>This is a meta issue listing the testing and packaging we should add before 2.0:
- [x] Extend integration testing to multi-node #12718 (Simon)
- [x] Install and test core plugins in vagrant tests #12717 (Nik)
- [x] Test command line tools with custom paths #12712 (Britta, PR #12954)
- [x] improve plugin integration tests #12654 (Clint)
- [x] Bats tests should use plugins built in the elasticsearch repository #12651 (Nik)
- [x] [cloud-gce] Move integration tests to unit tests #12786 (David)
- [x] try to improve integration tests setup/teardown #12063 (Britta, PR #12961)
- [x] Make sure all distribution modules have description in pom.xml #12550
- [x] Don't expand wildcards in command line options #12689 (depends on #12677)
- [x] elasticsearch script does not work with arguments that have spaces #12677 (PR #12710)
- [x] Add "name" to the plugin descriptor file #12715 (cbuescher, PR #12775)
- [x] Plugin manager should verify that it can write to all directories #12749 (AlexR, PR #12851)
- [x] Plugin manager should download only from https by default #12748 (AlexR, PR #12824)
- [x] Plugin manager should validate the SHA, where possible #12750 (Lee, PR #12888)
- [x] Plugin script fails when memory parameters are defined in environment file #12479 (AlexR, PR #12801)
- [x] Make sure elasticsearch.bat works with spaces #12848 (Martijn L, PR #12910)
</description><key id="100031523">12768</key><summary>Testing and packaging changes for 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>:Plugins</label><label>build</label><label>Meta</label><label>test</label><label>v2.0.0</label></labels><created>2015-08-10T10:34:44Z</created><updated>2015-10-07T16:20:08Z</updated><resolved>2015-10-07T16:20:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-10T21:02:40Z" id="139377851">Finally closed #12717 today! Hurray!
</comment><comment author="dadoonet" created="2015-10-06T14:46:59Z" id="145880080">Note that for #12719, I won't be able to make it for 2.0. So for now, we decided to do some manual testing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>javascript API: client.msearch document reference is incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12767</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html
https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference-1-4.html#api-msearch-1-4
https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference-1-3.html#api-msearch-1-3

``` js
client.msearch({
  body: [
    // match all query, on all indices and types
    {},
    { query: { match_all: {} } },

    // query_string query, on index/mytype
    { _index: 'myindex', _type: 'mytype' },
    { query: { query_string: { query: '"Test 1"' } } }
  ]
});
```

**Issue:** when index is defined as per reference document: '_index', elasticsearch returns query from all indexes, it does not set the index for the search.

It works if the field is set as "index": "myindex" removing the _
</description><key id="100018712">12767</key><summary>javascript API: client.msearch document reference is incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">boysmovie</reporter><labels /><created>2015-08-10T09:33:30Z</created><updated>2015-08-10T09:49:00Z</updated><resolved>2015-08-10T09:48:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T09:48:43Z" id="129385873">Thanks for reporting this! Can you open this issue at https://github.com/elastic/elasticsearch-js instead?

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Do not try other URLs if specific URL was passed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12766</link><project id="" key="" /><description>If an URL specified with --url on the command line cannot be reached,
the plugin manager tries URLs at download.elastic.co automatically.

This can lead to download wrong plugins and also exposes potentially
the name of an internal plugin to the download service.

This fix ensures, that the plugin manager simply aborts, if the specified
URL cannot be downloaded.

This was uncovered by CI tests, that returned a strange error code, instead of returning a regular failure due to running into timeout when trying to reach elastic.co even though it should not have.
</description><key id="100018618">12766</key><summary>PluginManager: Do not try other URLs if specific URL was passed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T09:32:59Z</created><updated>2015-08-13T14:00:50Z</updated><resolved>2015-08-10T11:09:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T09:49:52Z" id="129386240">LGTM
</comment><comment author="nik9000" created="2015-08-10T09:51:46Z" id="129386804">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Move qa's convert-plugin-name macrodef to dev-tools.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12765</link><project id="" key="" /><description>This would allow to use it for plugins which are in different repositories.
</description><key id="100014677">12765</key><summary>Tests: Move qa's convert-plugin-name macrodef to dev-tools.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T09:08:54Z</created><updated>2015-08-10T10:04:00Z</updated><resolved>2015-08-10T10:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T09:51:11Z" id="129386522">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose ClassloadingMXBean in Node Stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12764</link><project id="" key="" /><description>Closes #12738
</description><key id="100012942">12764</key><summary>Expose ClassloadingMXBean in Node Stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T08:58:15Z</created><updated>2015-08-13T13:59:38Z</updated><resolved>2015-08-12T12:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-12T08:40:51Z" id="130218822">LGTM
</comment><comment author="tlrx" created="2015-08-12T11:22:21Z" id="130264447">@martijnvg thanks for your review!

@clintongormley I'm not sure of the names of the stats... Do you have a better idea?

```
GET /_nodes/stats/jvm?pretty&amp;filter_path=**.classloading
{
  "nodes" : {
    "0vr_FRedQl-pM_znLD8a4Q" : {
      "jvm" : {
        "classloading" : {
          "current_loaded_count" : 5961,
          "total_loaded_count" : 5961,
          "total_unloaded_count" : 0
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-08-12T11:28:25Z" id="130266550">@tlrx how about just `classes`?  The stats themselves are pretty descriptive.
</comment><comment author="tlrx" created="2015-08-12T12:30:43Z" id="130283249">@clintongormley thanks! Merged we `classes`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cluster.blocks.read_only set as a persistent setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12763</link><project id="" key="" /><description>If you manage to set cluster.blocks.read_only=true as a persistent cluster setting it seems you get into a catch-22 since the cluster won't allow you to disable the setting when it's in read-only mode. I was eventually able to get out of it after a full cluster restart.

This raises a few questions:
- Should cluster.blocks.read_only even be settable as a persistent setting?
- If yes, how are you supposed to disable it?
- Regardless, can we improve [the documentation](https://www.elastic.co/guide/en/elasticsearch/reference/1.6/cluster-update-settings.html#_metadata) to make the intended semantics clear? Right now it easily gives the impression that among metadata operations it's only index creation and deletion that's blocked which clearly isn't true.

I initially raised this at https://discuss.elastic.co/t/recovering-from-cluster-blocks-read-only-true-in-dynamic-settings and additional details can be found there.

This was on ES 1.7.1.
</description><key id="100010872">12763</key><summary>cluster.blocks.read_only set as a persistent setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magnusbaeck</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2015-08-10T08:49:44Z</created><updated>2016-01-26T18:34:49Z</updated><resolved>2016-01-26T18:34:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T18:34:49Z" id="175163289">Hmmm - `cluster.blocks.read_only` can be set to `false` whether it is persistent or transient, including on 1.7.1.  In 3.0, you'll be able to unset this setting completely.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Mention in migration doc that order to dynamic arguments is important</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12762</link><project id="" key="" /><description>Due to the limited abilities of parsing of dynamic (not configured) arguments
like `http.cors.enabled`, that dont map to a command line argument but will
become configuration, we need to mention explicitely, that those dynamic arguments
must come last.

Also fixed some mentions of a memory index setting, that does not exist anymore.

Closes #12758
</description><key id="100009970">12762</key><summary>Docs: Mention in migration doc that order to dynamic arguments is important</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>docs</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T08:43:09Z</created><updated>2015-08-10T09:34:09Z</updated><resolved>2015-08-10T09:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-10T08:50:22Z" id="129368747">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for base_path in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12761</link><project id="" key="" /><description>Related to https://github.com/elastic/elasticsearch-cloud-aws/issues/230

We now can support setting a global `base_path` in `elasticsearch.yml` using `repositories.s3.base_path` key.
</description><key id="100009819">12761</key><summary>Add support for base_path in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T08:41:53Z</created><updated>2015-08-13T14:57:13Z</updated><resolved>2015-08-13T13:31:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-13T13:08:38Z" id="130663610">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[snapshot] support secondary repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12760</link><project id="" key="" /><description>Marking this as `discuss` as I don't know if we want to support this.

In [this PR](https://github.com/elastic/elasticsearch-cloud-azure/pull/93), the need to support alternative repository was raised. I'm wondering if this is something we would like to implement whatever the repository type is.

I can imagine something like:

``` json
// Create a first repo
PUT /_snapshot/repo_fs
{
  "type": "fs",
  "settings": {
    "location": "/mount/backups/my_backup"
  }
}

// Create a second repo
PUT /_snapshot/repo_azure
{
  "type": "azure",
  "settings": {
    "key": "my_key",
    "secret": "secret"
  },
  "secondary": "repo_fs"
}
```

So when we try to backup or restore to/from `repo_azure` if it fails, we try then `repo_fs`.

@imotov WDYT? Would this make sense to support what was asked in https://github.com/elastic/elasticsearch-cloud-azure/pull/93 but whatever the repositories are?

May be it's better to do this retry on a client level instead?

Thoughts?
</description><key id="100003173">12760</key><summary>[snapshot] support secondary repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2015-08-10T08:10:47Z</created><updated>2015-12-18T14:01:32Z</updated><resolved>2015-12-18T14:01:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-10T09:36:39Z" id="129381891">Why would this need to be built in? The user can already do this themselves. We shoudln't complicate things. They can push snapshots to multiple repos, try restoring from one, and on failure, try to restore from another.
</comment><comment author="dadoonet" created="2015-08-10T09:46:54Z" id="129385271">@rjernst What is your position regarding the original feature request ([PR 93](https://github.com/elastic/elasticsearch-cloud-azure/pull/93)) then?
</comment><comment author="clintongormley" created="2015-08-10T11:36:06Z" id="129414869">I agree with @rjernst's comment.  We should add support for multiple azure repositories in the same way as we do for AWS, etc.  I also agree with this comment from https://github.com/elastic/elasticsearch-cloud-azure/pull/93#issuecomment-113214013:

&gt; I like the idea of specifying the account in the repository settings, but the key should be specified in the startup configuration. This follows the pattern and rationale set by the path.repo setting which ensures that ES only writes to pre-configured locations. Specifying the key in the repository settings would allow any azure storage to be written to.
</comment><comment author="dadoonet" created="2015-08-10T11:43:57Z" id="129418263">So I added a comment to #12759. 

What about the original issue in [PR 93](https://github.com/elastic/elasticsearch-cloud-azure/pull/93)? Does it mean it won't be pulled in? So I can close this issue and the related PR?
</comment><comment author="clintongormley" created="2015-08-10T11:49:46Z" id="129419886">&gt; What about the original issue in PR 93? Does it mean it won't be pulled in? So I can close this issue and the related PR?

yes - I don't think that's the right solution for this.  The generic approach already provided by snapshot/restore works better here.
</comment><comment author="dadoonet" created="2015-12-18T14:01:31Z" id="165784543">Azure now supports secondary settings. According to the comments on this issue, we can close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cloud-azure] Allow multiple repositories with different settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12759</link><project id="" key="" /><description>As for now, azure cloud plugin only supports one single Azure configuration (`account` and `key`) as those settings are provided in `elasticsearch.yml`:

``` yml
cloud:
    azure:
        storage:
            account: your_azure_storage_account
            key: your_azure_storage_key
```

In some cases, there is a need to have multiple azure connections, as explained for example at https://github.com/elastic/elasticsearch-cloud-azure/pull/93

For this, we need to support providing settings when the repository is created:

``` json
PUT _snapshot/my_azure_repository1
{
    "type": "azure",
    "settings": {
        "account": "AZURE_ACCOUNT1",
        "key": "AZURE_KEY1"
    }
}

PUT _snapshot/my_azure_repository2
{
    "type": "azure",
    "settings": {
        "account": "AZURE_ACCOUNT2",
        "key": "AZURE_KEY2"
    }
}
```

If no `account` is defined in the repository creation call, then we fallback to the one in `elasticsearch.yml` and if no `account` is available, then we fail the creation.
Same for `key` parameter.

This will also fix #12446.
</description><key id="99998483">12759</key><summary>[cloud-azure] Allow multiple repositories with different settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Snapshot/Restore</label></labels><created>2015-08-10T07:51:47Z</created><updated>2015-12-18T14:02:31Z</updated><resolved>2015-12-18T14:02:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T11:42:00Z" id="129417074">@clintongormley so based on your https://github.com/elastic/elasticsearch/issues/12760#issuecomment-129414869, I guess we just don't want to support what I wrote in the above description:

&gt; Same for key parameter.

It would mean that we only support this:

``` json
PUT _snapshot/my_azure_repository2
{
    "type": "azure",
    "settings": {
        "account": "AZURE_ACCOUNT2"
    }
}
```

Correct?
</comment><comment author="clintongormley" created="2015-08-10T11:48:42Z" id="129419751">i think that's right.
</comment><comment author="craigwi" created="2015-08-14T21:56:25Z" id="131249157">The pull request I submitted back in June solves this: https://github.com/elastic/elasticsearch-cloud-azure/pull/93.  I learned on a call today that my release notes, which describes this in detail, was not surfaced well.  

I added the release notes to the pull request: https://github.com/elastic/elasticsearch-cloud-azure/pull/93.

Craig.
</comment><comment author="dadoonet" created="2015-08-14T22:07:31Z" id="131251504">@craigwi see also discussion which happened at https://github.com/elastic/elasticsearch/issues/12760
</comment><comment author="craigwi" created="2015-08-14T23:11:33Z" id="131265316">The idea of chained repositories in #12760 is not a replacement for the feature discussed here and implemented by my pull request.  

Azure storage accounts, when set up with replication of "READ-ACCESS GEO REDUNDANT" automatically replicate to a specific secondary region.  In the case of a storage account "in" US West, the secondary region is (always) US East.  To read from the secondary endpoint for the storage account one specifies a parameter LocationMode.SECONDARY_ONLY the APIs (in BlobRequestionOptions).  Access to the storage account through to the secondary endpoint is always READ ONLY.

I have cases when I need to use different combinations of one storage account or the other and different values of LocationMode.  Given other patterns I have seen in ES, it seemed simplest to enable both multiple storage accounts to be specified in the yml file AND any combination of those storage accounts with the location mode at the time the repository is registered.

I hope this helps to clarify the feature I built.

Craig.
</comment><comment author="craigwi" created="2015-08-31T19:57:44Z" id="136483083">I created a pull request https://github.com/elastic/elasticsearch/pull/13228 containing the changes I had made to the elasticsearch-cloud-azure plugin.

Craig.
</comment><comment author="dadoonet" created="2015-12-18T14:02:30Z" id="165784698">This has been fixed by #13779
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Startup CLI Parsing needs defined order of arguments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12758</link><project id="" key="" /><description>Due to the limited feature set of the CLI parser the following does not work and will return an error message

```
/bin/elasticsearch --http.cors.enabled=true --http.cors.allow-origin='*' -d -p /tmp/foo
```

The reason for this is, that the current implementation of the CLI parser will literally stop parsing as soon as it hits a dynamic parameter and add all the other parameters to the list of parameters that should be parsed dynamically with a special logic. So `-d` and `-p` will never be parsed regularly and added to the specific CLI parser options

**Workaround 1**: Document that "dynamic" arguments must come last in the list of arguments.

**Workaround 2**: Two a second parsing logic check for all the arguments, when doing the dynamic parsing for all the `--foo.bar` parameters...

**Long term workaround 1**: Treat configuration file settings as configuration file settings and add some specific switches for settings that should be changed via commandline (like node name, interface, cluster name), but make the parsing strict.

**Long term workaround 2**: Use a more powerful parsing library, which supports all of our needed features natively - which is also needed for subcommands, like `bin/elasticsearch plugin install`. [argparse4j](http://argparse4j.sourceforge.net/) comes into mind.
</description><key id="99995732">12758</key><summary>Startup CLI Parsing needs defined order of arguments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>discuss</label><label>v2.0.0-beta1</label></labels><created>2015-08-10T07:42:04Z</created><updated>2015-08-10T09:34:09Z</updated><resolved>2015-08-10T09:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-10T07:54:17Z" id="129346147">@spinscale I'd be ok with just documenting the requirement for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OOM when too many expired documents?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12757</link><project id="" key="" /><description>I have put too many documents (with _ttl  '3d' ) to ES on 2015-08-06. 
Then I stopped ES on 2015-08-07. 
The GC Old Gen reached 100% when I start ES on 2015-08-10.
I dumped Java Heap, and  found there are **too many DocToPurge instances**.
In `ExpiredDocsCollector`, the size of `docsToPurge` list is unlimited. When there are too many documents, the size of `docsToPurge` list will be very large.

Can the size of `docsToPurge` list be limited?
</description><key id="99961105">12757</key><summary>OOM when too many expired documents?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thihy</reporter><labels><label>discuss</label></labels><created>2015-08-10T03:19:05Z</created><updated>2016-01-26T18:16:29Z</updated><resolved>2016-01-26T18:16:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T18:16:29Z" id="175152599">Expiring docs with `_ttl` has been deprecated.  A better option is to use the delete-by-query plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for `disable_coord` param to `terms` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12756</link><project id="" key="" /><description>Trivial change to enable `disable_coord` support for TermsQuery .

Added test case to verify support for options in  `TermsQuery` Parser.

Closes #12755
</description><key id="99940185">12756</key><summary>Add support for `disable_coord` param to `terms` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">keety</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-09T23:34:01Z</created><updated>2015-08-11T14:14:34Z</updated><resolved>2015-08-11T14:13:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-10T12:37:56Z" id="129428278">Actually I was recently thinking about changing this query so that we always return a `TermsQuery`. It would break backward compatibility, but I'm not too happy with the current state where `terms`, when used as a `query`, is essentially a shortcut for creating a `bool` query. I would rather make the generated `query` and `filter` have more consistent behaviours and recommend to use `bool` instead of `terms` if `disable_coord` and/or `min_shoul_match` are required?
</comment><comment author="clintongormley" created="2015-08-10T13:34:47Z" id="129456920">@jpountz i agree with your comment - this should be a simple query.  All the `terms` query buys you is a more compact syntax when looking for lots of terms.  I would guess that min_should_match and coord is more useful when dealing with only a few terms, so using a bool query instead would be ok
</comment><comment author="jpountz" created="2015-08-11T08:34:03Z" id="129768489">Ohhh I understand why you opened this PR now: this used to be supported in 1.x but support for this parameter was accidentally lost in 2.x. So we have a bw compat break. Then I'm inclined to merge the change and to later think about how we could deprecate min_should_match/disable_coord.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms query  does not support disable_coord  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12755</link><project id="" key="" /><description>Terms query does not support `disable_coord` in  `elasticsearch 2.0.0 beta1 snapshot`.

Example:

```
#Setup Index :

curl -Xput "http://localhost:9200/test"

curl -Xput "http://localhost:9200/test/test/1 " -d'
{
    "data" : "is elastic"
}'

#Problematic Query:
curl -Xpost "http://localhost:9200/test/_search" -d'
{
   "query": {
      "terms": {
         "data": [
            "is",
            "elastic"
         ],
         "disable_coord": true
      }
   }
}'

#Response
{
   "error": {
      "root_cause": [
         {
            "type": "query_parsing_exception",
            "reason": "[terms] query does not support [disable_coord]",
            "index": "test",
            "line": 7,
            "col": 11
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "test",
            "node": "LuwqtxavQ-2mbQMirXKM6g",
            "reason": {
               "type": "query_parsing_exception",
               "reason": "[terms] query does not support [disable_coord]",
               "index": "test",
               "line": 7,
               "col": 11
            }
         }
      ]
   },
   "status": 400
}

```
</description><key id="99940075">12755</key><summary>Terms query  does not support disable_coord  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keety</reporter><labels /><created>2015-08-09T23:31:45Z</created><updated>2015-08-11T14:13:58Z</updated><resolved>2015-08-11T14:13:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Enable the license checker over distribution/* and plugins/*</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12754</link><project id="" key="" /><description>Moved the license checker config into the parent pom, and overrode
the license dir/target-to-check in distributions/pom.

Disabled the license checker explicitly for projects which run integration
tests but have no licenses dir:
- core
- distribution
- qa

Closes #12752
</description><key id="99906556">12754</key><summary>Enable the license checker over distribution/* and plugins/*</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>build</label><label>review</label><label>test</label></labels><created>2015-08-09T15:47:28Z</created><updated>2015-08-09T18:17:05Z</updated><resolved>2015-08-09T17:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T15:48:52Z" id="129201070">mvn verify works for me
</comment><comment author="clintongormley" created="2015-08-09T16:19:42Z" id="129212379">thanks @dadoonet 

updated to only specify the version in the parent pom
</comment><comment author="dadoonet" created="2015-08-09T16:40:19Z" id="129213878">It looks good to me.
I really like we enable this only when it makes sense instead of disabling when it does not.

However, I'm not a big fan of disabling this in plugins when no 3rd party lib is used.
I think we should have this `licenses` dir in every plugin but with an "empty" file like `no_lib.txt` to fix the git issue with empty dirs.

Because if at some point we need to add a lib to one of the existing plugin, we might forget to reenable this license check.

WDYT?
</comment><comment author="clintongormley" created="2015-08-09T17:09:09Z" id="129217619">&gt; Because if at some point we need to add a lib to one of the existing plugin, we might forget to reenable this license check.

Makes sense. I've pushed a new commit, and will rerun mvn verify.
</comment><comment author="dadoonet" created="2015-08-09T17:14:35Z" id="129217860">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JSON mappings differ from real mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12753</link><project id="" key="" /><description>There are a few things going on here. I would expect this create-index request to throw an exception because the `search_analyzer` is different between fields:

```
PUT t
{
  "mappings": {
    "x": {
      "properties": {
        "foo": {
          "type": "string",
          "analyzer": "standard", 
          "search_analyzer": "whitespace"
        }
      }
    },
    "y": {
      "properties": {
        "foo": {
          "type": "string",
          "analyzer": "standard",
          "search_analyzer": "simple"
        }
      }
    }
  }
}
```

It silently accepts the difference, and sets both `search_analyzer`'s to `simple`:

```
GET t/_mapping/*/field/foo

{
   "t": {
      "mappings": {
         "y": {
            "foo": {
               "full_name": "foo",
               "mapping": {
                  "foo": {
                     "type": "string",
                     "analyzer": "standard",
                     "search_analyzer": "simple"
                  }
               }
            }
         },
         "x": {
            "foo": {
               "full_name": "foo",
               "mapping": {
                  "foo": {
                     "type": "string",
                     "analyzer": "standard",
                     "search_analyzer": "simple"
                  }
               }
            }
         }
      }
   }
}
```

However, a plain GET-mapping shows the original settings:

```
GET t/_mapping/

{
   "t": {
      "mappings": {
         "x": {
            "properties": {
               "foo": {
                  "type": "string",
                  "analyzer": "standard",
                  "search_analyzer": "whitespace"
               }
            }
         },
         "y": {
            "properties": {
               "foo": {
                  "type": "string",
                  "analyzer": "standard",
                  "search_analyzer": "simple"
               }
            }
         }
      }
   }
}
```

Changing the `search_analyzer` on the `x` type throws a conflict exception (correctly) unless I specify `update_all_types`:

```
PUT t/_mapping/x?update_all_types=1
{
  "properties": {
    "foo": {
      "type": "string",
      "analyzer": "standard", 
      "search_analyzer": "pattern"
    }
  }
}
```

This request correctly changes the `search_analyzer` for both fields:

```
GET t/_mapping/*/field/foo

{
   "t": {
      "mappings": {
         "y": {
            "foo": {
               "full_name": "foo",
               "mapping": {
                  "foo": {
                     "type": "string",
                     "analyzer": "standard",
                     "search_analyzer": "pattern"
                  }
               }
            }
         },
         "x": {
            "foo": {
               "full_name": "foo",
               "mapping": {
                  "foo": {
                     "type": "string",
                     "analyzer": "standard",
                     "search_analyzer": "pattern"
                  }
               }
            }
         }
      }
   }
}
```

But it only changes the JSON mapping for the `x` type:

```
GET t/_mapping/

{
   "t": {
      "mappings": {
         "x": {
            "properties": {
               "foo": {
                  "type": "string",
                  "analyzer": "standard",
                  "search_analyzer": "pattern"
               }
            }
         },
         "y": {
            "properties": {
               "foo": {
                  "type": "string",
                  "analyzer": "standard",
                  "search_analyzer": "simple"
               }
            }
         }
      }
   }
}
```
</description><key id="99897971">12753</key><summary>JSON mappings differ from real mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-08-09T13:40:59Z</created><updated>2015-08-11T17:24:47Z</updated><resolved>2015-08-11T17:24:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>License/SHA checker not running?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12752</link><project id="" key="" /><description>I think the `check_license_and_sha.pl` script is not doing its job currently.  The `distribution/pom.xml` points to `${basedir}/../licenses` (see https://github.com/elastic/elasticsearch/blob/master/distribution/pom.xml#L111 and https://github.com/elastic/elasticsearch/blob/master/distribution/pom.xml#L119) while the licenses directory is actually in `${basedir}/licenses`, so the check just gets skipped as far as I can see.

Also, we're no longer using this script to check the plugins.
</description><key id="99885046">12752</key><summary>License/SHA checker not running?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>test</label></labels><created>2015-08-09T10:38:39Z</created><updated>2015-08-09T17:55:45Z</updated><resolved>2015-08-09T17:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-09T12:24:33Z" id="129174359">It can't be lenient anymore: we have to require this `licenses` directory or something like that. There is too much confusion about when it runs and when it does not run.

Leniency is the root of all evil. It is why elasticsearch is so fucked up.
</comment><comment author="clintongormley" created="2015-08-09T12:44:32Z" id="129181244">The only reason it was lenient was because it was in the parent pom, and so only ran for any project which contained a licenses directory.  Now that we're adding it only to specific poms then I think we're good.
</comment><comment author="rmuir" created="2015-08-09T12:47:11Z" id="129181610">I think licenses directory should just be mandatory. If its supposed to be lenient: then non-existing licenses directory means "no dependencies". 

I know git is a broken version control system that eats shit on empty directories, but we have to keep our sanity regardless of that.

I think license checker should run for every single module we release, it is too important because it also validates that the jars we think should be in the package are in fact there.
</comment><comment author="clintongormley" created="2015-08-09T12:50:52Z" id="129183505">I agree - it was in the parent pom so it was enabled for all modules, but some don't need license directories (eg dev-tools).  It's already in the distribution pom (i'll remove the skip clause) and i'll add it to the plugins pom so it runs for all plugins. (as part of the integration tests, or just normal mvn verify?)
</comment><comment author="rmuir" created="2015-08-09T12:53:46Z" id="129184097">Lets move it back to the parent. I think the issue was it didn't have a `&lt;skip&gt;${skip.integ.tests}&lt;/skip&gt;` before. This is important for the silly "pom-only" packages and stuff like dev-tools. For those the build will set this variable automatically so things should "just work"
</comment><comment author="clintongormley" created="2015-08-09T12:56:42Z" id="129184962">With the move to `distributions/` the paths to the licenses directory and which dir/zipfile to check changed, so the distributions are different from the plugins.  What I'll try doing is:
- put it in the parent pom
- store the path and target values in a var
- override these values in distribution/pom

that make sense?
</comment><comment author="rmuir" created="2015-08-09T12:57:57Z" id="129185384">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CLI parsing no longer supports single-dash parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12751</link><project id="" key="" /><description>It looks like no parameters of the form `-Des.xxx` are supported by bin/elasticsearch now, yet this form is still documented in various places:

```
docs/reference//indices/shadow-replicas.asciidoc:`-Des.security.manager.enabled=false` with the parameters while starting
docs/reference//migration/migrate_1_0.asciidoc:* Command line settings can now be passed without the `-Des.` prefix, for
docs/reference//setup/as-a-service.asciidoc:`ES_JAVA_OPTS`::          Any additional java options you may want to apply. This may be useful, if you need to set the `node.name` property, but do not want to change the `elasticsearch.yml` configuration file, because it is distributed via a provisioning system like puppet or chef. Example: `ES_JAVA_OPTS="-Des.node.name=search-01"`
docs/reference//setup/configuration.asciidoc:`-Des.max-open-files` set to `true`. This will print the number of open
docs/reference//setup/configuration.asciidoc:$ elasticsearch -Des.network.host=10.0.0.4
docs/reference//setup/configuration.asciidoc:$ elasticsearch -Des.config=/path/to/config/file
docs/reference//setup/configuration.asciidoc:$ elasticsearch -Des.index.store.type=memory
docs/reference//setup.asciidoc:$ bin/elasticsearch -Xmx2g -Xms2g -Des.index.store.type=memory --node.name=my-node
```

Also, the `-d` option only works if it is the first command line parameter, which isn't documented either.

Not sure if the right thing to do is to change CLI parsing or to document the changes?
</description><key id="99884240">12751</key><summary>CLI parsing no longer supports single-dash parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-08-09T10:33:16Z</created><updated>2015-08-10T07:08:32Z</updated><resolved>2015-08-10T07:08:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T10:34:14Z" id="129161035">@spinscale what do you think?
</comment><comment author="rmuir" created="2015-08-09T12:39:54Z" id="129179548">It is super confusing to see _es_ parameters being recommended to go in ES_JAVA_OPTS.

I'm gonna be completely honest here, I don't think ES should allow for -D as an argument at all: ES just is not robust enough for the task. Its totally confusing where JVM versus ES properties should go to the user, and if you do the wrong thing, its totally lenient and gives you no error message at all, just silently doesn't work.
</comment><comment author="clintongormley" created="2015-08-09T14:38:59Z" id="129192399">@rmuir not understanding the issues with -D, I tend to agree just for consistency's sake.  This just requires documentation then, and for `-d` (ie daemonize) either it should be documented that it must appear first in the args, or the parsing should be changed to support it wherever
</comment><comment author="spinscale" created="2015-08-10T06:03:43Z" id="129316944">Clinton, not sure I understand what you mean with not supported here?

```
# bin/elasticsearch -Des.node.name=mynode
[2015-08-10 08:00:02,011][INFO ][org.elasticsearch.node   ] [mynode] version[2.0.0-beta1-SNAPSHOT], pid[4934], build[cdad9e6/2015-08-10T05:58:54Z]

# bin/elasticsearch -Dnode.name=mynode
[2015-08-10 08:00:08,969][INFO ][org.elasticsearch.node   ] [mynode] version[2.0.0-beta1-SNAPSHOT], pid[4967], build[cdad9e6/2015-08-10T05:58:54Z]

# bin/elasticsearch -Dnode.name=mynode -d
# jps
5010 Elasticsearch

# curl localhost:9200/_cat/master
id                     host ip             node
S6yQ-kXAQGOnbTlRmZhpOQ orca 192.168.178.36 mynode
```

What does not work, is using the java arguments as part of the parameters now `bin/elasticsearch -Xmx2g` - this must be documented.
</comment><comment author="clintongormley" created="2015-08-10T07:08:32Z" id="129333080">@spinscale Turns out it is a problem with wildcards again:

```
bin/elasticsearch -Des.http.cors.enabled=true --http.cors.allow-origin='*'  -d
```

Fails with: 

```
ERROR: Parameter [-d]does not start with --
```

This isn't fixed by https://github.com/elastic/elasticsearch/pull/12710 

Closing in favour of #12710
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin manager should validate the SHA, where possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12750</link><project id="" key="" /><description>The plugin manager should perform SHA sum validation when available. If there is a shasum file in a specific location with a specific naming pattern, we can verify the sum after download.
</description><key id="99876402">12750</key><summary>Plugin manager should validate the SHA, where possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-09T08:21:40Z</created><updated>2015-09-16T13:34:08Z</updated><resolved>2015-08-14T20:30:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-12T10:14:30Z" id="130248880">Implementation detail: When running `mvn install` the creation of the checksums results simply in `.md5` and `.sha1` suffixes appended to the file...

Impl: We simply should GET that URL as well... `http://.../analysis-kuromoji.zip.sha1`
- If checksum file exists, compare the checksum with the downloaded file. If these differ, exit. If these match, print a message telling that checksums were ok
- If checksum does not exist, behave as usual, but print a warning, that no checksum comparison has happened, but proceed to install the plugin

Not sure if we need command line parameters like `--ignore-invalid-checksums` or `--abort-without-checksums` for now.
</comment><comment author="dakrone" created="2015-08-13T21:04:55Z" id="130844699">@spinscale I'm looking at this, it looks like we don't actually provide sha1 or md5 files for our own plugins? I am just curious if there's an already-available plugin I can use for manual testing of this.
</comment><comment author="dakrone" created="2015-08-13T21:28:53Z" id="130849632">Also, I guess nevermind on our own plugins, I can run a local server and test it with a URL that way, I was just curious if we already hosted the checksum files.
</comment><comment author="spinscale" created="2015-08-13T21:40:18Z" id="130853169">no, this is since 2.0...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin manager should verify that it can write to all directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12749</link><project id="" key="" /><description>The plugin manager needs to validate it can write to the appropriate directories before it starts. Pretty sure today (1.x at least) it will half install a plugin when it can't write to certain directions
</description><key id="99875599">12749</key><summary>Plugin manager should verify that it can write to all directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0</label></labels><created>2015-08-09T08:15:31Z</created><updated>2015-10-08T08:15:20Z</updated><resolved>2015-10-08T08:15:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-12T09:55:40Z" id="130243064">checked the code. There are no write permissions checks for `bin/$PLUGIN/` and `config/$PLUGIN/` - also you are right, we copy the plugin into the plugins directory and then try for `bin` and `config` directory, hoping all is fine...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin manager should download only from https by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12748</link><project id="" key="" /><description>The plugin manager should only try https URLs (with cert validation and hostname validation) by default, unless the user specifies an http URL on the command line themselves
</description><key id="99875474">12748</key><summary>Plugin manager should download only from https by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-08-09T08:13:58Z</created><updated>2015-09-14T17:17:14Z</updated><resolved>2015-08-31T09:53:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added Elasticsearch Browser Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12747</link><project id="" key="" /><description /><key id="99870569">12747</key><summary>Added Elasticsearch Browser Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">OlegKunitsyn</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-08-09T06:33:46Z</created><updated>2015-08-09T09:59:50Z</updated><resolved>2015-08-09T09:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T08:59:45Z" id="129142482">Hi @OlegKunitsyn 

Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="OlegKunitsyn" created="2015-08-09T09:32:20Z" id="129144985">Done :)
</comment><comment author="clintongormley" created="2015-08-09T09:59:50Z" id="129150145">thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Warning][Ubuntu] Warning abouy package quality.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12746</link><project id="" key="" /><description>See please:

```
The installation of any package that violate quality standards is not allowed. That could cause serious problems to your computer. Contact the person or organization who gave you this package file and include the details below.
```

This appears when i try install elasticsearch in Ubuntu with the debian package.
Please review this,
</description><key id="99861469">12746</key><summary>[Warning][Ubuntu] Warning abouy package quality.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SalahAdDin</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2015-08-09T04:31:14Z</created><updated>2015-08-10T20:33:59Z</updated><resolved>2015-08-09T10:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T08:56:22Z" id="129142136">&gt; and include the details below.

What were those details?
</comment><comment author="SalahAdDin" created="2015-08-09T09:48:34Z" id="129147357">This warning comes when I went to install elasticsearch in ubuntu, I did not get more data, just told me it was better not to install the package. I ignored the warning and equal Elasticsearch installed.
</comment><comment author="clintongormley" created="2015-08-09T10:01:55Z" id="129150196">Hi @SalahAdDin 

OK - without that other information, we can't do much about this ticket, so I'm going to close for now. If you manage to find it, please open a new ticket.

thanks
</comment><comment author="nik9000" created="2015-08-09T21:16:02Z" id="129241105">I've been installing elasticsearch on ubuntu for a year and a half and not
seen that message. If you have exact reproduction steps I'd love to see
them. Including which elasticsearch youdownloaded and which ubuntu you ran
it on.
On Aug 9, 2015 6:02 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; Closed #12746 https://github.com/elastic/elasticsearch/issues/12746.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12746#event-377314336.
</comment><comment author="SalahAdDin" created="2015-08-10T02:58:55Z" id="129285557">I see that will be for the new ubuntu release: 15.04, for the other versions i don't know.
</comment><comment author="nik9000" created="2015-08-10T20:33:59Z" id="129598272">This is what I get:

```
vagrant@vagrant-ubuntu-vivid-64:~$ uname -a
Linux vagrant-ubuntu-vivid-64 3.19.0-23-generic #24-Ubuntu SMP Tue Jul 7 18:52:55 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
vagrant@vagrant-ubuntu-vivid-64:~$ sudo dpkg -i elasticsearch-1.7.1.deb 
Selecting previously unselected package elasticsearch.
(Reading database ... 69040 files and directories currently installed.)
Preparing to unpack elasticsearch-1.7.1.deb ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Unpacking elasticsearch (1.7.1) ...
Setting up elasticsearch (1.7.1) ...
Processing triggers for systemd (219-7ubuntu6) ...
Processing triggers for ureadahead (0.100.0-19) ...
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disk allocation with default settings allows the disk to fill up past 88%, ignoring the 85% low watermark</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12745</link><project id="" key="" /><description>I'm observing the following behavior in our 5-node ES cluster (1TB drive on each node): we run with default settings for the disk allocation policy (ie: we did not override the policy itself, or the 85%,90% default watermarks). I expect that, once disk utilization on a node goes above 85%, shards would no longer be allocated to that node, but this is not the case. When my cluster had 3 nodes, on 07/31 I saw the 85% watermark being hit:

```
[2015-07-31 23:06:06,291][INFO ][cluster.routing.allocation.decider] [node-ca575976-f63d-4380-a94e-fad3a9900aa6] low disk watermark [15%] exceeded on [j4BdtG8pSUCLiHlU4SVBWw]
[node-9895fc4e-afc6-4165-a136-638d57767537] free: 146.2gb[14.8%], replicas will not be assigned to this node
```

However, when I expanded the cluster first to 4 nodes on 08/01, then to 5 nodes on 08/05, and continued ingesting data such that disk utilization went back up past 85% on the original nodes, I didn't see them logging any messages about exceeding the watermark. I don't see the disk usage exceeding 90%, so at some point ES must be doing its reallocation magic, but there is no evidence of that in the logs; and disk usage routinely exceeds 85%. 

Current disk usage (from `df -h` on each node):

```
de0  985G  815G  121G  88% /mnt
de1  985G  810G  125G  87% /mnt
de2  985G  800G  135G  86% /mnt
de3  985G  739G  196G  80% /mnt
de4  985G  451G  484G  49% /mnt
```

We create a new ES index for each day, and set the number of shards equal to number of nodes in the cluster, so my older indices have 3 shards, then from 08/01 to 08/05 4 shards, and after 08/05 5 shards per daily index. I imagined that ES would place more shards on the empty-ish disk on node de4 that i added last, instead of continuing to put shards on de0, de1, de2 which are all over 85% full, but that is not happening. For the last full day 08/08, I see an index of size 17GB put on each of the 5 nodes; and the day of 08/09 that recently started is also getting data on all 5 nodes. 

I'm looking for help to understand how reallocation works on my cluster. If ES is doing the expected thing,  then I will think about the right settings for the watermarks, and for my app (we currently have a 90% "stop ingest" threshold at the layer above ES, and ES cluster behavior seems to take us dangerously close to hitting that limit). Or perhaps I'm hitting an allocation bug and that's why my disks are filling up beyond 85%? 

Happy to provide additional data if needed, here are some settings that I know how to read out. We run ES version 1.5.0.

```
curl -XGET 'http://localhost:9200/_cluster/health' |python -mjson.tool
{
    "active_primary_shards": 353,
    "active_shards": 353,
    "cluster_name": "campfire.production.local",
    "initializing_shards": 0,
    "number_of_data_nodes": 5,
    "number_of_nodes": 5,
    "number_of_pending_tasks": 0,
    "relocating_shards": 0,
    "status": "green",
    "timed_out": false,
    "unassigned_shards": 0
}
```

Settings for one recent index:

```
curl -XGET 'http://localhost:9200/events-default@2015.08.09/_settings' |python -mjson.tool 
{
    "events-default@2015.08.09": {
        "settings": {
            "index": {
                "analysis": {
                    "analyzer": {
                        "jut_analyzer": {
                            "filter": [
                                "lowercase",
                                "word_delimiter"
                            ],
                            "tokenizer": "standard",
                            "type": "custom"
                        }
                    }
                },
                "creation_date": "1439078400496",
                "number_of_replicas": "0",
                "number_of_shards": "5",
                "uuid": "J-Zm3XzpRLu4dxdD-luacQ",
                "version": {
                    "created": "1050099"
                }
            }
        }
    }
}
```
</description><key id="99854566">12745</key><summary>Disk allocation with default settings allows the disk to fill up past 88%, ignoring the 85% low watermark</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dmehra</reporter><labels /><created>2015-08-09T02:31:17Z</created><updated>2015-09-03T19:52:13Z</updated><resolved>2015-09-03T19:51:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-09T03:58:07Z" id="129102302">This is related to https://github.com/elastic/elasticsearch/issues/11271, and happens because the DiskThresholdDecider averages out the usage for all the `path.data` disks, so the average is still below 85%
</comment><comment author="clintongormley" created="2015-08-09T08:55:40Z" id="129142114">Closing in favour of #11271
</comment><comment author="dmehra" created="2015-08-09T21:41:43Z" id="129244016">I read both the documentation at https://www.elastic.co/guide/en/elasticsearch/reference/current/disk.html, and the notes in https://github.com/elastic/elasticsearch/issues/11271, to mean that the disk usage calculation happens separately for each ES node (but gets averaged if there are multiple path.data disks on a node). In my case, there are 5 ES nodes in the cluster, so it is surprising that disk usage for the threshold is averaged across all of them. If this behavior is to remain, it would be good to update the documentation accordingly. 

Could I get a recommendation for how to get my cluster better rebalanced? To trigger reallocation of shards with my current state of de4 at 54% full and other disks at 84-89% full, I'm guessing I need to set the watermarks at something like 70% and 75%. Is this the advised way? Thank you.
</comment><comment author="dakrone" created="2015-08-10T14:13:38Z" id="129469165">@dmehra I'm sorry, I misunderstood your original post, I saw the disk listing and thought that was from a single node with 5 data paths in the listing (in which case it would be averaged). ES doesn't average across all the nodes (as the documentation correctly states). I'll re-open this.
</comment><comment author="dakrone" created="2015-08-10T22:38:05Z" id="129635591">@dmehra did you happen to notice for the nodes over 85% whether they had shards currently relocating away from them? When ES calculates the disk usage it tries to take relocating shards into account, so I'm wondering if maybe it thought it was only over the low threshold because of that.
</comment><comment author="dmehra" created="2015-08-10T22:53:36Z" id="129638550">The ES logs on all nodes were completely quiet, so based on that, no there wasn't any relocation going on; is there a better way to check?
</comment><comment author="dakrone" created="2015-08-10T23:08:46Z" id="129643197">@dmehra you can see it either in the cluster state, or the `/_cat/shards?v` output will tell you if a shard is relocating (the easiest to read human-wise)
</comment><comment author="dakrone" created="2015-08-10T23:10:51Z" id="129643470">@dmehra you can also increase the logging level for the "cluster" package and you can see the calculations that ES does for the free disk on the nodes and how big each shard is, that would be quite helpful if you have that info.
</comment><comment author="dmehra" created="2015-08-11T18:26:16Z" id="130003863">Ok, my nodes are now at 88%,89%,86%,88% and 73% full. On all of them, all shards report STARTED state according to `curl -XGET 'http://localhost:9200/_cat/shards?v'`, i don't see any relocating shards. 

However, I have figured out where the logs are; I now understand that only the ES master node logs about the watermark (not the nodes on which the watermark is in fact exceeded), and my master node has shifted to the newest node I added, so now I see it's been logging about the watermarks the whole time quite verbosely, every 30 seconds. Here is a sample of log messages:

```
[2015-08-11 18:09:56,508][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [VI3QaNWDQcC8bHZqSKDs5Q][node-ca575976-f63d-4380-a94e-fad3a9900aa6] free: 118.5gb[12%], replicas will not be assigned to this node
[2015-08-11 18:09:56,508][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [iZiQJM_JSvOiULq6tnJXTw][node-62be99da-c545-461e-8f85-f1d5ef679f9b] free: 134.6gb[13.6%], replicas will not be assigned to this node
[2015-08-11 18:09:56,508][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [If-K_WFWQ6G3sarNu5gk3Q][node-9895fc4e-afc6-4165-a136-638d57767537] free: 109.8gb[11.1%], replicas will not be assigned to this node
[2015-08-11 18:09:56,508][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [ty1fKrvZRoeyT6UqVlLRXw][node-03c4a439-bd99-4654-9f8d-8fd0718e844e] free: 121.5gb[12.3%], replicas will not be assigned to this node
```

The above is from current time; the reports of exceeding the watermark started on 08/06 when space usage was lower, here is the first instance for each node:

```
[2015-08-06 20:20:15,987][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [VI3QaNWDQcC8bHZqSKDs5Q][node-ca575976-f63d-4380-a94e-fad3a9900aa6] free: 147.3gb[14.9%], replicas will not be assigned to this node

[2015-08-08 00:01:20,360][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [iZiQJM_JSvOiULq6tnJXTw][node-62be99da-c545-461e-8f85-f1d5ef679f9b] free: 145.8gb[14.8%], replicas will not be assigned to this node

(for this node, we somehow hit the high watermark *first*, and then after some reallocation hit the low watermark, this strikes me as an odd timeline)
[2015-08-08 06:12:20,602][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-08-08 06:12:20,647][WARN ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] After allocating, node [If-K_WFWQ6G3sarNu5gk3Q] would have less than the required 10% free disk threshold (9.8% free), preventing allocation
[2015-08-08 06:30:20,615][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [If-K_WFWQ6G3sarNu5gk3Q][node-9895fc4e-afc6-4165-a136-638d57767537] free: 146.4gb[14.8%], replicas will not be assigned to this node

[2015-08-09 16:53:21,985][INFO ][cluster.routing.allocation.decider] [node-e458c8ad-fdcf-46b0-b6d2-a1a0ec7175c7] low disk watermark [15%] exceeded on [ty1fKrvZRoeyT6UqVlLRXw][node-03c4a439-bd99-4654-9f8d-8fd0718e844e] free: 147.3gb[14.9%], replicas will not be assigned to this node
```

Given that final space usage on the nodes is higher than 85%, and the fact that I see shards with non-negligible space usage stored on days after we hit the low watermark, something seems funny. For example, on my node de0 which is [VI3QaNWDQcC8bHZqSKDs5Q] that exceeded the low watermark on 08/06, there are shards stored on 08/08-08/11, and at no time did disk usage on it drop back below 85%.

```
17G     /mnt/elasticsearch/campfire.production.local/nodes/0/indices/events-default@2015.08.08
16G     /mnt/elasticsearch/campfire.production.local/nodes/0/indices/events-default@2015.08.09
44G     /mnt/elasticsearch/campfire.production.local/nodes/0/indices/events-default@2015.08.10
35G     /mnt/elasticsearch/campfire.production.local/nodes/0/indices/events-default@2015.08.11
```

If you'd like me to upload the log from master node, I can do that; if additional logging is needed, please advise how to turn it on, ideally at runtime without restarting ES.
</comment><comment author="dakrone" created="2015-08-11T22:17:35Z" id="130094086">@dmehra it looks to me like your servers are undergoing the desired behavior, once they get about 85% percent full, they won't allow any new shards to be allocated to the node (the low watermark) and if they get over 90% full (the high watermark), shards are relocated away from the node in order to prevent them from going over the disk limit.

It's totally possible for a node to be over 85% (as long as it's under 90%) because a shard can be relocated to a node, bringing it to 84.9% disk usage, then more documents can be indexed, increasing the size of the shard above the 85% low watermark (which is why we have two watermarks).

Does that make sense? If you want the disk to never go over 85% full, you need to set the **high** watermark to 85% instead of the default 90%.
</comment><comment author="dmehra" created="2015-09-03T19:52:13Z" id="137556588">We did additional experiments on our end and validated that ES behaves as expected in our expansion scenario - thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Settings.getAsClass</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12744</link><project id="" key="" /><description>This method on settings loaded a class, based on a setting value, using the default classloader. It had all kinds of leniency in how the classname was found, and simply cannot work with plugins having isolated classloaders.

This change removes that method. Some of the uses of it were for custom extension points, like custom repository or discovery types. A lot were just there to plugin mock implementations for tests. For the settings that were legitimate, all now support plugins adding the given setting via onModule. For those that were specific to tests for mocks, they now use Classes.loadClass (a helper around Class.forName). This is a temporary measure until (in a future PR) tests can change the implementation via package private statics.

I also removed a number of unnecessary intermediate modules, added a "jvm-example" plugin that can be filled in in the future as a smoke test for breaking plugins, and gave some documentation to "spawn" modules interface.

closes #12643
closes #12656
</description><key id="99841032">12744</key><summary>Remove Settings.getAsClass</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-08T23:00:48Z</created><updated>2015-08-10T22:27:03Z</updated><resolved>2015-08-10T21:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-09T06:01:34Z" id="129127051">That's fantastic!
I just looked at the plugin part and it looks great to me.
</comment><comment author="rmuir" created="2015-08-09T11:52:00Z" id="129170695">+1
</comment><comment author="s1monw" created="2015-08-10T12:38:21Z" id="129428337">I left some comments other than that LGTM. I'd really like to work on making all our internally plugged in mock things a `MockPlugin` too in a followup to make sure we can plug them in and at the same time we exercising our plugin system from core
</comment><comment author="rjernst" created="2015-08-10T20:34:10Z" id="129598312">@uboness @s1monw I pushed new commits, removing the deprecation/rename and addressing other suggestions.
</comment><comment author="s1monw" created="2015-08-10T21:00:36Z" id="129605041">LGTM
</comment><comment author="s1monw" created="2015-08-10T21:01:26Z" id="129605214">this is a great step forward. Let's get rid of spawn modules altogether in a followup we need to flatten this hierarchy...
</comment><comment author="s1monw" created="2015-08-10T21:01:52Z" id="129605299">oh and please squash these - it's pretty unreadable list of commits :)
</comment><comment author="rjernst" created="2015-08-10T21:07:34Z" id="129606416">I created #12783 as a follow up.
</comment><comment author="rjernst" created="2015-08-10T21:22:00Z" id="129610681">I also created #12784 to finish the test refactoring work here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12743</link><project id="" key="" /><description /><key id="99833970">12743</key><summary>Fixing typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Asimov4</reporter><labels><label>docs</label></labels><created>2015-08-08T21:15:12Z</created><updated>2015-08-09T08:53:47Z</updated><resolved>2015-08-08T21:31:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-08T21:32:07Z" id="129046262">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor geo_point validate* and normalize* for 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12742</link><project id="" key="" /><description>This PR refactors the old `validate` and `normalize` options for `geo_point` field type to `ignore_malformed` and `coerce`, respectively.  Geo queries are also updated to `ignore_malformed` GeoPoints.  This PR is for 2.0 only since mapping implementation has drastically changed from 1.x. For 1.7 changes see PR #12300 

closes #10170 
</description><key id="99832693">12742</key><summary>Refactor geo_point validate* and normalize* for 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-08-08T20:51:12Z</created><updated>2015-08-18T11:18:48Z</updated><resolved>2015-08-17T19:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-10T10:13:44Z" id="129396308">I like this change but it does mean that the meaning of `coerce` here would be subtly different from the meaning in other types. For example if I have an integer field and I set `coerce: false` and then try to index a document where the value of that field is `"200"` then it will produce and error (if `ignore_malformed` is also `false`). with this change (if I am reading correctly), a `geo_point` field with `coerce: false` (and `ignore_malformed: false`) will not error if I insert a geo_point `[480.5, 160.9]` but will index the geo_point with the un-normalised lat and long values.
</comment><comment author="nknize" created="2015-08-10T13:01:25Z" id="129433891">@colings86 The same applies. (e.g., see `GeoPointFieldMapperTests.testValidateLatLonValues`)  If you set `ignore_malformed: false` and `coerce: false` and insert a malformed geo_point (e.g., coordinate `[1.3, -91]`) the parser will throw an Exception.
</comment><comment author="colings86" created="2015-08-10T13:02:17Z" id="129434210">oh ok, great. Thanks for clarifying :)
</comment><comment author="rjernst" created="2015-08-13T22:45:39Z" id="130869993">@nknize I left a couple more comments. Mappings look good, I think we should add some more tests. 
</comment><comment author="rjernst" created="2015-08-14T08:22:40Z" id="131017339">This looks great. I left couple more comments about the tests to make sure we have real backcompat checks there.
</comment><comment author="nknize" created="2015-08-14T16:24:16Z" id="131168043">Thanks for the review @rjernst! Updated bwc testing and other minor edits from the comments.
</comment><comment author="rjernst" created="2015-08-14T21:14:51Z" id="131240618">LGTM, 1 more minor suggestion for the tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move existing PID file to new location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12741</link><project id="" key="" /><description>This fixes the init script breakage that was introduced in 1.6. Submitting this low-risk bugfix to master but please consider cherry-picking it into the 1.7 branch.

Closes #12649
</description><key id="99829950">12741</key><summary>Move existing PID file to new location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">magnusbaeck</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label></labels><created>2015-08-08T19:39:25Z</created><updated>2016-01-30T13:32:05Z</updated><resolved>2016-01-30T13:32:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T08:50:43Z" id="129141097">@tlrx could you review please?
</comment><comment author="magnusbaeck" created="2016-01-30T13:32:05Z" id="177177776">The associated bug was closed so I guess this PR will never be merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update upgrade.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12740</link><project id="" key="" /><description /><key id="99805844">12740</key><summary>Update upgrade.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PandiyanCool</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-08-08T14:32:42Z</created><updated>2016-01-13T06:17:00Z</updated><resolved>2016-01-13T06:17:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T08:48:58Z" id="129141047">Hi @PandiyanCool

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update upgrade.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12739</link><project id="" key="" /><description /><key id="99805188">12739</key><summary>Update upgrade.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PandiyanCool</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-08-08T14:14:21Z</created><updated>2015-08-09T08:48:39Z</updated><resolved>2015-08-09T08:48:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T08:47:03Z" id="129141000">Hi @PandiyanCool 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2015-08-09T08:48:39Z" id="129141039">Closing in favour of #12740
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose ClassloadingMXBean in stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12738</link><project id="" key="" /><description>There are various scripting engines and caching and other complexity, I think we should expose basic stats about classes in the JVM to assist with debugging:
- number of currently loaded classes
- total number of loaded and unloaded classes ever.

http://docs.oracle.com/javase/7/docs/api/java/lang/management/ClassLoadingMXBean.html
</description><key id="99804165">12738</key><summary>Expose ClassloadingMXBean in stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Stats</label><label>enhancement</label></labels><created>2015-08-08T13:56:19Z</created><updated>2015-08-12T12:30:04Z</updated><resolved>2015-08-12T12:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-10T08:58:49Z" id="129372505">+1, I made a pull request for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to start ElasticsearchF.java in eclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12737</link><project id="" key="" /><description>Hi ,
   earlier(a week before) I just used to follow http://www.lindstromhenrik.com/debugging-elasticsearch-in-eclipse/ and I was able to start ElasticsearchF.java and was able to even put a break point. But recently I am not able to do so . It gives a lot of errors like 

&lt;code&gt;
java.io.IOException: Resource not found: "org/joda/time/tz/data/ZoneInfoMap" ClassLoader: sun.misc.Launcher$AppClassLoader@c387f44
    at org.joda.time.tz.ZoneInfoProvider.openResource(ZoneInfoProvider.java:210)
    at org.joda.time.tz.ZoneInfoProvider.&lt;init&gt;(ZoneInfoProvider.java:127)
    at org.joda.time.tz.ZoneInfoProvider.&lt;init&gt;(ZoneInfoProvider.java:86)
    at org.joda.time.DateTimeZone.getDefaultProvider(DateTimeZone.java:514)
    at org.joda.time.DateTimeZone.getProvider(DateTimeZone.java:413)
    at org.joda.time.DateTimeZone.forID(DateTimeZone.java:216)
    at org.joda.time.DateTimeZone.getDefault(DateTimeZone.java:151)
    at org.joda.time.chrono.ISOChronology.getInstance(ISOChronology.java:79)
    at org.joda.time.DateTimeUtils.getChronology(DateTimeUtils.java:266)
    at org.joda.time.format.DateTimeFormatter.selectChronology(DateTimeFormatter.java:968)
    at org.joda.time.format.DateTimeFormatter.printTo(DateTimeFormatter.java:672)
    at org.joda.time.format.DateTimeFormatter.printTo(DateTimeFormatter.java:560)
    at org.joda.time.format.DateTimeFormatter.print(DateTimeFormatter.java:644)
    at org.elasticsearch.Build.&lt;clinit&gt;(Build.java:51)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:136)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
    at org.elasticsearch.bootstrap.ElasticsearchF.main(ElasticsearchF.java:30)
&lt;b&gt;Exception in thread "main" java.lang.IllegalStateException: java.nio.file.NoSuchFileException: /home/harish/Documents/code/elasticsearch/es/fork-new/elasticsearch/plugins/analysis-icu/plugin-descriptor.properties&lt;/b&gt;
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:111)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:146)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
    at org.elasticsearch.bootstrap.ElasticsearchF.main(ElasticsearchF.java:30)
Caused by: java.nio.file.NoSuchFileException: /home/harish/Documents/code/elasticsearch/es/fork-new/elasticsearch/plugins/analysis-icu/plugin-descriptor.properties
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.Files.newByteChannel(Files.java:407)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
    at java.nio.file.Files.newInputStream(Files.java:152)
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
    at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:318)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:108)
    ... 5 more

&lt;/code&gt;

also error indicates it is looking for elasticsearch/plugins/analysis-icu/plugin-descriptor.properties
also saw the code here in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java#L82-L87

and there is a check for plugin descriptor. But earlier it used to work without myself having to do anything. Did anything change in between to include plugins install and hence it is looking for plugin descriptor file ? 
</description><key id="99792790">12737</key><summary>Unable to start ElasticsearchF.java in eclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HarishAtGitHub</reporter><labels /><created>2015-08-08T10:54:15Z</created><updated>2015-08-09T12:14:11Z</updated><resolved>2015-08-08T11:23:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-08T11:23:32Z" id="128968578">This is not a supported way to run elasticsearch. 
</comment><comment author="HarishAtGitHub" created="2015-08-09T05:50:26Z" id="129126234">@rmuir but pls can u at least give me some clue why it was working previously but not now ... this clue can help me debug ...
</comment><comment author="dadoonet" created="2015-08-09T06:03:04Z" id="129127118">Try to start the Bootstrap class instead.
</comment><comment author="HarishAtGitHub" created="2015-08-09T08:06:36Z" id="129135137">@dadoonet,
I tried to do that too ...
initially I got error as follows
&lt;code&gt;
java.lang.IllegalStateException: failed to load bundle [] due to jar hell
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:360)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:109)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:146)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
    at org.elasticsearch.bootstrap.ElasticsearchF.main(ElasticsearchF.java:30)
Caused by: java.security.AccessControlException: access denied ("java.io.FilePermission" "/home/harish/Documents/code/elasticsearch/es/fork2-new/elasticsearch/core/eclipse-build" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
    at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
    at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
    at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
    at java.nio.file.Files.readAttributes(Files.java:1737)
    at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
    at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322)
    at java.nio.file.Files.walkFileTree(Files.java:2662)
    at java.nio.file.Files.walkFileTree(Files.java:2742)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:123)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:358)
    ... 6 more
Exception in thread "main" java.lang.IllegalStateException: failed to load bundle [] due to jar hell
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:360)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:109)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:146)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
    at org.elasticsearch.bootstrap.ElasticsearchF.main(ElasticsearchF.java:30)
Caused by: java.security.AccessControlException: &lt;b&gt;access denied ("java.io.FilePermission" "/home/harish/Documents/code/elasticsearch/es/fork2-new/elasticsearch/core/eclipse-build" "read")&lt;/b&gt;
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
    at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
    at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
    at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
    at java.nio.file.Files.readAttributes(Files.java:1737)
    at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
    at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322)
    at java.nio.file.Files.walkFileTree(Files.java:2662)
    at java.nio.file.Files.walkFileTree(Files.java:2742)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:123)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:358)
    ... 6 more

&lt;/code&gt;

I get above strange errors and it is due to this line https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/PluginsService.java#L358
.
These errors appeared strange because a week ago I checkedout code
and it just worked amazingly as expected .. but I don't know what happened all of  sudden.
so I just wanted to check if I can proceed if I bypass the error by commenting that line.
but I then landed up in this exact error which someone else too caught ..pls see this http://stackoverflow.com/questions/30837055/elastic-search-error .

ok some things is strange .... 
if this is so strange and it is not worth going further in this direction, at least tell me how you guys debug elastic search code in eclipse.
I can set a debug point by doing a remote debug in eclipse. for that I need to run elastic search in debug mode. But I don't know where it is to set the debug = yes and port on which to java vm runs so that I can add that port to eclipse ... I searched in config too ..
can you point me to some docs or wiki . That would be great help ..
</comment><comment author="clintongormley" created="2015-08-09T09:02:14Z" id="129142545">Hi @HarishAtGitHub 

I run ES in Eclipse with these VM arguments:

```
-Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=logs/heapdump.hprof -Delasticsearch -Des.foreground=yes -ea -Des.path.home=/Users/clinton/workspace/elasticsearch/core/ -Des.security.manager.enabled=false -Des.http.cors.enabled=true -Des.http.cors.allow-origin=*
```
</comment><comment author="dadoonet" created="2015-08-09T09:15:05Z" id="129143704">@clintongormley i think we should add this info to the TESTING doc. WDYT?
</comment><comment author="HarishAtGitHub" created="2015-08-09T09:39:04Z" id="129145190">Oh my god....  that was  really great help @clintongormley ....
-Des.security.manager.enabled=false is the one that did the magic ...
Thank you very much.
so because by default it was true ,, all the the JarHell.checkJarHell as in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/PluginsService.java#L358  was failing .

Thanks @clintongormley and @dadoonet .

Learning:
But this gave an opportunity to know about [JarHell](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java).. I was trying to add duplicate jars and it just caught it.. Super... . I have seen places where duplicate jars got into the system by plugins(especially logger jars) (eg in jenkins, nexus oss) gave errors which were hard to debug. This JarHell seems like a good solution for such problems . JarHell is amazing.
</comment><comment author="dadoonet" created="2015-08-09T09:42:23Z" id="129145286">&gt; JarHell is amazing.

I do agree ! 
</comment><comment author="clintongormley" created="2015-08-09T10:00:38Z" id="129150165">&gt; i think we should add this info to the TESTING doc. WDYT?

@dadoonet I agree - want to add it?
</comment><comment author="HarishAtGitHub" created="2015-08-09T10:12:04Z" id="129150672">Or shall we do it in [Contributing.md](https://github.com/elastic/elasticsearch/edit/master/CONTRIBUTING.md) . Because that is the one developers who are interested in debugging will look into first(as per unwritten convention).

Shall I do it and post a pull request if you say 'yes' ?

have a topic like "Debugging Elasticsearch codebase in Eclipse" ??
</comment><comment author="dadoonet" created="2015-08-09T10:16:15Z" id="129150886">Just added info here: https://github.com/elastic/elasticsearch/commit/2b9ef26006c0e4608110164480b8127dffb9d6ad
</comment><comment author="HarishAtGitHub" created="2015-08-09T10:30:37Z" id="129158626">Thank you very much. That information is neat.
</comment><comment author="HarishAtGitHub" created="2015-08-09T10:36:22Z" id="129162028">the reason why I was getting the Exception in thread "main" java.lang.IllegalStateException: java.nio.file.NoSuchFileException: /home/harish/Documents/code/elasticsearch/es/fork-new/elasticsearch/plugins/analysis-icu/plugin-descriptor.properties was that because I was having the
es.path.home= elastic search root folder. 
as there were plugins inside the plugins folder it was searching for properties files and it was failing.
and also it was looking for the "config" folder which was not there .

ya &lt;b&gt;elasticsearch-root-folder/core&lt;/b&gt; is the right value as @clintongormley  pointed out . it had a config with elasticsearch.yml and a plugins folder that was empty .

just documenting this so that no one else faces this problem .
</comment><comment author="rmuir" created="2015-08-09T12:14:10Z" id="129173896">You need to run ./run.sh

It is the only supported way to do this, because it runs bin/elasticsearch

We cannot even keep bin/elasticsearch working correctly, so we cant possible support other ways to run elasticsearch. I don't give any fucks if we supported 18 different ways to run ES before, we do not anymore.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix geo_bounds aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12736</link><project id="" key="" /><description>geo_bounds aggregator was returning "Infinity" in most cases. Looking at the code it seems like the grow array block was wrongly initializing the values.
</description><key id="99789642">12736</key><summary>Fix geo_bounds aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">codebunt</reporter><labels><label>:Aggregations</label><label>Awaiting CLA</label><label>bug</label><label>feedback_needed</label><label>review</label></labels><created>2015-08-08T10:07:41Z</created><updated>2016-03-10T11:22:54Z</updated><resolved>2016-03-10T11:22:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T08:43:24Z" id="129140879">Hi @codebunt 

Thanks for the PR.  Please could I ask you to:
- add some unit tests to demonstrate the bug that this fixes
- sign the CLA http://www.elasticsearch.org/contributor-agreement/

@colings86 could you take a look at this please?
</comment><comment author="colings86" created="2015-12-18T10:04:18Z" id="165736295">I have not been able to reproduce these problems with the geo_bounds agg returning infinity. Could you share a test case that shows the bug you are seeing?
</comment><comment author="clintongormley" created="2016-03-10T11:22:54Z" id="194799323">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove lenient store type parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12735</link><project id="" key="" /><description>Today we accept store types like `nio_fs`, `nioFs`, `niofs` etc.
this commit removes the leniency and only accepts plain values without underscore.
Yet, this commit also has a BWC layer that upgrades existing indices to the new version.
Affected enums are:
- `nio_fs`
- `mmap_fs`
- `simple_fs`
</description><key id="99717700">12735</key><summary>Remove lenient store type parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T19:50:07Z</created><updated>2015-08-10T11:40:07Z</updated><resolved>2015-08-10T11:40:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-07T19:50:24Z" id="128810803">@rjernst can you look
</comment><comment author="rjernst" created="2015-08-07T19:56:10Z" id="128815551">LGTM. Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Batch up cluster state events spawning from fault detection failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12734</link><project id="" key="" /><description>Have a scenario where a large number of new nodes were added to a 1.4.1 cluster (allocation disabled so no data is allocated to them yet) but the networking rules have not yet been set up yet (?).  This is causing fault detection pings to fail against all of these nodes.  As a resulting the pending task list was showing 70+ immediate tasks like the following (with one being executed):

```
    {
      "insert_order": 3524021,
      "priority": "IMMEDIATE",
      "source": "zen-disco-node_failed([data76][weK5x8_yT6O7CWuRaDpNGQ][data76][inet[/10.130.0.19:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout",
      "executing": true,
      "time_in_queue_millis": 72182,
      "time_in_queue": "1.2m"
    }
```

This causes other cluster state event tasks like put mapping, etc.. to fail due to a timeout.

Discussed briefly with @kimchy .  He noted that we should batch up the node-removals-due-to-ping-failures tasks.
</description><key id="99713911">12734</key><summary>Batch up cluster state events spawning from fault detection failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Cluster</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-07T19:34:20Z</created><updated>2016-01-26T18:15:17Z</updated><resolved>2016-01-26T18:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T18:15:16Z" id="175151727">Fixed in 2.2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add script compilation stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12733</link><project id="" key="" /><description>This commit adds basic support to track the number of times scripts are
compiled and compiled scripts are evicted from the script cache. These
statistics are tracked at the node level.

Closes #12673
</description><key id="99707050">12733</key><summary>Add script compilation stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T18:51:44Z</created><updated>2015-08-07T22:27:41Z</updated><resolved>2015-08-07T22:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-07T21:23:16Z" id="128836807">LGTM, this is nice to have!
</comment><comment author="jasontedor" created="2015-08-07T22:26:58Z" id="128847749">Thanks for reviewing @nik9000 and @dakrone!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A minor typo correction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12732</link><project id="" key="" /><description>Changing the name of the document is a little confusing. We are actually changing name key's value in the document.
</description><key id="99691204">12732</key><summary>A minor typo correction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ronakmshah</reporter><labels /><created>2015-08-07T17:12:22Z</created><updated>2015-08-07T17:16:39Z</updated><resolved>2015-08-07T17:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Move flag to track filter context to QueryShardContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12731</link><project id="" key="" /><description>Currently there is a flag in the QueryParseContext that keeps track of whether an inner query sits inside a filter and should therefore produce an unscored lucene query. This is done in the parseInnerFilter...() methods that are called in the fromXContent() methods or the parse() methods we haven't refactored yet. This needs to move to the toQuery() method in the refactored builders, since the query builders themselves have no information about the parent query they might be nested in.

This PR moves the `isFilter` flag from the QueryParseContext to the recently introduces QueryShardContext. The `parseInnerFilter...` methods need to stay in the QueryParseContext for now, but they already delegate to the flag that is kept in QueryShardContext. For refactored queries (like BoolQ.B.) references to `isFilter()` are moved from the parser to the corresponding builder. Builders where the inner query was previously parsed using `parseInnerFilter...()` now use a newly introduces `toFilter(shardContext)` method that produces the nested lucene query with the filter context flag switched on.

PR is against the query refactoring branch
</description><key id="99680067">12731</key><summary>Move flag to track filter context to QueryShardContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-08-07T16:13:43Z</created><updated>2016-03-11T11:51:01Z</updated><resolved>2015-08-10T13:37:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-07T16:33:20Z" id="128757905">I left a small comment, this looks great though! Thanks!
</comment><comment author="cbuescher" created="2015-08-10T10:30:52Z" id="129399859">@javanna Thanks, rebased and added the two //norelease comments and a meaningful log message. Maybe @jpountz wants to have a look since this concerns the filter/query merge an I might have missed something?
</comment><comment author="jpountz" created="2015-08-10T10:34:46Z" id="129401607">It looks good to me too
</comment><comment author="javanna" created="2015-08-10T11:09:43Z" id="129409451">LGTM too thanks @cbuescher 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shadow shards try to access translog for shadow engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12730</link><project id="" key="" /><description>This has changed recently in master, we used to not replay translogs for shadow shards, however, I get the following exception:

```
RemoteTransportException[[Umbo][inet[/127.0.0.1:9301]][indices:monitor/stats[s]]]; nested: NotSerializableExceptionWrapper[shadow engines don't have translogs];
Caused by: NotSerializableExceptionWrapper[shadow engines don't have translogs]
    at org.elasticsearch.index.engine.ShadowEngine.getTranslog(ShadowEngine.java:178)
    at org.elasticsearch.index.shard.IndexShard.translogStats(IndexShard.java:652)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:173)
    at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:54)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:192)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:56)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:270)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:266)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Caused by the exception thrown in `ShadowEngine` from:

``` java
@Override
public Translog getTranslog() {
    throw new UnsupportedOperationException("shadow engines don't have translogs");
}
```
</description><key id="99676360">12730</key><summary>Shadow shards try to access translog for shadow engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>blocker</label><label>bug</label><label>v2.0.0</label></labels><created>2015-08-07T15:51:16Z</created><updated>2015-10-08T07:43:56Z</updated><resolved>2015-10-08T07:43:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add `path.shared_data`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12729</link><project id="" key="" /><description>This allows `path.shared_data` to be added to the security manager while
still allowing a custom `data_path` for indices using shadow replicas.

For example, configuring `path.shared_data: /tmp/foo`, then created an
index with:

```
POST /myindex
{
  "index": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "data_path": "/tmp/foo/bar/baz",
    "shadow_replicas": true
  }
}
```

The index will then reside in `/tmp/foo/bar/baz`.

`path.shared_data` defaults to `${path.home}/data` if not specified.

Resolves #12714
Relates to #11065
</description><key id="99674335">12729</key><summary>Add `path.shared_data`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T15:43:57Z</created><updated>2016-03-03T19:12:01Z</updated><resolved>2015-08-12T17:06:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-10T15:50:18Z" id="129502822">@jpountz pushed new commits for your comments
</comment><comment author="dakrone" created="2015-08-10T18:02:21Z" id="129551013">@jpountz pushed a change to make `path.shared_data` null if unset and add better validation to avoid NPEs
</comment><comment author="jpountz" created="2015-08-11T13:32:48Z" id="129874687">This looks good to me, but I'd like that someone else gives it a look too. @rjernst maybe you?
</comment><comment author="rjernst" created="2015-08-12T16:45:55Z" id="130366535">LGTM too. Do you think #12776 is quick enough to get in today, so we can remove the old setting altogether for 2.x (since users will be forced to add path.shared_data anyways?)
</comment><comment author="dakrone" created="2015-08-12T16:47:05Z" id="130366805">&gt; Do you think #12776 is quick enough to get in today, so we can remove the old setting altogether for 2.x (since users will be forced to add path.shared_data anyways?)

Yeah, this should be a really quick change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain which nodes participate in master election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12728</link><project id="" key="" /><description>This commit updates the Zen Discovery documentation to explain which
nodes partcipate in master election (by default) as well as the
configuration parameters for controlling this.

Closes #12727
</description><key id="99672269">12728</key><summary>Explain which nodes participate in master election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label></labels><created>2015-08-07T15:33:28Z</created><updated>2015-08-07T22:27:34Z</updated><resolved>2015-08-07T22:18:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-07T16:26:29Z" id="128755896">tiny tweak, otherwise LGTM
</comment><comment author="jasontedor" created="2015-08-07T22:27:30Z" id="128847821">Thanks for reviewing @clintongormley and @debadair!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain which nodes participate in master election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12727</link><project id="" key="" /><description>Update the documentation to explain which nodes participate in master election and the relevant configuration parameters for controlling this. The documentation refers to "cluster nodes" but doesn't explain exactly which nodes, nor that it is configurable.

&gt; When the master node stops or has encountered a problem, the cluster nodes
&gt; start pinging again and will elect a new master.

This question came up on [#elasticsearch](https://webchat.freenode.net/#elasticsearch).
</description><key id="99671348">12727</key><summary>Explain which nodes participate in master election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label></labels><created>2015-08-07T15:30:08Z</created><updated>2015-08-07T22:18:54Z</updated><resolved>2015-08-07T22:18:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch Truck Factor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12726</link><project id="" key="" /><description>As part of my PhD research on code authorship, we calculated the Truck Factor (TF) of some popular GitHub repositories.

As you probably know, the [Truck (or Bus) Factor](https://en.wikipedia.org/wiki/Bus_factor) designates the minimal number of developers that have to be hit by a truck (or quit) before a project is incapacitated. In our work, we consider that a system is in trouble if more than 50% of its files become orphan (i.e., without a main author).

More details on our work in this preprint: https://peerj.com/preprints/1233

We calculated the TF for **Elasticsearch** and obtained a value of **2**.

The developers responsible for this TF are:

Shay Banon (kimchy) - author of 51% of the files
Simon Willnauer - author of 20% of the files

To validate our results, we would like to ask **Elasticsearch** developers the following three brief questions:

(a) Do you agree that the listed developers are the main developers of **Elasticsearch**?

(b) Do you agree that **Elasticsearch** will be in trouble if the listed developers leave the project (e.g., if they win in the lottery, to be less morbid)?

(c) Does **Elasticsearch** have some characteristics that would attenuate the loss of the listed developers (e.g., detailed documentation)?

Thanks in advance for your collaboration,

Guilherme Avelino
PhD Student
Applied Software Engineering Group (ASERG)
UFMG, Brazil
http://aserg.labsoft.dcc.ufmg.br/
</description><key id="99640498">12726</key><summary>Elasticsearch Truck Factor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gavelino</reporter><labels /><created>2015-08-07T12:43:27Z</created><updated>2015-08-26T20:12:43Z</updated><resolved>2015-08-26T20:12:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-08-07T16:14:09Z" id="128749831">Hi @gavelino, thanks for stopping by!  

Your preprint generated some interesting discussion internally when it first came out.  Knowledge sharing is something that we take seriously;  it's vital to the health of Elasticsearch that knowledge be spread around and not concentrated in a small handful of individuals.  

Especially since we are a distributed company, information sharing is something we spend a lot of time talking about and brainstorming new ways to help.  We can't simply walk over to our colleague's desk and ask a question, when they live across the ocean and in a different timezone :)

To summarize our discussions though, I don't think the calculated "truck factor" represents our project very well, for a variety of hard and soft reasons.  I realize that a lot of my complaints are due to the fact the study was automated, which necessarily limits what data you can collect.

Apologies for the long post, I come from an academic background and I miss reviewing papers :)

### Answers to your questions

I'll get these out of the way first, before diving into various complaints of the methodology.

&gt; (a) Do you agree that the listed developers are the main developers of Elasticsearch?
&gt; 
&gt; (b) Do you agree that Elasticsearch will be in trouble if the listed developers leave the project (e.g., if they win in the lottery, to be less morbid)?

To be honest, I think Shay is on the verge of no longer being a leading driver of code changes (sorry Shay!).  He obviously has huge input on major roadmap planning, and definitely lends his expertise to tricky refactoring or customer issues, but day-to-day code details have long since moved out of his hands.

IMO, the Core Contributors are: Simon, Uri, Martijn, Boaz, Adrien, Luca, Ryan, Robert, Igor, Lee, Mike.  I might be missing some, but that's off the top of my head.  If we lost them, _then_ I'd personally start to worry :)

&gt; c) Does Elasticsearch have some characteristics that would attenuate the loss of the listed developers (e.g., detailed documentation)?

We do:
- LGTM culture, where PRs require at least one review before merging... and preferably more.  We encourage junior devs (myself included) to review PRs, because it exposes them to new parts of the code
- Internal team groups, which help spread knowledge about various components, solicit reviews, get opinions/help, etc
- A newly created "Blackbelt sessions" project, where individual engineers field an hour-long Q&amp;A about a specific component of Elasticsearch, so that everyone (devs, solution architects, support, etc) can dig into the deep technical details about one sub-system.  Like most of our meetings, these are recorded and stored online for later viewing for any who missed it, or joins the company at a later date.
- A mentor program that pairs new hires with an existing engineer, to help facilitate the initial onboarding process
- Various instruments to get help: email, chat, video conferencing, an video room which people loiter in and can offer quick help, wiki, documentation in the ES repo itself
- Extensive unit and integration tests, which can provide a lot of insight into what a specific piece of code is supposed to do, the edge cases it encounters, etc.

### General Methodology Complaints

The major assumption of the paper seems to equate first authorship as the main indicator of _knowledge_.  While you would expect these to be correlated, I think the model used is too coarse to accurately reflect real development processes.
- The DOA model you are using from Murphy-Hill _et-al_ is lacking the very component that made the Murphy-Hill paper interesting: the Degree-of-interest (DOI) factor.  To quote the paper: 

&gt; [Existing approaches] ignore knowledge that is gained by a developer interacting with the code [...]

and (emphasis mine):

&gt; This study also found that other factors, such as authorship of code, should be used to **augment** DOI when attempting to gauge a developer's knowledge of the code

DOA should be used to _augment_ DOI, not stand alone as the entire metric.  Without the DOI component, the Murphy-Hill model is no better than existing systems which, as described by the original author, are not particularly good
- The weights on the original DOK model were determined via multiple linear regression.  Regression min-maxes the value of weights to fit the data points.  Obtained regression lines are no longer valid if you remove one of the weights entirely, such as removing the DOI component.  You would need to rerun the regression to find new weights.  Your DOA model is effectively modeling is this:

```
DOK = 3.293 + 1.098*FA + 0.164*DL - 0.321*ln(1+AC) + 0.19*ln(1+0)
                                                           ^
                                                           |
                         This component is basically zero  |
```

E.g. your DOA model is basically saying the Degree-of-interest for each developer is zero, which obviously would heavily skew the original Murphy-Hill model.  It simply is not legitimate to use the DOA component in isolation.
- I'm not a big fan of the original DOK model anyway.  It was "trained" with an incredibly small sample size, and only validated against two datasets.  It is very difficult to know if the model generalizes to many different development environments and projects.  Ask any engineer and they will have their own opinion about project structure, design patterns, modularity, etc.  And ask any data-scientist, and they'll tell you that small sample size almost guarantees overfitting your model which will lead to terrible predictions
- Indeed, the original DOK model only predicts file "ownership" 55% of the time.  Murphy-Hill attribute this low rate to "assignments [which] were sometimes guesses" and that after seeing the final results "they realized their assignments were likely wrong".  IMO this is a worrying sign that test data was so unreliable that opinions change after seeing the model results (which is also coincidentally a _faux pas_ in machine learning... you cannot revise your model based on the accuracy of the validation set).  So it begs the question...can training data be trusted when the data is "likely wrong" to begin with?  
- Interestingly, the 6 developers could only assign 64% of the code to being "owned" by a single developer.  Which seems to imply that a third of the codebase is not easily "assigned" to a single person, and therefore must be more collaborative (or perhaps abandoned).  You could argue these places are probably the most significant and important pieces of the code, since not a single person is responsible for it.  It also raises the question if asking who "owns" a physical file is really a useful metric?  
- The original DOK model had issues with files older than the 3-month evaluation window, which meant they lacked first-authorship information. If I had to guess, this would have skewed the regression.
- It's unclear to me if your pre-processing accounted for packages or files that had simply been moved.  E.g. a package or set of packages may be reorganized internally but remain untouched code-wise.  This gives an entirely new "first authorship" to the set of files and removes the old history of committers.
- Finally, seeing your results regarding Homebrew seems to indicate to me that the algorithm weights first authorship too heavily.  By simply including user-contributed recipes, the Homebrew metric is heavily skewed.  It is almost 100% likely that none of these external recipe authors have the knowledge or skills to contribute to meaningful development of core Homebrew code, yet your algorithm awards them a very high score.  This is not a knock against Homebrew, but rather an indicator that your algorithm might be a bit too coarse.

### Elasticsearch-specific
- As noted in the original Murphy-Hill paper, this approach ignores largely static code, such as APIs which by necessity do not change often.  Elasticsearch has a large amount of code that is effectively RESTful API boilerplate which does not change often.  I would add to this other boilerplate such as interfaces, base classes, unit tests, etc.  This code does not change often (sometimes never).  Much of this code is also very easily understood, e.g. an interface is self-descriptive.
- Your DOA model seems to place too high of an emphasis on first authorship, which negatively penalizes projects like Elasticsearch where a single author created the bulk of the original boilerplate and architectural framework.  Or projects like Cassandra which were developed internally and then open-sourced by a small handful of individuals.
- There doesn't seem to be a notion of TF-over-time, delta-TF or "momentum".  For example, Shay does indeed have a large number of commits and first-authorships as the founder.  But if you look at his commit history, it is obvious that his role in day-to-day coding has diminished _substantially_.
  ![screen shot 2015-08-07 at 11 02 48 am](https://cloud.githubusercontent.com/assets/1224228/9138769/e5bf584a-3cf3-11e5-83a8-ffa5a69c59a0.png)

Even ignoring my previous complaints over methodology, if you were to re-analyze Elasticsearch starting around 2013 (the time period when Elastic was founded as a company and developers were hired to work on ES full time), it is fairly obvious that a substantial amount of work is being done by authors who are not Shay or Simon:

![screen shot 2015-08-07 at 11 04 01 am](https://cloud.githubusercontent.com/assets/1224228/9138803/26b80a0e-3cf4-11e5-9d01-d536199819e5.png)
- Elasticsearch is in an interesting position where it is effectively a distributed wrapper around the Lucene OSS library.  A good chunk of knowledge is wrapped up in knowing how Lucene works, how it interfaces with Elasticsearch, etc.  To that end, we have 8+ Lucene developers who split their time between Elasticsearch and Lucene.  The application of this deep knowledge is not easily seen in commit charts in ES, since these low-level details tend to be surgical rather than sweeping refactoring.  E.g. when Mike joined, his work on the the ES/Lucene interface code had drastic improvements to performance and stability, but are a relatively modest amount of actual LoC changes.  And a fair portion of the work is in Lucene itself, not Elasticsearch, but is no less critical to ES for it.  To implement these changes he still needed relatively deep knowledge of the involved components, despite being first-author on almost none of the code itself.

### Conclusion

I hope my complaints come across as constructive, and not combative.  We really do take knowledge sharing seriously, and your paper sparked some more discussion about how we can improve.  But I think there are some very large flaws in the methodology that prevent it from being an accurate forecast of a project's trajectory.
</comment><comment author="gavelino" created="2015-08-12T13:31:27Z" id="130302551">Hi, polyfractal,

Thank you for your comments. We really appreciate the feedback. Our research is under development and the answers we are receiving for this survey will help to better interpret the results and improve our approach. Below, I tried to answer some of yours complains and questions.

&gt; IMO, the Core Contributors are: Simon, Uri, Martijn, Boaz, Adrien, Luca, Ryan, Robert, Igor, Lee, Mike. I might be missing some, but that's off the top of my head. If we lost them, then I'd personally start to worry :)

As you can see in list below, most of the core contributors elected by you are in the top-10 list generated by our approach. However, we just have listed the developers that represent the TF calculated.

Top-10: 
&#8226; Shay Banon  - 51%
&#8226; Simon Willnauer - 19%
&#8226; Martijn van Groningen - 7%
&#8226; uboness: - 6%
&#8226; Luca Cavanna - 6%
&#8226; Adrien Grand - 6%
&#8226; Igor Motov: - 5%
&#8226; Alexander Reelsen - 3%
&#8226; Britta Weber - 3%
&#8226; Lee Hinman - 3%

&gt; E.g. your DOA model is basically saying the Degree-of-interest for each developer is zero, which obviously would heavily skew the original Murphy-Hill model. It simply is not legitimate to use the DOA component in isolation.

We understand that the Degree-of-Interest is a importante part of the Degree of Authoship proposed by Murhy-Hill, but in the extended version of their work (http://dl.acm.org/citation.cfm?id=2600788.2512207) they comment that remotion of the DOI componente has a small impact on the weights of DOA, quoted below. As our aim is to automatically calculate the authorship for a large number of projects we had to focus only in the informations able to be recovered from CVS.

&#8220;Furthermore, applying linear regression to a model without DOI as a variable, results in weighting factors for FA, DL,and AC that are very close (on average, 4% change) to the ones from the model using DOI.&#8221;

&gt; It's unclear to me if your pre-processing accounted for packages or files that had simply been moved. E.g. a package or set of packages may be reorganized internally but remain untouched code-wise. This gives an entirely new "first authorship" to the set of files and removes the old history of committers.

In order to avoid the problem that you pointed, we use the git-log functionality to identify commits that perform renames. When this happens we merge the development history of the old and new files. That way, they are not counted as a new "first authorship".

&gt; Finally, seeing your results regarding Homebrew seems to indicate to me that the algorithm weights first authorship too heavily. By simply including user-contributed recipes, the Homebrew metric is heavily skewed. It is almost 100% likely that none of these external recipe authors have the knowledge or skills to contribute to meaningful development of core Homebrew code, yet your algorithm awards them a very high score. This is not a knock against Homebrew, but rather an indicator that your algorithm might be a bit too coarse.

Homebrew currently supports thousands of formulas, which are typically implemented by the package&#8217;s developers or users. For this reason, the system has one of the largest base of contributors on GitHub (almost 5K contributors, on July, 2015). However, if we do not consider the files in Library/Formula, HomeBrew&#8217;s Truck factor is just two. In some cases the knowledgement of particularies of the Project is necessary to filter the file history to be studied.
</comment><comment author="jpountz" created="2015-08-26T20:12:43Z" id="135157293">Thank you for bringing this up. Even though your analysis is probably right that we need to better share knowledge about this code base, I think things tend to get better over time. I'm closing this ticket but feel free to bring it up again on the forums if you want to discuss it more.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node in slow GC without obvious load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12725</link><project id="" key="" /><description>We have a cluster with 10 nodes which used to work very well. Recently, the cluster become unstable, after some debugging, we found that JVM can go into an unstable state some times without obvious load.
JVM heap usage will go up very quickly and trigger heavy GC and then go back again. This process continuous until at a certain time, the node completely hang up and causing shard rebalancing. However, sometimes, the node can "go back alive" for some seconds causing another round of shard rebalancing. Shard rebalancing continuous  until either bring the whole cluster unresponsive or calm down if one or two nodes triggers JVM OOM.
During the time, no obvious high workload, no slow query logged, only lots of lots slow GC logs.

Below are some screenshots from bigdesk. I'm not sure what I should paste him for experts, please just let me know if you want any additional information.

![image](https://cloud.githubusercontent.com/assets/12907027/9135896/8fedfdee-3d44-11e5-8f54-096872a8ad2d.png)
![image](https://cloud.githubusercontent.com/assets/12907027/9135906/a28e0016-3d44-11e5-8639-079931421205.png)
![image](https://cloud.githubusercontent.com/assets/12907027/9135909/ac9331e4-3d44-11e5-93ae-e179ffff1c02.png)
</description><key id="99640353">12725</key><summary>Node in slow GC without obvious load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">niu-lin</reporter><labels /><created>2015-08-07T12:42:03Z</created><updated>2015-08-07T13:09:38Z</updated><resolved>2015-08-07T13:03:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-07T13:03:37Z" id="128694486">Hi @niu-lin 

This is a common problem and usually has to do with setting up swap correctly (https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory) but could be due to other factors (eg hardcoding paramters into scripts, so scripts are recompiled frequently).

I suggest that you start by asking in the forum, and open a new issue if you think you've found a bug.  https://discuss.elastic.co/

thanks
</comment><comment author="niu-lin" created="2015-08-07T13:09:37Z" id="128695445">Hi @clintongormley 
Thank you for your response. I believe this has nothing to do with swap because we have disabled swapping and I verified the JVM does not use any swap space by looking at /proc/pid/status.
And for scripts, I'm not quite understand what do you mean by "hardcoding parameters". 
Anyway, I'll move to the forum. Thank you for advice.
Lin
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bats testing: Remove useless systemctl check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12724</link><project id="" key="" /><description>Bats testing uncovered a useless systemctl check, that resulted in an
error, because the systemctl file was uninstalled, but we hoped to
check for an explicetely configured SystemExitCode.

In addition we did not reload the systemctl configuration when uninstalling
elasticsearch, which now is fixed as well.

Closes #12682
</description><key id="99638279">12724</key><summary>Bats testing: Remove useless systemctl check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T12:27:09Z</created><updated>2015-08-13T14:01:59Z</updated><resolved>2015-08-07T12:44:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-07T12:29:12Z" id="128687768">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add basic tests for shaded JAR</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12723</link><project id="" key="" /><description>this commit adds a simple integration test that starts a
node from a shaded jar, indexes a doc and retrieves it. It
also has some basic unittests that try to load shaded classes and ensure
that their counterpart is not in the classpath.

Closes #12711
</description><key id="99637100">12723</key><summary>Add basic tests for shaded JAR</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T12:16:52Z</created><updated>2015-08-12T07:20:17Z</updated><resolved>2015-08-07T19:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-07T12:28:41Z" id="128687565">LGTM
</comment><comment author="s1monw" created="2015-08-07T19:51:48Z" id="128811467">@rmuir if u have a sec take a look 
</comment><comment author="rmuir" created="2015-08-07T19:54:42Z" id="128814037">+1 fantastic
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>queried words highlighting in fvh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12722</link><project id="" key="" /><description>We use fast vector highlighter

&lt;pre&gt;&lt;code&gt;
POST test
{
  "mappings": {
        "test" : {
            "_source" : { "enabled" : false },
            "properties" : {
                 "content": {
                  "term_vector": "with_positions_offsets",
                  "index_options": "offsets",
                 "type": "string",
                 "store": true,
                 "include_in_all": false
               }
            }
        }
  }
}
&lt;/code&gt;&lt;/pre&gt;


Then put a text:

&lt;pre&gt;&lt;code&gt;
POST test/test
{
  "content": "But nice it . Were the words stay hungry . Stay Foolish . It was their farewell message . As they signed off . Stay hungry . Stay Foolish . And I've always wished that . For myself . And now . As you graduate to be ."
}
&lt;/code&gt;&lt;/pre&gt;


Then search:

&lt;pre&gt;&lt;code&gt;
GET test/test/_search
{
    "highlight": {
        "fields": {
            "content": {
                "number_of_fragments": 1000,
                "fragment_size": 30
            }
        },
        "pre_tags": ["&lt;b&gt;"],
        "post_tags": ["&lt;\/b&gt;"],
        "encoder": "html"
    },
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "fields": [["content^1"]],
                    "default_operator": "AND",
                    "query": "\"stay\" OR \"foolish\""
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;


Result:

&lt;pre&gt;&lt;code&gt;
"highlight": {
               "content": [
                  "words &lt;b&gt;stay&lt;/b&gt; hungry . &lt;b&gt;Stay&lt;/b&gt; Foolish",
                  "off . &lt;b&gt;Stay&lt;/b&gt; hungry . &lt;b&gt;Stay&lt;/b&gt; Foolish"
               ]
            }
&lt;/code&gt;&lt;/pre&gt;

Last word "Foolish" don't have tags. I think a reason is in "fragment_size" algorithm. Elasticsearch founds snippets like "words stay hungry . Stay Fooli", then places tags, then extending to whole words and don't check, if new word is requested by query.
</description><key id="99636391">12722</key><summary>queried words highlighting in fvh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eugene-dm</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2015-08-07T12:09:22Z</created><updated>2016-11-24T19:35:45Z</updated><resolved>2016-11-24T19:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-07T12:13:51Z" id="128685227">&gt; "fields": [["content^1"]],

Why an array in an array here?
</comment><comment author="nik9000" created="2015-08-07T12:15:18Z" id="128685423">&gt; I think a reason is in "fragment_size" algorithm

This is possibly the problem here. If this is the problem its not something we're going to fix quickly.... I think the [experimental highlighter](github.com/wikimedia/search-highlighter) has a higher chance of working properly here....
</comment><comment author="nik9000" created="2015-08-07T12:26:34Z" id="128687184">Curl recreation: https://gist.github.com/nik9000/7c87e395d82565180390
</comment><comment author="eugene-dm" created="2015-08-07T12:58:44Z" id="128693524">Sorry, experimental plugin works like a charm.
Big thank you
</comment><comment author="nik9000" created="2015-08-07T13:00:00Z" id="128693718">Sure!

I'm sorry you had trouble with the others.

On Fri, Aug 7, 2015 at 8:58 AM, eugene-dm notifications@github.com wrote:

&gt; Sorry, experimental plugin works like a charm.
&gt; Big thank you
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12722#issuecomment-128693524
&gt; .
</comment><comment author="eugene-dm" created="2015-08-27T10:16:38Z" id="135371771">Unfortunately experimental highlighter doesn't work correctly with phrases search, so waiting for resolution of this bug in core of elasticsearch.
</comment><comment author="clintongormley" created="2016-11-24T19:35:45Z" id="262836015">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove wares transport plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12721</link><project id="" key="" /><description>We think that the transport-wares plugin is probably little used and doesn't have a real purpose, so we are thinking of removing this plugin.

Do you have a use case? If so, please tell us about it?

Pinging people who have starred the repo:  @allenhu, @aponb, @AshwinJay, @avengerpenguin, @boyce-ywr, @cabadsanchez, @carchrae, @ccw-morris, @cpliakas, @cwensel, @cythrawll, @d1rtym0nk3y, @davidmr001, @dlobue, @dmitrygusev, @egaumer, @gaurav46, @guanyum, @gustaveiffel, @jeffreyaikin, @ker2x, @ksclarke, @kyoken74, @lukas-vlcek, @maonsbatsis, @mhvenkat, @nedgot, @netconstructor, @paikan, @pfiled, @philshon, @pkadetiloye, @rseixas, @seralf, @ssp, @voodoorider, @whateverdood, @zepag
</description><key id="99616131">12721</key><summary>Remove wares transport plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugin Transport Wares</label><label>deprecation</label><label>discuss</label></labels><created>2015-08-07T09:42:36Z</created><updated>2016-09-27T15:35:02Z</updated><resolved>2016-09-27T15:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seralf" created="2015-08-07T09:49:19Z" id="128659300">I've tested mainly for developing self-contained prototypes

Alfredo

2015-08-07 11:43 GMT+02:00 Clinton Gormley notifications@github.com:

&gt; We think that the transport-wares plugin is probably little used and
&gt; doesn't have a real purpose, so we are thinking of removing this plugin.
&gt; 
&gt; Do you have a use case? If so, please tell us about it?
&gt; 
&gt; Pinging people who have starred the repo: @allenhu
&gt; https://github.com/allenhu, @aponb https://github.com/aponb,
&gt; @AshwinJay https://github.com/AshwinJay, @avengerpenguin
&gt; https://github.com/avengerpenguin, @boyce-ywr
&gt; https://github.com/boyce-ywr, @cabadsanchez
&gt; https://github.com/cabadsanchez, @carchrae https://github.com/carchrae,
&gt; @ccw-morris https://github.com/ccw-morris, @cpliakas
&gt; https://github.com/cpliakas, @cwensel https://github.com/cwensel,
&gt; @cythrawll https://github.com/cythrawll, @d1rtym0nk3y
&gt; https://github.com/d1rtym0nk3y, @davidmr001
&gt; https://github.com/da%20vidmr001, @dlobue https://github.com/dlobue,
&gt; @dmitrygusev https://github.com/dmitrygusev, @egaumer
&gt; https://github.com/egaumer, @gaurav46 https://github.com/gaurav46,
&gt; @guanyum https://github.com/guanyum, @gustaveiffel
&gt; https://github.com/gustaveiffel, @jeffreyaikin
&gt; https://github.com/jeffreyaikin, @ker2x https://github.com/ker2x,
&gt; @ksclarke https://github.com/ksclarke, @kyoken74
&gt; https://github.com/kyoken74, @lukas-vlcek
&gt; https://github.com/lukas-vlcek, @maonsbatsis, @mhvenkat
&gt; https://github.com/mhvenkat, @nedgot https://github.com/nedgo%20t,
&gt; @netconstructor https://github.com/netconstructor, @paikan
&gt; https://github.com/paikan, @pfiled https://github.com/pfiled,
&gt; @philshon https://github.com/philshon, @pkadetiloye
&gt; https://github.com/pkadetiloye, @rseixas https://github.com/rseixas,
&gt; @seralf https://github.com/seralf, @ssp https://github.com/ssp,
&gt; @voodoorider https://github.com/voodoorider, @whateverdood
&gt; https://github.com/whateverdood, @zepag https://github.com/zepag
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12721.
</comment><comment author="clintongormley" created="2015-08-07T09:51:42Z" id="128659628">@seralf so you're saying that you wouldn't miss it?
</comment><comment author="seralf" created="2015-08-07T10:17:11Z" id="128665764">I didn't use it very often to be honest

but I think it could be extended for prototyping or to be used as an
example on how to create plugins in itself, I'm not sure about the benefit
of completely removing it, while it will continue work. Just an idea

2015-08-07 11:52 GMT+02:00 Clinton Gormley notifications@github.com:

&gt; @seralf https://github.com/seralf so you're saying that you wouldn't
&gt; miss it?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12721#issuecomment-128659628
&gt; .
</comment><comment author="cpliakas" created="2015-08-07T11:39:35Z" id="128680668">Thanks for pinging. Plugin seemed cool to work ES in to an existing architecture, however that architecture has changed so we do not have a need for this plugin.
</comment><comment author="SMUnlimited" created="2015-08-10T10:12:37Z" id="129396163">We currently use this to abstract away the elasticsearch configuration away into a utility webapp. This webapp also contains all our business REST API logic for interacting with elasticsearch. We setup NodeServlet so that users can still make direct elasticsearch rest calls if there is something missing.
</comment><comment author="carchrae" created="2015-08-10T14:36:47Z" id="129476683">a few years ago, i adapted this code to work on playframework (https://github.com/carchrae/elastic-play), it was very helpful to have this example of the elasticsearch processing api.   however, i'm not using that right now.  the motivation for that was being able to elasticsearch and a server on a heroku instance with S3 for persistent storage.
</comment><comment author="roytmana" created="2015-08-19T17:31:37Z" id="132707658">@clintongormley is the plugin the same as transport-wares project that allows embedding elastic into a servlet application? I assume it is not the same but if it is, I would like to keep it very much as we and quite a few people I know use it for small embedded single node elastic deployments
</comment><comment author="dadoonet" created="2015-09-07T07:31:57Z" id="138215340">@clintongormley What is the status on this?
</comment><comment author="benoitdechateauvieux" created="2015-10-30T08:13:45Z" id="152456964">@clintongormley we use the transport-wares plugin to start our embedded ES instance when our application starts.
Is there another way to do it ? If not, could you plan to release a version of this plugin for ES 2.0 ?
</comment><comment author="roytmana" created="2015-11-09T22:59:29Z" id="155226077">+1
</comment><comment author="wasifaleem" created="2015-11-12T20:33:46Z" id="156225966">We use it to add strict role based authentication and authorization.
</comment><comment author="apatrida" created="2015-11-16T16:57:30Z" id="157096619">We used it to proxy and limit or modify queries as they passed through while allowing others to invisible "just be handled"   but that code is old and in use, but not sure would continue in the same way in the future.
</comment><comment author="HardyLoppmann" created="2015-11-23T19:33:25Z" id="159038794">Hi, I&#180;m interested at these plugin, too but I&#180;ve got another reason. We are implementing a full text search for a huge customer, which has a difficult approve process for products like elasticsearch. If you want to use a new product or self hosted service you have to fill about ~1000 documents ;-) If you use a already approved servlet engine like a tomcat, jboss or weblogic you can elasticsearch without any problems. At the moment I use ES 1.7.0
</comment><comment author="roytmana" created="2015-11-23T23:27:58Z" id="159102695">Exactly the same here. Approved platforms and mountains of red tape and paperwork for anything else particularly security aspects
</comment><comment author="ArunYokesh1" created="2015-11-27T13:08:50Z" id="160136262">Same here. Organisational procedures , get an elasticsearch cluster as a standalone app in my current organisation is big and hard to realise soon.

 Is there any plan to release a package supporting ES 2.0.0 ? 
</comment><comment author="thomasdelhomenie" created="2016-03-17T11:41:20Z" id="197840750">What is the status?
We are using this plugin to start our embedded ES instance when our application starts. We are using ES 1.7 and we plan to upgrade to 2.2.
</comment><comment author="sDaniel" created="2016-04-16T16:13:29Z" id="210848283">"Introduce a new standalone technology? Sure lets discuss this for a year or two then maybe we'll think about it." But "Deploy yet another war? Fill out standard form and you are good to go." 

It can be a good way to get started and then later you can go "It really would be better to have normal  cluster setup."

This "plugin" could be a good starting point for some organisations that won't use ES otherwise but lacks documentation - see also a ready to use version of the plugin would help here: https://github.com/elastic/elasticsearch-transport-wares/pull/8
</comment><comment author="manosbatsis" created="2016-05-04T16:04:23Z" id="216913281">We also use this plugin to embed an elasticsearch node to a number of apps for clients. Is this going to support 2.2.0?
</comment><comment author="dakrone" created="2016-09-27T15:32:54Z" id="249901873">Given that we are moving towards a non-embeddable version of Elasticsearch (see: https://www.elastic.co/blog/elasticsearch-the-server), I am going to close this since it seems like we are not going to maintain elasticsearch-transport-wares.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Harden PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12720</link><project id="" key="" /><description>Plugin manager should be run as root to install/update/remove plugins.  We should add restrictions with the security manager to limit the scope of what plugin manager can do. Needs different dynamic rules (e.g. write access)
</description><key id="99611712">12720</key><summary>Harden PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2015-08-07T09:13:42Z</created><updated>2015-08-10T10:35:00Z</updated><resolved>2015-08-10T10:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-10T10:34:59Z" id="129401738">Closing in favour of #12768
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix cloud plugins to have real mocks for tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12719</link><project id="" key="" /><description>- These only have shallow tests and are fragile. we rely on users to report bugs
- need disco test in qa/ with mock ec2
- need snapshot test in qa/ with mock s3
- ideally use proxy too for testing that
</description><key id="99611313">12719</key><summary>Fix cloud plugins to have real mocks for tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>test</label></labels><created>2015-08-07T09:10:09Z</created><updated>2016-04-13T07:31:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-12T14:18:04Z" id="130319129">@jasontedor please could you try to tackle this one
</comment><comment author="dadoonet" created="2015-08-12T14:24:56Z" id="130320925">@clintongormley @jasontedor FYI I'm currently working on ec2 mock
</comment><comment author="s1monw" created="2015-08-12T14:48:25Z" id="130328105">@dadoonet can you sync with @jasontedor on this please
</comment><comment author="dadoonet" created="2016-04-13T07:31:43Z" id="209276479">For the record, @ywelsch just added some new tests for EC2 with #17677 
@s1monw already added tests for GCE with #16881
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend integration testing to multi-node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12718</link><project id="" key="" /><description>- needed to test discovery, back compat, etc
- tricky, we need to keep integ testing RELIABLE and SIMPLE
- integ testing is immature/WIP
</description><key id="99611132">12718</key><summary>Extend integration testing to multi-node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-07T09:08:37Z</created><updated>2015-09-14T17:17:14Z</updated><resolved>2015-08-11T10:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Install and test core plugins in vagrant tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12717</link><project id="" key="" /><description>Vagrant tests should install and use all core plugins.
</description><key id="99610395">12717</key><summary>Install and test core plugins in vagrant tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>test</label></labels><created>2015-08-07T09:04:03Z</created><updated>2015-09-13T21:56:30Z</updated><resolved>2015-09-13T21:56:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-24T20:09:33Z" id="134362416">Ok -I have a pull request to install all the plugins in vagrant - after that is merged I could try starting with them all installed an running smoke tests **but** I think we should think about time here - it already takes 7 minutes to go through these tests on two operating systems. Could we get away with just verifying that the plugin installer puts the files in the right spots and do some kind of amalgamated smoke test of all the plugins in another module in the qa module?
</comment><comment author="nik9000" created="2015-09-09T12:30:38Z" id="138894878">&gt; Ok -I have a pull request to install all the plugins in vagrant - after that is merged I could try starting with them all installed an running smoke tests but I think we should think about time here - it already takes 7 minutes to go through these tests on two operating systems. Could we get away with just verifying that the plugin installer puts the files in the right spots and do some kind of amalgamated smoke test of all the plugins in another module in the qa module?

Update: I opened #13258 to start Elasticsearch with all the plugins installed so we can verify that they are listed in `_cat/plugins` at least.
</comment><comment author="nik9000" created="2015-09-13T21:56:30Z" id="139921895">&gt; Update: I opened #13258 to start Elasticsearch with all the plugins installed so we can verify that they are listed in _cat/plugins at least.

And I merged that. So this is closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch ubuntu init script doesn't prevent to start 2 elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12716</link><project id="" key="" /><description>On ubuntu 14.04 for Elasticsearch 1.7.0 debian package,
If we start twice elasticsearch very shortly, the pidofproc function doesn't return the PID of the first elasticsearch.
So 2 elasticsearch can be started at the same time. In my case this causes an OOM trigger. 

How to reproduce:

```
# service elasticsearch stop
# service elasticsearch start; service elasticsearch start; 
```
</description><key id="99608655">12716</key><summary>Elasticsearch ubuntu init script doesn't prevent to start 2 elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rvrignaud</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2015-08-07T08:53:46Z</created><updated>2017-01-12T10:37:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-07T11:31:07Z" id="128679553">Hi @rvrignaud 

As a workaround, you can set this in your elasticsearch.yml:

```
node.max_local_storage_nodes: 1
```
</comment><comment author="rvrignaud" created="2015-08-07T11:50:41Z" id="128681997">Hey @clintongormley,
I already have max_local_storage_nodes set to 1 but that doesn't prevent the second java process to start and to allocate memory (XMS and XMX setted to the same value).
</comment><comment author="electrical" created="2015-08-27T13:38:57Z" id="135433020">With the rpm init script we place a lockfile before starting ES and remove it afterwards.
If someone would try a second startup it would find the lockfile and abort directly.
It should be a fairly easy fix to implement this in the ubuntu/debian init script as well.
Since more things are moving to systemd this will not pose an issue later.
</comment><comment author="jordansissel" created="2016-06-09T04:55:13Z" id="224799775">I think the problem here is these two lines of the init script:
- [start-stop-daemon is given `-b` flag](https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L140)
- [elasticsearch is given -d flag](https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L82)

Given the above, I think Elasticsearch is backgrounding itself (`elasticsearch -d`) and `start-stop-daemon` is also forking, so I _think_ start-stop-daemon is tracking the pid of the parent Elasticsearch process which immediately dies after Elasticsearch daemonizes.

My suggestion is that one of the two things are changed:
- Don't give `-b` to start-stop-daemon
- or, don't give `-d` to elasticsearch.
</comment><comment author="jordansissel" created="2016-06-09T04:59:01Z" id="224800170">I think this is a dup of https://github.com/elastic/elasticsearch/issues/8796
</comment><comment author="dakrone" created="2016-09-27T15:33:38Z" id="249902125">We now default `node.max_local_storage_nodes` to 1, so closing this.
</comment><comment author="rvrignaud" created="2016-09-27T16:03:06Z" id="249911788">@dakrone as replied here https://github.com/elastic/elasticsearch/issues/12716#issuecomment-128681997 `node.max_local_storage_nodes` set to 1 does not fix the problem. IMHO this should be kept open.
</comment><comment author="dakrone" created="2016-09-27T16:16:03Z" id="249915564">@rvrignaud ahh I see, my apologies! I think this is related to the daemonization that we do and would have to be fixed there then, I'm re-opening this. Thanks for catching this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add "name" to the plugin descriptor file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12715</link><project id="" key="" /><description>Add "name" to plugin descriptor file, not what user provides

To be sane, probably needs to _not_ have `elasticsearch-`. Another painful artifact renaming? (think difficult build issues)
</description><key id="99608514">12715</key><summary>Add "name" to the plugin descriptor file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2015-08-07T08:52:57Z</created><updated>2015-08-24T12:06:51Z</updated><resolved>2015-08-20T09:32:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-07T12:18:29Z" id="128685978">possible solution here: open the zip archive of the specified location, peek into the descriptor file and extract the name out of it?
</comment><comment author="cbuescher" created="2015-08-10T15:53:51Z" id="129504115">I opened #12775 that adds a new mandatory property `name` to the plugin descriptor file which will overwrite the name that gets passed to the plugin CLI as an argument. See PR for some open questions I have around the overall goal and what this means for using the command line tool.
</comment><comment author="cbuescher" created="2015-08-19T10:59:52Z" id="132540563">Merged #12775 with master, @clintongormley let me know if an when this should go to the release branch. It should be backported together with #12879 in this case.
</comment><comment author="cbuescher" created="2015-08-20T09:32:03Z" id="132953018">Also merged #12775 with 2.0 (d236192c49a8941525e201e01edc781e5fc13dae). @clintongormley I think I can close this? Otherwise please reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix custom data paths to work with security manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12714</link><project id="" key="" /><description>Custom data paths in shadow replicas will not work with the security manager today.
</description><key id="99608230">12714</key><summary>Fix custom data paths to work with security manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Shadow Replicas</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T08:51:12Z</created><updated>2015-09-17T09:22:51Z</updated><resolved>2015-08-12T17:06:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Test command line tools with spaces in paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12713</link><project id="" key="" /><description /><key id="99607754">12713</key><summary>Test command line tools with spaces in paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>blocker</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-08-07T08:47:55Z</created><updated>2015-09-14T17:17:14Z</updated><resolved>2015-08-07T10:53:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-07T10:53:52Z" id="128671927">Closed in favour of #12710
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test command line tools with custom paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12712</link><project id="" key="" /><description>Test commandline tools with custom paths in qa/
- Especially stuff like custom plugin path
</description><key id="99606987">12712</key><summary>Test command line tools with custom paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>blocker</label><label>test</label><label>v2.0.0</label></labels><created>2015-08-07T08:42:36Z</created><updated>2015-10-07T08:41:34Z</updated><resolved>2015-10-07T08:41:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-07T12:46:05Z" id="128691413">I ran the following as a manual test to test the beta and also to start recording tests that we might want to have for this when we write an automated test to cover it. The test was run on a 2.0.0-beta1-snapshot build with no configuration changes at all (literally unzip and run).

### Installing a plugin to a custom path

ran:

```
bin/plugin -Des.path.plugins=/path/to/custom/plugins install scirpt-python -u file:/path/to/elasticsearch-master/plugins/lang-python/target/releases/elasticsearch-lang-python-2.0.0-beta1-SNAPSHOT.zip
```

output on command line was:

```
-&gt; Installing scirpt-python...
Trying file:/path/to/elasticsearch-master/plugins/lang-python/target/releases/elasticsearch-lang-python-2.0.0-beta1-SNAPSHOT.zip ...
Downloading ............................................................................................................................................................................................................................................................................................................................................DONE
PluginInfo{name='scirpt-python', description='The Python language plugin allows to have python as the language of scripts to execute.', site=false, jvm=true, classname=org.elasticsearch.plugin.python.PythonPlugin, isolated=true, version='2.0.0-beta1-SNAPSHOT'}
Installed scirpt-python into /path/to/custom/plugins/scirpt-python
```

`ls /path/to/custom/plugins/scirpt-python` gives: 

`elasticsearch-lang-python-2.0.0-beta1-SNAPSHOT.jar jython-standalone-2.7.0.jar                        plugin-descriptor.properties`

SUCCESSFUL
</comment><comment author="clintongormley" created="2015-08-07T12:48:42Z" id="128691792">@colings86 probably need to test custom logs/config/scripts/repo paths as well
</comment><comment author="colings86" created="2015-08-07T12:49:23Z" id="128691880">yup, getting to that now. Posting one test at a time ;)
</comment><comment author="colings86" created="2015-08-07T12:55:27Z" id="128692834">### Listing plugins in a custom directory

Ran:

```
bin/plugin -Des.path.plugins=/path/to/custom/plugins list
```

output:

```
Installed plugins in /path/to/custom/plugins:
    - scirpt-python
```

SUCCESSFUL

### Removing a plugin using a custom plugins path

ran:

```
bin/plugin -Des.path.plugins=/path/to/custom/plugins remove scirpt-python
```

output:

```
-&gt; Removing scirpt-python...
Removed scirpt-python
```

`/path/to/custom/plugins` directory no longer contained `scirpt-python` directory

SUCCESSFUL
</comment><comment author="colings86" created="2015-08-07T13:08:00Z" id="128695205">## bin/plugin with custom config directory

For this test I created a conf directory at `/path/to/custom/config` and copied the default elasticsearch.yml file changing it only to add `path.plugins: /path/to/custom/plugins`

### Installing a plugin

ran:

```
bin/plugin -Des.path.conf=/path/to/custom/config install script-python -u file:/path/to/elasticsearch-master/plugins/lang-python/target/releases/elasticsearch-lang-python-2.0.0-beta1-SNAPSHOT.zip
```

output:

```
-&gt; Installing script-python...
Trying file:/path/to/elasticsearch-master/plugins/lang-python/target/releases/elasticsearch-lang-python-2.0.0-beta1-SNAPSHOT.zip ...
Downloading ............................................................................................................................................................................................................................................................................................................................................DONE
PluginInfo{name='script-python', description='The Python language plugin allows to have python as the language of scripts to execute.', site=false, jvm=true, classname=org.elasticsearch.plugin.python.PythonPlugin, isolated=true, version='2.0.0-beta1-SNAPSHOT'}
log4j:WARN No appenders could be found for logger (org.elasticsearch.bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Installed script-python into /path/to/custom/plugins/script-python
```

Confirmed `script-python` directory added to `/path/to/custom/plugins`

SUCCESSFUL

### Listing plugins

ran:

```
bin/plugin -Des.path.conf=/path/to/custom/config list
```

output:

```
Installed plugins in /path/to/custom/plugins:
    - script-python
```

SUCCESSFUL

### Remove plugins

ran:

```
bin/plugin -Des.path.conf=/path/to/custom/config remove script-python
```

output:

```
-&gt; Removing script-python...
Removed script-python
```

Confirmed `script-python` directory removed from `/path/to/custom/plugins`

SUCCESSFUL
</comment><comment author="colings86" created="2015-08-07T13:16:05Z" id="128696412">## bin/plugin with custom log directory

### Installing a plugin

ran:

```
bin/plugin -Des.path.logs=/path/to/custom/logs install script-python -u file:/path/to/elasticsearch-master/plugins/lang-python/target/releases/elasticsearch-lang-python-2.0.0-beta1-SNAPSHOT.zip
```

Confirmed `script-python` directory added to `/path/to/custom/plugins` and `/path/to/custom/logs` directory created with the following files:

```
elasticsearch.log                        elasticsearch_deprecation.log            elasticsearch_index_indexing_slowlog.log elasticsearch_index_search_slowlog.log
```

Also confirmed default logs directory was not created.

SUCCESSFUL

### Listing plugins

ran:

```
bin/plugin -Des.path.logs=/path/to/custom/logs list
```

output:

```
Installed plugins in /path/to/custom/plugins:
    - script-python
```

Confirmed `/path/to/custom/logs` directory created with the following files:

```
elasticsearch.log                        elasticsearch_deprecation.log            elasticsearch_index_indexing_slowlog.log elasticsearch_index_search_slowlog.log
```

Also confirmed default logs directory was not created.

SUCCESSFUL

### Remove plugins

ran:

```
bin/plugin -Des.path.logs=/path/to/custom/logs remove script-python
```

output:

```
-&gt; Removing script-python...
Removed script-python
```

Confirmed `script-python` directory removed from `/path/to/custom/plugins` and `/path/to/custom/logs` directory created with the following files:

```
elasticsearch.log                        elasticsearch_deprecation.log            elasticsearch_index_indexing_slowlog.log elasticsearch_index_search_slowlog.log
```

Also confirmed default logs directory was not created.

SUCCESSFUL
</comment><comment author="spinscale" created="2015-08-11T15:11:05Z" id="129924100">how exactly should this be implemented for automated tests? Would we have another project in `qa/` that installs/removes plugins with custom paths using custom `./integration-tests.xml` files?

Or do we have more `&lt;ant antfile` calls in the `pom.xml` of the `smoke-test-plugins` that just installs plugins into different paths?
</comment><comment author="brwe" created="2015-08-18T10:07:19Z" id="132154496">I made a pr with the first option (another project in qa/ that installs/removes plugins with custom paths) here: #12954.
I wonder if a bats test for this would be better though. Maybe @nik9000 has an opinion on this?
</comment><comment author="nik9000" created="2015-09-09T12:34:55Z" id="138895611">&gt; I wonder if a bats test for this would be better though. Maybe @nik9000 has an opinion on this?

Sorry I hadn't noticed this! Right now the bats tests are used to test `bin/plugin` and `bin/elasticsearch` with a few custom paths already so its not a huge stretch, especially when many path errors come from space escaping issues with bash. So its a reasonable choice.
</comment><comment author="brwe" created="2015-10-07T08:41:33Z" id="146119355">This has been implemented as bats test now #12717, in https://github.com/elastic/elasticsearch/pull/13258 , https://github.com/elastic/elasticsearch/pull/13772 , https://github.com/elastic/elasticsearch/pull/13617 and https://github.com/elastic/elasticsearch/pull/13640
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test shaded jar as node/transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12711</link><project id="" key="" /><description> Need integ test in qa/ (shading is complicated)
</description><key id="99606763">12711</key><summary>Test shaded jar as node/transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>blocker</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T08:40:43Z</created><updated>2015-08-07T19:55:59Z</updated><resolved>2015-08-07T19:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>use spaces liberally in integration tests and fix space handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12710</link><project id="" key="" /><description>By using pathnames with spaces in tests we can kickout all the bugs.
I applied the fix for #12709 but we needed more fixes actually.

TODO: windows
</description><key id="99587381">12710</key><summary>use spaces liberally in integration tests and fix space handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-07T06:10:31Z</created><updated>2015-08-13T14:30:03Z</updated><resolved>2015-08-11T16:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-07T06:14:38Z" id="128608908">LGTM
</comment><comment author="rmuir" created="2015-08-07T06:20:42Z" id="128609606">Everything in the root of our ES dir has a space, so a few parameters for pidfile, repo path, classpath, etc have spaces. ES home itself has a space, and cwd (which is different) has a space. bin/plugin is also fed urls with spaces in the plugin smoketester. 

I removed all of ant argline usage (except what is fed by jenkins, which is gc params etc) to reduce the possibility of bugs, it makes the testing clear.

TODO: fix any bat files for windows. Test the ES_GC_LOG_FILE by just setting one up in integ tests. I will look into these tomorrow.
</comment><comment author="jaymode" created="2015-08-07T10:53:59Z" id="128671945">+1 thank you for doing this
</comment><comment author="nik9000" created="2015-08-07T11:56:35Z" id="128682658">LGTM
</comment><comment author="rmuir" created="2015-08-07T14:40:38Z" id="128717037">See my comment, i am not sure this is really 100% correct still. try something like this:

```
diff --git a/dev-tools/src/main/resources/ant/integration-tests.xml b/dev-tools/src/main/resources/ant/integration-tests.xml
index f67701b..2b45b05 100644
--- a/dev-tools/src/main/resources/ant/integration-tests.xml
+++ b/dev-tools/src/main/resources/ant/integration-tests.xml
@@ -140,6 +140,7 @@
       &lt;attribute name="es.http.port" default="${integ.http.port}"/&gt;
       &lt;attribute name="es.transport.tcp.port" default="${integ.transport.port}"/&gt;
       &lt;attribute name="es.pidfile" default="${integ.pidfile}"/&gt;
+      &lt;attribute name="es.gc.logfile" default="${integ.scratch}/gc@{es.http.port}.log"/&gt;
       &lt;attribute name="jvm.args" default="${tests.jvm.argline}"/&gt;
     &lt;sequential&gt;

@@ -152,6 +153,7 @@
           &lt;env key="JAVA_HOME" value="${java.home}"/&gt;
           &lt;!-- we pass these as gc options, even if they arent, to avoid conflicting gc options --&gt;
           &lt;env key="ES_GC_OPTS" value="@{jvm.args}"/&gt;
+          &lt;env key="ES_GC_LOG_FILE" value="@{es.gc.logfile}"/&gt;
           &lt;arg value="-Des.cluster.name=@{es.cluster.name}"/&gt;
           &lt;arg value="-Des.http.port=@{es.http.port}"/&gt;
           &lt;arg value="-Des.transport.tcp.port=@{es.transport.tcp.port}"/&gt;
```

Problem is that JAVA_OPTS and ES_JAVA_OPTS are not handled correctly and all folded in one string i think, and need something like the previous 'eval' reparsing that was happening (but ONLY for those two, not the rest of the commandline, and hopefully cleaner). 

I am going to go for a long run today and try to catch up on some errands. If someone who actually knows bash wants to take this from me, i would be more than happy. I dont know shell, i just hack until mvn verify passes. And there is still windows to possibly fix :(

But I think we want to fix this soonish, so argument processing is really correct.
</comment><comment author="rmuir" created="2015-08-07T14:42:30Z" id="128717426">and the fastest way to iterate here is to change files then do `./run.sh` which uses all of this logic, but runs in foreground and is easy to debug.
</comment><comment author="uschindler" created="2015-08-07T14:52:17Z" id="128720760">The usual horror :) it's so simple with ant...

please fix!
</comment><comment author="rmuir" created="2015-08-07T14:59:09Z" id="128723802">Also the way ES here accepts `-D` arguments, parses them as _es_ parameters only to call System.setProperty on each one, that's totally bogus. These need to go to the JVM, or it causes very difficult-to-debug issues, like if you set -Djava.io.tmpdir=..., its not really happening until too late, and by then JVM has already initialized e.g. temp file helpers with the default tmpdir in clinits, so your system property does not have the effect you like.
</comment><comment author="rmuir" created="2015-08-07T15:32:58Z" id="128737896">for the last part, just change Bootstrap parser to require es prefix on all those -D's and fail with an exception otherwise. Then there is no confusion.
</comment><comment author="rmuir" created="2015-08-07T15:46:56Z" id="128742043">The worst is leniency, `$ bin/elasticsearch -Doops.a.typo=4` does nothing but silently work. These settings need to be in a static list somewhere. plugins can register their own lists (could be managed as e.g. properties file, maybe needs wildcard support) and we fail if its an unknown setting. 
</comment><comment author="jasontedor" created="2015-08-07T15:50:07Z" id="128744120">@rmuir There are related issues for that: https://github.com/elastic/elasticsearch/issues/6732, and https://github.com/elastic/elasticsearch/issues/12657 (the latter being closed in favor of the former).

&gt; Setting an unknown setting, or a setting that can't be changed should throw an error.
</comment><comment author="rmuir" created="2015-08-07T16:02:55Z" id="128747608">That issue is full of confusion and nonsense. It should be done in a simple way with e.g. properties file for checks (and plugins can have them too). each property (prolly needs wildcard/pattern in some case from what i see) should include its type as well (if its being accessed by getAsBoolean its a boolean, etc), and a short description. This also means its easy to do really nice stuff like list all supported properties,  "did you mean" in error messages, etc later.
</comment><comment author="clintongormley" created="2015-08-10T07:09:59Z" id="129333259">Even with this PR, wildcards are still problematic:

```
bin/elasticsearch --http.cors.enabled=true --http.cors.allow-origin='*'  -d
```

Fails with:

```
ERROR: Parameter [-d]does not start with --
```

If the `-d` comes before the wildcard, then it works
</comment><comment author="spinscale" created="2015-08-10T07:21:55Z" id="129334818">@clintongormley looks like a parser issue (independent from this PR), when regular arguments come after the `--foo.bar` style args.. investigating
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Escape CLI parameters when starting elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12709</link><project id="" key="" /><description>I'm not sure if I should do this. 

Closes #12677 
</description><key id="99545106">12709</key><summary>Escape CLI parameters when starting elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label></labels><created>2015-08-06T23:31:01Z</created><updated>2015-08-07T16:27:30Z</updated><resolved>2015-08-07T16:26:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-07T03:27:29Z" id="128578366">I actually think maybe we should switch to `\"$@\"` instead of `\"$*\"`, see http://www.bashguru.com/2009/11/how-to-pass-arguments-to-shell-script.html for the difference when quoted (particularly for files with spaces in the name).
</comment><comment author="xuzha" created="2015-08-07T03:52:40Z" id="128583889">Hmm, probably I am not quite understand. But I don't think there is any difference between `\"$*\"` and `\"$@\"`, we escape the double quotes there which will be passed into java program, and `$*` and `$@` became same. 

Try this dirty bash script 

```
#!/bin/bash

function print_args_at {
    printf "%s\n" "$@"
}

function print_args_star {
    printf "%s\n" "$*"
}

function print_args_star1 {
    printf "%s\n" \"$*\"
}

function print_args_star2 {
    printf "%s\n" \"$@\"
}

print_args_at "one" "two three" "four"
print_args_star "one" "two three" "four"
print_args_star1 "one" "two three" "four"
print_args_star2 "one" "two three" "four"
```
</comment><comment author="rmuir" created="2015-08-07T04:55:06Z" id="128595569">I am working to make integration tests always use spaces in directory names (some fixes are needed) to blast out these bugs. But I agree with @dakrone here.
</comment><comment author="xuzha" created="2015-08-07T05:19:23Z" id="128601697">OK, thanks guys, updated. 
</comment><comment author="clintongormley" created="2015-08-07T11:16:18Z" id="128677704">@spinscale @xuzha is this PR related to https://github.com/elastic/elasticsearch/pull/12709 ?
</comment><comment author="spinscale" created="2015-08-07T12:35:34Z" id="128689336">Does this work for you? `./bin/elasticsearch  -Des.pidfile="/path/with space/es.pid"` - results in this error to me:

```
Exception in thread "main" java.nio.file.AccessDeniedException: /path
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
    at java.nio.file.Files.createDirectory(Files.java:674)
    at java.nio.file.Files.createAndCheckIsDirectory(Files.jav
...
```

Also, calling something like `/bin/elasticsearch --http.cors.enabled true --http.cors.allow-origin 'foo'` results in this as only a single argument is passed?

```
ERROR: Parameter [http.cors.enabled true http.cors.allow-origin foo] needs value
```
</comment><comment author="xuzha" created="2015-08-07T16:26:35Z" id="128755949">@spinscale You are right, adding \" \" make them only a single argument is passed.  But AccessDeniedException is the expected exception, right?

Anyway, Robert has a much better fix 12710. Ill just close this dumb one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warning in the docs for negative histogram values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12708</link><project id="" key="" /><description>As requested in https://github.com/elastic/elasticsearch/issues/8082#issuecomment-127962374
</description><key id="99533716">12708</key><summary>Warning in the docs for negative histogram values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">sylvinus</reporter><labels /><created>2015-08-06T22:00:16Z</created><updated>2015-08-07T11:14:49Z</updated><resolved>2015-08-07T11:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-07T11:14:49Z" id="128677295">thanks @sylvinus - updated and merged as c2f774ac57d8326d16b1c1d3008d8c973669076f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only compute scores when necessary with FiltersFunctionScoreQuery.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12707</link><project id="" key="" /><description>This was just done with FunctionScoreQuery, but FiltersFunctionScoreQuery works
in a similar way.
</description><key id="99520496">12707</key><summary>Only compute scores when necessary with FiltersFunctionScoreQuery.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T20:41:12Z</created><updated>2015-08-07T06:53:21Z</updated><resolved>2015-08-07T06:53:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-06T23:32:05Z" id="128538713">LGTM
</comment><comment author="nik9000" created="2015-08-07T01:56:29Z" id="128560462">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Let OSX build rpms for linux</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12706</link><project id="" key="" /><description>Closes #12701
</description><key id="99511326">12706</key><summary>Let OSX build rpms for linux</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T19:52:58Z</created><updated>2015-08-13T14:01:41Z</updated><resolved>2015-08-10T09:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-06T19:56:25Z" id="128486309">+1, make it platform independent.
</comment><comment author="nik9000" created="2015-08-06T19:58:53Z" id="128486749">&gt; +1, make it platform independent.

I think I declared the rpm to only work on linux.... But at least it'll build on osx. I don't know that that targetOS is anything but an inString comparison. My google-foo is failing me when I try to look that up because rpm just calls that field `os` but that appears on 12312325489134 pages about rpm already that have nothing to do with the field.
</comment><comment author="clintongormley" created="2015-08-07T10:49:43Z" id="128671429">@nik9000 looks like your PR includes a few too many commits?
</comment><comment author="nik9000" created="2015-08-07T11:51:58Z" id="128682148">&gt; @nik9000 looks like your PR includes a few too many commits?

It didn't yesterday....
</comment><comment author="nik9000" created="2015-08-07T12:57:47Z" id="128693387">&gt; It didn't yesterday....

So fixed.
</comment><comment author="nik9000" created="2015-08-10T09:26:58Z" id="129378210">Ping @spinscale  - I can't really test on my laptop without something like this.
</comment><comment author="spinscale" created="2015-08-10T09:47:00Z" id="129385316">ok, tested this

on master (using centos7 VM)

```
# rpm -i /elasticsearch/distribution/rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm
warning: /elasticsearch/distribution/rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm: Header V4 RSA/SHA1 Signature, key ID e3038d9c: NOKEY
    package elasticsearch-2.0.0-beta1_SNAPSHOT20150810093838.noarch is intended for a different operating system
```

with this PR

```
asticsearch-2.0.0-beta1_SNAPSHOT20150810093838.noarch is intended for a different operating system
[root@localhost ~]# rpm -i /elasticsearch/distribution/rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm
warning: /elasticsearch/distribution/rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm: Header V4 RSA/SHA1 Signature, key ID 4e5bfbf2: NOKEY
Creating elasticsearch group... OK
Creating elasticsearch user... OK
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service
```

Checking the arch and the OS of the RPM

```
rpm -qp --qf 'Arch:%{ARCH}\nOS:%{OS}\n'  distribution/rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm
Arch:noarch
OS:linux
```

LGTM
</comment><comment author="nik9000" created="2015-08-10T09:48:32Z" id="129385812">Rock on! Merging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>filter_path does not walk into _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12705</link><project id="" key="" /><description>GET api with filter_path does not allow a dot path into the _source.
Instead of 

.../_search?filter_path=hits.hits._source.address

I need to enter

.../_search?filter_path=hits.hits._source&amp;_source=address

Why doesn't it allow you to specify a path into _source ?
</description><key id="99511279">12705</key><summary>filter_path does not walk into _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">precbioinf</reporter><labels /><created>2015-08-06T19:52:34Z</created><updated>2015-08-07T07:28:13Z</updated><resolved>2015-08-07T07:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-07T07:28:13Z" id="128621728">@peterlimayo that's a limitation of the response filtering feature and it is related to the way elasticsearch internally returns the `_source` of documents.

The documentation may help you (see https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering):

&gt; Note that elasticsearch sometimes returns directly the raw value of a field, like the _source field. If you want to filter _source fields, you should consider combining the already existing _source parameter (see Get API for more details) with the filter_path parameter like this:
&gt; curl -XGET 'localhost:9200/_search?pretty&amp;filter_path=hits.hits._source&amp;_source=title'

I'm closing the issue since this is not a real bug, feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm and deb create scripts directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12704</link><project id="" key="" /><description>Elasticsearch will create this if it doesn't exist if it cant but because
it doesn't own /etc/elasticsearch when installed by rpm and deb it can't
create /etc/elasticsearch/scripts.

Closes #12702
</description><key id="99503874">12704</key><summary>rpm and deb create scripts directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T19:05:36Z</created><updated>2015-08-10T09:27:45Z</updated><resolved>2015-08-10T09:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-06T19:10:59Z" id="128476997">Looks good. Its awesome we can find this stuff.
</comment><comment author="nik9000" created="2015-08-06T19:13:33Z" id="128477958">I'm not holding this hostage for #12646 but I'm tempted.

Ping @spinscale as its packaging.
</comment><comment author="spinscale" created="2015-08-07T06:42:31Z" id="128613336">nice find! LGTM
</comment><comment author="spinscale" created="2015-08-07T11:17:22Z" id="128677873">merging here seems somehow borked. Also I think the bats test will fail, as it checks for a non-existing `/etc/elasticsearch` directory
</comment><comment author="nik9000" created="2015-08-07T11:50:05Z" id="128681933">&gt; Also I think the bats test will fail, as it checks for a non-existing /etc/elasticsearch directory

I'll have a look at the bats tests. I'd love to have #12646 first so I can run them more simply.
</comment><comment author="spinscale" created="2015-08-07T12:01:54Z" id="128683500">without this one, the packages wont even start I think? Doesnt it make sense to get this in first then?
</comment><comment author="nik9000" created="2015-08-07T12:03:42Z" id="128683713">&gt; without this one, the packages wont even start I think? Doesnt it make sense to get this in first then?

Whichever, honestly. I'm really confused about the contents of the pull request though.
</comment><comment author="nik9000" created="2015-08-07T12:58:48Z" id="128693539">&gt; Whichever, honestly. I'm really confused about the contents of the pull request though.

And rebasing on master and pushing again fixed the pull request issues.
</comment><comment author="nik9000" created="2015-08-10T09:27:42Z" id="129378311">Ok - merging now on the basis of @spinscale's approval three days ago - the only difference is a no-change rebase.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support jenkins randomization in integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12703</link><project id="" key="" /><description>Today the parameters like GC that are configured in jenkins (`tests.jvm.argline`) do not apply to the actual `bin/elasticsearch` instance that we start up.
</description><key id="99501479">12703</key><summary>Support jenkins randomization in integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T18:52:20Z</created><updated>2015-08-06T19:48:19Z</updated><resolved>2015-08-06T19:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-06T18:54:16Z" id="128473246">LGTM
</comment><comment author="rjernst" created="2015-08-06T19:00:22Z" id="128474534">LGTM too
</comment><comment author="rmuir" created="2015-08-06T19:48:13Z" id="128484641">here we go, lets see if it turns jenkins red.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM and DEB won't start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12702</link><project id="" key="" /><description>On master:

``` bash
$ sudo dpkg -i elasticsearch*.deb
$ sudo /etc/init.d/elasticsearch start
$ cat /var/log/elasticsearch.log
[2015-08-06 18:32:06,248][ERROR][org.elasticsearch.bootstrap] Exception
java.nio.file.AccessDeniedException: /etc/elasticsearch/scripts
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:383)
        at java.nio.file.Files.createDirectory(Files.java:630)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:734)
        at java.nio.file.Files.createDirectories(Files.java:720)
        at org.elasticsearch.bootstrap.Security.ensureDirectoryExists(Security.java:168)
        at org.elasticsearch.bootstrap.Security.addPath(Security.java:148)
        at org.elasticsearch.bootstrap.Security.createPermissions(Security.java:125)
        at org.elasticsearch.bootstrap.Security.configure(Security.java:57)
        at org.elasticsearch.bootstrap.Bootstrap.setupSecurity(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:166)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
```
</description><key id="99498647">12702</key><summary>RPM and DEB won't start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T18:36:37Z</created><updated>2015-08-10T09:27:45Z</updated><resolved>2015-08-10T09:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-06T18:41:57Z" id="128470618">the default path.scripts is path.conf/scripts. We ensure it exists so we can grant permissions to it. if path.conf is configured to not be writable by the es user, then this `scripts` directory needs to be created and exist up front. 

Note that i'm not sure how such a configuration is intended to work with plugins. Some plugins like shield install subdirectories too, e.g.  `conf/shield`. So how are users supposed to install that? Hopefully not as root.
</comment><comment author="nik9000" created="2015-08-06T18:48:38Z" id="128472031">The rpm and deb gave /etc/elasticsearch (path.conf) to root which is what I'd expect.

I have no clue what the plugins should do, but to make the rpm and deb useful again I'll create those directories when they install.
</comment><comment author="tlrx" created="2015-08-07T06:54:14Z" id="128614824">&gt; So how are users supposed to install that? Hopefully not as root.

@rmuir I think we agreed on installing plugins as root... At the same time we agreed on doing some ownership changes in #11016.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPMs build on osx won't install on centos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12701</link><project id="" key="" /><description>``` bash
[vagrant@localhost releases]$ sudo rpm -if elasticsearch-2.0.0-beta1-SNAPSHOT.rpm 
warning: elasticsearch-2.0.0-beta1-SNAPSHOT.rpm: Header V4 RSA/SHA1 Signature, key ID cd27c2b4: NOKEY
    package elasticsearch-2.0.0-beta1_SNAPSHOT20150806181247.noarch is intended for a different operating system
[vagrant@localhost releases]$ man rpm
[vagrant@localhost releases]$ sudo rpm -i --ignoreos elasticsearch-2.0.0-beta1-SNAPSHOT.rpm 
warning: elasticsearch-2.0.0-beta1-SNAPSHOT.rpm: Header V4 RSA/SHA1 Signature, key ID cd27c2b4: NOKEY
Creating elasticsearch group... OK
Creating elasticsearch user... OK
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service
[vagrant@localhost releases]$ sudo service elasticsearch start
Starting elasticsearch (via systemctl):                    [  OK  ]
[vagrant@localhost releases]$ 
```
</description><key id="99498291">12701</key><summary>RPMs build on osx won't install on centos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>build</label></labels><created>2015-08-06T18:35:00Z</created><updated>2015-08-10T09:48:36Z</updated><resolved>2015-08-10T09:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-06T18:35:53Z" id="128468805">This is master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multiple node spec does not work with _only_nodes search preference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12700</link><project id="" key="" /><description>Have 4 nodes, 3 are sharing the node attribute B, and 1 is using the node attribute A.

Per node specification (https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html#cluster-nodes), the following works: 

The following returns 3 nodes:

```
curl -XGET "http://localhost:9200/_nodes/pod:B?pretty"
```

The following returns 1 node:

```
curl -XGET "http://localhost:9200/_nodes/pod:A?pretty"
```

The following returns 4 nodes:

```
curl -XGET "http://localhost:9200/_nodes/pod:B,pod:A?pretty"
```

However, the `_only_nodes` specification for search preference does not work (https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-preference.html#search-request-preference):

```
curl -XGET "http://localhost:9200/only_node_test/_search?preference=_only_nodes:pod:B,pod:A"

{
   "error": "IllegalArgumentException[No data node with critera [pod:B,pod:A] found]",
   "status": 500
}
```

What is the right syntax for the argument to use multiple node attributes as part of _only_nodes search preference?
</description><key id="99489720">12700</key><summary>Multiple node spec does not work with _only_nodes search preference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha3</label></labels><created>2015-08-06T17:45:10Z</created><updated>2016-05-23T08:03:44Z</updated><resolved>2016-05-23T08:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-08-06T17:54:20Z" id="128458300">Seems like this affects all multiple forms of the specification, for example, the following multiple-node-id spec returns 3 nodes:

```
curl -XGET "http://localhost:9200/_nodes/Kbqa_XmMTIOlMD2iMDivRg,y4cT5bIRSliW0_UUs8OXkA,ebDNoVsMQSWdFa-UA66uTw?pretty"
```

But when used with _only_nodes search preference, it also doesn't accept it:

```
curl -XGET "http://localhost:9200/only_node_test/_search?preference=_only_nodes:Kbqa_XmMTIOlMD2iMDivRg,y4cT5bIRSliW0_UUs8OXkA,ebDNoVsMQSWdFa-UA66uTw"

{
   "error": "IllegalArgumentException[No data node with critera [Kbqa_XmMTIOlMD2iMDivRg,y4cT5bIRSliW0_UUs8OXkA,ebDNoVsMQSWdFa-UA66uTw] found]",
   "status": 500
}
```
</comment><comment author="ppf2" created="2016-05-10T17:36:32Z" id="218232776">Can we also fix this in the 2.x branch?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Test] Add random write/read errors on azure repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12699</link><project id="" key="" /><description>_From @dadoonet on February 27, 2015 22:32_

As we have in AWS plugin with elasticsearch/elasticsearch-cloud-aws#95, we should add random write and read errors while doing snapshot and restore so we can better handle issues and retry.

_Copied from original issue: elastic/elasticsearch-cloud-azure#76_
</description><key id="99486421">12699</key><summary>[Test] Add random write/read errors on azure repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Snapshot/Restore</label><label>test</label></labels><created>2015-08-06T17:27:42Z</created><updated>2015-08-11T17:50:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-06T17:27:43Z" id="128451105">Related to #51 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[azure] Discover without credentials</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12698</link><project id="" key="" /><description>_From @dadoonet on February 19, 2015 7:2_

When a Node is running Inside Azure platform, it sounds like new Azure API added by #63 supports API calls without the need to provide credentials. Let see if we can use that and simplify (a lot) the VM creation process.

**update**

Note that for now, Azure only supports authentication using:
- certificates (that's what we are using for now)
- Active Directory

See https://msdn.microsoft.com/en-us/library/azure/ee460782.aspx

Feature request opened Azure/azure-sdk-for-java#464

Another option in the feature could be using tokens instead of certificate. See https://github.com/Azure/azure-sdk-for-java/issues/456

_Copied from original issue: elastic/elasticsearch-cloud-azure#67_
</description><key id="99485728">12698</key><summary>[azure] Discover without credentials</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Discovery</label><label>:Plugin Cloud Azure</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-06T17:23:20Z</created><updated>2016-07-21T14:33:29Z</updated><resolved>2016-07-21T14:33:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-21T14:33:28Z" id="234272498">Won't be doable I guess as Azure requires credentials.
Note that we will see if it's actually possible when implementing #19146.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize AzureBlobStore#delete method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12697</link><project id="" key="" /><description>_From @dadoonet on February 17, 2015 22:16_

When we have to remove a lot of files, method `AzureBlobStore#delete` is called for each file and generates a Azure API call.

Would be nice to see if we can batch somehow those calls instead.

_Copied from original issue: elastic/elasticsearch-cloud-azure#66_
</description><key id="99485456">12697</key><summary>Optimize AzureBlobStore#delete method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label></labels><created>2015-08-06T17:21:33Z</created><updated>2016-07-21T14:28:53Z</updated><resolved>2016-07-21T14:28:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-06T17:21:33Z" id="128449555">@imotov As far as I understand the core code, I don't really think that it's doable. We need to process file deletion one by one, right?
</comment><comment author="dadoonet" created="2015-08-06T17:21:34Z" id="128449556">_From @imotov on July 29, 2015 15:25_

@dadoonet actually when we delete a lot of files, we have a list of blobs that should be deleted and we simply iterate through the list. We can easily modify the logic to pass entire list to the store.
</comment><comment author="imotov" created="2015-08-10T17:08:49Z" id="129528547">The feature in core was implemented and merged #12587. Now we need to take advantage of it in the azure plugin.
</comment><comment author="dadoonet" created="2016-07-21T14:28:53Z" id="234271005">I believe that we can close it as we merged #18813. We can revisit it if at some point we find some optimizations in Azure SDK.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve jvmcheck error failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12696</link><project id="" key="" /><description>Versions can be tricky with linux vendors and such. To help debug any possible issues, we should output a better version.

Today:

```
[elasticsearch] java.lang.RuntimeException: Java version: 1.7.0_55 suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-8024830 which can cause data corruption.
[elasticsearch] Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.
[elasticsearch] If you absolutely cannot upgrade, please add -XX:-UseSuperWord to the JAVA_OPTS environment variable.
[elasticsearch] Upgrading is preferred, this workaround will result in degraded performance.
[elasticsearch]     at org.elasticsearch.bootstrap.JVMCheck.check(JVMCheck.java:121)
[elasticsearch]     at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:270)
[elasticsearch]     at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
```

With patch:

```
java.lang.RuntimeException: Java version: Oracle Corporation 1.7.0_40 [Java HotSpot(TM) 64-Bit Server VM 24.0-b56] suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-8024830 which can cause data corruption.
Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.
If you absolutely cannot upgrade, please add -XX:-UseSuperWord to the JAVA_OPTS environment variable.
Upgrading is preferred, this workaround will result in degraded performance.
    at org.elasticsearch.bootstrap.JVMCheck.check(JVMCheck.java:121)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
```
</description><key id="99473429">12696</key><summary>Improve jvmcheck error failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T16:24:40Z</created><updated>2015-08-07T10:39:32Z</updated><resolved>2015-08-06T16:35:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-06T16:33:14Z" id="128428766">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow scripts to expose whether they use the `_score`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12695</link><project id="" key="" /><description>This commit adds a new API to allow scripts to say whether they need scores.
In practice, only the `expression` script engine makes use of it correctly,
other engines just return `true` since they can't predict whether they'll
need scores. This should make scripted aggregations and `function_query`
faster as we'll now be able to pass needsScores=false to Query.createWeight.
</description><key id="99462329">12695</key><summary>Allow scripts to expose whether they use the `_score`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T15:27:54Z</created><updated>2015-08-09T08:09:57Z</updated><resolved>2015-08-06T16:47:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-06T15:29:07Z" id="128410769">Native scripts can also make use of this optimization. But since we can't detect it like we do for `expression` scripts, they have to explicitely say whether they make use of scores or not.
</comment><comment author="rmuir" created="2015-08-06T15:34:13Z" id="128411886">Looks great. This is an important optimization to allow the whole rest of the query to be cached.
</comment><comment author="rjernst" created="2015-08-06T16:40:14Z" id="128432895">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge NamedWriteable changes from master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12694</link><project id="" key="" /><description>This PR merges recent changes from master in our query-refactoring branch. It includes changes around NamedWriteable from #12571. 

Sending it as a PR so people can have a look at it, see the impact of the changes and the changes required on our side before stuff gets in.
</description><key id="99431679">12694</key><summary>Merge NamedWriteable changes from master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label></labels><created>2015-08-06T13:16:27Z</created><updated>2015-08-07T09:24:33Z</updated><resolved>2015-08-07T09:08:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-06T14:24:18Z" id="128390057">@javanna I had a look at only the changes in the QueryBuilder hierarchy, looks good to me. Left one general comment, just a question that came to mind in general for clarification.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up the `function_score` query when scores are not needed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12693</link><project id="" key="" /><description>This change improves the `function_score` query to not compute scores at all
when they are not needed, and to not compute scores on the underlying query
when the combine function is to replace the score with the scores of the
functions.
</description><key id="99430234">12693</key><summary>Speed up the `function_score` query when scores are not needed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T13:07:55Z</created><updated>2015-08-09T08:09:57Z</updated><resolved>2015-08-06T13:14:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-06T13:12:06Z" id="128358391">+1, wonderful
</comment><comment author="nik9000" created="2015-08-06T13:12:40Z" id="128358491">LGTM. I like how you push the hard part of the TODO closer to the problem and get wins on the "easier" parts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Startup: Disable globbing in shell script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12692</link><project id="" key="" /><description>In order to not accidentally expand wilcard arguments
like --http.cors.allow-origin '*' on startup, globbing
needs to be disabled before Elasticsearch is started.

Closes #12689
</description><key id="99428514">12692</key><summary>Startup: Disable globbing in shell script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>review</label></labels><created>2015-08-06T12:57:45Z</created><updated>2015-08-07T13:22:59Z</updated><resolved>2015-08-07T13:22:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-07T07:31:58Z" id="128622302">LGTM
</comment><comment author="spinscale" created="2015-08-07T13:22:59Z" id="128697789">Closing in favor of #12710
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not track named queries that are null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12691</link><project id="" key="" /><description>Adding a named query that is null can lead to a `NullPointerException`
when copying the named queries. This is due to an implementation detail
in [`QueryParseContent.copyNamedQueries`](https://github.com/elastic/elasticsearch/blob/d0abffc9acb9afddc83ae99ae17848b813fd918f/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java#L188). In particular, this method uses
[`com.google.common.collect.ImmutableMap.copyOf`](http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/collect/ImmutableMap.html#copyOf%28java.util.Map%29). A documented requirement
of [`ImmutableMap`](http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/collect/ImmutableMap.html) is that none of the entries have a null key nor null
value. Therefore, we should not add such queries to the [`namedQueries`](https://github.com/elastic/elasticsearch/blob/d0abffc9acb9afddc83ae99ae17848b813fd918f/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java#L86)
map. This will not change any behavior since [`Map.get`](http://docs.oracle.com/javase/7/docs/api/java/util/Map.html#get%28java.lang.Object%29) returns null if no
entry with the given key exists anyway.

Closes #12683
</description><key id="99426747">12691</key><summary>Do not track named queries that are null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T12:47:05Z</created><updated>2015-08-09T08:09:57Z</updated><resolved>2015-08-06T13:33:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-06T13:02:54Z" id="128355343">left a couple of minor comments, fix looks good though!
</comment><comment author="javanna" created="2015-08-06T13:31:09Z" id="128365295">LGTM
</comment><comment author="jasontedor" created="2015-08-06T13:33:24Z" id="128366755">@javanna Thanks for reviewing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make RoutingNodes read-only by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12690</link><project id="" key="" /><description>ClusterState has 3 different methods to access RoutingNodes:
- #routingNodes() - mutable version
- #getRoutingNodes() - delegates to #getReadOnlyRoutingNodes()
- #getReadOnlyRoutingNodes() - it's docs say `NOTE, the routing nodes are mutable, use them just for read operations`

The latter also reuses the instance that it creates. This has several problems beside the obvious:
- creating RoutingNodes is costly and should be done only if really needed ie. use cached version as much as possible
- the common case is ReadOnly but all kinds of things are called
- mutable version are only needed in one place and should only be used in the AllocationService
- RoutingNodes can freeze it's ShardRoutings but doesn't
- RoutingNodes should check if it's read-only or not

This commit fixed all the problems and special cases the mutable case such that all accesses via ClusterState#getRoutingNodes()
is read-only and RoutingNodes enforces this.
</description><key id="99417602">12690</key><summary>Make RoutingNodes read-only by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T11:44:40Z</created><updated>2015-08-07T10:06:34Z</updated><resolved>2015-08-07T07:51:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-08-06T15:38:39Z" id="128412898">LGTM, much less trappy now!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't expand wildcards in command line options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12689</link><project id="" key="" /><description>Starting Elasticsearch as follows:

```
./bin/elasticsearch --http.cors.enabled true --http.cors.allow-origin '*' 
```

breaks with (eg) 

```
ERROR: Parameter [NOTICE.txt]does not start with --
```

The wildcard is being expanded by the CLI parsing, although that should be done by the shell (and in this case shouldn't be done because the wildcard is in quotes)
</description><key id="99404662">12689</key><summary>Don't expand wildcards in command line options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-08-06T10:25:00Z</created><updated>2015-08-13T09:43:37Z</updated><resolved>2015-08-13T09:43:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-13T09:43:37Z" id="130593087">Closed by #12710
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Chown data/logs when installing deb/rpm packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12688</link><project id="" key="" /><description>Issue from https://github.com/elastic/elasticsearch/issues/10671#issuecomment-127457647

If you remove a package then it removes the elasticsearch user/group.  Installing it again will create a new user/group with different IDs, which means that the data and logs directories will no longer be owned by the correct user.

Should we do a chown when installing the package?
</description><key id="99401518">12688</key><summary>Chown data/logs when installing deb/rpm packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>discuss</label><label>enhancement</label></labels><created>2015-08-06T10:03:30Z</created><updated>2016-03-25T01:21:11Z</updated><resolved>2016-03-25T01:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeffwidman" created="2015-12-26T08:58:06Z" id="167296128">I vote yes on chown when installing the package.

I got bit by this myself and couldn't figure it out until I ran across @imsaar's [helpful post](https://github.com/elastic/elasticsearch/issues/10671#issuecomment-127457647) (big thank you!) 

I have a fairly small ES install on a single node, so I thought I was being completely safe by removing the old ES 1.7 package, updating the repo info to ES v2, and then doing a fresh install of ES v2, and planning to just rebuild my indexes from scratch. I was befuddled for quite a while on why I couldn't connect to ES because every time I ran `service elasticsearch restart` it reported 'OK'. (Which I think is part of the problem, it should complain to systemd/the user). When I finally ran `service elasticsearch status` I got an exception that I was able to google and eventually find @imsaar's comment. 

There is a risk that the admin had purposefully set ownership to someone other the the default ES user/group, and will be surprised by the `chown`. But something should be done, as I suspect a lot more people will be bit by not `chown`'ing than the other way. If you want to be extra cautious, maybe as part of the install routing print a message saying that this was done and the admin needs to redo any customizations if they were done.
</comment><comment author="jasontedor" created="2016-03-18T13:08:38Z" id="198347904">The config directory in `/etc/elasticsearch` needs to be recursively `chown`'ed as well. I have a fix and a test case for this, but I'll keep it in my pocket until #17152 is in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add unittest for DiskThresholdDecider#getShardSize / #sizeOfRelocatingShards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12687</link><project id="" key="" /><description>This commit adds a basic unittest for the shard size routines and simplifies
some object creation.
</description><key id="99389390">12687</key><summary>Add unittest for DiskThresholdDecider#getShardSize / #sizeOfRelocatingShards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T08:55:30Z</created><updated>2015-08-06T15:30:29Z</updated><resolved>2015-08-06T09:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-06T09:01:55Z" id="128297881">LGTM
</comment><comment author="dakrone" created="2015-08-06T15:30:29Z" id="128411088">It's already been merged, but thanks for doing this Simon! Looks great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>when parsed query is null, return null.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12686</link><project id="" key="" /><description>Quick fix [12683](https://github.com/elastic/elasticsearch/issues/12683)

Parser should return null right away.
</description><key id="99350912">12686</key><summary>when parsed query is null, return null.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">xuzha</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-08-06T04:31:54Z</created><updated>2015-08-07T10:06:34Z</updated><resolved>2015-08-06T13:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-06T07:01:46Z" id="128271691">Hi @xuzha the fix looks good but can you please write a test for it? Also add a Closes #12683 to your commit message so the original issue gets closed once we merge this in?
</comment><comment author="jasontedor" created="2015-08-06T13:01:29Z" id="128354897">While this PR does fix the issue for common terms queries, the underlying issue is that null named queries are problematic in general. I've opened a different [PR](https://github.com/elastic/elasticsearch/issues/12691) to address the underlying issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ArrayIndexOutOfBoundsException when using 2-level terms aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12685</link><project id="" key="" /><description>I have the following _mapping info for the type
{
    "viva26": {
        "mappings": {
            "2655c2d1a7abe320": {
                "properties": {
                    "1": {
                        "type": "double"
                    },
                    "2": {
                        "type": "double"
                    },
                    "3": {
                        "type": "double"
                    },
                    "4": {
                        "type": "double"
                    },
                    "5": {
                        "type": "long"
                    },
                    "6": {
                        "type": "string"
                    }
                }
            }
        }
    }
}

I can use the data in it instantly after get data into ES for normal aggregation request even 2-level aggregation request . But just after several requests, the ES return the following failure response:
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[TeuXtz7zQqaz9oO-CCDwCA][viva26][0]: RemoteTransportException[[Starr the Slayer][inet[/10.207.161.38:9300]][indices:data/read/search[phase/query]]]; nested: ElasticsearchException[java.lang.ArrayIndexOutOfBoundsException]; nested: UncheckedExecutionException[java.lang.ArrayIndexOutOfBoundsException]; nested: ArrayIndexOutOfBoundsException; }{[zfw_Dwl7Rm-N5-sepQ5vVA][viva26][1]: RemoteTransportException[[Auntie Freeze][inet[/10.207.161.36:9300]][indices:data/read/search[phase/query]]]; nested: ElasticsearchException[java.lang.ArrayIndexOutOfBoundsException]; nested: UncheckedExecutionException[java.lang.ArrayIndexOutOfBoundsException]; nested: ArrayIndexOutOfBoundsException; }{[vFyU6GyqRVubZh4twRVETQ][viva26][2]: RemoteTransportException[[Desmond Pitt][inet[/10.207.161.40:9300]][indices:data/read/search[phase/query]]]; nested: ElasticsearchException[java.lang.ArrayIndexOutOfBoundsException]; nested: UncheckedExecutionException[java.lang.ArrayIndexOutOfBoundsException]; nested: ArrayIndexOutOfBoundsException; }{[TfXlVgnORvueAqE4ewxr0g][viva26][3]: RemoteTransportException[[Tyr][inet[/10.207.161.34:9300]][indices:data/read/search[phase/query]]]; nested: ElasticsearchException[java.lang.NumberFormatException: Invalid shift value (-59) in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value (-59) in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value (-59) in prefixCoded bytes (is encoded value really an INT?)]; }{[HfDT5WIpRGOFa9Rqq3oX-Q][viva26][4]: RemoteTransportException[[Fasaud][inet[/10.207.161.20:9300]][indices:data/read/search[phase/query]]]; nested: ElasticsearchException[java.lang.ArrayIndexOutOfBoundsException]; nested: UncheckedExecutionException[java.lang.ArrayIndexOutOfBoundsException]; nested: ArrayIndexOutOfBoundsException; }]",
    "status": 500

The request is 2-level aggregation like this:
{"aggs":{"sum":{"terms":{"field":"5","order":{"sum":"desc"}},"aggs":{"sum":{"sum":{"field":"1"}}}}}}

The version of ES is 1.5.1

How can I resolve the problem?
</description><key id="99350065">12685</key><summary>ArrayIndexOutOfBoundsException when using 2-level terms aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhangwei17</reporter><labels /><created>2015-08-06T04:23:59Z</created><updated>2015-08-06T12:22:36Z</updated><resolved>2015-08-06T09:53:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Stabaoc" created="2015-08-06T05:23:16Z" id="128245990">maybe you should add some query or filter
</comment><comment author="clintongormley" created="2015-08-06T09:53:20Z" id="128312030">This sounds very much like you have a mapping problem, eg fields with the same name, in the same type, but with different mappings (long vs string perhaps?).

Alternatively, if you are relying on dynamic mapping to add fields, you may find that the field has been mapped as a long on one shard and as a string on another shard, but the official mapping only accepts one of these types.  As soon as the "wrong" shard moves to another node, everything breaks.

These issues have been fixed in 2.0 with https://github.com/elastic/elasticsearch/issues/8870.

To resolve this now:
- make sure you have no conflicting field names (the migration plugin can help you to identify this problem: https://github.com/elastic/elasticsearch-migration)
- if that doesn't work, try putting your mapping explicitly (when you create the index) and reindex your data
</comment><comment author="zhangwei17" created="2015-08-06T12:22:36Z" id="128346690">Thanks a lot!
Finally we found that the problem will appear when field names with different core types are defined in multi types within current index.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Large index size cause high Java heap occupation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12684</link><project id="" key="" /><description>Hi all,

We use Elasticsearch 1.6.0 and run two data nodes in two servers with 128G RAM and 24 Core CPU.  ES java heap size is set to  30G and the index is configured to 5 shards with 1 replica.
Unlike common log files, our document is a bit complicated and average size of each document is about 500K. After bulk indexing around 12 million documents, the index size of each node in disk is about 5TB. Then ES servers become unstable. The slave sometimes  loses connection with the primary. One more problem is even after old GC, the java heap occupation is still around 20G.  Monitoring with Marval, I found the data of "Index Statistics-&gt;memory-&gt;LUCENE MEMORY" keeps on increasing. It is now about 30G. Does it related with Java heap occupation? What is the LUCENE MEMORY for? And anybody can suggest how to mitigate the memory issue?
</description><key id="99342454">12684</key><summary>Large index size cause high Java heap occupation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kenny-ye</reporter><labels /><created>2015-08-06T02:54:39Z</created><updated>2015-08-06T02:55:34Z</updated><resolved>2015-08-06T02:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-06T02:55:34Z" id="128221778">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>common terms query containing only stopwords with a _name causes a null pointer exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12683</link><project id="" key="" /><description>A common query containing only stopwords causes a NullPointerException if the query has a _name property.  This doesn't happen without the _name property, and doesn't happen with other types of queries.

```
# create index 
curl -XPOST localhost:9200/test -d '{
 "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "properties" : {
                "name" : { "type" : "string", "analyzer" : "stop" }
            }
        }
    }
}'
```

```
# common query with a stop word correctly returns no results
curl -XGET localhost:9200/test/type1/_search -d '{
    "query": {
        "common": {
            "name": {
                "query": "the"
            }
        }
    }
}'

{"took":35,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},
"hits":{"total":0,"max_score":null,"hits":[]}}
```

```
# common query with a _name causes a null pointer exception
curl -XGET localhost:9200/test/type1/_search -d '{
    "query": {
        "common": {
            "name": {
                "query": "the",
                "_name": "queryname"
            }
        }
    }
}'

{"error":"SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[vFIFOFczQTSxJJO-kobPBQ][test][0]: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
    \"query\": {
        \"common\": {
            \"name\": {
                \"query\": \"the\",
                \"_name\": \"queryname\"
            }
        }
    }
}]]]; nested: NullPointerException[Query may not be null]; }]","status":400}muzio:~ brett$

```

```
# a regular match query doesn't have this problem
curl -XGET 10.4.4.118:9200/test/type1/_search -d '{
    "query": {
        "match": {
            "name": {
                "query": "the",
                "_name": "queryname"
            }
        }
    }
}'
```
</description><key id="99330702">12683</key><summary>common terms query containing only stopwords with a _name causes a null pointer exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">brettrp</reporter><labels><label>:Query DSL</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-08-06T01:04:36Z</created><updated>2015-08-06T13:33:29Z</updated><resolved>2015-08-06T13:33:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-08-06T13:02:46Z" id="128355317">@brettrp Thank you for reporting this issue! The underlying issue is that because of the analyzer you have configured (the stop analyzer), the query is parsed to the null query. This issue helped us uncover a deeper bug in our handling of named queries that are null. We will have a fix in place soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bats test for removing the rpm fails every once in a while</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12682</link><project id="" key="" /><description>In my work on #12646 I've noticed that the test for removing the rpm fails about 30% of the time. I don't know why yet.  I'll work on that next but I don't think that should block #12646.
</description><key id="99330161">12682</key><summary>Bats test for removing the rpm fails every once in a while</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>bug</label><label>test</label></labels><created>2015-08-06T00:57:28Z</created><updated>2015-08-07T12:44:40Z</updated><resolved>2015-08-07T12:44:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-06T01:06:26Z" id="128198667">Here is the error:

```
     [exec] centos-7: ok 26 [RPM] rpm command is available
     [exec] centos-7: ok 27 [RPM] package is available
     [exec] centos-7: ok 28 [RPM] package is not installed
     [exec] centos-7: ok 29 [RPM] install package
     [exec] centos-7: ok 30 [RPM] package is installed
     [exec] centos-7: ok 31 [RPM] verify package installation
     [exec] centos-7: ok 32 [RPM] test elasticsearch
     [exec] centos-7: ok 33 [RPM] remove package
     [exec] centos-7: ok 34 [RPM] package has been removed
     [exec] centos-7: not ok 35 [RPM] verify package removal
     [exec] centos-7: # (in test file /vagrant/tests/src/test/resources/packaging/scripts/40_rpm_package.bats, line 122)
     [exec] centos-7: #   `[ "$status" -eq 1 ] || [ "$status" -eq 0 ]' failed
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get plugin smoketester running in jenkins.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12681</link><project id="" key="" /><description>We have a smoke_test_plugins.py, but its a bit slow, not integrated
into our build, etc.

I converted this into an integration test. It is definitely uglier
but more robust and fast (e.g. 20 seconds time to verify).

Also there is refactoring of existing integ tests logic, like printing
out commands we execute and stuff
</description><key id="99327268">12681</key><summary>Get plugin smoketester running in jenkins.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-06T00:27:38Z</created><updated>2015-08-06T13:47:35Z</updated><resolved>2015-08-06T02:33:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-06T02:31:15Z" id="128217096">This is a great start for providing tests which don't "fit" with a particular code module. LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>copy_to creates multiple terms for not_analyzed strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12680</link><project id="" key="" /><description>In trying to create a simple aggregate Group By field, I was attempting to use `copy_to` to combine 3 fields into one.

``` json
PUT /test
{
  "mappings": {
    "result" : {
      "properties": {
        "groupby" : {
          "type": "string",
          "index": "not_analyzed"
        },
        "school" : {
          "type" : "string",
          "index": "not_analyzed"
          , "copy_to": "groupby"
        },
        "grade" : {
          "type" : "integer",
          "copy_to": "groupby"
        },
        "student" : {
          "type" : "string",
          "index": "not_analyzed",
          "copy_to": "groupby"
        }
      }
    }
  }
}
```

However, the problem is that the result ends up creating multiple terms for the `groupby` field. If I were to index the document:

``` json
{ "grade": 2, "school": "Arbitrary", "student": "Chris" }
```

Then the value indexed to `groupby` would be `[ "2", "Arbitrary", "Chris" ]`. This means that it cannot be used for this purpose.

You can workaround this issue by using the index transformation to concatenate the values directly, but this is going away in Elasticsearch 2.0.
</description><key id="99326800">12680</key><summary>copy_to creates multiple terms for not_analyzed strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-08-06T00:21:43Z</created><updated>2015-08-06T09:46:57Z</updated><resolved>2015-08-06T09:46:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-06T09:46:57Z" id="128310847">This is correct behaviour.  It is the equivalent of passing an array of values to the field.  The `_all` field works in the same way.  Its purpose is to create a custom `_all` field (out of potentially large text fields) which can be searched as if it were one field, not to create a single field value.  Concatenation raises other questions like what should the concatenation character be?

If you want to do this, you should simply provide the concatenated field in your document.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boolean type coercion in multifields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12679</link><project id="" key="" /><description>It would be very nice if for property of any type a multifield with type boolean was supported by coercing the property value to boolean using following logic
- Integer - missing/null/0 - false otherwise true
- Boolean - null/false - false otherwise true
- Any other type (string, date) missing/null - false otherwise true

it will allow to aggregate (as bucket or metric) or filter on presence of a value very simply without polluting _source with pre-calculated flags
</description><key id="99320014">12679</key><summary>Boolean type coercion in multifields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2015-08-05T23:19:17Z</created><updated>2015-08-07T14:27:00Z</updated><resolved>2015-08-06T09:42:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-06T09:42:07Z" id="128310114">This works already:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "int": {
          "type": "integer",
          "fields": {
            "bool": {
              "type": "boolean",
              "null_value": false
            }
          }
        },
        "string": {
          "type": "string",
          "fields": {
            "bool": {
              "type": "boolean",
              "null_value": false
            }
          }
        },
        "date": {
          "type": "date",
          "fields": {
            "bool": {
              "type": "boolean",
              "null_value": false
            }
          }
        }
      }
    }
  }
}

PUT t/t/true
{
  "int": 5,
  "string": "foo",
  "date": "2015-01-01"
}

PUT t/t/false
{
  "int": 0,
  "string": ""
}

PUT t/t/null
{
  "int": null,
  "string": null,
  "date": null
}

GET t/_search
{
  "fielddata_fields": [
    "int.bool",
    "string.bool",
    "date.bool"
  ]
}
```
</comment><comment author="roytmana" created="2015-08-06T13:58:46Z" id="128375094">thank you @clintongormley 
Yes it does work. I also was trying this route last night. the only downside is that it requires sending null as field value. it will not handle missing values. I know it is a bigger issue (setting default value for missing values) than this use case and it probably does not make awful lot of sense in context of general default value for missing but in this case it would be very convenient as it will allow 100% transparently without any change of data add a boolean multifield for presence of value providing much simpler queries/aggregations and most likely better performance comparing with missing filter/agg

so I guess my question is would it make sense to consider support of default value for missing values the way it is done for nulls?
</comment><comment author="clintongormley" created="2015-08-07T10:22:56Z" id="128667433">@roytmana I don't think we should add yet another option for this.  If you want to take missing values into account, make them null in your application.
</comment><comment author="roytmana" created="2015-08-07T14:27:00Z" id="128714237">ok, thank you @clintongormley. I am doing it now. just wanted to check with you. thanks again for looking into it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid extra reroutes of delayed shards in RoutingService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12678</link><project id="" key="" /><description>In order to avoid extra reroutes, `RoutingService` should avoid
scheduling a reroute of any shards where the delay is negative. To make
sure that we don't encounter a race condition between the
GatewayAllocator thinking a shard is delayed and RoutingService thinking
it is not, the GatewayAllocator will update the RoutingService with the
last time it checked in order to use a consistent "view" of the delay.

Resolves #12456
Relates to #12515 and #12456 

This is the forward-port of #12532, but actually ended up not being that difficult so it's not much different.
</description><key id="99298139">12678</key><summary>Avoid extra reroutes of delayed shards in RoutingService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-05T21:00:45Z</created><updated>2015-08-07T13:42:45Z</updated><resolved>2015-08-06T16:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-08-06T15:40:58Z" id="128413525">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch script does not work with arguments that have spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12677</link><project id="" key="" /><description>If you attempt to start elasticsearch and have an argument that has a space in it, the argument is not parsed correctly and the value after the space is treated as a separate argument.

```
$ bin/elasticsearch -Des.pidfile="/path/with space/es.pid"
ERROR: Parameter [space/es.pid]does not start with --
```

The fix is probably similar to the ones in #12508
</description><key id="99267814">12677</key><summary>elasticsearch script does not work with arguments that have spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2015-08-05T18:26:49Z</created><updated>2015-09-17T09:21:59Z</updated><resolved>2015-08-11T16:16:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-06T23:33:07Z" id="128538892">I'm not quite sure we should change this, user could escape the ' " ' by doing this 

```
bin/elasticsearch -Des.pidfile=\"/path/with space/es.pid\"

```
</comment><comment author="jaymode" created="2015-08-07T10:16:32Z" id="128665681">Thanks for opening a PR @xuzha. I think having to escape the quotes on the command line makes it less user friendly. When I think of running commands with spaces, I would expect to need to escape spaces or put quotes around the item with spaces; I didn't expect that I'd need to escape the quotes.
</comment><comment author="clintongormley" created="2015-08-11T16:16:53Z" id="129952408">Closed by #12710
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to update document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12676</link><project id="" key="" /><description>Following scenario: We are updating (upserting) lots of documents into an index in elasticsearch-cluster (2 data nodes). Today we increased the volume of those updates by roughly a factor of 10. After a few minutes (I would say 10 to 20) we suddenly saw lots of errors returned by elasticsearch. All of those were like the following one:

`RemoteTransportException[[SERVER_NAME][inet[/SERVER_IP:9300]][update]]; nested: ElasticsearchIllegalArgumentException[failed to execute script]; nested: NoSuchElementException;`

The body of those requests always looks like that:
    {
        script: 'ctx._source.FIELD_1 = PARAM_1; ctx._source.FIELD_2 = PARAM_2',
        params: {
                PARAM_1: "some string",
                PARAM_2: 124
        },
        upsert: {
                PARAM_1: "some string",
                PARAM_2: 124
        }
    }

I played around a bit with the update and it worked (at least in this case) once I removed the update for FIELD_2/PARAM_2.

Additionally I tried a couple of things to fix the issue:
- Repeating the requests after a few seconds
- Reducing/Increasing the value of retry_on_conflict
- Replacing retry_on_conflict by loop + loopcounter and a timeout

Nothing worked. In the end I restarted the data-node that was holding (at this time) all primary shards, which finally seemed to have solved it for now.

Any ideas why this happened? Any suggestions what to try, in case it happens again?
</description><key id="99264860">12676</key><summary>Unable to update document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">XDestination</reporter><labels><label>:Scripting</label><label>feedback_needed</label></labels><created>2015-08-05T18:09:10Z</created><updated>2016-01-26T17:52:28Z</updated><resolved>2016-01-26T17:52:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T18:24:47Z" id="128099423">Hi @XDestination 

I assume your update uses "FIELD_1/2" and not "PARAM_1/2"?  What version of ES are you on? Are you using groovy or mvel? Inline scripts or file scripts? Anything else in the logs?
</comment><comment author="XDestination" created="2015-08-05T18:31:31Z" id="128101080">Hi,
you are right. PARAM_1/2 is wrong of course, we are in fact using the same field name in the script and then in the params/upsert object.
This cluster is currently running on version 1.3.7
As scripting language we are using the default one: mvel
The scripts are sent with the request, so no file based scripts
Thats the weird thing. The logs actually show nothing at all :/
</comment><comment author="clintongormley" created="2015-08-05T18:36:52Z" id="128102903">OK - this may well be an mvel bug with concurrency. I'd suggest moving to groovy and seeing what happens.
</comment><comment author="XDestination" created="2015-08-05T18:40:13Z" id="128103629">Even if the update fails a couple of minutes later too (And therefore no other updates related to that document are being executed.)?
</comment><comment author="clintongormley" created="2015-08-05T18:41:36Z" id="128103933">@XDestination that'd be my guess.  try it and see, the syntax should be pretty similar

We removed MVEL because of issues like this
</comment><comment author="XDestination" created="2015-08-05T18:51:57Z" id="128106610">Okay. Thanks for the advice. I will give it a try :)
</comment><comment author="clintongormley" created="2016-01-26T17:52:28Z" id="175140550">Nothing further. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nicer exception when sorting on mixed data types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12675</link><project id="" key="" /><description>If you sort on (eg) a date field but set the `unmapped_type` to (eg) boolean, you can end up with an exception which is difficult to understand, eg:

```
[2015-08-05 14:54:57,145][DEBUG][action.search.type       ] [Arizona Annie] [index1][2]: Failed to execute [org.elasticsearch.action.search.SearchRequest@22b8bc4f] while moving to second phase
java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.lucene.util.BytesRef
        at org.apache.lucene.search.FieldComparator$TermOrdValComparator.compareValues(FieldComparator.java:902)
        at org.apache.lucene.search.TopDocs$MergeSortQueue.lessThan(TopDocs.java:172)
        at org.apache.lucene.search.TopDocs$MergeSortQueue.lessThan(TopDocs.java:120)
        at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:225)
        at org.apache.lucene.util.PriorityQueue.add(PriorityQueue.java:133)
        at org.apache.lucene.search.TopDocs.merge(TopDocs.java:234)
```

We should at least provide the name of the field that is causing this exception.
</description><key id="99263363">12675</key><summary>Nicer exception when sorting on mixed data types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Exceptions</label><label>adoptme</label><label>enhancement</label></labels><created>2015-08-05T18:01:01Z</created><updated>2016-03-24T08:50:28Z</updated><resolved>2016-01-26T17:52:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T17:52:04Z" id="175140382">This appears to have been fixed in 2.x
</comment><comment author="lxbalex" created="2016-03-24T08:50:28Z" id="200739165">Hi, I have a question, my elasticsearch version is 1.4.3, when I execute a query like that, it end up with a exception...
`GET /test_index/_search
{
    "from": 0,
    "size": 200,
    "query": {
        "filtered": {
            "filter": {
                "bool": {
                    "must": {
                        "range": {
                            "senddate": {
                                "from": "2016-02-01 00:00:00",
                                "to": null,
                                "include_lower": false,
                                "include_upper": true
                            }
                        }
                    }
                }
            }
        }
    },
    "sort": [
        {
            "senddate": {
                "order": "asc"
            }
        }
    ]
}`

The exception is
`{
   "error": "ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[org.apache.lucene.util.BytesRef cannot be cast to java.lang.Long]; ",
   "status": 503
}`

And my mappings is
`senddate: {
store: true
format: yyyy-MM-dd HH:mm:ss
type: date
}`

Can anybody tell my why?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove mapping source transform scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12674</link><project id="" key="" /><description>The ability to transform the `_source` field before indexing (while storing the original `_source`) is confusing and error prone.  Most of what you can do with it can already be done with `copy_to`.  It also makes it difficult to know where a field value has come from, which means it can't support highlighting.

I think this is not widely used at all and should be removed in 2.0 (while continuing to support indices which already use it for 2.x releases)
</description><key id="99246204">12674</key><summary>Remove mapping source transform scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>breaking</label><label>deprecation</label></labels><created>2015-08-05T16:36:03Z</created><updated>2015-11-08T21:14:08Z</updated><resolved>2015-10-30T16:34:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-05T16:46:39Z" id="128067349">&gt; Most of what you can do with it can already be done with copy_to

Originally transforms were written because I needed conditional `copy_to`. Like "only documents with namespace == 0 need to copy their text field to the suggest field". We never ended up using it and since its been written people have used it for lots of somewhat unexpected things. Its lots of power.

So I think we have two options:
1. Remove it
2. Move it to a plugin
3. Leave it there and put more big signs on it about how it shouldn't be done

We should also talk about implementing conditional `copy_to` again. Its less powerful but its maybe less error prone.

One thing I should mention is that there is a parameter, `_source_transform` that you should be able to stick on get and search requests that will re-run the transform on load. Its meant for debugging but it works.

I'm partial to moving source transform to a plugin and no implementing conditional copy_to.
</comment><comment author="clintongormley" created="2015-08-05T16:47:29Z" id="128067537">&gt; I'm partial to moving source transform to a plugin and not implementing conditional copy_to.

sounds good to me
</comment><comment author="pickypg" created="2015-08-05T16:48:13Z" id="128067692">&gt; I'm partial to moving source transform to a plugin and no implementing conditional copy_to.

Sounds great to me too. People miss big signs because they frequently get on the road after them.
</comment><comment author="nik9000" created="2015-08-21T19:47:52Z" id="133541541">Moved to 2.1. I'll totally pick this up early next week but there is 0 chance it'll make it into 2.0.
</comment><comment author="nik9000" created="2015-08-27T19:59:03Z" id="135536571">Now that I'm looking at source transforms it looks half finished. There is support in the Java api to transform on load. But there isn't support in the REST api for it. It'd be so much less confusing if that were possible. Not that it'd be well loved, but less confusing.
</comment><comment author="nik9000" created="2015-08-27T20:01:42Z" id="135537077">I suspect when I first wrote transform I just didn't think about the rest API for that.
</comment><comment author="nik9000" created="2015-08-31T13:01:15Z" id="136364402">Just changed issue to remove rather than break into a plugin because:
1. Its hard to break into a plugin in satisfactory ways. I made an effort and it was just making the problem worse. See #13164.
2. So far as I know no one is really using this in the way it was intended to be used. People are using it as a simple transformation pipeline but its really really really not for that and that causes all kinds of confusion. Logstash is a much better fit. See that abandoned pull request I mentioned above for more. And for me describing what transforms are "really" for.
</comment><comment author="adichad" created="2015-10-30T07:15:14Z" id="152447027">@rjernst confirms over at #13657 that mapping transforms have not been removed in 2.x and will only be deprecated in 2.x to be removed in 3.0

request to reinstate the related documentation for example at

https://www.elastic.co/guide/en/elasticsearch/reference/1.7/mapping-transform.html
https://www.elastic.co/guide/en/elasticsearch/reference/1.7/_get_transformed.html

to also work at

https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-transform.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/_get_transformed.html

and maintain deprecation warnings there?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script compilation stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12673</link><project id="" key="" /><description>Variables hard coded into scripts can lead to a serious performance hit, which is not easy to spot.  Would be great to add a counter to the stats API to keep track of how many times scripts have been compiled.

Bonus points for adding script cache eviction counts too
</description><key id="99241556">12673</key><summary>Script compilation stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Stats</label><label>feature</label></labels><created>2015-08-05T16:14:10Z</created><updated>2015-08-08T01:02:27Z</updated><resolved>2015-08-07T22:27:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Completion field should be under multifield type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12672</link><project id="" key="" /><description>i believe the the completion field should be defined under CorePropertiesDescriptor.cs class since we should be able to define the completion type under multi-field type

e.g.
 "properties" : {
          "audience" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "author" : {
            "type" : "string",
            "fields" : {
              "raw" : {
                "type" : "string",
                "index" : "not_analyzed"
              },
              "suggestive" : {
                "type" : "completion",
                "analyzer" : "simple",
                "payloads" : true,
                "preserve_separators" : true,
                "preserve_position_increments" : true,
                "max_input_length" : 50
              }
            }
          },
</description><key id="99230946">12672</key><summary>Completion field should be under multifield type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertnadar</reporter><labels /><created>2015-08-05T15:26:34Z</created><updated>2015-08-05T17:40:12Z</updated><resolved>2015-08-05T17:40:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T17:40:11Z" id="128086673">This issue was moved to elastic/elasticsearch-net#1512
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use explict flag if index should be created on engine creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12671</link><project id="" key="" /><description>Today we try to detect if there is an index existing in the directory
and if not we create one. This can be tricky and errof prone since we
rely on the filesystem without taking the context into account when the
engine gets created. We know in all situations if the index should be created
so we can just use this infromation and rely on the lucene index writer to barf
if we hit a situations where we can't append to an index while we should.
</description><key id="99213055">12671</key><summary>Use explict flag if index should be created on engine creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-08-05T14:08:04Z</created><updated>2015-08-07T10:06:34Z</updated><resolved>2015-08-05T18:45:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-08-05T14:15:57Z" id="128011885">LGTM, thanks for doing this @s1monw!  So scary to rely on filesystem to tell us this ...
</comment><comment author="dakrone" created="2015-08-05T14:17:15Z" id="128012161">LGTM also, nice to get rid of that TODO
</comment><comment author="kimchy" created="2015-08-05T15:36:50Z" id="128042632">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent DirectCandidateGenerator to reuse an unclosed analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12670</link><project id="" key="" /><description>When postFilter generates a token that is identical to the input term
DirectCandidateGenerator should not preFilter this token. If postFilter
and preFilter are the same analyzer instance it would fail with :
"TokenStream contract violation: close() call missing"
</description><key id="99201860">12670</key><summary>Prevent DirectCandidateGenerator to reuse an unclosed analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nomoa</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.6.3</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-08-05T13:11:48Z</created><updated>2015-08-05T20:54:51Z</updated><resolved>2015-08-05T19:17:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-05T13:13:20Z" id="127993142">Hi @nomoa! You'll have to sign the CLA before I can merge this but I'll review it now.
</comment><comment author="s1monw" created="2015-08-05T13:13:24Z" id="127993166">hey, can you maybe sign the CLA so we can merge it?
</comment><comment author="nomoa" created="2015-08-05T13:18:03Z" id="127994745">Signed, it's weird because I already signed it last time... I will re-sign :)
The first time I used "nomoa" as github username.
This time I used my email address.
</comment><comment author="nomoa" created="2015-08-05T13:23:23Z" id="127995808">OK signed again with the correct email address :)
</comment><comment author="s1monw" created="2015-08-05T19:07:29Z" id="128112483">LGTM I think this is because of synonym handling maybe? I with I remember / would have documented it :(
</comment><comment author="nik9000" created="2015-08-05T19:14:17Z" id="128114384">LGTM. Thanks for adding the comments.
</comment><comment author="nik9000" created="2015-08-05T19:15:59Z" id="128115131">I'll do the merging if that is ok with @s1monw .
</comment><comment author="s1monw" created="2015-08-05T19:16:37Z" id="128115381">++
</comment><comment author="nomoa" created="2015-08-05T19:18:22Z" id="128116072">Thanks!
</comment><comment author="nik9000" created="2015-08-05T19:57:51Z" id="128130653">Thanks @nomoa !
</comment><comment author="nik9000" created="2015-08-05T20:54:51Z" id="128145440">And I've finally merged this to master. I couldn't figure out a good way to forward port it and keep your name on it, sorry about that. I referenced you in the commit message at least.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch hangs when execute query on analyzed filed  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12669</link><project id="" key="" /><description>Hi,

I have kibana and elasticsearch installed to analyse the webservers logs.

When i try to get the average request time for 5 or 10 request_urls the elasticsearch hanged and i have to restart the cluster.
![visualize](https://cloud.githubusercontent.com/assets/7731747/9084466/a549befe-3b7c-11e5-8604-1f1ed5ebde8b.png)

Am i doing something wrong?

Note that by choosing .raw field i get results.

Thanks
</description><key id="99181025">12669</key><summary>elasticsearch hangs when execute query on analyzed filed  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shorman88</reporter><labels /><created>2015-08-05T11:18:12Z</created><updated>2015-08-05T16:25:59Z</updated><resolved>2015-08-05T16:25:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-05T16:25:59Z" id="128058304">Please use https://discuss.elastic.co for question on using Elasticsearch.Those forums are specifically set up for questions whereas this list is reserved for bug reports and new feature requests.

If you look at your Elasticsearch server logs you will probably find out of memory errors which correlate with the times you performed these searches. Analyzed fields tends to be very high cardinality and lead to heavy memory pressure (including the JVM running out of available heap) due to this high cardinality. This is why a warning is shown in the Kibana UI when you select an analyzed field to use as part of an aggregation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add path.scripts directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12668</link><project id="" key="" /><description>Today this is "unofficial" as conf/scripts, but some people
want to share scripts across different nodes and so on. Because
they cannot configure it, they are forced to use dirty hacks
like symbolic links, which isnt going to work: we aren't going
to recursively scan conf/ and add permissions to all link targets
underneath it, thats crazy.

I really hate adding yet another configuration knob here, but
users resorting to using symlinks are going to be frustrated,
and do things in a more insecure way.
</description><key id="99177297">12668</key><summary>Add path.scripts directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-05T10:48:58Z</created><updated>2015-08-07T10:06:34Z</updated><resolved>2015-08-06T02:33:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-05T14:47:30Z" id="128022348">Hopefully one day we can eliminate file based scripts altogether, but this LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rethrow exception during recovery finalization even if source is not broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12667</link><project id="" key="" /><description>Today we miss to throw / rethrow an recovery exception if it happens during
the finalization of phase 1 if the source files are not affected. Even worse
this can cause some dataloss if the reason for this exception is a failure of
deleting a corruption marker or similar pre-existing corruptions since we continue
with the recovery and mark the target shared as started which will in-turn open
an engine with an empty index.
</description><key id="99176061">12667</key><summary>Rethrow exception during recovery finalization even if source is not broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.6.3</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-08-05T10:40:38Z</created><updated>2015-08-07T10:06:34Z</updated><resolved>2015-08-05T18:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-05T10:47:00Z" id="127953717">LGTM
But this part of the code is not really my cup of tea, so probably needs another review.
</comment><comment author="s1monw" created="2015-08-05T10:53:03Z" id="127954505">@kimchy @bleskes can you take a look
</comment><comment author="kimchy" created="2015-08-05T15:35:22Z" id="128042271">LGTM
</comment><comment author="s1monw" created="2015-08-05T18:58:46Z" id="128110186">pushed also to 1.6 and 1.7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better share threadpools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12666</link><project id="" key="" /><description>We have 17 threadpools today, and while all of them are configured individually to create a number of threads that is reasonable for the number of cores, summing up all created threads gives a high number. For instance a machine with 4 cores starts between 30 (only considering fixed threadpools) and 59 threads (considering fixed + scaling threadpools), and a machine with 32 cores starts between 215 and 373 threads. And this doesn't account for threads that are created by Lucene itself.

These numbers look too high to me, and while we could decrease the size of individual thread pools, this would have the downside that eg. only issuing get requests could not make use of a whole machine's resources. So maybe we could look at better sharing threadpools across tasks? For instance I was thinking we could merge the following threadpools:
- get, search, suggest, percolate, warmer (read operations)
- index, bulk, refresh, flush (write operations)
- fetch_shard_started, fetch_shard_stored
</description><key id="99164834">12666</key><summary>Better share threadpools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>discuss</label></labels><created>2015-08-05T09:39:06Z</created><updated>2016-01-26T18:00:13Z</updated><resolved>2016-01-26T18:00:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-26T18:00:13Z" id="175144548">No consensus could be reached. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Configuring path.repo for a Windows network share</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12665</link><project id="" key="" /><description>We run ES in three clusters on Windows. We have two uses for `fs` repositories
1. restore snapshots directly to our development cluster from our developer machines
2. physically transport snapshots to different clusters (sometimes faster than a roundtrip to S3, or reindexing).

These were both enabled by configuring the repository location to a Windows network share visible to all nodes: `\\MACHINENAME\snapshotfolder`.

The change in 1.6 to whitelist available locations for fs repositories seems to have broken our workflow. The below scenarios have been tested on a single node cluster on my machine. 

```
elasticsearch.json: 
"path.repo" : ["/", "\\\\ERUSS001L"],

POST http://localhost:9200/_snapshot/eruss HTTP/1.1
{"type":"fs","settings":{"location":"\\\\ERUSS001L\\snapshots"}}

[2015-08-05 10:00:03] {"error":"RemoteTransportException[[dev1_RTOOL001L_coordinator][inet[/192.168.10.77:9700]][cluster:admin/repository/put]]; nested: RepositoryException[[eruss] failed to create repository]; nested: CreationException[Guice creation errors:\r\n\r\n1) Error injecting constructor, org.elasticsearch.repositories.RepositoryException: [eruss] location [\\\\ERUSS001L\\snapshots] doesn't match any of the locations specified by path.repo\r\n at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;()\r\n at org.elasticsearch.repositories.fs.FsRepository\r\n at Key[type=org.elasticsearch.repositories.Repository, annotation=[none]]\r\n\r\n1 error]; nested: RepositoryException[[eruss] location [\\\\ERUSS001L\\snapshots] doesn't match any of the locations specified by path.repo]; ","status":500}
```

```
elasticsearch.json: 
"repositories.url.allowed_urls": ["file:////ERUSS001L/*"],

POST http://localhost:9200/_snapshot/eruss HTTP/1.1
{"type":"url","settings":{"location":"file:////ERUSS001L/snapshots"}}

[2015-08-04 17:57:22] {"error":"RemoteTransportException[[dev1_RTOOL001L_coordinator][inet[/192.168.10.77:9700]][cluster:admin/repository/put]]; nested: RepositoryException[[eruss] failed to create repository]; nested: CreationException[Guice creation errors:\r\n\r\n1) Error injecting constructor, org.elasticsearch.repositories.RepositoryException: [eruss] file url [file:////ERUSS001L/snapshots] doesn't match any of the locations specified by path.repo or repositories.url.allowed_urls\r\n at org.elasticsearch.repositories.uri.URLRepository.&lt;init&gt;()\r\n at org.elasticsearch.repositories.uri.URLRepository\r\n at Key[type=org.elasticsearch.repositories.Repository, annotation=[none]]\r\n\r\n1 error]; nested: RepositoryException[[eruss] file url [file:////ERUSS001L/snapshots] doesn't match any of the locations specified by path.repo or repositories.url.allowed_urls]; ","status":500} 
```

Both of the above seem like they should have worked. So:
1. how do I configure a Windows network share?
2. how do I disable this feature for our development cluster, so we can snapshot and restore directly to and from developer machines?
3. does path.repo need to be configured on all nodes, or just the coordinator?
</description><key id="99159257">12665</key><summary>Configuring path.repo for a Windows network share</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">rikkit</reporter><labels><label>:Snapshot/Restore</label><label>docs</label></labels><created>2015-08-05T09:11:38Z</created><updated>2017-07-26T11:30:07Z</updated><resolved>2015-08-12T16:38:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-08-11T20:32:54Z" id="130057964">In case of UNC paths you need to specify at least the machine name and the share name as a prefix. So, it should be:

```
repo.path: ["\\\\ERUSS001L\\snapshots"]
```

The `repo.path` setting should be set on all master and data nodes.
</comment><comment author="clintongormley" created="2015-08-12T09:35:49Z" id="130236797">@imotov could you add this to the docs please?
</comment><comment author="rikkit" created="2015-08-12T13:26:22Z" id="130300744">@imotov That doesn't appear to work.

My config is now:

```
"path.repo" : ["/", "\\\\ERUSS001L\\snapshots"],
```

Restarting nodes and testing with `\\\\ERUSS001L\\snapshots\\test` gives the same error as before:

```
{"error":"RemoteTransportException[[dev1_RTOOL001L_coordinator][inet[/192.168.10.77:9700]][cluster:admin/repository/put]]; nested: RepositoryException[[eruss] failed to create repository]; nested: CreationException[Guice creation errors:\r\n\r\n1) Error injecting constructor, org.elasticsearch.repositories.RepositoryException: [eruss] location [\\\\ERUSS001L\\snapshots\\test] doesn't match any of the locations specified by path.repo\r\n  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;()\r\n  at org.elasticsearch.repositories.fs.FsRepository\r\n  at Key[type=org.elasticsearch.repositories.Repository, annotation=[none]]\r\n\r\n1 error]; nested: RepositoryException[[eruss] location [\\\\ERUSS001L\\snapshots\\test] doesn't match any of the locations specified by path.repo]; ","status":500}
```

Also, you mean "path.repo", right? Not "repo.path"?
</comment><comment author="imotov" created="2015-08-12T14:33:05Z" id="130322996">@rikkit yes, I meant `path.repo`. I tested it on my windows box with both yaml and json configuration files  and it worked fine. When a error like this is returned to the user, a more complete error message should be logged into the log file. Could you post this error here?
</comment><comment author="rikkit" created="2015-08-12T15:20:20Z" id="130340244">@imotov OK, I messed up earlier...  :no_mouth: . I've got it working now. Thanks for your help!
</comment><comment author="imotov" created="2015-08-12T16:11:13Z" id="130358057">Converting it into a documentation issue then, so I don't forget to update docs. 
</comment><comment author="G0pal" created="2017-07-21T17:31:32Z" id="317063449">So what was the final solution for this? It would really help if you post the solution for this here.</comment><comment author="imotov" created="2017-07-23T20:25:53Z" id="317279653">@G0pal the final solution was to [add an example to documentation](https://www.elastic.co/guide/en/elasticsearch/reference/5.5/modules-snapshots.html#_shared_file_system_repository). If you still have any issues with setting up repository on Windows, please don't hesitate to ask your questions on our [forum](https://discuss.elastic.co/).</comment><comment author="rikkit" created="2017-07-26T11:30:07Z" id="318026707">@G0pal If i remember right, the example posted was correct but my cluster config wasn't updating due to some caching issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator should index query terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12664</link><project id="" key="" /><description>When a document is provided, the percolator evaluates every indexed query against this document. Instead, we should try to extract query terms from the query and index them. For some queries, this is not possible given that the terms that they match depend on the content of the index (eg. wildcard queries), but for some of our main queries, this would be easy (term, phrase and combinations through boolean or dismax queries). Then when a document is provided, we could quickly filter candidates by searching for queries that either contain terms that appear in the document or that we were not able to extract terms from. This could greatly speed up the percolator.
</description><key id="99158088">12664</key><summary>Percolator should index query terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Percolator</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-08-05T09:05:08Z</created><updated>2016-01-06T15:21:41Z</updated><resolved>2016-01-06T15:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>elasticsearch how to partition data by day?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12663</link><project id="" key="" /><description>Hi:
es group guys!

I've used es for days.Recently i found a situation that how can i use es to partition my data by days.
Like sql "partitioned by day"
I find an es bolg about this : 
https://www.elastic.co/blog/what-is-an-elasticsearch-index
It says we can use like this:
$ curl -XGET localhost:9200/logs-2013-02-22,logs-2013-02-21/Errors/_search?query="q:Error Message"

But i'd like to use like this (e.g by sql): select \* from esDB.table where day&lt;=20150130 and day&gt;=20150101 limit 50;
In addition,how can i use es to index data partitioned by day and search data by range of days? 

many thanks
</description><key id="99132505">12663</key><summary>elasticsearch how to partition data by day?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tzou</reporter><labels /><created>2015-08-05T06:14:48Z</created><updated>2015-08-05T07:46:12Z</updated><resolved>2015-08-05T07:46:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-05T07:46:12Z" id="127903147">Please ask questions on the [forums](http://discuss.elastic.co), we want to keep Github issues for bugs and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide Update details to Index Transform scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12662</link><project id="" key="" /><description>Currently, index transformers behave like triggers that allow you to mutate the incoming document without effecting the stored `_source`. It would also be useful if you could do other things, similar to the Update API, to block/log indexing operations based on selected criteria.

On the topic of blocking things, it would also be useful to pass in what the current operation is: a create or an update (since delete's aren't transformed, this can probably be a boolean). If not too unreasonable, as the document is on the same node and probably in memory already, it may be worthwhile to also pass in the previous version of the document so that old fields can be transformed (e.g., an indexed, trigger-based version field). This should allow true trigger-like manipulation.
</description><key id="99122084">12662</key><summary>Provide Update details to Index Transform scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-08-05T05:18:07Z</created><updated>2015-08-05T16:40:56Z</updated><resolved>2015-08-05T16:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T16:26:06Z" id="128058330">@pickypg this makes me want to run away screaming.  why on earth would you want to add this complexity to Elasticsearch?  Honestly, I'd prefer to remove the source transform script altogether.
</comment><comment author="pickypg" created="2015-08-05T16:40:56Z" id="128065964">The source transform is going away in ES 2.0 with #12674, so this is irrelevant.

If the transform reappears in a plugin, then I hope that it gains this feature because anyone using the source transformation as it is today should make changes on the client side to fix the data rather than writing scripts (that will slow things down!) to fix it on the other side. Adding trigger-like support adds support for use cases that aren't possible otherwise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES shard fail to recovery due do number of docs differ</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12661</link><project id="" key="" /><description>We use two ES data nodes with setting -Xmx30g -Xms30g. The two ES servers have 128G physical memory and 24 CPU cores.
After bulking insert for about 2 weeks, totally we indexed about 10 million document (5 shards, 1 replica). The index size is about 4.7TB in disk. Then ES server comes into frequently long old GC and even after old GC, the heap occupation is above 20G.  
I tried to release the heap usage by stopping the bulk insert and close the index. Then I open the index again, but the cluster goes to yellow. One replica shard failed to recovery because of document number mismatch with primary shard. How to solve this issue to recover the failed shard? 

The server log:
###### 

[2015-08-05 03:30:26,461][WARN ][indices.cluster          ] [ipattern-es01.iad1] [[ipattern-2015-07][3]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [ipattern-2015-07][3]: Recovery failed from [ipattern-es02.iad1][CB3p3ZnZR_qUHGc05ZKK7Q][ipattern-es02.iad1][inet[/10.40.146.47:9300]] into [ipattern-es01.iad1][gqF0BDR_S9SAlSIfGZVx_g][ipattern-es01.iad1][inet[ipattern-es01.iad1/10.40.146.46:9300]]
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [ipattern-es02.iad1][inet[/10.40.146.47:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [ipattern-2015-07][3] Phase[1] Execution failed
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)
    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [ipattern-2015-07][3] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)
    ... 10 more
Caused by: java.lang.IllegalStateException: try to recover [ipattern-2015-07][3] from primary shard with sync id but number of docs differ: 2435163 (ipattern-es02.iad1, primary) vs 2435160(ipattern-es01.iad1)
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)
    ... 11 more
</description><key id="99111170">12661</key><summary>ES shard fail to recovery due do number of docs differ</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kenny-ye</reporter><labels><label>:Core</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-08-05T03:44:20Z</created><updated>2017-01-07T13:41:04Z</updated><resolved>2016-01-26T17:45:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-05T14:23:15Z" id="128013824">@brwe any idea how the sync id can have differing numbers of documents?
</comment><comment author="s1monw" created="2015-08-05T14:33:14Z" id="128017355">are you using delete by query by any chance and can you tell me more about your usage pattern, the ES version and about the events that happened before you closed your index?
</comment><comment author="kenny-ye" created="2015-08-06T02:07:56Z" id="128213397">Before we close the index, the two nodes lost connection due to status ping timeout. The salve cannot connect the primary. The log is as below.

[2015-08-04 04:02:09,424][WARN ][transport                ] [ipattern-es01.iad1] Received response for a request that has timed out, sent [55643ms] ago, timed out [25642ms] ago, action [internal:discovery/zen/fd/master_ping], node [[ipattern-es02.iad1][CB3p3ZnZR_qUHGc05ZKK7Q][ipattern-es02.iad1][inet[/10.40.146.47:9300]]], id [15173706]
[2015-08-04 04:27:30,530][INFO ][monitor.jvm              ] [ipattern-es01.iad1] [gc][young][1253732][767265] duration [806ms], collections [1]/[1.6s], total [806ms]/[1.4d], memory [24.6gb]-&gt;[25.1gb]/[29.8gb], all_pools {[young] [133.7mb]-&gt;[8.1mb]/[1.1gb]}{[survivor] [149.7mb]-&gt;[149.7mb]/[149.7mb]}{[old] [24.4gb]-&gt;[24.9gb]/[28.5gb]}
[2015-08-04 05:57:01,795][INFO ][monitor.jvm              ] [ipattern-es01.iad1] [gc][young][1258935][773567] duration [776ms], collections [1]/[1.3s], total [776ms]/[1.4d], memory [23.7gb]-&gt;[24.1gb]/[29.8gb], all_pools {[young] [1.4mb]-&gt;[1.6mb]/[1.1gb]}{[survivor] [149.7mb]-&gt;[149.7mb]/[149.7mb]}{[old] [23.6gb]-&gt;[24gb]/[28.5gb]}
[2015-08-04 06:20:45,932][ERROR][marvel.agent.exporter    ] [ipattern-es01.iad1] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.08.04/_bulk]: SocketTimeoutException[Read timed out]
[2015-08-04 06:21:12,690][INFO ][discovery.zen            ] [ipattern-es01.iad1] master_left [[ipattern-es02.iad1][CB3p3ZnZR_qUHGc05ZKK7Q][ipattern-es02.iad1][inet[/10.40.146.47:9300]]], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2015-08-04 06:21:12,690][WARN ][discovery.zen            ] [ipattern-es01.iad1] master left (reason = failed to ping, tried [3] times, each with  maximum [30s] timeout), current nodes: {[ipattern-es01.iad1][gqF0BDR_S9SAlSIfGZVx_g][ipattern-es01.iad1][inet[ipattern-es01.iad1/10.40.146.46:9300]],}
[2015-08-04 06:21:12,690][INFO ][cluster.service          ] [ipattern-es01.iad1] removed {[ipattern-es02.iad1][CB3p3ZnZR_qUHGc05ZKK7Q][ipattern-es02.iad1][inet[/10.40.146.47:9300]],}, reason: zen-disco-master_failed ([ipattern-es02.iad1][CB3p3ZnZR_qUHGc05ZKK7Q][ipattern-es02.iad1][inet[/10.40.146.47:9300]])
[2015-08-04 06:21:13,213][WARN ][action.bulk              ] [ipattern-es01.iad1] failed to perform indices:data/write/bulk[s] on remote replica [ipattern-es02.iad1][CB3p3ZnZR_qUHGc05ZKK7Q][ipattern-es02.iad1][inet[/10.40.146.47:9300]][ipattern-2015-07][3]
org.elasticsearch.transport.NodeDisconnectedException: [ipattern-es02.iad1][inet[/10.40.146.47:9300]][indices:data/write/bulk[s][r]] disconnected
[2015-08-04 06:21:13,213][DEBUG][action.bulk              ] [ipattern-es01.iad1] observer timed out. notifying listener. timeout setting [1m], time since start [1.4m]
[2015-08-04 06:21:13,213][WARN ][cluster.action.shard     ] [ipattern-es01.iad1] can't send shard failed for [ipattern-2015-07][3], node[CB3p3ZnZR_qUHGc05ZKK7Q], [R], s[STARTED], no master known.
[2015-08-04 06:21:13,213][DEBUG][action.bulk              ] [ipattern-es01.iad1] observer timed out. notifying listener. timeout setting [1m], time since start [1.4m]
[2015-08-04 06:21:13,213][DEBUG][action.bulk              ] [ipattern-es01.iad1] observer timed out. notifying listener. timeout setting [1m], time since start [1.4m]
[2015-08-04 06:21:13,213][DEBUG][action.bulk              ] [ipattern-es01.iad1] observer timed out. notifying listener. timeout setting [1m], time since start [1.6m]
[2015-08-04 06:21:13,213][DEBUG][action.bulk              ] [ipattern-es01.iad1] observer timed out. notifying listener. timeout setting [1m], time since start [1.6m]
[2015-08-04 06:21:13,213][DEBUG][action.bulk              ] [ipattern-es01.iad1] observer timed out. notifying listener. timeout setting [1m], time since start [1.4m]
[2015-08-04 06:21:13,213][DEBUG][action.bulk              ] [ipattern-es01.iad1] observer timed out. notifying listener. timeout setting [1m], time since start [1.4m]
[2015-08-04 06:21:13,217][WARN ][action.bulk              ] [ipattern-es01.iad1] failed to perform indices:data/write/bulk[s] on remote replica [ipattern-es02.iad1][CB3p3ZnZR_qUHGc05ZKK7Q][ipattern-es02.iad1][inet[/10.40.146.47:9300]][ipattern-2015-07][3]
</comment><comment author="kenny-ye" created="2015-08-06T02:36:35Z" id="128217898">And one more question. After indexing 12 million documents, even after old GC, the java heap usage is about 20G. Is it normal? Unlike common log files, our document is a litter complicated and average size is about 500K. And monitoring with Marval, I found the data under "Index Statistics-&gt;memory-&gt;LUCENE MEMORY" keeps on increasing. It is now about 30G. Does it related with Java heap occupation? What is the LUCENE MEMORY for? How to mitigate the memory issue?

We use Elasticsearch 1.6.0. Two data nodes with 5 shards and 1 replica. Now the total index size in each node is about 5TB.
</comment><comment author="markwalkom" created="2015-08-06T02:38:05Z" id="128218437">What does your cluster look like, how many nodes, are they in the same datacenter?

Also you're better off putting that second question on https://discuss.elastic.co :)
</comment><comment author="kenny-ye" created="2015-08-06T02:58:24Z" id="128222471">Hi markwalkom,

Thanks for your suggestion. I already submit it to https://discuss.elastic.co/t/large-index-size-cause-high-java-heap-occupation/26936
</comment><comment author="brwe" created="2015-08-06T08:39:35Z" id="128292691">@kenny-ye Thanks a lot for the logs! This line:

```
[2015-08-04 06:21:13,213][WARN ][cluster.action.shard ] [ipattern-es01.iad1] can't send shard failed for [ipattern-2015-07][3], node[CB3p3ZnZR_qUHGc05ZKK7Q], [R], s[STARTED], no master known.
```

might indicate that you were running into #7572 . I assume elasticsearch issued a [synced flush](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-synced-flush.html) somewhen after primary and replica got out of sync. Synced flush does not check the number of documents in primary and replica before executing. It only makes sure that no indexing is going on while flushing. Such an inconsistency between primary and replica remains undetected until recovery starts.

To allocate the replica, can you try the following:

```
POST ipattern-es01.iad1/_flush?force
```

This will remove the sync id from the primary and make sure elasticsearch will not try to recover the files on the replica copy that misses the documents.
Then can you do 

```
POST /_cluster/reroute
```

this should then cause the replica to be recovered from the primary which has the documents. 
</comment><comment author="clintongormley" created="2016-01-26T17:45:33Z" id="175138030">No further info. Closing
</comment><comment author="anhlqn" created="2016-02-17T18:13:29Z" id="185330365">Ran into the same error with replica shards on one of my indexes. My fix was to change index setting to `"index.number_of_replicas": 0`, wait for the index to heal, then increase the number of replicas again.
</comment><comment author="abraxxa" created="2016-02-22T14:58:43Z" id="187219065">Just had the same problem after restarting one of my three nodes.
Thanks for the fix @anhlqn!
</comment><comment author="nomoa" created="2017-01-07T13:41:04Z" id="271084508">Happened to us as well on a 24 nodes cluster running 2.3.5, this cluster is receiving mostly bulk index requests. It looks like the problem happened after many nodes dropped out from the cluster caused by some network hiccups.
We use auto_expand_replica, as suggested by @anhlqn deleting corrupted replicas by setting 0-0 seemed to fix the issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve integration tests output when ES cannot be started.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12660</link><project id="" key="" /><description>This can happen for a number of reasons, including bugs.
Today you will get a super crappy failure, telling you a .pid file
was not found... you can go look in target/integ-tests/elasticsearch-xxx/logs
and examine the log file, but thats kinda a pain and not easy if its a jenkins
server.

Instead we can fail like this:

```
start-external-cluster-with-plugin:
     [echo] Installing plugin elasticsearch-example-jvm-plugin...
    [mkdir] Created dir: /home/rmuir/workspace/elasticsearch/plugins/jvm-example/target/integ-tests/temp
     [exec] -&gt; Installing elasticsearch-example-jvm-plugin...
     [exec] Plugins directory [/home/rmuir/workspace/elasticsearch/plugins/jvm-example/target/integ-tests/elasticsearch-2.0.0-SNAPSHOT/plugins] does not exist. Creating...
     [exec] Trying file:/home/rmuir/workspace/elasticsearch/plugins/jvm-example/target/releases/elasticsearch-example-jvm-plugin-2.0.0-SNAPSHOT.zip ...
     [exec] Downloading ...........DONE
     [exec] PluginInfo{name='example-jvm-plugin', description='Demonstrates all the pluggable Java entry points in Elasticsearch', site=false, jvm=true, classname=org.elasticsearch.plugin.example.ExampleJvmPlugin, isolated=true, version='2.0.0-SNAPSHOT'}
     [exec] Installed example-jvm-plugin into /home/rmuir/workspace/elasticsearch/plugins/jvm-example/target/integ-tests/elasticsearch-2.0.0-SNAPSHOT/plugins/example-jvm-plugin
     [echo] Starting up external cluster...
     [echo] [2015-08-04 22:02:55,130][INFO ][org.elasticsearch.node   ] [smoke_tester] version[2.0.0-SNAPSHOT], pid[4321], build[e2a47d8/2015-08-05T00:50:08Z]
     [echo] [2015-08-04 22:02:55,130][INFO ][org.elasticsearch.node   ] [smoke_tester] initializing ...
     [echo] [2015-08-04 22:02:55,259][INFO ][org.elasticsearch.plugins] [smoke_tester] loaded [uber-plugin], sites []
     [echo] [2015-08-04 22:02:55,260][ERROR][org.elasticsearch.bootstrap] Exception
     [echo] java.lang.NullPointerException
     [echo]     at org.elasticsearch.common.settings.Settings$Builder.put(Settings.java:1051)
     [echo]     at org.elasticsearch.plugins.PluginsService.updatedSettings(PluginsService.java:208)
     [echo]     at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
     [echo]     at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
     [echo]     at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
     [echo]     at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
     [echo]     at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 25.178 s
[INFO] Finished at: 2015-08-04T22:03:14-05:00
[INFO] Final Memory: 32M/515M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (integ-setup) on project elasticsearch-example-jvm-plugin: An Ant BuildException has occured: The following error occurred while executing this line:
[ERROR] /home/rmuir/workspace/elasticsearch/plugins/jvm-example/target/dev-tools/ant/integration-tests.xml:176: The following error occurred while executing this line:
[ERROR] /home/rmuir/workspace/elasticsearch/plugins/jvm-example/target/dev-tools/ant/integration-tests.xml:142: ES instance did not start
```
</description><key id="99101358">12660</key><summary>improve integration tests output when ES cannot be started.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-05T02:19:43Z</created><updated>2015-08-05T02:52:10Z</updated><resolved>2015-08-05T02:52:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-05T02:42:50Z" id="127823306">Awesome! This would have saved me a bunch of time this morning debugging. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API - versioning with upsert doesn't error when version passed is 0 and document already exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12659</link><project id="" key="" /><description>When not using scripted upserts, i would assume the pattern to ensure that two upserts arriving at the same time for a new document doesn't result in an undefined outcome would be to use the version query parameter - but setting this to 0 doesn't throw an error (maybe this is expected)?

Gists for ES 1.5.0:

```
curl -s -XPOST localhost/test/t1/abc/_update?version=0 -d '{ "doc" : { "count": 1 }, "doc_as_upsert" : true }'
{ "_index": "test", "_type": "blah", "_id": "abc", "_version": 1 }

curl -s -XPOST localhost/test/t1/abc/_update?version=0 -d '{ "doc" : { "count": 1 }, "doc_as_upsert" : true }'
{ "_index": "test", "_type": "blah", "_id": "abc", "_version": 2 }
```

I would have expected the second call to fail as calling again with version set to 2 does fail as expected:

```
curl -s -XPOST localhost/test/t1/abc/_update?version=2 -d '{ "doc" : { "count": 1 }, "doc_as_upsert" : true }'
{ "_index": "test", "_type": "blah", "_id": "abc", "_version": 3 }

curl -s -XPOST localhost/test/t1/abc/_update?version=2 -d '{ "doc" : { "count": 1 }, "doc_as_upsert" : true }'
{"error":"VersionConflictEngineException[[test][3] [blah][abc]: version conflict, current [3], provided [2]]","status":409}
```

Update API doesn't have the op_type query parameter like the Index API does, which would be another way to achieve this
</description><key id="99092796">12659</key><summary>Update API - versioning with upsert doesn't error when version passed is 0 and document already exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chriswhite199</reporter><labels /><created>2015-08-05T00:59:50Z</created><updated>2015-08-05T16:18:13Z</updated><resolved>2015-08-05T16:18:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chriswhite199" created="2015-08-05T01:04:49Z" id="127809445">Some context - i previously had used a scripted upsert to achieve counter increments in ES 1.1.1, but moving to ES 1.5.0 and scripting being more frowned upon for security reasons i'm looking to do a document get, and then upsert (with incremented counter value or 1 where the document didn't exist in my get call) passing the version acquired from the previous get.
</comment><comment author="clintongormley" created="2015-08-05T16:18:13Z" id="128056243">Hi @chriswhite199 

Version numbers start at 1, so a version of zero is ignored.  It doesn't make sense to use version numbers with upserts as a version number can never match a missing doc.  Really you want to use the create API here instead.

Regarding scripts, there is nothing to stop you from having the script saved in a file in config/scripts/ and using it in your update command. File scripts don't suffer from the same problems as inline scripts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make _version searchable without a script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12658</link><project id="" key="" /><description>Currently, the only way to search against the `_version` field is to create a script (preferably not dynamic, but this is just an example):

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "script": {
          "script": "doc['_version'].value &gt; version",
          "params" : {
            "version" : 1
          }
        }
      }
    }
  }
}
```

However, like any script, this adds some unnecessary overhead. It would be ideal if we could use the field like other fields:

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "_version" : {
            "gt" : 1
          }
        }
      }
    }
  }
}
```
</description><key id="99063852">12658</key><summary>Make _version searchable without a script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Search</label><label>feature</label></labels><created>2015-08-04T21:15:10Z</created><updated>2015-08-04T22:59:20Z</updated><resolved>2015-08-04T22:59:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2015-08-04T21:21:59Z" id="127765648">Does that work? I get `MissingPropertyException[No such property: _version for class: Script1]` when I try a script.
</comment><comment author="pickypg" created="2015-08-04T21:22:46Z" id="127765784">Works in 1.6.0, which I am running locally.
</comment><comment author="drewr" created="2015-08-04T21:31:01Z" id="127768310">Mine was on a 2.0 build from a few weeks ago, c315d54c.
</comment><comment author="pickypg" created="2015-08-04T21:34:28Z" id="127769102">Also works with ES 1.7.1.
</comment><comment author="drewr" created="2015-08-04T22:07:07Z" id="127776206">A lot changed with `_version` in 2.0, IIRC. But searching on `_version` never was a good idea anyway since it's ephemeral. I don't think we've ever wanted to support that?
</comment><comment author="rmuir" created="2015-08-04T22:51:07Z" id="127785205">Its not indexed, lets not try to search it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw An Error For Missing or Bad Config Values for AWS-Cloud Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12657</link><project id="" key="" /><description>I ran into a stack trace today that referenced an [old issue](https://github.com/elastic/elasticsearch-cloud-aws/issues/31#issuecomment-127756166) related to the `aws-cloud` plugin. The issue basically boils down to this: I had `cloud.aws.secret` as my config value and not `cloud.aws.secret_key` (the latter is correct).

Now, elasticsearch failed to boot (as one would expect), but the main issue was the error message it gave me on boot failure did not clearly indicate that I had bad or missing config values. It just referenced proxy objects and circular references. It took me a good deal of tinkering in my config to figure out my root cause.

Now, what do I think can be done about this? In my opinion, a very nice feature to add to the AWS-Cloud plugin would be something that verifies that the given config values for the `cloud.aws` namespace are all present and correct. In my scenario, it would have noticed that `cloud.aws.access_key` was set, but `cloud.aws.secret_key` was not, and would have thrown an error indicating just that, which would have saved me 10-20 minutes.

However, I think this can be taken a step **further** and a new feature could be added to elasticsearch itself that would make this even easier. This new feature would basically amount to throwing an error any time an unknown config value is encountered. So, in my case, an error would be thrown because neither elasticsearch, or any of its installed plugins, has `cloud.aws.secret` listed as a valid config value. This would have alerted me to my issue almost immediately and would have been a wonderful user experience that would have made DevOps faster, and more pleasant. Basically, the way I envision this working, is that elasticsearch would have its own set of allowed config values and then all ES plugins installed are also allowed to have a set of allowed config values; upon boot, ES parses it's config file and references every key found with itself and its plugins, and then throws an error the second it finds an unknown key. 

To me, the former is merely an improvement to the `cloud-aws` plugin, but the latter is an actual elasticsearch improvement that should make DevOps management of elasticsearch easier in general (no more guessing about the validity of your config; if you put in a bum key name, elasticsearch just doesn't boot). Extrapolating on all of this, adding another runnable to `bin` that validates an elasticsearch config, ensuring all keys present are expected keys with valid values, would be pretty great too. That tool could then be integrated into more elaborate build systems, such as Jenkins, to verify the validity of any new ES configs before deploying them (though I'm not sure how many people regularly edit their ES configs and deploy those through some sort of CI process).

If any of this is unclear, or there are any questions, I am more than happy to answer them.

The comment that started this whole thing, and what prompted David to refer me to create this issue can be found [here](https://github.com/elastic/elasticsearch-cloud-aws/issues/31#issuecomment-127754817).
</description><key id="99062186">12657</key><summary>Throw An Error For Missing or Bad Config Values for AWS-Cloud Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hjc</reporter><labels><label>:Plugin Cloud AWS</label></labels><created>2015-08-04T21:05:22Z</created><updated>2015-08-05T16:23:30Z</updated><resolved>2015-08-05T15:56:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T15:56:07Z" id="128048452">Splendidly explained, @hjc1710 - thank you

This is on our list of improvements to make, even though it has been hanging around for a long time.  Closing in favour of https://github.com/elastic/elasticsearch/issues/6732
</comment><comment author="hjc" created="2015-08-05T16:23:30Z" id="128057403">Awesome, it's good to know that the ES team already saw this as an improvement that could be made and have it written down!

Hopefully someone will get the throughput to squash that older issue sometime soon. Regardless, you guys rock, keep up the great work!

Going to go to the other issue now and voice my support.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin entry points using Settings.getAsClass are broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12656</link><project id="" key="" /><description>I think the underlying issue in #12643 is how `SpawnModules` works. This is an entry point for various pluggable classes that goes out and tries to load plugged in modules from the classloader. However, this no longer works now that plugins get their own classloader.

I think the right thing to do is eliminate SpawnModules altogether. Any plugin entry points should:
1. Have a method on a module the plugin can register new classes with
2. Have tests that these entry points actually work with a class _outside of the core classloader_

I think this has to block 2.0 beta, otherwise many plugins are just broken (like all the discovery plugins right now).
</description><key id="99051906">12656</key><summary>Plugin entry points using Settings.getAsClass are broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T20:13:52Z</created><updated>2015-08-10T21:06:50Z</updated><resolved>2015-08-10T21:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-04T23:44:15Z" id="127797741">This code is really broken, this class needs to die right here and now, there is no other way about it.
</comment><comment author="uboness" created="2015-08-05T00:15:06Z" id="127801597">I'm sorry... I don't see any problem with `SpawnModules` itself here. this construct is there to just promote modularity. The real problem here is that we currently allow/rely on configuring classes in settings - that's the wrong thing here... the fact that in the `DiscoveryModule` we do it when we spawn modules doesn't make the `SpawnModules` wrong (we could just as have that wrong everywhere else in the codebase)

I agree with that in general, all plugin points should be exposed as methods on the relevant modules (and the `DiscoveryModule` is a great example where it's done wrong).
</comment><comment author="rmuir" created="2015-08-05T00:21:58Z" id="127802344">I'll bite: `SpawnShittyModules` is broken because it enforces no contract whatsoever, its the most worthless interface: the last fucking thing we need is 10-20 class instantiation mechanisms.

I consider this blocker issue open until this class is removed. If you like it, go get it from source history.
</comment><comment author="uboness" created="2015-08-05T02:48:42Z" id="127825180">&gt; I'll bite:

sure... I'm immuned :)

&gt; `SpawnShittyModules` is broken because it enforces no contract whatsoever, its the most worthless interface: the last fucking thing we need is 10-20 class instantiation mechanisms.

I don't know `SpawnShittyModules`... can't find it... but if you do, please go ahead and remove it (it sounds like...hmmm... a shitty class)

This issue however is about `SpawnModules` and its contract is as simple as "a module that spawns other modules"... one could have named it `CompoundModule` which is a module that is made out of other modules, but that's not the case... the name is `SpawnModules`. Would have been nice if this would be ingrained in the `AbstractModule` class, but it isn't. Guice's `AbstractModule#install` method comes close, but it is meant to be used within the `configure` method. The `SpawnModules` enables adding its sub-modules before the injector is created and therefore works well with the `PreProcessModule` construct.

If you want to remove it, sure... go ahead, but at the same time provide a good alternative that doesn't force a single place in the code knowing about all the services in the codebase(s). In other words, If modules `A` depends on `a1` and `a2` modules, and `a1` defines an internal service `s1`, there's not reason for `A` to directly know about `s1`\- it's an implementation detail of `a1` and `a1` only.

And if you do provide an alternative that enables same level of modularity, that'd be awesome.  But then it's a cleanup as it doesn't really fix any bugs, and therefore I don't see any reason for it to be a blocker for beta1. The referenced bug in this issue has nothing to do with the sole existence of `SpawnModules` class. It's an implementation bug that can be implemented with or without this class (if you load classes by names from settings, you're bound to hit it, regardless of where you do it)
</comment><comment author="rmuir" created="2015-08-05T03:00:53Z" id="127827455">&gt; If you want to remove it, sure... go ahead, but at the same time provide a good alternative that doesn't force a single place in the code knowing about all the services in the codebase(s). In other words, If modules A depends on a1 and a2 modules, and a1 defines an internal service s1, there's not reason for A to directly know about s1- it's an implementation detail of a1 and a1 only.

I care very very little about this.

The top priority is making sure that the code we load up and use, the classes themselves, are correct. This is fundamental to a working application.
</comment><comment author="rjernst" created="2015-08-05T03:07:00Z" id="127828734">The underlying issue (which took me a lot more reading to understand thanks to _tons_ of concepts with modules, services, components, etc) is `Settings.getAsClass`. This is used as a "plugin entry point by naming convention". While I still agree SpawnModules is shitty (it is too hard to comprehend the recursive nature of module loading with it), I will rename this issue to reflect the blocker: `Settings.getAsClass` must go.
</comment><comment author="costin" created="2015-08-10T17:00:14Z" id="129526593">As a side comment, the problem with `Settings` object acting as a `ClassLoader` is problematic because `Settings` tend to be global, for the entire ES instance while the `ClassLoader` clearly per-plugin (though in ES 1.x it was also global).
We can have Guice inject _plugin-aware_ `Settings` but still, one can _leak_ the `Settings` object to a different class by accident and one ends up with GC and CNFE issues.

Long story short, removing any class loading functionality from `Settings` is cleaner since as it stands right now, there are mixed concerns with different lifecycles.
We could simply inject the `ClassLoader` to the plugin starting hook or simply ask the plugin to determine that itself (`getClass().getClassLoader()`). It doesn't introduce any global registry (which is problematic in terms of cleanup) and further more, makes it difficult for plugins to get a hold of each-others `ClassLoader`s.
</comment><comment author="uboness" created="2015-08-10T17:04:50Z" id="129527727">@costin absolutely... agree that `Settings` should not hold a class loader at all. Even with this PR in things will still be broken for the exactly same reason you mention and we should not let anyone get a class loader from Settings (as it's bound to fail for plugins)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds a setting to disable source output in slowlog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12655</link><project id="" key="" /><description>When logging large documents via slowlog, setting `index.slow_logging.source`
to `false` prevents the source from being displayed.

Closes #4485 

I don't see any tests for slowlog, so I used these steps to manually test:

```
curl -XPUT 'http://localhost:9200/bar'

# don't log source
curl -XPUT 'localhost:9200/bar/_settings' -d '
{
    "index" : {
        "slow_logging.source" : false
    }
}'

# bump the index threshold to 0s
curl -XPUT 'http://localhost:9200/bar/_settings' -d '
{
    "index.search.slowlog.threshold.query.warn" : "10s", 
    "index.search.slowlog.threshold.fetch.debug": "500ms", 
    "index.indexing.slowlog.threshold.index.info": "0ms" 
}'



curl -XPUT 'http://localhost:9200/bar/tweet/2' -d '
{
    "user": "kimchy",
    "postDate": "2009-11-15T13:12:00",
    "message": "Trying out Elasticsearch, so far so good?"
}'


curl -XPUT 'localhost:9200/bar/_settings' -d '
{
    "index" : {
        "slow_logging.source" : true
    }
}'


curl -XPUT 'http://localhost:9200/bar/tweet/3' -d '
{
    "user": "kimchy",
    "postDate": "2009-11-15T13:12:00",
    "message": "Foo bar baz"
}'

```
</description><key id="99050497">12655</key><summary>Adds a setting to disable source output in slowlog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">metadave</reporter><labels><label>adoptme</label><label>low hanging fruit</label><label>review</label></labels><created>2015-08-04T20:05:06Z</created><updated>2015-08-11T15:59:44Z</updated><resolved>2015-08-11T14:35:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-04T20:17:20Z" id="127745120">I wonder if it'd be possible to make the setting true/false/some_number_of_characters. If the document has more than that many characters then you could just truncate it. You have to figure out truncation in utf-16 unicode characters but I imagine Lucene has a function for that. I like truncating because usually the neat parts of the document are at the front....

On the other hand if this gets any more complicated than it is it'll really need testing. Could you maybe make a constructor for IndexingSlowLog that takes an ESLog as a parameter? Then you could mock it and get the results. Or something like that.
</comment><comment author="jpountz" created="2015-08-06T09:40:44Z" id="128309898">+1 to making it take an integer instead of a boolean and truncating. Lucene doesn't have utility methods for that, but Java has: we could just read one more UTF16 char if the last one happens to be a [high surrogate](http://docs.oracle.com/javase/7/docs/api/java/lang/Character.html#isHighSurrogate%28char%29)?
</comment><comment author="nik9000" created="2015-08-06T12:02:27Z" id="128343071">Ah yes. I remember that. I was thinking about combining codepoints too but
we might be able to ignore those.
On Aug 6, 2015 5:40 AM, "Adrien Grand" notifications@github.com wrote:

&gt; +1 to making it take an integer instead of a boolean and truncating.
&gt; Lucene doesn't have utility methods for that, but Java has: we could just
&gt; read one more UTF16 char if the last one happens to be a high surrogate
&gt; http://docs.oracle.com/javase/7/docs/api/java/lang/Character.html#isHighSurrogate%28char%29
&gt; ?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12655#issuecomment-128309898
&gt; .
</comment><comment author="nik9000" created="2015-08-07T01:58:14Z" id="128560664">I'll take this in a few days if no one else does.
</comment><comment author="nik9000" created="2015-08-11T14:35:54Z" id="129908694">Obsoleted by #12806.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve plugin integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12654</link><project id="" key="" /><description>Currently some of these are very minimal:
- cloud plugins just check the plugin was installed
- most analyzer plugins just check the analyze api (ideally they have one that indexes a document too)

Improving these with additional rest tests would be great. The analyzer ones should be relatively simple tests.
</description><key id="99048267">12654</key><summary>improve plugin integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>low hanging fruit</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-08-04T19:55:39Z</created><updated>2015-10-01T11:20:21Z</updated><resolved>2015-09-19T15:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-04T20:04:58Z" id="127741089">This issue itself isn't really low hanging fruit - but lots of the tests @rmuir is proposing certainly are. They would be a great place for any contributor to start.

Cloud plugins are hard - to really test them you need to run them against the cloud environment. I don't know how many of those environments have public test instances....
</comment><comment author="rmuir" created="2015-08-04T20:07:02Z" id="127741507">Actually @dadoonet did a lot of work to improve the analysis tests that i described here recently. So they might be in good shape at the moment. But still might be worth looking at analysis/ and lang/ plugins to see if there are gaps.

I agree the cloud plugins are hard. Ideally we would setup a mock in pre-integration-test or something like that, even if its a limited mock of the service.
</comment><comment author="nik9000" created="2015-08-04T20:20:04Z" id="127746620">&gt; I agree the cloud plugins are hard. Ideally we would setup a mock in pre-integration-test or something like that, even if its a limited mock of the service.

Yeah - mocks are fun because you can simulate failure modes.
</comment><comment author="dadoonet" created="2015-08-04T20:23:04Z" id="127747732">Well about mock, I tried to do some mocking for azure. For example: https://github.com/elastic/elasticsearch-cloud-azure/blob/master/src/test/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceTwoNodesMock.java

Would be easier may be if we could use something like Mockito ?
</comment><comment author="nik9000" created="2015-08-04T20:42:18Z" id="127755447">&gt; Would be easier may be if we could use something like Mockito ?

I've used Mockito when testing other Elasticsearch plugins pretty successfully - but its not really as good as having a real mock service you can play with because you don't test the full communication paths. Mockito is one of those things that untethers you from reality a bit too easy.
</comment><comment author="clintongormley" created="2015-08-13T09:38:58Z" id="130592306">I'll take this and work on rest tests for the non-cloud plugins.  Cloud plugins will be dealt with by #12719
</comment><comment author="clintongormley" created="2015-09-19T15:16:26Z" id="141678976">Had a look at the existing REST tests and we seem to be pretty well covered here. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change master branch back to 2.0-beta1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12653</link><project id="" key="" /><description>So we have CI on what we are working on everywhere.
</description><key id="99045167">12653</key><summary>Change master branch back to 2.0-beta1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-08-04T19:39:20Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-04T20:11:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-04T19:43:26Z" id="127732304">mvn verify passes and i wiped .m2/repository and inspected contents afterwards to make sure it didnt download anything.
</comment><comment author="uboness" created="2015-08-04T20:08:06Z" id="127741739">LGTM
</comment><comment author="rjernst" created="2015-08-04T20:10:33Z" id="127742211">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master node not respecting network interface to be used from a node trying to join the cluster. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12652</link><project id="" key="" /><description>I am trying to connect elasticsearch node with multiple interfaces. In config both , publish and bind addresses are recognized and request sent to correct master node but reply coming back from master node is on incorrect interface.
Elasticsearch version  used :1.5.2

a. IP addresses and interfaces on the node trying to join:
27: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:06 brd ff:ff:ff:ff:ff:ff
    **inet 172.17.0.6/16** scope global eth0

29: ethwe: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 65535 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 4e:54:ec:5d:66:91 brd ff:ff:ff:ff:ff:ff
    inet **10.10.8.3/22** scope global ethwe

b. Config in elasticsearch.yml:
network.bind_host: _ethwe:ipv4_
network.publish_host: _ethwe:ipv4_

c. in Startup log:

2015-08-04 18:51:56,822][INFO ][node                     ] [esm01] version[1.5.2], pid[17], build[62ff986/2015-04-27T09:21:06Z]
[2015-08-04 18:51:56,822][INFO ][node                     ] [esm01] initializing ...
[2015-08-04 18:51:56,826][INFO ][plugins                  ] [esm01] loaded [], sites []
[2015-08-04 18:51:59,106][INFO ][node                     ] [esm01] initialized
[2015-08-04 18:51:59,106][INFO ][node                     ] [esm01] starting ...
[2015-08-04 18:51:59,436][INFO ][transport                ] [esm01] **bound_address {inet[/10.10.8.3:9300]}, publish_address {inet[/10.10.8.3:9300]**}
[2015-08-04 18:51:59,454][INFO ][discovery                ] [esm01] ES_THOR/Z1TGBjjUR5qwT9a1gXArAw
[2015-08-04 18:52:29,455][WARN ][discovery                ] [esm01] waited for 30s and no initial state was set by the discovery
[2015-08-04 18:52:29,492][INFO ][http                     ] [esm01] bound_address {inet[/10.10.8.3:9200]}, publish_address {inet[/10.10.8.3:9200]}
[2015-08-04 18:52:29,493][INFO ][node                     ] [esm01] started
[2015-08-04 18:52:32,590][INFO ][discovery.zen            ] [esm01] failed to send join request to master [[es-thr-m-02][4Nm0-1q3Tne2brPLO1oUKQ][es-thr-m-02.dbplat.altus.bblabs.rim.net][inet[/10.236.133.76:9300]]{data=false, master=true}], reason [RemoteTransportException[[es-thr-m-02][inet[/10.236.133.76:9300]][internal:discovery/zen/join]]; nested: ConnectTransportException[[esm01][inet[/10.10.8.3:9300]] connect_timeout[30s]]; nested: ConnectTimeoutException[connection timed out: /10.10.8.3:9300]; ]

d. When checking response from master node using tcpflow, response coming to wrong interface.

cat 172.017.000.006.34485-010.236.133.076.09300
ESw&#222;&amp;internal:discovery/zen/unicast_gte_1_4
&#178;&#208;^ES_THOResm01Z1TGBjjUR5qwT9a1gXArAwesm01.weave.local
**172.17.0.6**
  @7ESy&#222;&amp;internal:discovery/zen/unicast_gte_1_4                                                    $&#178;&#208;^ES_THOResm01Z1TGBjjUR5qwT9a1gXArAwesm01.weave.local
**172.17.0.6**
  @9
</description><key id="99043812">12652</key><summary>Master node not respecting network interface to be used from a node trying to join the cluster. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels /><created>2015-08-04T19:30:40Z</created><updated>2016-01-26T17:45:03Z</updated><resolved>2016-01-26T17:45:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T17:45:03Z" id="175137897">Networking has had a significant overhaul in 2.x - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bats tests should use plugins built in the elasticsearch repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12651</link><project id="" key="" /><description>The bats tests currently install watcher to test installing plugins. It should really install one of the plugins built by the elasticsearch repository. Watcher can test itself. This will also solve the issue of not being able to run the tests properly when there isn't a watcher plugin available for the version of elasticsearch we're building. There will always be an appropriate version of the locally built plugins around because we build it as part of the build.
</description><key id="99042328">12651</key><summary>Bats tests should use plugins built in the elasticsearch repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label></labels><created>2015-08-04T19:21:13Z</created><updated>2015-08-19T22:25:19Z</updated><resolved>2015-08-17T19:18:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-04T19:49:10Z" id="127733427">Sounds like a good idea!
</comment><comment author="nik9000" created="2015-08-04T19:51:35Z" id="127734364">I'm so going to do this once we merge https://github.com/elastic/elasticsearch/pull/12646
</comment><comment author="jaymode" created="2015-08-11T15:28:09Z" id="129929921">I like this idea, but do we have a plugin in this repository that has a `bin` and `config` dir inside it? I think some of the tests look for that so we may need to add a test one with those directories or something
</comment><comment author="uboness" created="2015-08-11T15:30:25Z" id="129930661">+1 on ditching watcher... and +1 on a plugin with `config` and `bin` dirs
</comment><comment author="dadoonet" created="2015-08-11T15:36:48Z" id="129932706">I guess we can add that to the jvm example plugin: https://github.com/elastic/elasticsearch/tree/master/plugins/jvm-example
?
</comment><comment author="nik9000" created="2015-08-11T15:41:04Z" id="129934999">&gt; I like this idea, but do we have a plugin in this repository that has a bin and config dir inside it? I think some of the tests look for that so we may need to add a test one with those directories or something
&gt; 
&gt; I guess we can add that to the jvm example plugin: https://github.com/elastic/elasticsearch/tree/master/plugins/jvm-example

I've already started....
</comment><comment author="nik9000" created="2015-08-11T15:41:50Z" id="129935400">https://github.com/elastic/elasticsearch/pull/12787
</comment><comment author="nik9000" created="2015-08-17T19:17:34Z" id="131934468">Ok! All done. The tests all use the jvm-example plugin and no longer try to use watcher. They deserve some more refactoring and we should really be installing all the plugins to make sure they make it. But I'll do that under #12717.
</comment><comment author="nik9000" created="2015-08-17T19:20:24Z" id="131935039">Silly closed icon! I didn't reject this issue! I fixed it in four different pull requests!
</comment><comment author="dadoonet" created="2015-08-18T17:32:17Z" id="132289667">@nik9000 Why did not you push your change to 2.0 branch? On purpose?
cc @clintongormley 
</comment><comment author="nik9000" created="2015-08-19T21:45:55Z" id="132798300">&gt; @nik9000 Why did not you push your change to 2.0 branch? On purpose?

Do we need it in 2.0? I was going by the mantra of "don't push anything we don't need there". I think I have quite a few bats changes just in master that aren't in 2.0 yet. I can push them if you think its important. For the most part it'd probably work just fine to test 2.0-beta-whatever changes by staging them in the testroot or changing the elasticsearch.verion in master.
</comment><comment author="dadoonet" created="2015-08-19T22:25:19Z" id="132806375">I don't know. While merging some of my PR from master to 2.0 branch, I just discovered that was not merge and that 2.0 branch still wants to install shield for vagrant tests.

Not a big deal to me. Was just asking if it was intentional not to merge it or just that you forgot :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel not updating </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12650</link><project id="" key="" /><description>Marvel will not update when I kill a node in the cluster, the shard allocation will not work also. It is also saying I have 3 nodes in the cluster summary but it shows all 4 nodes in the nodes statistics. I was using Marvel last week and it worked fine, but now it is not working well
</description><key id="99040717">12650</key><summary>Marvel not updating </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bosfan265</reporter><labels /><created>2015-08-04T19:12:13Z</created><updated>2016-01-26T17:44:26Z</updated><resolved>2016-01-26T17:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T17:44:26Z" id="175137731">Closing as Marvel 2 is a complete rewrite
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changed pid file path causes init script to fail to restart ES after upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12649</link><project id="" key="" /><description>With RESTART_ON_UPGRADE disabled (default for at least 1.5.2), the init script of ES 1.7.x will be unable to restart ES after a Debian package upgrade from 1.5.x or earlier.

This is because the pid file path changed in 1.6.0 (IIRC), so when a user invokes e.g. `/etc/init.d/elasticsearch restart` after the package has been upgraded no pid file is found and ES is deemed to not be running. The subsequent start will then fail since it can't bind to any ports as they're still busy by the running ES process.

This broke my automated upgrade from 1.5.2 to 1.7.1, requiring a manual shutdown of ES on affected machines and/or a package rollback. Could we try a little harder to find the pid file by e.g. introducing a fallback pid file in the 1.7.x series? Perhaps migrating the old pid file with something as simple as

```
LEGACY_PID_FILE="/var/run/elasticsearch.pid"
if [ -f "$LEGACY_PID_FILE" -a "$LEGACY_PID_FILE" != "$PID_FILE" ] ; then 
        mv "$LEGACY_PID_FILE" "$PID_FILE" || exit 1
fi
```

right after the PID_DIR creation in /etc/init.d/elasticsearch would do?
</description><key id="99040677">12649</key><summary>Changed pid file path causes init script to fail to restart ES after upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magnusbaeck</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2015-08-04T19:12:03Z</created><updated>2016-01-26T17:43:58Z</updated><resolved>2016-01-26T17:43:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-05T08:36:14Z" id="127915164">@magnusbaeck I was thinking of something similar - it sounds a good idea to me.
</comment><comment author="magnusbaeck" created="2015-08-05T08:50:15Z" id="127919144">Great. I'll look into a patch.
</comment><comment author="magnusbaeck" created="2015-08-08T19:06:58Z" id="129036231">This is a duplicate of #11747. Keeping this one open for now since it's more descriptive and already has the "bug" label.
</comment><comment author="clintongormley" created="2016-01-26T17:43:58Z" id="175137586">We no longer restart after upgrade.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match phrase query fvh highlighter issue.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12648</link><project id="" key="" /><description>All the results that are returned contain the words monkey and business within 5 slop of each other.  However,  some results did not result a highlight field.  When i checked those docs, i noticed that all of them have the words "monkey" and "business" in reserve order.  In another word, all those docs that didn't return a highlight have the word "business" appear before "monkey".  

If the match phrase query with slop is matching words regardless of position, shouldn't the highlight also follow that?  Seems like a bug to me.

Query:
{
    "highlight": {
        "fields": {
            "content": {
                "type": "fvh"
            }
        }, 
        "fragment_size": 100, 
        "number_of_fragments": 10
    }, 
    "query": {
        "filtered": {
            "filter": {
                "and": [
                    {
                        "range": {
                           "doc_date": {
                                "gte": "20150101t235959", 
                                "lte": "20150730t235959"
                            }
                        }
                    }
                ]
            }, 
            "query": {
                "match_phrase": {
                    "content": {
                        "query": "monkey business", 
                        "slop": 5
                    }
                }
            }
        }
    }
}
</description><key id="99031992">12648</key><summary>Match phrase query fvh highlighter issue.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">lifo888</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2015-08-04T18:37:17Z</created><updated>2016-11-24T19:32:51Z</updated><resolved>2016-11-24T19:32:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-04T18:40:39Z" id="127707370">Just so we have a complete bug report, do can you make a [curl recreation](https://www.elastic.co/help)? I think this is a bug from your description but it'd be nice to have a runnable example.
</comment><comment author="lifo888" created="2015-08-04T20:12:27Z" id="127742588">curl  '127.0.0.1:9200/my_index/my_type/_search?pretty' - d '
{
    "_source": ["doc_date"],
        "highlight" : {
        "fields": {"content": {"type": "fvh"}},
            "fragment_size" : 100,
            "number_of_fragments" : 10
    },
    "query" : {
            "filtered": {
                "filter": {
                    "range": {
                        "doc_date": {
                            "gte": "20150101t235959",
                                "lte" : "20150730t235959"
                        }
                    }
                },
                    "query": {
                    "match_phrase": {
                        "content": {
                            "query": "monkey business",
                                "slop" : 5
                        }
                    }
                }
            }
        },
            "size": 100, "from" : 0
}
'

This is the full query.  Some of the entries in query result contain highlight and others done.
</comment><comment author="lifo888" created="2015-08-12T17:09:39Z" id="130374935">Any updates here?
</comment><comment author="nik9000" created="2015-08-12T18:23:16Z" id="130400762">I'm still deep in non highlighting stuff and don't have much time to
contribute over the next week and a half anyway. Even when I do I'm not
sure if this will be my highest priority.
On Aug 12, 2015 10:09 AM, "Guang.Chen" notifications@github.com wrote:

&gt; Any updates here?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12648#issuecomment-130374935
&gt; .
</comment><comment author="satazor" created="2016-07-06T09:23:19Z" id="230722018">I'm hitting this issue too. @lifo888 did you find any workaround?
</comment><comment author="clintongormley" created="2016-11-24T19:32:51Z" id="262835713">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update nested-type.asciidoc mapping example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12647</link><project id="" key="" /><description>The example `users` mapping is not consistent with the example document data which has `user` fields.
</description><key id="99031603">12647</key><summary>Update nested-type.asciidoc mapping example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">loopmachine</reporter><labels><label>docs</label><label>v1.6.3</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T18:35:21Z</created><updated>2015-08-09T08:09:57Z</updated><resolved>2015-08-04T18:45:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-04T18:45:05Z" id="127708620">Looks good. Thanks @loopmachine!
</comment><comment author="nik9000" created="2015-08-04T18:51:27Z" id="127713440">And merged to 1.6/1.7/master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run package tests in vagrant</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12646</link><project id="" key="" /><description>This creates a module in distribution called tests that can be run if you
have vagrant and virtualbox installed and will run the packaging tests in
trusty, precise, wheeze, jessie, centos-6.6, and centos-7.0.

See distribution/Vagrantfile for real documentation, but the tl/dr is run
`mvn verify -Pvagrant` and go get coffee.

Closes #12611 
</description><key id="99031490">12646</key><summary>Run package tests in vagrant</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T18:34:46Z</created><updated>2015-08-10T09:35:08Z</updated><resolved>2015-08-10T09:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-04T18:35:30Z" id="127705826">@electrical, @spinscale, @rmuir probably will have opinions on this.
</comment><comment author="dadoonet" created="2015-08-04T18:55:06Z" id="127715812">That looks awesome! Can we detect if vagrant is installed so we don't add the `tests` module if not?

Something like in https://github.com/elastic/elasticsearch/blob/master/distribution/pom.xml#L196-L220
</comment><comment author="nik9000" created="2015-08-04T18:59:54Z" id="127718753">&gt; That looks awesome! Can we detect if vagrant is installed so we don't add the tests module if not?

Right now we don't run it at all unless you do `-Pvagrant`. I fear its just too slow to do it by default if someone has vagrant installed. The first time you do it it downloads approximately 3.5GB of stuff. ~2.5GB of boxes and ~1GB of debs and rpms. I haven't timed the process of running all the tests once the machines are downloaded and provisioned but its tens of minutes. One machine is pretty manageable but 6 is intense.
</comment><comment author="rmuir" created="2015-08-04T19:02:57Z" id="127719956">I only glanced at it, and it looks awesome to me at a glance too. I hope to have some time to try it out locally soon but don't let me hold up the change, this is really needed.
</comment><comment author="nik9000" created="2015-08-05T17:16:55Z" id="128080303">I've marked this 2.0.0 - if we merge soon then I suspect it'll actually get into beta1.
</comment><comment author="nik9000" created="2015-08-05T19:06:51Z" id="128112353">@electrical, @spinscale, @rmuir, @dadoonet this could use another reviewer. @dakrone's run through it and found lots of issues I fixed. I'd love someone who's running debian or ubuntu to give it a run through. Baring that someone who can just review the code would be nice.
</comment><comment author="electrical" created="2015-08-05T19:27:41Z" id="128119876">I'll give this a run tomorrow UK time. 
</comment><comment author="nik9000" created="2015-08-05T19:28:35Z" id="128120351">Hurrah! Thanks for that.
</comment><comment author="nik9000" created="2015-08-06T01:07:00Z" id="128198730">Whoever looks at this next, keep in mind #12682.
</comment><comment author="dadoonet" created="2015-08-06T08:56:29Z" id="128295523">I tried your branch this morning on my laptop. Sadly until now, I did not succeed in making it run.

Here is what I did:

``` sh
mvn clean install -DskipTests
cd distribution
mvn verify -Pvagrant
```

It failed. But I saw in logs that the VM was successfully downloaded:

```
[INFO] Executing tasks

main:
     [echo] Running package tests on trusty, centos-7.0

vagrant-test-all-boxes:

vagrant-up:
     [exec] Bringing machine 'trusty' up with 'virtualbox' provider...
     [exec] ==&gt; trusty: Box 'ubuntu/trusty64' could not be found. Attempting to find and install...
     [exec]     trusty: Box Provider: virtualbox
     [exec]     trusty: Box Version: &gt;= 0
     [exec] ==&gt; trusty: Loading metadata for box 'ubuntu/trusty64'
     [exec]     trusty: URL: https://vagrantcloud.com/ubuntu/trusty64
     [exec] ==&gt; trusty: Adding box 'ubuntu/trusty64' (v20150609.0.10) for provider: virtualbox
     [exec]     trusty: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty64/versions/20150609.0.10/providers/virtualbox.box
     [exec] 
     [exec] There are errors in the configuration of this machine. Please fix
     [exec] the following errors and try again:
     [exec] 
     [exec] vm:
     [exec] * The 'fix-no-tty' provisioner could not be found.
     [exec] * The 'elasticsearch bats dependencies' provisioner could not be found.
     [exec] 
     [exec]     trusty: Progress: 0% (Rate: 0/s, Estimated time remaining: --:--:--)    trusty: Progress: 0% (Rate: 176k/s, Estimated time remaining: 0:39:21)   [...SKIPPED...]
trusty: Successfully added box 'ubuntu/trusty64' (v20150609.0.10) for 'virtualbox'!
```

Then, I tried to relaunch it again:

``` sh
cd tests
mvn verify
```

I get this error:

```
[INFO] --- maven-antrun-plugin:1.8:run (test-precise) @ elasticsearch-distribution-tests ---
[INFO] Executing tasks

main:
     [echo] Running package tests on trusty, centos-7.0

vagrant-test-all-boxes:

vagrant-up:
     [exec] Bringing machine 'trusty' up with 'virtualbox' provider...
     [exec] There are errors in the configuration of this machine. Please fix
     [exec] the following errors and try again:
     [exec] 
     [exec] vm:
     [exec] * The 'fix-no-tty' provisioner could not be found.
     [exec] * The 'elasticsearch bats dependencies' provisioner could not be found.
     [exec] 
```

Note:

``` sh
vagrant -v
```

gives

```
Vagrant 1.6.5
```

Do you know what it means? Did I do something wrong?
</comment><comment author="spinscale" created="2015-08-06T09:34:06Z" id="128308823">So when running all those tests, is the output somewhere redirected, so in case a build fails, I can follow what vagrant did without watching the screen?
</comment><comment author="dadoonet" created="2015-08-06T09:38:35Z" id="128309488">So I can confirm it starts with Vagrant 1.7.4 (thanks @spinscale for the help). May be we should try to control the version when starting Ant or at least document it? Not a big deal though.

Tests running ATM... Let's see where it goes...
</comment><comment author="dadoonet" created="2015-08-06T09:41:15Z" id="128309974">I'm wondering if we should not make each test part of the distribution we are testing.

I mean that I'd prefer do something like:

```
cd distribution/rpm
mvn verify -Pvagrant
```

```
cd distribution/deb
mvn verify -Pvagrant
```

Is this possible?
</comment><comment author="rmuir" created="2015-08-06T09:55:58Z" id="128312440">No, thats not possible or wanted. Sorry, we can't make our integration tests complicated. Please, add this to the `qa` folder.
</comment><comment author="dadoonet" created="2015-08-06T10:06:30Z" id="128314412">I'm not talking about complicating integration tests. I don't want it too.

I'm saying that if I run:

``` sh
cd distribution/rpm
mvn verify
```

It runs what we have today (package the rpm, test it using REST tests).

But it we run

``` sh
cd distribution/rpm
mvn verify -Pvagrant
```

It packages the rpm, test it using REST tests and run the Vagrant test for this distribution.

We should may be rename the `vagrant` profile to `qa` profile.
So if someone runs in the future (might be run by our release script and/or jenkins):

```
mvn install -Pqa
```

It would run for each module we have:
- compile
- unit test
- package
- integration test
- qa (test coverage, checkstyle, PMD whatever) and for some specific modules Vagrant tests.

So if a module is failing you don't have to run again all the Qa for all the things.

I Maven land, qa tools are supposed to run within their modules. Same for site plugin. If you want to generate a site, you basically generate it by module.

My 2 cents on it
</comment><comment author="rmuir" created="2015-08-06T10:23:26Z" id="128317890">I dont care about maven conventions in this case. Sorry, maven is wrong here and I am -1 to making integration tests complicated in this way.

Please, add it to the qa module. These vagrant tests depend on all kinds of stuff, like plugins. They are not specific to the packaging.
</comment><comment author="dadoonet" created="2015-08-06T10:36:20Z" id="128319697">&gt; These vagrant tests depend on all kinds of stuff, like plugins. They are not specific to the packaging.

Fair enough.
</comment><comment author="rmuir" created="2015-08-06T10:49:42Z" id="128323550">and by the way, the current solution (all in distribution/tests) is fine as a quick start. But ultimately we should move it to qa: https://github.com/elastic/elasticsearch/tree/master/qa

This way it can easily depend on e.g. kuromoji plugin or whatever and not require special 'mvn install' or anything like that.
</comment><comment author="nik9000" created="2015-08-06T12:12:27Z" id="128344384">I agree with the move. I don't want to put these tests with the package
they are testing because spinning up the vms had cost so we should do it
once if possible. Doing it this way isn't uncommon in maven multi project
land.

Looks like I missed a box rename. I'll do that.

Regarding redirecting the bats output so you don't have to watch the
console: that is what ci servers are for. Locally scrollback works just
fine. If bats generates some kind of debugging output (like every command
and its exit code or something) then I'll try to use that. Dump it to a
file and cat it on failure or something.
On Aug 6, 2015 6:49 AM, "Robert Muir" notifications@github.com wrote:

&gt; and by the way, the current solution (all in distribution/tests) is fine
&gt; as a quick start. But ultimately we should move it to qa:
&gt; https://github.com/elastic/elasticsearch/tree/master/qa
&gt; 
&gt; This way it can easily depend on e.g. kuromoji plugin or whatever and not
&gt; require special 'mvn install' or anything like that.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12646#issuecomment-128323550
&gt; .
</comment><comment author="nik9000" created="2015-08-06T12:18:51Z" id="128345668">&gt; So I can confirm it starts with Vagrant 1.7.4 (thanks @spinscale for the help). May be we should try to control the version when starting Ant or at least document it? Not a big deal though.
&gt; 
&gt; Tests running ATM... Let's see where it goes..

Ok - yeah - I can put a minimum version check for vagrant.
</comment><comment author="nik9000" created="2015-08-06T13:06:24Z" id="128356188">Ok - fixed the box name and added version checking for vagrant and rpmbuild. The version checking is with regexes which isn't great but it gets the job done.
</comment><comment author="dadoonet" created="2015-08-06T13:55:44Z" id="128373842">So on MacOS X with brew, I used the same issue as described at #12611.

&gt; and by the way, the current solution (all in distribution/tests) is fine as a quick start. But ultimately we should move it to qa: https://github.com/elastic/elasticsearch/tree/master/qa 

w00t! I did not notice you created this!

I think that within qa module we should separate then tests for rpm with all plugins and deb with all plugins. Not a big deal though.

BTW, how can I run the tests for `centos-7` only? 
</comment><comment author="nik9000" created="2015-08-06T13:59:27Z" id="128375241">&gt; I think that within qa module we should separate then tests for rpm with all plugins and deb with all plugins. Not a big deal though.

I still don't like it because it means we have to spin up the vms multiple times. I like just spinning them up once because it takes a while per.

&gt; BTW, how can I run the tests for centos-7 only?

Look in the Vagrantfile for more docs:

``` bash
mvn -Pvagrant -DboxesToTest=centos-7
```

should do it.
</comment><comment author="rmuir" created="2015-08-06T14:04:44Z" id="128376648">+1 for just `qa/vagrant` or whatever you like. similar to how you did it here, and the whole thing is a no-op unless you have `-Pvagrant`... start simple, we need the testing here.
</comment><comment author="nik9000" created="2015-08-06T14:07:21Z" id="128377839">Ok - I have one last pom tweak I'm testing and then I'd really like to start the squash/rebase dance. I want to squash here because I've made tons of tiny commits and master's changed a bit so I want to try to resolve the rebase all at once.  If folks are generally ok with merging this either as is or moved to `qa/vagrant` then I'll get started on that soon-ish.
</comment><comment author="nik9000" created="2015-08-06T14:08:03Z" id="128378149">While I'm waiting on reviews I've been looking for quality boxes to use for testing the other supported operating systems in https://www.elastic.co/subscriptions/matrix . That has been difficult. SLES doesn't have much love in the vagrant atlas.
</comment><comment author="rmuir" created="2015-08-06T14:12:46Z" id="128381706">&gt; If folks are generally ok with merging this either as is or moved to qa/vagrant then I'll get started on that soon-ish.

I would prefer the latter, only because it would potentially impact jenkins configs right? I'm guessing as a standalone job, we want `mvn -am -P vagrant -pl dev-tools,qa/vagrant verify` or similar? Later if its intended to run with normal jenkins jobs, we can try to make it simpler, e.g. activate the profile with -Dtests.vagrant property (to avoid profile complications), or whatever.

As far as all the logic, LGTM.
</comment><comment author="dadoonet" created="2015-08-06T14:13:44Z" id="128382449">I'm still failing on rpm test: 3 tests run for now, 3 failures :( with the error you mentioned in #12611:

```
     [exec] centos-7: not ok 35 [RPM] verify package removal
     [exec] centos-7: # (in test file /vagrant/tests/src/test/resources/packaging/scripts/40_rpm_package.bats, line 122)
     [exec] centos-7: #   `[ "$status" -eq 1 ] || [ "$status" -eq 0 ]' failed
```

Otherwise, it's a great addition. And it looks very good to me. I would indeed move it to qa/vagrant
</comment><comment author="nik9000" created="2015-08-06T14:19:19Z" id="128386809">&gt; As far as all the logic, LGTM.

I'll move it after I rebase I think. That just sounds less messy.

I don't think you'd _need_ to run dev-tools on this actually.
</comment><comment author="nik9000" created="2015-08-06T14:19:52Z" id="128387319">&gt; I'm still failing on rpm test: 3 tests run for now, 3 failures :( with the error you mentioned in #12611:

Yup. I'll fix that after merging this. It won't bother jenkins because he doesn't have vagrant and won't run the profile. Yet.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RecoveryFailedException when increasing replicas on index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12645</link><project id="" key="" /><description>We have some indexes that we load into the cluster with 0 replicas. We then increase the replicas to 1. Sometimes while these slave shards are restoring we get the following exception and were never able to recover:

org.elasticsearch.indices.recovery.RecoveryFailedException: [c140819_build_271_20150803120001][0]: Recovery failed from [Ruckus][qbYZpK3eTYymNMDz1Tlihg][worker-172-18-26-37][inet[/172.18.26.37:9300]]{aws_availability_zone=us-east-1a, master=false} into [Peregrine][0iXbWD7TRuiwPhSBYlsTEA][worker-172-18-29-201][inet[/172.18.29.201:9300]]{aws_availability_zone=us-east-1d, master=false} (no activity after [30m])

The elastic search version is 1.6.0. 
java version "1.7.0_75"
Java(TM) SE Runtime Environment (build 1.7.0_75-b13)

The cluster is a couple of weeks old. 
</description><key id="99020841">12645</key><summary>RecoveryFailedException when increasing replicas on index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">taizund12</reporter><labels><label>:Recovery</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-08-04T17:40:38Z</created><updated>2016-01-26T17:42:58Z</updated><resolved>2016-01-26T17:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T15:11:31Z" id="128032233">Hi @taizund12 

Looks like something timed out, maybe connectivity between the two availability zones?  Anything else in the logs?  Did recovery restart and then succeed or does it always fail?
</comment><comment author="drewdahlke" created="2015-08-05T17:35:23Z" id="128085583">Hey @clintongormley (I work with @taizund12). It seems to retry on loop for hours http://pastebin.com/KmF9UsDu

I have no problem curling the Ruckus node on 9200 from the machine logging that.
</comment><comment author="clintongormley" created="2015-08-05T18:06:50Z" id="128094316">@taizund12 could you increase the logging level to see what is going on?  Perhaps there are interesting logs on the other node?
</comment><comment author="drewdahlke" created="2015-08-05T20:32:48Z" id="128139497">Sure thing, which packages should I enable logging on? Global debug logging is too much.
</comment><comment author="clintongormley" created="2015-08-06T09:31:20Z" id="128308347">I'd go for `indices`
</comment><comment author="clintongormley" created="2016-01-26T17:42:57Z" id="175137291">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoShape data rejected while upgrading from 1.4.4 to 1.7.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12644</link><project id="" key="" /><description>Hi there,
I am evaluating an upgrade to 1.7.1 for our 1.4.4 production cluster. We seem to be having some data that has geo shapes that 1.4.4 accepted and indexed but 1.7.1 is choking on. The message below is only a warning but it seems to be rejecting the whole shard to be moved to the newer versioned machine, right? Can I reduce the level of validation checking for geo locations?

Do we have  a breaking change in geo shape validator between these versions?
[2015-08-04 11:41:19,543][WARN ][cluster.action.shard ] [hostname] [indexname][5] received shard failed for [indexname][5], node[lxvTTQWSRDiBA9Se21hjHw], relocating [BRUHrGoBQnO9MzyssyMCoQ], [P], s[RELOCATING], indexUUID [d2E1eeDHRB6FOs57k0IxaA], reason [Failed to perform [indices:data/write/bulk[s]] on replica, message [RemoteTransportException[[hostname][inet[/10.10.10.220:9302]][indices:data/write/bulk[s][r]]]; nested: MapperParsingException[failed to parse [message.location.geo]]; nested: InvalidShapeException[Provided shape has duplicate consecutive coordinates at: (-88.097892, 37.771743, NaN)]; ]]
</description><key id="99017952">12644</key><summary>GeoShape data rejected while upgrading from 1.4.4 to 1.7.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">ml4spark</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2015-08-04T17:22:58Z</created><updated>2015-11-11T19:38:49Z</updated><resolved>2015-11-11T19:38:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eliranmoyal" created="2015-08-10T12:53:12Z" id="129431101">+1
</comment><comment author="Ghost93" created="2015-08-10T12:56:02Z" id="129431735">+1
</comment><comment author="ml4spark" created="2015-08-11T10:39:41Z" id="129830350">Hi Nick, have you had a chance to take a look? Our upgrade to a newer version (and security fixes) is blocked until we have this resolved. Thanks
</comment><comment author="sergeant72" created="2015-08-11T12:19:43Z" id="129854646">+1
</comment><comment author="nknize" created="2015-08-11T13:45:26Z" id="129878288">@ml4spark can you (or anyone else experiencing this behavior) provide an example shape that throws the exception? In short the behavior is related to #9511 where the `ShapeBuilder` formerly failed on valid interior rings, or constructed incorrect base polygons due to shape ambiguity (a nasty form of silent spatial data corruption).

To eliminate these issues (especially the latter), we made the ShapeBuilder more stringent by ensuring full compliance with the OGC SFA spec.  Since this was a change to 1.4.5, 1.5.2, and 1.6.0 we'll need to make sure your 1.4.4 indexed shapes were interpreted correctly.

&gt; Can I reduce the level of validation checking for geo locations?

For safety reasons `ignore_malformed` is not yet supported for `geo_shape` fields.  We can certainly add support to accept non-compliant data (if the majority desires) so long as there is an understanding of the associated dangers. 
</comment><comment author="clintongormley" created="2015-08-11T13:58:08Z" id="129885856">&gt; For safety reasons ignore_malformed is not yet supported for geo_shape fields. We can certainly add support to accept non-compliant data (if the majority desires) so long as there is an understanding of the associated dangers.

I think we should definitely add `ignore_malformed` (default `false`), however I think it should behave in a consistent manner with all other uses of it: don't throw an exception but ignore the invalid value completely.

Using it to paper over invalid data is trappy and leads to hard to debug problems later on.  Either the data is valid, in which case you can query it, or it isn't, in which case you can't.
</comment><comment author="ml4spark" created="2015-08-11T14:16:01Z" id="129898160">Hi Guys, can you help me configure elasticsearch logging such that it provides the information about which document causes the issue. We have many TBs of data and it seems this issue is very seldom. I only know in which shard (~200GB) the issue was encountered based on the above message. We could modify/fix/delete the offending information if the log returned the document id of the offending document.
</comment><comment author="thebabelfish" created="2015-09-08T23:19:58Z" id="138729230">I encountered the same issue using logstash to import some postgis data.   Wanted to note that the geojson data passed the postgis st_isvalid(&lt;column&gt;) despite having duplicate coordinates which I confirmed do exist in my data.

I then bypassed logstash and single indexed the document with the invalid geojson and elaticsearch took it without error.  It was only when I bulk indexed (whether through logstash or "myself") that the duplicate coordinate error was raised.

Bringing it to attention for elasticsearch as a possible bug and the users looking for a "workaround".

I am running elasticsearch v1.7.1.
</comment><comment author="ml4spark" created="2015-09-29T15:35:32Z" id="144096258">Howdy elastic team,
just a brief note to let you know that we managed to avoid the issue by re-processing the offending data. We have upgraded all nodes to 1.7.1 now.
We do no longer require help with this issue.
</comment><comment author="muka" created="2015-11-06T11:31:00Z" id="154387928">+1 
</comment><comment author="nknize" created="2015-11-11T19:38:03Z" id="155888645">Thanks to all for the discussion. I'm closing this as a non-issue. For future reference I am including excerpts from the [OGC SFA](http://www.opengeospatial.org/standards/sfa) and  [ISO 19107:2003](http://www.iso.org/iso/catalogue_detail.htm?csnumber=26012) standard that we use for processing `geo_shape` types. 

&gt; A Curve is simple if it does not pass through the same Point twice with the possible exception of the two end points (Reference [1], section 3.12.7.3):
&gt; A Curve is closed if its start Point is equal to its end Point (Reference [1], section 3.12.7.3).
&gt; A Curve that is simple and closed is a Ring.
&gt; The boundary of a Polygon consists of a set of LinearRings that make up its exterior and interior boundaries; 

Not all geo processing packages interpret the spec the same, and adding leniency tends to introduce different bugs. Perhaps this presents the need for an Elasticsearch geo validator that can be used to preprocess the data.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to start AWS discovery with 2.0.0-SNAPSHOT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12643</link><project id="" key="" /><description>I started 2 AWS instances using latest 2.0.0-SNAPSHOT I built locally.

When starting with defaults, everything is fine.

When changing `elasticsearch.yml` to:

``` yml
discovery.type: ec2
discovery.ec2.tag.Name: dadoonet-test-2.0.0-SNAP
```

I get this error while launching:

```
[2015-08-04 16:37:18,363][ERROR][org.elasticsearch.bootstrap] Exception
NoClassSettingsException[Failed to load class setting [discovery.type] with value [ec2]]; nested: ClassNotFoundException[org.elasticsearch.discovery.ec2.Ec2DiscoveryModule];
    at org.elasticsearch.common.settings.Settings.loadClass(Settings.java:604)
    at org.elasticsearch.common.settings.Settings.getAsClass(Settings.java:592)
    at org.elasticsearch.discovery.DiscoveryModule.spawnModules(DiscoveryModule.java:53)
    at org.elasticsearch.common.inject.ModulesBuilder.add(ModulesBuilder.java:44)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:177)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:272)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.discovery.ec2.Ec2DiscoveryModule
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at org.elasticsearch.common.settings.Settings.loadClass(Settings.java:602)
    ... 8 more
```

The cloud plugin contains needed libs I think:

```
[ec2-user@ip-10-0-0-113 cloud-aws]$ ll
total 5776
-rw-rw-r-- 1 ec2-user ec2-user  513834  4 ao&#251;t  16:32 aws-java-sdk-core-1.10.0.jar
-rw-rw-r-- 1 ec2-user ec2-user 2153751  4 ao&#251;t  16:32 aws-java-sdk-ec2-1.10.0.jar
-rw-rw-r-- 1 ec2-user ec2-user  258130  4 ao&#251;t  16:32 aws-java-sdk-kms-1.10.0.jar
-rw-rw-r-- 1 ec2-user ec2-user  563917  4 ao&#251;t  16:32 aws-java-sdk-s3-1.10.0.jar
-rw-rw-r-- 1 ec2-user ec2-user  232771  4 ao&#251;t  16:32 commons-codec-1.6.jar
-rw-rw-r-- 1 ec2-user ec2-user   62050  4 ao&#251;t  16:32 commons-logging-1.1.3.jar
-rw-rw-r-- 1 ec2-user ec2-user   44814  4 ao&#251;t  16:32 elasticsearch-cloud-aws-2.0.0-SNAPSHOT.jar
-rw-rw-r-- 1 ec2-user ec2-user  592008  4 ao&#251;t  16:32 httpclient-4.3.6.jar
-rw-rw-r-- 1 ec2-user ec2-user  282793  4 ao&#251;t  16:32 httpcore-4.3.3.jar
-rw-rw-r-- 1 ec2-user ec2-user   39815  4 ao&#251;t  16:32 jackson-annotations-2.5.0.jar
-rw-rw-r-- 1 ec2-user ec2-user 1143162  4 ao&#251;t  16:32 jackson-databind-2.5.3.jar
-rw-rw-r-- 1 ec2-user ec2-user    2253  4 ao&#251;t  16:32 plugin-descriptor.properties
```

And the cloud jar file contains the class we are looking for: `org/elasticsearch/discovery/ec2/Ec2DiscoveryModule.class`

```
[ec2-user@ip-10-0-0-113 cloud-aws]$ unzip -l elasticsearch-cloud-aws-2.0.0-SNAPSHOT.jar 
Archive:  elasticsearch-cloud-aws-2.0.0-SNAPSHOT.jar
  Length      Date    Time    Name
---------  ---------- -----   ----
        0  08-04-2015 17:40   META-INF/
      525  08-04-2015 17:40   META-INF/MANIFEST.MF
        0  08-04-2015 17:40   org/
        0  08-04-2015 17:40   org/elasticsearch/
        0  08-04-2015 17:40   org/elasticsearch/cloud/
        0  08-04-2015 17:40   org/elasticsearch/cloud/aws/
        0  08-04-2015 17:40   org/elasticsearch/cloud/aws/blobstore/
        0  08-04-2015 17:40   org/elasticsearch/cloud/aws/network/
        0  08-04-2015 17:40   org/elasticsearch/cloud/aws/node/
        0  08-04-2015 17:40   org/elasticsearch/discovery/
        0  08-04-2015 17:40   org/elasticsearch/discovery/ec2/
        0  08-04-2015 17:40   org/elasticsearch/plugin/
        0  08-04-2015 17:40   org/elasticsearch/plugin/cloud/
        0  08-04-2015 17:40   org/elasticsearch/plugin/cloud/aws/
        0  08-04-2015 17:40   org/elasticsearch/repositories/
        0  08-04-2015 17:40   org/elasticsearch/repositories/s3/
     8232  08-04-2015 17:40   org/elasticsearch/cloud/aws/AwsEc2Service.class
     1641  08-04-2015 17:40   org/elasticsearch/cloud/aws/AwsModule.class
      689  08-04-2015 17:40   org/elasticsearch/cloud/aws/AwsS3Service.class
     1232  08-04-2015 17:40   org/elasticsearch/cloud/aws/AwsSigner.class
    11319  08-04-2015 17:40   org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.class
     7517  08-04-2015 17:40   org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.class
     6371  08-04-2015 17:40   org/elasticsearch/cloud/aws/blobstore/S3BlobStore.class
     3246  08-04-2015 17:40   org/elasticsearch/cloud/aws/blobstore/S3OutputStream.class
     8992  08-04-2015 17:40   org/elasticsearch/cloud/aws/InternalAwsS3Service.class
     2076  08-04-2015 17:40   org/elasticsearch/cloud/aws/network/Ec2NameResolver$Ec2HostnameType.class
     4004  08-04-2015 17:40   org/elasticsearch/cloud/aws/network/Ec2NameResolver.class
     3668  08-04-2015 17:40   org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.class
     1139  08-04-2015 17:40   org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider$1.class
     1472  08-04-2015 17:40   org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider$HostType.class
     9807  08-04-2015 17:40   org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.class
     2317  08-04-2015 17:40   org/elasticsearch/discovery/ec2/Ec2Discovery.class
     1635  08-04-2015 17:40   org/elasticsearch/discovery/ec2/Ec2DiscoveryModule.class
     2631  08-04-2015 17:40   org/elasticsearch/plugin/cloud/aws/CloudAwsPlugin.class
     6781  08-04-2015 17:40   org/elasticsearch/repositories/s3/S3Repository.class
     1099  08-04-2015 17:40   org/elasticsearch/repositories/s3/S3RepositoryModule.class
        0  08-04-2015 17:40   META-INF/maven/
        0  08-04-2015 17:40   META-INF/maven/org.elasticsearch.plugin/
        0  08-04-2015 17:40   META-INF/maven/org.elasticsearch.plugin/elasticsearch-cloud-aws/
     2172  08-04-2015 16:39   META-INF/maven/org.elasticsearch.plugin/elasticsearch-cloud-aws/pom.xml
      142  08-04-2015 17:40   META-INF/maven/org.elasticsearch.plugin/elasticsearch-cloud-aws/pom.properties
---------                     -------
    88707                     41 files
```
</description><key id="99012294">12643</key><summary>Unable to start AWS discovery with 2.0.0-SNAPSHOT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Discovery</label><label>:Plugin Cloud AWS</label><label>blocker</label><label>bug</label></labels><created>2015-08-04T16:52:17Z</created><updated>2015-08-10T21:06:50Z</updated><resolved>2015-08-10T21:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-04T17:08:41Z" id="127677828">We need tests that test this stuff... that needs to block any bugfix here.

I know if someone can fix it, they will feel pressure to push fix without tests. But we cannot develop software this way anymore!
</comment><comment author="dadoonet" created="2015-08-04T17:24:19Z" id="127683885">So I tried to run `Ec2DiscoveryITest` from my IDE using options:

```
-Dtests.thirdparty=true -Dtests.config=/Users/dpilato/Documents/Elasticsearch/work/aws/elasticsearch.yml -Des.logger.level=DEBUG
```

And it went well. My config is:

```
cloud:
  aws:
    access_key: "KEY"
    secret_key: "SECRET"

discovery.type: ec2
```

Same when running from command line with `mvn test`, I don't get this error. 

So I get only this when deploying the plugin in elasticsearch and trying to load AWS module.
</comment><comment author="rmuir" created="2015-08-04T18:00:42Z" id="127693284">ok, I think one thing we can do is cutover the thirdparty tests to run in the integration test phase so they run realistically? they are definitely integration tests. then they can be run in jenkins and we have more coverage for the discovery plugins: currently their integration tests only check that they were installed.
</comment><comment author="dadoonet" created="2015-08-04T18:51:39Z" id="127713578">So the problem is coming from the changes in ClassLoading.

The Discovery Module is started within a Node using the Node settings (and the Node classloader aka elasticsearch core classloader).
When the plugin is loaded, it's now loaded within a child classloader.

Discovery then tries to call something like `Classes.loadClass(settings.getClassLoader(), ...);`.

settings.getClassLoader() is actually elasticsearch core Classloader, not the plugin one.

So we need somehow to tell the discovery module that he should use the plugin Classloader if any.

I'm not sure yet how to do that.

I pushed some changes for now in my branch https://github.com/dadoonet/elasticsearch/tree/plugins/discovery but it does not fix the issue. It only falls back to the default discovery instead of failling elasticsearch to start.
</comment><comment author="dadoonet" created="2015-08-04T19:08:49Z" id="127723648">To reproduce the issue:

Build aws or azure plugin:

```
cd plugins/cloud-aws
mvn clean install -DskipTests
```

Build core:

```
cd core
mvn clean install -DskipTests
```

Build a distribution: 

```
cd distribution/tar
mvn clean install -DskipTests
cd target/releases
tar xzf elasticsearch-2.0.0-SNAPSHOT.tar.gz
cd elasticsearch-2.0.0-SNAPSHOT
bin/plugin install cloud-aws --url file:../../../../../plugins/cloud-aws/target/releases/elasticsearch-cloud-aws-2.0.0-SNAPSHOT.zip
```

Change `config/elasticsearch.yml` to:

``` yml
discovery.type: ec2
```

Then start elasticsearch.

If you want to iterate other the code, just in core run `mvn package` and then copy the elasticsearch jar file from target to `distribution/tar/target/releases/elasticsearch-2.0.0-SNAPSHOT/lib`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] mvn install renames artifacts when copying</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12642</link><project id="" key="" /><description>This PR:
- renames all distribution artifacts to `elasticsearch` so maven plugins will pick up the correct finalName without needing any hack.
- changes the groupId for every single distribution module as we can't have more than one module using the same groupId:artifactId
- does not attach anymore empty jar files for tar/zip/... modules as they don't contain any `src/main/java` stuff.

When you build it, you end up with:

```
$ tree ~/.m2/repository/org/elasticsearch/distribution
distribution
&#9500;&#9472;&#9472; deb
&#9474;   &#9492;&#9472;&#9472; elasticsearch
&#9474;       &#9500;&#9472;&#9472; 2.0.0-beta1-SNAPSHOT
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.deb
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.deb.md5
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.deb.sha1
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.md5
&#9474;       &#9474;   &#9492;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.sha1
&#9500;&#9472;&#9472; elasticsearch-distribution
&#9474;   &#9500;&#9472;&#9472; 2.0.0-beta1-SNAPSHOT
&#9474;   &#9474;   &#9500;&#9472;&#9472; elasticsearch-distribution-2.0.0-beta1-SNAPSHOT.pom
&#9474;   &#9474;   &#9500;&#9472;&#9472; elasticsearch-distribution-2.0.0-beta1-SNAPSHOT.pom.md5
&#9474;   &#9474;   &#9492;&#9472;&#9472; elasticsearch-distribution-2.0.0-beta1-SNAPSHOT.pom.sha1
&#9500;&#9472;&#9472; fully-loaded
&#9474;   &#9492;&#9472;&#9472; elasticsearch
&#9474;       &#9500;&#9472;&#9472; 2.0.0-beta1-SNAPSHOT
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.md5
&#9474;       &#9474;   &#9492;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.sha1
&#9500;&#9472;&#9472; rpm
&#9474;   &#9492;&#9472;&#9472; elasticsearch
&#9474;       &#9500;&#9472;&#9472; 2.0.0-beta1-SNAPSHOT
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.md5
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.sha1
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.rpm
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.rpm.md5
&#9474;       &#9474;   &#9492;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.rpm.sha1
&#9500;&#9472;&#9472; shaded
&#9474;   &#9492;&#9472;&#9472; elasticsearch
&#9474;       &#9500;&#9472;&#9472; 2.0.0-beta1-SNAPSHOT
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.jar
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.jar.md5
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.jar.sha1
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.md5
&#9474;       &#9474;   &#9492;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.sha1
&#9500;&#9472;&#9472; tar
&#9474;   &#9492;&#9472;&#9472; elasticsearch
&#9474;       &#9500;&#9472;&#9472; 2.0.0-beta1-SNAPSHOT
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.md5
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.sha1
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.tar.gz
&#9474;       &#9474;   &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.tar.gz.md5
&#9474;       &#9474;   &#9492;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.tar.gz.sha1
&#9492;&#9472;&#9472; zip
    &#9492;&#9472;&#9472; elasticsearch
        &#9492;&#9472;&#9472; 2.0.0-beta1-SNAPSHOT
            &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom
            &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.md5
            &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.pom.sha1
            &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.zip
            &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.zip.md5
            &#9500;&#9472;&#9472; elasticsearch-2.0.0-beta1-SNAPSHOT.zip.sha1
```

Closes #12536
</description><key id="98980383">12642</key><summary>[build] mvn install renames artifacts when copying</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T14:15:01Z</created><updated>2015-08-04T14:27:00Z</updated><resolved>2015-08-04T14:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-04T14:21:38Z" id="127629342">looks good to me. additionally full `mvn verify` passes on the branch, and i inspected .m2/repository and there is no org.elasticsearch, so I think its good to go.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Fix elastic.co download URLs, add snapshot ones</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12641</link><project id="" key="" /><description>The URL to download the main elasticsearch plugins did not match
what the S3 wagon is supposed to write to.

In addition this PR adds support for snapshots to access the
snapshot S3 bucket, so we can possibly download snapshot versions
of plugins.

Closes #12632
</description><key id="98976390">12641</key><summary>PluginManager: Fix elastic.co download URLs, add snapshot ones</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T13:57:02Z</created><updated>2015-08-05T17:04:35Z</updated><resolved>2015-08-05T11:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-04T14:33:45Z" id="127633855">The change looks good to me.
</comment><comment author="spinscale" created="2015-08-04T15:59:14Z" id="127658631">in case you are wondering, where the new URL scheme comes from, it was mentioned here https://github.com/elastic/elasticsearch/pull/12270
</comment><comment author="dadoonet" created="2015-08-04T16:02:41Z" id="127659497">Thanks @spinscale! I was wondering this when I started reviewing it! :)
</comment><comment author="nik9000" created="2015-08-04T18:53:37Z" id="127714748">LGTM
</comment><comment author="tlrx" created="2015-08-05T08:34:06Z" id="127914842">I like the snapshot support. LGTM FWIW
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ignore EngineClosedException on IndexShard#sync</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12640</link><project id="" key="" /><description>This method syncs the translog unless it's already synced. If the engine
is alreayd closed we are guaranteed to be synced already such that we can just
ignore this exception.

Closes #12603
</description><key id="98974125">12640</key><summary>Ignore EngineClosedException on IndexShard#sync</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2015-08-04T13:46:04Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-04T13:57:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-04T13:46:35Z" id="127618317">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't use port 9200/9300 for integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12639</link><project id="" key="" /><description>This just causes confusion if a dev is running ES locally already.

This fix is just a simple practical fix, we can look at more complicated stuff like http://www.mojohaus.org/build-helper-maven-plugin/reserve-network-port-mojo.html later, but we may want to refactor to ranges anyway to run distribution tests in parallel, etc.
</description><key id="98969654">12639</key><summary>Don't use port 9200/9300 for integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label><label>build</label></labels><created>2015-08-04T13:27:36Z</created><updated>2015-08-04T17:09:47Z</updated><resolved>2015-08-04T17:09:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-04T13:39:03Z" id="127615181">The change looks good to me. I left a comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove obsolete plugins.info_refresh_interval setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12638</link><project id="" key="" /><description>This setting has been removed in  #12367
</description><key id="98964662">12638</key><summary>Remove obsolete plugins.info_refresh_interval setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T13:10:05Z</created><updated>2015-08-04T19:48:05Z</updated><resolved>2015-08-04T19:47:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-04T15:52:38Z" id="127656795">LGTM
</comment><comment author="nik9000" created="2015-08-04T18:55:17Z" id="127715926">LGTM
</comment><comment author="tlrx" created="2015-08-04T19:47:38Z" id="127733097">@rjernst @nik9000 thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging tests try to use watcher for 1.7 in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12637</link><project id="" key="" /><description>The packaging tests try to download watcher/latests in the master/2.0 branches and fail to install watcher. If you manually build watcher for 2.0 from source and dump it into the current working directory then they work but this is a pain for some and impossible for others because the watcher repository is private to the elastic organization.
</description><key id="98963891">12637</key><summary>Packaging tests try to use watcher for 1.7 in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>blocker</label><label>test</label></labels><created>2015-08-04T13:07:39Z</created><updated>2015-08-07T12:43:49Z</updated><resolved>2015-08-07T12:43:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-07T09:25:47Z" id="128651538">I think you're talking about Shield; also these tests have been added to test that plugins with custom configuration files were correctly installed when elasticsearch is installed through TAR, RPM or DEB packages.

I think we can safely removed this test if we have good qa test for the plugin itself.
</comment><comment author="dadoonet" created="2015-08-07T09:30:44Z" id="128654386">Agreed. IMO tests relying on private plugins should be run within private repos not within elasticsearch public project.
</comment><comment author="nik9000" created="2015-08-07T12:39:11Z" id="128690223">&gt; Agreed. IMO tests relying on private plugins should be run within private repos not within elasticsearch public project.

Yeah - I'll move the tests to using kuromoji or something.
</comment><comment author="nik9000" created="2015-08-07T12:43:49Z" id="128690948">Lets resolve this by removing the watcher install entirely and install the plugins. And we have another issue for that that duplicates this, kind. Anyway, calling this one a dupe of #12717.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use UTC instead of default timezone for creation date in CAT endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12636</link><project id="" key="" /><description>this change was added recently which uses default timezone for the creation
date on CAT endpoints. We should be consistent and use UTC across the board.
This commit adds #getDefaultTimzone() to forbidden API and fixes the REST tests.

Relates to #11688 
@nik9000 do you wanna take a look since you merged #11688 
</description><key id="98959624">12636</key><summary>Use UTC instead of default timezone for creation date in CAT endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T12:46:24Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-04T12:54:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-04T12:47:46Z" id="127592266">see http://build-us-00.elastic.co/job/es_core_master_window-2008/1920/testReport/org.elasticsearch.test.rest/Rest0IT/test__yaml_cat_indices_10_basic_Test_cat_indices_output_/ for a related failure
</comment><comment author="rmuir" created="2015-08-04T12:52:05Z" id="127594271">looks good
</comment><comment author="nik9000" created="2015-08-04T13:01:44Z" id="127597142">&gt; see http://build-us-00.elastic.co/job/es_core_master_window-2008/1920/testReport/org.elasticsearch.test.rest/Rest0IT/test__yaml_cat_indices_10_basic_Test_cat_indices_output_/ for a related failure

I didn't realize it'd spit out Z there. I was trying to test with the -Dtests.timezone and it wasn't taking. Nor did I realize we were consistent about UTC - its good that we are though.
</comment><comment author="nik9000" created="2015-08-04T13:02:29Z" id="127597262">looks good to me too
</comment><comment author="s1monw" created="2015-08-04T13:04:55Z" id="127597661">&gt; I didn't realize it'd spit out Z there. I was trying to test with the -Dtests.timezone and it wasn't taking. Nor did I realize we were consistent about UTC - its good that we are though.

yeah it's better to be consistent and our timezone randomization will cause it to fail which is good too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a check if FieldMappers exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12635</link><project id="" key="" /><description>Closes #12572: Add a simple check if FieldMappers is not null 
</description><key id="98957676">12635</key><summary>Add a check if FieldMappers exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SergVro</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v1.7.2</label></labels><created>2015-08-04T12:36:00Z</created><updated>2015-09-08T14:56:06Z</updated><resolved>2015-09-08T14:55:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-04T15:36:02Z" id="127652593">@SergVro Can you add a test? You can see how it is tested on master here:
https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java#L49

I think the same change should be in the MissingFilterParser as well?
</comment><comment author="SergVro" created="2015-08-05T08:41:42Z" id="127916297">@rjernst I have added the same fix in the MissingFilterParser and tests for both filters
</comment><comment author="nik9000" created="2015-08-07T11:58:02Z" id="128682813">Note: this fix should apply to 1.6 and 1.7.
</comment><comment author="rjernst" created="2015-09-08T14:55:54Z" id="138591852">@SergVro Thanks, I merged this to 1.7. There won't be a 1.6.3 so I have not merged to there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a check if FieldMappers exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12634</link><project id="" key="" /><description>Closes #12572: Add a simple check if FieldMappers is not null 
</description><key id="98955518">12634</key><summary>Add a check if FieldMappers exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SergVro</reporter><labels /><created>2015-08-04T12:26:26Z</created><updated>2015-08-07T11:59:15Z</updated><resolved>2015-08-04T19:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-04T19:10:13Z" id="127724275">Closing because this is a dupe of #12635
</comment><comment author="SergVro" created="2015-08-05T06:51:04Z" id="127893369">Yes, it is the same changes but for different versions. Since the issue exists in both 1.6 and 1.7, I thought it should be fixed in both places. 
</comment><comment author="nik9000" created="2015-08-07T11:59:15Z" id="128682987">Ah! I see. Normally we use one pull request and the elastic employee who does the merging cherry picks the change to all appropriate branches. They'll stick a label on the pull request for all versions that are expected to get the fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use AllocationID for shard size caching on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12633</link><project id="" key="" /><description>Today we build a custom cache key that treats all replicas
equally for a shard. This might have problems if some replicas are
much bigger due to a pending merge etc. Now that we have an Allocaiton ID
we can utilize it as a cache key.
</description><key id="98954968">12633</key><summary>Use AllocationID for shard size caching on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>won't fix</label></labels><created>2015-08-04T12:22:17Z</created><updated>2016-01-05T08:51:39Z</updated><resolved>2016-01-05T08:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-04T12:22:30Z" id="127581839">@dakrone  can you take a look?
</comment><comment author="s1monw" created="2015-08-04T14:32:56Z" id="127633656">this actually has problems since we miss some essential testing here... I will try to work on the tests first
</comment><comment author="s1monw" created="2016-01-05T08:51:28Z" id="168936814">I am closing this since it's way out of date and I won't get back to this any time soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Ensure official plugins work with short notation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12632</link><project id="" key="" /><description>In order to simplify plugin manager usage, we need to ensure that the official plugins (those, that are part of the elasticsearch repo and thus bound to a release) can be installed with the simplified command

```
bin/plugin install elasticsearch-analysis-kuromoji
```

This is a blocker issue, to ensure this works as expected.
</description><key id="98945939">12632</key><summary>Plugins: Ensure official plugins work with short notation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T11:24:29Z</created><updated>2015-08-05T11:12:46Z</updated><resolved>2015-08-05T11:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>The check-license script now accepts either a directory or a .zip file only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12631</link><project id="" key="" /><description>Called as:

```
check_license_and_sha.pl --check path/to/licenses path/to/extracted/package/
```

or

```
check_license_and_sha.pl --check path/to/licenses path/to/file.zip
```

Relates to #12528
</description><key id="98944313">12631</key><summary>The check-license script now accepts either a directory or a .zip file only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T11:10:28Z</created><updated>2015-08-09T16:25:10Z</updated><resolved>2015-08-04T12:17:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-04T12:01:03Z" id="127577427">I pushed a commit to use this for all distribution tests... much simpler. I'm running a full verify just in case but I think this is ready to go.
</comment><comment author="rmuir" created="2015-08-04T12:14:33Z" id="127580510">mvn clean verify passes
</comment><comment author="clintongormley" created="2015-08-04T12:18:04Z" id="127581008">thanks @rmuir - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix upgrade RPM script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12630</link><project id="" key="" /><description>This is a partial backport of #12298. This fixes an
issue that rpms could not be upgraded, because of a bad
number check in the postrm script, which exits with a
failure.

Reminds us, we also need to test upgrading of RPMS in CI.

Closes #12606
</description><key id="98918431">12630</key><summary>Fix upgrade RPM script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.7.2</label></labels><created>2015-08-04T08:36:10Z</created><updated>2015-08-05T16:28:20Z</updated><resolved>2015-08-05T07:12:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-08-04T08:49:59Z" id="127529577">LGTM thanks for fixing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify BalanceUnbalancedClusterTest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12629</link><project id="" key="" /><description>This takes 10 seconds or more, while other allocation tests are almost instantaneous. Can we simplify this? It looks like it tries to do a basic allocation (5 shards, 1 replica) of a new index when a _ton_ of indexes already exist on just 4 nodes. Perhaps we could test similar circumstances without thousands of shards? Alternatively, we could just make this an integration test (leave the impl, but rename to IT). It doesn't really seem like a unit test as it is now.

Also, as a side note, this test is the only user of CatAllocationTestCase. Perhaps we can also eliminate this abstraction and just test directly (eliminating the zipped shard state)? @s1monw do you have any thoughts here?
</description><key id="98899372">12629</key><summary>Simplify BalanceUnbalancedClusterTest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Allocation</label><label>adoptme</label><label>test</label></labels><created>2015-08-04T06:28:28Z</created><updated>2016-01-26T17:42:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>node.name: "Franz Kafka"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12628</link><project id="" key="" /><description>Why is there a double quote?  Would it be ok not to have the double quote?

node.name: ${MY_NODE_NAME}

and MY_NODE_NAME=Franz Kafka would also work?
</description><key id="98886166">12628</key><summary>node.name: "Franz Kafka"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coderlol</reporter><labels /><created>2015-08-04T04:24:51Z</created><updated>2015-08-07T07:57:36Z</updated><resolved>2015-08-04T07:19:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-04T05:51:39Z" id="127486790">You should really set your own node name, it makes it easier to troubleshoot and monitor things.

However what do you want to know here? If double quotes can be used in `node.name`?
</comment><comment author="spinscale" created="2015-08-04T07:19:47Z" id="127505546">Hey,

so the double quote in the config file is optional in this example, but might be useful, if you are using arrays (which is not the case for hostnames). Your example setting environment variables would work as well, but in this case you need to use quotes.

Also, please ask such questions in our [forums](https://discuss.elastic.co/), as we try to use github issues for bugs/features/etc. Thanks!
</comment><comment author="coderlol" created="2015-08-07T07:57:36Z" id="128629687">Cool.  Thanks for the answer.  I did not realize you had a forum.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve site-example integ test to test served contents.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12627</link><project id="" key="" /><description>Today we only check that the plugin is loaded.

Closes #12578
</description><key id="98883333">12627</key><summary>Improve site-example integ test to test served contents.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T03:49:03Z</created><updated>2015-08-04T03:58:20Z</updated><resolved>2015-08-04T03:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-04T03:54:37Z" id="127470716">Nice! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix coverage analysis. Two versions of jacoco were being used and creating jar hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12626</link><project id="" key="" /><description /><key id="98878791">12626</key><summary>Fix coverage analysis. Two versions of jacoco were being used and creating jar hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T03:09:01Z</created><updated>2015-08-04T03:56:48Z</updated><resolved>2015-08-04T03:56:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-04T03:45:36Z" id="127469614">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NamingConventionTests should test subclasses of ESIntegTestCase end with IT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12625</link><project id="" key="" /><description>These are integration tests.
</description><key id="98869469">12625</key><summary>NamingConventionTests should test subclasses of ESIntegTestCase end with IT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T01:35:44Z</created><updated>2015-08-04T01:57:14Z</updated><resolved>2015-08-04T01:57:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-04T01:53:11Z" id="127453131">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cleanup more abstract test class -&gt; TestCase and integ -&gt; IT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12624</link><project id="" key="" /><description /><key id="98867062">12624</key><summary>cleanup more abstract test class -&gt; TestCase and integ -&gt; IT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-04T01:08:04Z</created><updated>2015-08-04T01:19:52Z</updated><resolved>2015-08-04T01:19:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-04T01:09:39Z" id="127445510">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename base tests cases to use "TestCase" suffix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12623</link><project id="" key="" /><description>Most of the abstract base test classes we have were previously @Ignored.
However, there were also some other tests ignored. Having two ways to
quiet tests is confusing, and clearly it has caused some tests
to get lost in the fold.

This change moves all base test classes to use the "TestCase" suffix,
which is not picked up by the test class name pattern. It also removes
@Ignore from (almost) all tests, and adds it to forbidden apis.
And since we were renaming, I shorted base test class names to use
"ES" instead of "Elasticsearch". I type this a lot of types a day,
and I have heard others express a similar desire for a shorter name.

closes #10659
</description><key id="98865105">12623</key><summary>Rename base tests cases to use "TestCase" suffix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-08-04T00:47:31Z</created><updated>2015-08-04T00:51:10Z</updated><resolved>2015-08-04T00:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-04T00:49:42Z" id="127443049">Yes, using Ignore there is dirty. +1 for this change!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GceComputeEngineTest is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12622</link><project id="" key="" /><description>All tests in this test class are ignored. I've changed to an AwaitsFix.
</description><key id="98864612">12622</key><summary>GceComputeEngineTest is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Plugin Cloud GCE</label><label>test</label></labels><created>2015-08-04T00:41:23Z</created><updated>2015-08-12T16:05:18Z</updated><resolved>2015-08-12T16:05:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-08T19:44:31Z" id="129040765">The tests actually failing are:

```
nodes_with_same_tags_and_one_tag_set
nodes_with_different_tags_and_two_tag_set
nodes_with_same_tags_and_one_tag_set
```

I found the following comment:

```
/**
     * We need to ignore this test from elasticsearch version 1.2.1 as
     * expected nodes running is 2 and this test will create 2 clusters with one node each.
     * @see ESIntegTestCase#ensureClusterSizeConsistency()
     * TODO Reactivate when it will be possible to set the number of running nodes
     */
```
</comment><comment author="rjernst" created="2015-08-08T19:48:17Z" id="129040925">Actually all the tests fail, just not consistently.
</comment><comment author="clintongormley" created="2015-08-09T08:52:10Z" id="129141472">@dadoonet would you mind taking a look at this please?
</comment><comment author="dadoonet" created="2015-08-12T08:17:38Z" id="130213404">Closing in favor of #12786
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DedicatedClusterSnapshotRestoreIT.chaosSnapshotTest is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12621</link><project id="" key="" /><description>The test should be fixed. It is always ignored. I've moved this to an AwaitsFix.
</description><key id="98862979">12621</key><summary>DedicatedClusterSnapshotRestoreIT.chaosSnapshotTest is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>test</label></labels><created>2015-08-04T00:20:36Z</created><updated>2016-11-17T08:35:12Z</updated><resolved>2016-11-17T08:35:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-11-17T08:35:12Z" id="261186505">The test has been removed in #21573.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BytesStreamsTests.testAccessAfterClose is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12620</link><project id="" key="" /><description>This test has @Ignore with this comment:

```
// we ignore this test for now since all existing callers of BytesStreamOutput happily
// call bytes() after close().
```
</description><key id="98862064">12620</key><summary>BytesStreamsTests.testAccessAfterClose is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-08-04T00:09:57Z</created><updated>2016-01-26T17:39:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-20T07:21:40Z" id="132916311">I think we could remove this test. 

&gt; ```
&gt; hhoffstaette added a note on Mar 4, 2014
&gt; @kimchy explicitly wanted close() a no-op because many callers still call close() prematurely and then something else.
&gt; ```

If close () is a no-op, then why we need this test. If the requirement changed now, we should change close() method.
</comment><comment author="rjernst" created="2015-08-31T02:43:14Z" id="136238304">We should fix any code that is calling `bytes()` after `close()`. This sounds very broken and error prone!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove references to tests.slow and tests.integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12619</link><project id="" key="" /><description>Another follow up to #12617.
</description><key id="98859632">12619</key><summary>Remove references to tests.slow and tests.integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-08-03T23:50:09Z</created><updated>2015-08-03T23:52:20Z</updated><resolved>2015-08-03T23:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-03T23:50:51Z" id="127435021">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid @Slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12618</link><project id="" key="" /><description>Now that all uses of @Slow are gone, we can forbid this annotations use
so tests are not confusing. Follow up to #12617.
</description><key id="98858863">12618</key><summary>Forbid @Slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-08-03T23:44:42Z</created><updated>2015-08-03T23:46:10Z</updated><resolved>2015-08-03T23:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-03T23:45:52Z" id="127434440">Haha, I like the error message for it. LGTM :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove uses of @Slow and @Integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12617</link><project id="" key="" /><description>Now that integ tests are moved into `mvn verify`, we don't really have
a need for @Slow, and especially not @Integration. This removes
uses of the first, and completely removes uses of the latter.
</description><key id="98856335">12617</key><summary>Remove uses of @Slow and @Integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-08-03T23:26:38Z</created><updated>2015-08-03T23:35:05Z</updated><resolved>2015-08-03T23:35:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-03T23:28:39Z" id="127430787">Worth adding `@Slow` to forbidden APIs to prevent people from accidentally adding it or forward-porting it in the future?

Otherwise, LGTM
</comment><comment author="rmuir" created="2015-08-03T23:33:19Z" id="127431323">+1 to followup with forbidding `@Slow` completely. Its there if we need it, but i think its ideal if we do not use it. we shouldnt even need to set the -D in our build at all. 

We can also cleanup some pom.xml logic related to slow/integ. 

I think its fine to do that in a followup change though, to make it easier to review as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut over some remaining integration tests to IT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12616</link><project id="" key="" /><description>A few subclasses got missed here
</description><key id="98854802">12616</key><summary>Cut over some remaining integration tests to IT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T23:09:06Z</created><updated>2015-08-03T23:18:04Z</updated><resolved>2015-08-03T23:18:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-03T23:10:36Z" id="127428582">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Give unit tests and integ tests separate load balancing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12615</link><project id="" key="" /><description>This is a mess today, e.g. tophints shows you slow integ tests
after unit tests runs and so on.

Lets track them independently, so we can keep an eye on slow
unit tests and slow integ tests, and have good load balancing.
</description><key id="98851969">12615</key><summary>Give unit tests and integ tests separate load balancing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T22:42:01Z</created><updated>2015-08-03T22:56:04Z</updated><resolved>2015-08-03T22:56:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-03T22:54:21Z" id="127426437">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update elision-tokenfilter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12614</link><project id="" key="" /><description>Typo fix
</description><key id="98849380">12614</key><summary>Update elision-tokenfilter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kagel</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T22:22:59Z</created><updated>2015-08-05T12:47:43Z</updated><resolved>2015-08-03T22:24:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-03T22:24:15Z" id="127421141">Thanks @kagel !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename integ tests to IT suffix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12613</link><project id="" key="" /><description>This rename effectively moves all integration tests to be run with `mvn
verify`. `mvn test` now runs in about 2 mins. This is a follow up to
</description><key id="98844686">12613</key><summary>Rename integ tests to IT suffix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-08-03T21:45:53Z</created><updated>2015-08-03T21:57:29Z</updated><resolved>2015-08-03T21:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-03T21:53:06Z" id="127416407">Looks great, thanks for doing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>question regarding _msearch?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12612</link><project id="" key="" /><description>I am using msearch API.
I used it to send 20 request and response of each request have time taken like 200ms - 500ms but total Multi request is taking 5 seconds.
I am wondering if elastic search msearch API internally run requests sequentially.
Or it is in parallel, is there some limit of maximum it runs in one batch. If it runs in batch, can we change the batch size?
</description><key id="98838773">12612</key><summary>question regarding _msearch?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lokeshhctm</reporter><labels /><created>2015-08-03T21:07:21Z</created><updated>2015-08-04T07:22:35Z</updated><resolved>2015-08-04T07:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-04T06:51:44Z" id="127499864">Hi @lokeshhctm 

Please ask questions like these on the forum instead: https://discuss.elastic.co/, many ES users would be more helpful. 

ElasticSearch will issue the 20 async search requests. So I will say ES exec these search in parallel. There is no batch. 

In addition, it might make sense not to put many requests in one multi search request, because the slowest of these queries will determine the total response time of your request and slow down all others. And also, the performance dependence on lots of factors, such as query complexity and network IO.
</comment><comment author="spinscale" created="2015-08-04T07:22:35Z" id="127505947">You might issue too much requests, so that the thread pools on your system (scaled by the number of CPUs) are running those requests sequentially instead of parallel. Check your thread pool stats/infos. See https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-thread-pool.html, https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html

Closing in order to continue this discussion in the forum.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automate the packaging tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12611</link><project id="" key="" /><description>Right now there are some packaging tests that run as [bats](https://github.com/sstephenson/bats) scripts. They are destructive to the system they run on and only work on certain operating systems. It doesn't make sense to test the RPM build on Debian, nor does it make sense to test the DEB build on CentOS.

I propose we automate some testing of these using vagrant or a similar technology. Vagrant sounds like a good place to start at least.
</description><key id="98827334">12611</key><summary>Automate the packaging tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label></labels><created>2015-08-03T19:59:43Z</created><updated>2015-08-10T09:35:07Z</updated><resolved>2015-08-10T09:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2015-08-04T08:47:35Z" id="127529187">I do a lot of testing using Docker. Perhaps worth looking into that?
Although i personally don't like it, Kitchen-CI has a plugin to run bats tests. and together with the docker hypervisor plugin you can run it in a docker container.
</comment><comment author="nik9000" created="2015-08-04T12:39:12Z" id="127587003">I've got vagrant mostly working actually. I can try docker from there too.
I'm weary as I've heard people say it doesn't work well for them but I'll
give it a shot.
On Aug 4, 2015 4:47 AM, "Richard Pijnenburg" notifications@github.com
wrote:

&gt; I do a lot of testing using Docker. Perhaps worth looking into that?
&gt; Although i personally don't like it, Kitchen-CI has a plugin to run bats
&gt; tests. and together with the docker hypervisor plugin you can run it in a
&gt; docker container.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12611#issuecomment-127529187
&gt; .
</comment><comment author="electrical" created="2015-08-04T12:41:02Z" id="127587891">I've been running puppet module tests on docker for over a year or so. It does require some tweaking and starting the docker containers slightly differently.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix plugin script to allow spaces in ES_HOME</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12610</link><project id="" key="" /><description>Fixes ES_HOME with spaces and installing plugins from a local directory
with spaces.

Closes #12504
</description><key id="98823757">12610</key><summary>Fix plugin script to allow spaces in ES_HOME</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T19:41:39Z</created><updated>2015-08-05T12:32:28Z</updated><resolved>2015-08-04T13:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-03T19:44:07Z" id="127383808">This is pretty much a clean forward port of #12544. It was difficult to test because the tests require a 2.0 compatible version of watcher which you have to build from source. And to do that you have to be past of the Elastic organization because that is a closed source plugin. That's certainly going to have to change.

Ping @tlrx, @spinscale, and @electrical who looked at the 1.7 version of this.
</comment><comment author="electrical" created="2015-08-03T20:29:45Z" id="127398691">Looks good to me as far as i can see.
</comment><comment author="tlrx" created="2015-08-04T07:25:47Z" id="127506437">&gt; It was difficult to test because the tests require a 2.0 compatible version of watcher which you have to build from source.

Are you talking about the BATS tests? If so, maybe we should move them apart.
</comment><comment author="jaymode" created="2015-08-04T11:56:25Z" id="127576728">LGTM
</comment><comment author="nik9000" created="2015-08-04T13:05:32Z" id="127597774">&gt; Are you talking about the BATS tests? If so, maybe we should move them apart.

They already have this problem because the plugin installer tries to fetch watch for 1.7 and can't. Since it isn't making anything worse I'll just merge this as is.

I'll create a followup issues to the bats tests not being consistent.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve sanity of securitymanager file permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12609</link><project id="" key="" /><description>conf/ and plugins/ do not need read-write access: this prevents a lot
of bad possibilities.
</description><key id="98815541">12609</key><summary>improve sanity of securitymanager file permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T18:53:33Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-03T21:39:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-08-03T19:07:16Z" id="127373276">LGTM
</comment><comment author="kimchy" created="2015-08-03T19:14:40Z" id="127376094">LGTM, added a comment around testing for plugins as well
</comment><comment author="rmuir" created="2015-08-03T21:00:26Z" id="127405257">I pushed a commit improving this test (it did really need it)
</comment><comment author="rjernst" created="2015-08-03T21:14:19Z" id="127407895">Nice! LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactors FunctionScoreQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12608</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="98814614">12608</key><summary>Refactors FunctionScoreQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>non-issue</label></labels><created>2015-08-03T18:48:41Z</created><updated>2015-09-22T16:04:35Z</updated><resolved>2015-09-22T16:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-08-03T18:50:18Z" id="127368556">@javanna Feel free to take a look. Thank you.
</comment><comment author="alexksikes" created="2015-08-13T15:10:35Z" id="130724352">@javanna I rebased it if you want to take a look. Thank you.
</comment><comment author="javanna" created="2015-09-22T16:04:20Z" id="142334132">superseded by #13653.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed typo "if" &gt; "is"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12607</link><project id="" key="" /><description /><key id="98813158">12607</key><summary>Fixed typo "if" &gt; "is"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jklingen</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T18:42:02Z</created><updated>2015-08-05T12:48:50Z</updated><resolved>2015-08-03T19:18:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-03T18:47:20Z" id="127367681">Looks good to me.

@jklingen can you sign the [cla](https://www.elastic.co/contributor-agreement) so I can merge it?
</comment><comment author="jklingen" created="2015-08-03T19:06:56Z" id="127373089">signed :heavy_check_mark: 
</comment><comment author="nik9000" created="2015-08-03T19:18:01Z" id="127376703">Rechecked CLA and passed. I'm not sure how to get the "checks" above to rerun but I believe this should be fine. Merging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.7.1 RPM warns "post remove script called with unknown argument"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12606</link><project id="" key="" /><description>From https://discuss.elastic.co/t/1-7-1-rpm-warns-post-remove-script-called-with-unknown-argument/26693:

```
$ ESVERSION=1.7.1
$ package=elasticsearch-$ESVERSION.noarch.rpm &amp;&amp; wget https://download.elasticsearch.org/elasticsearch/elasticsearch/$package -O /tmp/$package &amp;&amp; sudo rpm -Uvh /tmp/$package
--2015-08-02 20:27:24--  https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.7.1.noarch.rpm
Resolving download.elasticsearch.org... 174.129.211.252, 174.129.224.133, 23.21.179.119, ...
Connecting to download.elasticsearch.org|174.129.211.252|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 27323419 (26M) [application/x-redhat-package-manager]
Saving to: &#8220;/tmp/elasticsearch-1.7.1.noarch.rpm&#8221;

100%[===================================================&gt;] 27,323,419  8.56M/s   in 3.0s

2015-08-02 20:27:27 (8.56 MB/s) - &#8220;/tmp/elasticsearch-1.7.1.noarch.rpm&#8221; saved [27323419/27323419]

warning: /tmp/elasticsearch-1.7.1.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID d88e42b4: NOKEY
Preparing...                ########################################### [100%]
   1:elasticsearch          ########################################### [100%]
post remove script called with unknown argument `1'
warning: %postun(elasticsearch-1.7.0-1.noarch) scriptlet failed, exit status 1
```

Reported on Centos 6 and RHEL 6.6.
</description><key id="98812623">12606</key><summary>1.7.1 RPM warns "post remove script called with unknown argument"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">loren</reporter><labels><label>:Packaging</label></labels><created>2015-08-03T18:38:31Z</created><updated>2015-08-27T17:29:36Z</updated><resolved>2015-08-05T07:12:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-04T07:33:28Z" id="127507765">Currently digging into this. This happens when you upgrade and the installed elasticsearch version is &gt; 1.6...
</comment><comment author="tlrx" created="2015-08-04T07:36:49Z" id="127508318">I spotted it and resolved it in #12298  (see [this line](https://github.com/elastic/elasticsearch/pull/12298/files#diff-a1778856d31101e9f20ab266970e523aL42))
</comment><comment author="spinscale" created="2015-08-04T07:41:38Z" id="127508900">just found it as well. IMO the postrm fix should be backported into 1.7
</comment><comment author="spinscale" created="2015-08-05T07:12:54Z" id="127897202">Closed by https://github.com/elastic/elasticsearch/commit/a370e2d2261c7e08e4b6d4f29cf7aacf4d8f5414
</comment><comment author="tegansnyder" created="2015-08-27T15:19:02Z" id="135466362">@spinscale I'm still encountering this issue. It doesn't look like the rpm has been updated on your website.

I quick peak at `elasticsearch-1.7.1.noarch.rpm` reveals:

```
    # RedHat ####################################################
    1)
        # If $1=1 this is an install
        IS_UPGRADE=false
    ;;
    2)
        # If $1=1 this is an upgrade
        IS_UPGRADE=true
    ;;
```

Per the commit @a370e2d2261c7e08e4b6d4f29cf7aacf4d8f5414 it should read more like:

```
    1)
        # If $1=1 this is an upgrade
        IS_UPGRADE=true
    ;;
```
</comment><comment author="loren" created="2015-08-27T16:24:13Z" id="135484085">I think this fix is getting rolled into 1.7.2.

&gt; On Aug 27, 2015, at 8:20 AM, Tegan Snyder notifications@github.com wrote:
&gt; 
&gt; I'm still encountering this issue. It doesn't look like the rpm has been updated on your website.
&gt; 
&gt; I quick peak at elasticsearch-1.7.1.noarch.rpm reveals:
&gt; 
&gt; ```
&gt; # RedHat ####################################################
&gt; 1)
&gt;     # If $1=1 this is an install
&gt;     IS_UPGRADE=false
&gt; ;;
&gt; 2)
&gt;     # If $1=1 this is an upgrade
&gt;     IS_UPGRADE=true
&gt; ;;
&gt; ```
&gt; 
&gt; Per the commit @a370e2d https://github.com/elastic/elasticsearch/commit/a370e2d2261c7e08e4b6d4f29cf7aacf4d8f5414 it should read more like:
&gt; 
&gt; ```
&gt; 1)
&gt;     # If $1=1 this is an upgrade
&gt;     IS_UPGRADE=true
&gt; ;;
&gt; ```
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/12606#issuecomment-135466362.
</comment><comment author="tegansnyder" created="2015-08-27T17:29:36Z" id="135500531">@loren good to know. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update of documentation for better readability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12605</link><project id="" key="" /><description>"and" needs to be an article.
</description><key id="98791648">12605</key><summary>Update of documentation for better readability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dmabuada</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T16:46:39Z</created><updated>2015-08-05T12:49:02Z</updated><resolved>2015-08-03T18:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-03T18:12:14Z" id="127356831">Thanks @dmabuada !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[build] rename site example plugin to site-example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12604</link><project id="" key="" /><description>Just to be consistent in plugin naming convention we have today: `plugin_type-plugin_name`

So it's a `site` plugin.

Follow up for #12577.
</description><key id="98790280">12604</key><summary>[build] rename site example plugin to site-example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T16:42:13Z</created><updated>2015-08-03T16:46:13Z</updated><resolved>2015-08-03T16:45:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-03T16:43:06Z" id="127326601">+1, thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename indexShard.sync() to indexShard.trySync()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12603</link><project id="" key="" /><description>We should skip flushing/syncing if the engine is closed, because we sync during close anyway.
</description><key id="98754646">12603</key><summary>Rename indexShard.sync() to indexShard.trySync()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels /><created>2015-08-03T14:17:45Z</created><updated>2015-08-04T13:57:51Z</updated><resolved>2015-08-04T13:57:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add integration test phase for elasticsearch core/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12602</link><project id="" key="" /><description>Today for elasticsearch/core, `mvn test` runs a mix of unit and integration tests, and `mvn verify` doesnt really do anything additional.

Instead we currently have a lot of complicated stuff like `@Slow`, `@Integration`, and -D's to run them with to try to make it easier to run a subset of the tests, but I think this causes build failures because its too easy to forget to run tests.slow, etc before pushing. This stuff is really just workarounds for not setting things up properly.

So I think we should just move integration tests to the integration test phase. I think we should do it proper and remove those annotations like `@Slow` and `@Integration` and make our integration tests just ITs: no maven magic or magical test filtering, that gets confusing. It allows people to iterate quicker since unit testing phase will get fast once we move them all over.

And the build gets a lot simpler to prevent build failures: run `mvn verify` before pushing. I did just the rest tests here as a start.

Core integration tests run with parallel jvms and everything else so it is not a slowdown, just a separation.
</description><key id="98746882">12602</key><summary>Add integration test phase for elasticsearch core/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-08-03T13:47:47Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-03T14:15:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-03T13:57:46Z" id="127244972">LGTM
</comment><comment author="nik9000" created="2015-08-03T13:59:45Z" id="127247167">In general I'm always more for just doing things the way maven wants things done and this is in that vein so +1. Its not that I think maven does everything right - its just that it makes you life simpler with maven....

Anyway, it looks like you make just the rest tests integration test which is cool. There is a whole ElasticsearchIntegrationTest superclass you could look at too.

One issue here is that the intergration tests phases come after all the jar-ing and things and if you want to run them more quickly the jar-ing gets in the way. But for that we have IDEs which can kick off a single test.

I suspect we could use some documentation around what should be an integration test VS what should be a unit test.
</comment><comment author="rmuir" created="2015-08-03T14:01:14Z" id="127248159">&gt; Anyway, it looks like you make just the rest tests integration test which is cool. There is a whole ElasticsearchIntegrationTest superclass you could look at too.

There are hundreds of tests to cut over :)

I only did the rest ones to prove the concept is working, that parallel execution etc is working and so on. We are gonna have to cut over all the other tests in followup PRs.
</comment><comment author="s1monw" created="2015-08-03T14:10:50Z" id="127255389">LGTM 
</comment><comment author="nik9000" created="2015-08-03T14:14:21Z" id="127256915">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test that shaded jar contains only shaded classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12601</link><project id="" key="" /><description>Fail the build if classes are missing relocation definitions. This allows us to also safely change this from an inclusion list to an exclusion list. When a jar is added, it will automatically be added to the shaded jar by default (unless you exclude it), and the build will fail until you relocate packages.

Will look like this (e.g. for hdrhistogram jar):

```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (check-for-jar-hell) on project elasticsearch-shaded: An Ant BuildException has occured: The following error occurred while executing this line:
[ERROR] /home/rmuir/workspace/elasticsearch/distribution/shaded/target/dev-tools/ant/integration-tests.xml:299: shaded jar contains packages outside of org.elasticsearch: org/HdrHistogram/AbstractHistogram$1.class;org/HdrHistogram/AbstractHistogram$AllValues.class
```
</description><key id="98737735">12601</key><summary>Test that shaded jar contains only shaded classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-08-03T12:59:01Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-03T14:08:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-03T13:28:25Z" id="127233517">+1!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for search failures if shard is in POST_RECOVERY</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12600</link><project id="" key="" /><description>Currently, we do not allow reads on shards which are in POST_RECOVERY which unfortunately can cause search failures on shards which just recovered if there no replicas (#9421). 
The reason why we did not allow reads on shards that are in POST_RECOVERY is that after relocating a shard might miss a refresh if the node that executed the refresh is behind with cluster state processing.  If that happens, a user might execute index/refresh/search but still not find the document that was indexed.
More details [here](https://github.com/elastic/elasticsearch/compare/elastic:master...brwe:replicated_refresh?expand=1#diff-90cb64bedfe055b7d7f1cc28103784d5R971).

@bleskes and I discussed this briefly and he mentioned we could make refresh a replicated operation that goes the same route that index operations go and thereby make sure that the refresh reaches every shard. In this case we could also allow reads on POST_RECOVERY.

I make this PR as a proof of concept so that we can discuss if this is actually a good idea. 
This PR contains:
- a reliable test for #9421
- a fix for #9421 
- a test for the visibility issue that we have when we allow reads in POST_RECOVERY
- the change to make refresh a replicated action just like index, delete, etc.

Let me know what you think. I would make the same changes for flush also.
</description><key id="98735530">12600</key><summary>Fix for search failures if shard is in POST_RECOVERY</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2015-08-03T12:45:32Z</created><updated>2015-09-01T13:30:27Z</updated><resolved>2015-09-01T13:30:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-03T13:01:38Z" id="127223319">I like this a lot! I wonder if we can streamline the implementation here a little more and forward the original request with the shard ID together to have some code we can share between flush and refresh and whatever comes after that? Anyway I think we can just start with what we have here and see how flush turns out afterwards.
</comment><comment author="brwe" created="2015-08-03T13:09:01Z" id="127225036">I was actually wondering if we should also have a dedicated Action, similar to TransportBroadcastAction something like TransportReplicatedBroadcastAction or something that flush and refresh can derive from.   
</comment><comment author="brwe" created="2015-08-12T10:51:01Z" id="130258980">I continued on this. I tried to generalize what I did for refresh so that it can be used for flush too. Now I wonder: should this work for synced flush too? 
</comment><comment author="s1monw" created="2015-08-17T20:00:05Z" id="131944236">@brwe what's the status here, do you wait for reviews?
</comment><comment author="brwe" created="2015-08-18T10:17:57Z" id="132157440">@s1monw yes. @bleskes might have an opinion on that too.
</comment><comment author="bleskes" created="2015-08-19T09:45:38Z" id="132517114">@brwe and I talked this through and we decided to try and simplify things and remove some intermediate abstractions. Concretely try to:

1) Use ReplicationRequest/ReplicationResponse/TransportReplicationAction directly rather than having ReplicatedBroadcastShardRequest/ReplicatedBroadcastShardResponse/TransportReplicatedBroadcastShardAction.
2) Use BroadcastRequest/BroadcastResponse instead of ReplicatedBroadcastRequest/ReplicatedBroadcastResponse
3) Rename TransportReplicatedBroadcastAction to TransportBroadcastReplicationAction   

Also, we should break this into two PRs:
1) One changing the refresh/flush behavior.
2) A follow up PR to change the read on POST_RECOVERY semantics.
</comment><comment author="brwe" created="2015-08-24T09:42:08Z" id="134114218">&gt; Also, we should break this into two PRs:
&gt; 1) One changing the refresh/flush behavior.
&gt; 2) A follow up PR to change the read on POST_RECOVERY semantics.

I made a pr for the first part here: #13068
</comment><comment author="brwe" created="2015-09-01T13:30:19Z" id="136722840">Opened the second pr for the actual fix now here: https://github.com/elastic/elasticsearch/pull/13246 
I'll close this one here now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix concurrency issue in PrioritizedEsThreadPoolExecutor.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12599</link><project id="" key="" /><description>Tasks can be registered with a timeout, which runs as a task in a separate
threadpool. The idea is that the timeout runner cancels the main task when
the time is out, and the timeout runner is cancelled when the main task
starts executing. However, the following statement:

``` java
                    timeoutFuture = timer.schedule(new Runnable() {
                        @Override
                        public void run() {
                            if (remove(TieBreakingPrioritizedRunnable.this)) {
                                runAndClean(timeoutCallback);
                            }
                        }
                    }, timeValue.nanos(), TimeUnit.NANOSECONDS);
```

is not atomic: the removal task is first started, and then the (volatile)
variable is assigned. As a consequence, there is a short window that allows
a timeout task to wait until the time is out even if the task is already
completed.

See http://build-us-00.elastic.co/job/es_core_17_centos/496/ for an example of
such a failure.
</description><key id="98732110">12599</key><summary>Fix concurrency issue in PrioritizedEsThreadPoolExecutor.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.3</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T12:18:09Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-04T08:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-08-04T07:00:10Z" id="127502182">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move Streams.copyTo(String|Bytes)FromClasspath() into StreamsUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12598</link><project id="" key="" /><description>The Streams.copyTo(String|Bytes)FromClasspath() methods resolve resources using org.elasticsearch.io.Streams class classloader. This is fine in elasticsearch core and when running tests but if used in a plugin this can lead to FileNotFoundExceptions at runtime because plugin are loaded in a dedicated classloader.

This commit removes the methods from core so that plugin developers are not tempted to use them. Since they are widely used in tests they are moved in a StreamsUtils class in the test framework.
</description><key id="98724915">12598</key><summary>Move Streams.copyTo(String|Bytes)FromClasspath() into StreamsUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T11:37:59Z</created><updated>2015-08-05T13:53:59Z</updated><resolved>2015-08-04T08:33:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-03T12:09:18Z" id="127212155">looks great, thanks
</comment><comment author="jaymode" created="2015-08-03T13:19:38Z" id="127230135">LGTM
</comment><comment author="tlrx" created="2015-08-04T08:33:53Z" id="127526705">@rmuir @jaymode thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>s</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12597</link><project id="" key="" /><description /><key id="98710785">12597</key><summary>s</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vp-knguyen</reporter><labels /><created>2015-08-03T10:01:56Z</created><updated>2015-08-03T11:22:24Z</updated><resolved>2015-08-03T11:22:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add System/Runtime exit/halt to forbidden apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12596</link><project id="" key="" /><description>Its not going to work anyway, but we can catch usages at compile-time here.
</description><key id="98705203">12596</key><summary>add System/Runtime exit/halt to forbidden apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>low hanging fruit</label></labels><created>2015-08-03T09:27:51Z</created><updated>2015-11-21T09:32:52Z</updated><resolved>2015-11-04T08:31:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Full path validation for pipeline aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12595</link><project id="" key="" /><description>Previously only the first aggregation in a buckets_path was check to make sure the aggregation existed. Now the whole path is checked to ensure an aggregation exists at each element in the buckets_path

Closes #12360
</description><key id="98696113">12595</key><summary>Full path validation for pipeline aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T08:26:36Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-03T10:24:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-03T08:36:34Z" id="127164189">The test might need some polishing but other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix ShardUtils#getElasticsearchDirectoryReader()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12594</link><project id="" key="" /><description>fix ShardUtils#getElasticsearchDirectoryReader(...) to use FilterDirectoryReader#getDelegate()
</description><key id="98690937">12594</key><summary>Fix ShardUtils#getElasticsearchDirectoryReader()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-03T07:56:12Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-03T08:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-08-03T08:26:33Z" id="127162646">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge pull request #1 from elastic/master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12593</link><project id="" key="" /><description>sync with the elastic's
</description><key id="98686586">12593</key><summary>Merge pull request #1 from elastic/master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xpandan</reporter><labels /><created>2015-08-03T07:21:50Z</created><updated>2015-08-03T08:45:13Z</updated><resolved>2015-08-03T07:42:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-08-03T07:45:34Z" id="127150494">Hello @xpandan, do you need a hand here? :)
</comment><comment author="xpandan" created="2015-08-03T08:45:13Z" id="127165430">Hi @markwalkom, thanks for asking. I was trying to sync my fork with the official es repo via the github web GUI and later gave it up. :( I synced my code via cmd instead as I usually do...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregate results by query input params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12592</link><project id="" key="" /><description>I have two indexes, the first one called **areas** is an index containing of multiple polygons, and second one is an index containing of multiple **points** indicating  the document's lat/lon on earth, the index is called **stores**.

The problem is that I want to attach areas(polygons) that contain the **store's point** (lat/lon) inside of them, to the store's document as an embedded object. So I can easily find that a document on **stores** index has which areas.

In an **RDBMS** I was able to join this two table and find the needed records, In elasticsearch I can iterate over **areas** index documents one by one and perform a query to find the **stores** that fit in the **area's** polygon, and then perform an update on the **store** document, as the number of **areas** are huge, I have to perform over 50,000 queries on elasticsearch.

A good solution would be sending a bulk list of areas with the query and tell elasticsearch to aggregate returning results by them the answer would be something like this:
- Polygon A : [store1, store2, store3],
- Polygon B : [store1, store4, store5]

I didn't find anyway to implement this and I switched back to RDBMS solution which is not very optimized and fast too, but it is faster in design.

Is there anyway to implement this with elasticsearch?

I also planned for writing a plugin which does this functionality if it is not possible currently.  
</description><key id="98677266">12592</key><summary>Aggregate results by query input params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reshadman</reporter><labels /><created>2015-08-03T05:52:17Z</created><updated>2015-08-05T12:15:15Z</updated><resolved>2015-08-03T07:52:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-03T07:52:03Z" id="127151829">This question is a duplicate of a post on the Discuss forums: https://discuss.elastic.co/t/aggregate-results-by-query-input/26692

Please stick to asking questions on the forums rather than this issues list which is for products issues rather than questions.

Joining across indices is not possible in Elasticsearch. Due to the distributed nature of Elasticsearch indices it joins do not scale well. This is why for parent/child documents we required the parent and child document to be located on the same shard, because joining across shards (or indices) in a distributed environment would not perform well
</comment><comment author="reshadman" created="2015-08-05T10:59:38Z" id="127955382">Thanks @colings86 . I know I should post this kind of issues on forums, but I described my app specific  scenario to get better understanding of the elasticsearch related feature.

Anyway, I found a great feature which allows to reference polygons with their names on another index ([Pre index shapes](https://www.elastic.co/guide/en/elasticsearch/reference/1.6/query-dsl-geo-shape-filter.html#_pre_indexed_shape)) but there is not any option to aggregate results using this filter. Found a great plugin which has this functionality ( [elasticsearch-plugin-geoshap](https://github.com/opendatasoft/elasticsearch-plugin-geoshape) ).

Wouldn't it be nice to have aggregation on pre index shapes ?
</comment><comment author="colings86" created="2015-08-05T12:15:15Z" id="127978063">&gt; Thanks @colings86 . I know I should post this kind of issues on forums, but I described my app specific scenario to get better understanding of the elasticsearch related feature.

This is exactly what the forums are for.

&gt; Wouldn't it be nice to have aggregation on pre index shapes ?

You can already do this using the `geo_shape` filter with the `filters` aggregation. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_geo_point with lat_lon: true and store/search by array [lat,lon] produces inaccurate results. ( _geo_distance sort off by 2.5x with all distance_types)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12591</link><project id="" key="" /><description>I can't figure out why, but the distance between these two coordinates is roughly 75 miles

```
Origin:
42.587914999999995,
-72.5994104
Destination:
41.6913013, 
-72.8458687
```

[link to directions via google maps](https://www.google.com/maps/dir/42.587914999999995,-72.5994104/41.6913013,-72.8458687)

however, my index which stores geo_point data shows the sort value as roughly 25 miles.  Here is the Query:

``` json
{  
   "size":25,
   "from":0,
   "query":{  
      "filtered":{  
         "query":{  
            "match_all":{  

            }
         },
         "filter":{  
            "geo_distance":{  
               "distance":"500mi",
               "location":[  
                  42.587914999999995,
                  -72.5994104
               ]
            }
         }
      }
   },
   "sort":[  
      {  
         "_geo_distance":{  
            "location":[  
               42.587914999999995,
               -72.5994104
            ],
            "order":"asc",
            "unit":"miles",
            "distance_type":"arc"
         }
      }
   ]
}
```

The response is giving me a sort value of "25.020869790780612"

What could possibly account for this inaccuracy?
</description><key id="98646214">12591</key><summary>_geo_point with lat_lon: true and store/search by array [lat,lon] produces inaccurate results. ( _geo_distance sort off by 2.5x with all distance_types)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">intelligentspark</reporter><labels /><created>2015-08-02T23:48:43Z</created><updated>2015-08-05T17:49:42Z</updated><resolved>2015-08-05T12:04:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T12:04:35Z" id="127973410">You're getting lat and lon mixed up.  Arrays take [`lon`,`lat`]. See:

```
PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "location": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT test/test/1
{
  "location": {
    "lat": 41.6913013,
    "lon": -72.8458687
  }
}

GET test/_search
{
  "sort": [
    {
      "_geo_distance": {
        "location": {
          "lat": 42.587914999999995,
          "lon": -72.5994104
        },
        "order": "asc",
        "unit": "miles",
        "distance_type": "arc"
      }
    }
  ]
}
```
</comment><comment author="intelligentspark" created="2015-08-05T14:31:40Z" id="128016512">Ok, why is that
</comment><comment author="dadoonet" created="2015-08-05T14:34:35Z" id="128018006">@fbliss Read the end of https://www.elastic.co/guide/en/elasticsearch/guide/current/lat-lon-formats.html :)
</comment><comment author="intelligentspark" created="2015-08-05T14:43:06Z" id="128021308">I still am getting incorrect results, namely, results that go to the other extreme like 8,000 miles to San Francisco.  I should note that my mapping has set the geo_point setting lat_lon to TRUE, so that is indicated in the docu as being a way to tell it to reverse the order in which you're storing the data. [see here](https://www.elastic.co/guide/en/elasticsearch/reference/1.6/mapping-geo-point-type.html)

'''
PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "location": {
          "type": "geo_point",
          "lat_lon": true
        }
      }
    }
  }
}
'''

when i index my documents accordingly, I'm getting bad results.  I just rebuilt the index without the lat_lon = true setting and guess what?  all distances are accurate.  I think there is a problem using lat_lon = true in geo_point mappings.
</comment><comment author="clintongormley" created="2015-08-05T17:38:16Z" id="128086231">&gt;  I should note that my mapping has set the geo_point setting lat_lon to TRUE, so that is indicated in the docu as being a way to tell it to reverse the order in which you're storing the data.

No, you've completely misunderstood the `lat_lon` option :)

Normally the latitude and longitude values are not indexed, and distance calculations are done in memory.  By setting `lat_lon` to true, you are saying "also add the lat and lon values to the inverted index" as sometimes that can perform better.  Nothing to do with order.
</comment><comment author="intelligentspark" created="2015-08-05T17:49:42Z" id="128089748">Thanks for clearing that up.  I did get it working.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed official api name in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12590</link><project id="" key="" /><description>I think that `Nodes Info API` is official api name.
I fixed it.
- https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-info.html

Thank you so much.
</description><key id="98582003">12590</key><summary>Fixed official api name in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kakakakakku</reporter><labels><label>docs</label><label>v1.6.3</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-08-02T03:45:34Z</created><updated>2015-08-04T08:23:10Z</updated><resolved>2015-08-04T07:54:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-03T15:37:53Z" id="127280967">Thanks @Kakakakakku! Would you mind signing the [CLA](https://www.elastic.co/contributor-agreement) so that I can merge this change in?
</comment><comment author="kakakakakku" created="2015-08-03T20:19:14Z" id="127395558">@jpountz 
Oh, sorry I forgot.
I signed the CLA now, and I received mail.

But build status remains failed.
Can I retry to build?
</comment><comment author="kakakakakku" created="2015-08-03T20:20:03Z" id="127395703">When I commented now, build status is changed green.
Thanks :+1: 
</comment><comment author="jpountz" created="2015-08-04T07:54:41Z" id="127511007">I just merged it, thanks!
</comment><comment author="kakakakakku" created="2015-08-04T08:23:10Z" id="127521697">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix shaded jar packaging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12589</link><project id="" key="" /><description>There are a couple of problems:
- shaded jar is completely untested &lt;-- most important
- has the wrong dependencies
- has duplicated dependencies for what it shades (no reduced POM)
- possibly doesn't include all the stuff it should

Basically this isn't going to work.

This moves shading to distributions/shaded, so it gets a proper POM. and the plan is to add tests for what you are supposed to be able to do with it (e.g. node client, whatever). At the least we can jar hell check it for sanity and other simple tests.

Additionally dependencies for packaging are quite crazy, to add a jar you have to do it in a zillion places... we should try to reduce these (also work in progress).

I hacked a way at this a bit to get towards something manageable. Dependencies of the shaded jar currently look like this (i didnt change the current logic yet):

```
[INFO] test:testme:jar:1.0-SNAPSHOT
[INFO] \- org.elasticsearch.distribution:elasticsearch-shaded:jar:2.0.0-beta1-SNAPSHOT:compile
[INFO]    +- org.apache.lucene:lucene-core:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-queries:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-memory:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-misc:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-join:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-grouping:jar:5.2.1:compile
[INFO]    +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile
[INFO]    +- com.spatial4j:spatial4j:jar:0.4.1:compile
[INFO]    +- org.yaml:snakeyaml:jar:1.12:compile
[INFO]    \- org.hdrhistogram:HdrHistogram:jar:2.1.6:compile
```

Seem some jars already got forgotten to be added to the shading configuration, and maybe we really don't need all these jars? So we need to figure out what tests to add and then we can adjust this as necessary.
</description><key id="98553839">12589</key><summary>Fix shaded jar packaging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-08-01T18:14:59Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-03T12:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-01T18:55:56Z" id="126945083">I am also ok to name this distribution/client instead of shaded, since thats really what this thing is. Just seems like it would give us a path forwards to do this stuff in a cleaner way: shading is really sketchy.
</comment><comment author="dadoonet" created="2015-08-01T19:18:02Z" id="126947469">I think that moving shaded to its module is the way to go IMO.

You added  "elasticsearch-fully-loaded" module. I"m not sure yet what is the goal for this. Is this needed for this change (moving shaded to a distribution module)?
</comment><comment author="rmuir" created="2015-08-01T19:37:30Z" id="126948194">&gt; You added "elasticsearch-fully-loaded" module. I"m not sure yet what is the goal for this. Is this needed for this change (moving shaded to a distribution module)?

Most packaging distributions package up the optional dependencies (except slf4j). This needed to be pulled out of distribution/pom.xml so that the shaded distribution can work without having to explicitly disable each and every one of them. It also makes it easier to start removing these duplicate lists of our dependencies (see where i did that in the assembly.xml). 

Its just a POM we use internally, I dont think it hurts anything.
</comment><comment author="dadoonet" created="2015-08-01T20:31:13Z" id="126951758">Thank you for the explanation.

&gt; Its just a POM we use internally, I dont think it hurts anything.

Agreed.

On the other end, we should try to understand if there is really a need for a shaded jar and in which context.
So I'm +1 for creating this shaded module with your PR and then decide if we really want to keep it around.
</comment><comment author="rjernst" created="2015-08-01T21:46:31Z" id="126955770">&gt; I am also ok to name this distribution/client instead of shaded, since thats really what this thing is

+1
</comment><comment author="rmuir" created="2015-08-03T09:29:07Z" id="127175988">&gt; So I'm +1 for creating this shaded module with your PR and then decide if we really want to keep it around.

OK, ill keep it `shaded` for now. Ill try to add a test so we can get this in and iterate. Its all broken today and we can't stall this change on this stuff
</comment><comment author="rmuir" created="2015-08-03T09:56:09Z" id="127181435">I pushed a very simple test for the shaded jar, it just does the jar hell check. For now its something other than nothing...
</comment><comment author="dadoonet" created="2015-08-03T10:58:55Z" id="127194138">The change looks good to me. I did not test it though yet.
</comment><comment author="rmuir" created="2015-08-03T12:10:27Z" id="127212858">I tested, mvn verify passes: i will open some followup issues to improve the testing / do more cleanups
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12588</link><project id="" key="" /><description /><key id="98486411">12588</key><summary>Fix typo in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sylvinus</reporter><labels><label>docs</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T23:11:09Z</created><updated>2015-07-31T23:42:00Z</updated><resolved>2015-07-31T23:41:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sylvinus" created="2015-07-31T23:23:41Z" id="126834019">whoops, just signed the CLA.
</comment><comment author="dakrone" created="2015-07-31T23:42:00Z" id="126836524">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for bulk delete operation in snapshot repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12587</link><project id="" key="" /><description>Currently when we delete files belonging to deleted snapshots we issue one delete command to underlying snapshot store at a time. Some repositories can benefit from bulk deletes of multiple files.

Closes #12533
</description><key id="98456583">12587</key><summary>Add support for bulk delete operation in snapshot repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T19:33:53Z</created><updated>2015-08-05T15:57:13Z</updated><resolved>2015-08-04T21:22:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-01T07:48:13Z" id="126879102">LGTM. I left a small comment which might be wrong. Feel free to ignore it. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Bootstrap to not call System.exit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12586</link><project id="" key="" /><description>Its not going to work: its blocked by security policy
and will just add a confusing SecurityException to the mix, and
bogusly give an exit status of 0 when in fact something bad happened.

Finally, if ES can't startup, it is a serious problem, there is
no sense in hiding the reason why: deliver the full stack trace.
</description><key id="98442876">12586</key><summary>Fix Bootstrap to not call System.exit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T18:04:48Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-03T09:27:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-01T21:47:21Z" id="126955805">LGTM
</comment><comment author="jpountz" created="2015-08-03T08:37:42Z" id="127164364">LGTM2
</comment><comment author="jpountz" created="2015-08-03T08:38:30Z" id="127164475">This makes me wonder whether we should make it a forbidden API so that the build would catch if it gets added back?
</comment><comment author="tlrx" created="2015-08-03T09:15:00Z" id="127173885">+1 I like this, it's cleaner now
</comment><comment author="s1monw" created="2015-08-03T09:15:27Z" id="127173953">LGTM 2 FWIW
</comment><comment author="rmuir" created="2015-08-03T09:27:01Z" id="127175658">&gt; This makes me wonder whether we should make it a forbidden API so that the build would catch if it gets added back?

We could, but thats a bigger change impacting a (way-too-huge) codebase. Ill make a followup issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove core docs for delete-by-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12585</link><project id="" key="" /><description>We removed this dangerous core API, and re-implemented as a plugin, but failed to remove the docs for it...
</description><key id="98432675">12585</key><summary>Remove core docs for delete-by-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T17:13:27Z</created><updated>2015-08-05T11:26:12Z</updated><resolved>2015-08-01T09:34:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-31T18:28:46Z" id="126777289">looks great, thanks for looking into this Mike
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change capitalization of "as"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12584</link><project id="" key="" /><description>The documentation has "Running As a Service on Linux" and "Running as a Service on Windows." The capitalization ought to be consistent.
</description><key id="98431034">12584</key><summary>Change capitalization of "as"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitej17</reporter><labels /><created>2015-07-31T17:03:13Z</created><updated>2015-08-07T20:40:59Z</updated><resolved>2015-08-04T19:25:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-08-04T19:25:45Z" id="127728314">Looks great! Merging to 1.6, 1.7, and master. Thanks @whitej17!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"mvn verify" should catch bugs in the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12583</link><project id="" key="" /><description>I'm not sure how to implement this but I think it's important for the quality of our docs.

In Apache Lucene, if I mess up javadocs, "ant compile" or "ant pre-commit" will catch it.

But for ES, I think (?) I must clone the docs repo (https://github.com/elastic/docs) and follow its steps to make sure all is good.

Can we make this easier somehow, e.g. fold it under "mvn verify"?
</description><key id="98429751">12583</key><summary>"mvn verify" should catch bugs in the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>discuss</label></labels><created>2015-07-31T16:55:45Z</created><updated>2016-01-26T17:35:02Z</updated><resolved>2016-01-26T17:35:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-03T07:54:29Z" id="127152515">I think @clintongormley was working on making `mvn site` build the documentation but I'm not sure how far he got with it.
</comment><comment author="rjernst" created="2015-08-03T19:43:42Z" id="127383735">+1 to having this done with `mvn verify`.  Can we use the maven asciidoc plugin? I know it won't do all the cross link checking to other parts of the docs, but can we at least get syntax checking?
</comment><comment author="dadoonet" created="2015-08-03T20:00:17Z" id="127389443">See also https://github.com/elastic/elasticsearch-parent/issues/50 
</comment><comment author="rmuir" created="2015-08-03T20:02:51Z" id="127390354">one wierd thing is that site has some wacky separate lifecycle in maven, i dont think it would in fact get run during `verify` (which is the point of this PR).

Maybe an easier step / alternative is to either package up or fold in the perl scripts in the docs repo and invoke them in our build when we want and not use the site stuff for now?
</comment><comment author="colings86" created="2015-08-03T20:06:53Z" id="127391543">Yeah so I think @clintongormley was working on just using the maven-exec-plugin to run the perl scripts, like we do for the license checking stuff. He was doing it in `mvn site` because I think at the time we were thinking it would be good to keep it in a separate lifecycle, but should be easy to move it to `mvn verify` instead.
</comment><comment author="rmuir" created="2015-08-03T20:12:58Z" id="127393820">yeah I think `docs/` should just get a pom.xml and plug stuff in at all our normal phases. 
</comment><comment author="clintongormley" created="2015-08-05T12:48:27Z" id="127985963">I don't think we should package up the build scripts into core. Instead we can just rely on the fact that you have `build_docs.pl` in your command line.  The issue that I had when trying to do this was that the various books in docs/ don't map one to one to the other repos. I like the idea of having a pom.xml in the docs directory, then we can just build everything in that dir.
</comment><comment author="rmuir" created="2015-08-05T12:54:09Z" id="127988255">How can we rely on that? Relying on anything on the commandline makes setting up a new developer very different: we should avoid that at all costs. Its too system specific.

If we are going to do that shit in our build, then please, open a separate issue for that 'Convert build system to make'. And I will take it seriously!

But no, i dont want requiring special setup _AND_ thousands of lines of xml. thats the worst of both worlds.
</comment><comment author="clintongormley" created="2016-01-26T17:35:02Z" id="175133832">Closing as won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move the `_size` mapper to a plugin.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12582</link><project id="" key="" /><description>This is one of our esoteric metadata mappers so I think we should distribute
it in a plugin rather than in elasticsearch core.

This introduces one limitation: the value of the `_size` parameter is not
retrievable for documents that are only in the transaction log.
</description><key id="98428607">12582</key><summary>Move the `_size` mapper to a plugin.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T16:47:59Z</created><updated>2015-08-07T10:45:15Z</updated><resolved>2015-08-06T18:36:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-04T15:57:15Z" id="127658110">This is great! LGTM!
</comment><comment author="clintongormley" created="2015-08-04T16:02:16Z" id="127659405">Please open an issue on the migration plugin repo too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix setting timezone on default DateTime formatter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12581</link><project id="" key="" /><description>This PR prevents setting timezone on ValueFormatter.DateTime. Instead
the timezone information needed when printing buckets key-as-string
information is provided at constrution time of the ValueFormatter, making
sure we don't overwrite any constants. This, however, made it necessary to
be able to access the timezone information when resolving the format
in ValueSourseParser, so the `time_zone` parameter is now parsed alongside
the `format` parameter in ValueSourceParser rather than in DateHistogramParser.

Closes #12531
</description><key id="98420625">12581</key><summary>Fix setting timezone on default DateTime formatter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T16:04:26Z</created><updated>2015-08-10T13:42:23Z</updated><resolved>2015-08-10T13:05:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-04T11:46:23Z" id="127574803">@jpountz Added exception when trying to set null as timezone value, all callers should already use UTC as defaults where possible.
</comment><comment author="colings86" created="2015-08-10T10:28:47Z" id="129399089">@cbuescher I left one comment but otherwise it looks good :)
</comment><comment author="cbuescher" created="2015-08-10T11:08:25Z" id="129409219">@colings86 thanks, I looking at your comment I opted for changing to a slightly different solution which involves even less changes in the existing code. Mind to take another look?
</comment><comment author="colings86" created="2015-08-10T11:21:43Z" id="129411389">I actually prefer the previous solution (with Input made immutable) than this. The problem here for me is that the valuesSource interface now exposes timezone when it isn't a common property of valuesSource, its an option which is configured in the request which is exactly what the Input class represents. Also, in the upcoming aggregation refactoring changes the Input class will be exposed anyway so the previous solution will need to be implemented regardless and the timezone will need to be included in that Input class.
</comment><comment author="cbuescher" created="2015-08-10T12:54:34Z" id="129431292">@colings86 no problem, went back to the previous solution, making fields in Input final is impossible since ValueSourceParse directly updates them, but I made them all private an only expose the timezone which is needen in DateHistogramParser for now. Let me know if you think this is okay.
</comment><comment author="colings86" created="2015-08-10T12:56:25Z" id="129431866">@cbuescher ok great, it LGTM but maybe @jpountz would like to have a last look before its merged?
</comment><comment author="jpountz" created="2015-08-10T12:57:12Z" id="129432166">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch (again) adds CWD to classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12580</link><project id="" key="" /><description>This causes all kinds of bugs.

I fixed this already in #12001 but my code changes were _lost_

I will not allow this to happen again: I am going to make heavy changes to tests so that things fail if this is screwed up again. Expect them to break lots of functionality like IDE integration and other plugins tests: don't care.
</description><key id="98419483">12580</key><summary>elasticsearch (again) adds CWD to classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T15:58:19Z</created><updated>2015-08-05T11:22:28Z</updated><resolved>2015-07-31T17:00:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-31T16:26:26Z" id="126741485">By the way thanks to @spalger for discovering this.
</comment><comment author="rmuir" created="2015-07-31T17:00:45Z" id="126752373">Fixed.
</comment><comment author="clintongormley" created="2015-08-05T11:22:09Z" id="127959277">Fixed by 8d5b5ad862de589667211456f1e96c156412c90f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve Delete-By-Query plugin documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12579</link><project id="" key="" /><description>The documentation of the [Delete-By-Query plugin](https://github.com/elastic/elasticsearch/blob/master/docs/plugins/delete-by-query.asciidoc) is pretty straightforward and lacks many useful information that are already explained in [the source code](https://github.com/elastic/elasticsearch/blob/master/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java#L47-L72) (thanks to @s1monw). 

We should improve the documentation.
</description><key id="98406191">12579</key><summary>Improve Delete-By-Query plugin documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>docs</label><label>v2.0.0-beta2</label></labels><created>2015-07-31T14:48:57Z</created><updated>2015-09-14T17:17:14Z</updated><resolved>2015-08-15T16:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>improve 'example-site' rest test.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12578</link><project id="" key="" /><description>Currently this just checks the plugin is installed.

But as a site plugin, it serves up a little html page and it would be great to test that retrieving those contents really works.
</description><key id="98402128">12578</key><summary>improve 'example-site' rest test.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-07-31T14:27:36Z</created><updated>2015-08-04T03:58:19Z</updated><resolved>2015-08-04T03:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>don't represent site plugins with 'null' anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12577</link><project id="" key="" /><description>Lots of plugin logic manipulates Tuple&lt;PluginInfo,Plugin&gt; and historically site-only plugins used `null` for the Plugin part. This only makes for bugs: we can just have a simple Plugin implementation for these site plugins.

The bigger problem is we have no real site plugins in our build. This adds `example-site` plugin with a simple integration test.
</description><key id="98398686">12577</key><summary>don't represent site plugins with 'null' anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T14:07:49Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-07-31T14:43:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-07-31T14:17:50Z" id="126706675">left two small comments. I don't think adding the other integration test should block the PR. This is a big improvement. LGTM
</comment><comment author="rmuir" created="2015-07-31T14:26:33Z" id="126708297">I'm going to open with a followup to improve the rest test here to also try to check for the contents: its a really good idea and might be an easy task for someone more familiar with rest tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Remove duplicate code in AndQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12576</link><project id="" key="" /><description>When parsing inner array of filters, AndQueryParser seems to
check for correct "filters" field name but then does the same
kind of operation in the `else` branch of the stament. This
seems like it can be removed.
</description><key id="98397856">12576</key><summary>Query DSL: Remove duplicate code in AndQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label></labels><created>2015-07-31T14:04:05Z</created><updated>2015-08-11T14:04:31Z</updated><resolved>2015-08-11T14:04:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-31T14:28:16Z" id="126708629">left a small comment but the change looks good.
</comment><comment author="cbuescher" created="2015-08-11T14:04:31Z" id="129890924">Closing this since this change should go to a different feature branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shuffle shards for _only_nodes + support multiple specifications like cluster API </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12575</link><project id="" key="" /><description>_only_node never used to shuffle shards - was under the assumption that its done at much higher level in code . Added shuffle() to distribute traffic ; Also modified exception to show which shard is not available for specified node.

Closes https://github.com/elastic/elasticsearch/issues/12546
Closes #12700
</description><key id="98380713">12575</key><summary>Shuffle shards for _only_nodes + support multiple specifications like cluster API </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nirmalc</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-07-31T12:24:50Z</created><updated>2015-12-29T23:48:43Z</updated><resolved>2015-12-29T23:48:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T11:11:41Z" id="127956911">Thanks for the PR @nirmalc 

You really don't like writing tests, do you? :) Please could you add one

thanks
</comment><comment author="nirmalc" created="2015-08-06T15:29:58Z" id="128410970">Hey @clintongormley shouldnt have pushed PR on this - this needs some more work, will refresh this in a day or so . sorry about that 
</comment><comment author="nirmalc" created="2015-08-06T22:24:31Z" id="128526797">rebased with upstream/1.7 and added patch for https://github.com/elastic/elasticsearch/issues/12700
</comment><comment author="clintongormley" created="2015-08-27T09:01:25Z" id="135350241">@jasontedor  would you mind reviewing this please?
</comment><comment author="jasontedor" created="2015-09-01T10:35:00Z" id="136668542">@nirmalc Thanks for taking a look at these issues and making the contribution! I think that this generally looks okay, but I did leave a few minor comments. However, do you think that you can rebase this pull request on current master instead of the 1.7 branch? I suspect that you'll encounter a lot of changes, so if it's okay we will do another review after that.
</comment><comment author="nirmalc" created="2015-09-10T17:10:05Z" id="139312549">Hey @jasontedor  - thanks for review . I can rebase it against 2.0 , do you want separate PR for 1.7 and 2.0  and yes - I'll address your review comments in both PR's.
</comment><comment author="jasontedor" created="2015-09-10T18:26:46Z" id="139335729">@nirmalc Would it be a problem for you to rebase on master?
</comment><comment author="nirmalc" created="2015-09-10T18:31:47Z" id="139336932">Not a problem - sorry i meant master by 2.0
</comment><comment author="jasontedor" created="2015-12-14T18:11:17Z" id="164514001">@nirmalc Are you still interested in this pull request?
</comment><comment author="jasontedor" created="2015-12-29T23:48:35Z" id="167901184">Closing due to no feedback.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>test for issue #12573 (Cluster state delay can cause endless index request loop)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12574</link><project id="" key="" /><description /><key id="98374287">12574</key><summary>test for issue #12573 (Cluster state delay can cause endless index request loop)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label></labels><created>2015-07-31T11:43:26Z</created><updated>2016-02-02T13:00:15Z</updated><resolved>2016-02-02T13:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cluster state delay can cause endless index request loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12573</link><project id="" key="" /><description>When a primary is relocating from `node_1` to `node_2`, there can be a short time where the old primary is removed from the node already (closed, not deleted) but the new primary is still in `POST_RECOVERY`. In this state indexing requests might be sent back and forth between `node_1` and `node_2` endlessly.

Course of events: 
1. primary (`[index][0]`) relocates from `node_1` to `node_2`
2. `node_2` is done recovering, moves its shard to `IndexShardState.POST_RECOVERY` and sends a message to master that the shard is `ShardRoutingState.STARTED` 
   
   ```
   Cluster state 1: 
   node_1: [index][0] RELOCATING (ShardRoutingState), (STARTED from IndexShardState perspective on node_1) 
   node_2: [index][0] INITIALIZING (ShardRoutingState), (at this point already POST_RECOVERY from IndexShardState perspective on node_2) 
   ```
3. master receives shard started and updates cluster state to: 
   
   ```
   Cluster state 2: 
   node_1: [index][0] no shard 
   node_2: [index][0] STARTED (ShardRoutingState), (at this point still in POST_RECOVERY from IndexShardState perspective on node_2) 
   ```
   
   master sends this to `node_1` and `node_2`
4. `node_1` receives the new cluster state and removes its shard because it is not allocated on `node_1` anymore 
5. index a document 

At this point `node_1` is already on cluster state 2 and does not have the shard anymore so it forwards the request to `node_2`. But `node_2` is behind with cluster state processing, is still on cluster state 1 and therefore has the shard in `IndexShardState.POST_RECOVERY` and thinks `node_1` has the primary. So it will send the request back to `node_1`. This goes on until either `node_2` finally catches up and processes cluster state 2 or both nodes OOM.

I will make a pull request with a test shortly
</description><key id="98374056">12573</key><summary>Cluster state delay can cause endless index request loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Recovery</label><label>bug</label></labels><created>2015-07-31T11:41:34Z</created><updated>2016-02-02T13:00:15Z</updated><resolved>2016-02-02T13:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-07-31T11:44:17Z" id="126663930">here is a test that reproduces this: #12574
</comment><comment author="clintongormley" created="2016-01-26T17:30:31Z" id="175131298">I think this will be closed by https://github.com/elastic/elasticsearch/pull/15900
</comment><comment author="ywelsch" created="2016-01-27T18:26:35Z" id="175782401">I've opened #16274 to address this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exists filter throws NullPointerException on empty index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12572</link><project id="" key="" /><description>In 1.7.1 
Using Filtered query with Exists filter causes SearchParseException on an empty index.

Steps to reproduce:

```
# make sure there is no such index
curl -XDELETE localhost:9200/test_index

# create a new index
curl -XPUT localhost:9200/test_index

# issue a query with Exists Filter
curl -XGET localhost:9200/test_index/_search -d '
{
    "query": {
        "filtered": {
           "query": {
           "match_all": {}
           },
           "filter": {
               "exists": {
                  "field": "user"
               }
           }
        }
    }
}
'
```

Stack trace:

```
org.elasticsearch.search.SearchParseException: [test_index][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
    "query": {
        "filtered": {
           "query": {
           "match_all": {}
           },
           "filter": {
               "exists": {
                  "field": "user"
               }
           }
        }
    }
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:747)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:572)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:544)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:306)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.query.ExistsFilterParser.newFilter(ExistsFilterParser.java:87)
    at org.elasticsearch.index.query.ExistsFilterParser.parse(ExistsFilterParser.java:82)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:371)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:352)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:305)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:731)
    ... 9 more
```
</description><key id="98344431">12572</key><summary>Exists filter throws NullPointerException on empty index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SergVro</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label></labels><created>2015-07-31T08:45:15Z</created><updated>2015-08-13T14:08:52Z</updated><resolved>2015-08-04T12:10:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-04T12:10:17Z" id="127579464">This has been fixed in 2.0
</comment><comment author="SergVro" created="2015-08-04T12:34:43Z" id="127586230">We can't use 2.0, that's why I need this fixed in 1.6 or 1.7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: allow to de-serialize arbitrary objects given their name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12571</link><project id="" key="" /><description>This commit makes it possible to serialize arbitrary objects by having them extend Writeable. When reading them though, we need to be able to identify which object we have to create, based on its name. This is useful for queries once we move to parsing on the coordinating node, as well as with aggregations and so on.

Introduced a new abstraction called NamedWriteable, which is supported by StreamOutput and StreamInput through writeNamedWriteable and readNamedWriteable methods. A new NamedWriteableRegistry is introduced also where named writeable prototypes need to be registered so that we are able to retrieve the proper instance of the writeable given its name and then de-serialize it calling readFrom against it.

We decided to streamline the support for NamedWriteables and make related methods available across the board in StreamInput and StreamOutput. That said the new write\* and read\* methods are package private so they can be tested but won't be made public. The idea is to add specific methods once we have named writeable to be streamed, e.g.:

```
public QueryBuilder readQuery() {
    return readNamedWriteable("query");
}
```

and

```
public void writeQuery(QueryBuilder queryBuilder) {
    writeNamedWriteable("query", queryBuilder);
}
```

The above methods cannot be added yet as neither queries nor aggs are streamable yet.
</description><key id="98343652">12571</key><summary>Transport: allow to de-serialize arbitrary objects given their name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-31T08:40:16Z</created><updated>2015-08-09T08:09:57Z</updated><resolved>2015-08-06T10:41:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-31T12:32:52Z" id="126676055">Not sure how it could be done without producing a massive PR but its a shame that anything using the NamedWriteableAwareStreamInput needs to cast the StreamInput it receives.

Also I wonder if we should have a NamedWriteableAwareStreamOutput so we are sure that classes using NamedWritable are sure to write the object properly. So it would have writeNamedWritable(NamedWritable), writeOptionalNamedWritable(NamedWritable) and write NamedWritableArray(NamedWritable[])?

It seems like this change is going to get complicated due to the mechanism for determining when to wrap the StreamInput so I agree that maybe we should explore how big the context argument option is?
</comment><comment author="javanna" created="2015-07-31T12:50:57Z" id="126680970">&gt; Also I wonder if we should have a NamedWriteableAwareStreamOutput so we are sure that classes using NamedWritable are sure to write the object properly. So it would have writeNamedWritable(NamedWritable), writeOptionalNamedWritable(NamedWritable) and write NamedWritableArray(NamedWritable[])?

That is why we have the new serializer object that exposes the methods to read and write named writeables, you have to effectively go through it so you can read and write named writeables. That said I agree with you the casting is not great, and the current wrapping of the stream is even worse :)

I am all for adding a context argument to all readFrom methods at this point. The only condition is that the context needs to expose final objects only and must not change its state while reading.

Let's see what @jpountz and @s1monw think about this.
</comment><comment author="javanna" created="2015-08-04T15:54:07Z" id="127657161">After talking to @s1monw and @jpountz we decided to go back to something closer to the original implementation (what we have in the query-refactoring branch). We wrap the stream (only in case of request) and named writeables are supported across the board. The default registry is empty if the stream is not wrapped with one that has a non empty registry. Also we went for exposing specific readQuery and readAggregation method in the future rather than the generic readNamedWriteable and writeNamedWriteable methods. I updated the description of the PR accordingly and removed the WIP label, this is ready for review now.
</comment><comment author="javanna" created="2015-08-06T07:22:14Z" id="128275734">@jpountz I updated the PR according to your comments, it's ready for another round of review
</comment><comment author="javanna" created="2015-08-06T09:34:57Z" id="128308967">I pushed another commit that should address the last review
</comment><comment author="jpountz" created="2015-08-06T09:52:26Z" id="128311881">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] Adding downsides for the embedded node client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12570</link><project id="" key="" /><description>Note: this is being committed to the 1.6 and 1.7 branches.

Closes #11952 
</description><key id="98304879">12570</key><summary>[Docs] Adding downsides for the embedded node client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>docs</label></labels><created>2015-07-31T02:46:48Z</created><updated>2015-07-31T02:47:58Z</updated><resolved>2015-07-31T02:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add implicit discovery.zen.maximum_master_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12569</link><project id="" key="" /><description>We already have support for maintaining quorum of the minimum number of master nodes. It may also be nice to have an inverse setting to limit the expected number of master nodes that can join the cluster.

This is purely to solve operational misconfigurations, but it can also be implicitly determined by solving for `M` (eligible master nodes) from the quorum equation given `discovery.zen.minimum_master_nodes`.

```
quorum = (M / 2) + 1
M = 2 * (quorum - 1)
```

We can use this to prevent misconfigurations from becoming a problem by failing for the same reason when quorum is not met.

```
quorum &lt;= running masters &lt;=  M
```

rather than just

```
quorum &lt;= running masters
```

This also makes me wonder if we should deprecate `discovery.zen.minimum_master_nodes` in favor of a `discovery.zen.expected_master_nodes` (defaults to 1), similar to `gateway.expected_master_nodes`. Then we can calculate quorum ourselves to avoid confusion (truncation/integer division can sometimes catch people off guard).
</description><key id="98288664">12569</key><summary>Add implicit discovery.zen.maximum_master_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Discovery</label><label>discuss</label></labels><created>2015-07-30T23:55:55Z</created><updated>2015-08-05T10:47:15Z</updated><resolved>2015-08-05T10:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T10:47:15Z" id="127953744">Hmmm, I'm against this for a few reasons:
- having too many master nodes is not a problem, why add yet another setting
- explaining that the number of available masters is less than `minimum_master_nodes` is easier than explaining that it is less than `1+int(expected/2)`
- `minimum_master_nodes` is widely publicised, so we would break blogs everywhere
- it is possible that we will be able to remove this setting completely in the future, with changes to Zen
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch client nodes </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12568</link><project id="" key="" /><description>Hi!

We are trying to see if we can introduce client nodes into our Elasticsearch cluster. I have yet to find any recommendations as far as memory requirements and Java settings for client nodes. Do they follow the same principles as data nodes? 50% of ram to heap and 50% to OS (not to exceed 32gb though)? 

What about CPU? Is it preferable to have a lot of cores for client nodes? 

Your insight is greatly appreciated!

Andrew
</description><key id="98276320">12568</key><summary>Elasticsearch client nodes </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AGirin</reporter><labels /><created>2015-07-30T22:12:13Z</created><updated>2015-07-31T10:21:03Z</updated><resolved>2015-07-31T10:21:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-31T10:21:03Z" id="126635909">This was asked and answered on the dicuss forums: https://discuss.elastic.co/t/elasticsearch-client-nodes/26598
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete snapshot not able to interrupt shard level operation due to Azure client call hanging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12567</link><project id="" key="" /><description>Have a scenario where a delete snapshot is not able to interrupt the shard level request. The snapshot state shows aborted, with 1 shard still in started status:

```
{
  "snapshots" : [ {
    "snapshot" : "20150728t111647z",
    "repository" : "snapshot",
    "state" : "ABORTED",
    "shards_stats" : {
      "initializing" : 0,
      "started" : 1,
      "finalizing" : 0,
      "done" : 129,
      "failed" : 0,
      "total" : 130
    },
```

Shard in started status except that processed_size_in_bytes hasn't moved in &gt;1 day.

```
        "shards" : {
          "0" : {
            "stage" : "STARTED",
            "stats" : {
              "number_of_files" : 110,
              "processed_files" : 87,
              "total_size_in_bytes" : 2486849879,
              "processed_size_in_bytes" : 1493448551,
              "start_time_in_millis" : 1438082207888,
              "time_in_millis" : 0
            },
```

The hot threads is showing the snapshot thread stuck due to an Azure client call:

```
   0.0% (0s out of 500ms) cpu usage by thread 'elasticsearch[ESNode-ElasticSearchData_IN_2][snapshot][T#196]'
     10/10 snapshots sharing following 25 elements
       java.net.SocketInputStream.socketRead0(Native Method)
       java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
       java.net.SocketInputStream.read(SocketInputStream.java:170)
       java.net.SocketInputStream.read(SocketInputStream.java:141)
       java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
       java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
       java.io.BufferedInputStream.read(BufferedInputStream.java:345)
       sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704)
       sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647)
       sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1535)
       sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440)
       java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)
       com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:124)
       com.microsoft.azure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:244)
       com.microsoft.azure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:321)
       com.microsoft.azure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:285)
       org.elasticsearch.cloud.azure.blobstore.AzureOutputStream.close(AzureOutputStream.java:41)
       org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:559)
       org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:500)
       org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:140)
       org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:85)
       org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:852)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```

Is there a way to handle this situation better when the Azure call hangs so that the delete snapshot can successfully interrupt the outstanding (but stuck) shard request?
</description><key id="98244038">12567</key><summary>Delete snapshot not able to interrupt shard level operation due to Azure client call hanging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Plugin Cloud Azure</label><label>:Snapshot/Restore</label></labels><created>2015-07-30T19:29:01Z</created><updated>2016-01-19T17:03:23Z</updated><resolved>2016-01-19T17:03:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T10:35:33Z" id="127951548">This is a problem in the Azure client itself.  Threads do not timeout by default and so the only way to interrupt the thread is to restart the node.  There are two timeouts available:

`setMaximumExecutionTimeInMs`

The maximum execution time interval begins at the time that the client begins building the request. The default maximum execution is set in the client and is by default null, indicating no maximum time.

`setTimeoutIntervalInMs`

Sets the timeout to use when making this request. The default server timeout is set in the client and is by default null, indicating no server timeout.

See http://dl.windowsazure.com/storage/javadoc/com/microsoft/azure/storage/RequestOptions.html for full description.

We should probably do the following:
- expose the timeout settings
- check how they are _really_ used by the Azure client depending of the type of request (max. operation time seems to be checked in between 2 request retries but I'm not sure)
- check that delete command leaves the snapshot in a coherent state if a timeout happen
- add some docs about it
</comment><comment author="dadoonet" created="2016-01-14T08:20:03Z" id="171569348">&gt; `setTimeoutIntervalInMs`
&gt; 
&gt; Sets the timeout to use when making this request. The default server timeout is set in the client and is by default null, indicating no server timeout.

`setTimeoutIntervalInMs` has been implemented in #15080. 

&gt; We should probably do the following:
&gt; - expose the timeout settings

Done. See https://github.com/elastic/elasticsearch/pull/15080/files#diff-e1e28473441a5cb9762b0176ddc4d22bR126

&gt; - check how they are _really_ used by the Azure client depending of the type of request (max. operation time seems to be checked in between 2 request retries but I'm not sure)

That seems to still be an issue. According to recent reports with the support team, it looks like that even after 5 minutes, Azure client does not stop and raise an exception...

&gt; - check that delete command leaves the snapshot in a coherent state if a timeout happen

@imotov could comment on what would happen if the azure client raises an exception.

&gt; - add some docs about it

Done in #15080. https://www.elastic.co/guide/en/elasticsearch/plugins/master/repository-azure-usage.html

I wonder if we should implement `setMaximumExecutionTimeInMs` instead? @craigwi WDYT? Should we use this instead of `setTimeoutIntervalInMs`? Or may be there is an issue in the azure client?
</comment><comment author="craigwi" created="2016-01-14T21:02:17Z" id="171778737">Hi @dadoonet, this service request came from us and I just now looked at the details.  As far as I can tell, the problem is that I did not pass the BlobRequestOptions to the calls to openInputStream() and openOutputStream() (in AzureStorageServiceImpl.java).

I had assumed that the options on the CloudBlobClient would be inherited by all containers and blobs referenced by it, but the evidence here is that the timeout option, at least, does not get passed through.

I am going to change my fork of the plugin and make the two calls looks like:

```
    return client.getContainerReference(container).getBlockBlobReference(blob).openInputStream(null, client.getDefaultRequestOptions(), null);
```

and 

```
    return client.getContainerReference(container).getBlockBlobReference(blob).openOutputStream(null, client.getDefaultRequestOptions(), null);
```

The middle parameter is the key addition, of course.

I'll verify and let you know.

Craig.
</comment><comment author="dadoonet" created="2016-01-14T21:07:03Z" id="171779878">Fantastic !
</comment><comment author="dadoonet" created="2016-01-15T10:32:19Z" id="171928658">@craigwi I looked at the azure client code and found that when you call `openOutputStream()`, it calls `openOutputStream(null, null /* options */, null)`.

Options are then populated by calling `options = BlobRequestOptions.applyDefaults(options, BlobType.BLOCK_BLOB, this.blobServiceClient, false)` which does:

``` java
    protected static final BlobRequestOptions applyDefaults(final BlobRequestOptions options, final BlobType blobType,
            final CloudBlobClient client, final boolean setStartTime) {
        BlobRequestOptions modifiedOptions = new BlobRequestOptions(options);
        BlobRequestOptions.populateRequestOptions(modifiedOptions, client.getDefaultRequestOptions(), setStartTime);
        return BlobRequestOptions.applyDefaultsInternal(modifiedOptions, blobType, client, setStartTime);
    }
```

`BlobRequestOptions.populateRequestOptions` then calls `RequestOptions.populateRequestOptions` which does:

``` java
    protected static final RequestOptions populateRequestOptions(RequestOptions modifiedOptions,
            final RequestOptions clientOptions, final boolean setStartTime) {
        // ...
        if (modifiedOptions.getTimeoutIntervalInMs() == null) {
            modifiedOptions.setTimeoutIntervalInMs(clientOptions.getTimeoutIntervalInMs());
        }

        if (modifiedOptions.getMaximumExecutionTimeInMs() == null) {
            modifiedOptions.setMaximumExecutionTimeInMs(clientOptions.getMaximumExecutionTimeInMs());
        }

        if (modifiedOptions.getMaximumExecutionTimeInMs() != null
                &amp;&amp; modifiedOptions.getOperationExpiryTimeInMs() == null &amp;&amp; setStartTime) {
            modifiedOptions.setOperationExpiryTimeInMs(new Date().getTime()
                    + modifiedOptions.getMaximumExecutionTimeInMs());
        }

        return modifiedOptions;
    }
```

So to me your initial assumption is correct. The default client options are applied here.
I don't think that passing `client.getDefaultRequestOptions()` will change anything here unless I'm missing something.

May be it's a bug elsewhere? 
</comment><comment author="dadoonet" created="2016-01-15T11:00:02Z" id="171933503">@craigwi so I created a simple test case and looked in debug mode at what is happening on the azure client:

``` java
public void testDumb() throws URISyntaxException, StorageException {
    CloudStorageAccount storageAccount = CloudStorageAccount.getDevelopmentStorageAccount();
    CloudBlobClient client = storageAccount.createCloudBlobClient();
    client.getDefaultRequestOptions().setTimeoutIntervalInMs(10000);
    client.getContainerReference("dumb").getBlockBlobReference("blob").openOutputStream();
}
```

Options are correctly updated.

&lt;img width="607" alt="blobrequestoptions java - elasticsearch - -documents-elasticsearch-dev-es-gradle-elasticsearch intellij idea aujourd hui at 11 44 32" src="https://cloud.githubusercontent.com/assets/274222/12351644/6795ecee-bb7f-11e5-8371-a7a92f69b6ae.png"&gt;

I looked at the `BlobOutputStream` and the root problem is that actually the azure client does not use at all this time out setting.
This setting is only used by `BlobRequest` which we are not using.

&lt;img width="1084" alt="intellij idea intellij idea aujourd hui at 11 50 50" src="https://cloud.githubusercontent.com/assets/274222/12351651/73fb24a4-bb7f-11e5-8ff1-1349e2199299.png"&gt;

So I believe that the change we have to do is bigger than what we did until now. I think it's time to implement #12448 which would use not Azure Input/Output Streams but `StorageRequest` or `BlobRequest`. I'll look at the Azure storage client examples unless you find another idea in the meantime.
</comment><comment author="dadoonet" created="2016-01-15T13:29:52Z" id="171962184">I looked at the other parts of the Azure client code and I found only one case where the `RequestOptions` is used:

In `CloudBlockBlob.upload`:

``` java
        // If the stream is rewindable, and the length is known and less than
        // threshold the upload in a single put, otherwise use a stream.
        if (sourceStream.markSupported() &amp;&amp; descriptor.getLength() != -1
                &amp;&amp; descriptor.getLength() &lt; options.getSingleBlobPutThresholdInBytes() + 1) {
            this.uploadFullBlob(sourceStream, descriptor.getLength(), accessCondition, options, opContext);
        }
```

But when you use directly a stream, `RequestOptions` are not used.. 

So the only short-term workaround I can see is by using the `upload` method and make sure we enter that code bloc.

I opened https://github.com/Azure/azure-storage-java/issues/71.
</comment><comment author="imotov" created="2016-01-15T16:10:37Z" id="172002727">&gt; &gt; check that delete command leaves the snapshot in a coherent state if a timeout happen
&gt; 
&gt; @imotov could comment on what would happen if the azure client raises an exception.

The problem here is not in a timeout during a delete command. The problem is in absence of a timeout while writing a file to azure. The delete operation first makes sure that a snapshot that a user is trying to delete is not currently running, and if the snapshot is running the delete operation tries to cancel it. The cancellation process sends an abort message to all shards involved in the snapshot and waits for all shards to stop writing. The source of the problem is that one of the shards cannot stop writing because it's forever stuck in the write operation. If we will add the write operation timeout, it will resolve the core issue - the abort operation will not get stuck for more then the timeout interval and therefore will be easily cancellable. Moreover, it will help to eliminate the need to delete a stuck snapshot to start with, since the snapshot will not get stuck waiting for a write to finish in the first place. Instead it will just fail the snapshot for the shard that got stuck and move on. 
</comment><comment author="craigwi" created="2016-01-17T20:05:10Z" id="172373960">guys, per the post I made on azure-storage-java (https://github.com/Azure/azure-storage-java/issues/71), the primary issue (lack of timeout) is due to a bug in an old version of com.microsoft.azure.storage.  We need to use version 3.0.0 or newer.
</comment><comment author="craigwi" created="2016-01-19T04:28:27Z" id="172733697">I updated my version of the cloud azure plugin to use azure-storage-java v3.1: https://github.com/craigwi/elasticsearch-cloud-azure/releases/tag/v2.8.4-craigwi.  I picked v3.1 because v4.0.0 looked to have a larger scope of changes and more risk.  So far, my tests, including frequent snapshots and restores, appear to be successful.

Craig.
</comment><comment author="dadoonet" created="2016-01-19T12:46:06Z" id="172842982">@craigwi It sounds like we should set `client.getDefaultRequestOptions().setMaximumExecutionTimeInMs(timeout);` 
In that case, we will have a better control from elasticsearch when something goes wrong and takes too much time.

WDYT?

I wonder if we should set both timeouts or only `setMaximumExecutionTimeInMs`?
I guess that `setMaximumExecutionTimeInMs` would be enough.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.unassigned.node_left.delayed_timeout not working stably in 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12566</link><project id="" key="" /><description>For example in the case below (data retrieved from _cluster/health)
Right after I kill the node:

```
{
  "cluster_name" : "essandbox-cluster",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 19,
  "number_of_data_nodes" : 13,
  "active_primary_shards" : 783,
  "active_shards" : 1480,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 86,
  "delayed_unassigned_shards" : 86,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0
}
```

I set the timeout to 30s. The node is back at around 10s later. But shards only gradually start recovering at until ~1.5 min later. And it's not at the speed I&#8217;m expecting. And I don&#8217;t know why it&#8217;s relocating_shards. 

And worst is sometimes after a while it looks as if it stopped recovering, and I need to manually reroute unassigned.

```
{
  "cluster_name" : "essandbox-cluster",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 20,
  "number_of_data_nodes" : 14,
  "active_primary_shards" : 783,
  "active_shards" : 1482,
  "relocating_shards" : 2,
  "initializing_shards" : 1,
  "unassigned_shards" : 83,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0
}
```
</description><key id="98243720">12566</key><summary>index.unassigned.node_left.delayed_timeout not working stably in 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">mkliu</reporter><labels><label>:Allocation</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-07-30T19:27:23Z</created><updated>2016-01-14T14:25:12Z</updated><resolved>2016-01-14T14:25:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-30T19:29:14Z" id="126447585">@mkliu when the node left the cluster, do you see the log message about the delay in the ES logs, it should look like:

```
delaying allocation for [N] unassigned shards, next check in [Ns]
```

(where N is a number), can you paste what it says?
</comment><comment author="mkliu" created="2015-07-30T20:17:12Z" id="126468844">```
July 30th 2015, 11:13:47.609    essandbox-cluster   INFO    [xxx] delaying allocation for [86] unassigned shards, next check in [29.1s]
July 30th 2015, 11:14:19.007    essandbox-cluster   INFO    [xxx] delaying allocation for [0] unassigned shards, next check in [0s]
July 30th 2015, 11:14:19.580    essandbox-cluster   INFO    [xxx] delaying allocation for [0] unassigned shards, next check in [0s]
```
</comment><comment author="dakrone" created="2015-07-30T20:52:25Z" id="126482786">@mkliu according to the timestamp it looks like it did do the reroute at the correct time (13:47 and then ~30 seconds later at 14:19).

The log message is confusing and will be fixed by #12532
</comment><comment author="mkliu" created="2015-07-30T21:17:20Z" id="126491207">@dakrone hmm, it's actually not doing reroute, as described in the first post, I had the manually kick start in the end. The 

```
  "delaying allocation for [0] unassigned shards"
```

goes on and on and on and on.
</comment><comment author="dakrone" created="2015-08-05T18:13:27Z" id="128095821">@mkliu can you increase the logging level for your cluster to DEBUG and make the master log available so I can take a look?
</comment><comment author="dakrone" created="2015-10-30T20:59:54Z" id="152649935">I think this may have been fixed by https://github.com/elastic/elasticsearch/pull/12678 , @mkliu can you confirm?
</comment><comment author="clintongormley" created="2016-01-14T14:25:12Z" id="171656701">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_termvectors `took` parameter alternating between small and large number</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12565</link><project id="" key="" /><description>I have an index `dlstats` that has a single shard with one replica, in a two-node cluster:

```
index   shard prirep state   docs   store ip       node
dlstats 0     p      STARTED   21 265.6kb 10.0.0.5 Donald Pierce
dlstats 0     r      STARTED   21 265.6kb 10.0.0.5 Peepers
```

When looking at the (blank) termvectors for a doc, I get alternating `took` times. It seems to be an issue with round-robining the request. If I set `_preference:_primary` it still does it:

```
% for i in `seq 10`; do curl -s foo:secret@localhost:9200/dlstats/blob/2012-12-17/_termvectors\?_preference:_primary\&amp;format=yaml | grep -F took; done
took: 1
took: 1438281967692
took: 1
took: 1438281967750
took: 1
took: 1438281967802
took: 1
took: 1438281967840
took: 1
took: 1438281967875
```

But if I actually turn replicas off, it does not:

```
index   shard prirep state   docs   store ip       node
dlstats 0     p      STARTED   21 265.6kb 10.0.0.5 Donald Pierce
```

```
% for i in `seq 10`; do curl -s foo:secret@localhost:9200/dlstats/blob/2012-12-17/_termvectors\?_preference:_primary\&amp;format=yaml | grep -F took; done
took: 1
took: 1
took: 1
took: 1
took: 1
took: 1
took: 1
took: 1
took: 1
took: 1
```

Adding a replica back, does the same thing again:

```
% for i in `seq 10`; do curl -s foo:secret@localhost:9200/dlstats/blob/2012-12-17/_termvectors\?_preference:_primary\&amp;format=yaml | grep -F took; done
took: 1
took: 1438282196274
took: 1
took: 1438282196304
took: 1
took: 1438282196339
took: 1
took: 1438282196362
took: 1
took: 1438282196402
```

Then if I add another node and another replica, the funny `took` seems to happen per replica:

```
% for i in `seq 10`; do curl -s foo:secret@localhost:9200/dlstats/blob/2012-12-17/_termvectors\?_preference:_primary\&amp;format=yaml | grep -F took; done
took: 1438282274784
took: 1438282274796
took: 1
took: 1438282274828
took: 1438282274841
took: 1
took: 1438282274873
took: 1438282274885
took: 1
took: 1438282274912
```

So maybe there is a bug there and in `*TermVectors*` handling of `_preference`?

ES version:

```
{
  "name" : "Donald Pierce",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.0.0-beta1",
    "build_hash" : "c315d54c2a6695301512fecdccb8f7de22e3ccfe",
    "build_timestamp" : "2015-07-16T19:42:13Z",
    "build_snapshot" : true,
    "lucene_version" : "5.2.1"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="98239364">12565</key><summary>_termvectors `took` parameter alternating between small and large number</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:Term Vectors</label><label>bug</label></labels><created>2015-07-30T19:00:00Z</created><updated>2016-10-07T09:04:04Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-12T11:57:45Z" id="218736099">Closed by https://github.com/elastic/elasticsearch/pull/17817
</comment><comment author="maeserichar" created="2016-10-07T09:04:04Z" id="252189109">Hi! 
We are having the same problem in the 2.3.3 version.  Is there any posibility of porting #17817 to the 2.x branch?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cleanup plugin installation in bats tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12564</link><project id="" key="" /><description>When reviewing #12508, I noticed that the plugin installation in the bats tests used the plugin name typically used when installing without specifying the url:

```
/tmp/elasticsearch/bin/plugin install elasticsearch/shield/latest -u "file://$SHIELD_ZIP"
```

typically we say to install with just `shield` as the name when specifying the URL. I think we should cleanup the installation to be more realistic.
</description><key id="98228688">12564</key><summary>cleanup plugin installation in bats tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-07-30T18:07:43Z</created><updated>2015-08-11T15:45:54Z</updated><resolved>2015-08-11T15:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-11T15:45:44Z" id="129937755">closing in favor of [#12651](https://github.com/elastic/elasticsearch/issues/12651)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should Analyzer setting override search-analyzer for non-analyzed fields ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12563</link><project id="" key="" /><description>Analyzer  erroneously specified in mapping for a field of type non-analyzed  overrides the default  keyword search analyzer
Also the analyze api seems to suggest the index_analyzer is overridden even though it is not so

Example:

``` json

PUT test
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer" : {
                    "tokenizer" : "keyword",
                    "filter" : "lowercase"
                }
            }
        }
    },
    "mappings": {
        "test" : {
            "properties" : {
                "data" : {"type" :"string" ,"index" : "not_analyzed","analyzer" : "my_analyzer"}
            }
        }
    }
}

GET  test/_analyze?field=data&amp;text=IT IS ELASTIC

#Response gives the wrong impression that index-analyzer is overridden


PUT test/test/1 
{
    "data" : "it is elastic"
}

POST test/_search
{
    "query": {
        "match": {
           "data": "IT IS ELASTIC"
        }
    }
}
#Response indicates *search_analyzer* is overridden 
   "hits": {
      "total": 1,
      "max_score": 0.30685282,
      "hits": [
         {
            "_index": "test",
            "_type": "test",
            "_id": "1",
            "_score": 0.30685282,
            "_source": {
               "data": "it is elastic"
            }
         }
      ]
   }
```

Shouldn't analyzer setting be ignored for non-analyzed fields or at-least in the analyze api  for non-analyzed fields?

Tested in elasticsearch 1.6.0 and 2.0.0 beta snapshot
</description><key id="98225179">12563</key><summary>Should Analyzer setting override search-analyzer for non-analyzed fields ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keety</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-07-30T17:49:50Z</created><updated>2015-08-05T10:14:21Z</updated><resolved>2015-08-05T10:14:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T10:14:21Z" id="127947725">You are absolutely correct.  Weird that it has taken this long for this issue to show up for the first time.

I was going to put it up as a bug to be fixed, but then I remembered this change that we're planning for 2.0: https://github.com/elastic/elasticsearch/issues/12394

The `analyzed`/`not_analyzed` setting will be replaced by splitting the `string` type into `text` and `keyword` types, where `keyword` fields are not analyzed by default, but WILL support analyzers (as long as the tokenizer is  `keyword`).

So it will be acceptable to set an analyzer on a `keyword` field, which means this particular issue doesn't need fixing.

thanks for the good recreation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename position_offset_gap to position_increment_gap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12562</link><project id="" key="" /><description>The name "position_offset_gap" is confusing because Lucene has three similar sounding things:
- `Analyzer#getPositionIncrementGap`
- `Analyzer#getOffsetGap`
- `IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS` and `FieldType#storeTermVectorOffsets`

Elasticsearch's `position_offset_gap` is mapped directly to `Analyzer#getPositionIncrememntGap` - its the gap in term positions between multiple values of the same field in the same document.

`Analyzer#getOffsetGap` is the position gap between _tokens_ and shouldn't be messed with on the mapping level.

The word "offsets" `IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS` and `FieldType#storeTermVectorOffsets` refers to storing the offsets into the text of the field in either the postings list or the term vectors and are primarily used for highlighting.

I propose Elasticsearch renames `position_offset_gap` to `position_increment_gap` to line up with Lucene and reserves the work "offset" to talk about the offsets into the text of fields. I know Elasticsearch doesn't always have to line up with the Lucene names for things but in this case its quite confusing not to. We could always go with other names like `multi_value_position_gap` or `multi_value_term_gap` if we think they are more descriptive.

In reference to https://github.com/elastic/elasticsearch/pull/12544#issuecomment-126132622
</description><key id="98223676">12562</key><summary>Rename position_offset_gap to position_increment_gap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-07-30T17:40:58Z</created><updated>2015-09-14T17:17:14Z</updated><resolved>2015-08-26T22:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-30T17:50:46Z" id="126416328">+1 for `position_increment_gap`: even if we have to deprecate the old name and all that, it will be so much less confusing: the name makes sense because offsets are not involved, and if someone does a google search on this, they will already find all kinds of documentation/explanations about what it does. 

changing the position gap is useful and important if you have multiple values per field and are doing prox queries. changing the offset gap will basically just break highlighters.
</comment><comment author="nik9000" created="2015-07-30T18:03:14Z" id="126420050">What Elasticsearch policy on naming things like Lucene? I could think of lots of different ones:
0. "We strive to match the Lucene name for things where possible."
1. "We will name things like Lucene names them unless the Lucene name is silly and confusing."
2. "We will name things like Lucene names them unless we can think of a more descriptive name."
3. "Who cares what Lucene calls a thing? We are abstracting over it anyway and if we feel like we have to name something the same way our abstraction is leaking."

Personally I'd prefer 1 or 2 over the other options.
</comment><comment author="clintongormley" created="2015-08-05T10:17:03Z" id="127948479">I'm good with `position_increment_gap`.  For the novice user it won't make much sense (but neither will the current name). Either way it needs good documentation.

&gt; Personally I'd prefer 1 or 2 over the other options.

Agreed
</comment><comment author="clintongormley" created="2015-08-05T10:18:53Z" id="127948907">Note: existing indices should have the position_offset_gap setting automatically changed to the new setting.

When you do this, please open an issue on http://github.com/elastic/elasticsearch-migration/issues
</comment><comment author="xuzha" created="2015-08-25T16:35:43Z" id="134664574">Do we need this in 2.0?
</comment><comment author="s1monw" created="2015-08-26T07:15:59Z" id="134873498">+1 for 2.0
</comment><comment author="xuzha" created="2015-08-26T22:31:25Z" id="135196330">Thanks guys, merged the changes into master and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disallow type names to start with dots for new indices except for .percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12561</link><project id="" key="" /><description>This commit will disallow type names to start with dots except for .percolator for version 2 and later indices.

Closes #12560
</description><key id="98206733">12561</key><summary>Disallow type names to start with dots for new indices except for .percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-30T16:08:29Z</created><updated>2015-08-12T06:40:18Z</updated><resolved>2015-08-12T06:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-30T16:11:56Z" id="126386155">This looks good to me, but I would like @clintongormley to confirm we want this change before merging.
</comment><comment author="alexksikes" created="2015-07-31T09:45:33Z" id="126627929">This looks good, however we should still allow for plugins to register reserved types?
</comment><comment author="martijnvg" created="2015-07-31T10:57:44Z" id="126652228">Left one minor comment about the code. I do think we should mention this change in the `migrate_2_0.asciidoc` file.
</comment><comment author="jpountz" created="2015-07-31T10:59:39Z" id="126653152">&gt; This looks good, however we should still allow for plugins to register reserved types?

If we think plugins should be able to reserve types, I'd rather not enforce anything in MapperService than make the list of reserved types pluggable. For the record, this would already be an issue today without @jasontedor 's pull request as we are logging a warning for types that contain a dot.
</comment><comment author="javanna" created="2015-07-31T11:20:17Z" id="126658046">I think we can replace "reserved type names" in the description of this PR with "percolator type". We don't have a mechanism to register reserved types. The fact that the `.percolator` type is special is hardcoded, then whether we should open this up to plugins is a different question I think.

I was expecting though any type with name that starts with "." would be hidden, like the `.percolator` one, but that is not the case, only the `.percolator` type is treated differently. I wonder whether we should better streamline this "." naming convention or just go with this PR and allow the `.percolator` only, which is just what we need for now.
</comment><comment author="jasontedor" created="2015-08-04T16:49:54Z" id="127672102">@martijnvg I agree with your suggestion on the code. I squashed a new commit for this that includes updated migration docs as well.
</comment><comment author="rjernst" created="2015-08-04T17:03:36Z" id="127676776">LGTM. Let's go with this PR as is, and revisit reserving type names in plugins later.
</comment><comment author="jasontedor" created="2015-08-04T17:17:01Z" id="127680519">@clintongormley @jpountz mentioned getting confirmation from you before merging into master.
</comment><comment author="clintongormley" created="2015-08-05T09:21:50Z" id="127928529">Hmmm I can see that it could be a good thing to prevent type names from **beginning** with a dot, but what's the problem with them containing a dot?  It used to be a problem when type names were used as a prefix to disambiguate fields, but that syntax is no longer allowed.

Perhaps we should change the restriction to just prevent type names that begin with a dot?
</comment><comment author="jasontedor" created="2015-08-05T18:35:29Z" id="128102610">@clintongormley Given that change, I agree. I've updated the PR.
</comment><comment author="alexksikes" created="2015-08-10T09:02:03Z" id="129373167">@clintongormley Why would it be a good thing to prevent type names from beginning with a dot? IMHO with this PR aren't we disabling the ability for types to be hidden?
</comment><comment author="jpountz" created="2015-08-10T12:54:18Z" id="129431251">I don't think we should allow either applications or plugins to reserve hidden types. I know we have been doing this with the percolator, but I don't think this is something we should replicate.

In order to be useful, hidden types will likely need to store data, so they also need mappings. However in 2.0 we now check much more aggressively that all types have consistent mappings, so whenever a hidden type would add mappings to an index, it would restrict mappings that can be set on other regular types. If a plugin wants to reserve a namespace where it can store data, it should rather go with an index, like elasticsearch-core already does with the `.script` index and `watcher` also does with the `.watches` index.

So +1 to merge and reject type names that start with a dot.
</comment><comment author="javanna" created="2015-08-10T13:14:41Z" id="129440484">&gt; aren't we disabling the ability for types to be hidden?

I care to clarify that we don't have the ability for types to be hidden. Only the `.percolator` type is hidden at the moment, just because it is called `.percolator` not because its name starts with `.`. That said we are not disabling any feature with this PR. I guess we are just saying that `.percolator` is enough and we don't want to add the ability for types to be hidden in general. The `.percolator` type will stay untouched.
</comment><comment author="alexksikes" created="2015-08-10T13:31:14Z" id="129454556">@javanna thanks for the clarification, makes sense. @jpountz I see where you are coming from, you'd rather encourage people to use an index instead of a new type. I'm thinking we could just restrict the usage of `.percolator` and that's it (maybe later on a list of reserved system types could be added). It seems that forbidding the `.` in types by itself doesn't buy us anything IMHO.
</comment><comment author="jasontedor" created="2015-08-10T13:44:26Z" id="129459391">@alexksikes It reserves an entire class of names (those starting with dot) for future use. We can't reserve names in the future without potentially making breaking changes. This is why we want to do this on a major release when breaking changes are expected.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disallow type names to start with dots for new indices except for .percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12560</link><project id="" key="" /><description>Currently we [warn](https://github.com/elastic/elasticsearch/blob/2068fbfd1ea31cbc7b64d76a908a5e36a6aa62fb/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java#L275) but allow type names to contains dots. In Elasticsearch 2.0, we will disallow type names to start with dots for any new indices (indices created on 2.0 or later) except for .percolator.
</description><key id="98186086">12560</key><summary>Disallow type names to start with dots for new indices except for .percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-30T14:41:51Z</created><updated>2015-08-12T06:40:11Z</updated><resolved>2015-08-12T06:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in bulk operation leading to IndexMissingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12559</link><project id="" key="" /><description>ElasticSearch 1.7.0, 5 nodes, each running in a docker container.

Got the following exception during a bulk index operation

```
[2015-07-30 13:29:05,558][WARN ][action.bulk              ] [duvel-elasticsearch_3] unexpected error during the primary phase for action [indices:data/write/bulk[s]]
java.lang.NullPointerException
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:128)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.doRun(TransportShardReplicationOperationAction.java:354)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.doExecute(TransportShardReplicationOperationAction.java:112)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.doExecute(TransportShardReplicationOperationAction.java:74)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:207)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:189)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:222)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

Any following bulk operation returns a `IndexMissingException` error

In case that help, here is an extract of the returned message of the first error:

```
{
    "index": {
        "_index": "ads",
        "_type": "ads",
        "_id": "bdi-190-1901",
        "status": 500,
        "error": "RemoteTransportException[[duvel-elasticsearch_3][inet[/172.17.4.62:9302]][indices:data/write/bulk[s]]]; nested: NullPointerException; "
    },
    "bulkCommand": {
        "operation": {
            "index": {
                "_index": "ads",
                "_type": "ads",
                "_id": "bdi-190-1901"
            }
        },
        "data": {
            "some data": "..."
        }
    }
}
```

This may be a duplicate of #11671
</description><key id="98179526">12559</key><summary>NullPointerException in bulk operation leading to IndexMissingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Volune</reporter><labels><label>:Bulk</label><label>feedback_needed</label></labels><created>2015-07-30T14:11:08Z</created><updated>2015-08-07T13:27:50Z</updated><resolved>2015-08-07T13:10:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-05T09:06:26Z" id="127925067">Hi @Volune 

Was the index deleted?  The problem in #11671 was caused by the indices being deleted, so if yours was too, then it is a duplicate
</comment><comment author="Volune" created="2015-08-07T08:57:14Z" id="128645096">There was no log indicating that the index was deleted on any node. Also it seemed visible in the "ElasticSearch Head" interface, but empty. Auto-creation of indexes is disabled on the server. Still with the `IndexMissingException` I'm not sure what to trust.
</comment><comment author="clintongormley" created="2015-08-07T11:32:32Z" id="128679777">Can you upload your logs from master from before the scroll started?
</comment><comment author="Volune" created="2015-08-07T13:08:56Z" id="128695345">Sadly I just discovered that I don't have these logs anymore.
</comment><comment author="clintongormley" created="2015-08-07T13:10:59Z" id="128695639">thanks @Volune 

I think that for now we can assume that something similar happened, and close this in favour of #11671 
</comment><comment author="Volune" created="2015-08-07T13:27:50Z" id="128698751">Fine with me, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cat.nodeattrs/10_basic/Test somtimes fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12558</link><project id="" key="" /><description>FAILURE 0.14s | RestIT.test {p0=cat.nodeattrs/10_basic/Test cat nodes attrs output} &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError: field [$body] was expected to match the provided regex but didn't
&gt; Expected: ((\S+)\s+(\S+)\s+(\d{1,3}.){3}\d{1,3}\s+(\S+)\s+(\S+)\s*)+

See http://build-us-00.elastic.co/job/es_core_master_regression/2933
</description><key id="98175072">12558</key><summary>cat.nodeattrs/10_basic/Test somtimes fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-07-30T13:52:38Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-09-10T15:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-30T14:16:42Z" id="126343967">Ping @metadave.
</comment><comment author="rmuir" created="2015-07-30T14:22:32Z" id="126346542">interestingly enough, it seems to only fail in integration test phase, not when run as unit test. This is the case where we startup bin/elasticsearch externally and run the tests against it... maybe its looking for something that is different in that case?
</comment><comment author="metadave" created="2015-07-30T14:22:52Z" id="126346659">looks like an empty string is returned from `_cat/nodeattrs` in some cases. 
</comment><comment author="rmuir" created="2015-07-30T14:28:36Z" id="126348258">and these are the first test runs testing the .tar distribution (https://github.com/elastic/elasticsearch/pull/12549), so its possible something is screwy with tar distribution (or how we run it?) that causes this... but this is a mirror image of how we test the zip, just with untar instead of unzip.
</comment><comment author="metadave" created="2015-07-30T17:21:10Z" id="126409017">I'm not sure if it's currently possible to make this test pass during the integration test phase. When I initially wrote the test, there appeared to be several node attributes already defined. Changing the regex suffix from `+` to `*` would partially work, but would mask errors if node attributes aren't showing up at all.

See also:
https://github.com/elastic/elasticsearch/pull/12520#issuecomment-125827569
</comment><comment author="clintongormley" created="2015-08-05T10:05:26Z" id="127944743">@metadave changing it to `*` would definitely be the easiest fix.  Another possibility is the `skip` functionality. We can skip tests if some condition isn't met (the condition itself needs to be coded into the test runner).  For instance, we can skip the scripting tests if groovy scripting isn't enabled, by running a pre-check to determine whether a simple groovy script can execute. 

Could add the same thing to skip this test if no node attributes are defined.  Not sure it is worth it though.
</comment><comment author="nik9000" created="2015-08-28T19:26:24Z" id="135866640">I'm just going to swap `*` in for `+` to handle those cases where there isn't one set. I'm also adding a test for the help. Its not perfect but better than leaving it disabled.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ThrowableObjectInputStream does not use classloader from settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12557</link><project id="" key="" /><description>Cluster: 2 nodes
Index not unique document with create flag=true.
node throws DocumentAlreadyExistsException
but this class cannot be loaded  by thread context classloader and
NotSerializableTransportException is thrown.
</description><key id="98170742">12557</key><summary>ThrowableObjectInputStream does not use classloader from settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lytvynenkoinvest</reporter><labels><label>:Exceptions</label><label>adoptme</label><label>bug</label></labels><created>2015-07-30T13:36:44Z</created><updated>2016-01-26T17:11:28Z</updated><resolved>2016-01-26T17:11:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lytvynenkoinvest" created="2015-07-30T13:41:52Z" id="126328306">NettyTransportChannel.sendResponse(Throwable error) line 120 -&gt;
ThrowableObjectOutputStream.canSerialize(error) line 85 -&gt;
ThrowableObjectOutputStream.serialize(T t) line 74 -&gt;
ThrowableObjectInputStream(InputStream in) line 40: constructor does not use classloader from settings
</comment><comment author="lytvynenkoinvest" created="2015-07-30T13:47:40Z" id="126330393">It would be great if classloader will be passed to canSerialize method like this: ThrowableObjectOutputStream.canSerialize(error, transport.settings().getClassLoader())
</comment><comment author="clintongormley" created="2015-08-05T08:59:39Z" id="127923186">Hi @lytvynenkoinvest 

Which version is this in?
</comment><comment author="s1monw" created="2015-08-05T09:59:37Z" id="127943125">Are you running this with assertions enabeld ie. during unittests? can you provide more information about this?
</comment><comment author="lytvynenkoinvest" created="2015-08-05T11:07:30Z" id="127956317"> not unit tests.
there are 2 classloaders: 
1) classloader that loads class in which I create and start embedded node, and 
2) threadcontextclassloader that does not contains classes of elasticsearch.
First classloader I set to settings that I put to NodeBuilder.
Then I start 2 nodes in the same cluster, and &#160;I send requests to index(with create=true) in first node.
Periodicaly, first node redirects index requests to second node(round robin).
When second node throws DocumentAlreadyExistsException, the second node checks can this exception be serialized or not. there is the method that serializes exception to byte array and deserializes it and tries to load this class(as I described in issue).
But, to load class it uses threadcontext classloader, not classloader from settings (it is first classloader from list above).
But, ThreadContext classloader does not contain this class and the method throws NotSerializable exception.

&gt; &#1057;&#1088;&#1077;&#1076;&#1072;,  5 &#1072;&#1074;&#1075;&#1091;&#1089;&#1090;&#1072; 2015, 3:00 -07:00 &#1086;&#1090; Simon Willnauer notifications@github.com:
&gt; 
&gt; Are you running this with assertions enabeld ie. during unittests? can you provide more information about this?
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub .
</comment><comment author="lytvynenkoinvest" created="2015-08-05T11:07:40Z" id="127956344"> 1.7.1

&gt; &#1057;&#1088;&#1077;&#1076;&#1072;,  5 &#1072;&#1074;&#1075;&#1091;&#1089;&#1090;&#1072; 2015, 2:00 -07:00 &#1086;&#1090; Clinton Gormley notifications@github.com:
&gt; 
&gt; Hi  @lytvynenkoinvest
&gt; Which version is this in?
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub .
</comment><comment author="clintongormley" created="2016-01-26T17:11:28Z" id="175122404">This has been completely rewritten in 2.0. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion suggester housing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12556</link><project id="" key="" /><description>- Merged Areek's Branch 
- Merged es completion_suggester 
</description><key id="98153326">12556</key><summary>Completion suggester housing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mudit3774</reporter><labels /><created>2015-07-30T12:06:11Z</created><updated>2015-07-30T12:10:08Z</updated><resolved>2015-07-30T12:06:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Removed unused lockfile, Added SLES 11 support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12555</link><project id="" key="" /><description>I removed the lockfile as it is not used anywhere and the service manager should take care of the locking. I also added SLES 11 support. 
Tested on RHEL 6.7 and SLES 11 SP3

This fixes #6798 and #9366
</description><key id="98144149">12555</key><summary>Removed unused lockfile, Added SLES 11 support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jjfalling/following{/other_user}', u'events_url': u'https://api.github.com/users/jjfalling/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jjfalling/orgs', u'url': u'https://api.github.com/users/jjfalling', u'gists_url': u'https://api.github.com/users/jjfalling/gists{/gist_id}', u'html_url': u'https://github.com/jjfalling', u'subscriptions_url': u'https://api.github.com/users/jjfalling/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1078616?v=4', u'repos_url': u'https://api.github.com/users/jjfalling/repos', u'received_events_url': u'https://api.github.com/users/jjfalling/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jjfalling/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jjfalling', u'type': u'User', u'id': 1078616, u'followers_url': u'https://api.github.com/users/jjfalling/followers'}</assignee><reporter username="">jjfalling</reporter><labels><label>:Packaging</label><label>feature</label><label>review</label></labels><created>2015-07-30T11:24:27Z</created><updated>2016-06-14T17:14:38Z</updated><resolved>2016-04-22T12:57:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2015-08-04T09:08:13Z" id="127534103">@spinscale @tlrx ping
</comment><comment author="spinscale" created="2015-09-22T11:31:54Z" id="142260401">I only have a SP1 version to tests this against, and that one fails because of the RPM signature issue... no chance of automating this, unless there is a working SP3 vagrant VM
</comment><comment author="electrical" created="2015-09-22T11:43:38Z" id="142262425">@spinscale SP3 will fail as well with the signed RPM.
We will need a second rpm repo with the V3 signature signed repo.
</comment><comment author="salyh" created="2016-03-21T08:10:10Z" id="199166562">+1 (lot of customers with SLES 11 are complaining about start scripts not working properly). The support matrix however lists SLES as a supported OS (talking about ES 2.2)
</comment><comment author="jjfalling" created="2016-03-24T15:41:12Z" id="200892176">I've updated the pr to include all of the changes in master. Should be ready to test. 
</comment><comment author="dakrone" created="2016-04-06T20:55:30Z" id="206563648">@jasontedor you were the last to comment on this, do you want to review this?
</comment><comment author="jasontedor" created="2016-04-06T21:12:42Z" id="206570826">&gt; @jasontedor you were the last to comment on this, do you want to review this?

@dakrone This is not readily reviewable and can not be merged until we have Vagrant tests for SLES.
</comment><comment author="clintongormley" created="2016-04-22T12:57:24Z" id="213418194">We can't support RPM installation on SLES11-SP4 (old version of RPM, see https://github.com/elastic/elasticsearch/issues/17470) so I'm going to close this.
</comment><comment author="TomyLobo" created="2016-06-13T07:39:52Z" id="225508989">Great, after 5 PRs closed in favor of newer PRs, you finally decide to not take any of these simple patches to make elasticsearch work on SLES11.
It's a shitty, old OS, yes, but some people are forced to use it, have you ever thought about that before throwing away someone's work and making people's lives needlessly more difficult?
</comment><comment author="clintongormley" created="2016-06-13T19:36:54Z" id="225684940">@TomyLobo We won't pretend to support something that we can't by merging PRs that may or may not work on systems that we can't test.  On top of that, the tooling we have available in master for building RPMs do not build RPMs that are installable on SLES11.  Sorry, but doing so would make our developers' lives needlessly more difficult, and would potentially do the same for our users who are frustrated when things don't work as expected.
</comment><comment author="TomyLobo" created="2016-06-14T06:33:36Z" id="225791750">No one is asking you to pretend anything.
There's a difference between pretending to support something and making your best effort without giving any guarantees.

Getting SLES is a little hard, I know, but does this even run on any SUSE linux version? openSUSE Leap 42.1, for instance, the latest lts version?
According to this article, you're supposed to use rc_status, not status:
https://en.opensuse.org/openSUSE:Packaging_init_scripts#Status_Functions

http://rpmfind.net/linux/RPM/opensuse/updates/leap/42.1/oss/x86_64/aaa_base-13.2+git20140911.61c1681-13.1.x86_64.html
This package contains openSUSE Leap 42.1's version of rc.status and it looks like this function is still called rc_status, with no alias to status.

So everything here suggests that this RPM won't run on a modern SUSE either and that if you fix it there, the init.d script will work on SLES11 and SLES12 as well.
</comment><comment author="clintongormley" created="2016-06-14T11:48:51Z" id="225857588">&gt; There's a difference between pretending to support something and making your best effort without giving any guarantees.

Unfortunately, users don't see it this way in practice.  We've been bitten by this expectation before. We only support what we can test.  We can't install the RPMs on SLES 11 so we can't support it.

&gt; Getting SLES is a little hard, I know, but does this even run on any SUSE linux version? openSUSE Leap 42.1, for instance, the latest lts version?

We test the RPMs on openSUSE 13 and on SLES 12, see https://elasticsearch-ci.elastic.co/view/All/job/elastic+elasticsearch+master+packaging-tests/

We don't currently test on Leap as nobody has asked for it thus far.  Please open an issue if you would like us to support it.
</comment><comment author="TomyLobo" created="2016-06-14T17:14:38Z" id="225950892">right, i forgot they switched to systemd with opensuse 12.1/sles12, so this script would no longer even be used on those systems.
So this really is a sles11-only problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade HDRHistogram to version 2.1.6.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12554</link><project id="" key="" /><description>This release fixes https://github.com/HdrHistogram/HdrHistogram/pull/68 which
we have been hitting in our CI tests, for instance:
http://build-us-00.elastic.co/job/es_core_master_metal/10567/
</description><key id="98107236">12554</key><summary>Upgrade HDRHistogram to version 2.1.6.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-30T08:02:10Z</created><updated>2015-07-30T08:05:23Z</updated><resolved>2015-07-30T08:05:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-30T08:04:10Z" id="126216418">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Update build release script to reflect latest changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12553</link><project id="" key="" /><description>As the script now deploys to S3 and several things in master have
changed, this script needs to reflect the latest changes
- An unsigned RPM is built by default, so that users of older
  RPM based distros can download and use that RPM by default
- In addition a signed RPM is built, that is used for the repositories
- Paths for the new distributions have been fixed
- The check for the number of jars has been removed, as this is done
  as part of the license checking in `mvn verify`
- Checksum generation has been removed, as this is done as part of the
  mvn build
- Publishing artifacts of S3 has been removed
- Repostitory creation script has been updated
</description><key id="98103687">12553</key><summary>Release: Update build release script to reflect latest changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-30T07:38:33Z</created><updated>2015-08-07T10:06:33Z</updated><resolved>2015-08-05T07:39:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-30T10:43:45Z" id="126270918">I read the change and it makes sense to me. I did not test it though.
</comment><comment author="dadoonet" created="2015-08-05T07:27:09Z" id="127899077">I think it's good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extracting Date and time from timestamp in Elasticsearch and show in Kibana 4 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12552</link><project id="" key="" /><description>I have a "timestamp" field which is of date type format and stores the respective time in thlog files. Example "timestamp": "2015-07-13:09:20:34". My mapping is as follows

{
"sam": {
"mappings": {
"log_entry": {
"properties": {
"browser": {
"type": "string",
"index": "not_analyzed"
},
"timestamp": {
"type": "date",
"format": "yyyy-MM-dd:HH:mm:ss"
}
}
}
}
}
}

I want to extract the the date part as "yyyy-MM-dd" and time as "HH:mm:ss" so that i can plot a hits graph in which the time will be on x axis and there will be line for different days in Kibana 4 showing hits per hour of all the days i have. I am parsing the logs through java parser, hence splitting through logstash is not an option for me. as the code is already being deployed, i dont want to change in the java parser code(which is a very easy option).

I was thinking if there was a way to use query which i can use in filter and plot them in kibana 4
</description><key id="98091440">12552</key><summary>Extracting Date and time from timestamp in Elasticsearch and show in Kibana 4 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sauravmondallive</reporter><labels /><created>2015-07-30T05:48:06Z</created><updated>2015-07-30T15:27:40Z</updated><resolved>2015-07-30T15:27:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-30T15:27:39Z" id="126370299">Hi @sauravmondallive 

The best place to ask these questions is in the forums: http://discuss.elastic.co/

The issues list is for bug reports and feature requests

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take initializing shards into consideration during awareness allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12551</link><project id="" key="" /><description>It makes decision consistent.
Fixes #12522
</description><key id="98081527">12551</key><summary>Take initializing shards into consideration during awareness allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Allocation</label><label>bug</label><label>v1.7.2</label><label>v2.0.0-beta2</label></labels><created>2015-07-30T04:06:53Z</created><updated>2016-03-10T18:13:56Z</updated><resolved>2015-09-11T04:16:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-13T22:23:11Z" id="130865421">This looks good to me, but I think @s1monw should take a look also just to make sure.

Also, not sure about where this should go, @clintongormley what do you think?
</comment><comment author="s1monw" created="2015-09-10T21:35:04Z" id="139385917">left trivial comments - LGTM though
</comment><comment author="s1monw" created="2015-09-10T21:35:29Z" id="139385991">I think we should let it bake on master and 2.x and if it's stable port to 2.0?
</comment><comment author="masaruh" created="2015-09-11T05:54:11Z" id="139460131">Thanks. Updated test names.
Pushed it to master, 2.x and 1.7 for now.
</comment><comment author="s1monw" created="2015-09-11T07:20:44Z" id="139470863">if you push to 1.7 you also need to push to 2.0 otherwise this makes no sense
</comment><comment author="masaruh" created="2015-09-11T08:00:01Z" id="139478091">oh... pushed to 2.0 as well.
</comment><comment author="bleskes" created="2015-09-11T09:33:38Z" id="139499751">Late to the party, but I think there is a problem with this fix. Left a comment on it. The fact we didn't catch it also means our tests are not strong enough. @masaruh let me know if you want to continue here, o.w. I'll pick it up.
</comment><comment author="masaruh" created="2015-09-11T11:47:24Z" id="139524693">@bleskes you are right... Created #13512 (the test fails without the fix).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure all distribution modules have description in pom.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12550</link><project id="" key="" /><description>For example, distribution/rpm does not have this set, so it inherits the value from the parent, and you see this wacky stuff in the rpm metadata:

 [rpm-info] Name        : elasticsearch
 [rpm-info] Version     : 2.0.0
...
 [rpm-info] Summary     : Elasticsearch RPM Distribution
 [rpm-info] Description :
 [rpm-info] Elasticsearch Parent POM
</description><key id="98072873">12550</key><summary>Make sure all distribution modules have description in pom.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>build</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-07-30T02:41:23Z</created><updated>2015-08-10T14:22:03Z</updated><resolved>2015-08-10T14:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add integration tests for tar/deb/rpm packaging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12549</link><project id="" key="" /><description>There are no automated tests for these packages. But we can do various types of verification, at the minimum extract the contents of the package with `dpkg-deb` or `rpm` or `tar`, start elasticsearch, and run integration tests.

These tests are not perfect, they are not installing the stuff on a virtual machine. But they are better than no tests like this, and simple to understand.

There are more cleanups to do here after, but this is really missing.
</description><key id="98072023">12549</key><summary>add integration tests for tar/deb/rpm packaging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-30T02:30:38Z</created><updated>2015-08-12T07:37:06Z</updated><resolved>2015-07-30T13:18:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-30T03:17:22Z" id="126169276">`mvn verify` passes on machines with and without these tools (rpm, dpkg-deb) available.
</comment><comment author="spinscale" created="2015-07-30T08:12:52Z" id="126219318">ran this on OSX, where it worked flawless. However on ubuntu it seems to not run as part of the reactor build (ran from the root directory and skipped unit tests as this was a VM), so running `mvn clean verify -Dskip.unit.tests` resulted in

```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (integ-setup) on project elasticsearch-tar: An Ant BuildException has occured: The following error occurred while executing this line:
[ERROR] /tmp/elasticsearch/distribution/tar/target/dev-tools/ant/integration-tests.xml:198: The following error occurred while executing this line:
[ERROR] /tmp/elasticsearch/distribution/tar/target/dev-tools/ant/integration-tests.xml:127: The following error occurred while executing this line:
[ERROR] /tmp/elasticsearch/distribution/tar/target/dev-tools/ant/integration-tests.xml:53: /tmp/elasticsearch/distribution/tar/target/integ-tests/es.pid doesn't exist
[ERROR] around Ant part ...&lt;ant antfile="/tmp/elasticsearch/distribution/tar/target/dev-tools/ant/integration-tests.xml" target="start-external-cluster-tar"/&gt;... @ 4:134 in /tmp/elasticsearch/distribution/tar/target/antrun/build-main.xml
[ERROR] -&gt; [Help 1]
```

somehow some old .xml file is being reused or I did anything wrong?
</comment><comment author="rmuir" created="2015-07-30T08:45:46Z" id="126228915">Probably a port conflict (es already running on 9200) or something like that. Thats still a TODO of mine, to use a different port in testing. One that won't conflict with what devs do with their machines :)
</comment><comment author="rmuir" created="2015-07-30T08:56:03Z" id="126232187">@spinscale take a look inside .../distribution/tar/target/integ-tests/elasticsearch-xxxx/logs/prepare-release.log or whatever if you really want to confirm. And if you try again, do full mvn clean and check no other ES is running with `jps` first.
</comment><comment author="spinscale" created="2015-07-30T10:20:55Z" id="126262655">all right worked on osx and ubuntu after cleaning up. awesome!

LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch's CPU usage rate lasts high when no search request even</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12548</link><project id="" key="" /><description>The elasticsearch node's CPU usage rate lasts 60% for 3 weeks which  connects to kibana as a search node.In the meantime,there is no any request, so weird.

The hot thread is as follow,

curl XX:9200/_nodes/XX/hot_threads
::[RPRiDE6mRTOZclpQ8hTP9A][XX]]{data=false, master=false}

   100.2% (500.8ms out of 500ms) cpu usage by thread 'elasticsearch[][search][T#1]'
     7/10 snapshots sharing following 12 elements
       java.util.TreeMap.getEntryUsingComparator(TreeMap.java:376)
       java.util.TreeMap.getEntry(TreeMap.java:340)
       java.util.TreeMap.remove(TreeMap.java:595)
       java.util.TreeSet.remove(TreeSet.java:276)
       org.elasticsearch.common.collect.BoundedTreeSet.rebound(BoundedTreeSet.java:60)
       org.elasticsearch.common.collect.BoundedTreeSet.add(BoundedTreeSet.java:47)
       org.elasticsearch.search.facet.terms.strings.InternalStringTermsFacet.reduce(InternalStringTermsFacet.java:214)
       org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:314)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:145)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 11 elements
       java.util.TreeMap.getLastEntry(TreeMap.java:1977)
       java.util.TreeMap.lastKey(TreeMap.java:292)
       java.util.TreeSet.last(TreeSet.java:401)
       org.elasticsearch.common.collect.BoundedTreeSet.rebound(BoundedTreeSet.java:60)
       org.elasticsearch.common.collect.BoundedTreeSet.add(BoundedTreeSet.java:47)
       org.elasticsearch.search.facet.terms.strings.InternalStringTermsFacet.reduce(InternalStringTermsFacet.java:214)
       org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:314)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:145)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

```
0.1% (326.7micros out of 500ms) cpu usage by thread 'elasticsearch[][[transport_server_worker.default]][T#2]{New I/O worker #7}'
 10/10 snapshots sharing following 15 elements
   sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
   sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
   sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
   sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
   sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
   org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
   org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
   org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
   org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   java.lang.Thread.run(Thread.java:745)

0.0% (77.2micros out of 500ms) cpu usage by thread 'elasticsearch[][transport_client_timer][T#1]{Hashed wheel timer #1}'
 10/10 snapshots sharing following 5 elements
   java.lang.Thread.sleep(Native Method)
   org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
   org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
   org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
   java.lang.Thread.run(Thread.java:745)
```
</description><key id="98069726">12548</key><summary>Elasticsearch's CPU usage rate lasts high when no search request even</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Judyccb</reporter><labels /><created>2015-07-30T02:08:40Z</created><updated>2016-01-26T17:09:43Z</updated><resolved>2016-01-26T17:09:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-07-30T02:09:11Z" id="126159910">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment><comment author="Judyccb" created="2015-07-30T02:36:16Z" id="126163673">The URL you give can not be open in our area,any other way to discuss this problem?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation over indexed list is out of order and deduplicated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12547</link><project id="" key="" /><description>It looks like the actual document data indexed for the field "nums" is a list of 4 floats in descending order, but when the aggregation is run to collect the data, only 3 floats are returned and they are in ascending order.

``` bash
#! /bin/sh

# Delete the index
curl -XDELETE "http://localhost:9200/test"

# Insert a document with one field -- a list of floats
curl -XPUT "http://localhost:9200/test/doc/1?refresh=true" -d '{
  "nums": [5.0, 3.5, 3.5, 1.0]
}'

# Expected:
# aggregations.foobar.value == [5.0, 3.5, 3.5, 1.0]

# Actual:
# aggregations.foobar.value == [1.0, 3.5, 5.0]
curl -XGET "http://localhost:9200/test/_search" -d '{
    "query": {
        "match_all": {}
    },
    "filter": {},
    "aggregations": {
        "foobar": {
            "scripted_metric": {
                "init_script": "_agg[\"numList\"] = []",
                "map_script": "if (doc != null) { _agg.numList.add(doc[\"nums\"].getValues()) }",
                "combine_script": "return _agg.numList",
                "reduce_script": "return _aggs.flatten()"
            }
        }
    }
}'

```
</description><key id="98049608">12547</key><summary>Aggregation over indexed list is out of order and deduplicated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">MattFerraro</reporter><labels /><created>2015-07-29T23:21:44Z</created><updated>2015-07-30T17:39:13Z</updated><resolved>2015-07-30T01:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MattFerraro" created="2015-07-29T23:59:04Z" id="126132287">I've also opened this up for discussion on [stackoverflow](http://stackoverflow.com/questions/31712933/elasticsearch-aggregation-over-indexed-list-is-out-of-order-and-deduplicated)
</comment><comment author="jasontedor" created="2015-07-30T01:51:57Z" id="126156571">This is due to the way that [multivalued fields](https://www.elastic.co/guide/en/elasticsearch/guide/current/complex-core-fields.html#_multivalue_fields) are indexed. Since you are attempting to access the input values (including duplicates), you'll want to use the `_source` field in your aggregation.
</comment><comment author="MattFerraro" created="2015-07-30T07:32:45Z" id="126209457">Ah I see, thanks for the info.
</comment><comment author="clintongormley" created="2015-07-30T15:18:56Z" id="126368227">@MattFerraro @jasontedor using `_source` for aggregations will be very very very slow. I think you'll have to do something like using a separate field for each number (lousy solution I know).

An alternative would be to index ["val1_5.0", "val2_3.5", "val3_3.5", etc]. Another poor solution.  

Not sure if your script is representative of what you really want to do, but you could use a `nested` doc for each value, and use a `nested` aggregation to access them.
</comment><comment author="MattFerraro" created="2015-07-30T17:39:03Z" id="126413534">Yep, after trying with `_source` I found that the data appeared as expected, but the aggregation took so long that it became unusable. I suppose that makes a lot of sense because we're accessing non-indexed data.

We went ahead and indexed the exact floats we wanted as nested fields in a separate spot and now access them as `doc['nums.num1']` and `docs['nums.num2']`, and it is both quick and correct!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_only_nodes against a node attribute not shuffling requests between nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12546</link><project id="" key="" /><description>Reproducible using 1.7.1 and 5.0 Alpha 2.
https://gist.github.com/ppf2/a5a04d94cc7e0fc3859e

Not sure if `_only_nodes:pod:A` is the right way to specify the value, but it certainly complains about _only_nodes:bogus_name:A and _only_nodes:pod:bogus_value.  So far, not seeing it shuffle between the nodes sharing the same node attribute.
</description><key id="98041063">12546</key><summary>_only_nodes against a node attribute not shuffling requests between nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label><label>v5.0.0-alpha3</label></labels><created>2015-07-29T22:16:40Z</created><updated>2016-05-23T08:03:44Z</updated><resolved>2016-05-23T08:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nirmalc" created="2015-07-30T00:08:23Z" id="126133269">is it because explain() just picksup shardIterator.next() and does operation on first shard ? can you test with _search , set slow_query threshold to 1 ms or so and see if the query hits all nodes
</comment><comment author="ppf2" created="2015-07-30T00:28:20Z" id="126137784">GET only_node_test/_search?explain=true
shows that it is hitting all shards (and it is populating slowlog for all nodes).

With _only_nodes:pod:A and slowlog at 0ms, only 1 "A" node is getting slowlog entries so it appears to be consistent with the explain output.
</comment><comment author="ppf2" created="2016-05-10T17:35:31Z" id="218232493">Can we also fix this in the 2.x branch?  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to perform a sorted search across indices that don't contain the sort field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12545</link><project id="" key="" /><description>I have a daily index, e.g. "foo-YYYY.MM.DD".  Occasionally - but not every day - I have an event of type "bar" that has field called "enddate".  I would like to query for these events, and sort by the "enddate" field.

Unfortunately, elasticsearch is checking the mapping before it checks the data.  When the daily index contains no rows of this type, the "enddate" field doesn't appear in the mapping, and the query fails:

```
Parse Failure [No mapping found for [enddate] in order to sort on]]
```

The work-arounds seem to be either to define a fixed mapping, or insert dummy records every day.  Both are bad.
</description><key id="98033846">12545</key><summary>Unable to perform a sorted search across indices that don't contain the sort field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">webmstr</reporter><labels /><created>2015-07-29T21:33:34Z</created><updated>2015-08-05T09:35:27Z</updated><resolved>2015-07-30T15:09:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-30T15:09:16Z" id="126365429">Alternatively, you can read the documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_ignoring_unmapped_fields
</comment><comment author="webmstr" created="2015-07-30T15:44:01Z" id="126374433">Thanks for the pointer.  How about referencing the exact error message on the doc page so that mortals can find it?
</comment><comment author="clintongormley" created="2015-08-05T09:35:27Z" id="127934963">@webmstr would you like to send a docs PR to make the improvement?

Just click the "edit" button on the docs page and you can edit the source directly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default position_offset_gap to 100</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12544</link><project id="" key="" /><description>This is much more fiddly than it looks because of the way position_offset_gap
is applied in StringFieldMapper. Instead of setting the default to 10 its
simpler to make sure that all the analyzers default to 10 and that
StringFieldMapper doesn't override the default unless the user specifies
something different.

Closes #7268
</description><key id="98030305">12544</key><summary>Default position_offset_gap to 100</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>breaking</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-07-29T21:11:59Z</created><updated>2015-11-22T10:15:20Z</updated><resolved>2015-08-25T20:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-29T21:12:25Z" id="126096967">WIP because it needs tests and more testing. This is way way harder then it ought to be.
</comment><comment author="nik9000" created="2015-07-29T21:12:45Z" id="126097029">Oh and it needs documentation changes and breaking changes documentation.
</comment><comment author="rmuir" created="2015-07-30T00:01:45Z" id="126132571">Which one is it? the position gap or the offset gap? Lucene has both, and they both have different meanings. Changing the former to this value makes sense, the latter will break many things.
</comment><comment author="rmuir" created="2015-07-30T00:02:11Z" id="126132622">And please, please, we have to rename this, because its totally meaningless. positions and offsets are separate things...
</comment><comment author="nik9000" created="2015-07-30T00:24:50Z" id="126135594">I understand the rename. It's the positions.
On Jul 29, 2015 8:02 PM, "Robert Muir" notifications@github.com wrote:

&gt; And please, please, we have to rename this, because its totally
&gt; meaningless. positions and offsets are separate things...
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12544#issuecomment-126132622
&gt; .
</comment><comment author="nik9000" created="2015-08-03T13:44:37Z" id="127238900">Does anyone have an opinion on what would be ok behavior from a backwards compatibility standpoint?
</comment><comment author="nik9000" created="2015-08-03T18:01:04Z" id="127352608">&gt; Does anyone have an opinion on what would be ok behavior from a backwards compatibility standpoint?

Talked with @dakrone and we decided that the most correct thing as to make `position_offset_gap` be immutable once the index is created even if it isn't set. So indexes created before 2.0.0-beta1 will always have a default `position_offset_gap` of `0` regardless of which version of elasticsearch operates on them. Indexes created by versions of Elasticsearch on or after 2.0.0-beta1 will default to `10` like the issue says they should.
</comment><comment author="nik9000" created="2015-08-03T18:14:34Z" id="127357954">I believe this is ready for review. @jpountz - its more fiddly than I expected at first so it might be worth a review from you or someone who's pretty experienced with mapping.

@mute - I _think_ this is the right way to do #12538. Its much more complicated than it looked and certainly wasn't really low hanging fruit.
</comment><comment author="nik9000" created="2015-08-11T14:41:35Z" id="129911346">I'd love this in 2.0.0 at some point. It'll need a review soon if it is going to.
</comment><comment author="clintongormley" created="2015-08-11T16:20:40Z" id="129953312">I agree that 10 is small, and could quite easily overlap with a typical slop value.  Personally i'd go for eg 100
</comment><comment author="nik9000" created="2015-08-11T16:30:19Z" id="129957165">Ok - I'll make these changes. I don't think I'll have time today because I'm in class and can't really concentrate. But tonight or tomorrow "morning".
</comment><comment author="mikemccand" created="2015-08-11T16:39:22Z" id="129961526">Thanks @nik9000 this is a great change (so prox aware queries never match across 2 values of a multi-valued field), I just left some minor comments.

Can you open a follow-on issue to rename `positionOffsetGap`?  Need not block this good change, but this is extremely confusing :)  As @rmuir said, position gap and offset gap are wildly different things!
</comment><comment author="nik9000" created="2015-08-11T16:41:43Z" id="129962742">&gt; Can you open a follow-on issue to rename positionOffsetGap? Need not block this good change, but this is extremely confusing :) As @rmuir said, position gap and offset gap are wildly different things!

#12562
</comment><comment author="nik9000" created="2015-08-24T16:25:32Z" id="134288309">Swapped 2.0 for 2.1 because we're too late in the 2.0 release cycle to get this merged there.

I'm going to pick this one back up in a few minutes and have another read through. I'll see if I can address that last open comment of @mikemccand and rebase. And I'll change all the 2.0s into 2.1s in the code.
</comment><comment author="nik9000" created="2015-08-24T19:53:18Z" id="134356846">@mikemccand  - would you mind having another look at this? Its now ready for review again based on 2.1.
</comment><comment author="mikemccand" created="2015-08-24T22:37:01Z" id="134401122">LGTM, thanks @nik9000!
</comment><comment author="s1monw" created="2015-08-25T08:25:14Z" id="134521276">@nik9000 this looks good I think this should go into 2.0 though
</comment><comment author="nik9000" created="2015-08-25T14:59:37Z" id="134613676">&gt; @nik9000 this looks good I think this should go into 2.0 though

Ok - I'll merge to 2.1 now and backport it. I'm not super comfortable sticking it in 2.0 but I'll try because you want it in there.
</comment><comment author="nik9000" created="2015-08-25T18:29:48Z" id="134694542">I merged this into master but github doesn't recognize it - probably because I merged it locally and pushed. That failed so I rebased onto elastic's master and squashed. At this point github's must have lost any connection to the patch.

I'll leave this open to work the backport to 2.0. I'll close it once I've merged there.
</comment><comment author="nik9000" created="2015-08-25T19:57:12Z" id="134720391">And merged to 2.0. For my last trick on this pull request I'll fix master so that it has the same version range checks as 2.0. And then finally, finally, this is done.
</comment><comment author="nik9000" created="2015-08-25T20:05:25Z" id="134722032">&gt; And merged to 2.0. For my last trick on this pull request I'll fix master so that it has the same version range checks as 2.0. And then finally, finally, this is done.

Scratch that, I'll send that as another pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop the `action.get.realtime` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12543</link><project id="" key="" /><description>The get, multi get, termvector and multi termvector apis have a `realtime` option. If the realtime option isn't specified then default depends on the api.

The get and multi get apis default is based on which has been defined in the `action.get.realtime` setting. The default of the `action.get.realtime` is `true`. But the term vector apis don't have a setting like this and the `realtime` option just default to `true`.

I don't think this setting is actually used a lot and I like to remove it. The main reason for that this cleans up to code and simplifies it. Secondly the behaviour is now inconsistent between apis. Also for the `realtime` and similar options, I think being able to set on a per request level is enough, there no need to have a node level default.
</description><key id="98017647">12543</key><summary>Drop the `action.get.realtime` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Settings</label><label>breaking</label><label>v5.0.0-alpha2</label></labels><created>2015-07-29T19:58:55Z</created><updated>2017-06-28T20:20:32Z</updated><resolved>2016-04-19T15:20:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-07-29T21:48:10Z" id="126106718">+1 The node level default seems to be overkill.
</comment><comment author="nik9000" created="2015-07-29T22:05:09Z" id="126110250">Fine by me.
On Jul 29, 2015 5:48 PM, "Alex Ksikes" notifications@github.com wrote:

&gt; +1 The node level default seems to be overkill.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12543#issuecomment-126106718
&gt; .
</comment><comment author="clintongormley" created="2016-01-26T17:08:16Z" id="175120776">Agreed - let's remove this setting
</comment><comment author="clintongormley" created="2016-04-19T12:33:40Z" id="211902483">@martijnvg you want to pick this up for 5.0?
</comment><comment author="javanna" created="2016-04-20T07:44:57Z" id="212304672">Should this still be labelled deprecation given that the corresponding PR removes the setting? Did we deprecate the setting in 2.x or have we decided that deprecation is not needed?
</comment><comment author="martijnvg" created="2016-04-20T08:05:38Z" id="212316056">@javanna I think this setting is not needed. I think the deprecation label should be removed.
</comment><comment author="javanna" created="2016-04-20T08:15:24Z" id="212318248">I agree that it is not needed, wondering if there are users using it (hopefully not) and what the upgrade path is for them. A node would not startup with this setting in 5.0 I believe, which forces users to remove the non supported setting. Is that correct? 
</comment><comment author="martijnvg" created="2016-04-20T08:17:36Z" id="212318731">Yes, node startup would fail with a error message that it is a unknown setting. It is good that this will bubble up quickly.
</comment><comment author="wedneyyuri" created="2017-06-28T20:20:32Z" id="311777387">Hi @martijnvg, there are plans to change the default behavior of `realtime` option? 

In our environment we are setting the refresh interval to 10 minutes to prevent cache invalidation and we can't guarantee passing the realtime option on every request.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Blocking writes to slf4j causing shards to be stuck in initializing state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12542</link><project id="" key="" /><description>Two cases so far have occurred on 1.7 after making cluster settings to exclude a node by ip:
- A node is excluded; shards enter a migrating state with new target hosts selected. The state never advances.
- A node is excluded, then a node that the shard is migrating to is added to the exclusion list. The double-migrated shard gets stuck on the 2nd node and never complete migration.
</description><key id="98003306">12542</key><summary>Blocking writes to slf4j causing shards to be stuck in initializing state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">autocracy</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-07-29T18:45:34Z</created><updated>2015-10-16T09:39:42Z</updated><resolved>2015-10-16T09:39:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-30T14:55:01Z" id="126358597">Hi @autocracy 

Please could you spell out the scenarios with a few more details?  Would help us to understand what happened when.
</comment><comment author="autocracy" created="2015-07-30T16:05:42Z" id="126383958">After some digging yesterday, it appears that the root cause for this is the slf4j logger in SocketAppender mode locking up the threads of ES because of blocked writes.

The lack of tooling to force this into a fixed mode is rough, but it's the same issue encountered with Cassandra when using remote logging. I think nobody expects logging to block like that.
</comment><comment author="clintongormley" created="2015-10-16T09:39:42Z" id="148668304">We've just discussed this in FixItFriday and agree that logging across the network is a really bad idea for a busy Elasticsearch server, and not a configuration that we want to support.

There are plenty of tools available which can tail log files and ship them to a central server, including Logstash and [Filebeat](https://github.com/elastic/filebeat)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update TESTING doc to use run.sh script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12541</link><project id="" key="" /><description /><key id="98002675">12541</key><summary>update TESTING doc to use run.sh script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">metadave</reporter><labels /><created>2015-07-29T18:41:59Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-29T18:42:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-29T18:42:22Z" id="126054403">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a run.sh to run from current source code with debugger</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12540</link><project id="" key="" /><description>The maven magic needed for this is now heavier
</description><key id="97998944">12540</key><summary>Add a run.sh to run from current source code with debugger</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-29T18:22:45Z</created><updated>2015-07-30T13:59:49Z</updated><resolved>2015-07-29T18:38:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="metadave" created="2015-07-29T18:31:47Z" id="126051106">running `mvn clean` followed by `./run.sh` yields: https://gist.githubusercontent.com/metadave/de196bbb026211c69ed9/raw/18d52d8793abc99052fb0b4b4cd18a067dd2b1be/gistfile1.txt
</comment><comment author="dakrone" created="2015-07-29T18:32:06Z" id="126051186">This isn't working for me: http://p.draines.com/14381952224210898d64c.txt
</comment><comment author="rmuir" created="2015-07-29T18:33:48Z" id="126051611">Because your dev-tools was out of date (no mvn install). See my commit. I made execution depend on all poms and dev-tools in case that logic changes.
</comment><comment author="dakrone" created="2015-07-29T18:35:33Z" id="126052162">Great, works now, LGTM
</comment><comment author="rmuir" created="2015-07-29T18:37:44Z" id="126053110">I was trying to make it fast by cheating before... but the maven shading is really the slowest part anyway, better to be safer... and that shading can be moved out (it needs to, since its an artifact untested by anything).
</comment><comment author="metadave" created="2015-07-29T18:37:58Z" id="126053206">works for me now as well, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't search a number longer than 16 charachters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12539</link><project id="" key="" /><description>We have a field in elasticsearch that is about 19 to 21 characters (All numbers).  If we open a document and click on the magnifying glass for that field.  We get 0 results.  If we copy the first 15 digits then place a \* at the end.  We can find all results matching the first 15 digits.  I've tried this as a "long" and as a "string"  with and without "not_analyzed" in my templates.  With this as a "long" I can not even search using the 15 digits and a *.  As these are customer account numbers, I can't really change them.  Can someone let me know if I'm doing something wrong?  Or if there is a template I should be using to capture this.  
</description><key id="97986501">12539</key><summary>Can't search a number longer than 16 charachters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vehyla</reporter><labels /><created>2015-07-29T17:24:22Z</created><updated>2015-07-30T13:59:11Z</updated><resolved>2015-07-30T13:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-29T17:26:22Z" id="126027101">&gt; Or if there is a template I should be using to capture this.

Its best if you can post an example of it not working that anyone can recreate with curl.  This describes how to do it: https://www.elastic.co/help
</comment><comment author="Vehyla" created="2015-07-29T17:29:57Z" id="126028953">Sadly, these are customer account numbers.  So a search would probably need to have them indexed.  I can show my template though?

This is currently set to "long", but as mentioned we have tried "string" as well.
          "payLoad" : {
            "properties" : {
              "createTime" : {
                "type" : "string"
              },
              "accountId" : {
                "index" : "not_analyzed",
                "type" : "long"
              },
              "action" : {
                "type" : "string"
              },
              "revisionId" : {
                "type" : "string"
              }
            }
</comment><comment author="nik9000" created="2015-07-29T17:36:41Z" id="126030457">&gt; Sadly, these are customer account numbers. So a search would probably need to have them indexed. I can show my template though?

You don't have to recreate the problem with your data - just recreate it with similar enough data that it still happens. Also you might want to put code in three back ticks so the formatting sticks.
</comment><comment author="clintongormley" created="2015-07-30T13:59:11Z" id="126337086">Hi @Vehyla 

Elasticsearch doesn't support BigIntegers.  You should map these fields as `strings`.  You'll find the forum http://discuss.elastic.co/ is a good source of help on questions like these

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updating default position_offset_gap to 10.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12538</link><project id="" key="" /><description>Addresses #7268.

This commit includes the suggestion to update the default in CustomAnalyzerProvider as well. It looks like the position_offset_gap setting was added there first and then the default was added subsequently in StringFieldMapper (#1812 and #1813), so there might have just been an oversight in sharing the default between them.
</description><key id="97983340">12538</key><summary>Updating default position_offset_gap to 10.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">mute</reporter><labels><label>:Mapping</label></labels><created>2015-07-29T17:09:24Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-30T17:24:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-29T18:13:39Z" id="126046334">Ok - I did a little digging into the CustomAnalyzer bit. So far as I can tell the values set in that don't get used - the values used in the mapping do.

So, in that case, this LGTM.

I'd love it if you could add a test case that shows the default has changed.

I'l pull this locally and verify that the tests pass.
</comment><comment author="mute" created="2015-07-29T18:44:50Z" id="126054964">I'll work on adding a test case today -- thanks for the really quick feedback!
</comment><comment author="nik9000" created="2015-07-29T18:54:31Z" id="126057907">Ok - so you may want to wait for a bit. It looks like a bazillion test cases are failing on this change and its not your fault. There are some magic `if (positionOffsetGap &gt; 0)` style checks sitting around that are blowing up good. I'm debugging them now. I'll see if I can send a pull request to the branch that you're using for this pull request when I have a solution.
</comment><comment author="mute" created="2015-07-29T19:57:09Z" id="126078261">Your fix for the test looks good to me -- regarding the piece about non-custom analyzers still defaulting to 0, it sounds like there will still be a change necessary there too, right?

I'll probably need to look to see where that default is coming from.
</comment><comment author="nik9000" created="2015-07-29T20:01:10Z" id="126079850">There is lots of funkiness about this. I have a work around but I'm not sure its good. Its better than what I sent you in that pull request. Fewer test failures....
</comment><comment author="nik9000" created="2015-07-29T21:16:37Z" id="126097813">@mute so I took another crack at it after having made a mess of your pull request: #12544.  I think that does it in a less breaky way. That fixes the non-customized analyzers too.

Don't get discouraged! That was way harder to fix implement than it should have been. Do you want to send sort of pickup where you left off and write the tests for this? You could send a pull request to my branch that that other pull request is based on. I just don't want to discourage a contributor by taking over!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add RealtimeRequest marker interface to group realtime operations together</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12537</link><project id="" key="" /><description>The following classes will implement it: GetRequest, MultiGetRequest, TermVectorsRequest and MultiTermVectorsRequest..
</description><key id="97980083">12537</key><summary>Add RealtimeRequest marker interface to group realtime operations together</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-29T16:53:13Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-29T20:00:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-07-29T16:54:41Z" id="126016389">LGTM
</comment><comment author="nik9000" created="2015-07-29T16:56:47Z" id="126016809">Maybe add the `@Override` annotation on the methods that implement that interface?
</comment><comment author="martijnvg" created="2015-07-29T18:42:47Z" id="126054494">@nik9000 I'll add the `@Override` annotation.
</comment><comment author="martijnvg" created="2015-07-29T19:31:18Z" id="126068613">@nik9000 I updated the PR. About the realtime default, I think the `realtime` field should become a primitive boolean, it should just default to `true` everywhere (it is doing this already `action.get.realtime` defaults to `true`) and we should `remove` the `action.get.realtime` defaulting mechanism. But I think this should be done in a different change.
</comment><comment author="nik9000" created="2015-07-29T19:41:15Z" id="126074380">Fair enough. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging: mvn install renames artifacts when copying</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12536</link><project id="" key="" /><description>From a recent `mvn install` run

```
[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ elasticsearch-deb ---
[INFO] Installing /Users/alr/devel/elasticsearch/distribution/deb/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.deb to /Users/alr/.m2/repository/org/elasticsearch/distribution/elasticsearch-deb/2.0.0-beta1-SNAPSHOT/elasticsearch-deb-2.0.0-beta1-SNAPSHOT.deb
[INFO] Installing /Users/alr/devel/elasticsearch/distribution/deb/pom.xml to /Users/alr/.m2/repository/org/elasticsearch/distribution/elasticsearch-deb/2.0.0-beta1-SNAPSHOT/elasticsearch-deb-2.0.0-beta1-SNAPSHOT.pom
```

The artifact is renamed back to `elasticsearch-deb-2.0.0-beta1-SNAPSHOT.deb`, which happens for all the packages. Not sure if this also happens with `mvn deploy`, if so, this needs to be fixed.
</description><key id="97975880">12536</key><summary>Packaging: mvn install renames artifacts when copying</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>v2.0.0-beta1</label></labels><created>2015-07-29T16:33:31Z</created><updated>2015-08-04T14:26:38Z</updated><resolved>2015-08-04T14:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-30T00:12:29Z" id="126133732">In my source PR I wrote :

```
            &lt;configuration&gt;
                &lt;!-- By default it should generates target/${artifactId}_${version}.deb but we get elasticsearch_2.0.0~SNAPSHOT_all.deb --&gt;
                &lt;deb&gt;${project.build.directory}/releases/elasticsearch-${project.version}.deb&lt;/deb&gt;
                &lt;controlDir&gt;${project.build.directory}/generated-packaging/deb/scripts&lt;/controlDir&gt;
            &lt;/configuration&gt;
```

HTH
</comment><comment author="spinscale" created="2015-07-30T12:09:36Z" id="126300183">so, does this mean we can close this as `mvn deploy` is doing the right things or do we need to fix things?
</comment><comment author="dadoonet" created="2015-07-30T13:04:09Z" id="126315835">Sounds like I misread your issue at first.

I think that mvn deploy would deploy using the same name as the one you have in .m2.

You can try it. It will simply deploy a SNAPSHOT version which does not hurt. 

IMO it would be better to rename elasticsearch artifact to elasticsearch-core and for distribution modules use elasticsearch as artifact id. So all defaults should be good.

Not sure what would be the impact of this.

IIRC I tried to use elasticsearch as the artifactId in distribution modules but it was not possible without renaming core module (circular dependency). But I'm AFK and can't really test this ATM. 
</comment><comment author="rmuir" created="2015-08-04T09:09:53Z" id="127534690">Removed blocker. The renames done with finalName aren't going to have any impact on the maven repository... this is just how maven works.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve toString on EsThreadPoolExecutor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12535</link><project id="" key="" /><description>Improving the toString allows for nicer error reporting. Also cleaned up
the way that EsRejectedExecutionException notices that it was rejected
from a shutdown thread pool. I left javadocs about how its not 100% correct
but good enough for most uses.

The improved toString on EsThreadPoolExecutor mean every one of them needs
a name. In most cases the name to use is obvious. In tests I use the name
of the test method and in real thread pools I use the name of the thread
pool. In non-ThreadPool executors I use the thread's name.

Closes #9732
</description><key id="97971034">12535</key><summary>Improve toString on EsThreadPoolExecutor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-29T16:07:19Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-08-05T18:29:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-29T16:28:15Z" id="126008015">Ping @jpountz for review.
</comment><comment author="nik9000" created="2015-07-30T18:24:01Z" id="126424533">@clintongormley why the change in pull request name? I'm still feeling my way around the naming rules.

Also, @jpountz its ready for you to review again.
</comment><comment author="clintongormley" created="2015-08-05T10:21:43Z" id="127949367">@nik9000 the `:Core` label already provides the `Core:` heading in the release notes, so adding it to the subject (while useful in a commit message) just leads to duplication in the release notes.
</comment><comment author="jpountz" created="2015-08-05T10:29:20Z" id="127950669">LGTM
</comment><comment author="nik9000" created="2015-08-05T17:58:19Z" id="128091763">Rebasing on master and rerunning tests. I'd love another reviewer.
</comment><comment author="dakrone" created="2015-08-05T18:00:52Z" id="128092306">LGTM
</comment><comment author="nik9000" created="2015-08-05T18:01:19Z" id="128092448">@dakrone Thanks for the review!
</comment><comment author="nik9000" created="2015-08-05T18:29:51Z" id="128100679">Rebased and merging!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add _cat/nodeattrs API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12534</link><project id="" key="" /><description>This provides a `_cat/nodeattrs` API call, which presents custom node attributes in a denormalized table.

This was added in addition to the `_cat/nodes` endpoint as we don't want to create dynamic columns for each user defined attribute in `_cat/nodes`.

Closes #8000

cc @drewr 
</description><key id="97965074">12534</key><summary>Add _cat/nodeattrs API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">metadave</reporter><labels><label>:CAT API</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-07-29T15:40:43Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-29T21:01:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-29T15:47:35Z" id="125995326">Looks good to me. I'm surprised it takes three requests to get that but I don't really mind it.
</comment><comment author="drewr" created="2015-07-29T15:55:30Z" id="125997579">Other than small comment, LGTM, thanks @metadave!

@nik9000 That's the beauty of the cat API, it slices across the stats and info APIs internally so you don't have to! :joy_cat: 
</comment><comment author="metadave" created="2015-07-29T17:02:40Z" id="126019274">Updated with @drewr's suggestions. How do I go about getting a doc for http://www.elastic.co/guide/en/elasticsearch/reference/master/cat-nodeattrs.html ?
</comment><comment author="nik9000" created="2015-07-29T17:07:21Z" id="126020371">&gt; How do I go about getting a doc for http://www.elastic.co/guide/en/elasticsearch/reference/master/cat-nodeattrs.html ?

Make one in  docs/reference/cat I believe.
</comment><comment author="metadave" created="2015-07-29T17:08:13Z" id="126020567">thanks @nik9000 
</comment><comment author="nik9000" created="2015-07-29T17:08:37Z" id="126020725">Do it as part of this pull request if you can! Its better to commit the docs at the same time.

This has documentation on how to build them: https://github.com/elastic/docs
</comment><comment author="metadave" created="2015-07-29T17:51:15Z" id="126034712">Updated w/ asciidoc
</comment><comment author="nik9000" created="2015-07-29T18:08:32Z" id="126044361">Do you need to add a link in the cat.asciidoc?
</comment><comment author="metadave" created="2015-07-29T18:22:13Z" id="126048867">yes, indeed I do.
</comment><comment author="metadave" created="2015-07-29T19:51:20Z" id="126076469">shall I squash and merge?
</comment><comment author="dakrone" created="2015-07-29T20:04:44Z" id="126081000">Yep, LGTM, can you also add the `v2.0.0` tag?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for bulk delete operation in snapshot repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12533</link><project id="" key="" /><description>Currently when we delete files belonging to deleted snapshots we issue one delete command to underlying snapshot store at a time. Some repositories can benefit from bulk deletes of multiple files. See  elastic/elasticsearch-cloud-azure#66 for example.
</description><key id="97961765">12533</key><summary>Add support for bulk delete operation in snapshot repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-07-29T15:24:46Z</created><updated>2015-08-04T21:22:50Z</updated><resolved>2015-08-04T21:22:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-29T23:57:14Z" id="126132086">+1 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid extra reroutes of delayed shards in RoutingService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12532</link><project id="" key="" /><description>In order to avoid extra reroutes, `RoutingService` should avoid
scheduling a reroute of any shards where the delay is negative. To make
sure that we don't encounter a race condition between the
GatewayAllocator thinking a shard is delayed and RoutingService thinking
it is not, the GatewayAllocator will update the RoutingService with the
last time it checked in order to use a consistent "view" of the delay.

Resolves #12456
Relates to #12515 and #12456

This is a PR against 1.7 and will be forward-ported
</description><key id="97959408">12532</key><summary>Avoid extra reroutes of delayed shards in RoutingService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.7.2</label></labels><created>2015-07-29T15:13:14Z</created><updated>2015-08-07T13:42:45Z</updated><resolved>2015-08-05T18:37:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-29T15:13:21Z" id="125984387">@kimchy can you take a look?
</comment><comment author="kimchy" created="2015-08-04T14:53:39Z" id="127640461">left a few comments, LGTM otherwise.

For master forward patching, the code changed a bit, so it will be slightly different. I think we should in GatewayAllocator take a timestamp, and pass it to `ReplicaAllocator#allocateUnassigned`, and have a simplified one `allocateUnassigned` that passes `System.currentTimeInMillis`.
</comment><comment author="dakrone" created="2015-08-05T18:38:55Z" id="128103346">Merged this, but for the 2.0 forward port I am going to open another PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent usage of ValueFormatter.DateTime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12531</link><project id="" key="" /><description>ValueFormatter.DateTime is mutable so that we can set the timezone to use in order to display date/times (DateHistogramParser calls this method). But we also have the ValueFormatter.DateTime.DEFAULT constant, so if this constant is used and there are several concurrent requests, then they might invalidate each other's timezone parameter.
</description><key id="97941908">12531</key><summary>Inconsistent usage of ValueFormatter.DateTime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-07-29T13:56:55Z</created><updated>2015-08-10T13:05:39Z</updated><resolved>2015-08-10T13:05:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Build: Remove profile to create attached RPM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12530</link><project id="" key="" /><description>This is no longer necessary, as the RPM is now built as
a primary artifact, so no need to deploy it as a secondary
artifact anymore.

Closes #12529

Docs for this at http://www.mojohaus.org/rpm-maven-plugin/usage.html
</description><key id="97941669">12530</key><summary>Build: Remove profile to create attached RPM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-29T13:55:49Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-29T13:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-29T13:57:36Z" id="125960449">Looks good, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm distribution generates artifact errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12529</link><project id="" key="" /><description>See http://build-us-00.elastic.co/job/es_core_master_regression/2919/console

[ERROR] Failed to execute goal org.codehaus.mojo:rpm-maven-plugin:2.1.3:attached-rpm (attach-rpm) on project elasticsearch-rpm: Execution attach-rpm of goal org.codehaus.mojo:rpm-maven-plugin:2.1.3:attached-rpm failed: For artifact {org.elasticsearch.distribution:elasticsearch-rpm:2.0.0-beta1-SNAPSHOT:rpm}: An attached artifact must have a different ID than its corresponding main artifact. -&gt; 

Looks like its angry that its elasticsearch and not elasticsearch-rpm?
</description><key id="97930664">12529</key><summary>rpm distribution generates artifact errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>v2.0.0-beta1</label></labels><created>2015-07-29T13:04:42Z</created><updated>2015-07-29T13:58:49Z</updated><resolved>2015-07-29T13:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-29T13:34:47Z" id="125954357">this is triggered by `mvn -Dpackage.rpm`, which in turn triggers a maven profile. As the RPM is now a primary artifact, there is no need to actually have the `release` profile in `distribution/rpm/pom.xml` anymore, that creates the RPM as a secondary artifact, according to http://www.mojohaus.org/rpm-maven-plugin/usage.html

will create a PR to remove that profile
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>re-enable license checker for RPM builds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12528</link><project id="" key="" /><description>The license checker has some issues on the RPMs... not really sure what is happening.

Snapshot publishing has been broken all week (http://build-us-00.elastic.co/job/es_core_master_regression/), so I will disable it temporarily, and leave this issue as a blocker to fix.

I hate to disable it, but not having any snapshots is worse.
</description><key id="97925679">12528</key><summary>re-enable license checker for RPM builds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-07-29T12:35:20Z</created><updated>2015-08-04T12:18:33Z</updated><resolved>2015-08-04T12:18:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-29T12:40:59Z" id="125938305">It seems this is the root cause:

[exec] sh: 1: rpm2cpio.pl: not found
</comment><comment author="dakrone" created="2015-07-29T12:42:27Z" id="125938600">It does find `rpm2cpio` though, and tries to extract the RPM, then fails with this:

```
     [exec] cpio: premature end of archive
```

So either `rpm2cpio` is not working correctly, `cpio` is not working correctly, or the RPM is not being generated correctly.
</comment><comment author="dakrone" created="2015-07-29T12:43:28Z" id="125938774">Worth mentioning from the `cpio` man page:

```
__WARNING__
       The cpio utility is considered LEGACY based on POSIX specification.  Users are encouraged to use other archiving tools for archive creation.
```

We should use something that does not have a big warning at the top of the man page about it being a "legacy" tool.
</comment><comment author="rmuir" created="2015-07-29T12:43:46Z" id="125938835">I also see errors in RPM generation:

[INFO] error: Macro %__isa_name has empty body
[INFO] error: Macro %__isa_bits has empty body
</comment><comment author="rmuir" created="2015-07-29T12:45:36Z" id="125939156">&gt; We should use something that does not have a big warning at the top of the man page about it being a "legacy" tool.

Can we just extract with 'rpm' itself? This is already necessary to build the thing in the first place right?

I don't have RPM on my system, but looking at the manpage it seems it has the necessary options to "install" to a simple folder:

--dbpath DIRECTORY
    Use the database in DIRECTORY rather than the default path /var/lib/rpm 
--root DIRECTORY
    Use the file system tree rooted at DIRECTORY for all operations. Note that this means the database within DIRECTORY will be used for dependency checks and any scriptlet(s) (e.g. %post if installing, or %prep if building, a package) will be run after a chroot(2) to DIRECTORY. 
</comment><comment author="dakrone" created="2015-07-29T12:46:45Z" id="125939347">Looks like the macro error may be an upstream bug in RPM that was fixed: https://groups.google.com/forum/#!topic/linux.debian.bugs.dist/DeTpw_QtxW4 patch: http://rpm.org/gitweb?p=rpm.git;a=commitdiff;h=90dd51743200055f30d9e0e0337173118b4ae756
</comment><comment author="dakrone" created="2015-07-29T12:47:17Z" id="125939446">&gt; Can we just extract with 'rpm' itself?

I think this would be the best tool for extracting it.
</comment><comment author="spinscale" created="2015-07-29T12:56:02Z" id="125940906">hey,

I have asked the infra folks to install rpm2cpio on the build machines. There does not seem  to be a native way using the `rpm` binary to extract its content, similar to what `dpkg -x` is doing.

Another solution to this might be to write those tests in java and use packages like redline to  extract the contents, so we dont rely on cmd tools.
</comment><comment author="dakrone" created="2015-07-29T13:29:21Z" id="125952999">@spinscale it isn't because `rpm2cpio` isn't on the machines, the RPM package is not being generated properly.
</comment><comment author="rmuir" created="2015-07-30T02:36:20Z" id="126163678">According to my investigation, the generated rpm works (https://github.com/elastic/elasticsearch/pull/12549). It is possible to do the extraction here with rpm itself, thats how those integration tests work. We can do the same for the license checking.
</comment><comment author="clintongormley" created="2015-07-30T10:02:34Z" id="126258152">@rmuir with https://github.com/elastic/elasticsearch/pull/12549, you seem to be extracting all of the packages via maven.  Should I change the license checker script to just accept the directory where the package has been extracted, instead of extracting the package itself?

The only issue I see with that is that, for the `--update` option, the user needs to point the script at some source (currently one of the packages) but if we just use extracted directories then the user will need to extract the package themselves.  (Or we could just support zip extraction only)
</comment><comment author="rmuir" created="2015-07-30T12:15:12Z" id="126303869">&gt; @rmuir with #12549, you seem to be extracting all of the packages via maven. Should I change the license checker script to just accept the directory where the package has been extracted, instead of extracting the package itself?

I'd rather not change it to be the same directory, but, we can reuse the same logic. I don't want the license checker to fail because someone ran -DskipTests or something like that, it shouldnt really depend on integration tests. but it can just invoke the same logic from the same ant file :)

&gt; The only issue I see with that is that, for the --update option, the user needs to point the script at some source (currently one of the packages) but if we just use extracted directories then the user will need to extract the package themselves. (Or we could just support zip extraction only)

I don't know know what this option is... I am lost here sorry. Is it some tool to update the license data in case its out of date? If thats the case, we can just tell people to do it from the zip, or whatever... try to keep this stuff simple, this build is already overwhelmed with crazy requirements (shading, plugins, different packaging systems, etc etc)
</comment><comment author="rmuir" created="2015-07-30T12:27:21Z" id="126306740">@clintongormley if you can give the license checker script some option to just accept a directory, i can hook in the logic from https://github.com/elastic/elasticsearch/pull/12549. Basically it should look a lot like this in the target logic where it invokes the perl script:

```
    &lt;target&gt;
        &lt;!-- invoke rpm extraction from integration tests file --&gt;
        &lt;ant antfile="${elasticsearch.integ.antfile}" target="setup-workspace-rpm"/&gt;
        &lt;exec ... current logic that calls perl script ...&gt;
            &lt;arg some logic passing ${integ.scratch}/rpm-extracted directory/&gt;
        &lt;/exec&gt;
    &lt;/target&gt;
```
</comment><comment author="clintongormley" created="2015-08-04T12:18:33Z" id="127581182">Closed by https://github.com/elastic/elasticsearch/pull/12631
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Separating QueryParseContext and QueryShardContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12527</link><project id="" key="" /><description>The parse() method we are currently splitting into a query parsing part (fromXContent()) and a lucene query generating part (toQuery()) in the feature refactoring branch (#10217) takes QueryParseContext as an argument. At some point we need to excute these two steps in different phases on the coordinating node and the shards. This protoype PR tries to anticipate this by introducing a new QueryCreationContext object as the argument of the query-generating methods (toQuery() and all dependent methods). 
As a first step the existing QueryParseContext is renamed to QueryCreationContext, which itself acts as a wrapper around a new lightweight QueryParseContext that should only contain the functionality used during query parsing. In subsequent steps we can then try to separate the different behaviour needed in the two context objects.

This is WIP, the separation of the two context is not completely clean and local variables are not renames. Also the naming of the new context object is completely open. Would like to open this PR to get feedback and discuss. 

PR goes against query refactoring branch.
</description><key id="97916298">12527</key><summary>Separating QueryParseContext and QueryShardContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-29T11:30:48Z</created><updated>2016-03-11T11:51:02Z</updated><resolved>2015-08-05T14:24:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-29T13:25:26Z" id="125951454">left a bunch of comments
</comment><comment author="cbuescher" created="2015-07-31T13:05:55Z" id="126687022">I continued thinning down the new lightweight QueryParseContext, addressing some of the comments and trying to make QueryParseContext only refer back to its inner QueryCreationContext, not the other way round. However, I'm thinking if we shouldn't reverse this, making the new QueryParseContext that should only contain the parser, some additional state and the helper methods (parseInnerQuerBuilder etc...) an inner part of the new QueryCreationContext, exposing it there through "getParseContext()" in all the places (essentially the old `parse(QueryParseContext)` methods that haven't been refactored yet. So we could use the new `QueryParseContext` only in methods that we sure that only need parsing (like the parsers `fromXContent()`) and making the un-refactored `parse()` methods get a QueryCreationContext where they can retrieve a parse context if need be.
</comment><comment author="cbuescher" created="2015-08-03T16:42:28Z" id="127326083">@javanna I rebased and went with renaming the old QueryParseContext to QueryShardContext now, with similar exception naming. I went though the changes, commented where I had to adapt two queries we already touched. Apart from that I think the PercolatorQueriesRegistry needs some attantion later, also the IndexQueryParserService, but that can be done in subsequent PRs. Also the `isFilter` handling needs to move to `toQuery`, but I'd also prefer doing that in a separate PR (there's also reminders for that here: https://github.com/elastic/elasticsearch/issues/12022)
</comment><comment author="javanna" created="2015-08-04T12:35:18Z" id="127586313">I did another round, I think this is getting very close. The only issue I have is the fact that you can retrieve the QueryParseContext from the QueryShardContext. This confuses me, I find it conceptually wrong as you are in parse phase and might need to move on, hence create a QueryShardContext given a QueryParseContext. The other way around just because the latter is a lighter context compared to the former isn't a goo engough reason to do things this way to be honest, unless I am missing other reasons why we moved to this approach. Let me know what you think. I would still have all the parse methods accept a QueryParseContext and remove getParseContext from QueryShardContext.
</comment><comment author="cbuescher" created="2015-08-04T15:14:36Z" id="127645832">@javanna addressed most of your last comment, I have trouble removing catching those exceptions you mentioned, in one case test fails, in other case I still don't see why this should happen. Also need to have a final sync on the direction in which the two contexts refer to each other for now.
</comment><comment author="javanna" created="2015-08-05T09:45:28Z" id="127937675">I did a final round, left a few minor comments, but this LGTM once those are addressed
</comment><comment author="cbuescher" created="2015-08-05T10:47:31Z" id="127953771">@javanna did another rebase, will still need to do a final squash and work on the commit message if you think this should go into the branch now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make stats APIs pluggable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12526</link><project id="" key="" /><description>The stats APIs should have a hook that allows plugins to inject their own statistics.
</description><key id="97885034">12526</key><summary>Make stats APIs pluggable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-29T08:25:26Z</created><updated>2016-01-26T17:07:13Z</updated><resolved>2016-01-26T17:07:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-29T08:37:04Z" id="125881232">How does this fit into the current stats APIs? Another stats entry (say `plugins`) similar to `indices`, `os` etc?

Anyway I'll be happy to take this one :)
</comment><comment author="uboness" created="2015-08-04T16:51:42Z" id="127673043">@tlrx I don't think we need to have `plugins` or `plugin` as a category. Each plugin should define its own category... so plugin `foo` will register a stats/info source for `foo` category. 
</comment><comment author="clintongormley" created="2016-01-26T17:07:08Z" id="175120201">I don't think we need this anymore, so I'll close, for now at least.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updating default position_offset_gap to 10.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12525</link><project id="" key="" /><description>Addresses #7268.

Please feel free to let me know if I've missed something with the PR process.
</description><key id="97883404">12525</key><summary>Updating default position_offset_gap to 10.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">mute</reporter><labels><label>:Mapping</label><label>Awaiting CLA</label><label>enhancement</label></labels><created>2015-07-29T08:09:52Z</created><updated>2015-08-04T09:34:34Z</updated><resolved>2015-07-29T16:49:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mute" created="2015-07-29T08:12:29Z" id="125876809">I did fill out / sign the CLA earlier today -- not sure why that check has failed.
</comment><comment author="nik9000" created="2015-07-29T12:21:15Z" id="125934567">I'm not sure about the CLA check - I'll manually verify it when I have access to that.

I see another instance of position_offset_gap defaulting to 0 in CustomAnalyzerProvider. I haven't chased down how the setting flows through the system so it'd be nice to set that to StringFieldMapper.Defaults.POSITION_OFFSET_GAP as well. I'd be nice to track down why it doesn't default to that in the first place - I'm guessing that is just an oversight.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can't set doc_values for _type field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12524</link><project id="" key="" /><description>The reference said that _type field "By default, the _type field is indexed (but not analyzed) and not stored. "
I has a terms aggregation for _type field. so I want to set _type to be doc_values to save fielddata memory. I wrote template as follow:

```
...
        },
        "@version" : {
          "index" : "not_analyzed",
          "doc_values" : true,
          "type" : "string"
        }
      },
      "_type" : {
        "index" : "not_analyzed",
        "doc_values" : true,
        "type" : "string"
      },
      "_all" : {
        "enabled" : true
      }
    }
  },
  "aliases" : { }
}
```

template POST ok, but when I create a new index and GET the mapping, I can't find the _type defination in the mapping. And after sometimes running, I saw the `_type` fielddata memory using grows.
</description><key id="97868514">12524</key><summary>can't set doc_values for _type field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chenryn</reporter><labels /><created>2015-07-29T06:14:50Z</created><updated>2015-07-30T08:02:05Z</updated><resolved>2015-07-30T08:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-07-30T05:29:37Z" id="126188596">Which version are you using? I think _type is not configurable for now, [TypeFieldMapper](https://github.com/elastic/elasticsearch/blob/e0708813a9228c4d69af5b94bdf263c2bc7945ed/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java#L97)
</comment><comment author="chenryn" created="2015-07-30T06:05:52Z" id="126196357">@xuzha _type is not configurable for 2.0, I use version 1.6.0 now.
</comment><comment author="xuzha" created="2015-07-30T06:08:25Z" id="126196793">If you are using 1.7 or older versions, that could be possible. As I mentioned before, we locked down meta-fields, details see # [8143](https://github.com/elastic/elasticsearch/issues/8143). And doc-values should be enabled by default for 2.0. I don't know if we should fix this.
</comment><comment author="chenryn" created="2015-07-30T08:02:05Z" id="126215709">oh, I forgot doc_values will be enabled by default for 2.0. thanks. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date fails to parse exact match with milliseconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12523</link><project id="" key="" /><description>Since at least 1.6.0, exactly matching against a field mapped as a `date` using the Unix epoch format will fail. This worked in Elasticsearch 1.4.4. Of interest, we upgraded from Joda 2.3 to Joda 2.7 starting with ES 1.5.0.

``` json
{
  "query": {
    "query_string": { 
      "query": "@timestamp:1438142633505"
    }
  }
}
```

or

``` json
{
  "query": {
    "match": { 
      "@timestamp": "1438142633505"
    }
  }
}
```

Both of these produce

```
java.lang.IllegalArgumentException: Invalid format: "1438142633505" is malformed at "3505"
    at org.elasticsearch.common.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187)
    at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780)
    at org.elasticsearch.index.mapper.core.DateFieldMapper.parseValue(DateFieldMapper.java:276)
    at org.elasticsearch.index.mapper.core.DateFieldMapper.indexedValueForSearch(DateFieldMapper.java:265)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.termQuery(NumberFieldMapper.java:277)
```
### Workaround

Fortunately, for the unusual use case of exactly matching with millisecond precision, you can use range queries (or filters) to workaround the issue.

``` json
{
  "query": {
    "query_string": { 
      "query": "@timestamp:[1438142633505 TO 1438142633505]"
    }
  }
}
```

or

``` json
{
  "query": {
    "range": { 
      "@timestamp": {
        "gte" : 1438142633505,
        "lte" : 1438142633505
      }
    }
  }
}
```
</description><key id="97860139">12523</key><summary>Date fails to parse exact match with milliseconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-07-29T04:56:12Z</created><updated>2015-09-07T17:30:53Z</updated><resolved>2015-08-05T17:01:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HarishAtGitHub" created="2015-07-30T06:31:05Z" id="126201157">I am trying to reproduce this issue in the latest elastic search code(since you said it was not working "SINCE 1.6") that I pulled from master (elasticsearch_2.0.0~SNAPSHOT) in an attempt to fix the bug if it is ..
1) create index  http://localhost:9200/elasticsearch - POST
         elasticsearch is the indexname

2) to create an type and its mappings
http://localhost:9200/elasticsearch/indexone/_mapping - POST 

``` javascript
{
       "properties" : { 
           "@timestamp" : {
            "type" :   "date"
           },
           "name" : {
            "type" :   "string"
           }
        }
}
```

here @timestamp is the field mapped to date (as your issue says)

3) create an entry in the type indexone as 
http://localhost:9200/elasticsearch/indexone/1 - POST 
1 is the id of the entry

``` javascript
{
  "name" : "harish",
  "@timestamp":1438142633505
}
```

4) query it as 
http://localhost:9200/elasticsearch/_search - POST 

i/p - 

``` javascript
{
  "query": {
    "match": { 
      "@timestamp": "1438142633505"
    }
  }
}
```

o/p

``` javascript
{
    "took": 2,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 1,
        "hits": [
            {
                "_index": "elasticsearch",
                "_type": "indexone",
                "_id": "1",
                "_score": 1,
                "_source": {
                    "name": "harish",
                    "@timestamp": 1438142633505
                }
            }
        ]
    }
}
```

so I got the desired output. 

Am I going wrong in this test case ... Please point out ...

code for the test case is here :    

```
https://github.com/HarishAtGitHub/elasticsearch-tester/blob/master/12523.py
```

To run the above test case:

```
  git clone https://github.com/HarishAtGitHub/elasticsearch-tester.git
  python 12523.py
```

I even tried enabling the timestamp internal field that es provides and tried to do exact matching and it worked.

FYI: I am beginner trying to contribute to elastic search. So pls forgive my mistakes(if any) for a few days , then I will pick up....
</comment><comment author="clintongormley" created="2015-08-05T09:26:10Z" id="127931130">This bug was caused by https://github.com/elastic/elasticsearch/pull/10648

@markharwood please could you take a look
</comment><comment author="markharwood" created="2015-08-05T10:31:20Z" id="127950936">This works for me on 1.7.1:

```
{
  "query": {
    "match": { 
      "@timestamp": 1438142633505
    }
  }
}
```

(note the use of a numeric value rather than the string in your example)
`query_string` and your `match` example that passes a String fail from what looks to be an issue with this version of Joda time parsing Strings as opposed to Longs. 

Master branch works OK for `match` with Strings and `query_string`.
</comment><comment author="clintongormley" created="2015-08-05T17:01:38Z" id="128070659">OK so fixed in master. I'll close this then.  thanks @markharwood 
</comment><comment author="ejain" created="2015-09-07T17:30:53Z" id="138346299">Just ran into this issue when upgrading from 1.5.2 to 1.7.1. Had to replace `QueryBuilders.termQuery(field, value)` with `QueryBuilders.matchQuery(field, Long.parseLong(value))`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent awareness allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12522</link><project id="" key="" /><description>When awareness attribute is unbalanced, behavior of shard allocation is inconsistent depending on how many shards are already started.

Reproduction: https://gist.github.com/masaruh/63db5a030220a26cc2a8
It allocates all shards of an index to nodes on creation but when one of nodes leaves and comes back, it doesn't allocate previously allocated shard back to the node anymore.
(All shards shouldn't be allocated in the first place?)

Probably because AwarenessAllocationDecider doesn't take initializing shards into consideration when it checks if shards can be allocated on nodes (https://github.com/elastic/elasticsearch/blob/v1.7.0/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java#L189-L198)?

Looks to be related to #12431.
</description><key id="97855255">12522</key><summary>Inconsistent awareness allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2015-07-29T04:08:36Z</created><updated>2015-09-11T04:16:11Z</updated><resolved>2015-09-11T04:16:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Consolidate .gitignore entires for eclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12521</link><project id="" key="" /><description>Rather than specify paths for the .gitignored files that Eclipse uses for
project management just ignore all files and directories that look like
those directories. That way we can add new subprojects and we won't need
add more .gitignore entries.
</description><key id="97814477">12521</key><summary>Consolidate .gitignore entires for eclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-28T22:22:58Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-30T14:01:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-28T22:23:10Z" id="125770108">I'm not sure that we want a issue for something like this....
</comment><comment author="nik9000" created="2015-07-28T22:25:20Z" id="125770430">Ping @spinscale  - this came up because you created those new subprojects for packaging.
</comment><comment author="dadoonet" created="2015-07-28T22:53:21Z" id="125775961">LGTM
</comment><comment author="dakrone" created="2015-07-29T22:52:53Z" id="126120069">LGTM also
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add user defined attributes in _cat/nodes output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12520</link><project id="" key="" /><description>This PR allows custom node attributes to be displayed via `/_cat/nodes`.

Because custom node attributes are required to trigger this feature, rest-api-spec doesn't seem to satisfy the testing requirement. What's the best way to test this functionality, and where is a good place for me to start looking?

Here's a sample of the output when running with the following modifications to `elasticsearch.yml`:

```
node.rack: rack314
node.azone: us-east-1
```

Sample output:

```
$ curl -XGET 'http://localhost:9200/_cat/nodes'
host    ip          heap.percent ram.percent load node.role master name
epsilon 192.168.1.8            1          72 3.10 d         *      Alexander Bont
$ curl -XGET 'http://localhost:9200/_cat/nodes?h=node.rack,node.azone'
node.rack node.azone
  rack314  us-east-1
```

Closes #8000 
</description><key id="97799155">12520</key><summary>add user defined attributes in _cat/nodes output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">metadave</reporter><labels><label>:CAT API</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-07-28T20:49:14Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-29T12:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-07-28T21:14:38Z" id="125755896">This LGTM, but can you add a REST test in elasticsearch/rest-api-spec?
</comment><comment author="dakrone" created="2015-07-28T21:15:48Z" id="125756128">I don't think we currently have a way to add node attributes to the nodes started for the REST tests, @clintongormley do you know if that's currently possible?
</comment><comment author="drewr" created="2015-07-29T01:25:12Z" id="125797062">Apologies @metadave, I just added a comment to #8000 about the design here. Would you mind taking a look before merging?
</comment><comment author="jasontedor" created="2015-07-29T03:36:35Z" id="125827569">@dakrone is correct, it is not currently possible. I investigated the possibility of adding it and I do not think it's worthwhile. The underlying issue is that to enable this functionality, we would no longer be able to reuse the cluster across REST tests (i.e., switch to `Scope.TEST` from `Scope.SUITE`). This would take the REST tests from slow to unacceptably slow.
</comment><comment author="metadave" created="2015-07-29T12:57:57Z" id="125941292">I'm going to close this and work on a new API `_cat/nodeattrs` that has denormalized attributes. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query_string + uax_email_url + mixed unicode and ascii = broken highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12519</link><project id="" key="" /><description>A mix of both unicode and ASCII documents in the results of a highlight query_string query on uax_email_url analyzed documents causes highlighting to fail entirely.

Here's a set of curl commands that reproduce the issue.

First, we create an index and populate it with some test data:

```
curl -XPUT 'http://elasticsearch:9200/unicode/' -d '{
 "settings" : {
   "number_of_shards" : 1,
   "number_of_replicas" : 0,
   "analysis": {
     "analyzer": {
       "uax_url_email_analyzer": {
         "tokenizer": "uax_url_email_tokenizer"
       }
     },
     "tokenizer": {
       "uax_url_email_tokenizer": {
         "type": "uax_url_email",
         "max_token_length": "1024"
       }
     }
   }
 },
 "mappings" : {
   "doc" : {
     "properties" : {
       "name" : {
         "analyzer": "uax_url_email_analyzer",
         "type": "string"
       }
     }
   }
 }
}'
curl -XPUT "http://elasticsearch:9200/unicode/doc/1" -d'
{"name": "Document 1 ascii"}'
curl -XPUT "http://elasticsearch:9200/unicode/doc/2" -d'
{"name": "Document 2 &#226;&#8364;&#8482; unicode"}'
curl -XPUT "http://elasticsearch:9200/unicode/doc/3" -d'
{"name": "Document 3 ascii"}'
curl -XPUT "http://elasticsearch:9200/unicode/doc/4" -d'
{"name": "Document 4 &#226;&#8364;&#8482; unicode"}'
```

Output:

```
{"acknowledged":true}
{"_index":"unicode","_type":"doc","_id":"1","_version":1,"created":true}
{"_index":"unicode","_type":"doc","_id":"2","_version":1,"created":true}
{"_index":"unicode","_type":"doc","_id":"3","_version":1,"created":true}
{"_index":"unicode","_type":"doc","_id":"4","_version":1,"created":true}
```

Now we show that highlighting works for queries that return only ASCII documents:

```
curl -XGET "http://elasticsearch:9200/unicode/_search" -d'
{
 "query":{
   "query_string":{
     "query":"ascii",
     "allow_leading_wildcard":true
   }
 },
 "highlight":{
   "fields":{
     "*":{}
   }
 }
}'
```

Output:

```
{"took":3,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":2,"max_score":0.643841,"hits":[{"_index":"unicode","_type":"doc","_id":"1","_score":0.643841,"_source":
{"name": "Document 1 ascii"},"highlight":{"name":["Document 1 &lt;em&gt;ascii&lt;/em&gt;"]}},{"_index":"unicode","_type":"doc","_id":"3","_score":0.643841,"_source":
{"name": "Document 3 ascii"},"highlight":{"name":["Document 3 &lt;em&gt;ascii&lt;/em&gt;"]}}]}}
```

Next we shows that highlighting also works for queries that return only unicode documents:

```
curl -XGET "http://elasticsearch:9200/unicode/_search" -d'
{
 "query":{
   "query_string":{
     "query":"unicode",
     "allow_leading_wildcard":true
   }
 },
 "highlight":{
   "fields":{
     "*":{}
   }
 }
}'
```

Output:

```
{"took":3,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":2,"max_score":0.643841,"hits":[{"_index":"unicode","_type":"doc","_id":"2","_score":0.643841,"_source":
{"name": "Document 2 &#226;&#8364;&#8482; unicode"},"highlight":{"name":["Document 2 &#226;&#8364;&#8482; &lt;em&gt;unicode&lt;/em&gt;"]}},{"_index":"unicode","_type":"doc","_id":"4","_score":0.643841,"_source":
{"name": "Document 4 &#226;&#8364;&#8482; unicode"},"highlight":{"name":["Document 4 &#226;&#8364;&#8482; &lt;em&gt;unicode&lt;/em&gt;"]}}]}}
```

And finally, we show that highlighting does not work for queries that return both ASCII and unicode documents:

```
curl -XGET "http://elasticsearch:9200/unicode/_search" -d'
{
 "query":{
   "query_string":{
     "query":"document",
     "allow_leading_wildcard":true
   }
 },
 "highlight":{
   "fields":{
     "*":{}
   }
 }
}'
```

Output:

```
{"took":2,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":4,"max_score":0.3884282,"hits":[{"_index":"unicode","_type":"doc","_id":"1","_score":0.3884282,"_source":
{"name": "Document 1 ascii"}},{"_index":"unicode","_type":"doc","_id":"2","_score":0.3884282,"_source":
{"name": "Document 2 &#226;&#8364;&#8482; unicode"}},{"_index":"unicode","_type":"doc","_id":"3","_score":0.3884282,"_source":
{"name": "Document 3 ascii"}},{"_index":"unicode","_type":"doc","_id":"4","_score":0.3884282,"_source":
{"name": "Document 4 &#226;&#8364;&#8482; unicode"}}]}}
```

No highlighting, at all.

Adding "term_vector": "with_positions_offsets" to the mapping doesn't help. Using a match query instead of a query_string query fixes the highlighting. Using the standard tokenizer instead of uax_url_email fixes the highlighting. Apparently, the combination of query_string, uax_url_email, and highlighting does not play nice with a heterogeneous mix of character encodings?
</description><key id="97789998">12519</key><summary>query_string + uax_email_url + mixed unicode and ascii = broken highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smakolski</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2015-07-28T20:03:29Z</created><updated>2015-08-03T16:38:12Z</updated><resolved>2015-07-30T12:20:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-28T20:08:52Z" id="125738342">Thanks for the great bug report. Sorry for the issue.
</comment><comment author="GlenRSmith" created="2015-07-28T20:44:25Z" id="125748813">Notes:

Specifying the field name

```
GET /unicode/_search
{
  "query": {
    "query_string": {
      "query": "document",
      "allow_leading_wildcard": true
    }
  },
  "highlight": {
    "fields": {
      "name": {}
    }
  }
}
```

Doesn't help.

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 4,
      "max_score": 0.3884282,
      "hits": [
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "1",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 1 ascii"
            }
         },
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "2",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 2 &#226;&#8364;&#8482; unicode"
            }
         },
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "3",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 3 ascii"
            }
         },
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "4",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 4 &#226;&#8364;&#8482; unicode"
            }
         }
      ]
   }
}
```

Using a match query:

```
GET /unicode/_search
{
  "query": {
    "match": {
      "name": "Document"
    }
  },
  "highlight": {
    "fields": {
      "*": {}
    }
  }
}
```

behaves correctly.

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 4,
      "max_score": 0.3884282,
      "hits": [
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "1",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 1 ascii"
            },
            "highlight": {
               "name": [
                  "&lt;em&gt;Document&lt;/em&gt; 1 ascii"
               ]
            }
         },
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "2",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 2 &#226;&#8364;&#8482; unicode"
            },
            "highlight": {
               "name": [
                  "&lt;em&gt;Document&lt;/em&gt; 2 &#226;&#8364;&#8482; unicode"
               ]
            }
         },
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "3",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 3 ascii"
            },
            "highlight": {
               "name": [
                  "&lt;em&gt;Document&lt;/em&gt; 3 ascii"
               ]
            }
         },
         {
            "_index": "unicode",
            "_type": "doc",
            "_id": "4",
            "_score": 0.3884282,
            "_source": {
               "name": "Document 4 &#226;&#8364;&#8482; unicode"
            },
            "highlight": {
               "name": [
                  "&lt;em&gt;Document&lt;/em&gt; 4 &#226;&#8364;&#8482; unicode"
               ]
            }
         }
      ]
   }
}
```
</comment><comment author="clintongormley" created="2015-07-30T12:20:57Z" id="126304960">This is a mixup up querying on the `_all` field and querying on the `name` field.  

You run your query on the `_all` field (which uses the `standard` analyzer, not the `uax_url_email_analyzer` (which the `name` field uses).  Then you highlight on all fields. The `_all` field is not highlighted, because it doesn't have the original string field.  It tries to highlight the `name` field, which works fine for `unicode` and `ascii` but fails for `document`.  The reason is that you `uax_url_email_analyzer` doesn't lowercase, so the term `document` doesn't exist in the `name` field.  

however, `Document` does exist.  If you change the query to `name:document`, you'll find that no documents match.  However, if your query is `Document`, then the all field matches, AND the highlighting works.
</comment><comment author="smakolski" created="2015-07-30T15:37:02Z" id="126372679">If I understand you correctly, you seem to be suggesting that the following should work, complete with highlighting:

```
curl -XGET "http://elasticsearch:9200/unicode/_search" -d'
{
  "query":{
    "query_string":{
      "query":"Document",
      "allow_leading_wildcard":true
    }
  },
  "highlight":{
    "fields":{
      "*":{}
    }
  }
}'
```

I just ran that query, and no, I'm still seeing the exact same behavior. All four documents are returned, but I'm not seeing any highlighting:

```
{"took":2,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":4,"max_score":0.3884282,"hits":[{"_index":"unicode","_type":"doc","_id":"1","_score":0.3884282,"_source":
{"name": "Document 1 ascii"}},{"_index":"unicode","_type":"doc","_id":"2","_score":0.3884282,"_source":
{"name": "Document 2 &#226;&#8364;&#8482; unicode"}},{"_index":"unicode","_type":"doc","_id":"3","_score":0.3884282,"_source":
{"name": "Document 3 ascii"}},{"_index":"unicode","_type":"doc","_id":"4","_score":0.3884282,"_source":
{"name": "Document 4 &#226;&#8364;&#8482; unicode"}}]}}
```
</comment><comment author="nik9000" created="2015-08-03T16:38:12Z" id="127321806">Its to do with the way the query_string analyses its components using the field its targeting. This will work:

``` bash
curl -XGET "http://0:9200/unicode/_search?pretty" -d'
{
  "query":{
    "query_string":{
      "query":"Document",
      "allow_leading_wildcard":true,
      "default_field": "name"
    }
  },
  "highlight":{
    "fields":{
      "*":{}
    }
  }
}'
```

as will recreating _all with the right analyzer:

``` bash
curl -XDELETE 'http://0:9200/unicode?pretty'
curl -XPUT 'http://0:9200/unicode/' -d '{
 "settings" : {
   "number_of_shards" : 1,
   "number_of_replicas" : 0,
   "analysis": {
     "analyzer": {
       "uax_url_email_analyzer": {
         "tokenizer": "uax_url_email_tokenizer",
         "filter": "lowercase"
       }
     },
     "tokenizer": {
       "uax_url_email_tokenizer": {
         "type": "uax_url_email",
         "max_token_length": "1024"
       }
     }
   }
 },
 "mappings" : {
   "doc" : {
     "_all" : {
       "enabled": false
     },
     "properties" : {
       "name" : {
         "analyzer": "uax_url_email_analyzer",
         "type": "string",
         "copy_to": ["my_all"]
       },
       "my_all": {
         "analyzer": "uax_url_email_analyzer",
         "type": "string"
       }
     }
   }
 }
}'
curl -XPUT "http://:9200/unicode/doc/1" -d'
{"name": "Document 1 ascii"}'
curl -XPUT "http://0:9200/unicode/doc/2" -d'
{"name": "Document 2 &#226;&#8364;&#8482; unicode"}'
curl -XPUT "http://0:9200/unicode/doc/3" -d'
{"name": "Document 3 ascii"}'
curl -XPUT "http://0:9200/unicode/doc/4?refresh" -d'
{"name": "Document 4 &#226;&#8364;&#8482; unicode"}'


curl -XGET "http://0:9200/unicode/_search?pretty" -d'
{
 "query":{
   "query_string":{
     "query":"document",
     "allow_leading_wildcard":true,
     "default_field": "my_all"
   }
 },
 "highlight":{
   "fields":{
     "*":{}
   }
 }
}'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Protected against  `size` and `offset` larger than total number of document in a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12518</link><project id="" key="" /><description>PR for #12510
</description><key id="97786895">12518</key><summary>Protected against  `size` and `offset` larger than total number of document in a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-28T19:46:26Z</created><updated>2017-03-31T10:06:20Z</updated><resolved>2015-07-29T08:33:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-28T19:56:09Z" id="125735600">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bucket_script should allow user-defined params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12517</link><project id="" key="" /><description>Currently, the `bucket_script` pipeline only allows you to inject values that are derived from buckets.  It should also allow a generic `params` hash like other scripting functionality, so that the user can inject per-request variables which are independent of buckets.

E.g. something like this:

``` json
"bucket_script": {
  "buckets_path": {
    "mean": "movavg_mean.value",
    "std": "movavg_std.value"
  },
  "params": {
     "sigma": 3
  },
  "script": "mean + (sigma * std)"
}
```

**/cc** @colings86 
</description><key id="97775645">12517</key><summary>bucket_script should allow user-defined params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label></labels><created>2015-07-28T18:53:00Z</created><updated>2015-07-28T19:44:58Z</updated><resolved>2015-07-28T19:44:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-28T19:41:04Z" id="125732450">@polyfractal you can do this using the new scripts API in 2.0 try the following:

``` json
"bucket_script": {
  "buckets_path": {
    "mean": "movavg_mean.value",
    "std": "movavg_std.value"
  },
  "script": {
    "inline": "mean + (sigma * std)",
    "params": {
       "sigma": 3
    }
  }
}
```
</comment><comment author="polyfractal" created="2015-07-28T19:44:57Z" id="125733289">TIL.  Thanks, didn't realize it could do this!

![image](https://cloud.githubusercontent.com/assets/1224228/8941756/8408491a-353f-11e5-901c-24205e4ae148.png)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>send_refresh_mapping not working?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12516</link><project id="" key="" /><description>I have set `indices.cluster.send_refresh_mapping: false` to prevent data nodes to send mapping updates to masters.
This worked for a while, but now i have cluster task queue full of update mapping events.
The only thing I have done is upgrade elasticsearch version from 1.4.4 to 1.7.0
</description><key id="97772434">12516</key><summary>send_refresh_mapping not working?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sschepens</reporter><labels><label>:Mapping</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-07-28T18:41:27Z</created><updated>2016-05-24T10:29:39Z</updated><resolved>2016-05-24T10:29:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T17:05:06Z" id="175118981">Hi @sschepens 

Sorry for the delay in getting to this.  Did you find a resolution for this issue?  Given the complete mapping rewrite in 2.0, I'm inclined to close this.
</comment><comment author="clintongormley" created="2016-05-24T10:29:39Z" id="221229310">This issue is fairly old and there hasn't been much activity on it. Closing, but please re-open if it still occurs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix messaging about delayed allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12515</link><project id="" key="" /><description>Previously when RoutingService checks for delayed shards, it can see
shards that are delayed, but are past their delay time so the logged
output looks like:

```
delaying allocation for [0] unassigned shards, next check in [0s]
```

This change allows shards that have passed their delay to be counted
correctly for the logging. Additionally, it places a 5 second minimum
delay between scheduled reroutes to try to minimize the number of
reroutes run.

This also adds a test that creates a large number of unassigned delayed
shards and ensures that they are rerouted even if a single reroute does
not allocated all shards (due to a low concurrent_recoveries setting).

Resolves #12456 

(This PR is against 1.7 and will be forward-ported)
</description><key id="97748933">12515</key><summary>Fix messaging about delayed allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>bug</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-07-28T16:32:59Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-29T15:16:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-28T19:01:38Z" id="125721754">LGTM. Left a non blocking question.
</comment><comment author="nik9000" created="2015-07-28T22:04:57Z" id="125766793">LGTM
</comment><comment author="dakrone" created="2015-07-29T15:16:42Z" id="125985209">Closing this, handled a different way in #12532 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Preference Test changes in 1.7 branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12514</link><project id="" key="" /><description>To make sure tests are merged to 2.0 ( included changes from @spinscale as well ) 
</description><key id="97748127">12514</key><summary>Preference Test changes in 1.7 branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirmalc</reporter><labels><label>:Search</label><label>feedback_needed</label><label>review</label></labels><created>2015-07-28T16:29:23Z</created><updated>2016-04-06T20:55:49Z</updated><resolved>2016-04-06T20:55:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-10T11:22:25Z" id="194799256">Hi @nirmalc 

Apologies this one appears to have fallen through the cracks.  Would you be interested in rebasing against current master and resubmitting?
</comment><comment author="dakrone" created="2016-04-06T20:55:48Z" id="206563779">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin load order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12513</link><project id="" key="" /><description>Currently, plugins are loaded based on the hash of the plugin path. This can make (buggy) plugin pairs that seemingly work in one install break when run from somewhere else.

Plugins _should_ work regardless of the order they are loaded in. If they do not, then it's at least helpful to be able to reason about the order.
</description><key id="97746005">12513</key><summary>Plugin load order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexbrasetvik</reporter><labels /><created>2015-07-28T16:18:55Z</created><updated>2015-07-29T11:51:12Z</updated><resolved>2015-07-29T11:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-28T16:26:54Z" id="125670833">LGTM
</comment><comment author="jpountz" created="2015-07-29T11:51:12Z" id="125929166">Just discussed it with @alexbrasetvik and 2.0 doesn't have this issue, so I'll close this PR as it's not a bug fix either so shouldn't be merged to 1.7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make startTime part of ActionRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12512</link><project id="" key="" /><description>At the moment the _search api ensure that the current time (useful for scripts with now etc.) is consistent across all shards where the search request executed. This is because all the search related transport action extend from `AbstractAsyncAction`, which records the startTime of an request. 

What I think would be better if startTime becomes part of the ActionRequest (or an immediate abstract subclass) itself and for shard level related request classes we would then need another base class. So that enforcing that current time will evaluate constantly in all apis.
</description><key id="97738134">12512</key><summary>Make startTime part of ActionRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-28T15:41:37Z</created><updated>2016-01-26T15:21:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] Add tests for searching missing _all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12511</link><project id="" key="" /><description>In 2.x searching the _all field spat out NullPointerException. It never
has in 1.x but we never had a test for it. This adds that test.

Closes #12439
</description><key id="97733041">12511</key><summary>[TEST] Add tests for searching missing _all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v1.6.2</label><label>v1.7.1</label></labels><created>2015-07-28T15:18:08Z</created><updated>2015-07-28T16:16:00Z</updated><resolved>2015-07-28T16:16:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-28T15:21:17Z" id="125651248">LGTM
</comment><comment author="nik9000" created="2015-07-28T15:23:12Z" id="125652498">@jasontedor or @metadave want to have a look too so we can get the requisite 2 reviewers? I figure its not that big a deal for tests that have already been merged to other branches but, you know, practice is a good thing!
</comment><comment author="jasontedor" created="2015-07-28T15:25:00Z" id="125653545">LGTM.
</comment><comment author="metadave" created="2015-07-28T15:32:38Z" id="125655413">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>top hits: huge number in "size" makes node go OOM even with very few docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12510</link><project id="" key="" /><description>To reproduce, start a node with 1g heap and then:

```
DELETE test

PUT index/doc/id
{
  "text": "text"
}

POST index/_search
{
  "aggs": {
    "text": {
      "terms": {
        "field": "text",
        "size": 10
      },
      "aggs": {
        "top_hits": {
          "top_hits": {
            "size": 200000000
          }
        }
      }
    }
  }
}
```

Tested 1.7.0.
While it is not a good idea to retrieve 200000000 documents I would still not expect an OOM with only one document.
</description><key id="97719760">12510</key><summary>top hits: huge number in "size" makes node go OOM even with very few docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-07-28T14:29:07Z</created><updated>2017-03-31T10:06:20Z</updated><resolved>2015-07-29T08:33:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Default date formats to use underscores via PUT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12509</link><project id="" key="" /><description>A PUT will default to `strictDateOptionalTime` or `dateOptionalTime` before 2.0, this PR changes the default format to `strict_date_optional_time` or `date_optional_time` respectively. Existing tests modified to test new format.

cc @clintongormley 

Closes #12429
</description><key id="97716002">12509</key><summary>Default date formats to use underscores via PUT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">metadave</reporter><labels><label>:Dates</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-28T14:08:49Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-07-28T20:57:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-28T16:27:49Z" id="125671109">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin script: Fix ES_HOME with spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12508</link><project id="" key="" /><description>When ES_HOME has spaces we get funky "cannot find main class directory&gt;"
errors. This makes them stop by escaping directories and using eval instead
of exec.

Closes #12504
</description><key id="97710839">12508</key><summary>Plugin script: Fix ES_HOME with spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.7.2</label></labels><created>2015-07-28T13:43:27Z</created><updated>2015-08-07T10:06:51Z</updated><resolved>2015-08-03T13:41:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-28T13:45:12Z" id="125615501">I'm not sure if this is 1.7.2 or 1.7.1.
</comment><comment author="electrical" created="2015-07-28T14:38:46Z" id="125631115">@tlrx @spinscale could you guys review this? thanks!
</comment><comment author="jaymode" created="2015-07-30T13:37:28Z" id="126326887">I tested this out in a directory with spaces and this fixes the issue described in the ticket. If we also have a space in one of the parameters like the plugin URL we still have an issue in that the parameter is split into two by the space.

```
bin/plugin install plugin -u "file:///path/with space/plugin.zip"
```

It will try to install `space/plugin.zip`

The fix is similar to what's already been done here and it would be awesome to get it included now:

```
# real getopt cannot be used because we need to hand options over to the PluginManager
while [ $# -gt 0 ]; do
  case $1 in
    -D*=*)
      properties="$properties \"$1\""
      ;;
    -D*)
      var="$1"
      shift
      properties="$properties \"$var\"=\"$1\""
      ;;
    *)
      args="$args \"$1\""
  esac
  shift
done
```

@nik9000 what do you think? If you'd prefer to keep it separate, we can and I can open up a different PR.
</comment><comment author="nik9000" created="2015-07-30T14:10:55Z" id="126341947">&gt; @nik9000 what do you think? If you'd prefer to keep it separate, we can and I can open up a different PR.

It makes sense to me. This isn't really my area of expertise but I'm happy to add it.

BTW, this is against the 1.7 branch and I'm not sure that this is still an issue in 2.0. Is this another one of things we should just wait on 2.0 to fix?
</comment><comment author="jaymode" created="2015-07-30T14:17:34Z" id="126344246">I also tested with 2.0 and it is still an issue there as well, so we definitely need to fix it in master.
</comment><comment author="nik9000" created="2015-07-30T14:32:08Z" id="126349083">Cool. In that case I'll grab this issue in a bit and fix the spaces in
plugin paths issue. I'll also look at adding some bats tests. Then forward
port to 2.0.....

On Thu, Jul 30, 2015 at 10:17 AM, Jay Modi notifications@github.com wrote:

&gt; I also tested with 2.0 and it is still an issue there as well, so we
&gt; definitely need to fix it in master.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12508#issuecomment-126344246
&gt; .
</comment><comment author="nik9000" created="2015-07-30T14:32:25Z" id="126349147">Swap review for WIP because I have to add another fix and some bats tests.
</comment><comment author="nik9000" created="2015-07-30T15:59:30Z" id="126380989">@tlrx do you have any special setup that you do to run bats? The bats stuff in 1.7 claims to be very destructive to you system so I spun up an ubuntu vm to test it in.
</comment><comment author="nik9000" created="2015-07-30T17:22:31Z" id="126409308">Ok - ready for review I think.
</comment><comment author="jaymode" created="2015-07-30T17:54:35Z" id="126417558">LGTM. Built and ran it in a directory with spaces and used a path with spaces as well.
</comment><comment author="tlrx" created="2015-08-03T07:17:25Z" id="127146024">&gt; @tlrx do you have any special setup that you do to run bats? The bats stuff in 1.7 claims to be very destructive to you system so I spun up an ubuntu vm to test it in.

Yeah, always use a vm to run the tests! They install/remove packages, user and dirs. See https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc#testing-scripts
</comment><comment author="tlrx" created="2015-08-03T07:54:45Z" id="127152583">LGTM

BATS tests succeed on Debian 7.8, Debian 8, CentOS 6.6, CentOS 7, Ubuntu 12.04 and Ubuntu 15.04
</comment><comment author="electrical" created="2015-08-03T08:16:35Z" id="127160234">LGTM. ready to merge i think.
</comment><comment author="nik9000" created="2015-08-03T13:38:10Z" id="127236344">&gt; BATS tests succeed on Debian 7.8, Debian 8, CentOS 6.6, CentOS 7, Ubuntu 12.04 and Ubuntu 15.04

That is a good list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin script: Fix ES_HOME with spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12507</link><project id="" key="" /><description>When ES_HOME has spaces we get funky "cannot find main class &lt;part of a
directory&gt;" errors. This makes them stop by escaping directories and using
eval instead of exec.

Closes #12504
</description><key id="97703716">12507</key><summary>Plugin script: Fix ES_HOME with spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>bug</label></labels><created>2015-07-28T13:13:04Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-28T13:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-28T13:14:14Z" id="125602699">My bash foo is pretty weak but this makes the problem stop on my laptop. Feel free to tell me its silly and propose another fix.
</comment><comment author="tlrx" created="2015-07-28T13:28:33Z" id="125605787">@nik9000 I left some comments. Also, that would be nice if we can have an integration test for this (at least a *.bats test for now and maybe later something in `integration-tests.xml`?)
</comment><comment author="nik9000" created="2015-07-28T13:28:37Z" id="125605807">Ignore this for now. I blindly merged it into the wrong branch. I know it applies to 1.7.0 so let me redo it there.
</comment><comment author="nik9000" created="2015-07-28T13:29:57Z" id="125606335">Abandoned.
</comment><comment author="nik9000" created="2015-07-28T13:45:42Z" id="125615614">Created against the proper path as #12508.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Move plugin manager into its own project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12506</link><project id="" key="" /><description>The plugin manager now is its own subproject and not part
of the core anymore and will be the base for further refactoring.

Requires some changes in the packaging to include the jar as
well as some minor test modifications.
</description><key id="97703564">12506</key><summary>PluginManager: Move plugin manager into its own project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label></labels><created>2015-07-28T13:12:09Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-08-04T11:22:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-28T16:54:12Z" id="125677937">Would this make sense to put it within `plugins` module?
I think we will end up with many modules and we should try to keep them organized.

Or may be put it under a new `modules` module. So we have:

```
core
modules
plugins
distribution
...
```

WDYT?
</comment><comment author="rmuir" created="2015-07-28T17:27:03Z" id="125685097">I dont think we should commit any packaging changes until builds are green again.
</comment><comment author="spinscale" created="2015-07-30T12:44:57Z" id="126310333">@dadoonet I dont know if we are going to have more. Does it make sense to wait and only create something like that, once we add more modules?
</comment><comment author="dadoonet" created="2015-07-30T13:07:06Z" id="126316814">Moving stuff in git is easy so I guess we can wait.
Changing the groupId has more implications IMO but still doable.
</comment><comment author="spinscale" created="2015-07-31T11:33:08Z" id="126660690">so I have spent some more time with this PR and keep thinking, that this is not the route to go, given that the next step is to merge the `PluginManagerCliParser` with the `BootstrapCliParser`, so that we can run

``` bash
bin/elasticsearch plugin install elasticsearch/marvel/latest
```

With this PR in place, we would need to load the `PluginManagerCliParser` dynamically in the `core` project, without actually being sure that it is there - as this would requite the `plugin-manager` jar to be in the `lib/` directory and we might need to fail gracefully, if it is not.

Even though I see sense in modularization, I do not see sense to do this for the plugin manager, if we have those kind of circular dependencies. The current workaround is using `Class.forName()`to load the `PluginManagerCliParser`  in my local branch, which I really dislike.

An interesting workaround for this problem could be the possibility to register custom commands on startup in `core`, so that one could register commands like `bin/elasticsearch whatever` - however this is currently not supported and requires some more work. In addition this would also require us to change the CLI parser to support the construct of not only commands (like `whatever`, but also some kind of sub commands (like `plugin install ...`), because right now the CliTool is limited to only accept one word at the top-level and then expect parameters. I will try to come up with something.

/cc @clintongormley 

**Update**: Turns out that registering stuff without something like guice is pretty tricky, not sure that is a good solution
</comment><comment author="spinscale" created="2015-08-04T11:22:20Z" id="127569001">closing and postponing for now as this requires more work on the testing side
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes serialization of HDRHistogram in percentiles aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12505</link><project id="" key="" /><description>Previously we would write the entire ByteBuffer to the stream to serialise the HDRHistogram even if it was not all needed. Now we only write the bytes that are actually written to in the ByteBuffer.
</description><key id="97702932">12505</key><summary>Fixes serialization of HDRHistogram in percentiles aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-28T13:09:11Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-28T13:14:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-28T13:12:48Z" id="125602404">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Space in directory structure fails plugin binary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12504</link><project id="" key="" /><description>When one of the parent directories has a space in there the plugin binary can't find the correct path.

```
$ pwd
/home/richard/temp dir/elasticsearch-1.7.0
$ ./bin/plugin
Error: Could not find or load main class dir.elasticsearch-1.7.0.config
```
</description><key id="97697908">12504</key><summary>Space in directory structure fails plugin binary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">electrical</reporter><labels><label>:Plugins</label><label>bug</label></labels><created>2015-07-28T12:48:07Z</created><updated>2015-08-04T13:05:38Z</updated><resolved>2015-08-04T13:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-28T12:50:57Z" id="125592141">Confirmed on OSX and Ubuntu.
</comment><comment author="nik9000" created="2015-07-28T15:21:28Z" id="125651368">Assigned to myself as I sent a PR for it. Anyone else can take it if they want it and think they can make a better pull request.
</comment><comment author="nik9000" created="2015-08-03T13:42:23Z" id="127238181">Ok - we have a fix merged for 1.7 and now I'll need to forward port that to master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarifying use of allocate with allow_primary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12503</link><project id="" key="" /><description>Clarifying use of allocate with allow_primary
</description><key id="97671342">12503</key><summary>Clarifying use of allocate with allow_primary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nellicus/following{/other_user}', u'events_url': u'https://api.github.com/users/nellicus/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nellicus/orgs', u'url': u'https://api.github.com/users/nellicus', u'gists_url': u'https://api.github.com/users/nellicus/gists{/gist_id}', u'html_url': u'https://github.com/nellicus', u'subscriptions_url': u'https://api.github.com/users/nellicus/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/8770097?v=4', u'repos_url': u'https://api.github.com/users/nellicus/repos', u'received_events_url': u'https://api.github.com/users/nellicus/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nellicus/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nellicus', u'type': u'User', u'id': 8770097, u'followers_url': u'https://api.github.com/users/nellicus/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>docs</label></labels><created>2015-07-28T10:34:10Z</created><updated>2015-08-08T07:16:57Z</updated><resolved>2015-08-07T10:04:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nellicus" created="2015-07-28T10:34:48Z" id="125548725">@ESamir @jjfalling @clintongormley can anyone review please?
</comment><comment author="jjfalling" created="2015-07-28T11:28:15Z" id="125565732">Looks good
</comment><comment author="clintongormley" created="2015-08-07T10:04:21Z" id="128662060">Rephrased without so many commas and merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12502</link><project id="" key="" /><description /><key id="97652325">12502</key><summary>Fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">zhzhzoo</reporter><labels><label>docs</label></labels><created>2015-07-28T09:05:48Z</created><updated>2015-08-15T14:00:33Z</updated><resolved>2015-08-15T14:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-30T10:46:11Z" id="126271248">Hi @zhzhzoo 

Thanks for the fix.  Please could you sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2015-08-15T14:00:33Z" id="131384511">thanks @zhzhzoo - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to resolve "unassigned_shards" in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12501</link><project id="" key="" /><description>Hi, 

Nowdays, the health of my cluster is red and my shards they are unasigned:

GET _cluster/health?pretty

and the request is:

{
   "cluster_name": "juan",
   "status": "red",
   "timed_out": false,
   "number_of_nodes": 10,
   "number_of_data_nodes": 9,
   "active_primary_shards": 88,
   "active_shards": 88,
   "relocating_shards": 0,
   "initializing_shards": 0,
   "unassigned_shards": 104
## }

And I try to recollate the shards:

POST /_cluster/reroute
{
        "commands" : [ {
              "allocate" : {
                  "index" : "logstash-2015.07.28", 
                  "shard" : 2, 
                  "node" : "Robbie Robertson", 
                  "allow_primary" : true
              }
            }
        ]
    }

But I can not do it 

Could somebody giveme any solution?
</description><key id="97647574">12501</key><summary>How to resolve "unassigned_shards" in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">juandasgandaras</reporter><labels /><created>2015-07-28T08:38:15Z</created><updated>2015-07-30T11:01:15Z</updated><resolved>2015-07-30T10:31:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="juandasgandaras" created="2015-07-28T11:02:14Z" id="125558669">Hi,

Nowdays, the health of my cluster is red and my shards they are unasigned:

GET _cluster/health?pretty

and the request is:
{
"cluster_name": "juan",
"status": "red",
"timed_out": false,
"number_of_nodes": 4,
"number_of_data_nodes": 3,
"active_primary_shards": 46,
"active_shards": 46,
"relocating_shards": 0,
"initializing_shards": 0,
"unassigned_shards": 136
}

and
GET _cat/allocation?v

shards disk.used disk.avail disk.total disk.percent host ip node

11 8.4tb 1.4tb 9.9tb 85 bc10-05 10.8.5.15 Albion

33 8.4tb 1.4tb 9.9tb 85 bc10-05 10.8.5.15 Mystique

0 0b l8a 10.8.0.231 logstash-l8a-5920-4018
2 8.4tb 1.4tb 9.9tb 85 bc10-05 10.8.5.15 Carolyn Trainer

136 UNASSIGNED

And I try to recollate the shards:

POST /_cluster/reroute
{
"commands" : [ {
"allocate" : {
"index" : "logstash-2015.07.28",
"shard" : 2,
"node" : "Albion",
"allow_primary" : true
}
}
]
}

But I can not do it

Could somebody giveme any solution?

Thanks in advance and sorry by the inconvenience.
</comment><comment author="clintongormley" created="2015-07-30T10:31:38Z" id="126268375">Hi @juandasgandaras 

Please ask questions like these on the forum instead: https://discuss.elastic.co/

If you use the `explain` parameter with the reroute command, it will tell you why it was not possible to allocate the shard.
</comment><comment author="juandasgandaras" created="2015-07-30T11:01:15Z" id="126273716">Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support snapshots on AWS Glacier?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12500</link><project id="" key="" /><description>Apparently when data is moved from an S3 snapshot repository to Glacier we can no longer see it. This has come from https://discuss.elastic.co/t/snapshot-and-restore-s3-and-glacier/26337

Though this probably has an impact - _To keep costs low, Amazon Glacier is optimized for infrequently accessed data where a retrieval time of several hours is suitable._ https://aws.amazon.com/glacier/

Some more info from http://cloudacademy.com/blog/amazon-s3-vs-amazon-glacier-a-simple-backup-strategy-in-the-cloud/
_S3 objects that have been moved to Glacier storage using S3 Lifecycle policies can only be accessed (or shall I say restored) using the S3 API endpoints. As such they are still managed as objects within S3buckets, instead of Archives within Vaults, which is the Glacier terminology._

Based on that it looks like the biggest issue is going to be the retrieval time, as ES would expect a reasonably quick response from the "FS".
</description><key id="97614528">12500</key><summary>Support snapshots on AWS Glacier?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2015-07-28T04:47:07Z</created><updated>2016-05-06T00:58:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="geekpete" created="2015-09-23T04:35:17Z" id="142487529">Almost needs a migration tool to move snapshots from s3 to glacier and back, where it tarballs and gzips (or some other method to create fewer or one file for glacier) then transfers to glacier. Then the reverse to "put it back" for restoration of the snapshot. 

Would be nicer to be a full plugin or be supported in the aws plugin,etc.

Would also be nice if this could be very parallel.
</comment><comment author="CliveJL" created="2015-10-16T10:51:59Z" id="148681731">I have been successfully archiving ES Snapshots to Glacier using an S3 Bucket Lifecycle policy for some time now. This Lifecycle policy archives the S3 objects in the **"indices/" path only**. This covers all the large snapshot data files. The much smaller snapshot metadata files, in the "root" of the Bucket, **always remains in S3's Standard storage class**, so they can be read immediately without causing errors for actions such as displaying snapshot data, for example the Curator "show snapshots" command.

On the odd occasion when I've needed to restore an index from a snapshot, I've used the AWS CLI tools (aws s3 &amp; aws s3api) to restore the data file(s) from Glacier first, waited a few hours, and then use the ES API to restore the index(es) into the ES cluster. This process is a little long-winded but works.

One issue I have just come across is trying to delete snapshots that are no longer required. The Curator tool is using the DELETE snapshot call to the ES API, and this is throwing an error due to the storage class of the S3 object (presumably the Glaciered "indices/*" files).

It would be great to have the handling of Glaciered snapshots by Elasticsearch done transparently.
</comment><comment author="dadoonet" created="2015-11-30T14:16:05Z" id="160642123">See also discussion in #13656 
</comment><comment author="geekpete" created="2016-05-06T00:51:56Z" id="217320709">Also to note is the new Infrequent Access storage class for S3 that Amazon now has.
This isn't as restrictive as Glacier, doesn't save you quite as much in cost but is still a very reduced cost compared to leaving it as is in S3. Which might make the glacier storage class slightly more special use case for only long term archival rather than pure cost saving now if much of your data may be ok to be set to Infrequent Access.

https://aws.amazon.com/s3/storage-classes/

Actually [this ticket mentioned by dadoonet](https://github.com/elastic/elasticsearch/pull/13656) has had a pull request merged and might have solved it:
https://github.com/elastic/elasticsearch-cloud-aws/pull/243
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should we specify JVM versions in docs or link to support matrix?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12499</link><project id="" key="" /><description>In our reference documentation, we currently specify what JVM versions are supported on the setup page:

https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html

This is just duplicating the information in our support matrix here:

https://www.elastic.co/subscriptions/matrix

Are we worried about the reference and support matrix getting out of sync?  Shouldn't we just refer users to the support matrix?
</description><key id="97597557">12499</key><summary>Should we specify JVM versions in docs or link to support matrix?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">joshuar</reporter><labels><label>bug</label><label>docs</label></labels><created>2015-07-28T02:17:02Z</created><updated>2016-08-02T17:42:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-01-07T22:13:29Z" id="169823081">+1 to linking rather than duplicating with the potential to get out of sync.  It's just one less thing to forget about.
</comment><comment author="palecur" created="2016-01-08T01:07:53Z" id="169856501">Agreeing pretty hard with @eskibars here.
</comment><comment author="ppf2" created="2016-08-02T17:42:04Z" id="236982786">Currently, this is causing confusion, esp. around IcedTea OpenJDK 8 support.  

The reason why Oracle JVM 1.8 appears in the [matrix](https://www.elastic.co/support/matrix#show_jvm) but not Iced Tea OpenJDK 8 is because we haven't run enough tests against IcedTea OpenJDK 8 at this time.  While Iced Tea OpenJDK 8 may work fine (and many users are using it), it is not currently considered an officially supported Java version.

The way the documentation is written today (https://www.elastic.co/guide/en/elasticsearch/reference/2.3/setup.html#jvm-version) can be a bit misleading.

&gt; Elasticsearch is built using Java, and requires at least Java 7 in order to run. Only Oracle&#8217;s Java and the OpenJDK are supported. The same JVM version should be used on all Elasticsearch nodes and clients.
&gt; We recommend installing the Java 8 update 20 or later, or Java 7 update 55 or later. Previous versions of Java 7 are known to have bugs that can cause index corruption and data loss. Elasticsearch will refuse to start if a known-bad version of Java is used. 

So let's update the documentation to cross reference the https://www.elastic.co/support/matrix#show_jvm instead :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding retry when checking s3 snapshot repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12498</link><project id="" key="" /><description>I think we should do this because of some weird problems. 

Closes #12462
</description><key id="97593275">12498</key><summary>Adding retry when checking s3 snapshot repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud AWS</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-28T01:40:44Z</created><updated>2015-08-13T11:41:46Z</updated><resolved>2015-08-04T23:11:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-07-29T00:50:08Z" id="125792770">Please let me know if any changes need to be made.
</comment><comment author="clintongormley" created="2015-07-30T12:24:11Z" id="126306185">@tlrx please could you take a look as well?
</comment><comment author="tlrx" created="2015-08-03T08:58:10Z" id="127169048">Left a minor comment, otherwise LGTM
</comment><comment author="tlrx" created="2015-08-03T09:03:47Z" id="127170044">I had a note about something I discover here: https://github.com/elastic/elasticsearch/commit/35376107604fd988812c93ad71b824464eb0808a
</comment><comment author="tlrx" created="2015-08-04T07:31:16Z" id="127507413">LGTM
</comment><comment author="xuzha" created="2015-08-04T23:16:07Z" id="127792533">Thanks you. Merged. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevents users from building a BulkProcessor with a null client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12497</link><project id="" key="" /><description>I suggest updating the BulkProcessor so that it prevents users from building a BulkProcessor with a null client.  If the client is null, then the class should throw an exception with a message that describes the cause of the exception.  Earlier today, when I passed a null client to the builder() method, later received an exception, and attempted to get the exception's message in my afterBulk() listener (e.g. e.getMessage()), the message was null.  It took me a while to pinpoint the cause of the problem.
</description><key id="97561680">12497</key><summary>Prevents users from building a BulkProcessor with a null client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">oyiadom</reporter><labels><label>:Java API</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-07-27T21:49:47Z</created><updated>2015-08-17T10:45:58Z</updated><resolved>2015-08-17T07:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-17T07:44:01Z" id="131712556">Thanks @oyiadom 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take into account load activity on a node when routing search requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12496</link><project id="" key="" /><description>Currently, by default search routing across copies of a shard is done in a randomized fashion.  But it does not take into account the load of the node the shard copy is on.  For a high search throughput cluster where there are many replicas configured, if a node becomes loaded, it will be nice for the routing logic to be smarter and take load, search queue length/rejections, etc.. into consideration when routing requests
</description><key id="97560506">12496</key><summary>Take into account load activity on a node when routing search requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Cluster</label><label>discuss</label><label>enhancement</label></labels><created>2015-07-27T21:41:55Z</created><updated>2016-03-24T23:27:04Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-27T21:49:45Z" id="125352004">This is a bit scary to me. Its already possible for a single node to poison the cluster. I'm afraid this trades some of those problems for more problems.

Imagine you have a cluster that consistently runs a queue depth of 20 searches. But something happens to one node causing it to fail all requests quickly. Now you start sending tons of requests to that node effectively killing the cluster. You can work around this by adding more smarts to detect the error.

I'm just worried this is a big thing to bite off and a difficult one to get right.
</comment><comment author="bakks" created="2015-07-27T22:29:48Z" id="125366766">It only took 8 minutes to shoot this down, that must be some kind of record!

Here's some background on this problem which we're observing with our cluster. We have a 24 node cluster (16 cores on each), 2 indexes, 6 shards in each, replica factor of 3, so 24 shards in each index, and 2 shards per node. Here's a chart of load on the cluster members:
![screen shot 2015-07-27 at 3 03 29 pm](https://cloud.githubusercontent.com/assets/1172710/8919161/0826e89c-3472-11e5-9d38-1f2118bee49c.png)
When we look at the `_cat/thread_pool` output that node consistently has all of its search threads busy and a large number of queued requests.

We seem to have some as-of-yet undiagnosed pathological query problems, which means that our entire ES cluster is CPU bound. When we have problems its generally because CPU is capping on one or more nodes. As you'll observe from the chart above, a single node has much higher load than the other 23 nodes. This is the effect on application performance:
![screen shot 2015-07-27 at 3 17 19 pm](https://cloud.githubusercontent.com/assets/1172710/8919260/c37a50ac-3472-11e5-9b7e-95ca7cb805ce.png)

Effectively the fact that a single node is capping CPU means that the average latency for our search traffic goes from 20 ms to 1000+ ms. This is bad in itself, but also our entire application effectively must queue web requests because a core backend service is stressed because a _single node_ is having trouble. In my opinion this behavior obviates the entire point of running a distributed system, which is to divide the workload and handle isolated problems.

I'd like to request that this be looked at with a high priority - either there's something seriously broken about our deployment or this is a serious problem with ES query routing. I'm more than happy to provide more details about our deployment, @ppf2 has a diagnostic dump of what this looks like in practice. Thanks for looking at this!
</comment><comment author="nik9000" created="2015-07-29T00:48:15Z" id="125792579">Something is certainly wrong. @ppf2 will certainly know more with the dump. I'm just weary of any kind of automatic actions on single nodes that are in trouble because it makes me think of cascading failures. I'm sure there are good, safe things that Elasticsearch could do but I worry about doing something simple without thinking it through very very very hard.
</comment><comment author="bakks" created="2015-07-29T01:17:09Z" id="125796191">Here's the functionality I would suggest: pick shards for queries based on queue lengths. If a single node has a much larger queue length than every other node, it probably shouldn't be receiving more queries.

My understanding is that ES currently randomly selects among the running nodes that are serving the desired shard. So if a node drops out of the cluster, ES removes it from the pool and all is well. If a node is stressed, ie under higher load than its peers, then the entire cluster's performance is affected, because queries continue to be blindly routed to this node. As you add nodes to your cluster, the chances of a single node encountering issues increases.

I would invite you to test a large ES cluster and stress a single one of the nodes, observing query latency when this occurs.

I think the high-level question here is whether or not ES should be robust in situations where a node remains in the cluster but doesn't perform as well as other nodes. I would argue that its a poor distributed system if these scenarios are ignored. Currently its very easy for a single node to negatively impact overall ES query performance.
</comment><comment author="clintongormley" created="2015-07-30T12:31:27Z" id="126307336">@bakks it is not a question of importance - this is clearly an important thing to get right.  the problem is the second part: getting it right.  Simple heuristics are quite likely to make things worse.  We are investigating more robust techniques for taking load into account, but it is not something that we are going to be able to rush in.  It is a big project which needs a lot of careful testing before we can make any changes.

In your case, I'd try to figure out why one node is taking the brunt of the load.  Are you
- sending all search requests to one node?
- performing lots of updates?

Would be interesting to get a hot threads dump for the node in particular, especially compared to another node that doesn't have so much CPU usage.
</comment><comment author="bakks" created="2015-09-12T22:42:16Z" id="139825974">@clintongormley Thanks for your response - totally understand that you guys want to get this right.

What I was pointing out was not that we have a more loaded node than others, but rather that ES seems to handle that case very poorly. We distribute queries evenly among our data nodes and our updates are spread fairly evenly, so I don't think there's anything about our workload thats unbalanced in terms of ES shards. When we're running normally our nodes look similar in terms of observed load.

However, this ES characteristic is still relevant to us and others because:
- I think its pretty common to have a cluster spread over heterogeneous hardware. For example, In our cloud provider we split nodes into 3 separate zones, one of which runs an older generation CPU.
- Exogenous factors can effect node performance. Noisy neighbors, for example. In fact, as you increase the number of nodes the chance of an event outside your control affecting node performance also increases. Resiliency to node issues is obviously one of the key reasons to run a distributed system. Because ES doesn't handle single-node performance issues, you actually decrease your cluster stability when you add data nodes.
- As you approach the performance ceiling of a cluster, inevitably query queues increase and some node will be the first to overflow its queue and reject requests. From what I can tell, failing to route around this node effectively lowers Elasticsearch's query throughput ceiling.

@ppf2 has access to a bunch of telemetry from our clusters when we're experiencing heavy load, more than welcome to take a look at those as you investigate more.
</comment><comment author="bakks" created="2016-03-09T21:51:09Z" id="194525055">Just a ping on this issue - it is a serious problem with Elasticsearch's stability during load. Any movement on it?
</comment><comment author="ppf2" created="2016-03-24T23:27:04Z" id="201072357">Related: https://github.com/elastic/elasticsearch/issues/15914
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_all: Stop NPE querying _all when it doesn't exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12495</link><project id="" key="" /><description>This can happen in two ways:
1. The _all field is disabled.
2. There are documents in the index, the _all field is enabled, but there are
no fields in any of the documents.

In both of these cases we now rewrite the query to a MatchNoDocsQuery which
should be safe because there isn't anything to match.

Closes #12439
</description><key id="97547298">12495</key><summary>_all: Stop NPE querying _all when it doesn't exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T20:24:25Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-28T14:21:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-07-27T20:50:05Z" id="125338477">LGTM!
</comment><comment author="metadave" created="2015-07-27T20:58:02Z" id="125340593">LGTM
</comment><comment author="dakrone" created="2015-07-27T21:28:32Z" id="125347870">LGTM
</comment><comment author="nik9000" created="2015-07-27T21:31:09Z" id="125348372">Ok - three LGTM means I'll merge this in the morning.
</comment><comment author="nik9000" created="2015-07-28T13:29:40Z" id="125606189">@jpountz done
</comment><comment author="jpountz" created="2015-07-28T13:30:07Z" id="125606391">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndicesStore shouldn't try to delete index after deleting a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12494</link><project id="" key="" /><description>Port of #12487 into 2.0

When a node discovers shard content on disk which isn't used, we reach out to all other nodes that supposed to have the shard active. Only once all of those have confirmed the shard active, the shard has no unassigned copies _and_ no cluster state change have happened in the mean while, do we go and delete the shard folder.

Currently, after removing a shard, the IndicesStores checks the indices services if that has no more shard active for this index and if so, it tries to delete the entire index folder (unless on master node, where we keep the index metadata around). This is wrong as both the check and the protections in IndicesServices.deleteIndexStore make sure that there isn't any shard _in use_ from that index. However, it may be the we erroneously delete other unused shard copies on disk, without the proper safety guards described above.

Normally, this is not a problem as the missing copy will be recovered from another shard copy on another node (although a shame). However, in extremely rare cases involving multiple node failures/restarts where all shard copies are not available (i.e., shard is red) there are race conditions which can cause all shard copies to be deleted.

Instead, we should change the decision to clean up an index folder to based on checking the index directory for being empty and containing no shards.
</description><key id="97539940">12494</key><summary>IndicesStore shouldn't try to delete index after deleting a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>bug</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T19:40:12Z</created><updated>2015-07-28T09:08:48Z</updated><resolved>2015-07-27T20:36:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-07-27T20:02:07Z" id="125327140">I reviewed the resurrected findAllShardIds ...

I don't really like how lenient those methods are, checking `Files.isDirectory(location)` when it better be a directory and should already exist (I think?), using `Ints.tryParse`, but we can fix that later, we should just get this bug fix in.

The indexName should never be null here right?  Can we assert that, remove the @Nullable, and fix the javadocs?
</comment><comment author="mikemccand" created="2015-07-27T20:05:47Z" id="125327871">LGTM, thanks @imotov!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Combine YUM Repo into one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12493</link><project id="" key="" /><description>Currently for every "minor release" version change, a new YUM repo is created. For those of us using Enterprise Linux (RHEL, CentOS, Oracle Linux), that means we have to add a new repository for every minor release change.

Could we get a consolidated YUM repository for ElasticSearch RPMs?
</description><key id="97531693">12493</key><summary>Combine YUM Repo into one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dhollinger</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label><label>v2.0.0</label></labels><created>2015-07-27T19:02:10Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2015-10-07T13:04:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dhollinger" created="2015-08-03T20:02:23Z" id="127390193">Might be a good idea to discuss the same thing for Debian/Ubuntu repositories as well.
</comment><comment author="clintongormley" created="2015-08-05T12:44:44Z" id="127984514">@electrical you set it up this way in the beginning.  What was the reasoning back then?
</comment><comment author="electrical" created="2015-08-05T13:13:26Z" id="127993173">A bit of history around this.
We started building the repo's when we were still at 0.90.x and 1.x was coming close.
Since there was so much stuff that would break with upgrading to 1.x from 0.90.x we wanted to avoid users accidentally doing an upgrade and getting 1.x install and breaking everything.
Because of this we decided on separate repo's to avoid accidental breakage.

We continued with this since there was always a chance that if you would upgrade from 1.0.x to 1.3.x for example things could break as well ( java serialization and other things )

Our main concern was ensuring users experience by avoid accidental breakage.

I do agree its not easiest to manage solution but the safest we could think of.
Nobody wants their cluster to break because a sysadmin decides to do an apt-get upgrade :-)
</comment><comment author="dakrone" created="2015-08-05T14:19:36Z" id="128012771">&gt; Nobody wants their cluster to break because a sysadmin decides to do an apt-get upgrade :-)

At the same time, a lot of admins expect that doing an `apt-get upgrade` will give them the latest version of ES with security fixes and all that. I would suggest that it would be better to err on the side of upgrading than leaving an old (insecure) version out there because an admin didn't add a new repo?
</comment><comment author="clintongormley" created="2015-08-05T17:30:36Z" id="128084554">But perhaps we should keep using separate repos for major version upgrades?
</comment><comment author="dakrone" created="2015-08-05T17:45:56Z" id="128088173">&gt; But perhaps we should keep using separate repos for major version upgrades?

+1 to repos for 1.x, 2.x, and 3.x
</comment><comment author="dhollinger" created="2015-08-10T15:24:12Z" id="129493975">@dakrone Same thing on the RHEL side of things, we expect to be able to get the latest updates from a single repo. Especially on the RHEL side where tools like Satellite and Spacewalk are used, we end up combining all the Elasticsearch Repos into a single channel (spacewalk) anyway and good admin practices would be testing the upgrades before pushing to production anyway.

@electrical I see the concern there, but it is the responsibility of the Sys Admins and Engineers to enforce good update policies (meaning don't auto-upgrade without testing), rather than that fall to the Vendor/Developers

+1 to separating by Major release instead of by Minor release
</comment><comment author="dakrone" created="2015-08-10T15:27:51Z" id="129495845">@EagleDelta2 RHEL and debian do this by separating the packages by name, for example, the "maven" package and the "maven2" (old version) package so that they are separated, so the vendor does do this.
</comment><comment author="dhollinger" created="2015-08-10T15:30:22Z" id="129496999">@dakrone For major release updates, yeah.

I'm more concerned with minor releases having different repos - I should have clarified that. 

For example, pulling a new repo after two updates because the minor release changed.... or pulling several new repos because we want to test every version from 1.4.2 (our version) up to latest.
</comment><comment author="dakrone" created="2015-08-10T15:32:13Z" id="129497598">@EagleDelta2 I agree for the major updates, I think there should be a 1.x and 2.x repo, because 2.0.0 would break a lot of things if people do a `dnf update` or `apt-get upgrade` and suddenly jumped from 1.7.1 to 2.0.0, but I do agree someone should be able to have a single repo to upgrade from 1.5.0 to 1.7.1. Admins can specifically pin a version if they don't want it to be upgraded.
</comment><comment author="clintongormley" created="2015-10-06T14:11:37Z" id="145868846">This is now part of our release process - we use the same repo for minor versions, and a new repo for major versions
</comment><comment author="clintongormley" created="2015-10-06T14:24:12Z" id="145872094">Apparently some more tweaks are required - reopening
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update bulk.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12492</link><project id="" key="" /><description>Added an import statement.
</description><key id="97522999">12492</key><summary>Update bulk.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oyiadom</reporter><labels><label>docs</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T18:16:54Z</created><updated>2015-07-28T20:15:24Z</updated><resolved>2015-07-28T20:15:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-28T20:15:24Z" id="125740280">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add integration test for Azure snapshot repository bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12491</link><project id="" key="" /><description>This commit adds an integration test that replicates the Azure snapshot repository bug in elastic/elasticsearch-cloud-azure#51.

Closes elastic/elasticsearch-cloud-azure#100
</description><key id="97519299">12491</key><summary>Add integration test for Azure snapshot repository bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugin Cloud Azure</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T17:59:00Z</created><updated>2015-07-28T14:25:33Z</updated><resolved>2015-07-28T13:46:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-28T09:37:58Z" id="125522999">@jasontedor LGTM, thanks for adding this test.

Once it is merged here, can you please open pull requests in repository elastic/elasticsearch-cloud-azure for 1.6 and 1.7 branches?
</comment><comment author="jasontedor" created="2015-07-28T14:25:33Z" id="125626496">@tlrx Patch applied to the [es-1.6](https://github.com/elastic/elasticsearch-cloud-azure/commit/ba2376b26eafb33c467f09c4628817d52282e3a9) and [es-1.7](https://github.com/elastic/elasticsearch-cloud-azure/commit/1bb77b1c9ca258883df5ec2db23422739d30c1c0) branches of [elastic/elasticsearch-cloud-azure](https://github.com/elastic/elasticsearch-cloud-azure).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more debugging information to the Awareness Decider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12490</link><project id="" key="" /><description>Relates to #12431
</description><key id="97518354">12490</key><summary>Add more debugging information to the Awareness Decider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T17:54:08Z</created><updated>2015-08-25T22:29:18Z</updated><resolved>2015-07-28T19:53:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-28T19:31:17Z" id="125729747">One minor comment. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skip scheduling a reroute when there are no delayed shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12489</link><project id="" key="" /><description>Resolves #12456
</description><key id="97512311">12489</key><summary>Skip scheduling a reroute when there are no delayed shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Cluster</label><label>bug</label></labels><created>2015-07-27T17:19:17Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-28T13:44:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-28T13:44:32Z" id="125615367">Closing, this is an incorrect way to handle this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Add elasticsearch-flavor to list of community plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12488</link><project id="" key="" /><description>Add documentation for [elasticsearch-flavor](https://github.com/hadashiA/elasticsearch-flavor).
It is taking different approach with similar plugin [elasticsearch-taste](https://github.com/codelibs/elasticsearch-taste).
Without the need for batch processing, and returns the result in real time by the REST API.
</description><key id="97499145">12488</key><summary>[DOCS] Add elasticsearch-flavor to list of community plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">hadashiA</reporter><labels><label>docs</label></labels><created>2015-07-27T16:11:40Z</created><updated>2015-08-15T14:05:08Z</updated><resolved>2015-08-15T14:05:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-15T14:05:08Z" id="131384684">thanks @hadashiA 
merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndicesStore shouldn't try to delete index after deleting a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12487</link><project id="" key="" /><description>When a node discovers shard content on disk which isn't used, we reach out to all other nodes that supposed to have the shard active. Only once all of those have confirmed the shard active, the shard has no unassigned copies and no cluster state change have happened in the mean while, do we go and delete the shard folder.

Currently, after removing a shard, the IndicesStores checks the indices services if that has no more shard active for this index and if so, it tries to delete the entire index folder (unless on master node, where we keep the index metadata around). This is wrong as both the check and the protections in IndicesServices.deleteIndexStore make sure that there isn't any shard in use from that index. However, it may be the we erroneously delete other unused shard copies on disk, without the proper safety guards described above.

Normally, this is not a problem as the missing copy will be recovered from another shard copy on another node (although a shame). However, in extremely rare cases involving multiple node failures/restarts where all shard copies are not available (i.e., shard is red) there are race conditions which can cause all shard copies to be deleted.

Instead, we should change the decision to clean up an index folder to be based on checking the index directory for being empty and containing no shards.

Note: this PR is against the 1.6 branch.
</description><key id="97495603">12487</key><summary>IndicesStore shouldn't try to delete index after deleting a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>bug</label><label>resiliency</label><label>v1.6.2</label><label>v1.7.1</label></labels><created>2015-07-27T15:56:38Z</created><updated>2015-08-13T11:50:59Z</updated><resolved>2015-07-27T19:37:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-27T16:08:59Z" id="125257849">LGTM
</comment><comment author="s1monw" created="2015-08-05T15:27:53Z" id="128038593">to me it seems like that some of the safety that has been added here is still prone to concurrency issues. We for instance check `this.indices` in `canDeleteIndexContents` which can be concurrently modified by a createIndex operation so I think there are still races in this code that could cause problems?
</comment><comment author="imotov" created="2015-08-07T21:57:12Z" id="128842081">@s1monw to me it looked like all these calls are performed on updateTask thread so they shouldn't run into concurrency issues. What did I miss?
</comment><comment author="s1monw" created="2015-08-10T14:00:11Z" id="129463419">for instance 

``` Java
public boolean canDeleteIndexContents(Index index, Settings indexSettings) {
        final Tuple&lt;IndexService, Injector&gt; indexServiceInjectorTuple = this.indices.get(index.name());
        if (IndexMetaData.isOnSharedFilesystem(indexSettings) == false) {
            if (indexServiceInjectorTuple == null &amp;&amp; nodeEnv.hasNodeFile()) {
                return true;
            }
        } else {
            logger.trace("{} skipping index directory deletion due to shadow replicas", index);
        }
        return false;
    }
```

checks if the index is present but it could be created concurrently such that it can potentially check before it's created but delete after it's creation was successful? 
</comment><comment author="imotov" created="2015-08-10T14:04:29Z" id="129466780">@s1monw concurrently on which thread?
</comment><comment author="s1monw" created="2015-08-10T14:09:42Z" id="129468391">well we can call thsi API from everywhere I wonder if we should synchronize all these operations? there are pending deletes threads etc that are kicked off so I really thing we should protect that.
</comment><comment author="bleskes" created="2015-08-13T11:50:59Z" id="130636569">+1 to not relaying on the methods to be called from the cluster state update thread. If I understand the concern correctly, It relates to an index being deleted and created concurrently. In that case I _think_ the shard locking in NodeEnvironment will protect us from deleting data. However,  I agree it would be nice to have clear concurrency semantics on this level of the code. Right now we sometimes synchronize and sometimes not which make it hard to reason about- that is potentially a much bigger change and I was trying to make the smallest intervention.  @s1monw if I misunderstood what you said and there is a concrete hole in the logic  - can you open an issue so we won't forget?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose `search_type=scan` as a regular sorted scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12486</link><project id="" key="" /><description>We could remove `search_type=scan` and instead  expose it as a regular scroll sorted by `_doc`. This would reduce the surface form of our APIs, and in the future we could apply further optimizations in case where eg. the index is already sorted by the sort spec?
</description><key id="97481097">12486</key><summary>Expose `search_type=scan` as a regular sorted scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scroll</label><label>enhancement</label></labels><created>2015-07-27T14:49:47Z</created><updated>2015-08-20T12:52:18Z</updated><resolved>2015-08-20T12:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-20T12:52:18Z" id="132997881">Closed via https://github.com/elastic/elasticsearch/pull/12994. Scan are now deprecated in favour of regular scrolls.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Core Lib directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12485</link><project id="" key="" /><description>After #12010 we can remove the core/lib directory.
</description><key id="97479841">12485</key><summary>Remove Core Lib directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T14:44:05Z</created><updated>2015-07-29T13:14:27Z</updated><resolved>2015-07-27T14:53:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-27T14:44:51Z" id="125230892">+1
</comment><comment author="spinscale" created="2015-07-27T14:46:33Z" id="125231392">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid Files.isHidden.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12484</link><project id="" key="" /><description>As Robert pointed out on #12465, it has the undesirable property of relying on
the operating system. So it would be better to use a simple rule such as
checking whether the file name starts with a dot.
</description><key id="97473419">12484</key><summary>Forbid Files.isHidden.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T14:15:19Z</created><updated>2015-07-29T13:14:41Z</updated><resolved>2015-07-27T14:21:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-27T14:18:10Z" id="125222376">+1, this is great. now there is consistent platform-independent logic.
</comment><comment author="xuzha" created="2015-07-27T15:51:33Z" id="125252123">+1 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch shards sporadically failing when running queries against an alias that includes a filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12483</link><project id="" key="" /><description>I am currently running an Elasticsearch v.1.6 setup with two nodes, and an index with 5 shards. To this index, I have added an alias that contains a reference to my index, as well as a Groovy script filter that helps avoiding documents with invalid start and end dates. The alias looks like this:

``` JSON
"MyIndex" : {
   "aliases": { 
       "MyIndex_alias": { 
            "filter" : {
                "script":{ 
                    "script":
                        "\r\n entryDate = doc['availability.start'].date.getMillis();
                         \r\n exitDate =  doc['availability.end'].date.getMillis();
                         \r\n rightNow = DateMidnight.now().getMillis();
                         \r\n\r\n if (entryDate &lt; rightNow &amp;&amp; exitDate &gt; rightNow)\r\n {
                                       \r\n return true;
                             \r\n }
                             \r\n return false;",
                        "lang":"groovy"}}}}}
```

When performing a simple query, such as term \* (/MyIndex_alias/my_type/_search?request.q=*), a number of shards will fail the query returning the following message:

``` JSON
"failed": 1,
"failures": [
    {
        "index": "MyIndex",
        "shard": 2,
        "status": 500,
        "reason": "RemoteTransportException[[MyESCluster][inet[/10.131.43.61:9301]][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[[MyIndex][2]: query[ConstantScore(ScriptFilter(
        entryDate = doc['availability.start'].date.getMillis();
        exitDate = doc['availability.end'].date.getMillis();
        rightNow = DateMidnight.now().getMillis();

        if (entryDate &lt; rightNow &amp;&amp; exitDate &gt; rightNow)
        {
             return true;
        }
        return false;))],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: GroovyScriptExecutionException[ArrayIndexOutOfBoundsException[null]]; "
    }
]
```

I have already verified the following things:
1. All the documents in my index have valid start and end values, and none of them have a null value.
2. The filter does work on the shards that perform the query successfully
3. The cluster has a green health status
4. The error occurs randomly on shards both in the master and worker nodes
5. The errors do not occur if queries are directed to the index instead of the alias
6. Adding more nodes to the cluster seems to mitigate the problem. Running with 2 nodes usually leaves 2-3 shards failing, while 3 nodes limits the failure to 1-2 shards. Adding a final 4th node leaves the query only failing rarely on a single shard
7. Even when the query succeeds, I sometimes get fewer results than expected, again sporadically
   Any help on the matter would be greatly appreciated!
8. Upgrading to Elasticsearch 1.7 doesn't have any effect on the issue
</description><key id="97466050">12483</key><summary>Elasticsearch shards sporadically failing when running queries against an alias that includes a filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kristianholmsejersen</reporter><labels><label>:Aliases</label><label>:Scripting</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-07-27T13:42:49Z</created><updated>2016-05-24T10:29:25Z</updated><resolved>2016-05-24T10:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-30T10:43:38Z" id="126270895">@martijnvg any ideas here?
</comment><comment author="clintongormley" created="2016-01-26T15:17:56Z" id="175070983">Hi @kristianholmsejersen 

Sorry it has taken a long time to get back to you.  Is this something you're still seeing on ES 2 or ES 1.7.4?
</comment><comment author="clintongormley" created="2016-05-24T10:29:25Z" id="221229226">This issue is fairly old and there hasn't been much activity on it. Closing, but please re-open if it still occurs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature : 409 option parameter to retrieve the last document version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12482</link><project id="" key="" /><description>The feature request is to add an optional parameter  during document indexing (index, partial update, update) in order to retrieve if an http error 409: Conflict occur.

In the error, add the content of the last version of document in order to permit (without other request) the conflict resolution.

For Example during an update request

``` shell
curl -XPUT localhost:9200/test/type1/1?version=24&amp;getOnConflict -d '{
    "counter" : 1,
    "tags" : ["red"]
}'
```

the 409 error is completed by _document that contains the document server version

``` shell
{
   "error":"VersionConflictEngineException[[test][2] [type1][1]: version conflict, current [25], provided [24]]",
  "status":409,
  "_document": {
       _id : 1,
      _version: 25,
     _source: {
            "counter" : 7,
            "tags" : ["red"]
     }
  }
}
```
</description><key id="97448707">12482</key><summary>Feature : 409 option parameter to retrieve the last document version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmorille</reporter><labels /><created>2015-07-27T12:10:12Z</created><updated>2015-07-27T12:13:10Z</updated><resolved>2015-07-27T12:13:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T12:13:10Z" id="125183374">Hi @jmorille 

Thanks for opening the issue, but this can be done very easily on the client side.   I don't think we should add yet-another-option to the these APIs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update percolate.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12481</link><project id="" key="" /><description>request type must be set 'POST' when querying with request body.
</description><key id="97448434">12481</key><summary>Update percolate.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skucukbay</reporter><labels /><created>2015-07-27T12:08:19Z</created><updated>2015-07-27T12:10:56Z</updated><resolved>2015-07-27T12:10:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T12:10:42Z" id="125182929">Hi @skucukbay 

Thanks for the PR but GET is perfectly fine, as long as your HTTP client supports GET with a body
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make hidden file detection not rely on the OS environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12480</link><project id="" key="" /><description>We use Files.isHidden in some places already to detect hidden files, but this has the downside to depend on the operating system. Instead we should do something simple like startsWith(".").

See #12465 for more background.
</description><key id="97444782">12480</key><summary>Make hidden file detection not rely on the OS environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-07-27T11:45:55Z</created><updated>2017-03-13T02:31:29Z</updated><resolved>2017-03-13T02:31:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-27T11:52:21Z" id="125177624">Using isHidden, might as well have a if (WINDOWS) in the code, because it will behave differently on the environment. It prevents simple unit testing, it means things will be harder to debug, etc.
</comment><comment author="Helen-Zhao" created="2017-03-13T02:22:38Z" id="286000716">Should this issue be closed? The referenced PR #12465 references #12484 which appears to resolve the issue. </comment><comment author="jasontedor" created="2017-03-13T02:31:27Z" id="286001633">Closed by #12484</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin script fails when memory parameters are defined in environment file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12479</link><project id="" key="" /><description>The plugin script fails to initialize when the the environment file (for DEB installations, /etc/default/elasticsearch) is sourced and contains memory parameters in ES_JAVA_OPTS such as a larger new size.

Example output:

```
$ bin/plugin -r pluginname
Error occurred during initialization of VM
Too small initial heap for new size specified
```

This is caused by the hardcoded Xmx and Xms parameters in the plugin script. Removing the hardcoded values allows the script to execute successfully.

We may be able to just use the default JVM values here instead of hardcoding these values.
</description><key id="97439679">12479</key><summary>Plugin script fails when memory parameters are defined in environment file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Plugins</label><label>bug</label></labels><created>2015-07-27T11:13:36Z</created><updated>2015-09-15T15:06:33Z</updated><resolved>2015-09-15T15:06:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-10T12:49:34Z" id="129430404">steps to reproduce:
- You need java 1.7, does not happen with java 1.8
- set `ES_JAVA_OPTS="-XX:NewSize=256m"` in `/etc/default/elasticsearch`

Then call the plugin manager like

```
# /usr/share/elasticsearch/bin/plugin
Error occurred during initialization of VM
Too small initial heap for new size specified
```

I am not sure if it is a good idea to remove those values, because this means, that you can potentially run the plugin manager with the same heap configured than your elasticsearch process, given you used `-Xms` and `-Xmx` instead of `ES_HEAP_SIZE`, which is never used in the plugin manager

As java 7 is EOL and it works with Java8, I am currently leaning towards closing this, as soon as I figured out, what actually happens in Java8 - as in what values are really getting used and which get ignored.

**Update**: No need to have a package installed, you can simply use JAVA_OPTS on osx as well and call `bin/plugin`
</comment><comment author="spinscale" created="2015-08-10T13:14:40Z" id="129440462">So, using a small java class, that basically prints out `ManagementFactory.getMemoryMXBean().getHeapMemoryUsage()` shows, that java8 is doing the right thing and respects `Xmx` and `Xms`

java8:

```
# java -XX:NewSize=256m -Xmx64m -Xms16m Foo
init = 16777216(16384K) used = 1311456(1280K) committed = 15204352(14848K) max = 45613056(44544K)
# java -Xmx64m -Xms16m Foo
init = 16777216(16384K) used = 872696(852K) committed = 16252928(15872K) max = 59768832(58368K)
```

java7 (obviously)

```
# java -XX:NewSize=256m -Xmx64m -Xms16m Foo
Error occurred during initialization of VM
Too small initial heap for new size specified
# java -Xmx64m -Xms16m Foo
init = 16777216(16384K) used = 480376(469K) committed = 16777216(16384K) max = 59768832(58368K)
```

@jaymode objections against leaving as it is to prevent accidentally crazy high heaps for the plugin manager if people configure `-Xmx` in the `JAVA_OPTS` or `ES_JAVA_OPTS` for Elasticsearch itself?
</comment><comment author="jaymode" created="2015-08-10T13:39:56Z" id="129458489">I'm ok with leaving memory values specified. 

I think the question is can we do anything to make an error less likely if memory values are configured in that option? I can't think of a good option since sourcing the config files has value. Maybe we don't use `ES_JAVA_OPTS` and introduce a `CLI_JAVA_OPTS` or something? 
</comment><comment author="spinscale" created="2015-08-10T13:52:47Z" id="129461016">this also raises an interesting point, if we want to merge the plugin manager into the `BootstrapCliParser` as this means, both have to run with the same memory settings, which might be bad...
</comment><comment author="spinscale" created="2015-08-10T13:59:46Z" id="129463144">so we would need `ES_JAVA_OPTS`, `PLUGIN_JAVA_OPTS` and `COMMON_JAVA_OPTS` or are two options sufficient? Just mapping it out... given the single hit we had with this, I would postpone it for now
</comment><comment author="rjernst" created="2015-08-10T18:22:41Z" id="129556376">why doe the plugin manager need java opts at all?
</comment><comment author="jaymode" created="2015-08-10T18:41:25Z" id="129560880">We've used the ES_JAVA_OPTS as a way to specify a custom conf directory/file location. That was before we started reading those files. 

I think removing both JAVA_OPTS and ES_JAVA_OPTS is ok since we:
1. read the environment configuration files for RPM/DEB now
2. have the ability to specify the options with `--` syntax
</comment><comment author="rjernst" created="2015-08-10T19:00:58Z" id="129566534">Regardless of whether we remove those env settings, I don't see why plugin manager needs them. This is a tiny program that just installs/removes/lists plugins. It should not require setting eg heap size or crazy other java options.
</comment><comment author="jaymode" created="2015-08-10T19:22:59Z" id="129572684">&gt; Regardless of whether we remove those env settings, I don't see why plugin manager needs them.

+1
</comment><comment author="spinscale" created="2015-08-11T12:05:14Z" id="129851344">so, removing `JAVA_OPTS` and `ES_JAVA_OPTS`  settings still pass all integration tests... and works under CentOS, going to create a PR after testing the debian package
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Left over from the `query_cache` to `request_cache` rename</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12478</link><project id="" key="" /><description /><key id="97438918">12478</key><summary>Left over from the `query_cache` to `request_cache` rename</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cache</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-27T11:08:20Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-27T12:42:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-27T11:11:59Z" id="125168219">Thanks for looking after me! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Empty results beyond 10 pages with rescore query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12477</link><project id="" key="" /><description>Hi, 

we've recently implemented rescore to rank docs, I'm getting empty results when I go beyond 10 pages. if I remove rescore from my DSL query it's returning results. So something happening when paginating with rescore. 

For example,  let say my search DSL query has "size" is 1 and "from" is 0. It's returning results until "from"  is  10,  beyond 10 it's returning empty results. 

Elasticsearch version : 1.5.2 

Please check  this issue. 

My sample DSL query for 11th page 

{
  "_source": ["name","id"  ],
  "query": {
    "query_string": {
      "query": "any keyword",
      "fields": ["_all"]
    }
  },
  "from": 11,
  "rescore": {
    "query": {
      "rescore_query": {
        "function_score": {
          "script_score": {
            "script":"[my script]",
            "lang": "groovy"
          }
        }
      },
      "score_mode": "multiply"
    },
    "window_size": 300
  }, "size": 1
}

Sample result for 11th page 

{
"took": 21,
"timed_out": false,
"_shards": {
"total": 11,
"successful": 11,
"failed": 0
},
"hits": {
"total": 14021,
"max_score": 1,
"hits": [ ]
}
}
</description><key id="97438058">12477</key><summary>Empty results beyond 10 pages with rescore query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">parthibane</reporter><labels /><created>2015-07-27T11:01:57Z</created><updated>2015-07-27T12:01:54Z</updated><resolved>2015-07-27T12:01:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-27T11:49:40Z" id="125177110">You are going to have more luck if you can post a curl recreation.
On Jul 27, 2015 7:02 AM, "parthibane" notifications@github.com wrote:

&gt; Hi,
&gt; 
&gt; we've recently implemented rescore to rank docs, I'm getting empty results
&gt; when I go beyond 10 pages. if I remove rescore from my DSL query it's
&gt; returning results. So something happening when paginating with rescore.
&gt; 
&gt; For example, let say my search DSL query has size is 1 and from is 0. It's
&gt; returning results until from is 10 beyond 10 it's returns empty results.
&gt; 
&gt; Elasticsearch version : 1.5.2
&gt; 
&gt; Please check this issue.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12477.
</comment><comment author="clintongormley" created="2015-07-27T12:01:53Z" id="125180252">Fixed by https://github.com/elastic/elasticsearch/pull/11342
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused methods hitsExecutionNeeded and hitsExecute from FetchSubPhase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12476</link><project id="" key="" /><description>The hits execution was never used by any sub fetch phase.
</description><key id="97437730">12476</key><summary>Remove unused methods hitsExecutionNeeded and hitsExecute from FetchSubPhase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search</label></labels><created>2015-07-27T10:59:43Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-27T11:39:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-27T11:23:27Z" id="125171775">Not sure, they might come handy when we want to address #11223 aka highlight all documents at once. On the other hand we can always restore them or add some similar mechanism when needed in the future. This was also exposed to plugins right? So who knows if any plugins is using this or not.
</comment><comment author="brwe" created="2015-07-27T11:39:29Z" id="125174834">&gt; This was also exposed to plugins right? So who knows if any plugins is using this or not.

We could not plug in fetch phases in so far so I think that no plugin can use this.

&gt; Not sure, they might come handy when we want to address #11223 aka highlight all documents at once. 

Indeed, it might help there. I also wonder if it might help for https://github.com/elastic/elasticsearch/issues/12413 together with the pluggable fetch phases in #12400 . On the other hand I have the feeling that we though 'this might be handy' for four years now... 

I am not passionate about it but since it is not a huge improvement and this might be handy at some point, I'll just leave all as is and close the pr again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup unit tests in GatewayMetaStateTests and add test for closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12475</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/pull/11666 we found a bug in GatewayMetaState when indices are closed and opened, see e44c5ff70384751bfa9bd5224183c1bc8c68daa0 . But we added no unit test for it. Also, GatewayMetaStateTests can probably be simplified. The current way to create a cluster state is a mess.
</description><key id="97435403">12475</key><summary>Cleanup unit tests in GatewayMetaStateTests and add test for closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Cluster</label><label>adoptme</label><label>test</label></labels><created>2015-07-27T10:44:54Z</created><updated>2017-03-21T15:18:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-07-29T15:59:51Z" id="125998833">closed by #11666
</comment><comment author="brwe" created="2015-07-29T16:00:14Z" id="125998993">no it is not...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proto: Separating QueryParseContext and QueryGenerationContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12474</link><project id="" key="" /><description>The parse() method we are currently splitting into a query parsing part (fromXContent()) and a lucene query generating part (toQuery()) in the feature refactoring branch (#10217) takes `QueryParseContext` as an argument. At some point we need to excute these two steps in different phases on the coordinating node and the shards. This protoype PR tries to anticipate this by introducing a new QueryGenerationContext object as the argument of the query-generating methods (toQuery() and all dependent methods). As a first stept this new context simply wraps the old context object and delegated all calls made to it to the inner object. In subsequent steps we can then try to separate the different behaviour needed in the two context objects. 

PR goes against query refactoring branch.
</description><key id="97425371">12474</key><summary>Proto: Separating QueryParseContext and QueryGenerationContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-27T09:45:18Z</created><updated>2016-03-11T11:51:02Z</updated><resolved>2015-07-29T11:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-07-27T09:47:49Z" id="125149369">The approach taken here is just a WIP version intended for discussion, I'm currently also investigating if it is easier to got the other way round, renaming QueryParseContext to QueryGenerationContext (or similar) first and then replacing the former use in the parsing step (fromXContent) by a new, light-weight parsing context. 
</comment><comment author="javanna" created="2015-07-27T10:21:56Z" id="125157381">I like the change, left a few minor comments. I do think that doing it the other way around would probably be easier. You could rename QueryParseContext to QueryGenerationContext and have a new QueryParseContext with only what we need there. What I don't love here for now is that we don't clean up the QueryParseContext just yet, while many things should simply be moved to QueryGenerationContext (that said we might need to wait for that till we are done with all queries). Also, naming... not a big fan of the "generation" naming, that said I have to see if I can come up with something that I like better :)
</comment><comment author="cbuescher" created="2015-07-27T11:14:20Z" id="125169120">@javanna thanks, before adressing the comments and moving on here, I first want to try to tackle this refactoring the other way round like I initially mentioned and you also seem to prefer. 
</comment><comment author="cbuescher" created="2015-07-29T11:31:25Z" id="125924831">Would like to close this in favour of #12527 12527
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] remove createExpectedQuery from BaseQueryTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12473</link><project id="" key="" /><description>We currently test the toQuery method by comparing its return value with the output of createExpectedQuery. This seemed at first like a good deep test for the lucene generated queries, but ends up causing a lot of code duplication between tests and prod code, which smells like this test is not that useful after all.

This commit removes the requirement for createExpectedQuery, we test a bit less the lucene generated queries, but we do still have a testToQuery method in BaseQueryTestCase. This test acts as smoke test to verify that there are no issues with our toQuery method implementation for each query: it verifies that two equal queries should result in the same lucene query. It also verifies that changing the boost value of a query affects the result of the toQuery.

Part of the previous createExpectedQuery can be moved to assertLuceneQuery using normal assertions, we do it when it makes sense only though.

This commit also adds support for boost and queryName to SpanMultiTermQueryParser and SpanMultiTermQueryBuilder, plus it removes no-op setters for queryName and boost from QueryFilterBuilder. Instead we simply declare in our test infra that query filter doesn't support boost and queryName and we disable tests aroudn these fields in that specific case. Boost and queryName support is also moved down to the QueryBuilder interface.
</description><key id="97423224">12473</key><summary>[TEST] remove createExpectedQuery from BaseQueryTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>test</label></labels><created>2015-07-27T09:32:22Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-28T12:45:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-07-27T11:00:21Z" id="125165895">Left a few comments, in general prefer the idea here of not testing the generated queries to deeply in our unit tests here since this should happen (and already happens) in intergration tests elsewhere. However not that we are back to the beginning where we wrote individual assertions for the generation lucene queries in `assertLuceneQuery` we will again have to decide in each individual test which level of detail we want to test there. Maybe some general guideline here would help (maybe in the docs), e.g. the method should at least test generated query class type and if possible the effect some builder parameters have on the resulting query, or nested query collection size etc... This should just serve to shorten discussion when working on susequent query tests.
</comment><comment author="javanna" created="2015-07-27T15:01:51Z" id="125237935">thanks for the review @cbuescher I addressed your comments. As for the need for guidelines...I think what we test will depend on the query, it can be discussed case by case as part of the review process. We do want to try and test as much as possible, but we don't want to end up writing useless tests as we did up until now :) the trade-off really depends on the query.
</comment><comment author="cbuescher" created="2015-07-28T09:25:34Z" id="125518354">Went through the changes again, LGTM now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw error when a pipeline agg references an incompatible agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12472</link><project id="" key="" /><description>An exception will now be thrown if a pipeline aggregation specifies an Aggregation path which includes an incompatible aggregation (an aggregation which is not either a numeric metric aggregation or a single bucket aggregation). To support this change a marker interface (AggregationPathCompatibleFactory) has been added.

Closes #12360
</description><key id="97416220">12472</key><summary>Throw error when a pipeline agg references an incompatible agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-07-27T08:53:54Z</created><updated>2015-07-28T07:58:42Z</updated><resolved>2015-07-27T08:58:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-27T08:58:41Z" id="125134536">This change doesn't work because some pipeline aggs (e.g. max_bucket) can reference multi-bucket aggregations.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add BWC test for reading legacy global cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12471</link><project id="" key="" /><description>We currently have tests that make sure that we can open, read and search indices from the previous major version. That also validates we can read the meta data associated with an index and a shard. It does not cover reading metadata on the cluster level (i.e., cluster settings). We should add a test for that.
</description><key id="97400238">12471</key><summary>Add BWC test for reading legacy global cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>adoptme</label><label>test</label></labels><created>2015-07-27T07:06:22Z</created><updated>2017-03-21T15:18:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add support for DFS_SCAN search type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12470</link><project id="" key="" /><description>If you use `track_scores=true` and `search_type=scan`, then there is no way to get the improved term frequencies for improved scoring like you can with `search_type=dfs_query_then_fetch`.

This is admittedly an uncommon use case, given that scores are disabled by default for scoring and sorting is always disabled, but for scenarios where granularity is important, then it is a small hole.

In terms of the proposed solution:
#### Problem

There is no `SearchType.DFS_SCAN` option.
#### Solution

Provide the option and perform the traditional DFS lookup. _If_ `track_scores` is not enabled, then revert to normal `SearchType.SCAN`.
#### Workaround

None that I am aware of.
</description><key id="97390918">12470</key><summary>Add support for DFS_SCAN search type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Scroll</label><label>discuss</label></labels><created>2015-07-27T05:38:19Z</created><updated>2015-07-27T14:53:03Z</updated><resolved>2015-07-27T11:50:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T11:50:34Z" id="125177278">Scanning doesn't support scoring, so the DFS option is meaningless.  If you need scoring, use scrolling without scanning.  It no longers suffers from the issues with deep paging that exists with high `from` values
</comment><comment author="pickypg" created="2015-07-27T14:53:03Z" id="125234953">Related to #12486. Using scroll is definitely the way to achieve this and it can be combined with `dfs_query_then_fetch`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add line feed to responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12469</link><project id="" key="" /><description>There are a number of API responses that don't come back with a line feed, which messes up CLI actions. Here's one such example;

```
$ curl -uadmin 0:9200
Enter host password for user 'admin':
{"error":"AuthenticationException[unable to authenticate user [admin] for REST request [/]]","status":401}$
```

And it's pretty common.

Can we make sure we add a `\n` to everything to increase readability?
</description><key id="97354724">12469</key><summary>Add line feed to responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>enhancement</label></labels><created>2015-07-26T22:21:48Z</created><updated>2015-07-27T11:46:12Z</updated><resolved>2015-07-27T11:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-26T23:23:44Z" id="125047750">IIRC if you add ?pretty then a line feed is added. Would this work for you?
</comment><comment author="clintongormley" created="2015-07-27T11:46:11Z" id="125176095">Agreed - API default outputs are for computers.  Use pretty for readable responses
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcard searching on not analyzed fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12468</link><project id="" key="" /><description>Related to Issue #9973

I'm noticing wildcard searching is not working on not_analyzed fields or keyword analyzer analyzed fields

For Eg:-

PUT /exactsearchindex
{
    "mappings": {
        "my_type": {
          "properties": {
                "text": {
                  "type": "string",
                  "index": "not_analyzed"
          }
        }
    }
}
}

POST /exactsearchindex/my_type/_bulk
{ "index": { "_id": 1 }}
{ "text": "The quick fox jumping over the walls." }
{ "index": { "_id": 2 }}
{ "text": "Cats overboard casting." }

POST /exactsearchindex/_search
{
  "query": {
    "query_string": {
      "analyze_wildcard": true, 
      "query": "cats ov*",
      "fields": ["text"]
}
}
}

The above search does not return any results. Am I missing something?
</description><key id="97350163">12468</key><summary>Wildcard searching on not analyzed fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet85</reporter><labels /><created>2015-07-26T21:01:10Z</created><updated>2015-07-26T23:07:38Z</updated><resolved>2015-07-26T23:07:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-26T21:03:09Z" id="125040266">Maybe it's case getting to you. Try adding a lowercase filter after the
keyword.
On Jul 26, 2015 5:01 PM, "vineet85" notifications@github.com wrote:

&gt; Related to Issue #9973
&gt; https://github.com/elastic/elasticsearch/issues/9973
&gt; 
&gt; I'm noticing wildcard searching is not working on not_analyzed fields or
&gt; keyword analyzer analyzed fields
&gt; 
&gt; For Eg:-
&gt; 
&gt; PUT /exactsearchindex
&gt; {
&gt; "mappings": {
&gt; "my_type": {
&gt; "properties": {
&gt; "text": {
&gt; "type": "string",
&gt; "index": "not_analyzed"
&gt; }
&gt; }
&gt; }
&gt; }
&gt; }
&gt; 
&gt; POST /exactsearchindex/my_type/_bulk
&gt; { "index": { "_id": 1 }}
&gt; { "text": "The quick fox jumping over the walls." }
&gt; { "index": { "_id": 2 }}
&gt; { "text": "Cats overboard casting." }
&gt; 
&gt; POST /exactsearchindex/_search
&gt; {
&gt; "query": {
&gt; "query_string": {
&gt; "analyze_wildcard": true,
&gt; "query": "cats ov*",
&gt; "fields": ["text"]
&gt; }
&gt; }
&gt; }
&gt; 
&gt; The above search does not return any results. Am I missing something?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12468.
</comment><comment author="vineet85" created="2015-07-26T22:54:16Z" id="125046670">Tried the same search above with the keyword tokenizer and lowecase filter:-
PUT /exactsearchindex
{
  "settings": {
        "analysis": {
            "analyzer": {
                "keylower": {
                    "type": "custom",
                    "tokenizer": "keyword",
                    "filter": ["lowercase"]
                }
            }
        }
    },
    "mappings": {
        "my_type": {
          "properties": {
                "text": {
                  "type": "string",
                  "analyzer": "keylower"
          }
        }
    }
}
}

I am able to find Cats\* as below
POST /exactsearchindex/_search
{
  "query": {
    "query_string": {
      "query": "Cats*",
      "fields": ["text"]
}
}
}

But the movement the whitespace is introduced it does not work:-
POST /exactsearchindex/_search
{
  "query": {
    "query_string": {
      "query": "Cats overboard*",
      "fields": ["text"]
}
}
}
</comment><comment author="nik9000" created="2015-07-26T23:02:39Z" id="125047002">Ah! The trouble is query_string. It always always segments on spaces. Use
the prefix query instead.

Sorry about that!
On Jul 26, 2015 6:54 PM, "vineet85" notifications@github.com wrote:

&gt; Tried the same search above with the keyword tokenizer and lowecase
&gt; filter:-
&gt; PUT /exactsearchindex
&gt; {
&gt; "settings": {
&gt; "analysis": {
&gt; "analyzer": {
&gt; "keylower": {
&gt; "type": "custom",
&gt; "tokenizer": "keyword",
&gt; "filter": ["lowercase"]
&gt; }
&gt; }
&gt; }
&gt; },
&gt; "mappings": {
&gt; "my_type": {
&gt; "properties": {
&gt; "text": {
&gt; "type": "string",
&gt; "analyzer": "keylower"
&gt; }
&gt; }
&gt; }
&gt; }
&gt; }
&gt; 
&gt; I am able to find Cats\* as below
&gt; POST /exactsearchindex/_search
&gt; {
&gt; "query": {
&gt; "query_string": {
&gt; "query": "Cats*",
&gt; "fields": ["text"]
&gt; }
&gt; }
&gt; }
&gt; 
&gt; But the movement the whitespace is introduced it does not work:-
&gt; POST /exactsearchindex/_search
&gt; {
&gt; "query": {
&gt; "query_string": {
&gt; "query": "Cats overboard*",
&gt; "fields": ["text"]
&gt; }
&gt; }
&gt; }
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12468#issuecomment-125046670
&gt; .
</comment><comment author="vineet85" created="2015-07-26T23:07:38Z" id="125047166">Figured out the problem. Thanks for your suggestions
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geohash is not indexed unless the precision is 12</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12467</link><project id="" key="" /><description>If the `geohash_precision` is set to anything except the default (12), and `geohash_prefix` is not true, the geohash is not indexed:

```
PUT my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "location": {
          "type": "geo_point",
          "geohash": true,
          "geohash_precision": 11
        }
      }
    }
  }
}


PUT my_index/my_type/1
{
  "location": {
    "lat": 41.12,
    "lon": -71.34
  }
}

GET my_index/_search?fielddata_fields=location.geohash
```
</description><key id="97335102">12467</key><summary>Geohash is not indexed unless the precision is 12</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Geo</label><label>bug</label><label>v2.0.0-rc1</label></labels><created>2015-07-26T17:17:46Z</created><updated>2015-10-01T11:20:33Z</updated><resolved>2015-09-23T14:39:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-09-17T22:12:53Z" id="141250528">This is a silly mappings refactor mistake... the bigger issue is that we didn't have a test for catching this. I'll backport and make sure there's sufficient test coverage.
</comment><comment author="nknize" created="2015-09-23T14:39:28Z" id="142621566">Merged #13649 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a script or command to recover red cluster after data loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12466</link><project id="" key="" /><description>Assuming a user can get into a red cluster after losing data, allow some easy way to accept the data loss and return the cluster into a working state.

Currently, it's done with some ugly scripts (or complete cluster destroy and rebuild) that take a long time and are sub-optimal like:
https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html

Would be good if it's in an API that plugins like kopf, HQ etc will later allow one click fix.
</description><key id="97309744">12466</key><summary>Provide a script or command to recover red cluster after data loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofir-petrushka</reporter><labels /><created>2015-07-26T11:25:28Z</created><updated>2015-07-27T11:43:45Z</updated><resolved>2015-07-27T11:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T11:43:45Z" id="125175612">Closing in favour of #4285
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skip hidden files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12465</link><project id="" key="" /><description>Quick fix, skip hidden files when loading plugins. [https://github.com/elastic/elasticsearch/issues/12433]
</description><key id="97297581">12465</key><summary>Skip hidden files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-26T08:19:43Z</created><updated>2015-08-18T06:11:06Z</updated><resolved>2015-07-27T08:06:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-27T08:07:24Z" id="125117741">Thanks @xuzha !
</comment><comment author="rmuir" created="2015-07-27T11:30:11Z" id="125173037">I do not think we should use Files.isHidden, anywhere in our code.

This is too specific to the OS environment. This should be looking for a match with .DS_Store or startsWith(".") but not looking at any os or filesystem-specific attributes.

If we start going this route, then things are going to get confusing quickly.
</comment><comment author="jpountz" created="2015-07-27T11:46:10Z" id="125176089">For the record, I merged this pull request because I saw we were already ignoring hidden files this way in other places, but I'm good with changing hidden file detection to startsWith(".").

I opened #12480
</comment><comment author="xuzha" created="2015-07-27T15:52:41Z" id="125252388">That's a good point. Thanks Robert.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename `lat_lon` to `index` on geo-points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12464</link><project id="" key="" /><description>The `lat_lon` option on geo-point indexes the lat and lon values in the inverted index.  For consistency's sake, it should be renamed to `index`.
</description><key id="97262385">12464</key><summary>Rename `lat_lon` to `index` on geo-points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Geo</label><label>:Mapping</label><label>discuss</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-07-25T22:18:25Z</created><updated>2016-05-03T14:23:03Z</updated><resolved>2016-05-03T14:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-25T22:18:56Z" id="124906025">@nknize what do you think?
</comment><comment author="nknize" created="2015-07-26T02:23:12Z" id="124930918">I'm afraid it might add confusion once  the enhancement/LuceneGeoPointField_Integration feature branch is merged (was planning for 2.0_beta). This enhancement automatically indexes geo_point types using the new Lucene GeoPointFieldType. I had intended to keep the lat_lon option (for the time being) for bwc but eventually remove it. For consistency I can use the 'index' option to let users turn off the default indexing behavior? 
</comment><comment author="clintongormley" created="2015-07-26T16:38:34Z" id="125014837">This new field will still be `geo_point`?  If so, and you're planning on removing the `lat_lon` option anyway, then leave it as is.
</comment><comment author="clintongormley" created="2016-05-03T14:23:03Z" id="216544415">No longer relevant
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndicesStore shouldn't try to delete index after deleting a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12463</link><project id="" key="" /><description>When a node discovers shard content on disk which isn't used, we reach out to all other nodes that supposed to have the shard active. Only once all of those have confirmed the shard active, the shard has no unassigned copies _and_ no cluster state change have happened in the mean while, do we go and delete the shard folder.

Currently, after removing a shard, the IndicesStores checks the indices services if that has no more shard active for this index and if so, it tries to delete the entire index folder (unless on master node, where we keep the index metadata around). This is wrong as both the check and the protections in IndicesServices.deleteIndexStore make sure that there isn't any shard _in use_ from that index. However, it may be the we erroneously delete other unused shard copies on disk, without the proper safety guards described above.

 Normally, this is not a problem as the missing copy will be recovered from another shard copy on another node (although a shame). However, in extremely rare cases involving multiple node failures/restarts where all shard copies are not available (i.e., shard is red) there are race conditions which can cause all shard copies to be deleted.

 Instead, we should change the decision to clean up an index folder to be based on checking the index directory for being empty and containing no shards.

Note: this PR is against the 1.6 branch.
</description><key id="97252233">12463</key><summary>IndicesStore shouldn't try to delete index after deleting a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>resiliency</label></labels><created>2015-07-25T20:15:48Z</created><updated>2015-07-28T08:04:30Z</updated><resolved>2015-07-27T15:58:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-27T12:35:26Z" id="125190762">@bleskes This bug is sneaky. LGTM. Left a question about deleteIndexStore() itself, but that shouldn't block this PR.
</comment><comment author="dakrone" created="2015-07-27T14:12:02Z" id="125220125">Left a couple a pretty minor comments.
</comment><comment author="imotov" created="2015-07-27T15:58:09Z" id="125254092">I am taking over this PR and because I cannot push into @bleskes's branch I opened a new PR #12487. I am closing this one. Let's continue the discussion on the new PR. @dakrone could you take a look?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot repository failling HEAD request during start-up is not available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12462</link><project id="" key="" /><description>This manifests as `{"error":"RepositoryMissingException[[test-repo] missing]","status":404}`-errors if the repository is attempted used.

This behaviour is problematic in case a service has an intermittent hiccup just as Elasticsearch is starting (i.e S3 having some issues at the API level, temporary networking issues and so on).

Here's the stack trace from the instance start:

```
[2015-07-25 20:52:16,198][WARN ][repositories             ] [Red Shift] failed to create repository [s3][test-repo]
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, com.amazonaws.AmazonClientException: Unable to execute HTTP request: Connection refused
  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.s3.S3Repository
  while locating org.elasticsearch.repositories.Repository

1 error
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:407)
    at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:302)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: com.amazonaws.AmazonClientException: Unable to execute HTTP request: Connection refused
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:473)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:297)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3672)
    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1079)
    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1042)
    at org.elasticsearch.cloud.aws.blobstore.S3BlobStore.&lt;init&gt;(S3BlobStore.java:74)
    at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(S3Repository.java:125)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    ... 11 more
Caused by: java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:589)
    at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:117)
    at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:177)
    at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:304)
    at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:611)
    at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:446)
    at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863)
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57)
    at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:701)
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:462)
    ... 35 more
[2015-07-25 20:52:16,205][WARN ][repositories             ] [Red Shift] failed to create repository [test-repo]
org.elasticsearch.repositories.RepositoryException: [test-repo] failed to create repository
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:414)
    at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:302)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, com.amazonaws.AmazonClientException: Unable to execute HTTP request: Connection refused
  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.s3.S3Repository
  while locating org.elasticsearch.repositories.Repository

1 error
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:407)
    ... 7 more
Caused by: com.amazonaws.AmazonClientException: Unable to execute HTTP request: Connection refused
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:473)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:297)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3672)
    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1079)
    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1042)
    at org.elasticsearch.cloud.aws.blobstore.S3BlobStore.&lt;init&gt;(S3BlobStore.java:74)
    at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(S3Repository.java:125)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    ... 11 more
Caused by: java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:589)
    at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:117)
    at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:177)
    at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:304)
    at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:611)
    at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:446)
    at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863)
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57)
    at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:701)
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:462)
    ... 35 more
```
</description><key id="97247685">12462</key><summary>Snapshot repository failling HEAD request during start-up is not available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels><label>:Plugin Cloud AWS</label><label>:Snapshot/Restore</label><label>adoptme</label><label>v2.0.0-beta1</label></labels><created>2015-07-25T19:10:43Z</created><updated>2015-08-13T12:43:06Z</updated><resolved>2015-08-13T11:42:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-07-26T06:00:25Z" id="124949560">I think we need add backup and retry for this [client.doesBucketExist(bucket](https://github.com/elastic/elasticsearch/blob/master/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java#L74)
</comment><comment author="clintongormley" created="2015-07-27T11:42:22Z" id="125175360">@tlrx @dadoonet any thoughts on this?
</comment><comment author="tlrx" created="2015-07-27T11:45:40Z" id="125175987">@clintongormley I agree with @xuzha, it shouldn't be hard to do.
</comment><comment author="xuzha" created="2015-07-28T01:32:55Z" id="125401285">I think I'm going to create another quick PR for this. And I could do another refactor for this.
</comment><comment author="xuzha" created="2015-07-28T16:56:55Z" id="125678526">I think the last PR is quite straightforward. Please help me review, I think we need this.
</comment><comment author="alexbrasetvik" created="2015-07-28T17:47:14Z" id="125691723">I'd love to have this fix in 1.7 as well.
</comment><comment author="clintongormley" created="2015-08-13T11:42:01Z" id="130633511">Closed by #12498
</comment><comment author="nkvoll" created="2015-08-13T12:43:06Z" id="130656462">Do I understand it correctly if the current fix just adds retries? What if the repository is down for an hour after the cluster starts, but the snapshot/restore API is not used during this time?

As a user, I'd expect it to be able to work lazily, i.e, it's fine to log that it's not available during startup etc, but if it's available when it's used, I'd expect it to "just work".
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate `include_in_root` and `include_in_parent`?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12461</link><project id="" key="" /><description>Does any purpose remain for the `include_in_root` and `include_in_parent` settings for `nested` fields, now that we have `nested` and `reverse_nested` aggregations, and inner hits with highlighting?

If not, we should remove them.
</description><key id="97246678">12461</key><summary>Deprecate `include_in_root` and `include_in_parent`?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>:Nested Docs</label><label>deprecation</label><label>discuss</label></labels><created>2015-07-25T18:52:16Z</created><updated>2017-07-03T13:31:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-26T07:47:09Z" id="124953800">These options also allow to form simple (fast) queries on the parent instead of slower nested queries. We could potentially remove these options, but that would mean that we would rely on users to duplicate information on the parent/root document explicitely at index time.
</comment><comment author="jpountz" created="2015-09-27T22:19:22Z" id="143598490">Actually maybe it's not necessary and users should just do it explicitly with `copy_to` instead of implicitely with `include_in_*`.
</comment><comment author="adichad" created="2015-11-03T02:35:23Z" id="153220584">2.0 gives the following exception on trying to put an existing mapping that worked with 1.7.x:
`org.elasticsearch.index.mapper.MapperParsingException: Mapping definition for [title] has unsupported parameters:  [include_in_root : true]`
and also fails to install the mapping.
so maybe this is already merged in 2.0.0 and can be closed? Also would be helpful to mention this and the recommended alternative approach (copy_to) with an example in the documentation. 
The migration tool did not catch this either.
</comment><comment author="clintongormley" created="2015-11-08T19:01:34Z" id="154857862">@adichad I think you have the parameter in the wrong place, these params still work in 2.0:

```
PUT my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "foo": {
          "type": "nested",
          "include_in_root": true,
          "properties": {
            "bar": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-11-20T15:05:22Z" id="158425399">I really like the idea of removing `include_in_root|parent` in favour of the more explicit `copy_to` per field.  This way there is no magic, and you only (re-)index the fields that you really want at the top level.
</comment><comment author="jpountz" created="2015-12-22T21:24:30Z" id="166733546">I was thinking about it more, one downside of explicit copy_to compared to include_in_all is that it would force to use a different field name, which is worse in terms of index sparsity.
</comment><comment author="tbragin" created="2016-02-10T22:06:37Z" id="182601432">@clintongormley @jpountz 

For tracking purposes, it may be worth noting that we sometimes recommend using "include_in_parent" when someone wants to summarize data stored in nested docs within Kibana using aggregations, for instance, using the Terms aggregation to get Top N values across a field in a nested doc: https://github.com/elastic/kibana/issues/1084#issuecomment-112162113

This approach was also used to develop the Watch History dashboard, since it was based on a nested document structure: https://www.elastic.co/guide/en/watcher/current/watch-history.html

Hopefully the "copy to" functionality being discussed will be an equivalent alternative to that use case. It sounds like it will be and in fact, the added benefit is that you only have to reindex the affected fields, as opposed to all of the data, is that correct?

cc: @rashidkpc @skearns64 @eskibars 
</comment><comment author="clintongormley" created="2016-02-13T12:54:22Z" id="183660907">&gt; Hopefully the "copy to" functionality being discussed will be an equivalent alternative to that use case. It sounds like it will be and in fact, the added benefit is that you only have to reindex the affected fields, as opposed to all of the data, is that correct?

The `copy_to` functionality already exists and behaves as per your comment.
</comment><comment author="abulhol" created="2016-02-18T15:28:09Z" id="185774598">I vote in favor of keeping include_in_parent and include_in_root. For complex use cases, nested queries with inner_hits are suited much better, but I think there are use cases (like aggregations, see the Kibana examples) where it is much more convenient to grab the data from the root node. 
</comment><comment author="rjernst" created="2016-02-18T18:41:50Z" id="185853662">I think what Clint suggested would still maintain the functionality of include_in_parent/root, but it would be moved to `copy_to`. I like this simplification of the api, even if the underlying complexity is still there.
</comment><comment author="sronsiek" created="2016-04-26T10:03:21Z" id="214690881">I have not been able to drop-in-replace 'include_in_parent' with 'copy_to'  with nested objects (maintaining the same fieldname in 'parent' as that of the nested object). This may be because I have not found detailed documentation that describes the copy_to value args, but if it is not possible I would request not to remove a feature for which there is no full replacement. 
</comment><comment author="clintongormley" created="2016-04-26T18:16:28Z" id="214835037">@sronsiek yeah you wouldn't be able to use the same field name in the parent with copy_to.  Why is that important?  What's your use case?
</comment><comment author="smmckay" created="2016-09-30T19:45:47Z" id="250835429">I dunno about that guy, but our use case is "a not very thoughtful person set `include_in_parent` and `include_in_root` on _all_ of our (many) mappings, this is now routinely used across the codebase, and we also aggregate on the nested docs". So changing the field type is not an option and changing the queries is not feasible.
</comment><comment author="apidruchny" created="2017-01-30T16:37:38Z" id="276113180">Looks like include_in_parent and include_in_root are undocumented settings starting from v2.0. Should we add them back to the documentation? Is it a deliberate decision to not document them?</comment><comment author="jpountz" created="2017-02-21T13:40:35Z" id="281346967">I used to see value in `include_in_root` and `include_in_parent` due to the fact that they would reuse the same underlying field instead of creating a new sparse field. However due to upcoming improvements with sparse doc values in Lucene 7 and recent improvements to `nested` queries when `include_in_root` and `include_in_parent` are disabled (#23079), I am leaning towards deprecating `include_in_root` and `include_in_parent` in favour of `copy_to`.</comment><comment author="aarnaout" created="2017-06-27T16:28:43Z" id="311412130">The only way I got nested fields to work in Kibana was using "include_in_parent", please keep it</comment><comment author="clintongormley" created="2017-06-29T17:09:40Z" id="312031454">&gt; The only way I got nested fields to work in Kibana was using "include_in_parent", please keep it

@aarnaout For this purpose, `copy_to` would work just as well</comment><comment author="IdanWo" created="2017-07-01T13:03:30Z" id="312431120">Hey there. 

1. After reading the comments of this issue, I understand that `copy_to` is equivalent to `include_in_parent` and `include_in_root`. I think it worth mentioning in the [copy_to](https://www.elastic.co/guide/en/elasticsearch/reference/current/copy-to.html) section in the ElasticSearch documentation.
2. The classic use case of nested types, is when to root document has a a variety (but limited) nested documents in its hierarchy (something like between 0 to 25 nested documents I guess). Therefore, making a terms query or exists query about a field copied to the root document from all nested documents might be much faster than making a nested query - and really narrow down the documents being aggregated in my use case. I think it would be really nice to mention that in the "Document modeling" section in [tune for search speed](https://www.elastic.co/guide/en/elasticsearch/reference/5.4/tune-for-search-speed.html) documentation, in case that you agree.
3. Actually there is also a recent [question ](https://discuss.elastic.co/t/include-in-root-deprecated/91508) about this issue in the Elastic forum.
4. I know that bool query clauses making an optimization to choose which conditions to start with first. My question is: is it smart enough to first run the conditions in the root document and only then run the conditions involve nested query? Did ElasticSearch 5.4 nested improvements also take that under account?</comment><comment author="jpountz" created="2017-07-03T13:31:25Z" id="312646309">&gt; After reading the comments of this issue, I understand that copy_to is equivalent to include_in_parent and include_in_root. I think it worth mentioning in the copy_to section in the ElasticSearch documentation.

The only difference is that `include_in_*` reuse the same field name, which makes a difference to Lucene. Especially with Lucene 6 and earlier versions (which map to all Elasticsearch version up to 5.x included) since it does not like sparse fields that have norms or doc values. However, things are becoming better in the upcoming Lucene 7 so we might want to consider replacing `include_in_*` with a regular `copy_to`.

&gt; Therefore, making a terms query or exists query about a field copied to the root document from all nested documents might be much faster than making a nested query [...] I think it would be really nice to mention that in the "Document modeling" section in tune for search speed documentation

+1 !

&gt; I know that bool query clauses making an optimization to choose which conditions to start with first. My question is: is it smart enough to first run the conditions in the root document and only then run the conditions involve nested query? Did ElasticSearch 5.4 nested improvements also take that under account?

It used to do that before 5.4 already, but 5.4 improved things so that it also works well in the case that a script, phrase or range query occurs under the `nested` query.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_only_nodes` preference parsed incorrectly </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12460</link><project id="" key="" /><description> Fix for https://github.com/elastic/elasticsearch/issues/12389
</description><key id="97222388">12460</key><summary>`_only_nodes` preference parsed incorrectly </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirmalc</reporter><labels><label>:Search</label><label>bug</label><label>v1.7.1</label></labels><created>2015-07-25T12:23:28Z</created><updated>2015-08-07T10:06:50Z</updated><resolved>2015-07-28T11:05:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T11:40:09Z" id="125174968">@nirmalc any chance we can have a test for this?  If we'd had a test, the bug would have been caught during the backport

thanks
</comment><comment author="nirmalc" created="2015-07-27T14:43:10Z" id="125230042">Will try if i can add test to the Preferences class - as of now we dont have tests for any of that , At the minumum - we should be able to add tests for those static methods there.
</comment><comment author="clintongormley" created="2015-07-28T11:05:07Z" id="125559377">Merged in https://github.com/elastic/elasticsearch/commit/2fa7404c129b9406d98821acfc7dec7ab1e2a3a7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to open a node when you install marvel on the new beta version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12459</link><project id="" key="" /><description>I build the elasticsearch-2.0.0-beta1-SNAPSHOT using maven. I did start a node and it works fine. But when i do the same after installing marvel I get an exception.

I used ./bin/plugin -i elasticsearch/marvel/latest to install marvel. Please see the following screenshot for reference.
![screen shot 2015-07-24 at 11 06 18 pm](https://cloud.githubusercontent.com/assets/5770196/8887648/1c1ea472-3259-11e5-9833-f4ad635ff19c.png)
</description><key id="97185289">12459</key><summary>Unable to open a node when you install marvel on the new beta version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vvalleru</reporter><labels /><created>2015-07-25T03:10:42Z</created><updated>2015-07-27T11:34:04Z</updated><resolved>2015-07-27T11:34:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-07-25T04:57:29Z" id="124800288">I don't think current Marvel releases are compatible with 2.0.
I'd expect that we will release a version to go with the GA.
</comment><comment author="clintongormley" created="2015-07-27T11:34:04Z" id="125173804">Correct  - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Strange errors while indexing documents Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12458</link><project id="" key="" /><description>I'm raising an issue as there was no response on discuss.elastic.co
(https://discuss.elastic.co/t/strange-errors-while-indexing-documents/24978/1)

We've been observing some strange exceptions in our cluster. Some of the instances are throwing up exceptions like the ones below. We are running a 6 node cluster with ES 1.3.0. Data is being pushed to ES via a storm cluster. The data is being stored on an EBS device with provisioned 1000 IOPS.

Any clues as to what these exceptions mean and how to overcome?

[2015-07-03 12:24:19,611][WARN ][index.store ] [metrics-datastore-1-QA2906-perf] [qaautomation3-4-1250537325][1] Can't open file to read checksums
java.io.FileNotFoundException: No such file [_pmi.fdt]
at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:173)
at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:144)
at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:532)
at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:459)
at org.elasticsearch.index.store.Store$MetadataSnapshot.(Store.java:433)
at org.elasticsearch.index.store.Store.getMetadata(Store.java:144)
at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:724)
at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:576)
at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:183)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:444)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
</description><key id="97180207">12458</key><summary> Strange errors while indexing documents Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Srinathc</reporter><labels /><created>2015-07-25T01:45:32Z</created><updated>2015-07-27T11:33:44Z</updated><resolved>2015-07-25T02:13:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Srinathc" created="2015-07-25T01:46:44Z" id="124784845">Just to add - there have been a couple of "service elasticsearch restart" commands executed because the node seemed to be unresponsive. Could that be a reason?
</comment><comment author="markwalkom" created="2015-07-25T02:13:29Z" id="124788888">We reserve Github for confirmed bugs and feature requests, someone will answer there when they have time but please be aware that the forums are volunteer based.
</comment><comment author="clintongormley" created="2015-07-27T11:33:44Z" id="125173741">@Srinathc I also suggest upgrading to 1.7 - lots of bugs like these have been fixed since 1.3
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>