<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>WIP: Index creation waits for write consistency shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18759</link><project id="" key="" /><description>Before returning, index creation now waits for the write consistency
number of shards to be available. An index can not take any indexing or
other replication operations without the write consistency level of
shards being available anyway, so waiting on the index creation response
in order for this condition to be met makes sense, and allows API users
to not depend on cluster health checks before attempting indexing
operations on the newly created index.

Relates #9126 
</description><key id="158831550">18759</key><summary>WIP: Index creation waits for write consistency shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels /><created>2016-06-07T03:51:43Z</created><updated>2017-04-25T16:18:27Z</updated><resolved>2016-06-16T17:07:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-07T03:52:37Z" id="224166135">@bleskes @ywelsch I still need to add tests and fix broken tests, but just to get initial feedback.
</comment><comment author="pmusa" created="2017-04-25T16:18:26Z" id="297083830">see for the final PR https://github.com/elastic/elasticsearch/pull/18985</comment></comments><attachments /><subtasks /><customfields /></item><item><title>China  communication QQ class 247628376</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18758</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="158815311">18758</key><summary>China  communication QQ class 247628376</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kuipertan</reporter><labels /><created>2016-06-07T02:06:50Z</created><updated>2016-06-07T02:07:12Z</updated><resolved>2016-06-07T02:07:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-07T02:07:12Z" id="224144131">This appears to have been opened in error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Static For Each</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18757</link><project id="" key="" /><description>Added the ability to use Java-style for each loops on non-def target types.

Also added some documentation in a few other places.
</description><key id="158791794">18757</key><summary>Static For Each</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-06T22:39:09Z</created><updated>2016-06-07T01:29:27Z</updated><resolved>2016-06-07T01:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-07T01:28:10Z" id="224136913">+1 this is a big win.
</comment><comment author="jdconrad" created="2016-06-07T01:29:24Z" id="224137070">Thanks @rmuir for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bootstrap check for OnOutOfMemoryError and seccomp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18756</link><project id="" key="" /><description>This commit adds a bootstrap check for the JVM option OnOutOfMemoryError
being in use and seccomp being enabled. These two options are
incompatible because OnOutOfMemoryError allows the user to specify an
arbitrary program to fork when the JVM encounters an
OutOfMemoryError, and seccomp enables system call filters that prevents
forking.

This commit also adds support for bootstrap checks that are always
enforced, whether or not Elasticsearch is in production mode.

Closes #18736
</description><key id="158781214">18756</key><summary>Bootstrap check for OnOutOfMemoryError and seccomp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-06T21:37:37Z</created><updated>2016-06-07T13:26:58Z</updated><resolved>2016-06-07T13:26:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="samcday" created="2016-06-06T21:43:40Z" id="224097833">Awesome!

One suggestion: Maybe extend the check to also look for `-XX:OnError` flag too? I'm about 99% sure the OnError behaviour also relies on fork/exec in the same way `-XX:OnOutOfMemoryError` does.
</comment><comment author="jasontedor" created="2016-06-06T22:38:23Z" id="224110405">@samcday Thanks for the suggestion. It does rely on forking too. I pushed e94408c0d2748ce8829fb5096559221763026889.
</comment><comment author="s1monw" created="2016-06-07T07:56:47Z" id="224207248">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix global checkpoints test bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18755</link><project id="" key="" /><description>This commit fixes a test bug in a global checkpoints integration
test. Namely, if the replica shard is slow to start and is peer
recovered from the primary, it will not have the expected global
checkpoint due to these not being persisted and transferred on recovery.
</description><key id="158775260">18755</key><summary>Fix global checkpoints test bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Sequence IDs</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-06T21:07:33Z</created><updated>2016-06-08T12:22:31Z</updated><resolved>2016-06-08T12:22:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-06T21:10:39Z" id="224089604">I've added the logs from a failing test run to [gist](https://gist.github.com/jasontedor/0e25d9dbcb49f706e98c4cd83b210551). This failure does not reproduce. However, if I patch `RecoveryTargetService` with the following patch to artificially delay peer recovery, the failure reliably reproduces:

``` diff
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java
index bf5d1cb..bdd116e 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java
@@ -160,6 +160,12 @@ public class RecoveryTargetService extends AbstractComponent implements IndexEve
     private void doRecovery(final RecoveryTarget recoveryTarget) {
         assert recoveryTarget.sourceNode() != null : "can't do a recovery without a source node";

+        try {
+            Thread.sleep(175);
+        } catch (InterruptedException e) {
+            throw new RuntimeException(e);
+        }
+
         logger.trace("collecting local files for {}", recoveryTarget);
         Store.MetadataSnapshot metadataSnapshot = null;
         try {
```
</comment><comment author="bleskes" created="2016-06-08T10:12:56Z" id="224547358">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minor typo in MetaDataIndexTemplateService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18754</link><project id="" key="" /><description>typo
</description><key id="158771272">18754</key><summary>minor typo in MetaDataIndexTemplateService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpcarey</reporter><labels><label>:Core</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-06-06T20:48:16Z</created><updated>2016-06-06T21:00:43Z</updated><resolved>2016-06-06T21:00:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-06T21:00:19Z" id="224086858">Thanks @jpcarey!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch QueryBuilders to new MatchPhraseQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18753</link><project id="" key="" /><description>It was doing deprecated things with MatchQueryBuilder.
</description><key id="158734363">18753</key><summary>Switch QueryBuilders to new MatchPhraseQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Java API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-06T17:44:54Z</created><updated>2016-06-07T18:35:53Z</updated><resolved>2016-06-07T18:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-06T17:45:33Z" id="224033305">@colings86 or @cbuescher, can you have a look at this?
</comment><comment author="colings86" created="2016-06-07T07:52:38Z" id="224206435">LGTM
</comment><comment author="nik9000" created="2016-06-07T18:05:50Z" id="224364534">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove setRefresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18752</link><project id="" key="" /><description>It has been replaced with `setRefreshPolicy` which has support for
waiting until refresh with `setRefreshPolicy(WAIT_FOR)`.

Related to #1063
</description><key id="158729838">18752</key><summary>Remove setRefresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Java API</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-06T17:21:33Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-08T17:51:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-06T17:22:01Z" id="224026466">@bleskes, this is the cleanup after #17986 we talked about.
</comment><comment author="nik9000" created="2016-06-07T16:05:42Z" id="224329403">Added breaking label because it is a breaking change to the Java API.
</comment><comment author="bleskes" created="2016-06-08T13:19:34Z" id="224586051">LGTM. Thanks @nik9000 . No need for another review once the those test a sped up again. 
</comment><comment author="nik9000" created="2016-06-08T17:51:58Z" id="224673584">Thanks for reviewing @bleskes !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bool query does not allow a "query" clause</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18751</link><project id="" key="" /><description>On ES 2.3 at least, the query in the documentation fails with a query parsing exception "[bool] query does not support [query]"
</description><key id="158699150">18751</key><summary>bool query does not allow a "query" clause</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fcheung</reporter><labels><label>docs</label></labels><created>2016-06-06T14:59:56Z</created><updated>2016-06-07T14:48:39Z</updated><resolved>2016-06-07T14:48:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-07T14:48:33Z" id="224304638">thanks @fcheung - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose version type  to Update &amp; Delete by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18750</link><project id="" key="" /><description>This commit adds a `version_type` option to the request body for both Update-by-query and Delete-by-query. The option can take the value `internal` (default) and `force`. This last one can help to update or delete documents that have been created with an external version number equal to zero.

closes #16654
</description><key id="158693986">18750</key><summary>Expose version type  to Update &amp; Delete by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>enhancement</label></labels><created>2016-06-06T14:38:55Z</created><updated>2016-06-30T13:28:01Z</updated><resolved>2016-06-30T10:43:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-06-06T14:39:16Z" id="223978855">@nik9000 Do you want to review this one?
</comment><comment author="nik9000" created="2016-06-06T16:00:34Z" id="224004183">We did a ton of talking about version_type in #15125 and I think we came to the conclusion that exposing `force` was too dangerous, mostly because of the versioning semantics around replicas.

The trouble with update and delete by query is that they sort of imply internal versioning - you are letting Elasticsearch pick the next version. If you were syncing from an external source and you marked that source as "no longer visible" then it should get a version bump and whatever mechanism you have to syncing it with Elasticsearch should just pick it up and sync again, deleting it based on the change in the external source.
</comment><comment author="tlrx" created="2016-06-07T09:06:45Z" id="224222674">Thanks for the precisions Nik. I understand this decision but I'm still kind of mixed about this. 

Elasticsearch allows the documents to have an external version potentially starting from 0 where internal versionning starts from 1. Depending of the value of the version number, Update-By-Query and Delete-By-Query will work (in case of version &gt; 0) or report a version conflict (for version = 0). We know the implementation details and why it works like this but from a end-user point of view this might looks like a bug.

I'm OK to not expose `version_type` in Update-By-Query for the reason you gave, but I don't see why one could not delete documents with external versionning and version equal to 0 using Delete-By-Query. Using the last version of the document to delete should work as expected, I think. `FORCE` version type has been added for such case in the Delete API and we could expose it at least in DBQ too.

Anyway, this pull request can be changed to either:
a) Expose `FORCE` for Delete-By-Query only so that documents with version = 0 can still be deleted without any syncing with external source
b) Do not expose `version_type` but add documentation about how UBQ/DBQ APis handles documents externally versioned

but we must not let the situation as it is.
</comment><comment author="bleskes" created="2016-06-07T15:43:41Z" id="224322445">The main thing that makes this complicated is that strictly speaking, delete is an operation just like any other one and therefore the only _safe_ version types are `external` and `internal`. Under certain conditions, when people guarantee that there is no concurrency, you can use `external_gte` (which was added for deletes) and `force` (added for controlled fixing of issues with external versions where people wanted to repair their data). Strictly speaking, when you use external source, we expect people to reindex from source. Of course, sometime ES is just faster :) . I also agree it's an unfortunate side effect that delete by query doesn't work if people use external version and use version 0.

As @nik9000 already said, when we discussed the potential options with delete/update by query we decided to not get into all of this complexity. Long term, when we have seq# and they are used for replication versioning, we can expose this type of operations safely. For now I would opt for option b you describe - just doc things properly. 
</comment><comment author="clintongormley" created="2016-06-07T15:50:52Z" id="224324607">&gt; I also agree it's an unfortunate side effect that delete by query doesn't work if people use external version and use version 0.

This could possibly be worked around by allowing `ctx._version` to be settable in an update-by-query script.  Setting it to `null` should mean that the document is overwritten without checking the version... maybe?
</comment><comment author="tlrx" created="2016-06-30T10:43:53Z" id="229624341">Closed in favor of #19180 which documents this special case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch is using deleted tmp files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18749</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**:
`java version "1.7.0_101"
OpenJDK Runtime Environment (rhel-2.6.6.4.el6_8-x86_64 u101-b00)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)`

**OS version**: CentOS release 6.8 (Final)

**Description of the problem including expected versus actual behavior**:

Deleted temporary files are used by running elasticsearch instance

**Steps to reproduce**:
1. Start elasticsearch
2. Run `lsof -n | grep DEL`

**Expected**
No deleted files should be used by elasticsearch process

**Actual**

`java      41306 elasticsearch  DEL       REG               8,32              131176 /tmp/jna--1985354563/jna8161339909284970871.tmp`

`java      41306 elasticsearch  DEL       REG               8,32              131178 /tmp/ffidsWZY1`

**Provide logs (if relevant)**:
</description><key id="158685888">18749</key><summary>Elasticsearch is using deleted tmp files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karlosmid</reporter><labels /><created>2016-06-06T14:04:18Z</created><updated>2016-06-06T14:33:36Z</updated><resolved>2016-06-06T14:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-06T14:33:06Z" id="223976915">`DEL` means a map file that has been deleted. This is okay. The first of these is the jnidispatch native dispatch library that is loaded by JNA. This is intentionally deleted after loading by JNA, and you can verify this if you start Elasticsearch with the system property `jnidispath.preserve=true`. Similarly, the `ffi` file is the remnants of JNA loading its version of libffi, the foreign function interface library.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Method reference support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18748</link><project id="" key="" /><description>This adds support for method references (`Class::virtualMethod`, `Class::staticMethod`, `Class::new`).

The syntax for this was added in #18578, as a stub that throws UnsupportedOperationException. This adds the logic. It is useful because these are the simplest lambda types (non-capturing, no desugaring needed), but can make the functional apis usable to users in many ways, based on functions that already exist. For example plugging in different functions for comparators, using streams apis, etc.

There are two cases: static and dynamic. In the static case we call LambdaMetaFactory from bytecode, since we know everything we need at compile time. This is similar to javac's case, just infer interface from the method's argument type.

In the dynamic case, we don't yet know enough to do anything interesting, so we just push the reference on the stack as a string, and mark the argument as a function in a bitset passed as a static bootstrap param. Once types are known at runtime, then we do the same stuff as the static case, just with java code, and replace the "placeholders" using MethodHandles.filterArguments.

Array constructor references (`foo[]::new`) are not yet supported. We should do that as a followup if we want to support them, as they are a little more complicated (require us to write bridge methods). For cases like this we will need to "expand" our recipe param to be something better than a bitset, so that pointers to our own-written functions can work too.

There are also some random cleanups, and additional strictness to the whitelist: enforcing that we always whitelist covariant and generic overrides. Some of this is kinda unrelated, but happens to be here.

The PR is not yet ready. In particular it is super-messy, and there needs to be a lot of cleanup and refactoring before we should push it. For example, FunctionRef class should maybe work with MethodType rather than ASM types, and Definition should do the functional interface computation stuff (no reflection api needs to be used). The two different apis for the two ways we call this thing (Handle/MethodHandle and Type/MethodType) make things a mess at the moment. Needs help :)

@uschindler i know you are at buzzwords, but if you have some time to look, would be great to get your thoughts on some of this.
</description><key id="158682150">18748</key><summary>Method reference support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-06T13:48:07Z</created><updated>2016-06-13T08:26:47Z</updated><resolved>2016-06-07T18:22:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-06T16:44:59Z" id="224016312">I will check and commit changes when I come back from Berlin buzzwords. &#128521;

Am 6. Juni 2016 15:49:33 MESZ, schrieb Robert Muir notifications@github.com:

&gt; This adds support for method references (`Class::virtualMethod`,
&gt; `Class::staticMethod`, `Class::new`).
&gt; 
&gt; The syntax for this was added in #18578, as a stub that throws
&gt; UnsupportedOperationException. This adds the logic. It is useful
&gt; because these are the simplest lambda types (non-capturing, no
&gt; desugaring needed), but can make the functional apis usable to users in
&gt; many ways, based on functions that already exist. For example plugging
&gt; in different functions for comparators, using streams apis, etc.
&gt; 
&gt; There are two cases: static and dynamic. In the static case we call
&gt; LambdaMetaFactory from bytecode, since we know everything we need at
&gt; compile time. This is similar to javac's case, just infer interface
&gt; from the method's argument type.
&gt; 
&gt; In the dynamic case, we don't yet know enough to do anything
&gt; interesting, so we just push the reference on the stack as a string,
&gt; and mark the argument as a function in a bitset passed as a static
&gt; bootstrap param. Once types are known at runtime, then we do the same
&gt; stuff as the static case, just with java code, and replace the
&gt; "placeholders" using MethodHandles.filterArguments.
&gt; 
&gt; Array constructor references (`foo[]::new`) are not yet supported. We
&gt; should do that as a followup if we want to support them, as they are a
&gt; little more complicated (require us to write bridge methods). For cases
&gt; like this we will need to "expand" our recipe param to be something
&gt; better than a bitset, so that pointers to our own-written functions can
&gt; work too.
&gt; 
&gt; There are also some random cleanups, and additional strictness to the
&gt; whitelist: enforcing that we always whitelist covariant and generic
&gt; overrides. Some of this is kinda unrelated, but happens to be here.
&gt; 
&gt; The PR is not yet ready. In particular it is super-messy, and there
&gt; needs to be a lot of cleanup and refactoring before we should push it.
&gt; For example, FunctionRef class should maybe work with MethodType rather
&gt; than ASM types, and Definition should do the functional interface
&gt; computation stuff (no reflection api needs to be used). The two
&gt; different apis for the two ways we call this thing (Handle/MethodHandle
&gt; and Type/MethodType) make things a mess at the moment. Needs help :)
&gt; 
&gt; @uschindler i know you are at buzzwords, but if you have some time to
&gt; look, would be great to get your thoughts on some of this.
&gt; You can view, comment on, or merge this pull request online at:
&gt; 
&gt;  https://github.com/elastic/elasticsearch/pull/18748
&gt; 
&gt; -- Commit Summary --
&gt; - initial messy impl of painless method references
&gt; - don't do a no-op filter, that was just for testing
&gt; 
&gt; -- File Changes --
&gt; 
&gt;    M modules/lang-painless/src/main/antlr/PainlessParser.g4 (2)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java
&gt; (137)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java
&gt; (16)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/Definition.java
&gt; (79)
&gt; A
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/FunctionRef.java
&gt; (126)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/PainlessScriptEngineService.java
&gt; (1)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/ScriptImpl.java
&gt; (2)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java
&gt; (14)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java
&gt; (221)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java
&gt; (10)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java
&gt; (41)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCall.java
&gt; (2)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefArray.java
&gt; (4)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java
&gt; (14)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefField.java
&gt; (4)
&gt; M
&gt; modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewObj.java
&gt; (6)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/java.lang.txt
&gt; (14)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/java.math.txt
&gt; (2)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/java.text.txt
&gt; (1)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/java.time.chrono.txt
&gt; (53)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/java.time.txt
&gt; (32)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/java.time.zone.txt
&gt; (1)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/java.util.txt
&gt; (43)
&gt; M
&gt; modules/lang-painless/src/main/resources/org/elasticsearch/painless/org.elasticsearch.txt
&gt; (4)
&gt; M
&gt; modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicAPITests.java
&gt; (11)
&gt; M
&gt; modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java
&gt; (6)
&gt; M
&gt; modules/lang-painless/src/test/java/org/elasticsearch/painless/FunctionRefTests.java
&gt; (59)
&gt; M
&gt; modules/lang-painless/src/test/java/org/elasticsearch/painless/OverloadTests.java
&gt; (2)
&gt; 
&gt; -- Patch Links --
&gt; 
&gt; https://github.com/elastic/elasticsearch/pull/18748.patch
&gt; https://github.com/elastic/elasticsearch/pull/18748.diff
&gt; 
&gt; ---
&gt; 
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/pull/18748

## 

Uwe Schindler
H.-H.-Meier-Allee 63, 28213 Bremen
http://www.thetaphi.de
</comment><comment author="uschindler" created="2016-06-07T10:34:52Z" id="224242588">Hi Robert: I just fixed the methodhandles lookup object passed to LambdaMetaFactory. For real lambdas we need to pass the lookup object of the original invokedyanmic, otherwise it wont find the  lambda method impl. This fixes one todo
</comment><comment author="uschindler" created="2016-06-07T11:08:25Z" id="224249222">Hi Robert,
to mee this looks like a good start. We should only fix the issues with the fromMethodDescriptorString() because I have the feeling this is using wrong class loader. In my opinion, we should (as you say in your XXX comment), fix the whole lookup stuff in the FunctionRef / Def class.
</comment><comment author="uschindler" created="2016-06-07T11:12:12Z" id="224249943">BTW: filterArguments should be fine!

One question: How do we handle method calls using functional references that are statically compiled (e.g., calls on non-Def methods that pass a functional ref)
</comment><comment author="rmuir" created="2016-06-07T11:58:31Z" id="224258880">As i mentioned above: we dont need to be doing any lookuping or classloader passing. This is just because of shitty ASM apis. We can fix that.
</comment><comment author="rmuir" created="2016-06-07T12:02:42Z" id="224259716">&gt; One question: How do we handle method calls using functional references that are statically compiled (e.g., calls on non-Def methods that pass a functional ref)

This is the easy case: https://github.com/rmuir/elasticsearch/blob/3238868cc43346c34fe4068c0d7e60eff4e4c387/modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java#L67-L79

We just invoke LambdaMetaFactory right there, and push the result (which is e.g. a Comparator or whatever the functional interface is) on the stack. 
</comment><comment author="uschindler" created="2016-06-07T12:09:23Z" id="224261069">## Ah, missed that one!

Uwe Schindler
H.-H.-Meier-Allee 63, 28213 Bremen
http://www.thetaphi.de
</comment><comment author="rmuir" created="2016-06-07T12:14:46Z" id="224262161">Yeah, the thing done there is the same as javac, you know the type of the expected interface, so you know everything :) 
</comment><comment author="rmuir" created="2016-06-07T16:43:02Z" id="224340615">I cleaned this up, it is not so horrible anymore. I want to iterate with further stuff as a followup.
</comment><comment author="jdconrad" created="2016-06-07T16:44:12Z" id="224340930">LGTM.  Thanks for getting method refs working.
</comment><comment author="rmuir" created="2016-06-07T18:22:29Z" id="224369484">Thanks for the help here @uschindler @jdconrad . The ugly parts are fixed, we can try to extend it in followup issues with some of the deferred stuff, e.g. `type[]::new` or `object::virtual`. I am working on changes for some of those already.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.indices.IndicesServiceTests.testPendingTasks fails sometimes on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18747</link><project id="" key="" /><description>Build URL:

http://build-us-00.elastic.co/job/es_core_master_window-2012/3198/

reproduce command (does not reproduce locally for me):

```
gradle :core:test -Dtests.seed=288FBC31019CE7FC -Dtests.class=org.elasticsearch.indices.IndicesServiceTests -Dtests.method="testPendingTasks" -Dtests.es.logger.level=DEBUG -Dtests.assertion.disabled=org.elasticsearch -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=it -Dtests.timezone=America/Atikokan
```

Failure:

```
java.lang.AssertionError
    at __randomizedtesting.SeedInfo.seed([288FBC31019CE7FC:DCC65F1D8DBA8009]:0)
    at org.junit.Assert.fail(Assert.java:86)
    at org.junit.Assert.assertTrue(Assert.java:41)
    at org.junit.Assert.assertFalse(Assert.java:64)
    at org.junit.Assert.assertFalse(Assert.java:74)
    at org.elasticsearch.indices.IndicesServiceTests.testPendingTasks(IndicesServiceTests.java:210)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)

```

code at failure (IndeicesServiceTests:210):

``` java
assertFalse(indicesService.hasUncompletedPendingDeletes());
```
</description><key id="158679767">18747</key><summary>org.elasticsearch.indices.IndicesServiceTests.testPendingTasks fails sometimes on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label></labels><created>2016-06-06T13:37:29Z</created><updated>2016-06-06T13:51:07Z</updated><resolved>2016-06-06T13:51:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-06-06T13:38:08Z" id="223961043">@imotov @nik9000 Is this something either of you can look at? (it looks like task management stuff to me)
</comment><comment author="imotov" created="2016-06-06T13:47:37Z" id="223963768">I don't think it's related to task management. It looks like this test deals with pending delete that needs to be retried due to IOErrors and was introduced in #9784. So, @s1monw or @bleskes might be better candidates for figuring this one out.
</comment><comment author="ywelsch" created="2016-06-06T13:51:07Z" id="223964745">Was a test bug on my part, it's fixed by e3e8f101032545bd86244d609dbac4975f45884e. `if (randomBoolean())` took the wrong branch on my CI ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix some typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18746</link><project id="" key="" /><description /><key id="158661643">18746</key><summary>Fix some typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trangvh</reporter><labels><label>docs</label></labels><created>2016-06-06T12:02:57Z</created><updated>2016-06-07T14:41:45Z</updated><resolved>2016-06-07T14:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-07T14:41:45Z" id="224302399">thanks @trangvh - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix some typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18745</link><project id="" key="" /><description /><key id="158659347">18745</key><summary>Fix some typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trangvh</reporter><labels /><created>2016-06-06T11:47:39Z</created><updated>2016-06-06T12:04:26Z</updated><resolved>2016-06-06T12:04:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2016-06-06T11:48:32Z" id="223936859">Hi @trangvh, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git [commit](https://github.com/elastic/elasticsearch/pull/18745.patch). Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?
</comment><comment author="trangvh" created="2016-06-06T12:04:26Z" id="223939907">Sorry for my mistake, it is replaced by #18746 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UpdatebyQuery Rethrottle tests hangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18744</link><project id="" key="" /><description>build URL:
http://build-us-00.elastic.co/job/es_core_master_window-2012/3196/testReport/junit/org.elasticsearch.index.reindex/RethrottleTests/testUpdateByQuery/

failure:

```
Test abandoned because suite timeout was reached.
```

reproduce URL:

```
gradle :modules:reindex:test -Dtests.seed=A5B5FA09B74893C9 -Dtests.class=org.elasticsearch.index.reindex.RethrottleTests -Dtests.method="testUpdateByQuery" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=894m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=da-DK -Dtests.timezone=Europe/Isle_of_Man
```
</description><key id="158626435">18744</key><summary>UpdatebyQuery Rethrottle tests hangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Reindex API</label><label>jenkins</label></labels><created>2016-06-06T08:37:27Z</created><updated>2016-06-08T18:01:14Z</updated><resolved>2016-06-08T18:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-06-06T08:37:38Z" id="223899420">@nik9000 are you able to look into this?
</comment><comment author="tlrx" created="2016-06-06T08:55:51Z" id="223903124">This should be resolved by https://github.com/elastic/elasticsearch/pull/18731
</comment><comment author="nik9000" created="2016-06-06T12:27:51Z" id="223944481">&gt; This should be resolved by #18731

Thanks. I've updated the PR to mark it as closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.index.IndexWithShadowReplicasIT fails to delete index directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18743</link><project id="" key="" /><description>Build URL:
http://build-us-00.elastic.co/job/es_core_2x_window-2012/945/testReport/junit/junit.framework/TestSuite/org_elasticsearch_index_IndexWithShadowReplicasIT/

failure:

```
java.io.IOException: Could not remove the following files (in the order of attempts):
   Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3\index\_1.cfs: java.nio.file.AccessDeniedException: Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3\index\_1.cfs
   Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3\index\_0.cfs: java.nio.file.AccessDeniedException: Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3\index\_0.cfs
   Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3\index: java.nio.file.DirectoryNotEmptyException: Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3\index
   Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3: java.nio.file.DirectoryNotEmptyException: Y:\jenkins\workspace\es_core_2x_window-2012\core\target\J0\temp\org.elasticsearch.index.IndexWithShadowReplicasIT_6DC4FBD8628375FD-001\tempDir-024\test\3

    at org.apache.lucene.util.IOUtils.rm(IOUtils.java:295)
    at org.elasticsearch.env.NodeEnvironment.deleteShardDirectoryUnderLock(NodeEnvironment.java:403)
    at org.elasticsearch.indices.IndicesService.deleteShardStore(IndicesService.java:556)
    at org.elasticsearch.index.IndexService.onShardClose(IndexService.java:502)
    at org.elasticsearch.index.IndexService.access$200(IndexService.java:76)
    at org.elasticsearch.index.IndexService$StoreCloseListener.handle(IndexService.java:528)
    at org.elasticsearch.index.IndexService$StoreCloseListener.handle(IndexService.java:513)
    at org.elasticsearch.index.store.Store.closeInternal(Store.java:363)
    at org.elasticsearch.index.store.Store.access$000(Store.java:87)
    at org.elasticsearch.index.store.Store$1.closeInternal(Store.java:108)
    at org.elasticsearch.common.util.concurrent.AbstractRefCounted.decRef(AbstractRefCounted.java:64)
    at org.elasticsearch.index.store.Store.decRef(Store.java:345)
    at org.elasticsearch.index.store.Store.close(Store.java:353)
    at org.elasticsearch.index.IndexService.closeShardInjector(IndexService.java:457)
    at org.elasticsearch.index.IndexService.removeShard(IndexService.java:416)
    at org.elasticsearch.index.IndexService.close(IndexService.java:252)
    at org.elasticsearch.indices.IndicesService.removeIndex(IndicesService.java:413)
    at org.elasticsearch.indices.IndicesService.deleteIndex(IndicesService.java:478)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.deleteIndex(IndicesClusterStateService.java:732)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedIndices(IndicesClusterStateService.java:225)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:162)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

reproduce URL:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=6DC4FBD8628375FD -Dtests.class=org.elasticsearch.index.IndexWithShadowReplicasIT -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Des.node.mode=local -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+AggressiveOpts -Djava.net.preferIPv4Stack=true" -Dtests.locale=en-US -Dtests.timezone=UTC
```
</description><key id="158625692">18743</key><summary>org.elasticsearch.index.IndexWithShadowReplicasIT fails to delete index directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Shadow Replicas</label><label>jenkins</label></labels><created>2016-06-06T08:33:10Z</created><updated>2017-05-26T18:53:40Z</updated><resolved>2017-05-26T18:53:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-06-06T08:33:48Z" id="223898632">@dakrone is this something you can look into?
</comment><comment author="dakrone" created="2017-05-26T18:53:39Z" id="304361540">Shadow replicas have been removed and this is no longer applicable</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can I disable the sorting feature in Elasticsearch's aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18742</link><project id="" key="" /><description>As In my case, the bucket size can be as long as millions, and end-user just wants the bucket size instead of the bucket contents. As ES will always return the bucket ordered by doc_count,this content is not I want and very time-consuming. 
I've already used the response filtering to just receive the bucket size, but still the query is too slow. Can I just turn off the sorting in the ES's aggregation and speed up my query?
</description><key id="158610015">18742</key><summary>can I disable the sorting feature in Elasticsearch's aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shuangshui</reporter><labels /><created>2016-06-06T06:40:14Z</created><updated>2016-06-06T12:40:44Z</updated><resolved>2016-06-06T12:40:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-06T12:40:44Z" id="223947125">Closing: this is a question and should be asked at http://discuss.elastic.co, we try to only use github issues for feature requests and bugs. Maybe what you are looking after is the cardinality aggregation?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What version should the low level http client have?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18741</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/18735 proposes versioning the http client in lock step with elasticsearch, meaning the first release is going to be 5.0.0-alpha-4 or 5.0.0-beta-1. But the API is super experimental and new and we expect to change it. semver would have it be 0.something. It is compatible with pre-5.0 versions of Elasticsearch as well.

What should we do with the version? I'm labeling this a release blocker because releasing it as proposed sort of commits us to one course of action. You can't un-release code. And you shouldn't go backwards in numbers either.
</description><key id="158563631">18741</key><summary>What version should the low level http client have?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Java REST Client</label><label>blocker</label></labels><created>2016-06-05T17:00:20Z</created><updated>2016-07-13T12:24:59Z</updated><resolved>2016-06-08T12:15:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-07T13:57:57Z" id="224288462">The HTTP client will be a dependency of Elasticsearch and the supported API (ie the rest specs) will be tied to the Elasticsearch version, so it should live in the main repo and have the same version as ES core.
</comment><comment author="nik9000" created="2016-06-07T18:56:09Z" id="224379511">&gt; the supported API (ie the rest specs) will be tied to the Elasticsearch version,

The whole point of the REST client is that it'll work across major versions though! As it stands the low level rest client should work as far back as you are willing to try it because it is fairly un-opinionated.

&gt; HTTP client will be a dependency of Elasticsearch

Very pedantic quibble: it'll be a dependency of Elasticsearch's tests and of at least one of Elasticsearch's modules. It probably won't be a dependency of Elasticsearch's core.
</comment><comment author="s1monw" created="2016-06-08T12:11:10Z" id="224570419">&gt; The whole point of the REST client is that it'll work across major versions though! As it stands the low level rest client should work as far back as you are willing to try it because it is fairly un-opinionated.

true - it will still have the same release cycle as ES. Here is why, there are 2 reasons to do a release on a high level:
- bug fixes
- new features (new APIs added to ES that we have to support)

With the time based releases we release bugfixes frequently so users of the http client can get bugfixes through that. If there is a new API that needs to be supported ES need to be release anyways so things should be consistent and just come together.

Now to the practical aspects:
- having a different version number is confusing we have to explain which version we support in both directions.
- using the same version as anything else in the stack (other clients taken aside - they should move too) has the advantage that it's clear what our supported version is on the upper end. 
- the location of the client is irrelevant for BWC
- the location of the client is extremely relevant in development and should therefor be inside the ES project. if it's not in side we need to code against snapshots again and all the trouble starts from the beginning. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Creating array of IPs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18740</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.8.0_91

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

At first, I'm not sure if I misread the docs and this is not expected behavior, in which case, please look at this as an issue instead. Arrays of IPs don't seem to be able to be made. Whenever one pre-defines the mapping, and tries to create one, a type mismatch happens.

**Steps to reproduce**:
I'm following the ones someone suggested I follow on [the discussion board](https://discuss.elastic.co/t/how-to-store-array-of-ip/51461/2). Credit to msimos.
1. I created a dummy index following the steps above. My dummy index is called "com".

```
curl -XPUT 'http://localhost:9200/com' -d '{
    "mappings": {
        "type": {
            "properties": {
                "ips": {
                    "type": "ip"
                }
            }
        }
    }
}'
```
1. I then indexed the sample ip array into the "com" index in the "anytype" type with id "1".

```
curl -XPOST 'http://localhost:9200/com/anytype/1' -d '{
    "ips": ["123.123.123.123", "10.0.0.1"]
}'
```
1. This gave me a type mismatch error:

```
{
    "error": {
        "root_cause": [{
            "type": "mapper_parsing_exception",
            "reason": "failed to parse"
        }],
        "type": "mapper_parsing_exception",
        "reason": "failed to parse",
        "caused_by": {
            "type": "illegal_state_exception",
            "reason": "Mixing up field types: class org.elasticsearch.index.mapper.core.LongFieldMapper$LongFieldType != class org.elasticsearch.index.mapper.ip.IpFieldMapper$IpFieldType on field ips"
        }
    },
    "status": 400
}
```
</description><key id="158559664">18740</key><summary>Creating array of IPs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">esamudio</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>v2.4.0</label></labels><created>2016-06-05T15:29:17Z</created><updated>2016-06-28T12:44:40Z</updated><resolved>2016-06-13T08:41:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-07T13:54:15Z" id="224287384">Note, this is fixed in master
</comment><comment author="esamudio" created="2016-06-10T21:44:52Z" id="225303660">@clintongormley Thanks for the reply! How can I upgrade to master? I understand its not an official release but I was wondering what would be the cleanest way of upgrading to that version. I could really use not having that type mismatch. Thanks again!
</comment><comment author="jpountz" created="2016-06-13T08:25:49Z" id="225517693">OK I found the bug, this is unrelated to the use of arrays and is indeed fixed in master. What is happening here is that `ips` is already defined in `type` but not in `anytype`. For that cases, we have some logic that tries to borrow the mapping definitions from existing types, but it only works for types that are supported in templates currently (numbers, date and string), so it fails with ip addresses.

I will open a PR soon, but you can work around the problem by defining your ip field in the `_default_` mapping so that it will be used by all types.
</comment><comment author="jpountz" created="2016-06-13T08:41:59Z" id="225521039">Actually this was fixed by #17882 so I just backported it.
</comment><comment author="jpountz" created="2016-06-13T08:46:39Z" id="225522125">This will be addressed in the upcoming 2.4.0 release.
</comment><comment author="esamudio" created="2016-06-13T15:10:07Z" id="225610548">@jpountz  Thanks for the update! How do I go about defining the ip field in the _default_ mapping?
</comment><comment author="esamudio" created="2016-06-13T15:12:17Z" id="225611196">Is it just a PUT request with:?

{
    "_default_": {
        "_all": {
            "type": "ip"
        }
    }
}
</comment><comment author="jpountz" created="2016-06-13T15:12:35Z" id="225611295">That would be something like this:

```
curl -XPUT 'http://localhost:9200/com' -d '{
    "mappings": {
        "_default_": {
            "properties": {
                "ips": {
                    "type": "ip"
                }
            }
        }
    }
}'
```
</comment><comment author="esamudio" created="2016-06-13T16:21:12Z" id="225632195">thank you very much!
</comment><comment author="esamudio" created="2016-06-28T12:30:19Z" id="229034724">&lt;opening new issue&gt;
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Relocating many shards at once results in canceled relocations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18739</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7.3 (build hash 05d4530971ef0ea46d0f4fa6ee64dbc8df659682)

**JVM version**: java-1.8.0-openjdk-1.8.0.77-0.b03.9.amzn1.x86_64 (OpenJDK 8 update 77)

**OS version**: Linux 4.1.17-22.30.amzn1.x86_64 x86_64 GNU/Linux (Amazon Linux, quite similar to CentOS)

**Description of the problem including expected versus actual behavior**: In previous versions of Elasticsearch, we would "resize" an Elasticsearch cluster in AWS EC2 by disabling shard allocation, provisioning new EC2 nodes that joined the existing cluster, and using the reroute API to move shards from the old nodes to the new ones. We would allow Elasticsearch to do this at maximum speed by scheduling all shard relocations at once (we run an algorithm to decide which nodes get which shard movements, and then send one, big reroute API call to Elasticsearch). In previous versions (1.4 and earlier) of ES, this worked like a charm, even with ~30,000 shard movements (yes I know -- you can imagine how big the EC2 clusters are). However, when we upgraded to Elasticsearch 1.7.3, we noticed that when we did this, we would schedule, say, 9k shard movements, and Elasticsearch would start relocating shards, but after about an hour, the ES cluster health API would suddenly start reporting that there were only ~800 relocations. Since we knew it was impossible for ES to have moved the other ~8,000 shards in such an incredibly short window, we knew something was up. Upon investigation (or, enabling debug logging for "index.shard" activity), we saw that the shard relocations were canceled. Destination nodes had a log message like this:

&gt; [2016-06-03 21:31:01,898][DEBUG][index.shard              ] [elasticsearch-36-48] [&amp;shared-banana-bilberry][4] state: [RECOVERING]-&gt;[CLOSED], reason [failed recovery]

We also saw some errors that look like this:

&gt; [2016-06-03 21:31:01,898][WARN ][indices.cluster          ] [elasticsearch-36-48] [[&amp;shared-banana-bilberry][4]] marking and sending shard failed due to [failed recovery]
&gt; org.elasticsearch.indices.recovery.RecoveryFailedException: [&amp;shared-banana-bilberry][4]: Recovery failed from [elasticsearch-37-41][5pss4NQJRWifhVst_AaPoQ][elasticsearch-37-41][inet[/10.100.37.41:9300]]{client_bv=false, availability_zone=a, master=false} into [elasticsearch-36-48][SZpn-ZwFTe2qrqk9JdLWdw][elasticsearch-36-48][inet[/10.100.36.48:9300]]{client_bv=false, availability_zone=a, master=false} (no activity after [30m])
&gt;         at org.elasticsearch.indices.recovery.RecoveriesCollection$RecoveryMonitor.doRun(RecoveriesCollection.java:235)
&gt;         at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
&gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;         at java.lang.Thread.run(Thread.java:745)
&gt; Caused by: org.elasticsearch.ElasticsearchTimeoutException: no activity after [30m]
&gt;         ... 5 more

We also, in some cases, got out of memory exceptions trying to spawn new threads. Those errors took this form:

&gt; Caused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch-40-173][inet[/10.100.40.173:9300]][internal:index/shard/
&gt; recovery/start_recovery]
&gt; Caused by: java.lang.OutOfMemoryError: unable to create new native thread
&gt;         at java.lang.Thread.start0(Native Method)
&gt;         at java.lang.Thread.start(Thread.java:714)
&gt;         at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:950)
&gt;         at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1368)
&gt;         at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:79)
&gt;         at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:224)
&gt;         at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
&gt;         at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
&gt;         at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
&gt;         at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)

As it turns out, the root reason for this is because Elasticsearch spawns a new `RecoveryMonitor` thread (in the "generic" thread pool, which is a cached (unbounded!) thread pool) for every relocating shard, in order to monitor its state (see: https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java#L68).

Previous versions of ES didn't have this monitoring feature, and so we could schedule tens of thousands of relocations, send the request off to Elasticsearch, and just wait for it to complete.

As it stands now, once we send this reroute API call off to Elasticsearch, ES will cancel the majority of the shards in the request and we'll eventually see that, from our application's perspective, we've relocated all the shards. We then re-run the allocation algorithm as a final verification step, and see that there are still thousands (or tens of thousands) of shards remaining on the old EC2 nodes, which must be relocated, so we send off the reroute API again, and so on. This works, but every time we do it, we run the risk of getting out of memory exceptions.

In a nutshell, I'm convinced that Elasticsearch documentation and code should reject reroute API calls and put a maximum on the number of relocating shards equivalent to whatever Elasticsearch can reasonably handle. This way, I can send a reroute API call with 30,000 shards in it, and Elasticsearch will give me a response that says, "OK, I can move these 10,000 shards, but these other 20,000 shards cannot be moved yet -- you'll have to send them in a later batch after the first 10,000 are done." This way I can make my application my responsive to the maximum capabilities of Elasticsearch and just send the maximum batch size the current Elasticsearch cluster can handle. It's important for this kind of feedback to come from Elasticsearch because otherwise it's incredibly difficult to decide how many relocations I can send to the reroute API at once, since it will vary by the performance characteristics (size, number of cores, disk performance, and settings) of the Elasticsearch cluster itself.

One other idea might be to have a separate thread pool for recovery, and put a cap on it based on the number of cores or something, and that way I can just ask Elasticsearch the maximum relocations that I can give it at once, and I can batch up my reroute API calls to send batches of the most efficient size. This, of course, still doesn't help with the fact that sending the maximum batch size may ultimately result in some or many of the queued up relocating shards to timeout after 30 minutes.

Another idea might be instead of timing out due to "inactivity," timing out if there is some error condition -- I believe the activity of the shard itself is less relevant than, for example, whether the node sending the shard is still part of the cluster.

Lastly, I think that it would be very helpful if the log messages were a little bit more clear about what's happening. The log message on the destination nodes look like this, and it's somewhat confusing at first glance, but what's actually happening is the node created a local shard so that the copy could occur, then RecoveryMonitor threw an error since there was no activity for 30 minutes, and then the node has to actually delete the local shard to cancel the relocation. This series of events is not made clear by these log messages, even though that's exactly what they mean is happening:

&gt; [2016-06-04 00:04:15,301][DEBUG][index.shard              ] [elasticsearch-36-48] [&amp;shared-cloudberry-cherry][5] state: [CREATED]
&gt; [2016-06-04 00:04:15,301][DEBUG][index.shard              ] [elasticsearch-36-48] [&amp;shared-cloudberry-cherry][5] scheduling optimizer / merger every 1s
&gt; [2016-06-04 00:04:15,301][DEBUG][index.shard              ] [elasticsearch-36-48] [&amp;shared-cloudberry-cherry][5] state: [CREATED]-&gt;[RECOVERING], reason [from [elasticsearch-47-212][7A2hpTC1T-Sh428skXDbcQ][elasticsearch-47-212][inet[/10.100.47.212:9300]]{client_bv=false, availability_zone=c, master=false}]

followed by

&gt; [2016-06-04 04:26:14,984][DEBUG][indices.recovery         ] [elasticsearch-36-48] [&amp;shared-cloudberry-cherry][5] recovery canceled (reason: [shard closed])
&gt; [2016-06-04 04:26:14,984][DEBUG][index.shard              ] [elasticsearch-36-48] [&amp;shared-cloudberry-cherry][5] state: [RECOVERING]-&gt;[CLOSED], reason [removing shard (not allocated)]

I think, at least, the "reason" could be clarified that in fact, all of this is happening due to the lack of shard activity. Although the shard activity error does appear, it appears out-of-order in the logs, and so it's not clear that this "reason" is actually a direct result of the warning:

&gt; [2016-06-04 01:04:15,324][WARN ][indices.cluster          ] [elasticsearch-36-48] [[&amp;shared-cherry-strawberry][3]] marking and sending shard failed due to [failed recovery]
&gt; org.elasticsearch.indices.recovery.RecoveryFailedException: [&amp;shared-cherry-strawberry][3]: Recovery failed from [elasticsearch-36-34][ogK7RcvLTXqxI-d7jetmKQ][elasticsearch-36-34][inet[/10.100.36.34:9300]]{client_bv=false, availability_zone=a, master=false} into [elasticsearch-36-48][SZpn-ZwFTe2qrqk9JdLWdw][elasticsearch-36-48][inet[/10.100.36.48:9300]]{client_bv=false, availability_zone=a, master=false} (no activity after [30m])
&gt;         at org.elasticsearch.indices.recovery.RecoveriesCollection$RecoveryMonitor.doRun(RecoveriesCollection.java:235)
&gt;         at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
&gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;         at java.lang.Thread.run(Thread.java:745)
</description><key id="158507409">18739</key><summary>Relocating many shards at once results in canceled relocations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jonaf</reporter><labels><label>:Allocation</label></labels><created>2016-06-04T14:36:50Z</created><updated>2016-06-13T09:06:25Z</updated><resolved>2016-06-10T09:26:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-06T17:18:24Z" id="224025415">Related to https://github.com/elastic/elasticsearch/issues/11511
</comment><comment author="clintongormley" created="2016-06-06T17:19:11Z" id="224025645">@bleskes What do you think?
</comment><comment author="jasontedor" created="2016-06-06T17:58:42Z" id="224037272">&gt; in the "generic" thread pool, which is a cached (unbounded!) thread pool

Note that as of #17017, the generic thread pool is bounded.
</comment><comment author="jonaf" created="2016-06-06T19:10:22Z" id="224057513">It would be really awesome if Elasticsearch just had a queue for relocations and had a single thread fan out the relocations (up to the configured concurrent relocations setting) or something. It'd be really nice if, instead of just putting a limit on how many relocations I can do, allowing me to give all the relocations to Elasticsearch and let elasticsearch decide how to handle things (relocating at peak efficiency based on its ability to prioritize and do work). This worked _exceedingly well_ in versions of Elasticsearch prior to 1.5 and I don't see any reason why Elasticsearch can't continue allowing any number of relocations.
</comment><comment author="s1monw" created="2016-06-10T09:26:56Z" id="225135965">@jonaf you are using an absolute expert API that ignores all throttling decisions done by the allocation infrastructure to prevent these situations.  This might have worked in previous versions but there is no guarantee that the system can sustain any expert level API calls that forces the system to ignore it's own safety mechanisms. If you go and use allocation filtering to move all shards away from one or more nodes it will move them all without timeouts in a sustainable manner. If you force allocations via the move API you basically take over all ownership of the cluster and it will react with canceled allocations if you overload it. 

Long term these APIs are not maintainable and might be even more expert or even go away entirely. We will work on making allocation better, maybe you can tell us why you can't rely on the ES internal allocation logic?
</comment><comment author="jonaf" created="2016-06-10T13:11:56Z" id="225178020">@s1monw In our previous version of Elasticsearch (1.3.9), using allocation filtering was our first implementation. However, it was broken (it never "activated," so shards never moved). I'll see if I can reimplement allocation filtering in 1.7.3 and see if it works now, but reimplementing requires non-trivial engineering. I see no reason why the reroute API cannot use the same implementation that allocation filtering would use. There is nothing about the API in the documentation (historically, in its behavior either) that indicates this is an "expert" API or that it should be considered dangerous. In fact, it seems to have quite a few protections -- for example, you have to expressly say "allow_primary" to allocate primaries, which protects potential data loss, and it will reject relocations that do not meet the allocation criteria for the cluster.

How hard would it really be for the reroute API to just use a queue? Or the same mechanism allocation filtering uses? Because really, it seems like it should be the same implementation under the hood, because your'e doing the same thing, it's just that the filtering API has a blanket "rule" or set of rules to do the relocations, and the reroute API allows maximum flexibility with deciding a "rule" for every shard. To me, this is the identical problem under the hood.

To recap, I'll explain the use-case here. We provision (very large) elasticsearch clusters in EC2. We have software that manages these clusters (provisions them, for example). One of the features this software has is to "resize" a cluster on-the-fly. The cluster stays "green" 100% of the time, but we move all of the shards from one group of EC2 nodes to another group, usually to convert instance types or scale up/down the number of data nodes in the Elasticsearch cluster. We will always need a mechanism to do this relocation and manage the shards, so if you take away the reroute API, we will be really unhappy and most likely unable to upgrade to the next version of Elasticsearch, unless allocation filtering lives up to its promise (and so far, it has not).
</comment><comment author="jonaf" created="2016-06-10T16:21:31Z" id="225228795">@s1monw I've revisited the allocation filtering API, and I think it would be fine to replace our existing allocation algorithm with the filtering API. However, the current implementation requires tagging individual nodes with specific details, and it doesn't seem like these updates can be made to individual nodes after they are already running. The filtering API does support excluding nodes based on name or IP address, which is exactly what we need, but it only seems to support _one_ name or IP address, and I would need to provide a _list_ of names or IP addresses (sometimes up to 30 or more). Is there some way to provide a list of IP addresses of nodes to exclude in the filter API?
</comment><comment author="ywelsch" created="2016-06-10T16:52:05Z" id="225236090">You can specify multiple values, comma-separated:

https://www.elastic.co/guide/en/elasticsearch/reference/1.7/index-modules-allocation.html
</comment><comment author="jonaf" created="2016-06-10T17:15:31Z" id="225241730">Thanks @ywelsch ! I'll try that and see if it works. I guess I expected a JSON array rather than a comma-separated string. I didn't notice the comma-separation or that it would work for `_ip` exclusion.
</comment><comment author="s1monw" created="2016-06-10T18:23:14Z" id="225258489">&gt; Thanks @ywelsch ! I'll try that and see if it works. I guess I expected a JSON array rather than a comma-separated string. I didn't notice the comma-separation or that it would work for _ip exclusion.

I ran into the same trap lately, I think we should fix this in master and 2.x to also accept a json array.
</comment><comment author="s1monw" created="2016-06-10T18:29:27Z" id="225260110">&gt; How hard would it really be for the reroute API to just use a queue? Or the same mechanism allocation filtering uses? Because really, it seems like it should be the same implementation under the hood, because your'e doing the same thing, it's just that the filtering API has a blanket "rule" or set of rules to do the relocations, and the reroute API allows maximum flexibility with deciding a "rule" for every shard. To me, this is the identical problem under the hood.

the two APIs are orthogonal. When you move a shard you take all control on your end make the final decision and fire off relocations without respecting throttling. There is no state involved here we don't go and iteratively apply that move command. With allocation filtering you set a state on the index or the cluster that is applied in each reroute such that we can just throttle relocations since we know we will come back and retry on the next cluster change.

I'm really fed up with the `_reroute` API. I think we should remove it, it's too trappy and it's almost like making a native call in java, you should only do it if there is really no other solution out there. The problem is that 100% of the users that I have seen using it, use it because they didn't know about feature X or didn't configure feature X correctly or disable allocation and forget about it.  I will open an issue to remove this API. It's a bug that we have it.
</comment><comment author="jonaf" created="2016-06-10T18:47:45Z" id="225264725">Thanks for the explanation describing how the reroute API is different from other API's. As I said before, it was our only option for this use-case in Elasticsearch 1.3.9 (the filtering API was available, but did not work for our use-case). I'm doing some testing now to ensure that we can use the filtering API rather than manually allocating shards. It would be much better if we could use this API (our current allocation algorithm is _very_ complex).

I think there are some edge-cases where the reroute API may be the only way to recover a cluster, and sometimes you _really_ need to recover a cluster, rather than replace or reindex it. I think that it may make sense to move some API's into some "admin" area, expressly where dangerous (or as you put it, "expert") API's live, and set the documentation to describe clearly that the API should not be used for typical operations (i.e., on a healthy cluster). But I guess that's a separate discussion for the issue you open to remove it.
</comment><comment author="s1monw" created="2016-06-10T18:52:39Z" id="225265906">&gt; I think there are some edge-cases where the reroute API may be the only way to recover a cluster, and sometimes you really need to recover a cluster, rather than replace or reindex it. I think that it may make sense to move some API's into some "admin" area, expressly where dangerous (or as you put it, "expert") API's live, and set the documentation to describe clearly that the API should not be used for typical operations (i.e., on a healthy cluster). But I guess that's a separate discussion for the issue you open to remove it.

I'd be very interested in hearing what situations you think its useful in? see #18819 feel free to contribute
</comment><comment author="jonaf" created="2016-06-12T04:08:08Z" id="225409022">Thanks, @s1monw and everyone else for your help on this. I'll follow-up on the appropriate issues separately now.

&gt; I ran into the same trap lately, I think we should fix this in master and 2.x to also accept a json array.

Is there already an issue for this? If not, I'm happy to create one / submit a PR for it. I couldn't find an open one.

As a final note for the benefit of anyone else that comes across this issue: using allocation filtering in Elasticsearch 1.7.3 _did_ satisfy our requirements, and we've since switched to that in lieu of the reroute API and our own allocation algorithm (although we're continuing to use the reroute API in versions of Elasticsearch prior to 1.5).
</comment><comment author="s1monw" created="2016-06-13T09:06:02Z" id="225526517">&gt; Is there already an issue for this? If not, I'm happy to create one / submit a PR for it. I couldn't find an open one.

please do!
</comment><comment author="s1monw" created="2016-06-13T09:06:25Z" id="225526604">&gt; As a final note for the benefit of anyone else that comes across this issue: using allocation filtering in Elasticsearch 1.7.3 did satisfy our requirements, and we've since switched to that in lieu of the reroute API and our own allocation algorithm (although we're continuing to use the reroute API in versions of Elasticsearch prior to 1.5).

that is awesome news! thanks for reporting back
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bg_count is always zero when doing a significant terms aggregation with a custom background set that does not overlap with the foreground set on any shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18738</link><project id="" key="" /><description>ES 2.3.2 server and java api

Whenever I perform a significant terms aggregation with a custom background set (where the background set and foreground sets do not overlap) and iterate through the `SignificantTerms.Bucket` entry objects in the java object result, I always get a `subsetDF`, `subsetSize` and `supersetDF` but `supersetSize` is always zero. It doesn't matter which significance heuristic I use. Is this expected? Is there another way I should be getting the per-bucket counts from the background set?

Example json request and response below, notice that the bg_count is zero for all of the results

**Request**

``` json
{
  "size" : 0,
  "query" : {
    "bool" : {
      "filter" : {
        "range" : {
          "createdAt" : {
            "from" : "2016-05-19",
            "to" : "2016-05-25",
            "include_lower" : true,
            "include_upper" : true
          }
        }
      }
    }
  },
  "aggregations" : {
    "significant_categories" : {
      "significant_terms" : {
        "field" : "category",
        "size" : 5,
        "background_filter" : {
          "range" : {
            "createdAt" : {
              "from" : "2016-05-12",
              "to" : "2016-05-18",
              "include_lower" : true,
              "include_upper" : true
            }
          }
        },
        "mutual_information" : {
          "include_negatives" : true,
          "background_is_superset" : false
        }
      }
    }
  }
}
```

**Response**

``` json
{
  "took" : 35,
  "timed_out" : false,
  "_shards" : {
    "total" : 18,
    "successful" : 18,
    "failed" : 0
  },
  "hits" : {
    "total" : 543817,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "significant_categories" : {
      "doc_count" : 543817,
      "buckets" : [ {
        "key" : "Condiments_And_Sweeteners.Hot_sauces",
        "doc_count" : 29754,
        "score" : 0.028138457657103193,
        "bg_count" : 0
      }, {
        "key" : "Chemistry.Fatty_acids",
        "doc_count" : 11676,
        "score" : 0.01090831954637533,
        "bg_count" : 0
      }, {
        "key" : "Beverages.Milk_substitutes",
        "doc_count" : 10549,
        "score" : 0.009848021517760529,
        "bg_count" : 0
      }, {
        "key" : "Chemistry.Essential_oils",
        "doc_count" : 8521,
        "score" : 0.007944061722549585,
        "bg_count" : 0
      }, {
        "key" : "Condiments_And_Sweeteners.Salad_dressings",
        "doc_count" : 6036,
        "score" : 0.005618049544322981,
        "bg_count" : 0
      } ]
    }
  }
}
```
</description><key id="158486081">18738</key><summary>bg_count is always zero when doing a significant terms aggregation with a custom background set that does not overlap with the foreground set on any shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryanrozich</reporter><labels><label>:Aggregations</label><label>won't fix</label></labels><created>2016-06-04T04:03:15Z</created><updated>2016-06-15T13:05:11Z</updated><resolved>2016-06-13T13:33:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-06T17:14:44Z" id="224024429">@markharwood could you take a look please?
</comment><comment author="markharwood" created="2016-06-06T17:45:06Z" id="224033179">I see the foreground and background sets do not overlap at all in terms of documents.
Is it conceivable that the category fields used in the foreground set were not used at all in the background set? That would certainly be a significant change (nothing to 29754 sales). In my tests I see bg_count as &gt;0 except for when the foreground set terms do not exist in the background set.
</comment><comment author="ryanrozich" created="2016-06-08T05:25:22Z" id="224491126">Thanks @markharwood for taking a look here.

What I am doing in this example is comparing log events from a previous time period to a current time period, so you are right the foreground and background do not overlap in terms of documents. However, I have verified that these terms do exist in the background set through a second query.

Here is an example using region as the field for aggregating significant terms

**Request**

``` json
{
  "size" : 0,
  "query" : {
    "bool" : {
      "filter" : {
        "range" : {
          "createdAt" : {
            "from" : "2016-05-19",
            "to" : "2016-05-25",
            "include_lower" : true,
            "include_upper" : true
          }
        }
      }
    }
  },
  "aggregations" : {
    "significant_categories" : {
      "significant_terms" : {
        "field" : "geoip.real_region_name.raw",
        "size" : 20,
        "background_filter" : {
          "range" : {
            "createdAt" : {
              "from" : "2016-05-12",
              "to" : "2016-05-18",
              "include_lower" : true,
              "include_upper" : true
            }
          }
        },
        "mutual_information" : {
          "include_negatives" : true,
          "background_is_superset" : false
        }
      }
    }
  }
}
```

**Response**

``` json
{
  "took" : 40,
  "timed_out" : false,
  "_shards" : {
    "total" : 18,
    "successful" : 18,
    "failed" : 0
  },
  "hits" : {
    "total" : 543817,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "significant_categories" : {
      "doc_count" : 543817,
      "buckets" : [ {
        "key" : "California",
        "doc_count" : 40928,
        "score" : 0.03900360479716631,
        "bg_count" : 0
      }, {
        "key" : "New York",
        "doc_count" : 24273,
        "score" : 0.0228697776932465,
        "bg_count" : 0
      }, {
        "key" : "Florida",
        "doc_count" : 19577,
        "score" : 0.018386887777233945,
        "bg_count" : 0
      }, {
        "key" : "New Jersey",
        "doc_count" : 12524,
        "score" : 0.011707184425668314,
        "bg_count" : 0
      }, {
        "key" : "Massachusetts",
        "doc_count" : 11555,
        "score" : 0.01079440443731854,
        "bg_count" : 0
      }, {
        "key" : "Illinois",
        "doc_count" : 10948,
        "score" : 0.010223223979040349,
        "bg_count" : 0
      }, {
        "key" : "Indiana",
        "doc_count" : 9297,
        "score" : 0.00867199072530764,
        "bg_count" : 0
      }, {
        "key" : "Michigan",
        "doc_count" : 7680,
        "score" : 0.007156007888300512,
        "bg_count" : 0
      }, {
        "key" : "Connecticut",
        "doc_count" : 7320,
        "score" : 0.006818941429226905,
        "bg_count" : 0
      }, {
        "key" : "Georgia",
        "doc_count" : 7103,
        "score" : 0.006615843146036082,
        "bg_count" : 0
      }, {
        "key" : "Colorado",
        "doc_count" : 6712,
        "score" : 0.006250039611369389,
        "bg_count" : 0
      }, {
        "key" : "Arizona",
        "doc_count" : 6609,
        "score" : 0.006153708646216918,
        "bg_count" : 0
      }, {
        "key" : "Minnesota",
        "doc_count" : 6590,
        "score" : 0.00613594029555656,
        "bg_count" : 0
      }, {
        "key" : "Maryland",
        "doc_count" : 6420,
        "score" : 0.005976980257079576,
        "bg_count" : 0
      }, {
        "key" : "Missouri",
        "doc_count" : 5485,
        "score" : 0.0051033406093923235,
        "bg_count" : 0
      }, {
        "key" : "British Columbia",
        "doc_count" : 5466,
        "score" : 0.005085598730652028,
        "bg_count" : 0
      }, {
        "key" : "North Carolina",
        "doc_count" : 4702,
        "score" : 0.004372558367334927,
        "bg_count" : 0
      }, {
        "key" : "Louisiana",
        "doc_count" : 4329,
        "score" : 0.0040246998498886764,
        "bg_count" : 0
      }, {
        "key" : "Nevada",
        "doc_count" : 2683,
        "score" : 0.0024916935042830425,
        "bg_count" : 0
      }, {
        "key" : "Alabama",
        "doc_count" : 2592,
        "score" : 0.002407037622030747,
        "bg_count" : 0
      } ]
    }
  }
}
```

Notice that the `bg_count` is always zero in each bucket. So I've created a follow up query with a `filters` aggregtion over the background set to get the actual counts for each bucket. See request and response below:

**Request**

``` json
{
  "size" : 0,
  "query" : {
    "bool" : {
      "filter" : [ {
        "terms" : {
          "geoip.real_region_name.raw" : [ "North Carolina", "New York", "Indiana", "Minnesota", "British Columbia", "California", "Florida", "Alabama", "New Jersey", "Michigan", "Massachusetts", "Illinois", "Colorado", "Connecticut", "Missouri", "Louisiana", "Georgia", "Maryland", "Arizona", "Nevada" ]
        }
      }, {
        "range" : {
          "createdAt" : {
            "from" : "2016-05-12",
            "to" : "2016-05-18",
            "include_lower" : true,
            "include_upper" : true
          }
        }
      } ]
    }
  },
  "aggregations" : {
    "sigterms" : {
      "filters" : {
        "filters" : {
          "California" : {
            "term" : {
              "geoip.real_region_name.raw" : "California"
            }
          },
          "New York" : {
            "term" : {
              "geoip.real_region_name.raw" : "New York"
            }
          },
          "Florida" : {
            "term" : {
              "geoip.real_region_name.raw" : "Florida"
            }
          },
          "New Jersey" : {
            "term" : {
              "geoip.real_region_name.raw" : "New Jersey"
            }
          },
          "Massachusetts" : {
            "term" : {
              "geoip.real_region_name.raw" : "Massachusetts"
            }
          },
          "Illinois" : {
            "term" : {
              "geoip.real_region_name.raw" : "Illinois"
            }
          },
          "Indiana" : {
            "term" : {
              "geoip.real_region_name.raw" : "Indiana"
            }
          },
          "Michigan" : {
            "term" : {
              "geoip.real_region_name.raw" : "Michigan"
            }
          },
          "Connecticut" : {
            "term" : {
              "geoip.real_region_name.raw" : "Connecticut"
            }
          },
          "Georgia" : {
            "term" : {
              "geoip.real_region_name.raw" : "Georgia"
            }
          },
          "Colorado" : {
            "term" : {
              "geoip.real_region_name.raw" : "Colorado"
            }
          },
          "Arizona" : {
            "term" : {
              "geoip.real_region_name.raw" : "Arizona"
            }
          },
          "Minnesota" : {
            "term" : {
              "geoip.real_region_name.raw" : "Minnesota"
            }
          },
          "Maryland" : {
            "term" : {
              "geoip.real_region_name.raw" : "Maryland"
            }
          },
          "Missouri" : {
            "term" : {
              "geoip.real_region_name.raw" : "Missouri"
            }
          },
          "British Columbia" : {
            "term" : {
              "geoip.real_region_name.raw" : "British Columbia"
            }
          },
          "North Carolina" : {
            "term" : {
              "geoip.real_region_name.raw" : "North Carolina"
            }
          },
          "Louisiana" : {
            "term" : {
              "geoip.real_region_name.raw" : "Louisiana"
            }
          },
          "Nevada" : {
            "term" : {
              "geoip.real_region_name.raw" : "Nevada"
            }
          },
          "Alabama" : {
            "term" : {
              "geoip.real_region_name.raw" : "Alabama"
            }
          }
        }
      }
    }
  }
}
```

**Response**

``` json
{
  "took" : 18,
  "timed_out" : false,
  "_shards" : {
    "total" : 18,
    "successful" : 18,
    "failed" : 0
  },
  "hits" : {
    "total" : 214123,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "sigterms" : {
      "buckets" : {
        "California" : {
          "doc_count" : 39262
        },
        "New York" : {
          "doc_count" : 25489
        },
        "Florida" : {
          "doc_count" : 20512
        },
        "New Jersey" : {
          "doc_count" : 14308
        },
        "Massachusetts" : {
          "doc_count" : 13731
        },
        "Illinois" : {
          "doc_count" : 12788
        },
        "Indiana" : {
          "doc_count" : 4603
        },
        "Michigan" : {
          "doc_count" : 9815
        },
        "Connecticut" : {
          "doc_count" : 4457
        },
        "Georgia" : {
          "doc_count" : 7617
        },
        "Colorado" : {
          "doc_count" : 8229
        },
        "Arizona" : {
          "doc_count" : 7056
        },
        "Minnesota" : {
          "doc_count" : 7485
        },
        "Maryland" : {
          "doc_count" : 7218
        },
        "Missouri" : {
          "doc_count" : 6259
        },
        "British Columbia" : {
          "doc_count" : 5440
        },
        "North Carolina" : {
          "doc_count" : 10301
        },
        "Louisiana" : {
          "doc_count" : 3819
        },
        "Nevada" : {
          "doc_count" : 3140
        },
        "Alabama" : {
          "doc_count" : 2594
        }
      }
    }
  }
}
```

Notice that when I query the background set directly for doc counts using this secondary query and a `filters` aggregation I can get the actual counts in the background set. I would have expected these to be the counts that were returned in the `bg_count` field of the significant terms result. 

My use case here is looking for anomalies in a current time period versus a previous time period (i.e. what is trending up or down). My intention is to produce reports from the query above like:

```
California change:   +8.36% significance score: 0.039004
             *  cur_wk:  7.53% (   40,928 views out of   543,817)
             * prev_wk:  6.95% (   39,262 views out of   565,291)

  New York change:   -1.01% significance score: 0.022870
             *  cur_wk:  4.46% (   24,273 views out of   543,817)
             * prev_wk:  4.51% (   25,489 views out of   565,291)

   Florida change:   -0.79% significance score: 0.018387
             *  cur_wk:  3.60% (   19,577 views out of   543,817)
             * prev_wk:  3.63% (   20,512 views out of   565,291)

New Jersey change:   -9.01% significance score: 0.011707
             *  cur_wk:  2.30% (   12,524 views out of   543,817)
             * prev_wk:  2.53% (   14,308 views out of   565,291)

Massachusetts change:  -12.52% significance score: 0.010794
             *  cur_wk:  2.12% (   11,555 views out of   543,817)
             * prev_wk:  2.43% (   13,731 views out of   565,291)

  Illinois change:  -11.01% significance score: 0.010223
             *  cur_wk:  2.01% (   10,948 views out of   543,817)
             * prev_wk:  2.26% (   12,788 views out of   565,291)

   Indiana change: +109.95% significance score: 0.008672
             *  cur_wk:  1.71% (    9,297 views out of   543,817)
             * prev_wk:  0.81% (    4,603 views out of   565,291)

  Michigan change:  -18.66% significance score: 0.007156
             *  cur_wk:  1.41% (    7,680 views out of   543,817)
             * prev_wk:  1.74% (    9,815 views out of   565,291)

Connecticut change:  +70.72% significance score: 0.006819
             *  cur_wk:  1.35% (    7,320 views out of   543,817)
             * prev_wk:  0.79% (    4,457 views out of   565,291)

   Georgia change:   -3.07% significance score: 0.006616
             *  cur_wk:  1.31% (    7,103 views out of   543,817)
             * prev_wk:  1.35% (    7,617 views out of   565,291)

  Colorado change:  -15.21% significance score: 0.006250
             *  cur_wk:  1.23% (    6,712 views out of   543,817)
             * prev_wk:  1.46% (    8,229 views out of   565,291)

   Arizona change:   -2.64% significance score: 0.006154
             *  cur_wk:  1.22% (    6,609 views out of   543,817)
             * prev_wk:  1.25% (    7,056 views out of   565,291)

 Minnesota change:   -8.48% significance score: 0.006136
             *  cur_wk:  1.21% (    6,590 views out of   543,817)
             * prev_wk:  1.32% (    7,485 views out of   565,291)

  Maryland change:   -7.54% significance score: 0.005977
             *  cur_wk:  1.18% (    6,420 views out of   543,817)
             * prev_wk:  1.28% (    7,218 views out of   565,291)

  Missouri change:   -8.91% significance score: 0.005103
             *  cur_wk:  1.01% (    5,485 views out of   543,817)
             * prev_wk:  1.11% (    6,259 views out of   565,291)

British Columbia change:   +4.45% significance score: 0.005086
             *  cur_wk:  1.01% (    5,466 views out of   543,817)
             * prev_wk:  0.96% (    5,440 views out of   565,291)

North Carolina change:  -52.55% significance score: 0.004373
             *  cur_wk:  0.86% (    4,702 views out of   543,817)
             * prev_wk:  1.82% (   10,301 views out of   565,291)

 Louisiana change:  +17.83% significance score: 0.004025
             *  cur_wk:  0.80% (    4,329 views out of   543,817)
             * prev_wk:  0.68% (    3,819 views out of   565,291)

    Nevada change:  -11.18% significance score: 0.002492
             *  cur_wk:  0.49% (    2,683 views out of   543,817)
             * prev_wk:  0.56% (    3,140 views out of   565,291)

   Alabama change:   +3.87% significance score: 0.002407
             *  cur_wk:  0.48% (    2,592 views out of   543,817)
             * prev_wk:  0.46% (    2,594 views out of   565,291)

```

Without the `bg_count` I cannot calculate the proportion of events in the previous week and therefore cannot calculate the relative change week over week. The secondary query/aggregation is what I need to do for the time being to get this count.

This secondary query is a fine stopgap for a simple example like this, but as I want to do more advanced things like start nesting significant terms aggregations, having to do 2-stage queries for each bucket and each of the sub-buckets starts to get messy and expensive.
</comment><comment author="markharwood" created="2016-06-08T09:08:57Z" id="224532610">I can't reproduce on 2.3.2 and using REST api.

We can either work back from your broken system's config, removing config/data until it works or build up from a simpler working example adding data/config that is the same as your system until it breaks.

Here's my simple example which works with a single shard/node and also multiple shards and nodes

```
DELETE test
PUT test
{
   "settings": {
      "number_of_replicas": "0",
      "number_of_shards": "1"
   },
   "mappings": {
      "doc": {
         "properties": {
            "state": {
               "type": "string",
               "index": "not_analyzed"
            },
            "date": {
               "type": "date",
               "format": "yyyy-MM-dd"
            }
         }
      }
   }
}
POST test/doc
{ "state":"MA", "date":"2016-05-12" }
POST test/doc
{ "state":"CA", "date":"2016-05-12" }
POST test/doc
{ "state":"CA", "date":"2016-05-13" }
POST test/doc
{ "state":"CA", "date":"2016-05-14" }

POST test/doc
{ "state":"MA", "date":"2016-05-19" }
POST test/doc
{ "state":"CA", "date":"2016-05-19" }
POST test/doc
{ "state":"CA", "date":"2016-05-20" }
POST test/doc
{ "state":"CA", "date":"2016-05-21" }
POST test/doc
{ "state":"CA", "date":"2016-05-22" }

POST test/_search
{
   "query": {
      "bool": {
         "filter": {
            "range": {
               "date": {
                  "from": "2016-05-19",
                  "to": "2016-05-25",
                  "include_lower": true,
                  "include_upper": true
               }
            }
         }
      }
   },
   "size": 0,
   "aggs": {
      "keywords": {
         "significant_terms": {
            "field": "state",
            "background_filter": {
               "range": {
                  "date": {
                     "from": "2016-05-12",
                     "to": "2016-05-18",
                     "include_lower": true,
                     "include_upper": true
                  }
               }
            },
            "mutual_information": {
               "include_negatives": true,
               "background_is_superset": false
            }
         }
      }
   }
}
```

Can you help play spot-the-difference with your setup?
</comment><comment author="ryanrozich" created="2016-06-08T13:17:47Z" id="224585563">One difference might be that I am using a logstash-style index with one shard per day. So in this case my foreground and background set data would exist on separate shards, could that make a difference?
</comment><comment author="markharwood" created="2016-06-08T13:41:46Z" id="224592182">&gt; t I am using a logstash-style index with one shard per day.

To be clear, presumably one _index_ per day, not shard.

&gt; could that make a difference?

Yes. In the simple case if you were looking at this Thursday Vs last Thursday and had daily indices each shard would either have a zero-sized foreground or zero-sized background. If the former it would return no results (nothing matched the query) and if the latter it would return bg_count ==0 which is what you are seeing.

There's an assumption in significant_terms that each shard has a reasonable mix of both foreground and background data from which we can spot promising changes locally. If your data/queries are not organised this way it won't work well.

Given your data layout you're probably better advised to issue a single query and return two filtered aggregations one for the states in the current time range and one for the states in the historic time range and diff the values using your own code.
</comment><comment author="ryanrozich" created="2016-06-08T20:48:30Z" id="224722657">Thanks Mark.

If the significant terms are calculated per-shard, its surprising that I am getting any results back at all, since none of the shards will contain examples from both foreground and background sets. What terms am I getting returned in these examples?

For examples like states with low cardinality its feasible to do two filter aggregations and do the diff in memory, but some fields have very high cardinality (tens of thousands of terms). Are there any other suggestions on looking for date-range-based anomalies for indexes that are partitioned by date?
</comment><comment author="ryanrozich" created="2016-06-08T21:43:35Z" id="224738523">From your comment on significant terms assuming that each shard needing to have a reasonable mix of foreground and background data to spot changes locally, I assumed that the buckets I was getting back were meaningless since this is never the case in my examples. 

However, it actually looks like these buckets do represent significant differences between the two date ranges. For instance here is example output based on referrer site

```
 Views coming from referrer site buzzfeed.com changed by +1002.89% over the previous period (significance score: 0.004579)
     *  cur_wk:  0.91% (    4,923 views out of   543,817)
     * prev_wk:  0.08% (      464 views out of   565,291)
```

Taking a quick look in kibana at the foreground and background date range, there does seem to be a very significant difference between the foreground and background sets

![kibana screenshot](https://gmkr.io/s/57588e718e519a8474c1e0a3/0)

I did have to look up the background count in the separate query with the secondary filter query, but isn't it odd that the significant terms aggregation called this out, given how my indexes are partitioned?

Just to check if I got lucky with this other example, I looked at another:

```
 Views coming from referrer site facebook.com changed by  -30.27% over the previous period (significance score: 0.009720)
     *  cur_wk:  1.91% (   10,413 views out of   543,817)
     * prev_wk:  2.75% (   15,523 views out of   565,291)
```

visualizing the data in kibana again shows that there is indeed a negative trend between the background and foreground sets

![Screenshot of Kibana](https://gmkr.io/s/57589030cf468e6678b1e0c2/0)

I did this for several of the other top significant terms buckets, they all seem to represent real significant differences in the foreground and background sets. I'm wondering how this could be, given that on each shard, the foreground count=0 or background count=0?
</comment><comment author="markharwood" created="2016-06-09T08:30:24Z" id="224831918">&gt; What terms am I getting returned in these examples?

From shards with no foreground you'll get no results.
From shards with no background, some significance heuristics I expect will throw an error. You have opted for mutual information and set the `background_is_superset` to false meaning that it should expect foreground and background numbers to be independent. In the absence of _any_ background stats it looks like this heuristic would simply rank based on straight foreground popularity.

&gt; Are there any other suggestions on looking for date-range-based anomalies for indexes that are partitioned by date?

If you could introduce just one dummy document into each shard that listed all states and had a dummy=true flag on it you could change your foreground query to be dateRange=X OR dummy=true. This would have the effect of always bringing each state into the foreground set at least once regardless if the shard was in the desired date range. That then means each shard would always return foreground and background stats and the final answer computed on the reduce node would have all the background stats - they would just be off by one because of the dummy doc.
</comment><comment author="markharwood" created="2016-06-13T13:33:00Z" id="225581602">Currently working as-designed and although it would be nice to have a fix we don't see an easy work-around from the elastic side. My last comment offers a possible solution although this is less than ideal.

Closing for now but I can re-open if we have any further bright ideas.
</comment><comment author="ryanrozich" created="2016-06-15T13:01:10Z" id="226179805">Thanks Mark for the help and explanations. It would be great in the future to be able to have significant terms work for looking for differences in date ranges on dat-partitioned indexes, but for now I'll do the diffs in code. At least I can use the elasticsearch SignificanceHeurisitic implementations so I don't have to re-implement all the math.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index creation does not cause the cluster health to go RED</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18737</link><project id="" key="" /><description>Previously, index creation would momentarily cause the cluster health to
go RED, because the primaries were still being assigned and activated.
This commit ensures that when an index is created or an index is being
recovered during cluster recovery and it does not have any active
allocation ids, then the cluster health status will not go RED, but
instead be YELLOW.

Relates #9126 

Closes #9106
</description><key id="158483907">18737</key><summary>Index creation does not cause the cluster health to go RED</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha4</label></labels><created>2016-06-04T02:57:09Z</created><updated>2016-06-27T13:36:10Z</updated><resolved>2016-06-20T14:55:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-04T03:00:47Z" id="223732495">@bleskes @ywelsch FYI, most of the PR is tests.
</comment><comment author="djschny" created="2016-06-05T13:21:29Z" id="223812770">How will this affect client programmers. For example after creating an index, as a developer it was common to code in logic to check the cluster health and make sure it is not `red` prior to starting to index data. With this change it looks like there might be a false state of affairs as indexing could start even though the primaries have not been fully allocated yet, and hence indexing errors.

If I'm misinterpreting the implementation of the change, then my apologies. If not then I believe we need to make sure we have a `wait_for_completion` style option on the index creation request (I thought we had issue for this, by searching is failing me).
</comment><comment author="abeyad" created="2016-06-05T14:05:51Z" id="223814929">@djschny you are correct, the work is split into two parts that will go into a feature branch before going into master.  The second part is to wait until at least the primaries are initialized before returning from the index creation call, unless a primary shard allocation actually failed.  See the comment here: https://github.com/elastic/elasticsearch/issues/9126#issuecomment-218765557
</comment><comment author="djschny" created="2016-06-05T14:09:31Z" id="223815101">Cool thanks @abeyad did not realize this was part of larger picture goal. All &#128077; as my previous comments are addressed in the parent issue you referenced. Thanks.
</comment><comment author="ywelsch" created="2016-06-08T10:13:59Z" id="224547567">Left some comments. As @bleskes noted, we should ensure that cluster health goes red if we cannot allocate newly created/recovered index to any node. I would suggest adding a boolean to `UnassignedInfo` that captures the information if shard could not be assigned on first (or subsequent) tries due to allocation deciders saying NO.
</comment><comment author="bleskes" created="2016-06-09T14:57:09Z" id="224921572">I think it will be clearer and easier to debug if we store the last allocation decision on the UnassignedInfo, instead of a boolean.
</comment><comment author="ywelsch" created="2016-06-15T13:19:30Z" id="226184307">@abeyad I left some comments here and there. I don't think we should add the full allocation decision to the unassigned info, that will blow up the cluster state unnecessarily. We can only use the decision type (YES/NO/THROTTLE). As you've noticed, there are also places where it's not clear what that the decision type should be (during shard fetching e.g., where I suggested using THROTTLE).
</comment><comment author="bleskes" created="2016-06-15T13:25:49Z" id="226186049">&gt; during shard fetching e.g., where I suggested using THROTTLE

Just a thought and it might not fit, but if we don't have an explicit decision (like when we're fetching) maybe we should make it explicit and have the last decision be `null` - i.e., no decision.
</comment><comment author="abeyad" created="2016-06-15T13:30:04Z" id="226187211">&gt; Just a thought and it might not fit, but if we don't have an explicit decision (like when we're fetching) maybe we should make it explicit and have the last decision be null - i.e., no decision.

@bleskes 
I have that option in place now for decisions on `UnassignedInfo` (the decision itself is an `Optional` in `UnassignedInfo`).  @ywelsch is suggesting that should never be nullable and we should come up with a logical decision in its place, even when there is none from an allocation decider.
</comment><comment author="abeyad" created="2016-06-17T14:34:21Z" id="226785460">@ywelsch @bleskes https://github.com/elastic/elasticsearch/pull/18737/commits/aee01c4d6f0d6221c1bcff2dceed9683bff10c5f addresses the latest review comments
</comment><comment author="ywelsch" created="2016-06-17T15:31:29Z" id="226801557">Left a few very minor comments. LGTM otherwise. Thanks @abeyad 
</comment><comment author="abeyad" created="2016-06-20T14:55:18Z" id="227166757">@ywelsch thank you for your review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>-XX:OnOutOfMemoryError is broken by Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18736</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**: any

**OS version**: BSD / Linux

**Description of the problem including expected versus actual behavior**:
#13753 introduced seccomp stuff, which uses kernel voodoo that I don't really understand (like, at all) to drop permissions to exec/fork/execve/etc syscalls. Unfortunately, the side effect of this is `-XX:OnOutOfMemoryError` is now utterly broken.

**Steps to reproduce**:
1. Install my [production-ready, webscale OOM plugin](https://github.com/samcday/elasticsearch-oomonster)
2. Run `JAVA_OPTS="-XX:OnOutOfMemoryError=pwd" bin/elasticsearch`
3. `curl localhost:9200/_cat/oom`

With the above steps, you'll see some output like this:

```
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="pwd"
#   Executing "pwd"...
```

But the command doesn't actually run. On Linux, that's the end of it. If you run it in OSX, you'll actually get a more insightful message immediately after the above output:

```
os::fork_and_exec failed: Resource temporarily unavailable (35)
```

If you then follow the same steps above, but instead run `JAVA_OPTS=-Des.bootstrap.seccomp=false bin/elasticsearch`, you'll see that the OOM handler works properly.
</description><key id="158482444">18736</key><summary>-XX:OnOutOfMemoryError is broken by Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samcday</reporter><labels /><created>2016-06-04T02:17:26Z</created><updated>2016-06-07T13:26:57Z</updated><resolved>2016-06-07T13:26:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-04T13:24:01Z" id="223755477">this works as designed. the only think i can think of is that we should maybe add this to the documentation OR fail to start up if seccomp is enabled and `-XX:OnOutOfMemoryError`is set too.
</comment><comment author="jasontedor" created="2016-06-04T14:02:06Z" id="223757259">You should just upgrade to 8u92 and use the new flag `ExitOnOutOfMemoryError`, it does not need to fork and is compatible with seccomp.

Applying this patch to Elasticsearch:

``` diff
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index cf33770..ecd4c47 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -261,6 +261,12 @@ public class Node implements Closeable {
             }
         }

+        try {
+            int[] a = new int[16777216];
+        } catch (OutOfMemoryError e) {
+            // intentional so we do not otherwise die
+        }
+
         logger.info("initialized");
     }
```

so that we are attempting to allocate a 64 MB array _after_ seccomp is installed:

```
09:53:58 &#9178; [jason:~/elasticsearch/elasticsearch-5.0.0-SNAPSHOT] $ ES_JAVA_OPTS="-Xms64m -Xmx64m -XX:+ExitOnOutOfMemoryError" ./bin/elasticsearch
[2016-06-04 09:54:08,358][INFO ][node                     ] [Cassandra Nova] version[5.0.0-SNAPSHOT], pid[89211], build[2d57bbd/2016-06-04T12:30:20.088Z], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_92/25.92-b14]
[2016-06-04 09:54:08,359][INFO ][node                     ] [Cassandra Nova] initializing ...
[2016-06-04 09:54:08,973][INFO ][plugins                  ] [Cassandra Nova] modules [percolator, lang-mustache, lang-painless, ingest-grok, reindex, aggs-matrix-stats, lang-expression, lang-groovy], plugins []
[2016-06-04 09:54:08,995][INFO ][env                      ] [Cassandra Nova] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [367.7gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2016-06-04 09:54:08,995][INFO ][env                      ] [Cassandra Nova] heap size [61.8mb], compressed ordinary object pointers [true]
[2016-06-04 09:54:10,355][INFO ][node                     ] [Cassandra Nova] initialized
[2016-06-04 09:54:10,355][INFO ][node                     ] [Cassandra Nova] starting ...
[2016-06-04 09:54:10,410][INFO ][transport                ] [Cassandra Nova] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2016-06-04 09:54:10,412][WARN ][bootstrap                ] [Cassandra Nova] please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster
[2016-06-04 09:54:13,456][INFO ][cluster.service          ] [Cassandra Nova] new_master {Cassandra Nova}{d8Gh-txJRzK0EBO-lZuHqA}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-06-04 09:54:13,473][INFO ][http                     ] [Cassandra Nova] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2016-06-04 09:54:13,473][INFO ][node                     ] [Cassandra Nova] started
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid89211.hprof ...
Heap dump file created [25956023 bytes in 0.163 secs]
Terminating due to java.lang.OutOfMemoryError: Java heap space
09:54:13 &#9178; [jason:~/elasticsearch/elasticsearch-5.0.0-SNAPSHOT] 3 $ 
```

and without the flag:

```
09:54:13 &#9178; [jason:~/elasticsearch/elasticsearch-5.0.0-SNAPSHOT] 3 $ ES_JAVA_OPTS="-Xms64m -Xmx64m" ./bin/elasticsearch
[2016-06-04 09:57:41,286][INFO ][node                     ] [Nathaniel Richards] version[5.0.0-SNAPSHOT], pid[89285], build[2d57bbd/2016-06-04T12:30:20.088Z], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_92/25.92-b14]
[2016-06-04 09:57:41,287][INFO ][node                     ] [Nathaniel Richards] initializing ...
[2016-06-04 09:57:41,792][INFO ][plugins                  ] [Nathaniel Richards] modules [percolator, lang-mustache, lang-painless, ingest-grok, reindex, aggs-matrix-stats, lang-expression, lang-groovy], plugins []
[2016-06-04 09:57:41,810][INFO ][env                      ] [Nathaniel Richards] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [367.7gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2016-06-04 09:57:41,810][INFO ][env                      ] [Nathaniel Richards] heap size [61.8mb], compressed ordinary object pointers [true]
[2016-06-04 09:57:43,251][INFO ][node                     ] [Nathaniel Richards] initialized
[2016-06-04 09:57:43,252][INFO ][node                     ] [Nathaniel Richards] starting ...
[2016-06-04 09:57:43,328][INFO ][transport                ] [Nathaniel Richards] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2016-06-04 09:57:43,331][WARN ][bootstrap                ] [Nathaniel Richards] please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster
[2016-06-04 09:57:46,385][INFO ][cluster.service          ] [Nathaniel Richards] new_master {Nathaniel Richards}{lyd84Tf9R4GficBHI3-gWg}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-06-04 09:57:46,402][INFO ][http                     ] [Nathaniel Richards] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2016-06-04 09:57:46,402][INFO ][node                     ] [Nathaniel Richards] started
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid89285.hprof ...
Heap dump file created [26087334 bytes in 0.147 secs]
[2016-06-04 09:57:46,665][INFO ][node                     ] [Nathaniel Richards] error
java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.node.Node.start(Node.java:407)
        at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:197)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:256)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
        at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
[2016-06-04 09:57:46,667][INFO ][gateway                  ] [Nathaniel Richards] recovered [0] indices into cluster_state
```

If you need to do something other than exit on out of memory error then echoing @rmuir and @s1monw, sorry, we are not going to support that.
</comment><comment author="samcday" created="2016-06-04T20:46:19Z" id="223777396">Awesome, I did not know about the `-XX:ExitOnOutOfMemoryError`. We wanted a script to fire so that we could raise a Datadog event for the OOM (then we can easily setup alarms on this occurrence).

FWIW, I don't think it's reasonable to close this issue just yet. I wasted a solid 3 hours trying to figure out why my OOM handler wasn't working. There was _zero_ documentation on this apparently known issue. Can we re-open this and close once there's documentation indicating that `-XX:OnOutOfMemoryError` is intentionally and permanently broken, and `-XX:ExitOnOutOfMemoryError` is the only option?
</comment><comment author="samcday" created="2016-06-06T21:27:15Z" id="224093742">@s1monw  thoughts?
</comment><comment author="jasontedor" created="2016-06-06T21:38:02Z" id="224096425">I opened #18756.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Low level Rest Client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18735</link><project id="" key="" /><description>The low level RestClient can be created by simply providing the hosts that it should point to: `RestClient.builder().setHosts(new HttpHost("localhost", 9200).build()`. 
It also allows to provide a `CloseableHttpClient` instance to be used, otherwise one with default settings is going to be created. 
It exposes a `performRequest` method, that accepts all the needed arguments to send an http request: method, endpoint, parameters, body, and headers.
It supports trace logging like all other language clients, to log each request and response in curl format.

`client` is a new gradle submodule that depends on apache http client and only a few other transitive deps. 
`client-sniffer` is an additional gradle submodule that depends on `client` and jackson, as it's able to parse the nodes info api response and inject hosts into an existing RestClient instance. Sniffer comes also with a listener that gets notified on failure and can optionally sniff hosts on failure.

Both new modules are java 7 compatible and get compiled with source 1.7 &amp; target 1.7.

This PR also makes our Rest tests use the new RestClient, and it also replaces any usage of apache http client in tests with the new low level RestClient.

What's left, which can probably be worked on after this PR gets in:
- write some more tests, especially around custom http clients, concurrency and sniffing task.
- javadocs should be ok but we need some docs page on how to use it and configuration for common scenarios like ssl, proxy, basic auth etc. (although we can refer to apache http client documentation)
- add async variant of performRequest method

Closes #7743
</description><key id="158472943">18735</key><summary>Low level Rest Client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>das awesome</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T23:29:37Z</created><updated>2016-07-13T12:24:25Z</updated><resolved>2016-06-22T07:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-04T00:23:53Z" id="223723772">&gt; forbidden-apis for test classes is currently disabled, find a way to make excludes work so that we can re-enable it

We'll probably want a different list for the clients, but I think you can make `SuppressForbidden` work just by making your own annotation named that.
</comment><comment author="nik9000" created="2016-06-04T00:25:51Z" id="223723955">&gt; client-sniffer requires license and SHA for client dependency, not sure that's correct

We are supposed to be skipping our own libraries in that check....

Yeah, this stuff can wait until after merge I think...
</comment><comment author="nik9000" created="2016-06-04T00:47:53Z" id="223725706">@javanna this makes me so happy!
</comment><comment author="nik9000" created="2016-06-04T00:50:26Z" id="223725903">Maybe instead of `:client` and `:client-sniffer` it should be `:client:core` and `:client:sniffer`?
</comment><comment author="nik9000" created="2016-06-04T00:52:48Z" id="223726080">What lengths do we plan to go to to get 1.7 compatibility? How far do we feel like we have to go before we can merge vs before we can release?
</comment><comment author="javanna" created="2016-06-04T06:39:52Z" id="223740116">thanks for taking a look @nik9000 :

&gt; We'll probably want a different list for the clients, but I think you can make SuppressForbidden work just by making your own annotation named that.

The only problem is the HttpServer we use for testing. I tried with @SuppressForbidden and excludes but neither of them worked for me, no idea why. help! :)

&gt; What lengths do we plan to go to to get 1.7 compatibility? How far do we feel like we have to go before we can merge vs before we can release?

I don't know. I don't feel safe enough compiling against java 8 although with target and source 1.7. But we could solve it having CI compiling and running with real java 7 against these modules? not sure it will work though. I guess we require 1.8 for the build itself?
</comment><comment author="costin" created="2016-06-04T08:12:51Z" id="223743433">A big +1 for Java 7 compatibility from my side. To keep things in check and make sure we don't end up using Java 8 APIs by accident, we could use something like [Animal Sniffer](http://www.mojohaus.org/animal-sniffer/) to double check they are not leaking in.
This way the rest client could simply compile on Java8 like the rest of ES.

Another idea though is to make the rest client a separate project from ES mainly because their lifecycles are different. A big advantage of using REST is that the client is not tightly coupled to ES itself and thus it is not forced to upgrade on dot releases.
</comment><comment author="dadoonet" created="2016-06-04T12:20:42Z" id="223752724">&gt; Another idea though is to make the rest client a separate project from ES

Was thinking the same. 

It should be treated as like any other client IMO.
</comment><comment author="djschny" created="2016-06-05T12:34:43Z" id="223810752">+++ for separate from ES code base; its the proper way
</comment><comment author="nik9000" created="2016-06-05T13:13:50Z" id="223812431">This proposal puts it in in the Elasticsearch project because Elasticsearch is required to test it and because it is required to test Elasticsearch. Running the REST tests does that for both of them. And the http client is used to test a few places inside Elasticsearch. And it'll be used in the reindex module for reindex-from-remote.

I agree the two projects probably deserve different release cycles though. I agree it doesn't make a lot sense to version the initial release of this 5.0.0-beta3. Nor does it make sense to release versions of it every time we release an Elasticsearch version. They just might not be changes. And sometimes we might want to release the client without Elasticsearch. Releasing Elasticsearch is a big effort but I expect we'll want to release the client more frequently at first.

Speaking of releases, we'll probably also want to have a version matrix for the java client too one day.

I don't know how to make sense of any of this. The second half of my comment is all "it should be its own project" stuff but the first have makes it pretty clear that "it isn't separate". I dunno what the solution is. Gradle should let us _technically_ organize it and release it how we like. I think we'll just have to pick the least painful way for all involved.
</comment><comment author="s1monw" created="2016-06-05T15:23:32Z" id="223819173">&gt; +++ for separate from ES code base; its the proper way

we will have to share code eventually - moving in to a different code base now will only complicate things. Don't put statements like this in such an issue they are just distraction and bike-shedding. We get a client in now to solve a ton of problems. I don't know yet if we will have different release cycles, we can but we don't have to there is nothing forcing us except of potential scenarios. In reality clients are released to:
- fix bugs (we do that too in ES very very frequently)
- add support for APIs (ES has to be released for this) 

there is nothing that couples the REST client to an ES release so there is 0 argument to put it into a different codebase. We can have this discussion later if there are good reasons not just bike-shedding.

&gt; I agree the two projects probably deserve different release cycles though. I agree it doesn't make a lot sense to version the initial release of this 5.0.0-beta3. Nor does it make sense to release versions of it every time we release an Elasticsearch version. They just might not be changes. And sometimes we might want to release the client without Elasticsearch. Releasing Elasticsearch is a big effort but I expect we'll want to release the client more frequently at first.

this discussion can happen once we have something committed and working. Our major target here is a replacement for transport client and remove the dependency on the binary protocol and in turn only depend on REST interface. My reasoning for going with java 7 was to allow the server to move faster if we need to and make it interesting for users that have java 7 requirement. We will stick with the core and it's release cycle we don't need to rush anything. stuff will be experimental, we will move as fast as we can just as we do with core.

&gt; I don't know how to make sense of any of this. The second half of my comment is all "it should be its own project" stuff but the first have makes it pretty clear that "it isn't separate". I dunno what the solution is. Gradle should let us technically organize it and release it how we like. I think we'll just have to pick the least painful way for all involved.

we don't need to be perfect on the first shot. It's experimental, we can move things out later but at this stage we need to move fast and get stuff out there to gain feedback lets not complicate things because the code is not as well organized as it could.
</comment><comment author="djschny" created="2016-06-05T16:06:58Z" id="223821466">&gt; Don't put statements like this in such an issue they are just distraction and bike-shedding.

It was not meant to be a distraction, it was to register feedback for that direction. My statement did not imply that it had to be done as part of this request and that was not the intention. How else would the team recommend for the community to engage on discussion of this nature if not here?
</comment><comment author="nik9000" created="2016-06-05T17:03:18Z" id="223824341">I'm happy with this direction and think we should do it. We can always move things around later. I vote @javanna crams some NORELEASEs in it and merges it and we open issues for things like the version number (#18741), and whatever else we see. We can resolve them as we can resolve them. Getting this in soon lets more folks work in parallel to improve stuff and lets me start using it for stuff like reindex from remote. We're early enough in the 5.0 release cycle still this should be safe.

I'm happy to work on the build issues if @rjernst is willing to review my horrible gradle hackery.
</comment><comment author="nik9000" created="2016-06-05T21:34:55Z" id="223839182">So I just looked into running animal sniffer. I think that might be enough. I mean, we seem to be doing it ok in 2.x. Right?

Using [this](https://github.com/xvik/gradle-animalsniffer-plugin) (so far very easy) gradle plugin, running animal sniffer is like a 3 line build change:
https://gist.github.com/nik9000/0b150b8a186db8216d52aeb7a9e09fa7

It can wait until after the merge but I've added this here so if we want to use it we can.
</comment><comment author="nik9000" created="2016-06-06T00:57:45Z" id="223849126">&gt; It can wait until after the merge

It _should_ wait until after the merge because it doesn't pass. But that is fine. We can fix it!
</comment><comment author="javanna" created="2016-06-06T14:55:09Z" id="223983861">I addressed some comments, but before we discuss NORELEASEs and we shove this PR in, I think it needs some serious cycles of review, too few comments till now :)

The SHA problem with the client submodule is a blocker, as it needs a new SHA at each change which is not feasible. I am looking into it and reached out to Nik for help. Same problem with licenses.

What do others think about animal-sniffer? shall I integrate it into the build?
</comment><comment author="nik9000" created="2016-06-06T15:23:27Z" id="223992622">&gt; What do others think about animal-sniffer? shall I integrate it into the build?

I'd do it after I think. It saw quite a few small things that needed to change but nothing jumped out at me as scary.
</comment><comment author="makeyang" created="2016-06-07T08:54:34Z" id="224219876">is it possible to support java 6 for this feature or will it compatiable if I compile it with java 6 by my own?
</comment><comment author="s1monw" created="2016-06-07T10:26:36Z" id="224240810">&gt; is it possible to support java 6 for this feature or will it compatiable if I compile it with java 6 by my own?

we will not support Java 6 for this. We are still in the process of deciding if we go with java 7 which seems likely, or if we go with java 8. Java 6 was EOLed Feb 2013 which is &gt; 3 years. We try to be compatible with older JVMs here but Java 7 already is very old and has been EOLed over a year ago. 
</comment><comment author="tstibbs" created="2016-06-08T09:44:44Z" id="224540920">I'm really excited about a java rest client, but not sure if I am missing something here - it doesn't seem to add very much above what you get from just using apache http client?

I realise this is referred to as a "low level" client - long term are we going to see something that implements the `org.elasticsearch.client.Client` interface and uses the `IndexRequest` and `IndexResponse` classes? This would make it much easier to migrate.
</comment><comment author="s1monw" created="2016-06-08T09:50:19Z" id="224542220">&gt; I'm really excited about a java rest client, but not sure if I am missing something here - it doesn't seem to add very much above what you get from just using apache http client?

you are right, it will only provide sniffing etc. It will also be tested and have defaults etc.

&gt; I realise this is referred to as a "low level" client - long term are we going to see something that implements the org.elasticsearch.client.Client interface and uses the IndexRequest and IndexResponse classes? This would make it much easier to migrate.

we can't do this at this point and it will never implement that interface It would pull in all ES dependencies and you would have no win what so ever. We will build sugar on top of the _low level_ client especially for CRUD actions (search etc) but not in this issue. This is purely the infrastructure to build on top of it. Imagine all the API work this PR would be unreviewable! 

Trust me we will invest a lot into this client and it will remove the transport client etc. and make REST the primary interface. We will go very incremental and wise on this development. We might change out mind X times down the road but this is a feature for users and it should have good APIs, they will just come later.
</comment><comment author="javanna" created="2016-06-09T15:50:05Z" id="224938534">I have addressed all comments I got up until now. Here is what's left:
- ignore our own libraries in  dependency licenses check and sha check (in progress)
- make @SuppressForbidden work for forbiddenApisTest (in progress)
- move tests to `RandomizedRunner` rather than `LuceneTestCase`. Are we sold on having a `client-test` project shared by `client` and `client-sniffer`? I wish there were better options than yet another module.
- deeper java7 compatibility check. Shall we go for animal sniffer? I didn't get much feedback on that yet.
</comment><comment author="s1monw" created="2016-06-09T16:10:29Z" id="224944747">&gt; deeper java7 compatibility check. Shall we go for animal sniffer? I didn't get much feedback on that yet.

++

&gt; move tests to RandomizedRunner rather than LuceneTestCase. Are we sold on having a client-test project shared by client and client-sniffer? I wish there were better options than yet another module.

is there a way to have it all in a single module and have dedicated builds to build the individual jars? maybe @rjernst has ideas
</comment><comment author="s1monw" created="2016-06-09T16:14:27Z" id="224945856">@javanna I wonder if we can merge all in one module and make the dependencies on jackson etc optional?
</comment><comment author="nik9000" created="2016-06-09T18:16:14Z" id="224981239">&gt; @javanna I wonder if we can merge all in one module and make the dependencies on jackson etc optional?

I'm not sure it is worth the effort to have fewer modules. The cost of a modules isn't super high compared to the complexity of optional dependencies, building multiple jars from the same code, and making sure the dependencies are right for all of those jars. Those problems are the things that having multiple modules is meant to solve.
</comment><comment author="soran1991" created="2016-06-10T10:52:11Z" id="225152623">Hi Luka, I urgently need to speak to you in regards to a comment you posted a while ago on stack overflow. The post was on finding the perimeter of an image. Can you please get in touch with me. Many thanks.
</comment><comment author="s1monw" created="2016-06-13T10:01:06Z" id="225539053">I left some more comments - awesome job! I think we are close here!

I think from here we have 2 immediate followups:
- work on adding async version of the client for serach etc.
- work on adding a higher level API to make it easier for the users to use the client.

For the latter I think it should be again it's own module and we might even start with just depending on core for things like SearchSourceBuilder? I really think it could take us very far if we start redesigning a client interface just for the basic functions like CRUD and reusing the builders we have in core? We can even make this dependency optional and allow people to use it if they need to? 
</comment><comment author="javanna" created="2016-06-14T07:51:52Z" id="225805525">I addressed or replied to all comments. 

As for compatibility 1.7, I think what we have now is as good as it gets: we compile with target and source 1.7, use animal-sniffer to detect usage of 1.8 apis, and tests depend on lucene-test version 5 that is compatible with 1.7. Not sure if this last bit can cause problems in our builds, looking for feedback on that.

There's also a couple of points that are still under discussion about blacklisting nodes and making FailureListener immutable in RestClient. Waiting for some resolution on that too.
</comment><comment author="s1monw" created="2016-06-14T08:58:48Z" id="225820713">@javanna I left some comments to resolve the blockers. LGTM otherwise!!!
</comment><comment author="rjernst" created="2016-06-14T18:16:48Z" id="225969201">&gt; and tests depend on lucene-test version 5 that is compatible with 1.7. Not sure if this last bit can cause problems in our builds, looking for feedback on that

This should be ok, as long as randomized runner doesn't break compat (ie so both 5 and 6 use the same randomized runner apis). The gradle changes look ok to me.

As a follow up I think we should do what Nik recommended, which was to move these under a `client` dir. I think then we can have the rest api spec in there as well.  Remember the project name in gradle does not need to match what we call it in maven, but we also need to be careful: I would not call it `:client:core` because I have seen issues with intellij and even with gradle itself where it gets confused when projects have the same name (even if their full path is different).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for the new Java 9 MethodHandles#arrayLength() factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18734</link><project id="" key="" /><description>Since build 120, Java 9 contains a factory method to get a highly optimized (inlined) MethodHandle for the array length. This PR adds support for it through dynamic lookup.

Once ES is on Java 9, the workaround hack should be removed (same for the StringConcat).

See: https://bugs.openjdk.java.net/browse/JDK-8156915
</description><key id="158459657">18734</key><summary>Add support for the new Java 9 MethodHandles#arrayLength() factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T21:37:48Z</created><updated>2016-06-04T12:36:19Z</updated><resolved>2016-06-03T22:21:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-03T21:38:11Z" id="223701084">This PR was tested with build 121 of Java 9.
</comment><comment author="uschindler" created="2016-06-03T21:42:48Z" id="223701921">In an ideal world, Elasticsearch would create a Multi-Release JAR file, and would compile the new code only against Java 9, while compiling the old code against Java 8 (both in same JAR file); similar for String Concats!

This is another issue. I am investigating the same for Apache Lucene, where we can supply alternative class files for stuff like VarHandles in MMapDirectory or the new Math functions.
</comment><comment author="rmuir" created="2016-06-03T22:19:26Z" id="223708332">&gt; This PR was tested with build 121 of Java 9.

Thanks for doing this! looks like we are currently on 9-ea+120 in jenkins, but it should still be fine right? I see 8156915 in that one: http://download.java.net/java/jdk9/changes/jdk-9+120.html
</comment><comment author="uschindler" created="2016-06-03T22:20:53Z" id="223708555">Yes build 120 has it. I was just not able to test it, because there was no Windows build for the Policeman.
</comment><comment author="rmuir" created="2016-06-03T22:22:03Z" id="223708743">thanks @uschindler 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrap lines at 140 characters (:qa projects)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18733</link><project id="" key="" /><description /><key id="158449267">18733</key><summary>Wrap lines at 140 characters (:qa projects)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T20:34:13Z</created><updated>2016-06-28T09:51:39Z</updated><resolved>2016-06-05T19:56:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-03T20:37:03Z" id="223688205">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add rollover API to switch index aliases given some predicates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18732</link><project id="" key="" /><description>The rollover index API rolls an alias over to a new index when the existing
index is considered to be too large or too old.

The API accepts a single alias name and a list of `conditions`.  The alias
must point to a single index only.  If the index satisfies the specified
conditions then a new index is created and the alias is switched to point to
the new alias.

``` bash
PUT /logs-0001 &lt;1&gt;
{
  "aliases": {
    "logs_write": {}
  }
}

POST logs_write/_rollover &lt;2&gt;
{
  "conditions": {
    "max_age":   "7d",
    "max_docs":  1000
  }
}
```

&lt;1&gt; Creates an index called `logs-0001` with the alias `logs_write`.
&lt;2&gt; If the index pointed to by `logs_write` was created 7 or more days ago, or
    contains 1,000 or more documents, then the `logs-0002` index is created
    and the `logs_write` alias is updated to point to `logs-0002`.

The above request might return the following response:

``` bash
{
  "old_index": "logs-0001",
  "new_index": "logs-0002",
  "rolled_over": true, &lt;1&gt;
  "dry_run": false, &lt;2&gt;
  "conditions": { &lt;3&gt;
    "[max_age: 7d]": false,
    "[max_docs: 1000]": true
  }
}
```

 &lt;1&gt; Whether the index was rolled over.
 &lt;2&gt; Whether the rollover was dry run.
 &lt;3&gt; The result of each condition.

**Naming the new index**

If the name of the existing index ends with `-` and a number -- e.g.
`logs-0001` -- then the name of the new index will follow the same pattern,
just incrementing the number (`logs-0002`).

If the old name doesn't match this pattern then you must specify the name for
the new index as follows:

``` bash
POST my_alias/_rollover/my_new_index_name
{...}
```

**Defining the new index**

The settings, mappings, and aliases for the new index will be taken from any
matching index templates. Additionally, the body of the
request accepts `settings`, `mappings`, and `aliases` just like the
indices-create-index API, which will override any values
set in matching index templates:

``` bash
PUT /logs-0001
{
  "aliases": {
    "logs_write": {}
  }
}

POST logs_write/_rollover
{
  "conditions" : {
    "max_age": "7d",
    "max_docs": 1000
  },
  "settings": { &lt;1&gt;
    "index.number_of_shards": 2 &lt;1&gt;
  }
}
```

&lt;1&gt; Set settings to override matching index template, `mappings` and `aliases`
    can also be provided.

**Dry run**

The rollover API supports `dry_run` mode, where request conditions can be
checked without performing the actual rollover:

``` bash
PUT /logs-0001
{
  "aliases": {
    "logs_write": {}
  }
}

POST logs_write/_rollover?dry_run
{
  "conditions" : {
    "max_age": "7d",
    "max_docs": 1000
  }
}
```

closes #18647
</description><key id="158445386">18732</key><summary>Add rollover API to switch index aliases given some predicates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Index APIs</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T20:11:26Z</created><updated>2016-06-17T15:49:06Z</updated><resolved>2016-06-17T15:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-03T20:55:45Z" id="223692312">cool!
</comment><comment author="s1monw" created="2016-06-03T21:23:06Z" id="223697979">I like the overall approach! Yet, I think we should work more on:
- the request, I think there is too much magic involved and we should really just ask for the target index name in the request
- on the response end I think we should have a boolean that says if we rolled over or not. if we did we can also return which conditions met and which index was created etc.
- I think we need a simulate flag to check only the conditions but don't roll over?
- After mulling on it I think we should only have age and num docs as conditions, index size is arbitrary and might be completely off due to pending merges etc.
</comment><comment author="areek" created="2016-06-06T23:05:04Z" id="224115419">Thanks @nik9000  and @s1monw for the review! I addressed the comments and would love another round of feedback. The rest test is still not passing and I am currently investigating. 
Updated PR: 
- simplified the request to only ask for the `alias` to be switched 
- added flag for simulation (don't rollover even if conditions are met)
- enhanced the response to include condition status, whether the index was rolled over and whether the rollover index was automatically created.
- nuked max size condition
</comment><comment author="s1monw" created="2016-06-07T09:00:29Z" id="224221266">@areek it looks great. I left some minors, I think we also need to add body support for the create index request....
</comment><comment author="areek" created="2016-06-08T22:42:17Z" id="224753005">Thanks @s1monw for the feedback! I added body support for the underlying create index request to override matching index template settings (though unlike create index request overriding custom index metadata is not supported in the body). Added documentation and updated the PR description. Would  be awesome to have this reviewed :)
</comment><comment author="s1monw" created="2016-06-09T09:26:08Z" id="224844184">i think it looks great! @clintongormley can you take a look too please
</comment><comment author="clintongormley" created="2016-06-09T10:38:12Z" id="224859192">&gt;  creating the index if it does not exist. 

I think it should fail if the index already exists.

&gt; Using the http GET method for the API runs the rollover in simulated mode, where request conditions can be checked without performing the actual rollover. Setting simulate=true as a request parameter also runs the request in simulated mode.

This will break in Console (and maybe the clients, depending on settings).  Doing a GET request (without simulate=true) can get changed to a POST request because of the body, and end up actually executing the rollover.  I'd change this simulate mode to always require the flag (which I'd call `dry_run`, as we use this elsewhere).

&gt; The _rollover API is similar to indices-create-index and accepts settings, mappings and aliases
&gt; to override the index create request for a non-existent rolled over index.

I'm having second thoughts about this.  If you make changes in the body, then these changes won't be reflected in the next rollover unless you either (a) repeat the changes in the next rollover request or (b) update the templates.  I'm wondering if we should make templates the only way to configure the rolled-over index.

&gt; I am still torn on if we should force the user into our naming pattern. Today we require the user to start with ${some_name}-${some_number} and we increment that number on each rollover. I think we should make it optional for the user to specify the next indices name maybe via /{index}/_rollover/{new_index} and then skip the pattern enforcement. @clintongormley what do you think?

I agree, eg what if the user wants to convert `mylogs` into a rolling index, or even `mylogs-01` to `mylogs-00001`? They should be able to do so with `POST mylogs_alias/_rollover/mylogs-00001` 

If the target index name is not provided, then we should require that the index name matches our pattern.
</comment><comment author="s1monw" created="2016-06-09T10:43:56Z" id="224860236">&gt; This will break in Console (and maybe the clients, depending on settings). Doing a GET request (without simulate=true) can get changed to a POST request because of the body, and end up actually executing the rollover. I'd change this simulate mode to always require the flag (which I'd call dry_run, as we use this elsewhere).

++
</comment><comment author="s1monw" created="2016-06-09T10:52:07Z" id="224861721">&gt; I'm having second thoughts about this. If you make changes in the body, then these changes won't be reflected in the next rollover unless you either (a) repeat the changes in the next rollover request or (b) update the templates. I'm wondering if we should make templates the only way to configure the rolled-over index.

templates will apply always - I really want this stuff to be per request since relying on templates make things very hard if you wanna do something different based on a condition that matched. lets say you have a condition `age &gt; 2d AND numDocs &lt; 1M` you might want to cut the number of shards in a half. the next request is `age &lt; 2d AND numDocs &gt;1M`  you might wanna bump stuff up? (that reminds me that you might need some boolean logic here &#128131; - I think we should add MUST | SHOULD operators to the conditions as well to be really useful? Followup might also add conditions like day of week and things like this? maybe I am  just confused?
</comment><comment author="clintongormley" created="2016-06-09T11:48:17Z" id="224872206">&gt; templates will apply always - I really want this stuff to be per request since relying on templates make things very hard if you wanna do something different based on a condition that matched. lets say you have a condition age &gt; 2d AND numDocs &lt; 1M you might want to cut the number of shards in a half. the next request is age &lt; 2d AND numDocs &gt;1M you might wanna bump stuff up? (that reminds me that you might need some boolean logic here &#128131; - I think we should add MUST | SHOULD operators to the conditions as well to be really useful? Followup might also add conditions like day of week and things like this? maybe I am just confused?

No, I think this is a good idea, but rather than creating a new syntax, we could just support scripting and pass size, age, etc, in as parameters to the script.  This would allow ultimate flexibility.  We should make this index-matcher a reusable component that can be applied to other index management APIs.

But as a follow up, not in this PR. 

+1 to keeping the create-index body in this PR.
</comment><comment author="s1monw" created="2016-06-09T11:52:07Z" id="224872912">&gt; No, I think this is a good idea, but rather than creating a new syntax, we could just support scripting and pass size, age, etc, in as parameters to the script. This would allow ultimate flexibility. We should make this index-matcher a reusable component that can be applied to other index management APIs.

huge +1
</comment><comment author="areek" created="2016-06-09T18:53:47Z" id="224991643">&gt; I think it should fail if the index already exists.

Now we fail when the rollover index already exists.

&gt; This will break in Console (and maybe the clients, depending on settings). Doing a GET request (without simulate=true) can get changed to a POST request because of the body, and end up actually executing the rollover. I'd change this simulate mode to always require the flag (which I'd call dry_run, as we use this elsewhere).

aah, I removed the option of using `GET` request for the rollover api and renamed `simulate` option to `dry_run`. Now the user has to explicitly set `dry_run=true` in the request params to execute rollover in dry-run mode.

&gt; I agree, eg what if the user wants to convert mylogs into a rolling index, or even mylogs-01 to mylogs-00001? They should be able to do so with POST mylogs_alias/_rollover/mylogs-00001
&gt; If the target index name is not provided, then we should require that the index name matches our pattern.

Now users have the option to provide the name for the new rolling index in the rollover request.

```
POST logs_alias/_rollover/mylogs-1
```

and when the rolling index name is not specified, we fall back to requiring the old index name to have `{prefix}-{num}` format to generate the new rolling index name.

&gt; No, I think this is a good idea, but rather than creating a new syntax, we could just support scripting and pass size, age, etc, in as parameters to the script. This would allow ultimate flexibility. We should make this index-matcher a reusable component that can be applied to other index management APIs.

That sounds great! I was wondering how to generalize this so other index management APIs can work on indices that match conditions. Though, I think we can do this in a separate PR. 

&gt;  think we should add MUST | SHOULD operators to the conditions as well to be really useful?

Should we do this in a separate PR?

Thanks @s1monw and @clintongormley for the feedback! I updated the PR and the description according to the discussions, WDYT?
</comment><comment author="s1monw" created="2016-06-10T07:35:43Z" id="225113479">I left some minors, LGTM - if @clintongormley is happy with the docs I don't need another review round once the last comments are fixed
</comment><comment author="clintongormley" created="2016-06-13T19:18:01Z" id="225680275">I improved the docs - please check that they are actually correct :)
</comment><comment author="s1monw" created="2016-06-14T14:49:16Z" id="225906148">@clintongormley I don't see you comment about accepting the full create index body here? did I miss something?
</comment><comment author="clintongormley" created="2016-06-14T14:53:25Z" id="225907553">@s1monw it's here: https://github.com/elastic/elasticsearch/pull/18802#issuecomment-225667500
</comment><comment author="clintongormley" created="2016-06-14T14:54:54Z" id="225908036">And I've just realised my comment is in the wrong place.  It was meant to apply to this issue.  Copied here:

&gt; @areek Sorry for the late comment but I think that the rollover API should accept a full create-index body, rather than be limited to settings and aliases, otherwise you have to use index templates regardless.

In fact I may have misunderstood completely and the above is already the case on the rollover API?
</comment><comment author="areek" created="2016-06-15T19:30:52Z" id="226294743">Thanks @clintongormley for the awesome documentation, it LGTM. I updated the docs 

&gt; In fact I may have misunderstood completely and the above is already the case on the rollover API?

Yes, it is already the case for the rollover API and works as documented.

This looks ready to be merged :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix a race condition in reindex's rethrottle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18731</link><project id="" key="" /><description>If you rethrottled the request while is was performing a scroll
request then it wouldn't properly pick up the rethrottle for that
batch. This was causing test failure and might cause issues for
users. The work around is simple though: just issue the rethrottle
again with a slightly faster throttle than the first time.

Caught by:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=centos/525/console

Closes #18744
</description><key id="158441580">18731</key><summary>Fix a race condition in reindex's rethrottle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>bug</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T19:48:45Z</created><updated>2016-06-08T18:01:14Z</updated><resolved>2016-06-08T18:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-03T19:49:18Z" id="223677152">@dakrone and @tlrx, this might interest you?
</comment><comment author="nik9000" created="2016-06-07T18:11:57Z" id="224366359">Rebased to make the merge clean. Looks like no review started yet so it was safe.
</comment><comment author="nik9000" created="2016-06-08T17:32:54Z" id="224667912">@dakrone, could you have a look when you get a chance?
</comment><comment author="dakrone" created="2016-06-08T17:52:23Z" id="224673698">Left one comment, otherwise LGTM
</comment><comment author="nik9000" created="2016-06-08T17:59:36Z" id="224675907">Thanks for reviewing @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move naming conventions check into buildSrc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18730</link><project id="" key="" /><description>This will let things that don't depend on :test:framework like the client use it.
</description><key id="158412598">18730</key><summary>Move naming conventions check into buildSrc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T17:11:30Z</created><updated>2016-06-14T22:31:06Z</updated><resolved>2016-06-14T22:31:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-03T17:14:06Z" id="223637811">@javanna, this if for you!

@rjernst, can you have a look at this? I played around a bit with putting these in a subproject of buildSrc but that didn't seem right because it is just two java files. The funnies part of this is how I get the classpath of buildSrc in the main build. I'm ok with doing it this way but, like lots of gradle things, I'm sure I could do it in a more sane way. 
</comment><comment author="javanna" created="2016-06-04T06:40:58Z" id="223740164">thanks a lot @nik9000 !
</comment><comment author="nik9000" created="2016-06-05T13:56:50Z" id="223814446">@rjernst, I reworked a bit after poking around. Instead of using `Class.forName(name)` I've started using `Class.forName(name, false, NamingConventions.class.getClassLoader())` because it speeds up the test a bunch not to initialize the classes we're checking the naming conventions for. I can't use `Class.forName(name, false, null)` because that'd load the class with the bootstrap classloader rather than `NamingConventions`'s classloader.
</comment><comment author="nik9000" created="2016-06-13T22:18:10Z" id="225725751">@rjernst can you have a look when you get a chance?
</comment><comment author="rjernst" created="2016-06-14T18:31:15Z" id="225973787">This looks good, but I think we need to make sure it will still work for external plugin authors, since we add the check automatically there. I left a comment on how to fix that.
</comment><comment author="nik9000" created="2016-06-14T19:31:20Z" id="225990828">@rjernst I reworked the dependency. It works for local projects and I think it ought to work for external ones as well.
</comment><comment author="nik9000" created="2016-06-14T19:32:46Z" id="225991230">&gt; @rjernst I reworked the dependency. It works for local projects and I think it ought to work for external ones as well.

Almost. Now there is circular dependency. I'll fix it!
</comment><comment author="nik9000" created="2016-06-14T20:40:06Z" id="226009137">&gt; Almost. Now there is circular dependency. I'll fix it!

Or not.... Breaking the dependency wouldn't be too hard but I'm not happy with the experience switching from a branch with this to one without - the build fails and the only way to fix it is to `rm -rf buildSrc/build-bootstrap`. So I think I should make this its own module then....

If so, should it be a module in buildSrc or a module in test?

@rjernst, what do you think?
</comment><comment author="rjernst" created="2016-06-14T20:44:29Z" id="226010343">&gt; If so, should it be a module in buildSrc or a module in test?

IIRC, @ywelsch found subprojects in buildSrc break intellij in horrible ways (which is really unfortunate). 

What is the exact problem you see? I would expect there to be no problem if you depend on the build tools artifact, and have a project substitution with build-tools.
</comment><comment author="nik9000" created="2016-06-14T21:23:51Z" id="226020696">@rjernst I broke the circular dependency. It was a project dependency so I had to reach in a break it in PrecommitTasks. If there is a better way please let me know. I hate having such deep magic in there but.... &#128557; 
</comment><comment author="rjernst" created="2016-06-14T21:31:42Z" id="226022642">LGTM. Your hack is fine I think. We have something similar in a couple places, and it is because we have a most things needing "everything" and a few that want minimal. If we want to make this "right" we would split this up into more gradle plugins, instead of having the all encompassing `elasticsearch.build` plugin. We may get there eventually, but what you have is no worse than what we already have (see similar examples in BuildPlugin).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace replica count, auto expand and rack awareness with topology constraints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18729</link><project id="" key="" /><description>The current replica count, auto expand and rack awareness are really all competing for the same idea of shard safety, and not succeeding.  They try to give shard safety but fail to be flexible and cause either hardship to managing the system, false errors, or situations where things just don't appear to work.  
### The following would replace these concepts in their entirety:
#### _Introducing "Elasticsearch Topology Contraints"_

You can guide or force the distribution of replicas by setting the following constraints on combinations of node attributes.  The use of "replicas count" below indicates primary + replica shards as a whole as is no longer "extra copies".
- DESIRE MIN REPLICAS &lt;count&gt;   (soft minimum)
- DESIRE MAX REPLICAS &lt;count&gt;  (soft maximum)
- REQUIRE MIN REPLICAS &lt;count&gt; 
- REQUIRE MAX REPLICAS &lt;count&gt; 
- DESIRE PRIMARY _(primary shards)_
- REQUIRE PRIMARY _(primary shards)_
- DISALLOW PRIMARY  _(primary shards)_
- DESIRE COMPLETE SHARDS _(shard sets)_
- REQUIRE COMPLETE SHARDS _(shard sets)_
- DISALLOW COMPLETE SHARDS _(shard sets)_
- DESIRE INCOMPLETE SHARDS _(shard sets)_
- REQUIRE INCOMPLETE SHARDS _(shard sets)_
- DISALLOW INCOMPLETE SHARDS _(shard sets)_
- DESIRE PRIMARY AND REPLICA SHARDS COLOCATED _(but not within same node)_
- REQUIRE PRIMARY AND REPLICA SHARDS COLOCATED _(but not within same node)_
- DISALLOW PRIMARY AND REPLICA SHARDS COLOCATED _(default for not within same node)_
- REQUIRE INDEX APPROVAL _(for indexes to be on node)_
- INCLUDE INDEX, EXCLUDE INDEX, REQUIRE INDEX, APPROVE INDEX _(for indexes on node)_

Node attributes are defined by the configuration as:
- what are the list of possible attributes for the entire cluster, such as "zone", "rack", "nodetype"
- what are the known attribute values for a given node, such as "zone: us-east", "rack: 1a", "nodetype: webapp"
- special values are allowed to be matched as node attributes:  `_name`, `_host_ip`, `_publish_ip`, `_ip`, `_host`,  `_index`, `_alias` -- where the special value of `_index` matches index name, and `_alias` matches any index that has the alias attached.  Others are the same as used in the old allocation filtering.  

And node attribute values are matched against constraints using:
- literal values
- wildcards
- some special values such as `_all`
### Deprecated Features

These features are deprecated as they are encompassed in Topology Constraints:
- replica count in index settings
- auto-expand replicas in index settings
- shard allocation awareness
- forced shard allocation awareness
- index shard allocation include, exclude, require
- total shards per node
## Example
### Example Cluster

For all examples below, here is a cluster definition given each node and the node attributes set.
- cluster node attributes:  `zone`, `rack`, `nodetype`
- cluster nodes:
  - `main-node-1`:  zone `us-east`, rack `1a`, nodetype `main`
  - `main-node-2`:  zone `us-east`, rack `1c`,  nodetype `main`
  - `main-node-3`:  zone `us-east`, rack `1d`,  nodetype `main`
  - `main-node-4`:  zone `us-west`, rack `1a`,  nodetype `main`
  - `main-node-5`:  zone `us-west`, rack `1c`,  nodetype `main`
  - `main-node-6`:  zone `us-west`, rack `1d`,  nodetype `main`
  - `webapp-node-1`: zone `us-east`, rack `1a`, nodetype `webapp`
  - `webapp-node-2`:  zone `us-east`, rack `1c`, nodetype `webapp`
  - `webapp-node-3`:  zone `us-west`, rack `1a`, nodetype `webapp`
  - `webapp-node-4`:  zone `us-west`, rack `1d`, nodetype `webapp`
  - `batch-node-1`: zone `us-east`, rack `1d`, nodetype `batch`
- created indexes:
  - `Alpha` with no initial replica count setting
  - `Beta` with no initial replica count setting
  - `ReadHeavy` with no initial replica count setting
  - `VeryImportant` with initial REQUIRE MINIMUM REPLICAS count `3` defined in index settings

_note: the above uses zones that may imply a WAN is present between data centers, and that may be a bad idea for a single cluster, so ignore that and focus on the example and not what those words mean in any given attribute, this is just to make an example that makes sense._
### USE CASE 1:

I require each index to have minimum of 2 replicas, and each zone to have at least 1 replica

Topology constraints:
- ATTRIBUTE `["_index"]` VALUE `["*"]` REQUIRE MIN REPLICAS `2`  
- ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE MIN REPLICAS `1`

In this case a `Alpha`, `Beta`, `ReadHeavy` would have 2 or more replicas (primary+copies) and with one in each of `us-east` and `us-west`, while `VeryImportant` would have 3 replicas (primary+copies) with one spread across the zones.  To avoid this shard split, you can add the constraint:
- ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE COMPLETE SHARDS

Now the 3rd replica set will be in one of the two zones, if you want to lean it towards zone `us-east` then have a higher desired count there.
- ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "VeryImportant"]` DESIRE MIN REPLICAS `2`

The planner would check all of the constraints and given that all the REQUIRED can be satisfied while meeting the DESIRED as well, it would put 2 full copies of `VeryImportant` index in zone `us-east` and one copy in `us-west` to satisfy the required count of `3` for this index.

Final constraints are therefore:
- ATTRIBUTE `["_index"]` VALUE `["*"]` REQUIRE MIN REPLICAS `2`  
- ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE MIN REPLICAS `1`
- ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE COMPLETE SHARDS
- ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "VeryImportant"]` DESIRE MIN REPLICAS `2`

_note:  the use of `_index` in each attribute is for clarity, it could be allowed to be omitted when an index setting is used, therefore assuming `*` or all indexes._
### USE CASE 2:

Same as use case 1 but I also desire that all primary shards to be in zone `us-east` unless that is not possible.
- from case 1:
  - ATTRIBUTE `["_index"]` VALUE `["*"]` REQUIRE MIN REPLICAS `2`  
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE MIN REPLICAS `1`
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE COMPLETE SHARDS
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "VeryImportant"]` DESIRE MIN REPLICAS `2`
- for this case add:
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "*"]` DESIRE PRIMARY
### USE CASE 3:

Same as use case 2 but I also desire that all nodes have replicas of a `VeryImportant` and `ReadHeavy` indexes.
- from case 2:
  - ATTRIBUTE `["_index"]` VALUE `["*"]` REQUIRE MIN REPLICAS `2`  
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE MIN REPLICAS `1`
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE COMPLETE SHARDS
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "VeryImportant"]` DESIRE MIN REPLICAS `2`
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "*"]` DESIRE PRIMARY
- for this case add:
  - ATTRIBUTE `["_index]` VALUE `["VeryImportant,ReadHeavy"]` DESIRE MIN REPLICAS `_all`
### USE CASE 4:

Same as use case 3 but I also want each individual `rack` to have at least 1 replica of each index for safety.
- from case 3:
  - ATTRIBUTE `["_index"]` VALUE `["*"]` REQUIRE MIN REPLICAS `2`  
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE MIN REPLICAS `1`
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE COMPLETE SHARDS
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "VeryImportant"]` DESIRE MIN REPLICAS `2`
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "*"]` DESIRE PRIMARY
  - ATTRIBUTE `["_index]` VALUE `["VeryImportant,ReadHeavy"]` DESIRE MIN REPLICAS `_all`
- for this case add:
  - ATTRIBUTE `["zone", "rack", "_index"]` VALUE `["*", "*", "*"]` DESIRE MIN REPLICAS `1`
### USE CASE 5:

Same as use case 4 but I do not want any indexes on `webapp` or `batch` nodes that are not approved for those nodes.  I only want `ReadHeavy` index on those nodes but without any primary shards.
- from case 4:
  - ATTRIBUTE `["_index"]` VALUE `["*"]` REQUIRE MIN REPLICAS `2`  
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE MIN REPLICAS `1`
  - ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE COMPLETE SHARDS
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "VeryImportant"]` DESIRE MIN REPLICAS `2`
  - ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "*"]` DESIRE PRIMARY
  - ATTRIBUTE `["_index]` VALUE `["VeryImportant,ReadHeavy"]` DESIRE MIN REPLICAS `_all`
  - ATTRIBUTE `["zone", "rack", "_index"]` VALUE `["*", "*", "*"]` DESIRE MIN REPLICAS `1`
- for this case add:
  - ATTRIBUTE `["nodetype", "_index"]` VALUE `["webapp, batch", "*"]` REQUIRE INDEX APPROVAL
  - ATTRIBUTE `["nodetype"]` VALUE `["webapp, batch"]` APPROVE INDEX `ReadHeavy`
  - ATTRIBUTE `["nodetype", "_index"]` VALUE `["webapp, batch", "ReadHeavy"]` DISALLOW PRIMARY

We now have `ReadHeavy` index spreading some of its shards across `webapp` and `batch` nodes excluding any primaries.  But what we actually want is to try and have a full copy per `webapp` machine and surely must have a copy per `batch` machine.  The `webapp` machines could query through to the main cluster if not yet replicated their copy, but the `batch` machines should not to avoid pounding the main cluster nodes from heavy querying.  So we add these constraints:
- ATTRIBUTE `["nodetype", "_host", "_index"]` VALUE `["webapp", "*", "ReadHeavy"]` DESIRE MIN REPLICAS `1`
- ATTRIBUTE `["nodetype", "_host", "_index"]` VALUE `["batch", "*", "ReadHeavy"]` REQUIRE MIN REPLICAS `1`

Now we have caused each host to have a full copy of the index for `webapp` and `batch` nodes, where it is a hard requirement for `batch` and soft for `webapp`.

Final constraints are therefore:
- ATTRIBUTE `["_index"]` VALUE `["*"]` REQUIRE MIN REPLICAS `2`  
- ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE MIN REPLICAS `1`
- ATTRIBUTE `["zone", "_index"]` VALUE `["*", "*"]` REQUIRE COMPLETE SHARDS
- ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "VeryImportant"]` DESIRE MIN REPLICAS `2`
- ATTRIBUTE `["zone", "_index"]` VALUE `["us-east", "*"]` DESIRE PRIMARY
- ATTRIBUTE `["_index]` VALUE `["VeryImportant,ReadHeavy"]` DESIRE MIN REPLICAS `_all`
- ATTRIBUTE `["zone", "rack", "_index"]` VALUE `["*", "*", "*"]` DESIRE MIN REPLICAS `1`
- ATTRIBUTE `["nodetype", "_index"]` VALUE `["webapp, batch", "*"]` REQUIRE INDEX APPROVAL
- ATTRIBUTE `["nodetype"]` VALUE `["webapp, batch"]` APPROVE INDEX `ReadHeavy`
- ATTRIBUTE `["nodetype", "_index"]` VALUE `["webapp, batch", "ReadHeavy"]` DISALLOW PRIMARY
- ATTRIBUTE `["nodetype", "_host", "_index"]` VALUE `["webapp", "*", "ReadHeavy"]` DESIRE MIN REPLICAS `1`
- ATTRIBUTE `["nodetype", "_host", "_index"]` VALUE `["batch", "*", "ReadHeavy"]` REQUIRE MIN REPLICAS `1`

Which is fairly complex, but incredibly powerful.
### USE CASE 6:

What if I have the reverse case of this `ReadHeavy` index and have an index `WriteHeavy` where i want to write mostly to special nodes and then replicate to the others.  The same type of constraints could apply by REQUIRE PRIMARY to a few write-heavy nodes and setting DESIRE MIN REPLICAS to `1` for the same nodes so that they have a complete set of PRIMARY 

If I want to hold replication to other nodes, I could add a REQUIRE INDEX APPROVAL to the other nodes (_note:  need a negation for values so can express "everything but this attribute values"_), do the full index, then remove that constraint letting it replicate across once complete.
### USE CASE 7:

I add a daily index that needs to have constraints, so the name suffix changes on each index, how do constraints apply?  Use wildcards at end of index naming, for example:
- ATTRIBUTE `["nodetype"]` VALUE `["webapp, batch"]` APPROVE INDEX `DailySomething-*`

or
- ATTRIBUTE `["nodetype", "_host", "_index"]` VALUE `["webapp", "*", "DailySomething-*"]` DESIRE MIN REPLICAS `1`

or if the indexes are all added to an alias, constraints could be based on the alias.  The only issue is that sometimes these type of indexes are created and then added to the index after a while, so maybe the temporary index has constraints that match wildcards, and the final alias has constraints that match on it.
- ATTRIBUTE `["zone", "_alias"]` VALUE `["us-east", "DailyAll"]` DESIRE MIN REPLICAS `3`
## Constraints on specific indexes:

Should there be constraints on indexes other than starting minimum replicas?  And should these override topology constraints?  For example, restore a daily generated index with initial REQUIRE MAX REPLICAS `1` replicas and then once restored increase by changing to an index specific DESIRED MIN REPLICAS `_all`; or remove my index specific MAX of 1 and let the topology constraints take over.

Per index constraints are the beginner case, and special case.  Topology should be the production norm.  But allowing index overrides (warning when in conflict with Topology) for special cases allows cases described above.
## Constraint weighting

Should conflicts between constraints be resolved by some weight given to constraints?  If same weight it is a problem, if different weight then the higher weight wins.  You put your safety constraints as the highest.
## Constraint precedence:

Should there be precedence rules for constraints?

For example, for replica count there is precedence here when multiple constraints overlap.  MIN wins over DESIRED and MAX, MAX wins over DESIRED.  A conflict with MAX results in a cluster warning that can be viewed from cluster state.  Only failures on MIN to be satisfied result in index state of RED.  
## GREEN/YELLOW/RED Cluster and Index Status:

Cluster and index status should be queried a bit differently. 
- If you ask at the cluster level you have a rollup of all index status and the constraints that apply. 
- If you ask at the index level you have a rollup of all constraints that apply to the index status and the general index health (all primaries are valid and assigned).  
- If you ask at the node attribute matching level, you have a rollup of all matching indexes/nodes
- If you ask at the specific constraint level, you have the results of that constraint

So in use case 5 where `batch` nodes MUST have a full copy of the index, and `webapp` SHOULD have a full copy.  A `batch` process would wait for yellow state on `ReadHeavy` index by querying status for:
- STATUS OF CONSTRAINTS ATTRIBUTE `["nodetype", "_host", "_index"]` VALUE `["batch", "_me", "ReadHeavy"]`

So if index health of `ReadHeavy` is generally ok (all primaries exist) and the constraints that affect nodetype `batch` on "_me" (my) host for index `ReadHeavy` are all satisfied then I receive a GREEN response.  And my batch would continue.

TODO:  this needs heavy spec'ing to figure out how status vs. constraints are met.  But I think the index base health is only based on index specific constraints and not the global ones.  Other health checks should be from the perspective of the user of the index to ensure the constraints that matter to them are satisfied.  You could always say "for index ReadHeavy are ALL constraints satisfied EVERYWHERE" and do a higher level index check.  But more fine grained makes sense, for example one data center being YELLOW but mine being GREEN should allow me to do what I want to do in my app.
## For Later, DISTANCE Constraints

Adding distance constraints later will allow the system to do smart things about how to know what is SAME rack, NEAR in datacenter, or FAR such as WAN, or with a numeric value so that index replication across WANs can be handled differently such as a topology constraint for FAR sets of nodes use async replication instead of sync.   But they are not needed for the above use cases.
## Related:

see related: #18723 which isn't needed if this issue is done.  Maybe #18723 is a stopgap for 5.x while this issue is for 6.x of Elasticsearch.
</description><key id="158406963">18729</key><summary>Replace replica count, auto expand and rack awareness with topology constraints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2016-06-03T16:38:10Z</created><updated>2016-09-23T19:13:16Z</updated><resolved>2016-06-10T09:45:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-10T09:45:14Z" id="225140399">We just discussed it in our weekly Fixit Friday meeting. While this would be more flexible, this also makes things more complicated while the current way of configuring allocation is simple (at least in the simple cases) and enough for most users. So we'd rather keep the current way.
</comment><comment author="s1monw" created="2016-06-10T09:52:05Z" id="225141721">I also want to add that it is extremely difficult to digest issues like this. I can see what you tried to achieve but we have to break things up into smaller and more digestible solutions. This entire thing seems like implementable in a lot of small PRs and issues and deprecate things step by step. That way it's faster to get improvements to the user, get better code reviews, add better APIs, make things simpler and clear. 
</comment><comment author="apatrida" created="2016-09-21T15:22:38Z" id="248646179">@s1monw First the concept (like this) needs to be accepted as an idea that we want to pursue.  After we can break it down and re-address it as elemental PR's/Issues.  But it has to be thought through as a whole to make something that actually works.  While @jpountz says the current system is simple, because it has some simple parts that you can use to make some simple things happen ... but it also completely fails at making truly scalable topologies for big deployments that need control and have absolutely no way to make it work.  The current individual control mechanisms do not work together to describe cohesive topologies.

So what I was hoping for here was to get agreement on something along these lines, check out use cases and collect more info, then come back with the building blocks to make it happen.  Instead of closing big ideas, there needs to be someway (from outside your internal meetings) to discuss the big things, then come back at them.
</comment><comment author="apatrida" created="2016-09-21T15:37:00Z" id="248650913">@jpountz not sure how simple is a combination of the current features:
- rack awareness, which causes replicas and masters to try to be on separate racks
- forced rack awareness, which works if you can define all of your racks ahead of time and ensures each rack has a copy 
- replica count range such as `3-ALL` which has a permanent bug #2869 which interferes with the other two and doesn't always seem to respond or do what it says it will, and definitely doesn't when using the above

With these three features you can end up in error states and things not really close to what you intended.  You can't control master distribution, you can't do things like ensuring a full copy of an index is on each node, etc.

By constraints you can say a few simple ones to replace the above "simple" features and have documentation that is one page.  So with 2 or 3 constraints that are in identical form you express the same thing, from which you can add a few more to do wondrous topologies.  So there is a simple easy to document version of the above for those simple cases.  Simplicity is how you present it and what you show the simple users.  A documentation problem.   That should limit writing a feature to have full control.

I can write a sample constraint solver if you want to see how it is not very complicated code to answer for a given index and node what should happen.  If that helps.
</comment><comment author="clintongormley" created="2016-09-23T14:15:53Z" id="249204662">Sorry @apatrida - I read this through again and it just made my head hurt.  While reading your examples I couldn't translate them into what the cluster would actually look like. It feels like reading Sendmail config.  Clearly our existing settings are not perfect, but they are at least fairly easy to understand.  I'd prefer working on incremental improvements to what we have instead.
</comment><comment author="apatrida" created="2016-09-23T17:14:54Z" id="249250098">@clintongormley maybe another approach is for PR to allow some plugin to decide shard placement and replica count preferences for the core engine and swap out the current model via plugins.  So an advanced topology plugin could take over this decision making.
</comment><comment author="apatrida" created="2016-09-23T17:16:20Z" id="249250433">Has anyone at elastic actually hit use cases that cannot be done at all with current settings?  I'd be surprised if you have not, because we do regularly and hit walls with the current model that prevent improving performance or reliability.
</comment><comment author="bleskes" created="2016-09-23T19:13:16Z" id="249278587">@apatrida may I suggest that you come up with a simple example where you show a concrete set of nodes (node 1,2,3,4... ), a concrete set of index shards and show how you want them to be layouted on your nodes in the form of "index 1 shard 1 primary, on node 1"?  Also please explain why you want them that way. I see you gave quite detailed examples of possible configuration but they involve many moving pieces and I fail to understand the why. That will help understand what your are trying to achieve. See why it can not  be done with the current system through a concrete example (as you say there are many theoretical limitations) and also think about potential alternative solutions to let you do what you try to do.

Note that while I understand that you try to create a system that allows to do anything, there is a lot of value in a simple system everyone can understand albeit constrained. We try er towards the latter.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>added missing quotes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18728</link><project id="" key="" /><description /><key id="158401259">18728</key><summary>added missing quotes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prosanelli</reporter><labels><label>docs</label></labels><created>2016-06-03T16:06:20Z</created><updated>2016-06-06T16:29:36Z</updated><resolved>2016-06-06T16:29:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T16:09:40Z" id="223622067">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close SearchContext if query rewrite failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18727</link><project id="" key="" /><description>If a query failed to be rewritten (I think that `WrapperQueryBuilder`, `TemplateQueryBuilder` and `PercolateQueryBuilder` are good candidates to fail at rewriting phase) and throws an exception, the `SearchContext` is not properly closed. 

Because it's not closed, the ref count on the underlying `Store` is not up-to-date and will not trigger the `closeInternal()` method on the `Store` when the index is deleted. The test framework is nice and checks/warns about shards that are still locked but in production I'm wondering if this situation can prevent some resources from being correctly released.
</description><key id="158385591">18727</key><summary>Close SearchContext if query rewrite failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T14:53:58Z</created><updated>2016-06-06T07:49:59Z</updated><resolved>2016-06-06T07:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-06-03T15:43:41Z" id="223614900">good catch! LGTM
</comment><comment author="s1monw" created="2016-06-04T13:28:11Z" id="223755660">LGTM 2
</comment><comment author="tlrx" created="2016-06-06T07:49:59Z" id="223890250">@martijnvg @s1monw Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] wait for yellow after setup doc tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18726</link><project id="" key="" /><description>We have many places in the doc where we expect and index to be
yellow before we execute a query. Therefore we have to
always wait for yellow after setup.

Related test failure (I think): https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/606/consoleText
</description><key id="158367028">18726</key><summary>[TEST] wait for yellow after setup doc tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T13:27:41Z</created><updated>2016-06-03T14:37:28Z</updated><resolved>2016-06-03T14:37:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-03T13:32:26Z" id="223579490">I'm still hoping for https://github.com/elastic/elasticsearch/issues/9126 but in the mean time this should help. It won't get everything because we have cases where we use make an index and use it in the same snippet, but this'll still help.
</comment><comment author="brwe" created="2016-06-03T13:42:18Z" id="223581859">@nik9000 addressed your comment. Thank you! 
</comment><comment author="nik9000" created="2016-06-03T13:46:40Z" id="223582917">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support optional ctor args in ConstructingObjectParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18725</link><project id="" key="" /><description>You declare them like

```
static {
  PARSER.declareInt(optionalConstructorArg(), new ParseField("animal"));
}
```

Other than being optional they follow all of the rules of regular
`constructorArg()`s. Parsing an object with optional constructor args
is going to be slightly less efficient than parsing an object with
all required args if some of the optional args aren't specified because
ConstructingObjectParser isn't able to build the target before the
end of the json object.
</description><key id="158366850">18725</key><summary>Support optional ctor args in ConstructingObjectParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T13:26:44Z</created><updated>2016-06-13T17:04:46Z</updated><resolved>2016-06-08T16:39:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-03T13:27:15Z" id="223578193">@jasontedor, I've found myself wanting this for things like reindex from remote. Could you have a look?
</comment><comment author="nik9000" created="2016-06-08T12:50:17Z" id="224578698">@jasontedor, sorry to bother you, but can you have a look? It'd make my persistent task loading nicer....
</comment><comment author="nik9000" created="2016-06-08T14:59:06Z" id="224616800">Thanks for reviewing @jasontedor ! I pushed dfca470 to address the issues you raised.
</comment><comment author="jasontedor" created="2016-06-08T15:33:25Z" id="224627793">LGTM.
</comment><comment author="nik9000" created="2016-06-08T16:39:21Z" id="224650419">Thanks for reviewing @jasontedor !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Settings in elasticsearch.yml don't show up in /_cluster/settings API call </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18724</link><project id="" key="" /><description>Currently, any cluster settings in elasticsearch.yml are not visible when doing a `GET /_cluster/settings` API call.   Without any prior setting of values via this API call, the response is simply:

```
{"persistent":{},"transient":{}}
```

It would be great to have this api call return all the current settings, including those set in the elasticsearch.yml config and even any non-defined default values.  
</description><key id="158361671">18724</key><summary>Settings in elasticsearch.yml don't show up in /_cluster/settings API call </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hartfordfive</reporter><labels /><created>2016-06-03T12:59:15Z</created><updated>2016-06-03T13:02:27Z</updated><resolved>2016-06-03T13:02:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T13:02:27Z" id="223572613">Already fixed in 5.0.  Just add '?include_defaults' to the GET request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Further shard / index placement control by node/racks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18723</link><project id="" key="" /><description>For scalability sometimes you have a read heavy index you want to replicate onto other nodes, but no other indexes.  Use cases that benefit from this control:
- a batch system starts, wants access to the cluster, but wants some heavy read indexes replicated locally.  so collocate a node on the batch shard that starts with the node, replicates, then runs batches and stays up to date automatically.  You don't want any other indexes residing on this node but a full copy of the desired indexes replicated.
- a web app is running and doing really high QPS on something such as a recommendation engine or percolation, so wants a local full copy of some of the indexes, that are live and up to date, while accessing the rest of the system at the same time.
- use cases where you have write-little/read-a-billion and want a local copy per reading process, but kept up to date at all times with the writes coming in.

You can solve these by having two clusters, but that means either more machines or more processes on the same machines, and trying to balance CPU and memory usage between competing clusters that are not coordinated.

Trying to do this in one cluster, is almost possible with rack awareness, but the settings never allow real control over the behavior.  You cannot say "I want my webapp nodes to have full copies of only indexes X,Y,Z and nothing else, ever, not even on accident".  If you try this, there is always some disappointment, such as:
- you cannot do it with rack awareness on its own, that only means shards and replicas won't appear on same rack.
- you cannot do it with forced rack because that doesn't play well with auto expand replicas and each rack needs to be present so you end up with error conditions that are false and you can't safely set the replica count on the indexes without endangering the state of the cluster (yellow/red)... and some times auto expand and forced rack just don't seem to work at all together.
- you cannot do it exclude/include/require settings because the minute you do this, you break auto expand replicas which doesn't consider the correct count of nodes after considering these settings so you end up in bad error states.
- exclude/include/require is hard to use if you have old systems creating new daily indexes that do not set the exclude flag for your nodes to which you do not want the index to propagate, so is hard to fend of accidental indexes jumping onto nodes that may not be able to handle them.

So what I would propose is these settings:
- a per node setting that says "only accept indexes/replicas I specifically approve to be on my node"
- a per index setting "this index is approved for nodes with attribute X" (using attributes as allowed by shard allocation awareness) 
- the previous setting (or additional) would indicate if it is a full or balanced copy.  A full copy meaning the node receives a copy of all shards.  A balanced copy indicates that all nodes matching the approval receive some portion of one copy of the shards.

These do not require replica count to be increased or auto-expand, it basically says "I need more replicas to cover these conditions" and if the nodes all die, or one vanishes, it is not an error condition just as auto-expand wouldn't have caused it to be an error condition.

Therefore no indexes created would ever go to these nodes on accident, only approved indexes.
</description><key id="158361109">18723</key><summary>Further shard / index placement control by node/racks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>:Allocation</label></labels><created>2016-06-03T12:56:26Z</created><updated>2017-03-31T14:48:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T15:04:06Z" id="223604093">surely all you need is eg an attribute `pool` with values `general` and `dedicated`.

You set  `index.routing.allocation.require.pool` to `general` for all indices except for the ones you want on these special nodes, which have it set to `dedicated` instead.

&gt; you cannot do it exclude/include/require settings because the minute you do this, you break auto expand replicas which doesn't consider the correct count of nodes after considering these settings so you end up in bad error states.

You break auto-expand-replicas `0-all`, sure, but that's a bug which we will fix (https://github.com/elastic/elasticsearch/issues/2869).  In the meantime you can just set an upper limit of the number of nodes you have in your `dedicated` pool, eg `0-5`.

&gt; exclude/include/require is hard to use if you have old systems creating new daily indexes that do not set the exclude flag for your nodes to which you do not want the index to propagate, so is hard to fend of accidental indexes jumping onto nodes that may not be able to handle them.

Just have an index template like this:

```
PUT _template/general_pool
{
  "template": "*",
  "settings": {
    "index.routing.allocation.require.pool": "general"
  }
}
```

Other than the auto-expand bug which needs fixing, I'm unclear as to what problems the above leaves unsolved?
</comment><comment author="apatrida" created="2016-06-03T16:59:48Z" id="223634424">The template solves the new index creation danger, and your upper limit on auto expand doesn't really completely help.  Because you want safety in the core cluster that there are enough index replicas to be happy, and every node has one (maybe), and then also an unknown number of dedicate machines popup and vanish and want a copy of the full index while there.   Your upper bound would have to be coordinated with the dedicated machines add/remove. 

So the fix for #2869 (I read that before, seemed in doubt, and no scheduled) would help this, until then you still have risk if you set the upper bound too high and lose a machine that really is "optional"

I've also seen that auto-expand is not reliable, sometimes it just never kicks in when the setting is changed.  Maybe that is another bug somewhere else, was was some other interplay I wasn't aware of when seeing it fail.  

So trying to more or less FORCE a copy of each shard of an index into a node AND BE SURE you never have a primary there, unless there is a way I don't see to limit the shard allocation of primaries by attribute already.

see #18729 for a bigger idea covering more use cases.
</comment><comment author="apatrida" created="2016-09-21T15:24:20Z" id="248646730">@clintongormley #2869 still is marked "discuss" so this whole issue remains a problem.
</comment><comment author="clintongormley" created="2016-09-23T16:21:26Z" id="249237340">&gt; #2869 still is marked "discuss" so this whole issue remains a problem.

oh I absolutely agree.  we need to improve things here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw exception if using a closed transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18722</link><project id="" key="" /><description>Today when attempting to use a closed transport client, an exception
saying that no nodes are available is thrown. This is because when a
transport client is closed, its internal list of nodes is cleared. But
this exception is puzzling and can be made clearer. This commit changes
the behavior so that attempting to execute a request using a closed
transport client throws an illegal state exception.

Closes #18708
</description><key id="158360344">18722</key><summary>Throw exception if using a closed transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T12:52:46Z</created><updated>2016-06-06T15:27:27Z</updated><resolved>2016-06-06T15:27:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-06T08:37:37Z" id="223899416">@jasontedor can you add a comment to the code why we first read `this.nodes` before the closed check? Just to make it explicit the order matters there. LGTM.
</comment><comment author="jasontedor" created="2016-06-06T15:26:44Z" id="223993611">@ywelsch I pushed bc0b8f32e9197603bb25e83594ab19e3f49fa7b9. Thanks for your review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a parameter to cap the number of searches the msearch api will concurrently execute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18721</link><project id="" key="" /><description>Before the msearch api would concurrently execute all searches concurrently. If many large
msearch requests would be executed this could lead to some searches being rejected
while other searches in the msearch request would succeed.

The goal of this `max_parallel_searches` parameter is to avoid this exhausting of the search TP.
</description><key id="158360113">18721</key><summary>Add a parameter to cap the number of searches the msearch api will concurrently execute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T12:51:41Z</created><updated>2016-10-25T08:19:45Z</updated><resolved>2016-06-13T08:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-06-08T10:45:58Z" id="224553890">@clintongormley @jasontedor I've updated this PR and changed how the msearch api executes concurrently and how the default number of search requests to execute concurrently is determined
</comment><comment author="martijnvg" created="2016-06-09T06:29:19Z" id="224810947">Thanks @jasontedor, I've updated the PR with this commit: e06b3107aaa8425450a2e4861720a72b915b1a4d
</comment><comment author="ebuildy" created="2016-06-09T20:47:18Z" id="225021347">What about a great multi search "pipeline" with parrallel and sequential execution? (I am always executing "fallback queries" if main query didn't return any result and search a clean way to achieve this..)
</comment><comment author="martijnvg" created="2016-06-10T06:40:11Z" id="225105260">@ebuildy That isn't what the msearch api is designed for. I guess the best place for the fallback logic is on the application side as this is very use case specific. (maybe if not enough hits are returned, or specific results don't show up another search request should be executed)
</comment><comment author="jasontedor" created="2016-06-10T19:42:35Z" id="225277223">I love it @martijnvg, I left a few minor comments and a small suggestion. No need for another round, fire at will.
</comment><comment author="martijnvg" created="2016-06-13T08:14:20Z" id="225515385">@jasontedor Thx for reviewing!
</comment><comment author="messense" created="2016-10-24T04:06:40Z" id="255644007">I have a dumb question. Is multi search executing concurrently on Elasticsearch 1.x?
</comment><comment author="martijnvg" created="2016-10-25T08:19:45Z" id="255969002">@messense Yes, however the number of searches executing concurrently wasn't capped, which could cause searches inside msearch request to fail (if the msearch contained many search requests), because some nodes couldn't handle the amount of searches being executed concurrently.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix merge stats rendering in RestIndicesAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18720</link><project id="" key="" /><description>the title says it all
</description><key id="158346210">18720</key><summary>Fix merge stats rendering in RestIndicesAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:CAT API</label><label>bug</label><label>review</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-03T11:23:33Z</created><updated>2016-06-03T11:42:09Z</updated><resolved>2016-06-03T11:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-03T11:25:42Z" id="223555144">Yup. Looking at:

```
        table.addCell("merges.total", "sibling:pri;alias:mt,mergesTotal;default:false;text-align:right;desc:number of completed merge ops");
        table.addCell("pri.merges.total", "default:false;text-align:right;desc:number of completed merge ops");

        table.addCell("merges.total_docs", "sibling:pri;alias:mtd,mergesTotalDocs;default:false;text-align:right;desc:docs merged");
        table.addCell("pri.merges.total_docs", "default:false;text-align:right;desc:docs merged");

        table.addCell("merges.total_size", "sibling:pri;alias:mts,mergesTotalSize;default:false;text-align:right;desc:size merged");
        table.addCell("pri.merges.total_size", "default:false;text-align:right;desc:size merged");
```

this is how it should be. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term filters with an array of terms only match on the last term in the array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18719</link><project id="" key="" /><description>**Elasticsearch version**: Reproduced in 1.5.2 and 1.7.2 on 3 June 2016. First noticed around 31 May.

**JVM version**: 1.8.0_40 w/ ES 1.5.2 and 1.7.0_71 w/ ES 1.7.2

**OS version**: OSX 10.11.3 w/ ES 1.5.2 and OSX 10.9.5 w/ ES 1.7.2

**Description of the problem including expected versus actual behavior**:

We have an Elasticsearch index containing documents with ids like `divvybot:1:13`. Previously term filters behaved as expected: The requests described matched and returned one document for every id, since all of the ids in these examples exist in the index I'm querying.

```
curl -XPOST localhost:9200/divvy.bots/divvy.bot/_search -d '
{
    "size": 10,
    "filter": {
        "term": {"_id": [
            "divvybot:1:10", "divvybot:1:11",
            "divvybot:1:12", "divvybot:1:13"
        ]}
    }
}
'
```

Currently the query gets one hit, the document for `divvybot:1:13`.

```
curl -XPOST localhost:9200/divvy.bots/divvy.bot/_search -d '
{
    "size": 10,
    "filter": {
        "term": {"_id": [
            "divvybot:1:10", "divvybot:1:11",
            "divvybot:1:12"
        ]}
    }
}
'
```

This query also gets one hit, in this case the document for `divvybot:1:12`.

In every case I've tried, including for other indexes and other fields, when I attempt to match using a list of terms only the last term in the list is actually matched. I have tried disabling caching for the term filter and enclosing the term within `"bool": {"must": [{ &lt;term&gt; }]}`, as well as within `"query": {"constant_score": { &lt;filter&gt; }}` and found no difference in behavior in any case.

**Steps to reproduce**:
Unknown. Especially confounding is how this issue has occurred simultaneously on two different systems with differing setups, both mine and another developer's.

**Addendum**:

I spent a moment feeling like an idiot for using `term` instead of `terms`, but there is no question that using `term` for an array of terms to match absolutely worked before, and has stopped working without bothering to explain itself.
</description><key id="158336856">18719</key><summary>Term filters with an array of terms only match on the last term in the array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pineapplemachine</reporter><labels /><created>2016-06-03T10:22:39Z</created><updated>2016-06-03T11:22:01Z</updated><resolved>2016-06-03T10:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T10:37:32Z" id="223547152">&gt;  but there is no question that using term for an array of terms to match absolutely worked before, and has stopped working without bothering to explain itself.

Sorry @pineapplemachine but the `term` filter never worked like this.  In 5.0 we have done a major refactoring of query parsing that will now complain appropriately.
</comment><comment author="pineapplemachine" created="2016-06-03T10:47:13Z" id="223548736">This is a diff from when we added a query using an array of terms given to `term`:

```
    def get_bulk(self, bots):
        """Get information for many bots at once."""
        bot_ids = [bot.resource_id.to_string() for bot in bots]
        BotStorage.logger.error(bot_ids)
        return elasticsearch.helpers.scan(
            self.connection,
            index=BotStorage.index_name, doc_type=BotStorage.doc_type,
            query={'filter': {'term': {'_id': bot_ids}}}
        )
```

I have output from when this was written on 25 May that was as intended - that is, one document was received and processed per id passed to the query. I am and have been using version 1.6.0 of Elasticsearch's python API.

It _did_ work, though that it ever did may be the real bug here.
</comment><comment author="clintongormley" created="2016-06-03T10:56:27Z" id="223550275">&gt; It did work, though that it ever did may be the real bug here.

This test:

```
PUT test/doc/1?refresh
{
  "tag": "foo"
}

PUT test/doc/2?refresh
{
  "tag": "bar"
}

GET test/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "term": {
          "tag": [
            "foo",
            "bar"
          ]
        }
      }
    }
  }
}
```

returns a single document:

```
{
  "took": 93,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "test",
        "_type": "doc",
        "_id": "2",
        "_score": 1,
        "_source": {
          "tag": "bar"
        }
      }
    ]
  }
}
```

Tested on 0.20.5, 0.90.0, 1.0.0, 1.1.0, 1.2.1, 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0.

In 2.0.0, it throws the exception:

```
"[term] query does not support array of values"
```
</comment><comment author="pineapplemachine" created="2016-06-03T11:04:19Z" id="223551582">I checked the commit history and the output we were getting when we added the feature the diff was from. `term` acted as `terms` in 1.5.2 on one system and in 1.7.2 on another starting no later than May 23 - and I recall similarly misusing it to test other features a week or so previous - and abruptly ending around May 31.
</comment><comment author="clintongormley" created="2016-06-03T11:08:47Z" id="223552325">@pineapplemachine I've given you a test showing that this isn't true.  If I've made a mistake somewhere, then please provide a test which proves the opposite.
</comment><comment author="pineapplemachine" created="2016-06-03T11:22:01Z" id="223554547">I tried setting my system clock to May 23, and the test you provided still passed, and the filters that were mistakenly functional before still returned only one document.

Other than traveling back in time, it seems I cannot produce a test because the incorrect behavior was transient. I can only point to our commit history and the results we were getting from our term filter usage until we noticed it was no longer functional around 31 May.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>5.0 java api change - ImmutableOpenMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18718</link><project id="" key="" /><description>a minor breaking change in the 5.0 java api:
ImmutableOpenMap.keysIt() returns an Iterator iso a UnmodifiableIterator

this is not yet documented
</description><key id="158328314">18718</key><summary>5.0 java api change - ImmutableOpenMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brackxm</reporter><labels><label>:Java High Level REST Client</label><label>enhancement</label></labels><created>2016-06-03T09:36:09Z</created><updated>2017-07-03T09:15:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T11:45:25Z" id="223558314">thanks @brackxm - we tend not to crowd the breaking changes docs with small changes to the java API which will be highlighted by your IDE, but thanks for trying out 5.0!
</comment><comment author="jpountz" created="2016-06-06T12:44:30Z" id="223947973">This reminds me of the fact that we should probably avoid leaking hppc collections into the client API and try to only use java.util collections instead.
</comment><comment author="nik9000" created="2016-06-06T13:28:27Z" id="223958490">Probably, but I think we're probably better off spending effort on http client things. I'd _love_ for us to say "Client is an internal interface" one day. Might be years away, but yeah.
</comment><comment author="nik9000" created="2017-03-21T15:05:52Z" id="288107867">&gt; Probably, but I think we're probably better off spending effort on http client things. I'd love for us to say "Client is an internal interface" one day. Might be years away, but yeah.

Update on this statement: we're reusing the "response" classes as part of the high level rest client so we still should do this.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put field stats behind the result cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18717</link><project id="" key="" /><description>The field stats API is fast, but still needs to look at every segment every time it is called.  Also, the same fields stats call is often made multiple times concurrently.

Putting it behind the result cache should make it much faster and reduce the amount of work needed to answer concurrent requests.
</description><key id="158320248">18717</key><summary>Put field stats behind the result cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2016-06-03T08:51:20Z</created><updated>2016-06-14T17:58:09Z</updated><resolved>2016-06-14T17:58:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>searching on fields one by one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18716</link><project id="" key="" /><description>Hello ,
I am using elastic search in my project .I want to search on fields one by one. means if i want to search fortis hospital  then search in first field  than search in second field and so on.

then i need output like :  show all record according first field then show all record according second field and so on.

Please Help Me

Thank You.
Tarun Tyagi
</description><key id="158315923">18716</key><summary>searching on fields one by one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tarun-oodles</reporter><labels /><created>2016-06-03T08:25:52Z</created><updated>2016-06-03T11:11:17Z</updated><resolved>2016-06-03T10:11:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-03T10:11:33Z" id="223542379">Please use discuss for questions.

I saw you did. It's the best place for that.
</comment><comment author="tarun-oodles" created="2016-06-03T11:11:17Z" id="223552741">Ok sorry for this i will remember next time

On Fri, Jun 3, 2016 at 3:42 PM, David Pilato notifications@github.com
wrote:

&gt; Closed #18716 https://github.com/elastic/elasticsearch/issues/18716.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18716#event-680948905,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AN3anAMqWjWQI8riKhqndxguABjzA7dLks5qH_4kgaJpZM4ItTx-
&gt; .

## 

[image: Oodles Technologies Pvt Ltd] http://www.oodlestechnologies.com/

Tarun Tyagi Java Developer, Oodles Technologies Pvt Ltd
Phone:   +91 124 4368 395
Mobile:   +91 9873552515 http:///
Email:     tarun.tyagi@oodlestechnologies.com
Website:  http://www.oodlestechnologies.com/
Skype:     tarun-oodles
Address: Unit No 951-955 , Tower B1, Spaze I-Tech Park , Sector 49 , Sohna
Road , Gurgaon , India , 122001
[image: Twitter] https://twitter.com/oodlestech [image: Facebook]
https://www.facebook.com/OodlesTech [image: Google +]
https://plus.google.com/+Oodlestechnologies/posts [image: LinkedIn]
https://www.linkedin.com/company/oodles-technologies-pvt-ltd

This e-mail message may contain confidential or legally privileged
information and is intended only for the use of the intended recipient(s).
Any unauthorized disclosure, dissemination, distribution, copying or the
taking of any action in reliance on the information herein is prohibited.
E-mails are not secure and cannot be guaranteed to be error free as they
can be intercepted, amended, or contain viruses. Anyone who communicates
with us by e-mail is deemed to have accepted these risks. Oodles
Technologies is not responsible for errors or omissions in this message and
denies any responsibility for any damage arising from the use of e-mail.
Any opinion and other statement contained in this message and any
attachment are solely those of the author and do not necessarily represent
those of the company
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Aggregation does not match underlying documents...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18715</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: (not sure, using found)

**OS version**: (using found)

**Description of the problem including expected versus actual behavior**:
I'm trying out a new metrics system, and I need to do math on the numerical values. The values reported in the aggregation are wrong, even though the values in the actual documents are correct.

request:

``` json
GET metrics-2016.06.02/_search
{
  "size": 0,
  "aggs": {
    "values" : {
      "terms": {
        "field": "value"
      }
    }
  }
}
```

condensed response:

``` json
{
  "hits": {
    "hits": [
      {
        "_id": "AVUSjXsGy-UE6d0LYWba",
        "_source": {
          "value": 1464895109
        }
      },
      {
        "_id": "AVUSjaJqy-UE6d0LYXgO",
        "_source": {
          "value": 1464895119
        }
      }
    ]
  },
  "aggregations": {
    "values": {
      "buckets": [
        {
          "key": 1464895104,
          "doc_count": 2
        }
      ]
    }
  }
}
```

Note: each document has a unique value in the field "value", yet the aggregation groups them together into a single bucket. This is wrong and seriously cramping my style. If I wanted to bucket them together in ranges I would have done that instead.

I tried using "execution_hint" as documented [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-execution-hint), but it had no effect.

**Steps to reproduce**:
see above.

Note:
I had to obfuscate the documents/results slightly, so I apologize if I got a syntax error somewhere.

**Provide logs (if relevant)**:
additional information:

full response:

``` json
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 1,
    "hits": [
{
        "_index": "metrics-2016.06.02",
        "_type": "metric",
        "_id": "AVUSjXsGy-UE6d0LYWba",
        "_score": 1,
        "_source": {
          "value": 1464895109,
          "mtype": "timestamp",
          "unit": "seconds",
          "@timestamp": "2016-06-02T19:18:29.237Z",
          "type": "metric"
        }
      },
      {
        "_index": "metrics-2016.06.02",
        "_type": "metric",
        "_id": "AVUSjaJqy-UE6d0LYXgO",
        "_score": 1,
        "_source": {
          "value": 1464895119,
          "mtype": "timestamp",
          "unit": "seconds",
          "@timestamp": "2016-06-02T19:18:39.328Z",
          "type": "metric"
        }
      }
    ]
  },
  "aggregations": {
    "values": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": 1464895104,
          "doc_count": 2
        }
      ]
    }
  }
}
```

My index template:

``` json
{
  "template": "metrics-*",
  "settings": {
    "index": {
      "refresh_interval": "5s"
    }
  },
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "strings": {
            "match": "*",
            "match_mapping_type": "string",
            "mapping": { "type": "string",  "doc_values": true, "index": "not_analyzed" }
          }
        }
      ],
      "_all":            { "enabled": true },
      "_source":         { "enabled": true },
      "properties": {
        "@timestamp":    { "type": "date",    "doc_values": true },
        "value":         { "type": "float",   "doc_values": true, "coerce": true }
      }
    }
  }
}
```
</description><key id="158273948">18715</key><summary>Terms Aggregation does not match underlying documents...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astropuffin</reporter><labels /><created>2016-06-03T01:30:36Z</created><updated>2016-06-03T11:31:26Z</updated><resolved>2016-06-03T11:31:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T11:31:26Z" id="223556075">@astropuffin the `value` field is a `float`, and your values are too high to be represented accurately as a float.  Change the field to be a `double` or and `integer` or `long`,  whichever is appropriate.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18714</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="158273629">18714</key><summary>2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">werwrdf</reporter><labels /><created>2016-06-03T01:26:46Z</created><updated>2016-06-03T01:32:06Z</updated><resolved>2016-06-03T01:32:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-03T01:32:06Z" id="223468917">This appears to have been opened in error. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add "</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18713</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="158273435">18713</key><summary>add "</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lilien1010</reporter><labels /><created>2016-06-03T01:24:41Z</created><updated>2016-06-03T01:33:16Z</updated><resolved>2016-06-03T01:33:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-03T01:33:16Z" id="223469163">Thanks for the contribution, but this is correct as-is, and would not be correct with the change. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minimum_should_match ignored on more_like_this query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18712</link><project id="" key="" /><description>Seems that while `"minimum_should_match": "99%"`, we shouldn't hit here, but we do:

```
PUT /test/docs/1 
{ 
"message": "Ge's. Japan's. Business seize. Power. What. Said. Session. An unlikely. Greatest hits of the sixties seventies and eighties this whose fourteen sixty C. jail why gw elf A Chorus radio station. Sterling, and brown. Data because. Yeah. Well. Or. She doesn't get. Last, year, it's. Actually works. Going to die for. With. That. Is. Fields. The touch of. Class. One. Gone. This. Overall. This is also. Mystery new release. Fantasy. Six. Once. So how will the. Cold. This feel. Now. Think that a. Person. Once. Was." 
}
&#8203;
PUT /test/docs/2 
{ 
"message": "One person was killed and two others sustained injuries in a collision between a tractor and a car on National Highway near foo area of bar district on Tuesday. According to the force, the spiritual leader namely doogie along with two colleagues was on its way in a car when a rashly driven tractor hit the vehicle. As a result, they died on the spot while two other sustained injuries. The injured were rushed to nearby hospital. doogie's force has registered a case." 
}
&#8203;

POST /test/_search
{
   "query": {
      "bool": {
         "must": [
            {
               "more_like_this": {
                  "fields": [
                     "message"
                  ],
                  "docs": [
                     {
                        "_index": "test",
                        "_type": "docs",
                        "_id": "1"
                     }
                  ],
                  "minimum_should_match": "99%",
                  "min_term_freq": 1,
                  "min_doc_freq": 1
               }
            }
         ]
      }
   }
}
```
</description><key id="158270372">18712</key><summary>minimum_should_match ignored on more_like_this query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:More Like This</label><label>feedback_needed</label></labels><created>2016-06-03T00:51:45Z</created><updated>2016-06-03T12:22:18Z</updated><resolved>2016-06-03T12:13:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T11:13:17Z" id="223553025">@PhaedrusTheGreek please provide the ES version you're on and some description of the actual problem, without making me try to guess.
</comment><comment author="Bonfondp" created="2016-06-03T11:39:47Z" id="223557401">Given the document 1 &amp; 2 inserted in an index. When running the "morelikethis" query, we retrieve document 2 as a candidate but both document are very different and we did specified a 99% for minimum_should match.

Hope this clarify the problem.
</comment><comment author="Bonfondp" created="2016-06-03T11:41:03Z" id="223557627">And @clintongormley we are using version 1.7.3 
</comment><comment author="clintongormley" created="2016-06-03T12:13:59Z" id="223563202">This is just a misunderstanding of how MLT works.  First, it extracts the K terms with the highest inverse doc frequency (ie the best relevance), then it applies min-should-match to those K terms.

You have two documents, one on each shard.  The query is run independently on each shard.  So it looks at the shard that holds ONLY document 2 and finds the terms on that shard with the highest IDF.  By definition, ALL of these terms are in document 2, so all of them match.

MLT works with term statistics, so having so few docs makes for meaningless tests.  Btw, if you index both docs in an index with a single shard, then this particular test passes.
</comment><comment author="Bonfondp" created="2016-06-03T12:22:18Z" id="223564694">Thank you for clarifying and for providing an alternative for our tests by using only 1 shard.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve painless compile-time exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18711</link><project id="" key="" /><description>Followup of https://github.com/elastic/elasticsearch/pull/18600
#18600 only improved painless exceptions for the runtime case.  This gives the same format for errors that happen at compile time (lexing, parsing, analysis).

The goals are the same: don't make exception handling complicated to code and tests, don't wrap exceptions with BS ones, use the correct exceptions.

In most cases changes just look like this:

```
-                throw new IllegalArgumentException(error("Extraneous for loop."));
+                throw createError(new IllegalArgumentException("Extraneous for loop."));
```

The original exception is returned, but with an artificial stack frame added, which looks just like the runtime case. This allows for more consistency (maybe more code sharing in the future too). 
</description><key id="158254150">18711</key><summary>Improve painless compile-time exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>das awesome</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-02T22:29:38Z</created><updated>2016-06-03T11:11:59Z</updated><resolved>2016-06-03T00:43:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-02T22:41:43Z" id="223444222">+1.  Great change!  Thanks for doing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using fuzziness parameter in multi_match query interferes with per field boosting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18710</link><project id="" key="" /><description>To reproduce, we start with an index with default settings containing two records in different shards and a query that boosts one of the fields:

```
DELETE test

PUT test/doc/1
{
  "title": "bar",
  "body": "foo"
}

PUT test/doc/2
{
  "title": "foo",
  "body": "bar"
}

GET test/doc/_search
{
  "query": {
    "multi_match": {
      "query": "foo",
      "fields": ["title^100", "body"]
    }
  }
}
```

We get back a reasonable result: 

```
{
  "took" : 98,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "tests",
      "_type" : "test",
      "_id" : "2",
      "_score" : 1.0,
      "_source" : {
        "title" : "foo",
        "body" : "bar"
      }
    }, {
      "_index" : "tests",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.0059061614,
      "_source" : {
        "title" : "bar",
        "body" : "foo"
      }
    } ]
  }
}
```

Now, add the `fuzziness` parameter to the query (the value doesn't matter as long as it doesn't interfere with matches):

```
GET test/doc/_search
{
  "query": {
    "multi_match": {
      "query": "foo",
      "fields": ["title^100", "body"],
      "fuzziness": 0
    }
  }
}
```

the result is both records are scored the same:

```
{
  "took" : 17,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "tests",
      "_type" : "test",
      "_id" : "2",
      "_score" : 1.0,
      "_source" : {
        "title" : "foo",
        "body" : "bar"
      }
    }, {
      "_index" : "tests",
      "_type" : "test",
      "_id" : "1",
      "_score" : 1.0,
      "_source" : {
        "title" : "bar",
        "body" : "foo"
      }
    } ]
  }
}
```

However, the same experiment repeated on a single shard index produces reasonable results. It looks like if the fuzziness parameter is present, queryNorm is correctly applied only on shards where the boosted field matched at least one record.

I was able to reproduce the issue on both 2.3.3 and the latest master.

The issue was originally reported on the [discussion forum](https://discuss.elastic.co/t/ne-rabotaet-boost-v-zaprosah/51648).
</description><key id="158249576">18710</key><summary>Using fuzziness parameter in multi_match query interferes with per field boosting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-06-02T22:00:23Z</created><updated>2016-10-12T14:57:11Z</updated><resolved>2016-10-12T14:57:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T11:10:33Z" id="223552637">I thought that this might be a query norm thing which might change with BM25, but I'm seeing the same on master.  @cbuescher is this something we're doing when rewriting queries, or is this lucene level?
</comment><comment author="imotov" created="2016-06-03T13:33:14Z" id="223579706">@clintongormley yes, in my experiments BM25 was affecting only the numeric value of the score but not dropping queryNorms. The strange thing is that the problem only affects shards where the boosted field doesn't match anything, which makes it look like some sort of shortcut in execution. My first thought was, maybe it's somehow related to two-phase execution. However, I was able to reproduce it on v0.90.7, v1.7.5, v2.3.3 and the current master. So it must be something else.
</comment><comment author="cbuescher" created="2016-06-03T13:37:18Z" id="223580668">From looking at how we create the lucene queries, I can't see anything obvious where we might ignore the boost when fuziness is set, but I will have a closer look at it. Most of the query creation happens in MatchQuery/MultiMatchQuery but I'm not too familiar with that code yet, but I will take a look at this starting from your examples.
</comment><comment author="cbuescher" created="2016-06-03T14:04:23Z" id="223587439">@imotov yes, I also suspect something on the execution level of the fuzzy query. If I use one or two shards for the minimal example above, the scores for both docs are different. As soon as a third shard gets added, the scores for the query with the added "fuzziness" are the same.
</comment><comment author="imotov" created="2016-06-03T14:37:16Z" id="223596423">@cbuescher yes, the 2 shard case is the same as 1 shard case, because when you have 2 shards, both records will be on the shard 1, so you are basically searching a single shard. 
</comment><comment author="cbuescher" created="2016-06-13T09:11:06Z" id="225527678">I wrote a test to see how the lucene query using the `fuzziness = 0` parameter gets rewritten. If the two docs reside on two different shards, the original fuzzy query `(body:foo~0 | (title:foo~0)^100.0)` gets rewritten to `(() | (title:foo)^100.0)` on one shard and to `(body:foo | ()^100.0)` on the other where no doc has the `title`field. In that case both doc scores are the same. 
In the case the docs are on the same shard, the query gets rewritten to (body:foo | (title:foo)^100.0) and doc2 scores higher, as expected. At this point I would like to know how the expectations for rewrites and scoring for these two cases are in theory on the lucene level. Maybe @jpountz or @jimferenczi have an idea where to look next?
</comment><comment author="jimczi" created="2016-06-13T16:01:19Z" id="225626485">This 2 queries: `(() | (title:foo)^100.0)` and `(body:foo | ()^100.0)` are supposed to be normalized the same way but it doesn't work as expected because the clause `()^100` is ignored. It's the problem with queries that are expanded on the shard directly. The normalization factor for the first query is 10,000 (100*100) and the norm is equal to 0.01. This is why we have a score of 1 for hits that match this query (100 *0.01). 
The normalization factor and the norm for the second query should be the same and the score should be multiplied by the norm. Though the normalization factor and the norm are set to 1 because the empty boolean query `()^100` is ignored. 
I opened a ticket on Lucene land:
https://issues.apache.org/jira/browse/LUCENE-7337
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor matrix agg documentation from modules to main agg section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18709</link><project id="" key="" /><description>Since Matrix aggregations are a module, the documentation was initially placed under the `Modules` documentation section. This PR refactors the documentation to the main `Aggregations` section, as requested.
</description><key id="158245808">18709</key><summary>Refactor matrix agg documentation from modules to main agg section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nknize</reporter><labels><label>docs</label></labels><created>2016-06-02T21:38:27Z</created><updated>2016-06-06T12:40:16Z</updated><resolved>2016-06-06T12:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-03T10:17:24Z" id="223543536">One comment, otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Closed TransportClient should throw meaningful exceptions when subsequently used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18708</link><project id="" key="" /><description>**Elasticsearch version**:
2.3.2

**Description of the problem including expected versus actual behavior**:
When `TransportClient#close()` is called, all connections are dropped and that client is no longer usable. However, if for some reason (due to a simple matter of programming error in our case &#128521;) the client is subsequently used, a not very helpful error is provided:

```
org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: [{#transport#-1}{x.x.x.x}
```

This error makes it sound like there's connectivity issues or something to the Elasticsearch cluster, which is not correct. Instead a more useful exception like `org.elasticsearch.client.transport.ClientStoppedException` should be created and thrown in this scenario.

**Steps to reproduce**:
1. Create a standard `TransportClient` and connect it to an existing Elasticsearch cluster.
2. Call `stop()` on the client.
3. Call `admin().cluster().prepareHealth().get()` or something similar on the client and observe the thrown error.
</description><key id="158238489">18708</key><summary>Closed TransportClient should throw meaningful exceptions when subsequently used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">samcday</reporter><labels><label>:Java API</label><label>low hanging fruit</label></labels><created>2016-06-02T20:59:09Z</created><updated>2016-06-06T15:27:26Z</updated><resolved>2016-06-06T15:27:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Restore of 2.x snapshot throws checksum missing exceptions on 5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18707</link><project id="" key="" /><description>Lets say we have a repository with a snapshot `A` created in v2.3.3.  Now, if we start ES 5.0 (master branch) and try to restore snapshot `A`, we get these exceptions:

```
Recovery failed from null into {Gomi}{S29Q6GFKQDC7m8DlfwAiwQ}{127.0.0.1}{127.0.0.1:9300}]; nested: IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null]; ]
RecoveryFailedException[[i1][2]: Recovery failed from null into {Gomi}{S29Q6GFKQDC7m8DlfwAiwQ}{127.0.0.1}{127.0.0.1:9300}]; nested: IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null];
    at org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$133(IndexShard.java:1450)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [i1/o_88kBP1Q8OTmY0VJ-5quA][[i1][2]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null];
    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:311)
    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:244)
    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1149)
    at org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$133(IndexShard.java:1446)
    ... 4 more
Caused by: [i1/o_88kBP1Q8OTmY0VJ-5quA][[i1][2]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null];
    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:413)
    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$387(StoreRecovery.java:246)
    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:269)
    ... 7 more
Caused by: [i1/o_88kBP1Q8OTmY0VJ-5quA][[i1][2]] IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null];
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:408)
    ... 9 more
Caused by: java.lang.NullPointerException: checksum must not be null
    at java.util.Objects.requireNonNull(Objects.java:228)
    at org.elasticsearch.index.store.StoreFileMetaData.&lt;init&gt;(StoreFileMetaData.java:64)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot$FileInfo.fromXContent(BlobStoreIndexShardSnapshot.java:316)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.fromXContent(BlobStoreIndexShardSnapshot.java:515)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.fromXContent(BlobStoreIndexShardSnapshot.java:45)
    at org.elasticsearch.repositories.blobstore.BlobStoreFormat.read(BlobStoreFormat.java:113)
    at org.elasticsearch.repositories.blobstore.ChecksumBlobStoreFormat.readBlob(ChecksumBlobStoreFormat.java:111)
    at org.elasticsearch.repositories.blobstore.BlobStoreFormat.read(BlobStoreFormat.java:89)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$Context.loadSnapshot(BlobStoreIndexShardRepository.java:342)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:802)
```

These exceptions are related to the `StoreFileMetaData` class throwing an exception if the checksum value is null.  This is related to the change found here: https://github.com/elastic/elasticsearch/commit/5008694ba1a140c430a92c05ff84885de6a7d28a

The problem is, for snapshots created in 2.x, the segments_N files do not have checksums when stored in the repository, so when we try to restore a snapshot from 2.x into ES 5.0, we get this exception thrown.  

Interestingly, it does not prevent the index itself from being restored, as I am able to get and search against the index that was restored from the snapshot and retrieve documents.

Steps to reproduce:
1. Install ES 2.3.3
2. In the elasticsearch.yml file, add the line: `path.repo: ["/path/to/repository/dir"]`
3. Start ES 2.3.3
4. Create a repository at the above location: `
curl -XPUT localhost:9200/_snapshot/my_repo -d '{
"type": "fs",
"settings": {
"location": "/path/to/repository/dir",
"compress": false
}
}'`
5. Create an index and index documents:`
curl -XPOST localhost:9200/idx1/type1 -d '{ "name": "ali", "sane": "absolutely not" }'
curl -XPOST localhost:9200/idx1/type1 -d '{ "name": "igor", "sane": "partially" }'`
6. Create a snapshot of the index:`
curl -XPUT "localhost:9200/_snapshot/my_repo/snap1?wait_for_completion=true" -d '{ "indices": ["idx1"] }'`
7. Stop ES 2.3.3
8. Install ES 5.0 from master branch
9. In the elasticsearch.yml file, add the line: `path.repo: ["/path/to/repository/dir"]`
10. Start ES 5.0
11. Repeat step 4
12. Try to restore the snapshot created earlier:`curl -XPOST "localhost:9200/_snapshot/my_repo/snap1/_restore"`
</description><key id="158232141">18707</key><summary>Restore of 2.x snapshot throws checksum missing exceptions on 5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-02T20:28:22Z</created><updated>2016-06-10T13:58:20Z</updated><resolved>2016-06-10T13:58:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-02T20:32:19Z" id="223413123">@s1monw @imotov What do you think is the best approach for solving this?  None of the 2.x snapshots will have checksums for the segments_N files.
</comment><comment author="abeyad" created="2016-06-02T20:46:34Z" id="223417039">Also, this only seems to happen when the number of documents in the index are few.  I suspect when not all primary shards are populated with at least one document, though I need to dig further to confirm this.
</comment><comment author="abeyad" created="2016-06-02T20:59:08Z" id="223420515">And when this happens, we can not subsequently take a snapshot of the index in question again, getting "primary shard not allocated" errors.  The reason is evident when looking at the cluster state for the index:

```
"idx1" : {
        "shards" : {
          "2" : [
            {
              "state" : "UNASSIGNED",
              "primary" : true,
              "node" : null,
              "relocating_node" : null,
              "shard" : 2,
              "index" : "idx1",
              "restore_source" : {
                "repository" : "my_repo",
                "snapshot" : "snap1",
                "version" : "2.3.2",
                "index" : "idx1"
              },
              "unassigned_info" : {
                "reason" : "ALLOCATION_FAILED",
                "at" : "2016-06-02T20:45:10.822Z",
                "failed_attempts" : 5,
                "delayed" : false,
                "details" : "failed recovery, failure RecoveryFailedException[[idx1][2]: Recovery failed from null into {Aegis}{TQPQL-DTRaq_HhapThIQSg}{127.0.0.1}{127.0.0.1:9300}]; nested: IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null]; "
              }
            },
            {
              "state" : "UNASSIGNED",
              "primary" : false,
              "node" : null,
              "relocating_node" : null,
              "shard" : 2,
              "index" : "idx1",
              "unassigned_info" : {
                "reason" : "NEW_INDEX_RESTORED",
                "at" : "2016-06-02T20:45:10.629Z",
                "delayed" : false,
                "details" : "restore_source[my_repo/snap1]"
              }
            }
          ],
          "1" : [
            {
              "state" : "UNASSIGNED",
              "primary" : false,
              "node" : null,
              "relocating_node" : null,
              "shard" : 1,
              "index" : "idx1",
              "unassigned_info" : {
                "reason" : "NEW_INDEX_RESTORED",
                "at" : "2016-06-02T20:45:10.629Z",
                "delayed" : false,
                "details" : "restore_source[my_repo/snap1]"
              }
            },
            {
              "state" : "UNASSIGNED",
              "primary" : true,
              "node" : null,
              "relocating_node" : null,
              "shard" : 1,
              "index" : "idx1",
              "restore_source" : {
                "repository" : "my_repo",
                "snapshot" : "snap1",
                "version" : "2.3.2",
                "index" : "idx1"
              },
              "unassigned_info" : {
                "reason" : "ALLOCATION_FAILED",
                "at" : "2016-06-02T20:45:10.819Z",
                "failed_attempts" : 5,
                "delayed" : false,
                "details" : "failed recovery, failure RecoveryFailedException[[idx1][1]: Recovery failed from null into {Aegis}{TQPQL-DTRaq_HhapThIQSg}{127.0.0.1}{127.0.0.1:9300}]; nested: IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null]; "
              }
            }
          ],
          "4" : [
            {
              "state" : "STARTED",
              "primary" : true,
              "node" : "TQPQL-DTRaq_HhapThIQSg",
              "relocating_node" : null,
              "shard" : 4,
              "index" : "idx1",
              "restore_source" : {
                "repository" : "my_repo",
                "snapshot" : "snap1",
                "version" : "2.3.2",
                "index" : "idx1"
              },
              "allocation_id" : {
                "id" : "6Mrum9dpRPGkklfb9lixEA"
              }
            },
            {
              "state" : "UNASSIGNED",
              "primary" : false,
              "node" : null,
              "relocating_node" : null,
              "shard" : 4,
              "index" : "idx1",
              "unassigned_info" : {
                "reason" : "NEW_INDEX_RESTORED",
                "at" : "2016-06-02T20:45:10.629Z",
                "delayed" : false,
                "details" : "restore_source[my_repo/snap1]"
              }
            }
          ],
          "3" : [
            {
              "state" : "UNASSIGNED",
              "primary" : false,
              "node" : null,
              "relocating_node" : null,
              "shard" : 3,
              "index" : "idx1",
              "unassigned_info" : {
                "reason" : "NEW_INDEX_RESTORED",
                "at" : "2016-06-02T20:45:10.629Z",
                "delayed" : false,
                "details" : "restore_source[my_repo/snap1]"
              }
            },
            {
              "state" : "UNASSIGNED",
              "primary" : true,
              "node" : null,
              "relocating_node" : null,
              "shard" : 3,
              "index" : "idx1",
              "restore_source" : {
                "repository" : "my_repo",
                "snapshot" : "snap1",
                "version" : "2.3.2",
                "index" : "idx1"
              },
              "unassigned_info" : {
                "reason" : "ALLOCATION_FAILED",
                "at" : "2016-06-02T20:45:10.815Z",
                "failed_attempts" : 5,
                "delayed" : false,
                "details" : "failed recovery, failure RecoveryFailedException[[idx1][3]: Recovery failed from null into {Aegis}{TQPQL-DTRaq_HhapThIQSg}{127.0.0.1}{127.0.0.1:9300}]; nested: IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [snap1]]; nested: NullPointerException[checksum must not be null]; "
              }
            }
          ],
          "0" : [
            {
              "state" : "STARTED",
              "primary" : true,
              "node" : "TQPQL-DTRaq_HhapThIQSg",
              "relocating_node" : null,
              "shard" : 0,
              "index" : "idx1",
              "restore_source" : {
                "repository" : "my_repo",
                "snapshot" : "snap1",
                "version" : "2.3.2",
                "index" : "idx1"
              },
              "allocation_id" : {
                "id" : "Yli19wMdQnOu4itVUo9IPg"
              }
            },
            {
              "state" : "UNASSIGNED",
              "primary" : false,
              "node" : null,
              "relocating_node" : null,
              "shard" : 0,
              "index" : "idx1",
              "unassigned_info" : {
                "reason" : "NEW_INDEX_RESTORED",
                "at" : "2016-06-02T20:45:10.629Z",
                "delayed" : false,
                "details" : "restore_source[my_repo/snap1]"
              }
            }
          ]
        }
      }
```

While some primaries are activated, others remain unassigned due to the allocation failure resulting from the missing checksum throwing a NPE.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field stats API returns results for _all when specific indices are requested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18706</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**Description of the problem including expected versus actual behavior**:

This came up while debugging a [Kibana issue](https://github.com/elastic/kibana/issues/5739). On the Discover page in Kibana we do a field stats request to figure out which indices to query for a certain time range before pulling back actual results. A couple of users have reported that they're seeing documents from _all_ of their indices in Discover instead of just their index pattern. I asked them to provide an example of the field stats request/response, and this is what I got:

URL:
`https://xxx/elasticsearch/logstash-*/_field_stats?level=indices`

Request Payload:

```
{"fields":["@timestamp"],"index_constraints":{"@timestamp":{"max_value":{"gte":1464883025215,"format":"epoch_millis"},"min_value":{"lte":1464883925215,"format":"epoch_millis"}}}}
```

Response Payload:

```
{"_shards":{"total":80,"successful":80,"failed":0},"indices":{"_all":{"fields":{"@timestamp":{"max_doc":200008,"doc_count":200008,"density":100,"sum_doc_freq":800032,"sum_total_term_freq":-1,"min_value":1463589044264,"min_value_as_string":"2016-05-18T16:30:44.264Z","max_value":1464883879721,"max_value_as_string":"2016-06-02T16:11:19.721Z"}}}}}
```

So you can see, despite requesting `logstash-*` in the URL, the API is returning results for `_all`. I couldn't find anything in the docs or existing issues about this, so I thought it might be a bug.

@lifeofguenter If there are any additional details you can provide about your setup that might help reproduce the issue, please add them here.
</description><key id="158220485">18706</key><summary>Field stats API returns results for _all when specific indices are requested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bargs</reporter><labels /><created>2016-06-02T19:29:17Z</created><updated>2016-06-02T21:59:03Z</updated><resolved>2016-06-02T21:59:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lifeofguenter" created="2016-06-02T21:56:53Z" id="223435016">This was a problem with my nginx setup modifying the request - not a problem with kibana or es. Thanks again @Bargs for helping out a lot!
</comment><comment author="Bargs" created="2016-06-02T21:59:02Z" id="223435502">Thanks for the follow up @lifeofguenter !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reworked docs for index-shrink API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18705</link><project id="" key="" /><description>@s1monw please review
</description><key id="158213966">18705</key><summary>Reworked docs for index-shrink API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Index APIs</label><label>docs</label><label>review</label></labels><created>2016-06-02T18:56:05Z</created><updated>2016-06-03T07:50:51Z</updated><resolved>2016-06-03T07:50:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-03T07:42:36Z" id="223512568">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>node.client: false setting not allowed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18704</link><project id="" key="" /><description>**Elasticsearch version**:
5.0.0 Alpha 3
**JVM version**:
1.8.0-092
**OS version**:
Ubuntu 3.16.0-71-lowlatency
**Description of the problem including expected versus actual behavior**:
Node startup error with node type setting

**Steps to reproduce**:
 Elasticsearch.yml should have attribute set in elasticsearch.yml. 

**Provide logs (if relevant)**:

[2016-06-02 18:45:17,147][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: unknown setting [node.client]
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:267)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
        at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:125)
        at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
        at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
        at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:103)
        at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:148)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:249)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:162)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:180)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:180)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:255)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
        at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
        Suppressed: java.lang.IllegalArgumentException: unknown setting [path.work]
</description><key id="158212903">18704</key><summary>node.client: false setting not allowed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels /><created>2016-06-02T18:51:10Z</created><updated>2017-05-09T08:16:02Z</updated><resolved>2016-06-02T18:55:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-02T18:55:48Z" id="223387882">The `node.client` setting was removed in #16963 and is covered in the [breaking changes](https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_settings_changes.html#_node_types_settings) documentation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix repository S3 Settings and add more tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18703</link><project id="" key="" /><description>Follow up for #18662 and #18690.
- For consistency, we rename method parameters and use `key` and `secret` instead of `account` and `key`.
- We add some tests to check that settings are correctly applied.
- Tests revealed that some checks are bad like for #18662.
</description><key id="158186660">18703</key><summary>Fix repository S3 Settings and add more tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-06-02T16:40:55Z</created><updated>2016-07-22T22:48:06Z</updated><resolved>2016-07-22T22:47:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-02T16:43:19Z" id="223350200">@rjernst Could you also review this one?
</comment><comment author="dadoonet" created="2016-06-13T06:27:51Z" id="225498583">@rjernst Do you have time to review this?
</comment><comment author="rjernst" created="2016-06-14T18:42:39Z" id="225977260">The changes look ok, and thanks for the added tests. I just want to be sure these are reproducible.
</comment><comment author="dadoonet" created="2016-07-22T21:41:38Z" id="234664031">@rjernst I also updated this branch. Could you please tell me what you think?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not start scheduled pings until transport start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18702</link><project id="" key="" /><description>Today, scheduled pings in NettyTransport can start before the transport
is started. Instead, these pings should not be scheduled until after the
transport is started. This commit modifies NettyTransport so that this
is the case.
</description><key id="158169387">18702</key><summary>Do not start scheduled pings until transport start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-02T15:26:19Z</created><updated>2016-06-02T17:12:21Z</updated><resolved>2016-06-02T17:12:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-02T15:47:15Z" id="223333712">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix recovery throttling to properly handle relocating non-primary shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18701</link><project id="" key="" /><description>Relocation of non-primary shards is realized by recovering from the primary shard. Recovery throttling wrongly equates non-primary relocation as recovering a shard from the non-primary relocation source, however.

Closes #18640
</description><key id="158148392">18701</key><summary>Fix recovery throttling to properly handle relocating non-primary shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-02T14:02:12Z</created><updated>2016-06-03T12:12:45Z</updated><resolved>2016-06-03T12:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-02T14:32:32Z" id="223310182">left some questions... looks good in general
</comment><comment author="s1monw" created="2016-06-03T07:41:19Z" id="223512350">LGTM would just love some doc strings on the allocation decider what each condition means
</comment><comment author="ywelsch" created="2016-06-03T11:16:00Z" id="223553463">@s1monw I've added a method in 0d1fc7635fdb786756acb920a9553bb8ce8e0676 that helps with documenting the two cases. It shows that the if/else branching is determined by whether we are going to have a peer recovery or not.
</comment><comment author="s1monw" created="2016-06-03T11:25:06Z" id="223555055">great thanks!  LGTM
</comment><comment author="bleskes" created="2016-06-03T11:43:51Z" id="223558046">LGTM2 . left some very minor and optional suggestions.
</comment><comment author="ywelsch" created="2016-06-03T12:12:45Z" id="223562966">@bleskes I've added your suggestions and will do a follow-up to merge ThrottlingAllocatonDecider with ReplicaAfterPrimaryActiveAllocationDecider. @s1monw @bleskes thanks for the reviews.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make constant score queries eligible for caching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18700</link><project id="" key="" /><description>I see users using `range` and other constant score queries under `bool.must` rather than under `bool.filter`, which means that they currently aren't considered for filter caching.

Would it make sense to rewrite (during the coordinating node rewrite process) eg `range` queries as `constant_score: { filter: { range...` so that users get the benefit of caching regardless?
</description><key id="158136886">18700</key><summary>Make constant score queries eligible for caching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2016-06-02T13:10:31Z</created><updated>2016-08-02T21:09:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-02T13:12:58Z" id="223287010">If we want to do this, we don't even need to add a rewrite phase, we could just make the parser wrap the query inside of a ConstantScoreQuery.
</comment><comment author="rpeddacama" created="2016-08-02T19:15:30Z" id="237012712">I am interested in working on this ticket. Could you please let me know the next steps to proceed further. Thanks.
</comment><comment author="jpountz" created="2016-08-02T21:09:02Z" id="237044681">There are two ways to tackle this: either modify RangeQueryBuilder.toQuery to wrap the produced query in a ConstantScoreQuery, or modify the `rangeQuery` method of the MappedFieldType implementations to wrap the produced query in a ConstantScoreQuery. I think I like the latter better. @rjernst Do you have an opinion?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow `_shrink` to N shards if source shards is a multiple of N</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18699</link><project id="" key="" /><description>Today we allow to shrink to 1 shard but that might not be possible due to
too many documents or a single shard doesn't meet the requirements for the index.
The logic can be expanded to N shards if the source index shards is a multiple of N.
This guarantees that there are not hotspots created due to different number of shards
being shrunk into one.

There is still some work to do on the documentation end etc. but I wanted to get it out there to get initial feedback if we should do it at all.
</description><key id="158134152">18699</key><summary>Allow `_shrink` to N shards if source shards is a multiple of N</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-02T12:56:56Z</created><updated>2016-06-07T08:06:42Z</updated><resolved>2016-06-07T08:06:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-02T16:42:25Z" id="223349936">This is not the part of the codebase I'm most familiar with, but it looks good.
</comment><comment author="s1monw" created="2016-06-03T09:00:07Z" id="223527579">this is ready for review, @clintongormley I messed with docs you might wanna make sure the art is not damaged 
</comment><comment author="clintongormley" created="2016-06-03T11:42:30Z" id="223557844">@s1monw couple of small comments, otherwise LGTM
</comment><comment author="ywelsch" created="2016-06-03T13:35:37Z" id="223580268">Can you also update the estimate shard size for shrinked indices (#18659)? I was also wondering if we can relax the condition to have all shards active on one node... i.e. reduce it to the ones that are needed for local recovery (selectShrinkShards for the shard that is being recovered).
</comment><comment author="s1monw" created="2016-06-03T14:19:52Z" id="223591586">&gt; Can you also update the estimate shard size for shrinked indices (#18659)? I was also wondering if we can relax the condition to have all shards active on one node... i.e. reduce it to the ones that are needed for local recovery (selectShrinkShards for the shard that is being recovered).

all of those should be done in a follow up. I am currently not very in favor to reduce that limitation just now
</comment><comment author="ywelsch" created="2016-06-03T14:25:02Z" id="223593013">ok, Iet's do a follow-up for estimated shard size then. LGTM
</comment><comment author="s1monw" created="2016-06-03T21:30:47Z" id="223699600">&gt; ok, Iet's do a follow-up for estimated shard size then. LGTM

@ywelsch I misunderstood, I agree we should fix the estimated shard size here - the node restriction should be a followup. I will push a new commit later or next week
</comment><comment author="s1monw" created="2016-06-06T13:52:32Z" id="223965142">@ywelsch I pushed a new commit
</comment><comment author="ywelsch" created="2016-06-06T14:45:19Z" id="223980666">LGTM. Thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back pending deletes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18698</link><project id="" key="" /><description>It was lost in #18602
</description><key id="158131532">18698</key><summary>Add back pending deletes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>bug</label></labels><created>2016-06-02T12:44:13Z</created><updated>2016-06-06T13:15:22Z</updated><resolved>2016-06-06T13:14:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-05T18:40:12Z" id="223829514">LGTM, with a minor suggestion. This introduces old behavior and I know that is what you wanted to achieve here. Long term I think we can clean up more things here (like shard level pending deletes which are really only processed as far as I can tell when index deletion happens).
</comment><comment author="ywelsch" created="2016-06-06T13:15:22Z" id="223955152">Thanks for the review, @bleskes. I've added a comment to `hasUncompletedPendingDeletes()` why it's needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> feature request - same field different type auto rename filed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18697</link><project id="" key="" /><description>**Describe the feature**:
I know elasticsearch couldn't process same field with different type. I want to let the elasticsearch auto rename the field name, then it could be processed.

Ex:
{ name: 'elasticsearch' }
{ name: { first: 'elastic', last: 'search' } }

process first log is ok, when process the second, the name is not string type, but object. Could elasticsearch auto rename the second name filed to 'name_object', then process it?

This is a big problem we met currently, if this feature could not be implement, we could just use plain text codec, not json codec.
</description><key id="158116035">18697</key><summary> feature request - same field different type auto rename filed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">huaoguo</reporter><labels /><created>2016-06-02T11:11:23Z</created><updated>2016-06-02T11:18:31Z</updated><resolved>2016-06-02T11:18:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="huaoguo" created="2016-06-02T11:15:47Z" id="223262967">rename field to old_field_name + 'typeof old_field_name'
</comment><comment author="clintongormley" created="2016-06-02T11:18:31Z" id="223263437">@huaoguo we can't do things like this automatically, but you will be able to with the ingest pipelines coming in 5.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve percolate query performance by not verifying certain candidate matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18696</link><project id="" key="" /><description>We can skip the MemoryIndex verification for candidate matches that we know upfront are always an actual match and we don't care about scores (so `percolate` query is wrapped in `bool` filter clause or `constant_score` query). Skipping the MemoryIndex verification can boost performance as that is the most expensive step during the executing of the `percolate` query.

At index time during query term extraction we store in a numeric docvalues field wether this candidate match is verified. So if during pre selecting this query matches we check if this doc values field is set and if so report it as a match.

A number queries that can skip MemoryIndex verification step:
- TermQuery
- TermsQuery
- BooleanQuery with only should clauses, no minimum_should_match set and at least one should clause is a verified query.
- BooleanQuery with a single must clause that is a verified query.
</description><key id="158115947">18696</key><summary>Improve percolate query performance by not verifying certain candidate matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-02T11:10:56Z</created><updated>2016-06-26T10:35:59Z</updated><resolved>2016-06-24T13:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-15T15:13:10Z" id="226218804">I left some comments, but I like it!
</comment><comment author="jpountz" created="2016-06-17T10:30:03Z" id="226736214">I just did another round.
</comment><comment author="martijnvg" created="2016-06-21T09:23:08Z" id="227386805">@jpountz Thx! I've updated and rebased the PR. The last 4 commits are actually new.
</comment><comment author="jpountz" created="2016-06-22T13:16:23Z" id="227739733">I left some more comments.
</comment><comment author="jpountz" created="2016-06-24T08:55:13Z" id="228293102">LGTM
</comment><comment author="martijnvg" created="2016-06-24T13:48:16Z" id="228350202">thanks @jpountz!
</comment><comment author="martijnvg" created="2016-06-24T13:50:26Z" id="228350730">Pushed via to master via: 599a5489981e3b8f58e77c0433c382096dcfd591
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Add status bar on download</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18695</link><project id="" key="" /><description>As some plugins are becoming biggish now, it is hard for the user to know, if the plugin
is being downloaded or just nothing happens. I also had some cases running on a VM where I did not know if the networking was broken or still downloading, while testing.

This commit adds a progress bar during download, which can be disabled by using the `-q`
parameter - it is pretty much unstyled right now, but at least shows an indicator that things are happening.

In addition this updates to jimfs 1.1, which allows us to test the batch mode, as adding
security policies are now supported due to having jimfs:// protocol support in URL stream
handlers.

TODO: Update docs
TODO: Test on windows (only tested under osx/linux so far)

This is how it looks like right now

![Video demo](https://files.slack.com/files-pri/T0CUZ52US-F1DH7FLUR/output3.gif?pub_secret=a70887fc29)
</description><key id="158082527">18695</key><summary>Plugins: Add status bar on download</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-02T08:10:46Z</created><updated>2016-06-29T14:44:22Z</updated><resolved>2016-06-29T14:44:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-02T08:11:45Z" id="223224521">Looks great so far @spinscale! 
</comment><comment author="rjernst" created="2016-06-02T15:41:04Z" id="223331836">@spinscale I left some comments.
</comment><comment author="spinscale" created="2016-06-06T09:02:45Z" id="223904580">incorporated your review comments, thx for the review!
</comment><comment author="rjernst" created="2016-06-14T18:50:40Z" id="225979743">I left a couple more comments. LGTM otherwise.
</comment><comment author="dadoonet" created="2016-06-15T09:22:13Z" id="226134141">I checked it on a windows box and it gives the following results:

```
PS C:\Users\IEUser\Downloads\elasticsearch-5.0.0-alpha4-SNAPSHOT&gt; .\bin\elasticsearch-plugin install https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/repository-s3/5.0.0-alpha3/repository-s3-5.0.0-alpha3.zip
-&gt; Downloading https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/repository-s3/5.0.0-alpha3/repository-s3-5.0.0-alpha3.zip
[                                                 ] 1%  
[&gt;                                                ] 2%  
[&gt;                                                ] 3%  
[=&gt;                                               ] 4%  
[=&gt;                                               ] 5%  
[==&gt;                                              ] 6%  
[==&gt;                                              ] 7%  
[===&gt;                                             ] 8%  
[===&gt;                                             ] 9%  
[====&gt;                                            ] 10%  
[====&gt;                                            ] 11%  
[=====&gt;                                           ] 12%  
[=====&gt;                                           ] 13%  
[======&gt;                                          ] 14%  
[======&gt;                                          ] 15%  
[=======&gt;                                         ] 16%  
[=======&gt;                                         ] 17%  
[========&gt;                                        ] 18%  
[========&gt;                                        ] 19%  
[=========&gt;                                       ] 20%  
[=========&gt;                                       ] 21%  
[==========&gt;                                      ] 22%  
[==========&gt;                                      ] 23%  
[===========&gt;                                     ] 24%  
[===========&gt;                                     ] 25%  
[============&gt;                                    ] 26%  
[============&gt;                                    ] 27%  
[=============&gt;                                   ] 28%  
[=============&gt;                                   ] 29%  
```

Not as nice as on Linux but still working though. :) 
</comment><comment author="dadoonet" created="2016-06-29T12:48:56Z" id="229345600">Tested on windows:

```
C:\Users\IEUser\Downloads\elasticsearch-5.0.0-alpha4-SNAPSHOT\bin&gt;elasticsearch-plugin install x-pack
-&gt; Downloading x-pack from elastic
[=================================================] 100%&#160;&#160;
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.com.sun.activation.registries
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.RuntimePermission setFactory
* java.security.SecurityPermission createPolicy.JavaPolicy
* java.security.SecurityPermission getPolicy
* java.security.SecurityPermission putProviderProperty.BC
* java.security.SecurityPermission setPolicy
* java.util.PropertyPermission * read,write
* javax.net.ssl.SSLPermission setHostnameVerifier
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-&gt; Installed x-pack
```

Works well!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove allow running as root</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18694</link><project id="" key="" /><description>This commit removes the escape hatch for running Elasticsearch as root.

Closes #18688
</description><key id="158052380">18694</key><summary>Remove allow running as root</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-02T03:29:00Z</created><updated>2016-09-14T16:57:15Z</updated><resolved>2016-06-02T09:03:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-02T06:41:20Z" id="223208464">LGTM
</comment><comment author="lin-zhao" created="2016-09-13T17:56:57Z" id="246767302">Hi @jasontedor was there a particular reason to absolutely disable root? We usually run services in docker containers as root and this change breaks it. There's a way to make a docker image non-root compatible but it complicates the deployment and is a deviation from all of our services.
</comment><comment author="jasontedor" created="2016-09-13T18:10:56Z" id="246771605">@lin-zhao Yes, following the principle of least privilege, it limits the damage that can be done if a running Elasticsearch JVM is exploited.
</comment><comment author="lin-zhao" created="2016-09-13T22:26:08Z" id="246845552">A docker "root" only has access to resource assigned to the container and is in most cases much safer than, say, ec2-user.

It's simply wrong that a user must be unsafe just because it's called "root", and vice-versa. The decision should be made by people making the deployment.
</comment><comment author="jasontedor" created="2016-09-13T22:52:23Z" id="246851417">&gt; A docker "root" only has access to resource assigned to the container and is in most cases much safer than, say, ec2-user.

Those are still resources that you want protected, and also you are making a gross assumption that there are [no exploits in Docker that allow the container to be broken out of](https://blog.docker.com/2014/06/docker-container-breakout-proof-of-concept-exploit/). Note this quote from [Docker Container Breakout Proof-Of-Concept](https://blog.docker.com/2014/06/docker-container-breakout-proof-of-concept-exploit/):

&gt; We want to emphasize that this vulnerability only applies to users who are running Docker Engine in a **non-recommended way** by running untrusted applications with root privileges inside containers.
</comment><comment author="lin-zhao" created="2016-09-14T00:14:39Z" id="246866958">This post is two year old discussing a docker 0.11 vulnerability, which is already fixed in 1.0.

I don't assume any system to be exploit free. But the current docker is well within my comfort zone. On the other hand this change is a gross generalization that all users named "root" are unsafe. This is particularly questionable with docker's user namespace support, which allows the daemon running as non-root user while the container processors pretend to be running as root.
</comment><comment author="jasontedor" created="2016-09-14T00:38:02Z" id="246870922">&gt; This post is two year old discussing a docker 0.11 vulnerability, which is already fixed in 1.0.

Yet it still establishes that container breakout bugs are a thing. And the [Docker Security](https://docs.docker.com/engine/security/security/) docs today say:

&gt; Docker containers are, by default, quite secure; especially if you take care of running your processes inside the containers as non-privileged users (i.e., non-root).
</comment><comment author="s1monw" created="2016-09-14T08:18:46Z" id="246939759">&gt; It's simply wrong that a user must be unsafe just because it's called "root", and vice-versa. The decision should be made by people making the deployment.

just don't run as root and you are good. we have to draw a line here that might not make everybody happy. If it prevents a single incident then I think it's a good thing. I am sorry for you that you have to adopt.
</comment><comment author="bleskes" created="2016-09-14T08:33:33Z" id="246943240">&gt; But the current docker is well within my comfort zone. 

There is another aspect here, that relates exactly to this statement. You are an expert and that's great. This means that whatever we do, you know how to adapt and at the price of a small annoyment (and disagreement) you will get your system up and running. There are many people who are not so into docker as you are and justifyingly so do the easiest thing and expect the system to guide them in the right direction. We have seen numerous problems caused by people running ES as root. All of those problem have been addressed but we recognize there might be future ones. Looking at all our users as a whole, we have chosen to make this change. Sadly, you might pay the price (even if you disagree). Hopefully knowing why helps.
</comment><comment author="lin-zhao" created="2016-09-14T16:57:15Z" id="247080944">Thanks everyone for the explanation. I respect your design decision and will adapt.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Why is lucene query execution time more than query's "took" value, according to Profile API?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18693</link><project id="" key="" /><description>There is a query that was taking 3 seconds so I thought of profiling it using the `Profile API`.
You can see the result below:

```
{
  "took": 125241,
  "timed_out": false,
  "_shards": {
    "total": 24,
    "successful": 24,
    "failed": 0
  },
  "hits": {
    "total": 265076,
    "max_score": 3,
    "hits": [...]
  },
  "profile": {
    "shards": [
      {
        "id": "[klNxl8HWTyqF-f8BRBbiiA][index][0]",
        "searches": [
          {
            "query": [
              {
                "query_type": "BooleanQuery",
                "lucene": "...",
                "time": "247695.9417ms",
.
.
.
.
            "rewrite_time": 505668,
            "collector": [
              {
                "name": "SimpleFieldCollector",
                "reason": "search_top_hits",
                "time": "110.2805510ms"
              }
            ]
          }
        ]
      }
    ]
  }
}
```

I cannot understand when the value of `took` is `125241 ms`, how come the top-level Lucene query and the rewrite time are greater than this value? Am I missing something here?

ES Version: 2.3.3
Java: 1.8
</description><key id="158020055">18693</key><summary>Why is lucene query execution time more than query's "took" value, according to Profile API?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">apanimesh061</reporter><labels><label>:Search</label><label>feedback_needed</label></labels><created>2016-06-01T22:20:25Z</created><updated>2016-07-14T13:29:43Z</updated><resolved>2016-07-14T13:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-02T09:23:54Z" id="223240274">@polyfractal any ideas?
</comment><comment author="polyfractal" created="2016-06-02T13:27:46Z" id="223290921">@apanimesh061 Could you gist up somewhere the full output of the profiler response?  The short answer is I'm not sure.  It might be a recursive-style bug where timings are recorded multiple times, or perhaps one of the leaf components in your query isn't behaving correctly.

Note: the `rewrite` time is in nanoseconds, so that's only 0.5ms.  But the rest of the time looks strange, I agree :)
</comment><comment author="apanimesh061" created="2016-06-03T14:58:47Z" id="223602591">@polyfractal I have added the whole output [here](https://gist.github.com/apanimesh061/49ae3208287ced5d7ca33b125b2fbd27).
</comment><comment author="polyfractal" created="2016-06-03T16:58:38Z" id="223634135">Thanks @apanimesh061, that helped a bunch!

@jpountz, it looks like my assumptions about method invocation order/dependence may be wrong?  Timings assume that the `breakdown` for each component are "independent" and do not include their children times.  E.g. the `score()` of a boolean query does not include the timing of `score()` for the children.

But looking closer, I think this is wrong.  For example, I'm looking at `ConjunctionScorer`, and it's `score()` method calls the `score()` of all it's children.  I imagine this is similar for the other methods (nextDoc, etc)...and it makes sense, as the Boolean would need to call down to all of it's children.

So I think the `breakdown` timings are actually inclusive of children already.  This seems to be backed up by the timings:

```
"query_type": "GlobalOrdinalsQuery",
"lucene": "GlobalOrdinalsQuery{joinField=_parent#influencer}",
"time": "17080.18500ms",
"breakdown": {
  "score": 4154770,
  "create_weight": 19246,
  "next_doc": 25930,
  "match": 3595232475,
  "build_scorer": 31100,
  "advance": 9849058759
},
"children": [
  {
    "query_type": "TermQuery",
    "lucene": "_type:post",
    "time": "3631.662722ms",
    "breakdown": {
      "score": 0,
      "create_weight": 15276,
      "next_doc": 9534,
      "match": 0,
      "build_scorer": 23276,
      "advance": 3631614636
    }
  }
]
```
- `TermQuery`:  breakdown == 3631.662722 == reported time
- `GlobalOrdinalsQuery`: breakdown == 13448.52228 != reported time

But if you add the child's time to the breakdown (13448.52228 + 3631.662722) you get the reported time of 17080.18500.

Further, the top-level breakdown comes out to 101,065ms, which is suspiciously close to the took time of 116,727ms (minus some non-profiled overhead).  Whereas the reported time is over double that.

So I think we can just [remove the recursive part of the calculation](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/profile/query/InternalQueryProfileTree.java#L234-L237) and everything should work?

Simple change, but I'm afraid I've over-simplified how Lucene works, would be interested in hearing what you think. :)
</comment><comment author="apanimesh061" created="2016-06-08T16:28:58Z" id="224647321">@polyfractal 
So, basically you mean to say is that the time on a single node is already inclusive of the times of its children but due to that recursive function they are being added again?
</comment><comment author="polyfractal" created="2016-06-08T16:49:49Z" id="224654022">@apanimesh061 Yep, that's correct.  

The `breakdown` timings of a node is the node + the timing of children.  But when the `"time"` summary is calculated, it recursively adds the `breakdown` + the `breakdown` of the children, so it's essentially counting the times twice.

You should be able to use just the `breakdown` time for now and get an accurate profile (although it'll be in nanoseconds, which is less pleasant to work with).

I'm out this week and part of the next, but I'll work on this as soon as I get back :)
</comment><comment author="polyfractal" created="2016-07-05T13:24:20Z" id="230476844">Quick note (and reminder to self): I'm back from my traveling extravaganza and will fix this imminently.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FVH + hunspell breaks when searching multiple fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18692</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2
**JVM version**: 1.8.0_91
**OS version**: Ubuntu 14.04
**Hunspell dictionary**: http://extensions.openoffice.org/en/project/english-dictionaries-apache-openoffice

When using the fast vector highlighter and the hunspell stemmer, the highlights for a given field change depending on which fields are searched. If I disable stemming, use `kstem`, or use the plain highlighter then everything works as expected. It appears to be some weird interaction between FVH, hunspell, and my particular search query.

**Index**:

``` sh
curl -XPUT 'http://localhost:9200/highlight_test' -d '{
    "settings": {
        "analysis": {
            "filter": {
                "en_US": {
                    "type": "hunspell",
                    "language": "en_US"
                }
            },
            "analyzer": {
                "en_US": {
                    "tokenizer": "standard",
                    "filter": [
                        "lowercase",
                        "en_US"
                    ]
                }
            }
        }
    },
    "mappings": {
        "default": {
            "_all": {
                "enabled": false
            },
            "properties": {
                "raw_text": {
                    "analyzer": "en_US",
                    "type": "string",
                    "term_vector": "with_positions_offsets"
                },
                "title": {
                    "analyzer": "en_US",
                    "type": "string"
                }
            }
        }
    }
}'

curl -XPUT 'http://localhost:9200/highlight_test/default/1' -d '{
    "raw_text": "EX-99.2\n10\nf8k072915ex99ii_globalpart.htm\nGLOBAL PARTNER ACQUISITION CORP. ANNOUNCES CLOSING OF INITIAL PUBLIC OFFERING\n\n\n\nExhibit 99.2\n\n\n\n\n\n\n\n\nGlobal Partner\nAcquisition Corp. Announces Closing of Initial Public Offering\n\n\n\n\n\nNEW YORK, August 4, 2015 /PRNewswire/\n-- Global Partner Acquisition Corporation (NASDAQ:GPACU) (the \"Company\") announced today that it closed its initial\npublic offering of 15,525,000 units, including 2,025,000 units issued pursuant to the full exercise by the underwriters of their\nover-allotment option. The offering was priced at $10.00 per unit, resulting in gross proceeds of $155,250,000. The Company is\na newly organized blank check company formed for the purpose of effecting a merger or other business combination with a target\ncompany. The proceeds of the offering will be used to fund such business combination.\n\n\n\n\n\nThe Companys units began trading\non the NASDAQ Capital Market under the ticker symbol &#8220;GPACU&#8221; on July 30, 2015. Each unit consists of one share of the\nCompanys common stock and one warrant. Each warrant will entitle the holder thereof to purchase one-half of one share of the Companys\ncommon stock at $5.75 per half share. Once the securities comprising the units begin separate trading, the common stock and warrants\nare expected to be listed on the NASDAQ Stock Market under the ticker symbols &#8220;GPAC&#8221; and &#8220;GPACW,&#8221; respectively.\n\n\n\n\n\nDeutsche Bank Securities Inc. acted\nas sole book-running manager for the offering.\n\n\n\n\n\nThe offering is being made only\nby means of a prospectus, copies of which may be obtained from Deutsche Bank Securities Inc., 60 Wall Street, New York, NY 10005-2836,\nAttention: Prospectus Group, Telephone: (800) 503-4611, Email: prospectus.cpdg@db.com.\n\n\n\n\n\nA registration statement relating\nto these securities has been filed with, and declared effective by, the Securities and Exchange Commission on July 29, 2015.\n\n\n\n\n\nThis press release shall not constitute\nan offer to sell or the solicitation of an offer to buy, nor shall there be any sale of these securities in any state or jurisdiction\nin which such an offer, solicitation or sale would be unlawful prior to registration or qualification under the securities laws\nof any such state or jurisdiction.\n\n\n\n\n\nFor more information, please contact: pzepf@globalpartnerac.com.\n\n\n\n\n\nFORWARD-LOOKING STATEMENTS\n\n\n\n\n\nThis press release contains statements\nthat constitute &#8220;forward-looking statements,&#8221; including with respect to the anticipated use of the net proceeds. No\nassurance can be given that the net proceeds of the offering will be used as indicated. Forward-looking statements are subject\nto numerous conditions, many of which are beyond the control of the Company, including those set forth in the Risk Factors section\nof the Companys registration statement and preliminary prospectus for the offering filed with the Securities and Exchange Commission\n(&#8220;SEC&#8221;). Copies are available on the SECs website, www.sec.gov. The Company undertakes no obligation to update\nthese statements for revisions or changes after the date of this release, except as required by law.\n\n\n\n\n\nContact:\n\n\nPaul Zepf\n\n\nChief Executive Officer\n\n\nGlobal Partner Acquisition Corporation\n\n\npzepf@globalpartnerac.com",
    "title": "EX-99.2 - Global Partner Acquisition Corp. Announces Closing Of Initial Public Offering"
}'
```

**Search**:

``` sh
curl -XGET 'http://localhost:9200/highlight_test/default/_search' -d '{
    "fields": [],
    "query": {
        "bool": {
            "must": {
                "multi_match": {
                    "query": "initial public offering",
                    "type": "best_fields",
                    "operator": "and",
                    "fields": [
                        "raw_text",
                        "title"
                    ]
                }
            },
            "should": [
                {
                    "multi_match": {
                        "query": "initial public offering",
                        "type": "phrase",
                        "boost": 100,
                        "fields": [
                            "raw_text",
                            "title"
                        ]
                    }
                },
                {
                    "multi_match": {
                        "query": "initial public offering",
                        "type": "phrase",
                        "slop": 10,
                        "boost": 10,
                        "fields": [
                            "raw_text",
                            "title"
                        ]
                    }
                }
            ]
        }
    },
    "highlight": {
        "order": "score",
        "fields": {
            "raw_text": {
                "number_of_fragments": 1
            }
        }
    }
}'
```

**Expected Result**:
If I only search `raw_text` in the above query rather than both `title` and `raw_text`, then I will receive a great highlight.

``` js
{
    "took": 1,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 0.09119049,
        "hits": [
            {
                "_index": "highlight_test",
                "_type": "default",
                "_id": "1",
                "_score": 0.09119049,
                "highlight": {
                    "raw_text": [
                        "ACQUISITION CORP. ANNOUNCES CLOSING OF &lt;em&gt;INITIAL PUBLIC OFFERING&lt;/em&gt;\n\n\n\nExhibit 99.2\n\n\n\n\n\n\n\n\nGlobal Partner"
                    ]
                }
            }
        ]
    }
}
```

**Actual Result**:
Notice that the highlight for `raw_text` is different and worse than the expected result.

``` js
{
    "took": 2,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 0.33662215,
        "hits": [
            {
                "_index": "highlight_test",
                "_type": "default",
                "_id": "1",
                "_score": 0.33662215,
                "highlight": {
                    "raw_text": [
                        "as sole book-running manager for the &lt;em&gt;offering&lt;/em&gt;.\n\n\n\n\n\nThe &lt;em&gt;offering&lt;/em&gt; is being made only\nby means of a prospectus"
                    ]
                }
            }
        ]
    }
}
```
</description><key id="158015128">18692</key><summary>FVH + hunspell breaks when searching multiple fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rpedela</reporter><labels><label>:Highlighting</label></labels><created>2016-06-01T21:49:50Z</created><updated>2016-11-28T09:47:08Z</updated><resolved>2016-11-25T15:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-25T15:49:39Z" id="262983988">Closing in favour of #21621</comment><comment author="rpedela" created="2016-11-26T16:36:28Z" id="263072770">@clintongormley How does integrating the UnifiedHighlighter fix a bug with FVH?</comment><comment author="clintongormley" created="2016-11-28T09:47:08Z" id="263226983">@rpedela by replacing the FVH with the UH</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve Matrix Agg RunningStats performance by using field ordinals as hash keys</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18691</link><project id="" key="" /><description>This improvement comes from feedback in PR #18300 where it was suggested to use ordinals as hash keys instead of field names.
</description><key id="158014708">18691</key><summary>Improve Matrix Agg RunningStats performance by using field ordinals as hash keys</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>enhancement</label></labels><created>2016-06-01T21:47:27Z</created><updated>2016-06-01T21:47:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix EC2 discovery settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18690</link><project id="" key="" /><description>Fix EC2 discovery setting

Closes #18652

Follow up for #18662

We add some tests to check that settings are correctly applied.
Tests revealed that some checks were missing.

Another PR will come after for S3 repositories but it's a bit more complex for repositories.
</description><key id="158013027">18690</key><summary>Fix EC2 discovery settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-06-01T21:37:50Z</created><updated>2016-07-22T22:48:23Z</updated><resolved>2016-07-22T22:47:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-01T21:38:57Z" id="223133065">@rjernst Could you have a look at this one please?
</comment><comment author="dadoonet" created="2016-07-11T21:03:06Z" id="231864140">@rjernst I pushed also another commit to this PR. Let me know.
</comment><comment author="rjernst" created="2016-07-12T07:46:12Z" id="231963699">@dadoonet I don't think your change addresses my concern. That is, the try/catch in the _test_ means we don't know what we are testing.
</comment><comment author="dadoonet" created="2016-07-12T13:56:05Z" id="232054793">&gt; That is, the try/catch in the test means we don't know what we are testing.

I see. Here is what I tried:
- Add in `repository-s3/build.gradle`:

```
test {
  // this is needed for insecure plugins, remove if possible!
  systemProperty 'tests.artifact', project.name
  systemProperty 'AWS_ACCESS_KEY_ID', 'DUMMY_ACCESS_KEY'
  systemProperty 'AWS_SECRET_KEY_ID', 'DUMMY_SECRET_KEY'
}
```
- Change the test with:

``` java
    public void testAWSCredentialsWithSystemProviders() {
        AWSCredentialsProvider credentialsProvider = InternalAwsS3Service.buildCredentials(logger, "", "");

        AWSCredentials credentials = credentialsProvider.getCredentials();
        assertThat(credentials.getAWSAccessKeyId(), is("DUMMY_ACCESS_KEY"));
        assertThat(credentials.getAWSSecretKey(), is("DUMMY_SECRET_KEY"));
    }
```

But this fails. Is this the way I'm supposed to define system properties with Gradle?

It fails with:

```
java.lang.AssertionError: 
Expected: is "DUMMY_ACCESS_KEY"
     but: was "&lt;HERE IS MY OWN AWS ACCESS KEY&gt;"
```
</comment><comment author="dadoonet" created="2016-07-12T15:16:16Z" id="232080527">@rjernst FYI I added a similar commit (which fails) to the other PR: https://github.com/elastic/elasticsearch/pull/18703/commits/5e3503576cf139c6442c7318f31bf44e64d694d8
</comment><comment author="dadoonet" created="2016-07-20T16:30:42Z" id="234004175">@rjernst If you have some spare cycle could you help me on that error?
</comment><comment author="rjernst" created="2016-07-20T17:05:06Z" id="234013973">@dadoonet did you mean for those to be env vars, not sysprops?
</comment><comment author="dadoonet" created="2016-07-20T20:25:43Z" id="234071063">&gt; did you mean for those to be env vars, not sysprops?

Hmmm. Indeed, when using sysprops I should define:

```
  systemProperty 'aws.accessKeyId', 'DUMMY_ACCESS_KEY'
  systemProperty 'aws.secretKey', 'DUMMY_SECRET_KEY'
```

But it still fails because the Credential chain is for now:

``` java
            credentials = new AWSCredentialsProviderChain(
                new EnvironmentVariableCredentialsProvider(),
                new SystemPropertiesCredentialsProvider(),
                new InstanceProfileCredentialsProvider()
            );
```

As I have local env var for S3, it fails with:

```
java.lang.AssertionError: 
Expected: is "DUMMY_ACCESS_KEY"
     but: was "&lt;HERE IS MY OWN AWS ACCESS KEY&gt;"
```

Obviously, if I change the order in the chain with:

``` java
            credentials = new AWSCredentialsProviderChain(
                new SystemPropertiesCredentialsProvider(),
                new EnvironmentVariableCredentialsProvider(),
                new InstanceProfileCredentialsProvider()
            );
```

It works from the CLI because Sysprops have now precedence on other settings.

For the record it does not work from the IDE as the IDE seems to not define the Sysprops in that case.
Not sure if we can omit this test if run from the IDE?

So? What do you think?

Should I change the order in the chain to make that test pass?
Or detect as I was doing previously if there is env vars that key/secret should correspond to the env var.
If env var not set, then check that key/secret is `DUMMY_ACCESS_KEY/DUMMY_SECRET_KEY`.

Thoughts?

See also latest commit: https://github.com/elastic/elasticsearch/pull/18703/commits/10a7160f35bb09d0d5b8920af83e3b44a455000f
</comment><comment author="rjernst" created="2016-07-21T17:07:44Z" id="234318993">I would not do the previous "detection". That was my entire concern: the test would work differently depending on the environment it was in (eg you have aws keys in your env and I do not). I had thought we exposed the ability to add environment variables to the randomized runner (it supports it in ant), but apparently I forgot to add that to the gradle hook.  Moving the check to sysprop first is fine with me, but I would personally also make this more transparent to the user. Perhaps they should choose that they want to pull creds from sysprops, env, or instance profile? Rather than us having an arbitrary fallback order...it won't fix the testing issue here, but I think it would be a good change in general.

In any case, to test this, I think we need to:
1. Add env vars support to our gradle wrapper around randomized runner
2. Put either sys props or env vars in the test task
3. Add another task to run a one off test with the opposite set of creds

To test instance profile works, I think we will need to have a fixture (and not even sure that can work...), but the env and sysprops at least seem testable.
</comment><comment author="dadoonet" created="2016-07-22T10:00:30Z" id="234505083">&gt; Moving the check to sysprop first is fine with me, but I would personally also make this more transparent to the user. Perhaps they should choose that they want to pull creds from sysprops, env, or instance profile? 

In term of coding and testability, I agree that it makes more sense.
In term of simplicity for the user, I believe that using a chain of credential providers is really nice. 

Because users won't have to define it in the YML file explicitly. It seems to be a common practice on AWS and [recommended by AWS](http://docs.aws.amazon.com/java-sdk/latest/developer-guide/credentials.html). 

I think BTW that we should use `DefaultAWSCredentialsProviderChain` instead of providing the detail of the chain ourselves. If we do, you can ignore most of the next comments because testing this will still be hard as it depends on the plaform we run the test on.

I'd like to do this in another PR. The question of testing it will still remain as it will depend on 4 possibilities:

&gt; AWS credentials provider chain that looks for credentials in this order:
&gt; - Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for .NET), or AWS_ACCESS_KEY and AWS_SECRET_KEY (only recognized by Java SDK)
&gt; - Java System Properties - aws.accessKeyId and aws.secretKey
&gt; - Credential profiles file at the default location (~/.aws/credentials) shared by all AWS SDKs and the AWS CLI
&gt; - Credentials delivered through the Amazon EC2 container service if AWS_CONTAINER_CREDENTIALS_RELATIVE_URI" environment variable is set and security manager has permission to access the variable,
&gt;   Instance profile credentials delivered through the Amazon EC2 metadata service

But here, I don't think that we want to test that `DefaultAWSCredentialsProviderChain` is working well. Well, it's provided by a library and we have to trust it. What we want to test here is basically that `if/else` condition:

``` java
        if (key.isEmpty() &amp;&amp; secret.isEmpty()) {
             // Scenario 1
        } else {
             // Scenario 2
        }
```

I don't want to block this PR because of this and I propose to mark this test as `@Ignore` or remove it entirely for now and discuss it in another PR where we change to the credential chain to `DefaultAWSCredentialsProviderChain`.

Do you agree with that plan?
</comment><comment author="rjernst" created="2016-07-22T13:54:37Z" id="234549984">Sure,  fixing the test as a follow up here is fine. 
</comment><comment author="dadoonet" created="2016-07-22T21:14:59Z" id="234658586">Great! I created #19556 as a follow up for this.
</comment><comment author="dadoonet" created="2016-07-22T21:36:58Z" id="234663083">@rjernst I updated this PR. Could you give a final review for it?
Do you think I should rebase on master and squash the commits?
</comment><comment author="rjernst" created="2016-07-22T21:42:56Z" id="234664316">LGTM, I left some minor comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unxepected error when running command elasticsearch-plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18689</link><project id="" key="" /><description>**Elasticsearch version**:
5.0.0. alpha 3
**JVM version**:
1.8.0-92
**OS version**:
Ubuntu 3.16.0-71-lowlatency
**Description of the problem including expected versus actual behavior**:
Trying to install plugins with elasticsearch-plugin command, Checking new syntax with out any option or help gives error below:
**Steps to reproduce**:
 just run command

```
./elasticsearch-plugin
```

**Provide logs (if relevant)**:

```
/usr/share/elasticsearch/bin# ./elasticsearch-plugin
A tool for managing installed elasticsearch plugins

Commands
--------
list - Lists installed elasticsearch plugins
install - Install a plugin
remove - Removes a plugin from elasticsearch

Non-option arguments:
command

Option         Description
------         -----------
-h, --help     show help
-s, --silent   show minimal output
-v, --verbose  show verbose output
_**ERROR: E is not a recognized option**_

```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="158002932">18689</key><summary>Unxepected error when running command elasticsearch-plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels><label>:Plugins</label><label>bug</label><label>feedback_needed</label></labels><created>2016-06-01T20:45:27Z</created><updated>2017-05-09T08:16:02Z</updated><resolved>2016-09-14T21:09:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-01T21:01:33Z" id="223123472">This is because you have not specified a command, one of `list`, `install`, or `remove`. The reason for the error message is because the plugin script passes something like `-Edefault.path.conf=/Users/jason/elasticsearch/elasticsearch-5.0.0-alpha3/config` to the `org.elasticsearch.plugins.PluginCli` Java class as a program argument. When you specify a command, this argument will trickle down to the appropriate sub-command. However, since you have not, `org.elasticsearch.plugins.PluginCli` itself is attempting to parse, but this command-line argument does not mean anything to the top-level class. I'm afraid there is not an issue here, but please let us know if you think otherwise?
</comment><comment author="rjernst" created="2016-06-01T21:14:14Z" id="223126773">This is a bug IMO. I've seen this myself, and I imagine it would be very confusing to a user.
</comment><comment author="rjernst" created="2016-06-01T21:15:56Z" id="223127213">@jasontedor Now that custom plugin paths are gone, what need is there for reading elasticsearch.yml?
</comment><comment author="jasontedor" created="2016-06-02T02:47:38Z" id="223181871">&gt; Now that custom plugin paths are gone, what need is there for reading elasticsearch.yml?

There isn't a need to read elasticsearch.yml, but there is a need to know where `path.conf` is for plugins that do package a plugin config file.
</comment><comment author="jasontedor" created="2016-09-14T01:35:38Z" id="246879741">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Option to run ES as root not recognized</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18688</link><project id="" key="" /><description>**Elasticsearch version**:
5.0.0-alpha2

**JVM version**:
openjdk version "1.8.0_51"
OpenJDK Runtime Environment (build 1.8.0_51-b16)
OpenJDK 64-Bit Server VM (build 25.51-b03, mixed mode)

**OS version**:
uname -a
Linux ****\* 2.6.32-504.8.1.el6.centos.plus.x86_64 # 1 SMP Wed Jan 28 20:58:59 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:
It was found, in previous versions, that to run Elasticsearch as root (Even though not recommended) was possible by running bin/elasticsearch with the -Des.insecure.allow.root=true option set.

I am unable to run ES as root in the new versions using this param.  On the command line, it outputs the following:

```
(18:27:48)-&gt; bin/elasticsearch -Des.insecure.allow.root=true
starts elasticsearch

Option             Description
------             -----------
-E &lt;KeyValuePair&gt;  Configure an Elasticsearch setting
-V, --version      Prints elasticsearch version
                     information and exits
-d, --daemonize    Starts Elasticsearch in the background
-h, --help         show help
-p, --pidfile      Creates a pid file in the specified
                     path on start
-s, --silent       show minimal output
-v, --verbose      show verbose output
ERROR: D is not a recognized option
```

I tried setting the option in the java.options file and I see that it is correctly passing the if check on line 90 of Bootstrap.java evident by the log output, however, it then throws an 'unknown setting' error for IllegalArgumentException (log output below).

**Provide logs (if relevant)**:

```
[2016-06-01 18:27:45,920][WARN ][bootstrap                ] running as ROOT user. this is a bad idea!
[2016-06-01 18:27:45,947][WARN ][bootstrap                ] unable to install syscall filter:
java.lang.UnsupportedOperationException: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in
        at org.elasticsearch.bootstrap.Seccomp.linuxImpl(Seccomp.java:335)
        at org.elasticsearch.bootstrap.Seccomp.init(Seccomp.java:616)
        at org.elasticsearch.bootstrap.JNANatives.trySeccomp(JNANatives.java:215)
        at org.elasticsearch.bootstrap.Natives.trySeccomp(Natives.java:99)
        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:99)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:152)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
[2016-06-01 18:27:46,500][INFO ][node                     ] [pet-LAB2-1] version[5.0.0-alpha2], pid[16464], build[e3126df/2016-04-26T12:08:58.960Z]
[2016-06-01 18:27:46,500][INFO ][node                     ] [pet-LAB2-1] initializing ...
[2016-06-01 18:27:47,216][INFO ][plugins                  ] [pet-LAB2-1] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-06-01 18:27:47,265][INFO ][env                      ] [pet-LAB2-1] using [1] data paths, mounts [[/ (/dev/mapper/vg_centos66template-lv_root)]], net usable_space [37.2gb], net total_space [43.6gb], spins? [possibly], types [ext4]
[2016-06-01 18:27:47,265][INFO ][env                      ] [pet-LAB2-1] heap size [1007.3mb], compressed ordinary object pointers [true]
[2016-06-01 18:27:48,274][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: unknown setting [insecure.allow.root]
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:267)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
        at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:120)
        at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
        at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
        at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:103)
        at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:148)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:235)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```
</description><key id="157977298">18688</key><summary>Option to run ES as root not recognized</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dayjavid</reporter><labels /><created>2016-06-01T18:36:49Z</created><updated>2017-05-09T08:16:02Z</updated><resolved>2016-06-02T09:02:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-01T18:57:59Z" id="223091196">Sorry for the chaos here. Elasticsearch previously permitted multiple ways of specifying settings and system properties, and entangled system properties and settings (each got converted to the other). Additionally, settings use to be extremely lenient.

In Elasticsearch 5.0.0, we are attempting to make progress on these fronts. Now, all settings must be registered. Additionally, we have removed some of the multiple ways of specifying settings and system properties, and removed the entanglement between settings and system properties. Some of that work was in-progress when 5.0.0-alpha2 shipped. That's what you're running into here.

In particular, you can't pass system properties as `-D` arguments to Elasticsearch anymore. Instead, you have to pass them as Java options, as you figured out. The next problem though is the system properties and settings entanglement. In particular, system properties would get converted to Elasticsearch settings. But all settings must be registered, and `insecure.allow.root` is not a registered setting. Thus, the failure.

In 5.0.0-alpha3, all of this has been cleaned up. The entanglement between system properties and settings has been removed. This means that the system property `es.insecure.allow.root` will not automatically be converted to a setting which means it's no longer a problem that it's not registered.

The 5.0.0-alpha3 was released yesterday and is free of this problem. If you have a chance to download and try it out, I'd appreciate feedback on whether or not it worked out for you.

That said, I would like to know your use case for running as root? I think that eventually it would be nice if we could remove this functionality while continuing to disallow running as root.
</comment><comment author="dayjavid" created="2016-06-01T19:04:49Z" id="223093081">I'll DL apha3 right now and re-test. Sorry about that.

As for my use case, I wish I had a great one for you. I ended up running into a lot of one-off issues that were just more easily resolved by running as root. If I recall it had something to do with the fact that I was installing via the tar/zip files, and they did not include init scripts (like the Repo versions do), so I had written my own, and wasn't able, at the time, to figure out how to get an init script to run the process as a different user other than root. 
Once I got into a working state, I've worked on more pressing issues. The risks did not outweigh the time it would take to fix. It's something I will probably change eventually - possibly even for my 5.0 migration when that times comes. As for right now, I've just been shooting for as close to 'backwards compatible' as I can get for my testing. 
I do have on my to-do list to drop in the init scripts delievered with the repo versions, (and eventually switch over to installing from the repo instead of the downloaded packages), and I imagine that will be a big step for me to be able to use without root.
</comment><comment author="jasontedor" created="2016-06-01T19:16:34Z" id="223096044">Thanks for the feedback @dayjavid! :smile:

&gt; I had written my own, and wasn't able, at the time, to figure out how to get an init script to run the process as a different user other than root. 

The reason that the services must be started as the root user is exactly so that the scripts can launch the processes as the elasticsearch user.

&gt; I imagine that will be a big step for me to be able to use without root.

It sounds like it might be okay then if we push to remove this sooner rather than later. Am I reading you correctly?
</comment><comment author="dayjavid" created="2016-06-01T19:24:26Z" id="223098047">I will make sure that when I migrate to 5.0 I will be able to run as non-root, yes.
</comment><comment author="clintongormley" created="2016-06-02T09:02:45Z" id="223235543">Sounds like there is nothing left to do here, closing
</comment><comment author="jasontedor" created="2016-06-02T09:03:04Z" id="223235625">&gt; I will make sure that when I migrate to 5.0 I will be able to run as non-root, yes.

Thanks @dayjavid. :smile:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update reindex.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18687</link><project id="" key="" /><description>Potentially fixing some copy/paste errors
</description><key id="157972470">18687</key><summary>Update reindex.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ZacharyThomas</reporter><labels><label>docs</label></labels><created>2016-06-01T18:14:00Z</created><updated>2016-06-01T18:15:19Z</updated><resolved>2016-06-01T18:15:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T18:15:19Z" id="223079256">thanks @ZacharyThomas - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 5.0 (5.0.0-alpha2) Disable multicast - unknown setting error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18686</link><project id="" key="" /><description>**Elasticsearch version**:
5.0.0-alpha2

**JVM version**:
openjdk version "1.8.0_51"
OpenJDK Runtime Environment (build 1.8.0_51-b16)
OpenJDK 64-Bit Server VM (build 25.51-b03, mixed mode)

**OS version**:
uname -a
Linux ****\* 2.6.32-504.8.1.el6.centos.plus.x86_64 # 1 SMP Wed Jan 28 20:58:59 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:

```
Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [discovery.zen.ping.multicast.enabled]
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:267)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
        at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:120)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:235)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
```

I am attempting to disable multicast using methods defined from previous versions. Searches did not turn up any results on this. 

**Steps to reproduce**:
1. Setting in  elasticsearch.yml config file: `discovery.zen.ping.multicast.enabled: false`  
</description><key id="157971699">18686</key><summary>Elasticsearch 5.0 (5.0.0-alpha2) Disable multicast - unknown setting error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dayjavid</reporter><labels /><created>2016-06-01T18:10:20Z</created><updated>2017-05-09T08:16:02Z</updated><resolved>2016-06-01T18:11:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T18:11:59Z" id="223078307">Hi @dayjavid 

Multicast was only available as a plugin from 2.0 onwards, and that plugin was removed in 5.0, so the setting no longer applies
</comment><comment author="dayjavid" created="2016-06-01T18:15:52Z" id="223079410">I was under the impression that disabling multicast was recommended, especially on larger networks. 

Is that not true anymore? Or are there other ways to accomplish what that was trying to achieve?
</comment><comment author="clintongormley" created="2016-06-01T18:16:50Z" id="223079666">Multicast no longer exists.  Only unicast.
</comment><comment author="dayjavid" created="2016-06-01T18:18:52Z" id="223080205">Ahh ok. Understood. Thanks much!
</comment><comment author="mcxu" created="2016-12-14T16:08:44Z" id="267075208">What should be the configuration for unicast? Also, is there a page with documentation for the configurations? Thanks!</comment><comment author="jasontedor" created="2016-12-18T14:59:48Z" id="267825839">&gt; What should be the configuration for unicast? Also, is there a page with documentation for the configurations?

@mcxu The [docs](https://www.elastic.co/guide/en/elasticsearch/reference/5.1/modules-discovery-zen.html) you're looking for cover your first question. If you have additional questions, please use the [forum](https://discuss.elastic.co); Elastic reserves GitHub for verified bug reports and feature requests.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use JAVA_HOME or java.exe in PATH like the Linux scripts do</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18685</link><project id="" key="" /><description>I've enhanced the Windows batch scripts to retrieve the JAVA_HOME environment from the symlink installed by Oracle's Java. See also the PR elastic/logstash#4913 in logstash repo.
</description><key id="157965470">18685</key><summary>Use JAVA_HOME or java.exe in PATH like the Linux scripts do</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">StefanScherer</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-01T17:39:42Z</created><updated>2016-06-06T17:05:35Z</updated><resolved>2016-06-03T19:30:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T18:04:45Z" id="223076293">@gmarz could you take a look at this please?
</comment><comment author="jasontedor" created="2016-06-02T03:24:07Z" id="223186107">Sorry, I don't think this change should be made. `JAVA_EXE` is a non-standard environment variable, and we already have a standard environment variable in `JAVA_HOME` for pointing to Java. We are moving strongly in the direction of having one way to do things, and not multiple ways to do things and I don't see the benefit of multiple ways to point to Java when we already have the fairly standard way of respecting `JAVA_HOME`.
</comment><comment author="StefanScherer" created="2016-06-02T04:43:50Z" id="223194155">The `JAVA_EXE` is a temporary variable to fetch the target of the symlink. I can update the PR and unset the variable again.

Having one way to do things is ok, but in my opinion using `JAVA_HOME` is hard to maintain (one admin updates Java for security reasons and forgets about updating JAVA_HOME...). Oracle's Java MSI does not set JAVA_HOME, but provides this symbolic link.

So that's why I try to give you an updated code. At our work we have put ELK into MSI packages and our testers and later users struggle about their forgotten JAVA_HOME variable. If Elasticsearch finds the current Java installed would be much more convenient as no manual set environment is involved.
</comment><comment author="jasontedor" created="2016-06-02T07:41:56Z" id="223218706">Sorry, I realize my comment was a little confusing, but here's what I mean. Currently we have exactly one way to specify the location of Java: `JAVA_HOME`. This is consistent across all of our packaging. This introduces a second way, namely a symbolic link that gets set into `JAVA_EXE`. Except this is only available on Windows, and only if you install from an MSI which sets this symbolic link (which, I think is only possible if you have a commercial license?). This means we now sometimes (but not always) have two ways of specifying the location of Java: `JAVA_HOME` on all systems, and this symbolic link on Windows if it's available. These would be the multiple ways and it complicates rather than simplifies the situation. Then we have to add testing for it to make sure that it's maintained properly. But rather than introduce this into our scripts, I think it should just be handled _outside_ the scripts exactly like we do on all other systems. 

&gt; (one admin updates Java for security reasons and forgets about updating JAVA_HOME...)

That's exactly why changes like this should be automated. It shouldn't the case that one admin updates Java and another must remember to update `JAVA_HOME`, but rather that `JAVA_HOME` is automatically updated as part of updating Java.

In particular, can't you just point `JAVA_HOME` to this same symbolic link on your systems?
</comment><comment author="StefanScherer" created="2016-06-02T18:24:58Z" id="223379041">AFAIK there is only Oracle Java for Windows and not openjdk. So this PR exists to simplify that only Java installation way for Windows: Downloading and installing the EXE (sorry, no MSI, but still the only standard package from Oracle).

This PR doesn't change the behaviour if someone still insists to set `JAVA_HOME` on a Windows machine. It then uses the value of JAVA_HOME. But if you just install the standard java through its installer you just don't have to do this extra step.

There is no second way to do it, just do the right way simpler.

Why do I want to set JAVA_HOME when there is a standard way from the makers of Java that add a symlink for me? I just don't understand why I should do this extra step, it's just not necessary. Sorry, I'm only a parttime user of Java, and never understood that. I think that if Oracle expected that JAVA_HOME must be set, then I would expect that their installer would do that for me.

This PR just reads the target of the symlink (which directs to the java.exe and not the value needed for JAVA_HOME) and calculates the right value for JAVA_HOME automatically.
</comment><comment author="jasontedor" created="2016-06-02T19:26:38Z" id="223396104">&gt; I think that if Oracle expected that JAVA_HOME must be set, then I would expect that their installer would do that for me.

Their [docs](https://www.google.com/search?client=safari&amp;rls=en&amp;q=java_home+oracle&amp;ie=UTF-8&amp;oe=UTF-8#q=java_home+site:docs.oracle.com) even explain that it needs to be set and how to set it for some of their own applications.

A change that I would very much welcome is setting `JAVA_HOME` _if_ `java.exe` is found in the path and `JAVA_HOME` is not set. I welcome this change because it would bring the Windows script to feature-parity with the Unix-based script, it's a simple change to make and maintain, and it's very common for scripts to do this.

I think that would satisfy your need, do you agree?
</comment><comment author="gmarz" created="2016-06-02T19:38:43Z" id="223399190">Windows is a different beast than Linux, and while I agree that it can be painful/confusing for Windows users to have to set JAVA_HOME manually, I think @jasontedor is right in that we should stick to the hard rule that JAVA_HOME should be set outside of the scripts.

There are too many things that can go wrong by assuming the correct location of Java.  What if there are multiple versions installed?  JRE vs JDK? 32 vs 64-bit? Does the Oracle MSI update the symlink when these things change? Maybe it does the right thing, but I think it's best not to assume and leave it to the user.

IMO, it's a task better suited for the Elasticsearch MSI (which we're currently working on) where we can bake in more complex logic (checking the registry) and invoke user interaction if needed.
</comment><comment author="gmarz" created="2016-06-02T19:40:11Z" id="223399562">&gt; A change that I would very much welcome is setting JAVA_HOME if java.exe is found in the path and JAVA_HOME is not set.

+1
</comment><comment author="StefanScherer" created="2016-06-02T19:43:39Z" id="223400515">This is how it looks like after Java 8 installation.

&lt;img width="1176" alt="bildschirmfoto 2016-06-02 um 21 42 00" src="https://cloud.githubusercontent.com/assets/207759/15758439/e7895424-290a-11e6-942f-ae2af3bd82f8.png"&gt;

```
Microsoft Windows [Version 10.0.10586]
(c) 2015 Microsoft Corporation. All rights reserved.

C:\Users\vagrant&gt;where java
C:\ProgramData\Oracle\Java\javapath\java.exe

C:\Users\vagrant&gt;set java
Environment variable java not defined

C:\Users\vagrant&gt;java -version
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b15, mixed mode)

C:\Users\vagrant&gt;dir C:\ProgramData\Oracle\Java\javapath
 Volume in drive C is Windows 10
 Volume Serial Number is 9C9F-63C7

 Directory of C:\ProgramData\Oracle\Java\javapath

06/02/2016  09:39 PM    &lt;DIR&gt;          .
06/02/2016  09:39 PM    &lt;DIR&gt;          ..
06/02/2016  09:39 PM    &lt;SYMLINK&gt;      java.exe [C:\Program Files\Java\jre1.8.0_91\bin\java.exe]
06/02/2016  09:39 PM    &lt;SYMLINK&gt;      javaw.exe [C:\Program Files\Java\jre1.8.0_91\bin\javaw.exe]
06/02/2016  09:39 PM    &lt;SYMLINK&gt;      javaws.exe [C:\Program Files\Java\jre1.8.0_91\bin\javaws.exe]
               3 File(s)              0 bytes
               2 Dir(s)  49,184,186,368 bytes free

C:\Users\vagrant&gt;
```

So `java.exe` is in PATH, but you have to read the symlink to know where JAVA_HOME should be.
That's why this PR was born.
</comment><comment author="StefanScherer" created="2016-06-02T19:51:55Z" id="223402566">@gmarz Cool that your are working on a MSI, so I can drop ours soon :-) For Kibana and Logstash as well?

That you can install the wrong CPU type and different versions of Java on one machine is the heritage of the lack of a proper package management in the first place. But yes this it is how we have to work with it now. That way may go back to Sun...

Just found out that the Server JRE is only deployed as a tar.gz and Oracle does not give any hint how to extract this on a Windows machine: http://docs.oracle.com/javase/8/docs/technotes/guides/install/windows_server_jre.html#CFHGHHFJ
So yes your are on your own and then you are willing to set JAVA_HOME as well, I think.

My focus for this PR was for the EXE installer that I normally use and that would make it easy to make Elasticsearch run out-of-the-box.
</comment><comment author="StefanScherer" created="2016-06-02T20:00:58Z" id="223404779">One other thing. As you can see I'm testing this in Vagrant boxes. I just found the `Vagrantfile` in your repo with lots of platforms. Great! Does it make sense to add one or more Windows platforms as well?
Don't know which tests are running in the Vagrant boxes.
</comment><comment author="jasontedor" created="2016-06-02T20:02:46Z" id="223405244">&gt; Does it make sense to add one or more Windows platforms as well?

We are working on that, it is badly needed exactly so we can feel confident when making packaging changes. See #18475.

&gt; Don't know which tests are running in the Vagrant boxes.

We basically run a full suite of packaging tests. Does Elasticsearch start? Can it install a plugin? Does it work if there is a path with a space in the name? Does it work with a custom config? All sorts of fun things like that.
</comment><comment author="jasontedor" created="2016-06-02T20:09:29Z" id="223406923">&gt; So `java.exe` is in PATH, but you have to read the symlink to know where JAVA_HOME should be.
&gt; That's why this PR was born.

No, because we can just do something very simple like this:

``` bat
if defined JAVA_HOME goto cont

set JAVA=java.exe
%JAVA% -version &gt;NUL 2&gt;&amp;1
if "%ERRORLEVEL%" == "0" goto start

@rem print some error message here
goto fail

:cont
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA=%JAVA_HOME%/bin/java.exe

if exist %JAVA% goto start

@rem print some error message here
goto fail

:start
```

and then replace the launch command from `%JAVA_HOME%\bin\java%` to `%JAVA%`.

This is completely off the cuff, completely untested, but gives the idea.
</comment><comment author="StefanScherer" created="2016-06-02T20:24:29Z" id="223410914">Ah, so the JAVA_HOME environment is only needed for the path to `java.exe`? I didn't know the details and thought that  a lib/etc. also needs this to find files in the installation tree.
</comment><comment author="jasontedor" created="2016-06-02T20:26:09Z" id="223411387">&gt; Ah, so the JAVA_HOME environment is only needed for the path to `java.exe`?

Exactly. And that's why I'm comfortable saying, hey, if `java.exe` is already on the path we can just use that.
</comment><comment author="StefanScherer" created="2016-06-03T13:43:12Z" id="223582076">OK, I'll update the PR. But I prefer to use `where java.exe` instead of calling `java.exe -version` to avoid running the Java binary only to check if it's in the PATH. The Linux version looks good to me:

``` bash
if [ -x "$JAVA_HOME/bin/java" ]; then
    JAVA="$JAVA_HOME/bin/java"
else
    JAVA=`which java`
fi

if [ ! -x "$JAVA" ]; then
    echo "Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME"
    exit 1
fi
```

I will keep the Windows version in about that order.
</comment><comment author="jasontedor" created="2016-06-03T13:52:03Z" id="223584209">&gt; But I prefer to use `where java.exe` instead of calling `java.exe -version` to avoid running the Java binary only to check if it's in the PATH.

Can you double-check if `where` is available on all of the Windows platforms that we support _and_ that might be in use by developers? That's the reason I didn't use it, I know it's a new addition to Windows but I didn't look up the exact history. As long as it's available in the places that matter, it's fine to use.
</comment><comment author="StefanScherer" created="2016-06-03T13:59:00Z" id="223585978">I've worked with `where.exe` since XP. Found this at stack overflow http://stackoverflow.com/questions/304319/is-there-an-equivalent-of-which-on-the-windows-command-line

The old variant

```
C:\Users\vagrant&gt;for %i in (java.exe) do set JAVA=%~$PATH:i

C:\Users\vagrant&gt;set JAVA=C:\ProgramData\Oracle\Java\javapath\java.exe
```

works still with Windows 10. I probably use this.
</comment><comment author="jasontedor" created="2016-06-03T14:07:13Z" id="223588186">&gt; I probably use this.

I prefer this option too.
</comment><comment author="StefanScherer" created="2016-06-03T15:48:16Z" id="223616202">I've come up with this batch solution

``` cmd
IF DEFINED JAVA_HOME (
  set JAVA=%JAVA_HOME%\bin\java.exe
) ELSE (
  FOR %%I IN (java.exe) DO set JAVA=%%~$PATH:I
)
IF EXIST "%JAVA%" GOTO cont

:err
ECHO Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME 1&gt;&amp;2
EXIT /B 1

:cont
```

which looks pretty similar to the bash solution.

I've commited that for the elasticsearch*.bat scripts. Still working on `service.bat` as this needs some more care as it uses `JAVA_HOME` to specify the jvm dll...

But you can have a look at the `elasticsearch*.bat` scripts.
</comment><comment author="StefanScherer" created="2016-06-03T16:56:21Z" id="223633567">I've updated `service.bat` as well. But this script has to calculate `JAVA_HOME`
- Tested with `java.exe` in PATH:
  1. `Path=C:\ProgramData\Oracle\Java\javapath;C:\Windows\system32;C:\Windows` with the symlink to `C:\Program Files\Java\jre1.8.0_91\bin\java.exe` -&gt; sets JAVA + JAVA_HOME correctly
  2. `Path=C:\Program Files\Java\jre1.8.0_91\bin;C:\Windows\system32;C:\Windows` with the exe in `bin` folder -&gt; sets JAVA + JAVA_HOME correctly
- Tested with `JAVA_HOME` set regardless if java.exe is in PATH:
  1. `JAVA_HOME=C:\Program Files\Java\jre1.8.0_91` -&gt; sets JAVA correctly

PTAL
</comment><comment author="jasontedor" created="2016-06-03T18:15:04Z" id="223653129">Thanks @StefanScherer, it looks great. Can you update the title of the PR?
</comment><comment author="StefanScherer" created="2016-06-03T18:23:07Z" id="223655249">@jasontedor better? 
</comment><comment author="jasontedor" created="2016-06-03T18:24:24Z" id="223655563">@StefanScherer Thanks! I will merge soon.
</comment><comment author="jasontedor" created="2016-06-03T19:36:33Z" id="223673935">Thanks for working through this one @StefanScherer, your contribution is greatly appreciated.
</comment><comment author="StefanScherer" created="2016-06-03T19:41:57Z" id="223675233">Thanks for merging @jasontedor. That's what open source and PR's are for. To review and to understand the intention behind the code both of the maintainer's view and the contributor's view. I've learned a lot :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test rethrottle test case for delete-by-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18684</link><project id="" key="" /><description>and remove some type parameters that we don't need that were getting
in the way.
</description><key id="157958419">18684</key><summary>Add test rethrottle test case for delete-by-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-01T17:04:03Z</created><updated>2016-06-02T19:04:52Z</updated><resolved>2016-06-02T19:04:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-01T17:04:18Z" id="223058983">@tlrx, can you review this one?
</comment><comment author="tlrx" created="2016-06-02T06:40:36Z" id="223208365">LGTM, thanks @nik9000 
</comment><comment author="nik9000" created="2016-06-02T19:04:51Z" id="223390426">Thanks for reviewing @tlrx !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Display plugins versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18683</link><project id="" key="" /><description>This is useful to determine if a plugin needs to be updated when using deployment automation solution (like Ansible).

A similar feature is now available in Kibana: https://github.com/elastic/kibana/pull/7221
</description><key id="157934128">18683</key><summary>Display plugins versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mogztter</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v5.0.0-beta1</label></labels><created>2016-06-01T15:18:28Z</created><updated>2016-10-16T19:05:48Z</updated><resolved>2016-09-12T22:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-06-01T15:23:32Z" id="223028180">The verbose option was extended to display all plugin properties in #18051.
</comment><comment author="Mogztter" created="2016-06-01T17:09:21Z" id="223060392">Yes I saw but the output is on multiples lines (ie. harder to parse with an automation tool).
The format name@version is quite common and the behavior will be the same across Kibana and Elasticsearch (and soon Logstash ?)

The verbose output is still useful for human.
</comment><comment author="rjernst" created="2016-06-01T17:44:15Z" id="223070295">Human consumption is all you can really do. Knowing the plugin version doesn't tell you if it needs updating. Any time you update elasticsearch, plugins need to be updated, because they currently must be built against each specific version of elasticsearch.

I think the non verbose output should stay as simply listing the plugin names installed, eg like `ls` just lists file names. But I have marked this for discussion.
</comment><comment author="jasontedor" created="2016-06-01T17:59:42Z" id="223074794">&gt; I think the non verbose output should stay as simply listing the plugin names installed, eg like ls just lists file names.

+1
</comment><comment author="Mogztter" created="2016-06-01T18:50:30Z" id="223089120">&gt; Human consumption is all you can really do. Knowing the plugin version doesn't tell you if it needs updating. Any time you update elasticsearch, plugins need to be updated, because they currently must be built against each specific version of elasticsearch.

But you can still have a new version of a plugin without a new version of Elasticsearch ?

&gt; I think the non verbose output should stay as simply listing the plugin names installed, eg like ls just lists file names. But I have marked this for discussion.

"Make everything as simple as possible, but not simpler".
If I want to know the content of a directory, I will use `ls` not a Java program :wink: 

I think the plugin version is an important information and `ListPluginsCommand` already knows this information, why not make it available and easy to read by an automation tool ? What are the drawbacks of adding this piece of information ?

Here's some benefits:
- Check that a plugin need to be updated
- Check that the plugin version is compatible with Elasticsearch version
- Do not remove/install a plugin if the version did not change

Right now, in Ansible the basic idea is remove everything, install everything: https://github.com/elastic/ansible-elasticsearch/blob/master/tasks/elasticsearch-plugins.yml#L20 :unamused: 
</comment><comment author="rjernst" created="2016-06-01T18:55:33Z" id="223090532">&gt; Right now, in Ansible the basic idea is remove everything, install everything

That is what must happen for all official elasticsearch plugins. You are correct that community plugins may release multiple versions per elasticsearch version. But knowing what version is currently installed of a community plugin does not tell you anything about whether there is another version, and upgrading elasticsearch means another version of that community plugin _must_ be installed.
</comment><comment author="Mogztter" created="2016-06-01T19:38:16Z" id="223101585">&gt; But knowing what version is currently installed of a community plugin does not tell you anything about whether there is another version

Ansible will know because I will tell him to install a (newer) version for this plugin. Being able to determine if I need to remove a community plugin in order to install a newer version will be really useful.
IMHO this is just tiny addition that can be useful to users and will certainly make my life easier :smile: 
</comment><comment author="dadoonet" created="2016-06-24T09:27:12Z" id="228300318">We discussed it on fix it friday and found that it could be a nice feature to have. We don't see any drawback by adding this.
It's also consistent with the rest of the stack (kibana as explained in the PR) and if it makes our users life easier, let's do it.

I'm changing the label to "review".

@rjernst As you were not available for this Fix It friday session, feel free to comment and raise any objection if any and change back the label to "discuss".
</comment><comment author="Mogztter" created="2016-06-24T14:47:08Z" id="228366002">Thanks @dadoonet !
</comment><comment author="Mogztter" created="2016-08-19T09:40:46Z" id="240974530">@dadoonet Any news ?
</comment><comment author="dakrone" created="2016-09-12T21:35:43Z" id="246501972">This LGTM, I will merge this
</comment><comment author="dakrone" created="2016-09-12T22:26:51Z" id="246515222">@Mogztter merged this, thanks!
</comment><comment author="dakrone" created="2016-09-12T22:52:22Z" id="246520989">Backported to 5.x and 5.0
</comment><comment author="dadoonet" created="2016-09-13T07:07:16Z" id="246594128">Thanks @dakrone! I was planning to merge it today! \o/
</comment><comment author="Mogztter" created="2016-09-13T07:16:31Z" id="246595929">Thanks \o/
Looking forward to the next 5.0 release ;)
</comment><comment author="jasontedor" created="2016-10-11T16:00:34Z" id="252961757">This has been reverted in #20807.
</comment><comment author="Mogztter" created="2016-10-11T16:51:09Z" id="252975829">Wait what... but why ? :sob: 
</comment><comment author="jasontedor" created="2016-10-11T16:57:52Z" id="252977764">For the initial reasons given against this PR and additionally as elaborated in #20668.
</comment><comment author="Mogztter" created="2016-10-11T17:02:27Z" id="252979041">Just read #20668

&gt;  I was against that change because it's conflating a human-readable API with a machine-readable API.

Fair enough but where can I find a machine-readable API ?
As a workaround, you can use `localhost:9200/_cat/plugins` API but the server must be running.
I think I will just create a tiny script to read the properties file.
</comment><comment author="jasontedor" created="2016-10-13T15:19:33Z" id="253545140">&gt; Fair enough but where can I find a machine-readable API ?

There isn't one, but that does not mean we should mix the two.

&gt; I think I will just create a tiny script to read the properties file.

I'm wondering why you don't unconditionally uninstall and then install the right version?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create get task API that falls back to the .tasks index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18682</link><project id="" key="" /><description>This was edited from its original form. The original is below for posterity:
## Rewrite

This adds a get task API that supports `GET /_tasks/${taskId}` and removes that responsibility from the list tasks API. The get task API supports wait_for_complation just as the list tasks API does but doesn't support any of the list task API's filters. In exchange, it supports falling back to the `.results` index when the task isn't running any more. Like any good GET API it 404s when it doesn't find the task.

Then we change reindex, update-by-query, and delete-by-query to persist the task result when `wait_for_completion=false`. The leads to the neat behavior that, once you start a reindex with `wait_for_completion=false`, you can fetch the result of the task by using the get task API and see the result when it has finished.
## Original

This changes the task list API so that, for requests that look like `GET /_tasks/${taskId}`, it'll first look at the running tasks on the node and, if the task isn't found, perform a GetRequest on `.results/result/${taskId}`.

Then we change reindex, update-by-query, and delete-by-query to persist the task result when `wait_for_completion=false`.

The leads to the neat behavior that, once you start a reindex with `wait_for_completion=false`, you can fetch the result of the task by a task list request and you'll get the result, even if the task has finished.
</description><key id="157928991">18682</key><summary>Create get task API that falls back to the .tasks index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-01T14:58:17Z</created><updated>2016-06-14T17:38:55Z</updated><resolved>2016-06-14T17:38:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-01T15:47:28Z" id="223036058">@imotov this is probably for you.
</comment><comment author="imotov" created="2016-06-01T21:36:58Z" id="223132559">@nik9000 left a few comments.
</comment><comment author="nik9000" created="2016-06-06T13:34:35Z" id="223960102">&gt; @nik9000 left a few comments.

Sorry this is taking so long - mostly through the comment and I'll push something soon.
</comment><comment author="nik9000" created="2016-06-07T23:13:50Z" id="224442286">@imotov I rebased and started on the get task API. Like the list tasks API it can wait for completion, but unlike the list tasks API it also falls back to the results index if the task isn't found. I didn't make that a disable-able thing.
</comment><comment author="nik9000" created="2016-06-08T15:02:20Z" id="224617871">Note to self: make sure that we have a sufficient test case for the get task API's json output. This probably means expanding one reindex's tests because it is the only thing where we can be sure that the response came back without the test plugin.
</comment><comment author="nik9000" created="2016-06-08T20:34:05Z" id="224718672">@imotov this is ready for another round I think. I have to go and fix up reindex's rest tests but it the stuff in core is ready.
</comment><comment author="imotov" created="2016-06-09T19:40:33Z" id="225004019">I think it's almost there. I think we just need to finish separation between list tasks and get task by rebasing GetTask on NodeAction instead of TasksAction or by adding a specialized TaskAction and then we should be all set. 
</comment><comment author="nik9000" created="2016-06-10T13:38:51Z" id="225184192">@imotov I switched TransportGetTaskAction to extending HandledTransportAction and that cleaned the code up quite a bit! No more null support in GetTaskResponse, no more hijacking, etc.

I'm not experienced with actually building these sorts of transport actions, so please have a look and let me know if it is funky.

I didn't want to extend NodesTransportAction because that wants to work with multiple nodes and wants my request to extends BaseNodesRequest, neither of which make much sense. Instead I just, sort of, bounce execution of the action over to the correct node.
</comment><comment author="clintongormley" created="2016-06-13T18:22:39Z" id="225665794">Sorry for the late and somewhat off topic comment, but I think we should call the index `.task_status` or `.tasks` rather than `.results`, which is just too generic.
</comment><comment author="imotov" created="2016-06-13T18:34:14Z" id="225668983">`.task_results`?
</comment><comment author="nik9000" created="2016-06-13T18:35:28Z" id="225669316">&gt; .task_results?

What if we want to snapshot running tasks into it? I kind of think we will soon. I prefer `.tasks` if so.
</comment><comment author="imotov" created="2016-06-13T18:45:33Z" id="225672017">Yeah, `.tasks` sounds good to me.
</comment><comment author="nik9000" created="2016-06-13T18:48:12Z" id="225672709">&gt; Yeah, .tasks sounds good to me.

Should I make that change as part of this or should we do it after?
</comment><comment author="imotov" created="2016-06-13T18:53:56Z" id="225674205">Sure, why not?
</comment><comment author="nik9000" created="2016-06-13T18:54:49Z" id="225674417">&gt; Sure, why not?

Cool. It is time for me to switch branches anyway, so I'll do it now.
</comment><comment author="nik9000" created="2016-06-13T19:53:54Z" id="225689331">@imotov renamed the index, the type, and a few of the things who's names no longer matched. Have a look when you are ready. I tried to make the commit messages useful.
</comment><comment author="imotov" created="2016-06-14T13:56:10Z" id="225888462">Simpler and cleaner now. Left a couple of really minor comments. LGTM.
</comment><comment author="nik9000" created="2016-06-14T16:36:56Z" id="225940210">@imotov I squashed and rebased this one to get it ready to merge and found an issue. The wait_for_completion code in the get request was sometimes running in the _requesting_ thread. So it'd make the tests timeout and fail. Sometimes. I've moved it to the generic thread pool and the test is stable again.
</comment><comment author="imotov" created="2016-06-14T17:32:14Z" id="225955969">LGTM
</comment><comment author="nik9000" created="2016-06-14T17:38:55Z" id="225957935">Thanks for all the reviews and design help @imotov !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Fix tests that rely on assumption that data dirs are removed after index deletion acknowledged</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18681</link><project id="" key="" /><description>Relates to #18602
</description><key id="157924798">18681</key><summary>[TEST] Fix tests that rely on assumption that data dirs are removed after index deletion acknowledged</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>bug</label><label>test</label></labels><created>2016-06-01T14:42:48Z</created><updated>2016-06-01T15:02:21Z</updated><resolved>2016-06-01T15:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-01T14:51:09Z" id="223017353">I left one suggestion/question . Feel free to push if it doesn't makes sense.
</comment><comment author="ywelsch" created="2016-06-01T15:02:20Z" id="223021174">Thanks for reviewing @bleskes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cached query/filter performance degradation from 1.7.3 to 2.3.2 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18680</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**:  1.8.0_91&#8211;b14

**OS version**: 3.13.0-62-generic #102-Ubuntu

I have a set of 300 queries that are submitted concurrently.  When tested on a 1.7.3 cluster the queries complete in less than 2 seconds where as on a 2.3.2 cluster with the same hardware they take 60+ seconds to complete. 

Here are the hot threads captured during the queries on the 2.3.2 cluster:

Using the default store:
https://gist.github.com/jfenc91/6c8aec1dcca232be533743528aa544a2

Using the mmap store:  (I have enough ram for everything to be in memory)
https://gist.github.com/jfenc91/85d30a17a5684a2a2d7b0e9e8924ba0f

Before the filter cache is checked, there is apparently a call to query.createWeight that can get pretty expensive: https://github.com/apache/lucene-solr/blob/releases/lucene-solr/5.5.0/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java#L904-L907

Hiding the complexity of queries was part of the awesomeness of cached filters/queries.  Its seems like with the changes, that some of the evaluation of the query, where terms are compared against the document index, are happening, even though the query results are cached, because of that call to createWeight: https://github.com/apache/lucene-solr/blob/releases/lucene-solr/5.5.0/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java#L390-L395 

I feel like extremely complex queries that changed occasionally were one of the best use cases for caching, but it doesn&#8217;t really seem to work with the cache rework that came with ES 2.x. If there were intended to be alternative ways to use this I would love to know. Thanks for reading!  
</description><key id="157885177">18680</key><summary>Cached query/filter performance degradation from 1.7.3 to 2.3.2 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfenc91</reporter><labels><label>:Cache</label><label>:Search</label><label>feedback_needed</label></labels><created>2016-06-01T11:35:39Z</created><updated>2017-03-31T09:56:25Z</updated><resolved>2017-03-31T09:56:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-01T13:04:49Z" id="222985753">Can you share the queries you're running ? The createWeight you are mentioning is supposed to be very fast but it seems that you have phrase queries with a lot of terms. What is the average number of terms in your phrase query ?
</comment><comment author="clintongormley" created="2016-06-01T17:53:56Z" id="223073127">And is this a real-world test case?  Caching is based on query expense and reuse.  Does the rest reflect that?
</comment><comment author="jfenc91" created="2016-06-01T21:51:05Z" id="223135876">Thanks for taking a look @jimferenczi @clintongormley !

This is a real world use case. The traffic against this cluster was replicated from a production setting. 

I have confirmed that caching is being used to some extent through checking the cluster metrics.  Additionally, I had had to debug an issue where I set the indices.queries.cache.count too high and caused slow cache lookups. Currently I have that set at 500,000 which seems to not cause any issues. If this was not using the cache I would expect to be seeing many more stack traces regarding query evaluation (which I did see on the first few times the query was executed on the cluster). 

Unfortunately the exact query is not mine to share, but here is an anonymized sample version.  Admittedly it is still structured for 1.7.3 caching. However, when I was profiling with hotthreads I did account for the reality that it takes a certain amount of reuse of the query before it ends up in the LRUCache. 

https://gist.github.com/jfenc91/240616ff625bf4d60957c42a6f7fc0f2

On total the query has somewhere between 1-1.5k terms. The phrase queries that I have are ranging from 1 &#8211; 12 tokens and are typically no more than 5. The parts of this query that are indicated for caching get reused a few hundered times a day and tend to change a few times a week to a few times a month. 

Also some other stats. 
number of Indexes: 7
shards per index: 5
Total segments at time of test: ~350 
Available cores on data nodes: 160

Cheers! 
</comment><comment author="jpountz" created="2016-06-01T22:04:12Z" id="223138998">I think you are right we should try to avoid seeking the terms dict when we are going to use a cached entry anyway.
</comment><comment author="jpountz" created="2016-06-02T08:44:05Z" id="223231370">I opened https://issues.apache.org/jira/browse/LUCENE-7311 for discussion.
</comment><comment author="jfenc91" created="2016-06-02T19:00:43Z" id="223389326">Thanks for jumping in here @jpountz! 

So I did a quick hack to make the call to createWeight more lazy:
https://github.com/elastic/elasticsearch/compare/b9e4a6acad4008027e4038f6abed7f7dba346f94...jfenc91:feature_queryCacheHack
https://github.com/jfenc91/lucene-solr/compare/release_5.5.0...jfenc91:feature_queryCacheHack

Unfortunately, even with that change, the caching is not competitive to the 1.7.3 version. Here is what I am now getting out of hotthreads:
https://gist.github.com/jfenc91/88a3179cf3c3136964c1a8167fbf8b73

So I am left with a few questions/thoughts

1)  I am seeing a bunch of LRUQueryCache.java:643 which in my fork happens when shouldCache(context)= false. So for some reason some parts of the query are not being cached. Since I am fairly certain I am hitting the lru requirement I am less sure about this line: ReaderUtil.getTopLevelContext(context).reader().maxDoc(). Is that basically the number of docs that the query is filtering against? Or the resulting number of docs we would get if the query were &#8220;*&#8221; instead of some other term? 

2)  In 1.7.3 I specified what needed to be cached. So my 1.5k terms in a query phrase resulted in one cached result. Here it seems like lucene is decomposing the query and building a cache for individual portions of it? Should the top level Boolean query be cached as well? If so, is the idea that the other portions of the query will eventual get lru-ed out of the cache and we will just be left with the bitset for the top level Boolean query? 

Here are some of my cache stats:
&#8226; _size_in_bytes: 18428944,
&#8226; total_count: 5121219796,
&#8226; hit_count: 48890920,
&#8226; miss_count: 5072328876,
&#8226; cache_size: 10147,
&#8226; cache_count: 232641,
&#8226; evictions: 22249
</comment><comment author="jfenc91" created="2016-06-05T04:20:26Z" id="223792317">After looking into this more, my problem is an ineffective query caching policy. 

Things get a bit better with the following:
1) An XUsageTrackingQueryCachingPolicy with a configurable history size. It seems that boolean queries that are composed of more than 256 sub-queries simply can't be cached right now because the length of the history:

https://github.com/elastic/elasticsearch/blob/v2.3.2/core/src/main/java/org/apache/lucene/search/XUsageTrackingQueryCachingPolicy.java#L83

2) Removing the guard against caching document sets that are very small. My complex queries tend to match very few results in the index and it looks like there is (smaller than .03% of the segment/shard/index I'm not sure... But regardless I am failing that check):

https://github.com/apache/lucene-solr/blob/releases/lucene-solr/5.5.0/lucene/core/src/java/org/apache/lucene/search/QueryCachingPolicy.java#L95

3) Removing the guard against caching in large segments/shards (not sure which). This seems to be another one of those guards that really prevent caching from working at moments where it is needed most. Especially with roaring bitmaps, shards tons of documents combined with queries that have very few hits are really where caching is at it's best. Admittedly I understand this check the least. So, maybe I am missing something and this wasn&#8217;t really necessary to remove.

https://github.com/apache/lucene-solr/blob/releases/lucene-solr/5.5.0/lucene/core/src/java/org/apache/lucene/search/LRUQueryCache.java#L613

Even after addressing these three issues, I found my cache was being swamped by simpler queries and the stuff that was really important to cache wasn't really surviving evictions. I could implement and propose changes to the existing query cache policy to better fit my use case, but at the end of the day, it is really hard to make sure those changes won't end up hurting some other use case. I think this kind of highlights the need for pluggable query cache policies. 

What are your thoughts @clintongormley @jpountz?

More Notes:
After eliminating some locking in the LRU cache (I changed it to a ReentrantReadWriteLock) and replacing the logic of shouldCache() with something that better fit my use case, my 3 second latency spikes with real traffic on 1.7 are now comparable to 14 second latency spikes with 2.3. The query equality checks are possibly slowing things down compared to the previous cache keys. But anyway, It would be really awesome if there was a path to make future versions of Elasticsearch performant for my use case.   
</comment><comment author="breadfan" created="2016-06-21T08:57:52Z" id="227380935">Very interesting, we also have at least doubled our latency by using ES 2.3.3 and we are using simpler filters than in your query. I hope there will be some progress.
</comment><comment author="ben-manes" created="2016-07-17T03:54:45Z" id="233163717">#16802 might be helpful here.
</comment><comment author="breadfan" created="2016-07-18T08:11:16Z" id="233263294">Thanks for the hint! Looks promising!
</comment><comment author="amazoyer" created="2017-02-03T13:06:31Z" id="277242432">I have the same concern with the caching policy. It would be great if caching policy was pluggable.
@jfenc91 : did you find a work around for your problem?</comment><comment author="jfenc91" created="2017-02-03T18:08:53Z" id="277319330">@amazoyer I'm still on 1.7.3. The best option I have come across is indexing (and then reindexing + updating) the query result with a query id. Although ES is ridiculously fast and doesn't need query caching for the typical use case.</comment><comment author="kainosnoema" created="2017-03-15T05:48:00Z" id="286647225">We were also just bitten by the caching changes in 2.x+ on a production workload. Currently still trying to find a solution, but it's looking like ES just can't do what it used to here. Queries that include about a half-dozen cached terms filters with up to hundreds of terms each went from ~80ms to 8-9 seconds&#8212;no longer able to be run in real-time.

Are there any plans to bring some level of caching control back?</comment><comment author="setaou" created="2017-03-15T08:36:02Z" id="286674495">We did hit what seemed to be the same bug in the end of 2016. Having found no solution, we tried to fork ES+Lucene to bring back the _cache query parameter in order to better control the caching (see https://github.com/geneanet/elasticsearch/commits/geneanet-cachetuning and https://github.com/geneanet/lucene-solr/commits/geneanet-cachetuning for reference). It did not improve the situation, so either our patch was not really effective or the problem was more complex than just adjusting what should be cached or not.

Currently, our only solution is to keep using ES 1.7, as ES 5 has for our use cases a slightly lower performance and a much greater resource usage (about 3 x more CPU usage).</comment><comment author="jpountz" created="2017-03-15T20:51:33Z" id="286875074">&gt; Are there any plans to bring some level of caching control back?

We made and will keep making changes that improve caching, however we have no plans to bring back cache control options.

&gt; It did not improve the situation, so either our patch was not really effective or the problem was more complex than just adjusting what should be cached or not.

This is very likely indeed. ES 1.7 was relying a lot on caching into bitsets in order to be able to run conjunctions using bitwise operations, but in a way that did not fit well into how Lucene performs query planning. In ES 2.0 we changed cache entries to have a sparse representation and conjunctions to run more consistently, in a way that is easier to reason about but does not use bitwise operations. As a consequence, some queries might indeed perform slower (but other queries also perform faster) and just changing the query caching heuristics is not enough to restore the 1.7 behaviour.</comment><comment author="jfenc91" created="2017-03-16T02:10:03Z" id="286936972">I got 75% of the way to the old performance when I ensured that the cache was used optimally as if I was explicitly caching. The remaining bit may be obtainable by optimizing out the query comparison (since that is very expensive for the use case where caching matters) and instead only comparing provided cache keys. 

Honestly, I believe the situation where ES is now targeting for caching is where things didn't need to be cached in the first place. I would love to see ES go back to the direction of at least offering explicate caching (because automatic caching is really hard to get right and it probably is not possible to get to an effective automatic caching solution by only leveraging the lucene layer). </comment><comment author="clintongormley" created="2017-03-21T20:22:41Z" id="288206389">Keep upgrading - things have improved greatly in 5.x, and we have more optimizations which will be released shortly.</comment><comment author="kainosnoema" created="2017-03-21T20:28:01Z" id="288207943">@clintongormley thanks, that's actually our current strategy. We're re-writing all our queries to be 5.x compatible and will be upgrading later this week or early next. I'll update here with the results.</comment><comment author="jpountz" created="2017-03-31T09:56:24Z" id="290671025">There have been a number of changes that should make things better in 5.3/5.4 so I'm going to close that issue. I'm not saying it will now perform on par with 1.7, some queries are still going to be slower and others faster. When facing slow queries, it is also important to look a whether documents are modeled in a good way: the best speedups are obtained by modeling documents in a way that makes queries simpler. Here is a subset of changes that have helped filter caching over the last months:
 - #21566 term queries are not cached anymore, leaving space in the history for more complex queries
 - #23079 nested queries and mappings add fewer implicit filters
 - https://issues.apache.org/jira/browse/LUCENE-7677 Compound filters are cached earlier than leaf filters.
 - https://issues.apache.org/jira/browse/LUCENE-7235 Do not take the lock on small segments.
 - https://issues.apache.org/jira/browse/LUCENE-7237 Skip using the cache under contention.
 - #22316 Improved concurrency of ShardCoreKeyMap.

In addition to that, there have been efforts to make queries faster when they are not cached, in particular `range` (https://issues.apache.org/jira/browse/LUCENE-7643) and `nested` (https://issues.apache.org/jira/browse/LUCENE-7654).</comment></comments><attachments /><subtasks /><customfields /></item><item><title>azure-keyvault doesnt work behind proxy version 0.9.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18679</link><project id="" key="" /><description>I am trying to use azure key vault java client to encrypt &amp; decrypt the data. From my office network which have proxy to connect to internet the calls to azure key vault erros with connection timeout.

I have tested from home network which doenst have any proxy works fine.

Also the .net version of keyvault client works fine behind proxy.

Thanks
Amar.
</description><key id="157879280">18679</key><summary>azure-keyvault doesnt work behind proxy version 0.9.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amarpandharpure</reporter><labels /><created>2016-06-01T11:00:15Z</created><updated>2016-06-01T11:35:06Z</updated><resolved>2016-06-01T11:35:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Register "cloud.node.auto_attributes" setting in EC2 discovery plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18678</link><project id="" key="" /><description>I'm unable to set "cloud.node.auto_attributes" setting. I get this

```
java.lang.IllegalArgumentException: unknown setting [cloud.node.auto_attributes]
```
</description><key id="157872719">18678</key><summary>Register "cloud.node.auto_attributes" setting in EC2 discovery plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>:Settings</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-01T10:23:00Z</created><updated>2017-05-09T08:16:02Z</updated><resolved>2016-06-01T11:27:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-01T10:39:08Z" id="222956069">great find! I will pull this soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename PipelineAggregatorBuilder to PipelineAggregationBuilder.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18677</link><project id="" key="" /><description>This is a follow-up to #18377.
</description><key id="157854570">18677</key><summary>Rename PipelineAggregatorBuilder to PipelineAggregationBuilder.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-01T08:51:31Z</created><updated>2016-06-17T12:36:20Z</updated><resolved>2016-06-17T12:36:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-02T06:42:38Z" id="223208656">@colings86 Do you agree with this change?
</comment><comment author="colings86" created="2016-06-02T09:28:21Z" id="223241294">Yes. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw IllegalStateException when handshake fails due to version or cluster mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18676</link><project id="" key="" /><description>We do throw ConnectTransportException which is logged in trace level hiding a potentially
important information  when an old or wrong node wants to connect. We should throw ISE and
log as warn.
</description><key id="157846710">18676</key><summary>Throw IllegalStateException when handshake fails due to version or cluster mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Logging</label><label>:Network</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-01T08:06:44Z</created><updated>2016-06-01T08:28:41Z</updated><resolved>2016-06-01T08:28:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-01T08:12:01Z" id="222922951">LGTM. I think we should go further and change https://github.com/elastic/elasticsearch/pull/18676/files#diff-029ce303f3503d9f33f0974c76f69918R356 to also throw a hard exception - we already succesfully connected to that node.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>discrepancies between missing aggregation and terms aggregation with missing parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18675</link><project id="" key="" /><description>**Elasticsearch version**: 2.1.1

**JVM version**: 1.8.0_25

**OS version**: Mac OS X EI Capitan

**Steps to reproduce**:

_Missing aggregation_

```
GET /.../_search?size=0
{
    "aggs" : {
        "wo_exp_level": {"missing": {"field": "experience_level"}},
        "exp_level" : { 
            "terms" : { "field" : "experience_level"}}
    }
}
```

returns 

```
 "exp_level": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "Mid-Level",
               "doc_count": 1466
            },
            {
               "key": "Senior",
               "doc_count": 1164
            },
            {
               "key": "Lead",
               "doc_count": 854
            },
            {
               "key": "Junior",
               "doc_count": 824
            }
         ]
      },
      "wo_exp_level": {
         "doc_count": 65692
    }
```

_Terms aggregation with missing parameter_

```
GET /.../_search?size=0
{
    "aggs" : {
        "exp_level" : { 
            "terms" : { "field" : "experience_level", "missing": "notset"}}
    }
}
```

gives 

```
"exp_level": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "notset",
               "doc_count": 65574
            },
            {
               "key": "Mid-Level",
               "doc_count": 1463
            },
            {
               "key": "Senior",
               "doc_count": 1274
            },
            {
               "key": "Lead",
               "doc_count": 863
            },
            {
               "key": "Junior",
               "doc_count": 826
            }
         ]
      }
```

I expect the value in the bucket `notset` (65574) to be same as `doc_count` in the missing aggregation (65692), which is `wo_exp_level`. But they're not. Also the counts in other buckets also change with and without the `missing` parameter.
</description><key id="157828269">18675</key><summary>discrepancies between missing aggregation and terms aggregation with missing parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asldevi</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2016-06-01T05:47:20Z</created><updated>2016-06-02T05:32:41Z</updated><resolved>2016-06-02T05:32:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-01T14:28:20Z" id="223009893">I am wondering that this might have been addressed by #15746. Could you check whether you can reproduce the bug with a more recent version of elasticsearch?
</comment><comment author="asldevi" created="2016-06-02T05:32:41Z" id="223199386">I checked it with `elastic 2.3.3` and that works perfect. Thank you so much.
Team elastic is simply awesome &#128175; 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Register thread pool settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18674</link><project id="" key="" /><description>This commit refactors the handling of thread pool settings so that the
individual settings can be registered rather than registering the top
level group. With this refactoring, individual plugins must now register
their own settings for custom thread pools that they need, but a
dedicated API is provided for this in the thread pool module. This
commit also renames the prefix on the thread pool settings from
"threadpool" to "thread_pool". This enables a hard break on the settings
so that:
- some of the settings can be given more sensible names (e.g., the max
  number of threads in a scaling thread pool is now named "max" instead
  of "size")
- change the soft limit on the number of threads in the bulk and
  indexing thread pools to a hard limit
- the settings names for custom plugins for thread pools can be
  prefixed (e.g., "xpack.watcher.thread_pool.size")
- thread pool settings are now node-level settings

Relates #18613, closes #9216
</description><key id="157809169">18674</key><summary>Register thread pool settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>:Settings</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-01T02:13:43Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-07T02:09:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-01T08:33:00Z" id="222927539">@jasontedor I think if we are breaking things here hard we should go and make them node level settings and prevent them from being updated.
</comment><comment author="jasontedor" created="2016-06-01T20:12:57Z" id="223110469">&gt; I think if we are breaking things here hard we should go and make them node level settings and prevent them from being updated.

I think that having them as node-level settings makes the most sense, especially when considering heterogeneous clusters. The one downside to the settings not being dynamically updatable is that right now there is an "out" if an executor or backing queue is too small, it can be dynamically increased as solution to the problem.

I'm happy to remove this though; do you think that should be done in this PR or a follow-up @s1monw? 
</comment><comment author="s1monw" created="2016-06-03T14:31:33Z" id="223594884">I added some minors LGTM otehrwise - no need for another round of review. 
</comment><comment author="s1monw" created="2016-06-03T21:24:40Z" id="223698338">LGTM thanks @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Health class improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18673</link><project id="" key="" /><description>Make cluster health classes immutable and have them implement Writeable instead of Streamable.  Also removes the use of a `static Fields` class in favor of just class static fields.
</description><key id="157793269">18673</key><summary>Cluster Health class improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T23:32:56Z</created><updated>2016-06-01T03:14:37Z</updated><resolved>2016-06-01T03:04:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-01T02:20:48Z" id="222875292">LGTM.
</comment><comment author="abeyad" created="2016-06-01T03:14:37Z" id="222881839">Thanks @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>WIP: Index creation does not cause the cluster health to go RED</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18672</link><project id="" key="" /><description>Previously, index creation would momentarily cause the cluster health to
go RED, because the primaries were still being assigned and activated.
This commit ensures that when an index is created or an index is being
recovered during cluster recovery and it does not have any active
allocation ids, then the cluster health status will not go RED, but
instead be YELLOW.
</description><key id="157781436">18672</key><summary>WIP: Index creation does not cause the cluster health to go RED</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>WIP</label></labels><created>2016-05-31T22:05:42Z</created><updated>2016-06-04T12:36:46Z</updated><resolved>2016-06-04T02:53:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-31T22:16:06Z" id="222837955">@bleskes This is a WIP PR, I wanted to get your thoughts on it, as well as feedback on the following points:
1. Dealing with the `CLUSTER_RECOVERED` scenario is a bit tricky.  The idea is that, if we are recovering cluster state, and a particular index was not previously allocated its primary shards before the cluster restart happened, then on cluster recovery, that index should not cause the cluster health to go `RED` while the primaries are initializing.  You mentioned that our check for this case should be an unassigned info reason of `CLUSTER_RECOVERED` combined with no active allocation ids.  However, what if the cluster was restarted half way through index creation where some primaries were activated (have allocation ids) while others weren't?  Also, even if no primaries were activated before, during the recovery process, it is possible (very likely in fact) that some primaries reach the activated state while others lag behind and are still initializing.  In this case, we have _some_ active allocation ids while still having unassigned primary shards due to `CLUSTER_RECOVERED`.  This will cause the cluster to go `RED` even though it should wait until all primaries are assigned.  The cluster health APIs have no way of detecting this.  I believe we should only go `RED` in this scenario iff unassigned info reason == `CLUSTER_RECOVERED` &amp;&amp; activeAllocationIds.size == numberOfShards.  What do you think?
2. The test here: https://github.com/elastic/elasticsearch/pull/18672/files#diff-c88e890f2f8fe06a354f19a751d93ca7R177 does not work yet, partially due to the aforementioned issue but also because of nodes closing while trying to get cluster health and holding onto observers.  And overall, the test itself seems very complex.  Any feedback you have on a cleaner way to accomplish this would be most welcomed.  
</comment><comment author="bleskes" created="2016-06-01T08:28:22Z" id="222926482">Hey, didn't look at the code yet, but some quick answers:

&gt; what if the cluster was restarted half way through index creation where some primaries were activated (have allocation ids) while others weren't?

This means that some shards will be red (those with alloc ids and that were previously started) and other yellow. The whole index will than turn red and the cluster as well.

I think what you're missing here is that active allocation ids are maintain per shard (group) and are only set once a shard have previously been started.

Re tests - my advice would be to start with unit tests first and then worry about integration tests. I get that you're using a test to simulate and understand cluster behavior - which is great, but in that case it doesn't matter how ugly the test is :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate bootstrap.mlockall</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18671</link><project id="" key="" /><description>The setting bootstrap.mlockall is useful on both POSIX-like systems
(POSIX mlockall) and Windows (Win32 VirtualLock). But mlockall is really
a POSIX only thing so the name should not be tied POSIX. This commit
deprecates the setting in favor of "bootstrap.memory_lock".

Relates #18669 
</description><key id="157779517">18671</key><summary>Deprecate bootstrap.mlockall</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>deprecation</label><label>enhancement</label><label>review</label><label>v2.4.0</label></labels><created>2016-05-31T21:54:39Z</created><updated>2016-08-26T13:12:43Z</updated><resolved>2016-06-01T20:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-01T03:09:25Z" id="222881245">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Rework eclipse settings copy so it does not get automatically cleaned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18670</link><project id="" key="" /><description>Gradle has "rules" for certain task names, and clean is one of these.
When you run clean, it searches for any tasks named cleanX, and tries to
reverse engineer the X task. For eclipse, this means it finds
cleanEclipse, and internally runs it (but this does not show up as a
dependency of clean in tasks list!!). Since we added .settings as an
additional file to delete with cleanEclipse, this gets deleted when
running just "clean". It doesn't delete the other files because those
have their own clean methods, and dependencies are not followed in this
insanity. This change simply makes a separate task for cleaning eclipse
settings.
</description><key id="157774166">18670</key><summary>Build: Rework eclipse settings copy so it does not get automatically cleaned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>PITA</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T21:24:34Z</created><updated>2016-05-31T21:26:48Z</updated><resolved>2016-05-31T21:26:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-31T21:26:07Z" id="222826066">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename boostrap.mlockall to bootstrap.memory_lock</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18669</link><project id="" key="" /><description>The setting bootstrap.mlockall is useful on both POSIX-like systems
(POSIX mlockall) and Windows (Win32 VirtualLock). But mlockall is really
a POSIX only thing so the name should not be tied POSIX. This commit
renames the setting to "bootstrap.memory_lock".
</description><key id="157772432">18669</key><summary>Rename boostrap.mlockall to bootstrap.memory_lock</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T21:15:16Z</created><updated>2016-06-01T20:25:52Z</updated><resolved>2016-06-01T20:25:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-31T21:17:41Z" id="222823780">LGTM, I added the breaking label
</comment><comment author="gmarz" created="2016-06-01T00:13:50Z" id="222858441">LGTM &#128077; 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoBoundingBoxQueryBuilder should throw IAE when topLeft and bottomRight are the same coordinate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18668</link><project id="" key="" /><description>Related to #18458, GeoBoundingBoxQueryBuilder should check if topLeft and bottomRight are the same coordinate. If not caught at the parse phase the lucene `GeoPointInBBoxQuery` will throw an exception at query time. This PR adds exception handling at parse time and updates integration tests accordingly.

closes #18631
</description><key id="157766314">18668</key><summary>GeoBoundingBoxQueryBuilder should throw IAE when topLeft and bottomRight are the same coordinate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-05-31T20:45:04Z</created><updated>2016-07-11T15:52:47Z</updated><resolved>2016-07-11T15:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-06-24T15:29:44Z" id="228377246">@nknize I found this in the PR queue and had a look. 
LGTM, except that I don't know the details of the bbox calculation. I played around with that geohash to bbox calculation part a bit and came up with some small unit test for GeoHashUtils#bbox() that might be useful to add, I can link to my branch if you'd like to take a look or do this as a follow up PR.
</comment><comment author="nknize" created="2016-06-27T13:15:01Z" id="228741732">thanks for looking at this @cbuescher! Might as well take care of the test now. Go ahead and link your PR and I'll add the test.
</comment><comment author="cbuescher" created="2016-06-27T15:01:26Z" id="228772241">Hi @nknize, not sure how I can open PR against your branch, so I opened it against master in #19095. Not sure if the test is doing the right thing at all, feel free to comment there, take the commit from there or just drop the test for a new one. I was mainly trying to understand how the geohash relates to the bbox in terms of width &amp; heigwt. If my assumptions were wrong please let me know.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[MapperParsingException[failed to parse]; nested: IllegalStateException[Mixing up field types: class org.elasticsearch.index.mapper.core.StringFieldMapper$StringFieldType != class org.elasticsearch.index.mapper.geo.BaseGeoPointFieldMapper$GeoPointFieldType on field coordinates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18667</link><project id="" key="" /><description>![es_exception](https://cloud.githubusercontent.com/assets/12480500/15691513/7d1f6aa8-2746-11e6-970f-c2f976919a16.PNG)

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**:1.8

**OS version**: Windows 10

**Description of the problem including expected versus actual behavior**:
I am trying to add mapping as a geo_points to the field named "coordinates.". Index is getting created with this mapping, but after that when I am using java bulk index api. While adding type to index, I am getting following exception.
message [MapperParsingException[failed to parse]; nested: IllegalStateException[Mixing up field types: class org.elasticsearch.index.mapper.core.StringFieldMapper$StringFieldType != class org.elasticsearch.index.mapper.geo.BaseGeoPointFieldMapper$GeoPointFieldType on field coordinates];]

This is the mapping I want to add - 
{
    "properties": {
     "coordinates": {
        "type": "geo_point"
      }
    }
}

When I checked mapping for an index and it's type it is different -
{
  "testgeo": { //index
    "mappings": {
      "twitter_mapping": {
        "properties": {
          "coordinates": {
            "type": "geo_point"
          }
        }
      },
      "test": {} //index type
    }
  }
}

I tried to create mapping with different types (other than geo_point), my code works fine with other types. Like this -
{
  "tweet_media": {    //index
    "mappings": {
      "test": {      //index type
        "properties": {
          "created_at": {
            "type": "date",
            "format": "YYYY-MM-DD'T'HH:mm:ss.SSSZ"
          },
          "expanded_url": {
            "type": "string",
            "analyzer": "case_insensitive_keyword_analyzer"
          },
          "has_media": {
            "type": "boolean"
          },
          "hashtags": {
            "type": "string",
            "analyzer": "tag_analyzer"
          },
          "language": {
            "type": "string",
            "analyzer": "en_analyzer"
          },
          "name": {
            "type": "string",
            "analyzer": "case_insensitive_keyword_analyzer"
          },
          "text": {
            "type": "string",
            "analyzer": "en_analyzer"
          },
          "user_mentions": {
            "type": "string",
            "analyzer": "case_insensitive_keyword_analyzer"
          }
        }
      },
      "twitter_mapping": {
        "properties": {
          "created_at": {
            "type": "date",
            "format": "YYYY-MM-DD'T'HH:mm:ss.SSSZ"
          },
          "expanded_url": {
            "type": "string",
            "analyzer": "case_insensitive_keyword_analyzer"
          },
          "has_media": {
            "type": "boolean"
          },
          "hashtags": {
            "type": "string",
            "analyzer": "tag_analyzer"
          },
          "language": {
            "type": "string",
            "analyzer": "en_analyzer"
          },
          "name": {
            "type": "string",
            "analyzer": "case_insensitive_keyword_analyzer"
          },
          "text": {
            "type": "string",
            "analyzer": "en_analyzer"
          },
          "user_mentions": {
            "type": "string",
            "analyzer": "case_insensitive_keyword_analyzer"
          }
        }
      }
    }
  }
}

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:
failure in bulk execution:
[1]: index [testgeo], type [test], id [AVUIFTEIEfgnJAE-P1J7], message [MapperParsingException[failed to parse]; nested: IllegalStateException[Mixing up field types: class org.elasticsearch.index.mapper.core.StringFieldMapper$StringFieldType != class org.elasticsearch.index.mapper.geo.BaseGeoPointFieldMapper$GeoPointFieldType on field coordinates];]

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="157759919">18667</key><summary>[MapperParsingException[failed to parse]; nested: IllegalStateException[Mixing up field types: class org.elasticsearch.index.mapper.core.StringFieldMapper$StringFieldType != class org.elasticsearch.index.mapper.geo.BaseGeoPointFieldMapper$GeoPointFieldType on field coordinates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">manaligaikwad</reporter><labels /><created>2016-05-31T20:14:59Z</created><updated>2016-12-27T14:56:54Z</updated><resolved>2016-06-01T17:28:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T17:28:45Z" id="223065805">You're trying to index a field called `location` with a string value (which is not a valid geo-point) into a field called `location` which is mapped as a `geo_point`.
</comment><comment author="Gramler" created="2016-09-06T10:23:27Z" id="244910331">I ran into this problem when my mapping type I used to create the mapping (map1) and the mapping type I passed when writing to the index (map2) was different. It then creates a new mapping on the fly for the "map2" that is a String, or Double, depending on the type of geoPoint info I had in there. Anyway... it then falls over with the given error message when it tries to write to the index

So double check your mapping types. It might be that.  
</comment><comment author="scipilot" created="2016-12-27T14:56:54Z" id="269336332">Thanks @Gramler that was indeed my problem.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move PageCacheRecycler into BigArrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18666</link><project id="" key="" /><description>PageCacheRecycler is really just an implementation detail of
BigArrays. There is no need to leak this class anywhere outside of it.

Relates to #18664
</description><key id="157755489">18666</key><summary>Move PageCacheRecycler into BigArrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T19:52:49Z</created><updated>2016-06-01T17:35:22Z</updated><resolved>2016-06-01T07:43:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-31T19:53:08Z" id="222800549">@jasontedor can you take a look
</comment><comment author="jasontedor" created="2016-05-31T19:58:16Z" id="222801933">@s1monw I left a minor suggestion, otherwise LGTM. Fire at will.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Performance of Postings Highlighter has regressed significantly in ES 2.1.1 from ES 1.7.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18665</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.1.1

**JVM version**: 1.8.0_72

**OS version**: Windows 10

**Description of the problem including expected versus actual behavior**:
I indexed around 40K documents in a single shard, zero replica index with `_source` enabled. Each document has around 40 to 50 fields. There is only one mapping that I'm using for all documents. It has around 500 fields. I execute a `multi_match` query that matches only 38 documents and mostly the same field. The effective number of fields that get searched on through this `multi_match` query is around 400. This is the same setup I used for both Elasticsearch 1.7.1 and Elasticsearch 2.1.1 and below are the average search times under different scenarios:

```
                                | Elasticsearch 1.7.1 | Elasticsearch 2.1.1
============================================================================
No highlighter                  |               40 ms |               35 ms
Plain highlighter               |              140 ms |             1000 ms
Postings highlighter            |               80 ms |             3000 ms
```

The timings in the table above are fairly consistent even when the same queries are fired back to back multiple times. As you can see, if I disable highlighting all together, they both perform equally well but `postings` highlighter performs significantly worse in the new version. Interestingly, `plain` highlighter performs better than `postings` highlighter in ES 2.1.1. I also tried Elasticsearch 2.3.3 but had similar experience as Elasticsearch 2.1.1. This is a major blocker for us to move to Elasticsearch 2.x.

Is this a know issue? Let me know if you need more information and I'd be happy to help.
</description><key id="157747319">18665</key><summary>Performance of Postings Highlighter has regressed significantly in ES 2.1.1 from ES 1.7.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">bittusarkar</reporter><labels><label>:Highlighting</label><label>feedback_needed</label></labels><created>2016-05-31T19:10:48Z</created><updated>2017-03-31T14:33:23Z</updated><resolved>2017-03-31T14:33:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T17:15:40Z" id="223062115">@bittusarkar could you upload a small recreation showing what you're doing? I can't think of anything that has changed here, except maybe the extraction of query terms.
</comment><comment author="bittusarkar" created="2016-06-01T17:55:38Z" id="223073624">@clintongormley Let me work on creating the repro. It might take some time. In the meanwhile, let me add some information I missed earlier. I profiled Elasticsearch 2.1.1 by running a single query and almost 77% of inherent time was spent in `org.apache.lucene.search.postingshighlighter.CustomPostingsHighlighter.highlightField()`. Then at 4%, it was `org.apache.lucene.search.IndexSearcher.createNormalizedWeight()` followed by other methods. Hope this information is useful. I can share a JProfiler snapshot file if you're interested.
</comment><comment author="javanna" created="2016-06-02T06:47:27Z" id="223209417">Not surprised that method is the hotspot. The way we use the lucene postings highlighter is not the best...  #11223 would help here so that we can highlight documents in bulk rather than one document and one field at a time.

That said I don't yet see what made things so much worse when we moved to the lucene postings highlighter, which we did in 2.x, nor what changed in the plain highlighter that made things so much worse. The recreation might help.
</comment><comment author="bittusarkar" created="2016-06-15T19:55:05Z" id="226301157">@javanna, @clintongormley I have a reproduction available but there is a huge file for which I cannot create a gist. Is there another way by which I can share the files? Email maybe?
</comment><comment author="clintongormley" created="2016-06-16T08:53:58Z" id="226426708">@bittusarkar sure - clinton at elastic dot co
</comment><comment author="bittusarkar" created="2016-06-16T11:05:45Z" id="226455027">Thanks @clintongormley. I just sent you the email.
</comment><comment author="jimczi" created="2016-11-14T15:29:39Z" id="260366400">@bittusarkar sorry for the (very) late response, we've reproduced your issue and found why you're seeing such a big difference between 1.7 and 2.x. In 2.x we've removed our fork of the PostingsHighlighter and we now rely solely on the Lucene implementation. We did that because some of the functionalities we wanted were not present in Lucene at that time. We realized afterward that it was hard to maintain this fork and that we missed a lot of optimizations of the original implementation. This is why we decided to remove our fork and to integrate the original PostingsHighlighter directly. Though the way we've integrated the original Highlighter is not optimal and for some use cases it can be very slow. For instance in our fork we extract the terms of the query once and then we reuse the set of terms for each hit/field. In the new version we extract the terms of the query for each field in each hit. For simple queries it is fast but if you have a lot of fields and a lot of terms in your query it can be the bottleneck. In the example you sent us you have 240 fields to highlight and the query contains 240 terms (one for each field). This is really a bad use case for this highlighter since it needs to extract the terms of this big query 240 times for each hit. 
Since the issue is clearly related to the number of fields you want to highlight in a single query, 
can you explain why you need so many fields ? Maybe you could use a different model that requires less fields ? 
</comment><comment author="colings86" created="2017-03-31T14:33:23Z" id="290728760">No further feedback. @bittusarkar if you have answers to the questions above please comment and reopen this issue</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove thread pool from page cache recycler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18664</link><project id="" key="" /><description>The page cache recycler has a dependency on thread pool that was there
for historical reasons but is no longer needed. This commit removes this
now unneeded dependency.

Relates #18613
</description><key id="157740032">18664</key><summary>Remove thread pool from page cache recycler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T18:35:01Z</created><updated>2016-05-31T19:08:48Z</updated><resolved>2016-05-31T18:51:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-31T18:42:08Z" id="222781624">neat. LGTM
</comment><comment author="s1monw" created="2016-05-31T19:05:42Z" id="222788380">awesome as a sideeffect can we make the page cache recycler an impl detail of BigArrays?
</comment><comment author="jasontedor" created="2016-05-31T19:08:48Z" id="222789229">&gt; awesome as a sideeffect can we make the page cache recycler an impl detail of BigArrays?

+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue with EC2 Discovery Plugin - Does not honor discovery.ec2.tag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18663</link><project id="" key="" /><description>**Elasticsearch version**: Elasticsearch 5.0.0-alpha2

**JVM version**: Java 8.0

**OS version**: Linux

**Description of the problem including expected versus actual behavior**:

Based on documentation, I see that we can make use of discovery.ec2.tag in the EC2 discovery plugin as a Prefix to filter down the EC2 instances to be included in the discovery process. I created a tag by name Stage with value as DEV-ELK in EC2 instances in AWS and  I included the following setting in latest Elasticsearch 5.0.0-alpha2 release

discovery.ec2.tag.Stage=DEV-ELK

But while starting up Elastic Search instance after including the above tag in elasticsearch.yml file, it complains Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [discovery.ec2.tag.Stage]

**Steps to reproduce**:
 1.Create EC2 instances with Tag as Stage and Value as DEV-ELK
 2.Configure elasticsearch.yml file to use Discovery Plugin and include the following setting to filter down the instance discovery.ec2.tag.Stage=DEV-ELK
 3.Restart the Elastic Search instance. As soon as you start, you will see the following error pasted in logs section

**Provide logs (if relevant)**:

Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [discovery.ec2.tag.Stage]
at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:267)
at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:120)
at &lt;&lt;&lt;guice&gt;&gt;&gt;
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:235)
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
at org.elasticsearch.cli.Command.main(Command.java:53)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
</description><key id="157727866">18663</key><summary>Issue with EC2 Discovery Plugin - Does not honor discovery.ec2.tag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">krishmah</reporter><labels /><created>2016-05-31T17:35:26Z</created><updated>2016-06-16T16:05:22Z</updated><resolved>2016-06-16T16:05:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="krishmah" created="2016-06-16T11:13:03Z" id="226456272">Any update on this?
</comment><comment author="clintongormley" created="2016-06-16T12:49:55Z" id="226475312">@dadoonet could you look at this please?
</comment><comment author="s1monw" created="2016-06-16T13:09:22Z" id="226479918">@krishmah did you install the discovery plugin? It seems like it's registered but if you haven't installed the plugin it will fail with this error.
</comment><comment author="krishmah" created="2016-06-16T14:55:01Z" id="226510163">I have installed this plugin. It is working with Security group based
discovery. Just that Tag based discovery is not working
On Jun 16, 2016 6:40 PM, "Simon Willnauer" notifications@github.com wrote:

&gt; @krishmah https://github.com/krishmah did you install the discovery
&gt; plugin? It seems like it's registered but if you haven't installed the
&gt; plugin it will fail with this error.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18663#issuecomment-226479918,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AJNg9zBKRvDx2mO4p694JMEzv2GfKExLks5qMUs9gaJpZM4Iqxfm
&gt; .
</comment><comment author="s1monw" created="2016-06-16T16:05:22Z" id="226532599">duplicate of https://github.com/elastic/elasticsearch/pull/18257 - this is fixed in alpha3, please upgrade :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix EC2 discovery settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18662</link><project id="" key="" /><description>`cloud.aws.ec2.access_key` has fallback setting `cloud.aws.access_key`.

```
Setting&lt;String&gt; KEY_SETTING = new Setting&lt;&gt;("cloud.aws.ec2.access_key", AwsEc2Service.KEY_SETTING, Function.identity(),
            Property.NodeScope, Property.Filtered);
```

`cloud.aws.access_key`'s default value is empty string.

```
Setting&lt;String&gt; KEY_SETTING =
        Setting.simpleString("cloud.aws.access_key", Property.NodeScope, Property.Filtered);
```

```
public static Setting&lt;String&gt; simpleString(String key, Property... properties) {
        return new Setting&lt;&gt;(key, s -&gt; "", Function.identity(), properties);
    }
```

Same for `cloud.aws.ec2.secret_key`

Closes #18652
</description><key id="157711022">18662</key><summary>Fix EC2 discovery settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">alexshadow007</reporter><labels><label>:Plugin Discovery EC2</label></labels><created>2016-05-31T16:12:00Z</created><updated>2017-05-09T08:16:02Z</updated><resolved>2016-06-03T15:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-31T17:18:54Z" id="222757427">w00t! Thanks a lot @alexshadow007!
I'll merge it soonish.

I added you to the pioneer program
</comment><comment author="alexshadow007" created="2016-05-31T17:44:50Z" id="222764852">@rjernst @dadoonet done
</comment><comment author="dadoonet" created="2016-06-01T16:13:17Z" id="223044172">@alexshadow007 Could you squash your commits? I'd like to pick your commit within another PR I'm writing ATM to add tests on that part.
</comment><comment author="alexshadow007" created="2016-06-01T16:39:48Z" id="223051956">@dadoonet Done
</comment><comment author="dadoonet" created="2016-06-01T17:05:46Z" id="223059398">Cool thanks!

While testing all that stuff I found other issues like the one you fixed. I'll come with a more global fix (also for S3 repositories).
</comment><comment author="dadoonet" created="2016-06-01T22:02:59Z" id="223138742">I opened #18690 which contains your commit and some more fix with tests.

I think we can close this one?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only filter intial recovery (post API) when shrinking an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18661</link><project id="" key="" /><description>Today we use `index.routing.allocation.include._id` to filter the allocation
for the shrink target index. That has the sideeffect that the user has to
delete that setting / change it once the primary has been recovered (shrink is done)
This PR adds a dedicated filter that can only be set internally that only filters
allocation for unassigned shards.

@ywelsch do you wanna take a look?
</description><key id="157704098">18661</key><summary>Only filter intial recovery (post API) when shrinking an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T15:42:50Z</created><updated>2016-06-02T13:38:51Z</updated><resolved>2016-06-02T13:38:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-31T15:43:47Z" id="222729979">hmm I guess I should have a unittest for this as well... adding...
</comment><comment author="s1monw" created="2016-05-31T19:07:53Z" id="222788988">@ywelsch I added a test - do you mind looking at it
</comment><comment author="ywelsch" created="2016-06-01T08:59:46Z" id="222933955">Left minor comments and wonder if we should clean up the private setting once all shards have been activated.
</comment><comment author="s1monw" created="2016-06-01T09:51:30Z" id="222945795">@ywelsch pushed a new commit
</comment><comment author="ywelsch" created="2016-06-01T10:20:33Z" id="222952359">LGTM. Thanks @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increasing Number of Replicas - RecoveryFailedException and RemoteTransportException and NotSerializableExceptionWrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18660</link><project id="" key="" /><description>**Elasticsearch version**:
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },

**JVM version**:
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**:
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=14.04

**Description of the problem including expected versus actual behavior**:

I have a 5 node cluster. 50 indices.   110 shards.
I had previously set the number of replicas to 0 to bulk load about 5 TB of data.  Now that all the indexing is complete and all nodes are green, all shards are STARTED, I would like to, obviously, increase the number of replicas to at least 1.   

I issued the PUT to increase number_of_replicas for just one index, but the new shard gets stuck in INITIALIZING.  Upon examining the logs, I find a serious of exceptions, ranging from RecoveryFailedException, RemoteTransportException and NotSerializableExceptionWrapper. 

Logs are included below.   

**Steps to reproduce**:
increase the number of replicas for the index ( in my case from 0 to 1 )
{
    "index" : {
        "number_of_replicas" : 1
    }
} 

**Provide logs (if relevant)**:

33604   RecoveryFailedException[[d-0516-ak][0]: Recovery failed from {AVES2}{GJq4BB0NQqepIogVu94eCQ}{172.31.32.129}{172.31.32.129:9300}{master=true} into {AVES1}{an91GFrhRXKhICZMgPKRJg}{172.31.35.8}{172.31.35.8:9300}{master=true}]; nested: RemoteTransportException[[AVES2][172.31.32.129:9300][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [100] files with total size of [8gb]]; nested: RemoteTransportException[[AVES1][172.31.35.8:9300][internal:index/shard/recovery/file_chunk]]; nested: NotSerializableExceptionWrapper[i_o_exception: Input/output error];
 33605      at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:258)
 33606      at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:69)
 33607      at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:508)
 33608      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
 33609      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 33610      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 33611      at java.lang.Thread.run(Thread.java:745)
 33612  Caused by: RemoteTransportException[[AVES2][172.31.32.129:9300][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [100] files with total size of [8gb]]; nested: RemoteTransportException[[AVES1][172.31.35.8:9300][internal:index/shard/recovery/file_chunk]]; nested: NotSerializableExceptionWrapper[i_o_exception: Input/output error];
 33613  Caused by: [d-0516-ak][[d-0516-ak][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [100] files with total size of [8gb]]; nested: RemoteTransportException[[AVES1][172.31.35.8:9300][internal:index/shard/recovery/file_chunk]]; nested: NotSerializableExceptionWrapper[i_o_exception: Input/output error];
 33614      at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:135)
 33615      at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:126)
 33616      at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:52)
 33617      at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:135)
 33618      at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
 33619      at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
 33620      at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
 33621      at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
 33622      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
 33623      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 33624      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 33625      at java.lang.Thread.run(Thread.java:745)
 33626  Caused by: [d-0516-ak][[d-0516-ak][0]] RecoverFilesRecoveryException[Failed to transfer [100] files with total size of [8gb]]; nested: RemoteTransportException[[AVES1][172.31.35.8:9300][internal:index/shard/recovery/file_chunk]]; nested: NotSerializableExceptionWrapper[i_o_exception: Input/output error];
 33627      at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:453)
 33628      at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:133)
 33629      ... 11 more
 33630  Caused by: RemoteTransportException[[AVES1][172.31.35.8:9300][internal:index/shard/recovery/file_chunk]]; nested: NotSerializableExceptionWrapper[i_o_exception: Input/output error];
 33631      Suppressed: RemoteTransportException[[AVES1][172.31.35.8:9300][internal:index/shard/recovery/file_chunk]]; nested: NotSerializableExceptionWrapper[i_o_exception: Input/output error];
 33632      Caused by: NotSerializableExceptionWrapper[i_o_exception: Input/output error]
 33633          at sun.nio.ch.FileDispatcherImpl.force0(Native Method)
 33634          at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)
 33635          at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:388)
 33636          at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:392)
 33637          at org.apache.lucene.store.FSDirectory.fsync(FSDirectory.java:288)
 33638          at org.apache.lucene.store.FSDirectory.sync(FSDirectory.java:232)
 33639          at org.apache.lucene.store.FileSwitchDirectory.sync(FileSwitchDirectory.java:170)
 33640          at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:78)
 33641          at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:78)
 33642          at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:468)
 33643          at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:419)
 33644          at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
 33645          at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
 33646          at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
 33647          at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
 33648          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 33649          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 33650          at java.lang.Thread.run(Thread.java:745)
 33651      Suppressed: RemoteTransportException[[AVES1][172.31.35.8:9300][internal:index/shard/recovery/file_chunk]]; nested: NotSerializableExceptionWrapper[i_o_exception: Input/output error];
 33652      Caused by: NotSerializableExceptionWrapper[i_o_exception: Input/output error]
 33653          at sun.nio.ch.FileDispatcherImpl.force0(Native Method)
 33654          at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)
 33655          at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:388)
 33656          at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:392)
 33657          at org.apache.lucene.store.FSDirectory.fsync(FSDirectory.java:288)
 33658          at org.apache.lucene.store.FSDirectory.sync(FSDirectory.java:232)
 33659          at org.apache.lucene.store.FileSwitchDirectory.sync(FileSwitchDirectory.java:170)
 33660          at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:78)
 33661          at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:78)
 33662          at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:468)
 33663          at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:419)
 33664          at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
 33665          at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
 33666          at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
 33667          at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
 33668          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 33669          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 33670          at java.lang.Thread.run(Thread.java:745)
</description><key id="157696367">18660</key><summary>Increasing Number of Replicas - RecoveryFailedException and RemoteTransportException and NotSerializableExceptionWrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cragun</reporter><labels /><created>2016-05-31T15:11:01Z</created><updated>2016-05-31T18:24:09Z</updated><resolved>2016-05-31T18:24:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-31T15:21:57Z" id="222723067">&gt; i_o_exception: Input/output error

I think that you have a hardware issue.
</comment><comment author="cragun" created="2016-05-31T15:54:47Z" id="222733437">hey Jason.. thanks for the quick input..

ya.. I would think too that an IO error would be hardware... yet the system shows no signs thereof.   I can index new docs.. individually and in bulk.  Across all nodes and shards.  So ES, at least parts of ES, seemly have no issues with the drives.   

The OS sees no issues with the drive array either. (see log below).

Any other thoughts... ?

ubuntu@ip-172-31-35-8:~$ sudo mdadm --detail /dev/md127
/dev/md127:
        Version : 1.2
  Creation Time : Tue May 24 03:47:38 2016
     Raid Level : raid0
     Array Size : 2097149952 (2000.00 GiB 2147.48 GB)
   Raid Devices : 4
  Total Devices : 4
    Persistence : Superblock is persistent

```
Update Time : Mon May 30 19:58:18 2016
      State : clean 
```

 Active Devices : 4
Working Devices : 4
 Failed Devices : 0
  Spare Devices : 0

```
 Chunk Size : 512K

Number   Major   Minor   RaidDevice State
   0     202       96        0      active sync   /dev/xvdg
   1     202      112        1      active sync   /dev/xvdh
   2     202      128        2      active sync   /dev/xvdi
   4     202       80        3      active sync   /dev/xvdf
```
</comment><comment author="jasontedor" created="2016-05-31T16:00:52Z" id="222735391">&gt; yet the system shows no signs thereof.

It is showing a sign, this I/O error that is coming from inside the JVM. I think that you need to run full diagnostics on your machine. Since this is in an fsync, it's likely a failing disk so maybe check the SMART status but I wouldn't rule out other components as well.

&gt; Any other thoughts... ?

Nope, this is really the tell-tale sign of a hardware failure.
</comment><comment author="cragun" created="2016-05-31T16:17:40Z" id="222740350">cool.  On it.  Ill update my results when I have some.  thanks for the insight.
</comment><comment author="cragun" created="2016-05-31T16:26:46Z" id="222743015">Hey, one more thought though that I think works in contrary to the prevailing theory...

I have tried initiating a replica from every combination of Nodes, to every combination of nodes.
A-&gt;B, B-&gt;A, A-&gt;C,C-&gt;B,C-&gt;A   etc...  To keep things simple, i particularly tested indices with single shards.  So adding a simple replica would only affect one targeted machine.

yet, they all fail with the same error.  I do find it very improbably that all machines are failing with the same error.  

would you disagree?
</comment><comment author="jasontedor" created="2016-05-31T17:15:56Z" id="222756613">That is definitely suspicious, but I've seen bum batches of hardware before. The string "Input/output error" comes from Linux and is passed all the way back up to the application when the `fsync` calls returns with error. Have you checked the SMART status? How about `dmesg` or `/var/log/kern.log`? I'm not ruling out that there is something else going on, but since this is still screaming hardware issue to me I think we should continue pursuing that path until we can rule it out and go down another.
</comment><comment author="cragun" created="2016-05-31T17:27:31Z" id="222759879">OK,... we are on to something  ( and it probably points to me making a newbie error.. but not sure yet )...

kern.log is chock full of "bio too big device..."

Im reading up on how to solve this... but I thought Id post where Im at in the interim...

May 31 16:05:30 ip-172-31-35-8 kernel: [240732.201722] bio too big device xvdf (1024 &gt; 256)
May 31 16:05:30 ip-172-31-35-8 kernel: [240732.204391] bio too big device xvdg (1024 &gt; 256)
May 31 16:05:30 ip-172-31-35-8 kernel: [240732.207031] bio too big device xvdh (1024 &gt; 256)
May 31 16:05:30 ip-172-31-35-8 kernel: [240732.209817] bio too big device xvdi (1024 &gt; 256)
May 31 16:05:30 ip-172-31-35-8 kernel: [240732.212528] bio too big device xvdf (736 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.819248] bio too big device xvdf (288 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.821968] bio too big device xvdg (1024 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.824635] bio too big device xvdh (1024 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.827323] bio too big device xvdi (1024 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.830035] bio too big device xvdf (1024 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.832698] bio too big device xvdg (1024 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.835400] bio too big device xvdh (1024 &gt; 256)
May 31 16:05:31 ip-172-31-35-8 kernel: [240732.838107] bio too big device xvdi (1024 &gt; 256)
</comment><comment author="s1monw" created="2016-05-31T18:11:52Z" id="222772680">@cragun can we close this - seems like a HW issue on your end?
</comment><comment author="cragun" created="2016-05-31T18:21:48Z" id="222775601">Yup. It does.  Thanks. 

&gt; On May 31, 2016, at 12:13 PM, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; @cragun can we close this - seems like a HW issue on your end?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.
</comment><comment author="jasontedor" created="2016-05-31T18:24:09Z" id="222776282">Thanks @cragun. Good luck!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Estimate shard size for shrinked indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18659</link><project id="" key="" /><description>When we shrink an index we can estimate the shards size for the primary
from the source index. This is important for allocation decisions since we
should try out best to ensure we have enough space on the node we shrink the
index.

@ywelsch  can you look at this?
</description><key id="157674692">18659</key><summary>Estimate shard size for shrinked indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T13:42:29Z</created><updated>2016-05-31T14:33:38Z</updated><resolved>2016-05-31T14:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-31T14:30:18Z" id="222706301">LGTM. Thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve date api for expressions/painless fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18658</link><project id="" key="" /><description>ScriptDocValues.Longs exposes a `.date` member that other scripting engines use to get convenient access to date components.

For example `doc['field'].date.monthOfYear`. 

Expressions does this inconsistently, it supports a few (but not all) of these methods, but via a different syntax without the `.date`, and sometimes with different semantics (!) because it uses java.util.Calendar.

For expressions this pr adds `.date` working the same way/syntax is it does for other engines, and undocuments the old methods: the old methods are left unchanged for anything using them. For painless it adds ReadableDateTime to the whitelist so the same syntax works there too.

We can also fix a few other inconsistencies in expressions, like supporting `.length`/`.size()` for the count of values for the field to match other engines: this way its more of a proper subset for most uses.
</description><key id="157674117">18658</key><summary>improve date api for expressions/painless fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Expressions</label><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T13:39:51Z</created><updated>2016-06-28T09:30:39Z</updated><resolved>2016-05-31T16:33:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-31T15:40:07Z" id="222728811">LGTM
</comment><comment author="jdconrad" created="2016-05-31T16:24:13Z" id="222742260">LGTM also!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting a null_pointer_exception when executing suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18657</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.3.2
**JVM version**:
1.8.0_73
**OS version**:
CentOS release 6.6 (Final)
**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/suggester-context.html
 1.

```
PUT services/_mapping/service
{
    "service": {
        "properties": {
            "name": {
                "type" : "string"
            },
            "tag": {
                "type" : "string"
            },
            "suggest_field": {
                "type": "completion",
                "context": {
                    "color": { 
                        "type": "category",
                        "path": "color_field",
                        "default": ["red", "green", "blue"]
                    },
                    "location": { 
                        "type": "geo",
                        "precision": "5m",
                        "neighbors": true,
                        "default": "u33"
                    }
            }
        }
    }
    }
}
```

 2.

```
PUT services/service/1
{
    "name": "knapsack",
    "suggest_field": {
        "input": ["knacksack", "backpack", "daypack"],
        "context": {
            "color": ["red", "yellow"]
            }
        }
}
```

 3.

```
POST services/_suggest?pretty'
{
    "suggest" : {
        "text" : "m",
        "completion" : {
            "field" : "suggest_field",
            "size": 10,
            "context": {
                "color": "red"
            }
        }
    }
}
```

**The response I'm getting is:**

```
{
  "_shards": {
    "total": 5,
    "successful": 4,
    "failed": 1,
    "failures": [
      {
        "shard": 3,
        "index": "services",
        "status": "INTERNAL_SERVER_ERROR",
        "reason": {
          "type": "exception",
          "reason": "failed to execute suggest",
          "caused_by": {
            "type": "null_pointer_exception",
            "reason": null
          }
        }
      }
    ]
  }
}
```

**Provide logs (if relevant)**:

```
RemoteTransportException[[192.168.1.100-node2][192.168.1.100:29300][indices:data/read/suggest[s]]]; nested: ElasticsearchException[failed to execute suggest]; nested: NullPointerException;
Caused by: ElasticsearchException[failed to execute suggest]; nested: NullPointerException;
        at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:152)
        at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:61)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:282)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:278)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.suggest.context.ContextMapping$ContextQuery.toAutomaton(ContextMapping.java:264)
        at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider$2.getLookup(AnalyzingCompletionLookupProvider.java:279)
        at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$CompletionTerms.getLookup(Completion090PostingsFormat.java:264)
        at org.elasticsearch.search.suggest.completion.CompletionSuggester.innerExecute(CompletionSuggester.java:69)
        at org.elasticsearch.search.suggest.completion.CompletionSuggester.innerExecute(CompletionSuggester.java:44)
        at org.elasticsearch.search.suggest.Suggester.execute(Suggester.java:41)
        at org.elasticsearch.search.suggest.SuggestPhase.execute(SuggestPhase.java:85)
        at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:147)
        ... 9 more

```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

https://github.com/elastic/elasticsearch/blob/v2.3.2/core/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java : 308

```
            List&lt;ContextQuery&gt; queries = new ArrayList&lt;&gt;(mappings.size());
            for (ContextMapping mapping : mappings.values()) {
                queries.add(querySet.get(mapping.name));
            }
```

Not checked  "querySet.get(mapping.name)" equal to null
</description><key id="157667458">18657</key><summary>Getting a null_pointer_exception when executing suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Popo-lth</reporter><labels><label>:Suggesters</label><label>bug</label><label>v2.4.7</label></labels><created>2016-05-31T13:09:21Z</created><updated>2017-07-21T09:31:18Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-31T13:36:13Z" id="222690066">thanks for submitting the issue! that seems like a bug not checking this, would you be willing to submit a pullrequest to fix this? The analysis of the problem looks awesome and we are happy to guide you to fix the problem? @areek what do you think?
</comment><comment author="robacarp" created="2016-07-13T15:33:49Z" id="232393541">I ran across the same error but it took me a while to understand what I could do to resolve the issue until the patch comes through. In the example from @areek, the `service` suggest field has two contexts: color and location. The suggest query only specifies the color context and not the location context.

My new understanding of the design is: Queries against the `_suggest` endpoint must specify all contexts defined in the suggester.  

Hope that helps!
</comment><comment author="robacarp" created="2016-07-14T17:18:10Z" id="232731670">It's also worth noting that the example code given [in the official docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/suggester-context.html#_category_query) triggers this bug.
</comment><comment author="sergbass" created="2016-07-21T12:16:54Z" id="234236466">&gt; My new understanding of the design is: Queries against the _suggest endpoint must specify all contexts defined in the suggester. 

@robacarp, I think that it would be more convenient to allow specifying less categories/contexts that indexed field has. In my case, I need to suggest hotels by name, and I have several cases:
1. Suggest hotel name that's found by current search
2. Suggest hotel by selected area
3. Suggest hotel by selected city
4. More combinations possible in future

Currently I added all these parameters as separate categories to one suggest field. If we do like you proposed, I'll need to create separate suggest fields to every usage case. Other workaround in my case would be to add just one category with values like "city-1234", "area-4321", etc., by it seems to eliminate category idea.
</comment><comment author="clintongormley" created="2016-07-21T12:37:13Z" id="234240539">@sergbass in 5.0 the completion suggester has been completely rewritten and supports the functionality you suggest
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Treat zero token in `common` terms query as MatchNoDocsQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18656</link><project id="" key="" /><description>Currently we return `null` when the query in a common terms query has zero tokens after analysis. It would be better if query builders
`toQuery()` would never return null and return a meaningful lucene query instead. In this case MatchNoDocsQuery sounds like the right
choice.
</description><key id="157664812">18656</key><summary>Treat zero token in `common` terms query as MatchNoDocsQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T12:56:43Z</created><updated>2016-05-31T14:30:36Z</updated><resolved>2016-05-31T14:30:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-31T12:58:24Z" id="222680373">For context, this came up in a discussion in https://github.com/elastic/elasticsearch/pull/17624#discussion_r65148474
</comment><comment author="s1monw" created="2016-05-31T13:33:54Z" id="222689432">LGTM should this be flagged as breaking?
</comment><comment author="cbuescher" created="2016-05-31T13:52:12Z" id="222694674">@s1monw thanks, I don't think this is breaking the existing behaviour. I just checked, on 2.3 when the `common` parses to null (still CommonTermsQueryParser) this gets caught in IndexQueryParserService and rewritten to a match-no-docs query. There's even an IT test for it (CommonTermsQueryParserTests), so we just get rid of the null return value earlier. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to set doc a score(boost)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18655</link><project id="" key="" /><description> Use solr to set the boost, but ES did not find can set boost, please help about this question&#12290;

bulkRequest.add(client.prepareIndex("twitter", "tweet", "8")
                .setSource(XContentFactory.jsonBuilder()
                            .startObject()
                                .field("user", "tom")
                                .field("postDate", new Date())
                                .field("message", "score")
                            .endObject()));

SolrInputDocument solrDoc = new SolrInputDocument();
                    //solrDoc.setDocumentBoost(documentBoost);
</description><key id="157662890">18655</key><summary>how to set doc a score(boost)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Fly365</reporter><labels /><created>2016-05-31T12:46:48Z</created><updated>2016-05-31T14:33:47Z</updated><resolved>2016-05-31T14:33:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-31T14:33:47Z" id="222707381">Hi @Fly365, please use the forums at https://discuss.elastic.co/ to ask generall questions about usage.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add reindex example on how to reindex daily indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18654</link><project id="" key="" /><description>This can be a common case with beats in case the template changes between two versions and the old data should be reindex with the new templates.
</description><key id="157656867">18654</key><summary>Add reindex example on how to reindex daily indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ruflin</reporter><labels><label>:Reindex API</label><label>docs</label><label>v5.0.0-beta1</label></labels><created>2016-05-31T12:13:42Z</created><updated>2016-10-04T21:11:08Z</updated><resolved>2016-08-26T17:08:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-19T13:13:41Z" id="233627996">@ruflin do you still want to do this?
</comment><comment author="ruflin" created="2016-07-20T06:17:28Z" id="233848707">@nik9000 Definitively. Still on my todo list, but will take me some more time.
</comment><comment author="ruflin" created="2016-08-12T10:50:04Z" id="239417247">@nik9000 I updated the PR with the changes. 
</comment><comment author="msvechla" created="2016-08-26T12:56:01Z" id="242727819">Is this also possible on 2.3.1? As far as I know the painless scripting language is not available yet, right? Could you also provide an example that works with 2.3.X?
</comment><comment author="nik9000" created="2016-08-26T13:03:18Z" id="242729399">&gt; @nik9000 I updated the PR with the changes.

Sorry it took so long to get back to you. I left I minor thing, LGTM otherwise.
</comment><comment author="nik9000" created="2016-08-26T13:05:59Z" id="242729944">&gt; Could you also provide an example that works with 2.3.X?

We've been trying to use painless mostly for the 5.x docs because it gives painless more practice and it is more convenient because you don't need to enable dynamic scripting, it is enabled out of the box for painless.

You ought to be able to just replace `painless` with `groovy` in the example and it should just work in 2.x. Painless's syntax tries very hard to be the same as groovy. In this example I think it matches perfectly.
</comment><comment author="nik9000" created="2016-08-26T16:47:59Z" id="242788316">LGTM. @ruflin, if you want to merge yourself please run `gradle docs:check` first just to triple check. Otherwise I can do it as I have it all checked out and ready.
</comment><comment author="ruflin" created="2016-08-26T16:52:33Z" id="242789443">@nik9000 Just wanted to do the check again myself but now run into gradle issue (I accidentally installed gradle 3.0 it seems -&gt; not compatible with the build). Would be nice if you could run the check for me and merge. Thanks a lot.
</comment><comment author="nik9000" created="2016-08-26T17:08:47Z" id="242793486">I'm fairly upset about gradle compatibility..... I wish we could have gradlew without committing a binary wrapper....

Anyway, I tests generated from the snippets locally and this passed so I'm just going to merge it.
</comment><comment author="ghost" created="2016-10-04T21:06:46Z" id="251513716">Hey guys, is there an example on how to do this with Groovy instead of Painless, since painless is not installed in 2.3.2, and I am not finding a Jar for Painless scripting online.
</comment><comment author="msvechla" created="2016-10-04T21:11:08Z" id="251514857">@chromechris1 as @nik9000 already replied to me, you can simply replace `painless` with `groovy`. This worked perfectly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to run mvn test, failing for randomizedtesting </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18653</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3

**JVM version**: "1.8.0_91"

**OS version**: Rhel 7.2

**Description of the problem including expected versus actual behavior**:
mvn clean package and mvn test command are failing with 
"[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.3.2:junit4 (tests) on project elasticsearch"

**Steps to reproduce**:
1. Install maven 3.3.3 from tar ball link : http://archive.apache.org/dist/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz and install Java 8 (java-1.8.0-openjdk-devel
2. Set desired PATH to have maven and java usable
3. Copy the source for Elastic-search 
4. Then run maven commands inside the elasticsearch directory; "mvn clean package/ mvn test"

**Relevant Logs**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

Tests with failures:
- org.elasticsearch.plugins.PluginManagerPermissionTests.testThatUnaccessiblePluginConfigDirectoryAbortsPluginInstallation
- org.elasticsearch.plugins.PluginManagerPermissionTests.testThatUnaccessibleBinDirectoryAbortsPluginInstallation
- org.elasticsearch.plugins.PluginManagerPermissionTests.testThatNonWritablePluginsDirectoryLeavesNoLeftOver

[INFO] JVM J0:     0.98 ..    95.07 =    94.10s
[INFO] JVM J1:     0.98 ..    95.29 =    94.31s
[INFO] JVM J2:     0.97 ..    95.18 =    94.21s
[INFO] Execution time total: 1 minute 35 seconds
[INFO] Tests summary: 516 suites, 2943 tests, 3 failures, 10 ignored (10 assumptions)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Build Tools and Resources .......................... SUCCESS [02:28 min]
[INFO] Rest API Specification ............................. SUCCESS [  0.465 s]
[INFO] Elasticsearch: Parent POM .......................... SUCCESS [02:35 min]
[INFO] Elasticsearch: Core ................................ FAILURE [05:04 min]
[INFO] Distribution: Parent POM ........................... SKIPPED
[INFO] Distribution: TAR .................................. SKIPPED
[INFO] Distribution: ZIP .................................. SKIPPED
[INFO] Distribution: Deb .................................. SKIPPED
[INFO] Plugin: Parent POM ................................. SKIPPED
[INFO] Plugin: Analysis: Japanese (kuromoji) .............. SKIPPED
[INFO] Plugin: Analysis: Smart Chinese (smartcn) .......... SKIPPED
[INFO] Plugin: Analysis: Polish (stempel) ................. SKIPPED
[INFO] Plugin: Analysis: Phonetic ......................... SKIPPED
[INFO] Plugin: Analysis: ICU .............................. SKIPPED
[INFO] Plugin: Cloud: Google Compute Engine ............... SKIPPED
[INFO] Plugin: Cloud: Azure ............................... SKIPPED
[INFO] Plugin: Cloud: AWS ................................. SKIPPED
[INFO] Plugin: Delete By Query ............................ SKIPPED
[INFO] Plugin: Discovery: Multicast ....................... SKIPPED
[INFO] Plugin: Language: Python ........................... SKIPPED
[INFO] Plugin: Language: JavaScript ....................... SKIPPED
[INFO] Plugin: Mapper: Murmur3 ............................ SKIPPED
[INFO] Plugin: Mapper: Size ............................... SKIPPED
[INFO] Plugin: JVM example ................................ SKIPPED
[INFO] Plugin: Example site ............................... SKIPPED
[INFO] QA: Parent POM ..................................... SKIPPED
[INFO] QA: Smoke Test Plugins ............................. SKIPPED
[INFO] QA: Smoke Test Multi-Node IT ....................... SKIPPED
[INFO] QA: Smoke Test Client .............................. SKIPPED
[INFO] QA: Smoke Test Command Line Params ................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 11:03 min
[INFO] Finished at: 2016-05-16T12:16:20+00:00
[INFO] Final Memory: 39M/435M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.16:junit4 (tests) on project elasticsearch: There were test failures: 516 suites, 2943 tests, 3 failures, 10 ignored (10 assumptions) -&gt; [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &lt;goals&gt; -rf :elasticsearch

**Additional Info**:
I think this is similar issue as that of #3674.
Can someone please guide me on this, any pointers would be useful.
</description><key id="157654127">18653</key><summary>Unable to run mvn test, failing for randomizedtesting </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meghalidhoble</reporter><labels /><created>2016-05-31T11:58:05Z</created><updated>2016-08-02T07:06:29Z</updated><resolved>2016-08-02T07:06:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-31T13:59:47Z" id="222697036">which branch are you trying to build? Our tests are green on the 2.x end?
</comment><comment author="meghalidhoble" created="2016-06-01T05:22:51Z" id="222895696">This is with branch 2.3.
Also I missed to mention that I am observing this on the powerPC platform.
</comment><comment author="s1monw" created="2016-06-01T07:42:07Z" id="222917053">&gt; This is with branch 2.3.
&gt; Also I missed to mention that I am observing this on the powerPC platform.

ok can you provide the actual failures you are seeing? 
</comment><comment author="meghalidhoble" created="2016-06-01T08:13:22Z" id="222923285">**Here are the details of failure log:**

&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=3E0503E7B91CDFFF -Dtests.class=org.elasticsearch.plugins.PluginManagerPermissionTests -Dtests.method="testThatNonWritablePluginsDirectoryLeavesNoLeftOver" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=lt -Dtests.timezone=Europe/Ljubljana
&gt; FAILURE 0.65s J1 | PluginManagerPermissionTests.testThatNonWritablePluginsDirectoryLeavesNoLeftOver &lt;&lt;&lt;
&gt; Throwable #1: java.lang.AssertionError: Expected IOException due to read-only plugins/ directory
&gt;    at __randomizedtesting.SeedInfo.seed([3E0503E7B91CDFFF:C0C0DB6EA268E674]:0)
&gt;    at org.elasticsearch.plugins.PluginManagerPermissionTests.testThatNonWritablePluginsDirectoryLeavesNoLeftOver(PluginManagerPermissionTests.java:172)
&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=3E0503E7B91CDFFF -Dtests.class=org.elasticsearch.plugins.PluginManagerPermissionTests -Dtests.method="testThatUnaccessibleBinDirectoryAbortsPluginInstallation" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=lt -Dtests.timezone=Europe/Ljubljana
&gt; FAILURE 0.71s J1 | PluginManagerPermissionTests.testThatUnaccessibleBinDirectoryAbortsPluginInstallation &lt;&lt;&lt;
&gt; Throwable #1: java.lang.AssertionError: Expected IOException but did not happen
&gt;    at __randomizedtesting.SeedInfo.seed([3E0503E7B91CDFFF:B9027C256E2A9041]:0)
&gt;    at org.elasticsearch.plugins.PluginManagerPermissionTests.testThatUnaccessibleBinDirectoryAbortsPluginInstallation(PluginManagerPermissionTests.java:89)
&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   2&gt; NOTE: leaving temporary files on disk at: /root/meghali/elastic-search-port/elasticsearch/core/target/J1/temp/org.elasticsearch.plugins.PluginManagerPermissionTests_3E0503E7B91CDFFF-001
&gt;   2&gt; NOTE: test params are: codec=Asserting(Lucene54): {}, docValues:{}, sim=RandomSimilarity(queryNorm=true,coord=yes): {}, locale=lt, timezone=Europe/Ljubljana
&gt;   2&gt; NOTE: Linux 3.10.0-210.ael7a.ppc64le ppc64le/Oracle Corporation 1.8.0_91 (64-bit)/cpus=4,threads=1,free=337761568,total=528482304
&gt;   2&gt; NOTE: All tests run in this JVM: [PropertiesSettingsLoaderTests, BytesReferenceTests, NettySizeHeaderFrameDecoderTests, GeoDistanceTests, ChannelsTests, SimpleNumericTests, BlobStoreTests, PrioritizedExecutorsTests, EsExecutorsTests, SpecialPermissionTests, FieldStatsRequestTests, JavaVersionTests, DoubleIndexingDocTests, MultiPercolatorRequestTests, TranslogVersionTests, IndicesServiceTests, CompoundAnalysisTests, XContentBuilderTests, SameShardRoutingTests, SortedSetDVStringFieldDataTests, RoutingFieldTypeTests, NetworkModuleTests, DiskThresholdDeciderTests, JsonSettingsLoaderTests, InternalEngineSettingsTests, ASCIIFoldingTokenFilterFactoryTests, CopyOnWriteHashSetTests, RetryTests, SingleShardOneReplicaRoutingTests, HppcMapsTests, DefaultSourceMappingTests, IndexStoreBWCTests, ShingleTokenFilterFactoryTests, PutWarmerRequestTests, SimpleStringMappingTests, RestUtilsTests, ExpectedShardSizeAllocationTests, UpdateMappingTests, WildcardExpressionResolverTests, PriorityComparatorTests, StemmerTokenFilterFactoryTests, SnowballAnalyzerTests, FieldNamesFieldMapperTests, PrimaryElectionRoutingTests, VersionsTests, IndexFieldTypeTests, TimestampMappingTests, CodecTests, XContentHelperTests, BytesRefHashTests, ClusterStateToStringTests, CountDownTests, InternalSearchHitTests, ESExceptionTests, SimpleLuceneTests, Log4jESLoggerTests, NullValueTests, FieldDataCacheTests, RoutingServiceTests, DeflateCompressedStreamTests, SnapshotRequestsTests, BytesRestResponseTests, DeadNodesAllocationTests, SloppyMathTests, NettyTransportTests, JsonPathTests, PipelineAggregationHelperTests, DistanceUnitTests, JvmStatsTests, RestTableTests, RestTestParserTests, ClusterStateDiffPublishingTests, BigArraysTests, ThreadPoolTypeSettingsValidatorTests, CancellableThreadsTests, WordDelimiterTokenFilterFactoryTests, TenShardsOneReplicaRoutingTests, SimilarityTests, KeyedLockTests, NettyScheduledPingTests, TimestampFieldTypeTests, GatewayMetaStateTests, MetaDataIndexUpgradeServiceTests, DeflateXContentTests, IndexSearcherWrapperTests, NetworkServiceTests, CustomSeparatorBreakIteratorTests, BalanceUnbalancedClusterTests, CustomPassageFormatterTests, SimpleExternalMappingTests, GeoHashGridParserTests, JNANativesTests, NettyTransportMultiPortTests, IndexCacheModuleTests, FailedShardsRoutingTests, ShortFieldTypeTests, IndexStoreTests, SlicedInputStreamTests, UidFieldTypeTests, BulkShardRequestTests, AllFieldTypeTests, SizeValueTests, CommitPointsTests, IndexCacheableQueryTests, VectorHighlighterTests, ExtensionPointTests, RecoverySettingsTests, ClusterBlockTests, TerminalTests, BlendedTermQueryTests, FunctionScoreEquivalenceTests, HeadersAndContextCopyClientTests, AwarenessAllocationTests, ByteSizeValueTests, TransportBroadcastByNodeActionTests, ShadowEngineTests, PathTests, PostingsFormatTests, NettyHttpPublishPortTests, IndicesQueryCacheTests, SortParserTests, TribeServiceTests, ClusterChangedEventTests, ClusterRebalanceRoutingTests, AnalysisFactoryTests, JavaMultiFieldMergeTests, CountRequestBuilderTests, MultiDataPathUpgraderTests, RecoveriesCollectionTests, TemplateQueryBuilderTests, MetaStateServiceTests, CamelCaseFieldNameTests, BroadcastReplicationTests, RatioValueTests, CborXContentTests, NodeVersionAllocationDeciderTests, DateBackwardsCompatibilityTests, FsProbeTests, TransportReplicationActionTests, ESPolicyTests, GeoShapeQueryTests, PluginManagerPermissionTests]
&gt; Completed [423/558 (1!)] on J1 in 6.85s, 9 tests, 3 failures &lt;&lt;&lt; FAILURES!

I have attached the complete log file.
[log.txt](https://github.com/elastic/elasticsearch/files/292928/log.txt)
</comment><comment author="s1monw" created="2016-06-01T08:24:22Z" id="222925571">weird, what filesystem are you running on?
</comment><comment author="meghalidhoble" created="2016-06-01T08:31:54Z" id="222927292">It's running on xfs.
</comment><comment author="s1monw" created="2016-06-02T15:03:48Z" id="223320011">we also run on XFS and Rhel 7.2 I am not sure what's going on.. I also can't reproduce this problem sorry
</comment><comment author="wenpos" created="2016-06-28T08:12:39Z" id="228981805">hi, did you solve the problem? I also met the problem, thinks~
</comment><comment author="meghalidhoble" created="2016-06-28T10:05:40Z" id="229007125">Hi @wenpos,
Yes, I could resolve this problem, though I am sure of the root-cause of it.  There were some persmission issues when I reached to the test-case failure source code and found that its not expecting root user. 
Hence I did all the steps all over again, with non-root user and it worked fine for me. 
See if this helps you as well. 
</comment><comment author="wenpos" created="2016-06-29T08:39:00Z" id="229293238">@meghalidhoble
thank you for your advice O(&#8745;_&#8745;)O
</comment><comment author="danielmitterdorfer" created="2016-08-02T07:06:29Z" id="236819532">Closing as is was not reproducible on our end and @meghalidhoble could solve the problem locally. Please feel free to reopen if this should appear again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AWS IAM role not working with Elasticsearch 5.0.0-alpha2, but works with 2.3 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18652</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:  5.0.0-alpha2

**JVM version**: 1.8.0_91(1.8.0_91-b14)

**OS version**: Red Hat Enterprise Linux 7.2 (3.10.0-327.18.2.el7.x86_64)

**Description of the problem including expected versus actual behavior**:
I am trying to use AWS IAM role with Elasticsearch 5.0.0-alpha2 and EC2 discovery plug-in, but it does not seem to be working and I am getting below error:

&gt; "Exception while retrieving instance list from AWS API: Authorization header or parameters are not formatted correctly. (Service: AmazonEC2; Status Code: 401; Error Code: AuthFailure"

I am using below configuration with jdk8:

&gt; cluster.name: "test-cluster"
&gt; cloud.aws.region: "us-west-2"
&gt; cloud.aws.ec2.region: "us-west-2"
&gt; cloud.aws.ec2.protocol: "http"
&gt; discovery.type: "ec2"
&gt; # bootstrap.mlockall: true
&gt; 
&gt; node.master: true
&gt; node.data: false
&gt; node.name: ${HOSTNAME}-Master
&gt; discovery.zen.minimum_master_nodes: 1
&gt; network.host: ec2:privateIp
&gt; discovery.ec2.any_group: true
&gt; discovery.ec2.groups : sg-9d856tfe

And, below is IAM role permission that I have configured with elasticsearch instance:

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "ec2:Describe*"
            ],
            "Effect": "Allow",
            "Resource": [
                "*"
            ]
        }
    ]
}
```

However, same configuration works fine with with Elasticsearch 2.3 version. Has anyone faced the same issue with the latest Elasticsearch version 5.0.0-alpha2?

Also, today I enabled the debug mode of AWS call and I could notice(see below) that its loading credentials from **StaticCredentialsProvider** - its wrong behavior. As access and secret key is absent in the config(elasticsearch.yml) file, so ideally it should load credential from **InstanceProfileCredentialsProvider**.

&gt; [DEBUG][com.amazonaws.auth.AWSCredentialsProviderChain] Loading credentials from com.amazonaws.internal.StaticCredentialsProvider@40bf7b26

**Steps to reproduce**:
1. Start elasticsearch master data node with the above mentioned configuration.

**Provide logs (if relevant)**:

```
com.amazonaws.AmazonServiceException: Authorization header or parameters are not formatted correctly. (Service: AmazonEC2; Status Code: 401; Error Code: AuthFailure; Request ID: )
        at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1239)
        at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:823)
        at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:506)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:318)
        at com.amazonaws.services.ec2.AmazonEC2Client.invoke(AmazonEC2Client.java:11901)
        at com.amazonaws.services.ec2.AmazonEC2Client.describeInstances(AmazonEC2Client.java:5940)
        at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.fetchDynamicNodes(AwsEc2UnicastHostsProvider.java:117)
        at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:232)
        at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:217)
        at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:54)
        at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.buildDynamicNodes(AwsEc2UnicastHostsProvider.java:103)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:344)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:249)
        at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
        at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
        at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:845)
        at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:376)
        at org.elasticsearch.discovery.zen.ZenDiscovery.access$4500(ZenDiscovery.java:89)
        at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1166)
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="157643305">18652</key><summary>AWS IAM role not working with Elasticsearch 5.0.0-alpha2, but works with 2.3 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">randhirkr</reporter><labels><label>:Plugin Discovery EC2</label><label>bug</label></labels><created>2016-05-31T10:52:06Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-07-22T22:47:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="randhirkr" created="2016-05-31T16:17:34Z" id="222740320">Thanks Alex for fixing this issue. Looks like that was the issue even though key was empty, but not null, it was going to else flow and looking for credentials in **StaticCredentialsProvider**.

Let me re-install the **discovery-ec2** plug-in and test this. I will let you know, if any issue. Thanks again.
</comment><comment author="randhirkr" created="2016-05-31T16:40:10Z" id="222746780">Hi Alex,
How can I test your fix? I have removed the discovery-ec2 plug-in and installed again(as shown below), but getting the same error as above.

&gt; bin/elasticsearch-plugin remove discovery-ec2
&gt; bin/elasticsearch-plugin install discovery-ec2

Can you please help me here so that I can get the latest change w.r.t. discovery-ec2 plug-in? Thanks.
</comment><comment author="dadoonet" created="2016-05-31T17:18:48Z" id="222757394">Thanks a lot @randhirkr. I added you to the pioneer program.
</comment><comment author="dexterous" created="2016-07-14T20:08:17Z" id="232777232">@dadoonet do we have to be added to the pioneer program to get these fixes? If so, I'd like to get on too as I'm facing this issue as well. Also, what additional configuration will I have to setup WRT the pioneer program? Thanks. 
</comment><comment author="dadoonet" created="2016-07-14T20:43:58Z" id="232786572">The PR has not been merged yet so this is not fixed. :(
</comment><comment author="clintongormley" created="2016-07-15T08:45:14Z" id="232896273">@dexterous see https://www.elastic.co/blog/elastic-pioneer-program
</comment><comment author="Gmanweb" created="2016-11-04T14:39:15Z" id="258448804">@randhirkr I am having the exact same problem, what is the fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove index_writer_max_memory stat from segment stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18651</link><project id="" key="" /><description>With #14121 we now have a single shared pool for all shards for the indexing buffer, and we (IMC) periodically ask shards to move bytes to disk whenever the total indexing memory used across all shards exceeds the budget.

But this makes the `index_writer_max_memory` stat (added in #7440) pointless since it will always show the max value set in the code (currently 256 MB, but that can change).

We should remove the stat since it's now confusing and misleading, making it (falsely) appear like your shards in the worst case could use way too much heap.
</description><key id="157639779">18651</key><summary>Remove index_writer_max_memory stat from segment stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T10:31:17Z</created><updated>2016-06-28T09:30:28Z</updated><resolved>2016-05-31T13:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-31T12:03:59Z" id="222668688">LGTM. Since the indexing buffer settings is now a node level thing (shared and divided between active shards on the node) and it's settings are non trivial (10% of node memory, plus min and max) I wonder if it makes sense to add that to the node info?
</comment><comment author="mikemccand" created="2016-05-31T13:54:05Z" id="222695279">&gt; I wonder if it makes sense to add that to the node info?

+1, I think that makes sense, but I'll push this first and do that as a separate change.
</comment><comment author="bleskes" created="2016-05-31T13:54:58Z" id="222695525">++

&gt; On 31 May 2016, at 15:54, Michael McCandless notifications@github.com wrote:
&gt; 
&gt; I wonder if it makes sense to add that to the node info?
&gt; 
&gt; +1, I think that makes sense, but I'll push this first and do that as a separate change.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you commented.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `ignore_failure` option to all ingest processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18650</link><project id="" key="" /><description>If this option is enabled on a processor it silently catches any processor related failures and continues executing the rest of the pipeline.

 PR for #18493
</description><key id="157633993">18650</key><summary>Add `ignore_failure` option to all ingest processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T09:59:53Z</created><updated>2016-06-01T08:29:57Z</updated><resolved>2016-06-01T08:29:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-05-31T19:10:44Z" id="222789789">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18649</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="157630411">18649</key><summary>1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ElenaRen</reporter><labels /><created>2016-05-31T09:41:46Z</created><updated>2016-05-31T10:01:55Z</updated><resolved>2016-05-31T10:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-31T10:01:55Z" id="222645322">this looks like a mistake. Closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 6.0.1.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18648</link><project id="" key="" /><description>It was released a couple days ago. See http://lucene.apache.org/#28-may-2016-apache-lucene-601-and-apache-solr-601-available

Closes https://github.com/elastic/elasticsearch/issues/17535
</description><key id="157625806">18648</key><summary>Upgrade to Lucene 6.0.1.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>review</label><label>upgrade</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T09:18:56Z</created><updated>2016-06-28T09:30:03Z</updated><resolved>2016-06-01T08:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-05-31T09:24:42Z" id="222636812">LGTM
</comment><comment author="s1monw" created="2016-05-31T09:25:16Z" id="222636935">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a `_rollover` API to simplify time-based indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18647</link><project id="" key="" /><description>Today a lot of users make use of `time-based` indices to implement usecases like logging. The main reasons why this is desirable are:
- retention policies (certain data should be available for N days or weeks and can easily be deleted if entire indices can simply be dropped)
- indexing performance is important such that indices with a large amount of shards are created and rolled over on a daily or weekly basis.
- an index can not grow infinitely  (the number of shards can't be changed) and therefore the number of shards is picked to be sufficient for the time period.

Yet, this has several problems and the foremost is known as `shard-explosion` ie. to achieve the required indexing speed `12` shards are used with `1` replica each. These indices are _rolled over_ every day such that with a retention period of _1 year_ the user ends up with `24 * 365 = 8760` shards. In an ideal world we would be able to provide a system that allows to _fan out_ for indexing (have 20 shards) and roll the index over when the index is big enough to be optimal for a single shard to contain all docs but still be able to provide  desired search experience. Let's say a single shard is fine with up to _100M_ documents. That means we can sustain an indexing speed of _~12k doc/sec_ and end up with a single shard after _24h_ of constant indexing.  (all these number are just examples)
It's also common that these indices have peak times which can cause shards to fill up much quicker or there are certain days in the week where indexing is almost quite. At the end of the day the user would end up with a large number of shards that are far from ideal something like time is used to roll over indices. 

With the addition of `_shrink` API in #18270 going from X shards to 1 is easily possible such that users should be able to define their _ideal_ shard size and move from _N_ shards for fast indexing into 1 to prevent shard explosion. While making this entire process fully automatic is desired, it's also tricky since it requires state management of long running tasks. Instead, this issue proposes encapsulated  and stateless building blocks like a `_rollover` endpoint. Such an endpoint is basically just syntactic sugar that executes certain actions give some predicates, for instance:

the user creates an index with 2 aliases, one for search and one for indexing.

```
PUT logs
{
 &#160;"settings": {
 &#160; &#160;"number_of_shards": 5,
 &#160; &#160;"routing.allocation.include.box": "hot"
 &#160;},
 &#160;"aliases": {
 &#160; &#160;"logs_index": {},
 &#160; &#160;"logs_search": {}
 &#160;},
 &#160;"mappings": {}
}
```

the user then periodically calls the `_rollover` endpoint to potentially check if we need to flip the alias and roll over to a new index:

```
# creates copy of old index, templates can override 
POST logs_index/_rollover/{logs_search} # logs_index must be an alias with one index {logs_search} is optional where we add the index too
{
 &#160;"conditions": {
 &#160; &#160;"max_docs": 1000,
 &#160; &#160;"max_age": "7d",
 &#160; &#160;"max_size": "1g"
 &#160;}
}

# Response
{
 &#160;"old_index": "logs",
 &#160;"new_index": "logs-001"
}
```

The naming of the indices can either be determined by the rollover API or can be user provided which hasn't been decided yet. I personally would just append a numeric value to each index name like `logs_1, logs_2` etc. to simplify control with index templates to change `# of shards` etc. we can still add more complex solutions to the API later.  
Together with the `_shrink` API users can then move the index to a _shrink node_ and merge it to one shard.

In addition we  can also support a delete action to implement retention policies

```
DELETE logs_search/_rollover/ # logs_search must be an alias we go and check if it contains indices with a certain retention policy and take them out of the search alias and delete them
{
 &#160;"conditions": {
 &#160; &#160;"max_age": "1m"
 &#160;}
}
```
</description><key id="157625670">18647</key><summary>Add a `_rollover` API to simplify time-based indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Index APIs</label><label>:Index Templates</label><label>enhancement</label></labels><created>2016-05-31T09:18:17Z</created><updated>2016-06-23T16:53:22Z</updated><resolved>2016-06-17T15:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chenryn" created="2016-06-18T08:04:21Z" id="226928498">There is a problem for the rollover API with time-based indices: we can search the just one `-yyyy.MM.dd` index by calculate the time-range of query. Would it be slower if we must search the whole indices through the alias because the indices name don't contain the datetime?
</comment><comment author="clintongormley" created="2016-06-20T14:17:18Z" id="227155052">@chenryn you can use the field stats API (https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-field-stats.html) to figure out which indices should be queried.  Also, we've added an optimization on range queries which will check the min and max values against each shard and either (a) rewrite the range to a match_all if all documents fall within the range, (b) rewrite the range to a match_none if no documents fall into the range or (c) leave the range as is.

This will provide a big performance improvement.
</comment><comment author="chenryn" created="2016-06-20T15:33:25Z" id="227178272">@clintongormley tks for your reply. The optimization on range queries was implemented in es2.3 too?
</comment><comment author="clintongormley" created="2016-06-20T16:03:20Z" id="227187402">@chenryn no, only in 5
</comment><comment author="untergeek" created="2016-06-23T16:53:22Z" id="228112724">Using `_rollover` in Watcher is awesome.  I will add this functionality to Curator as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Remove unnecessary evil jarhell tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18646</link><project id="" key="" /><description>We have 3 evil tests for jarhell. They have been failing in java 9
because of how evil they are. The first checks the leniency we add for
jarhell in the jdk itself. This is unecessary, since if the leniency
wasn't there, we would already be failing all jarhell checks. The second
is checking the compile version is compatible with the jdk. This is
simpler since we don't need to fake the java version: we know 1.7 should
be compatibile with both java 8 and 9, so we can use that as a constant.
Finally the last test checks if the java version system property is
broken. This is simply something we should not check, we have to trust
that java specifies it correctly, and again, if it was broken, all
jarhell checks would be broken.
</description><key id="157588296">18646</key><summary>Tests: Remove unnecessary evil jarhell tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-05-31T04:35:14Z</created><updated>2016-05-31T07:08:37Z</updated><resolved>2016-05-31T07:08:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-31T07:06:49Z" id="222608487">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to show cluster settings "default" values ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18645</link><project id="" key="" /><description>Using `indices.recovery.max_bytes_per_sec` as an example, it defaults to `40mb`. If i make no change to this setting via the API and it's not defined in the `.yaml` then trying to view this value using something like `curl -XGET "http://localhost:9200/_cluster/settings?pretty"` just gives me 

```
{
  "persistent" : { },
  "transient" : { }
}
```

Is there anyway to show these default values pragmatically, instead of having to reference the documentation ? I wasn't able to find a way to query cluster settings, and return all current active settings. Not just those that have been user modified from their defaults, which i assume what `_cluster/settings` returns.

It would be nice if it also included settings that have been set via `elasticsearch.yml` and not just transient/persistent cluster settings changed via the API. 

Thanks
Scott
</description><key id="157588130">18645</key><summary>How to show cluster settings "default" values ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thechile</reporter><labels /><created>2016-05-31T04:33:03Z</created><updated>2016-05-31T07:11:26Z</updated><resolved>2016-05-31T07:11:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-31T07:11:26Z" id="222609292">Closed by #16054 - in 5.0 you can specify `?include_defaults=true` to get all settings including node defaults 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/elasticsearch script fails when grep color is on</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18644</link><project id="" key="" /><description>This line breaks for me https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch#L88

because I have `grep` globally configured to highlight matches. This means that the first match `-Xms256m` is actually sent to Java as `&lt;weird_encoded_-_char&gt;Xmx256m` and Java tries to run that as a class - and fails.

I can fix this by temporarily changing my grep settings, but it did cost me an hour or so tracking down what was happening. Therefore - for other users like me - you might want to modify the script: `grep color=never` should do the trick.

Also I see that there is a mix of `grep` and `egrep` in the script.

/cc @jasontedor
</description><key id="157553514">18644</key><summary>bin/elasticsearch script fails when grep color is on</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JnBrymn-EB</reporter><labels /><created>2016-05-30T20:31:18Z</created><updated>2016-05-31T01:51:10Z</updated><resolved>2016-05-31T01:26:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-30T22:36:27Z" id="222562064">I assume that you're doing this with `GREP_OPTIONS=--color=always` instead of via an alias? I'm assuming this because bash does not expand aliases by default in scripts, and we do not `shopt -s expand_aliases`.  In general, the use of global `--color=always` is going to lead to trouble; this is not the first time that I've seen it break something. In fact, the use of `GREP_OPTIONS` is discouraged and deprecated and will be remove in a future version of GNU `grep` exactly because it causes problems for otherwise portable scripts. A slightly better choice is `--color=auto` because then the coloring is only applied when the output is going directly to a terminal.

&gt; I can fix this by temporarily changing my grep settings, but it did cost me an hour or so tracking down what was happening.

I think that `bash -x ./bin/elasticsearch` would have helped quickly here. It does help to have seen this problem before though, sorry.

&gt; Therefore - for other users like me - you might want to modify the script: `grep color=never` should do the trick.

This requires GNU `grep`. Maybe this is something that we should require, but it is not something that we require today; note that [POSIX `grep` does not support `--color`](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/grep.html) and thus adding `--color=never` to our script is not a decision to be made lightly. Personally, I'm opposed, but I am just one voice.

&gt; Also I see that there is a mix of grep and egrep in the script.

This is a consequence of needing extended regular expressions in some places, and some `grep`s (non POSIX-compliant `grep`s on OS that we do _not_ support so I'm in favor of removing) do not support `grep -E`.

In general, I'm inclined to recommend that you change something on your end. As I mentioned, I've seen other scripts break because of global `grep` options of `--color=always`. At a minimum, you could change to an alias instead of `GREP_OPTIONS` but you should also consider `--color=auto`.
</comment><comment author="JnBrymn-EB" created="2016-05-31T01:23:15Z" id="222573347">Fair 'nuf. Close this ticket if you'd like. Perhaps I'm in the minority;
I'll change my grep to just use an alias.

-John

On Mon, May 30, 2016 at 5:37 PM, Jason Tedor notifications@github.com
wrote:

&gt; I assume that you're doing this with GREP_OPTIONS=--color=always instead
&gt; of via an alias? I'm assuming this because we bash does not expand aliases
&gt; by default in scripts, and we do not shopt -s expand_aliases. In general,
&gt; the use of global --color=always is going to lead to trouble; this is not
&gt; the first time that I've seen it break something. In fact, the use of
&gt; GREP_OPTIONS is discouraged and deprecated and will be remove in a future
&gt; version of grep exactly because it causes problems for otherwise portable
&gt; scripts. A slightly better choice is --color=auto because then the
&gt; coloring is only applied when the output is going directly to a terminal.
&gt; 
&gt; I can fix this by temporarily changing my grep settings, but it did cost
&gt; me an hour or so tracking down what was happening.
&gt; 
&gt; I think that bash -x ./bin/elasticsearch would have helped quickly here.
&gt; It does help to have seen this problem before though, sorry.
&gt; 
&gt; Therefore - for other users like me - you might want to modify the script: grep
&gt; color=never should do the trick.
&gt; 
&gt; This requires GNU grep. Maybe this is something that we should require,
&gt; but it is not something that we require today; note that POSIX grep does
&gt; not support --color http://pubs.opengroup.org/onlinepubs/9699919799/
&gt; and thus adding it to our script is not a decision to be made lightly.
&gt; 
&gt; Also I see that there is a mix of grep and egrep in the script.
&gt; 
&gt; This is a consequence of needing extended regular expressions in some
&gt; places, and some greps (non POSIX-compliant greps on OS that we do _not_
&gt; support so I'm in favor of removing) do not support grep -E.
&gt; 
&gt; In general, I'm inclined to recommend that you to change something on your
&gt; end. As I mentioned, I've seen other scripts break because of global grep
&gt; options of --color=always.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18644#issuecomment-222562064,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AKx9NWZDiOTUk4Nv7yQxoiu2D_MID7LDks5qG2azgaJpZM4IqBPd
&gt; .
</comment><comment author="jasontedor" created="2016-05-31T01:26:35Z" id="222573659">Thanks for understanding @JnBrymn-EB. :smile:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] CorruptedFileIT.testCorruptionOnNetworkLayerFinalizingRecovery failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18643</link><project id="" key="" /><description>This test failed today: http://build-us-00.elastic.co/job/es_core_master_window-2012/3148/

**Error Message**

timed out waiting for green state

**Stacktrace**

``` java
java.lang.AssertionError: timed out waiting for green state
    at __randomizedtesting.SeedInfo.seed([D8778D71DDB3117C:6E677D54B1277DBF]:0)
    at org.junit.Assert.fail(Assert.java:88)
    at org.elasticsearch.test.ESIntegTestCase.ensureGreen(ESIntegTestCase.java:876)
    at org.elasticsearch.test.ESIntegTestCase.ensureGreen(ESIntegTestCase.java:861)
    at org.elasticsearch.index.store.CorruptedFileIT.testCorruptionOnNetworkLayerFinalizingRecovery(CorruptedFileIT.java:365)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)


```
</description><key id="157529530">18643</key><summary>[CI] CorruptedFileIT.testCorruptionOnNetworkLayerFinalizingRecovery failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Recovery</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-05-30T16:16:03Z</created><updated>2016-05-30T19:57:53Z</updated><resolved>2016-05-30T19:57:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-30T19:57:53Z" id="222547501">closed with https://github.com/elastic/elasticsearch/commit/38bee27b112b62cfd227a86eb330a0349958f190
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add native sql support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18642</link><project id="" key="" /><description>There are many use cases where SQL access to elasticsearch is desired.  There are a few possibilities currently possible:

https://github.com/NLPchina/elasticsearch-sql
which is a plugin that gives some sql abilities and

https://crate.io/a/sql-for-elasticsearch/
which is really a sql database built on top of elasticsearch (it doesn't give you sql access to existing elasticsearch data - you have to use crate completely.)

It seems that Solr has sql support:
https://cwiki.apache.org/confluence/display/solr/Parallel+SQL+Interface
so it seems it should be possible to add to elasticsearch also.

The advantages of this being integrated over a plugin might be performance and just "out of the box" support which would make elasticseach more appealing for business use cases, especially if they are using sql-based graphing/reporting solutions (eg, tableau, periscope)
</description><key id="157523168">18642</key><summary>add native sql support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yehosef</reporter><labels><label>feature</label><label>high hanging fruit</label></labels><created>2016-05-30T15:29:36Z</created><updated>2016-06-10T09:30:15Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="evanv" created="2016-06-02T16:30:38Z" id="223346706">It's interesting to note that one of the bigger drivers of Spark adoption has been their SQL syntax support on the DataFrame API (they're expanding SQL support in 2 actually because of that, eg, https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html).

Just throwing that out there... actually have mixed feelings about this, personally. ES's DSL is far richer than SQL or SQL plus a few more commands. But then again, exposing SQL-like queries probably would drive adoption quite a bit and those non-ES users that adopt it probably would get more comfortable with the ES DSL if they had a SQL-like bridge to help them start using it. 
</comment><comment author="yehosef" created="2016-06-03T08:16:15Z" id="223518587">It's also valuable for tooling that supports sql like tableau - and it makes it easier for things like apache Drill to use elasticsearch in that stack.
</comment><comment author="ajaybhatnagar" created="2016-06-03T15:08:40Z" id="223605377">It will be good to have sql like interface as it helps migrating/adopting from RDBMS setups with lower training/adoptions needs of the organizations.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation for `quote_field_suffix`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18641</link><project id="" key="" /><description>I stumbled upon this feature of the `query_string` query, which seems useful eg to redirect quoted queries to a field that does not perform stemming. But it does not seem to be documented.

Example:

``` json
DELETE test 

PUT test 
{
  "mappings": {
    "test": {
      "properties": {
        "foo": {
          "type": "string",
          "analyzer": "english",
          "fields": {
            "exact": {
              "type": "string",
              "analyzer": "standard"
            }
          }
        }
      }
    }
  }
}

PUT test/test/1
{
  "foo": "table"
}

PUT test/test/2
{
  "foo": "tables"
}


### Matches both documents
GET test/_search
{
  "query": {
    "query_string": {
      "default_field": "foo",
      "quote_field_suffix": ".exact",
      "query": "tables"
    }
  }
}

### Matches only the 2nd document
GET test/_search
{
  "query": {
    "query_string": {
      "default_field": "foo",
      "quote_field_suffix": ".exact",
      "query": "\"tables\""
    }
  }
}
```
</description><key id="157518515">18641</key><summary>Documentation for `quote_field_suffix`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2016-05-30T14:59:44Z</created><updated>2016-10-28T07:11:57Z</updated><resolved>2016-10-28T07:11:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Recovery throttling does not properly account for relocating non-primary shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18640</link><project id="" key="" /><description>Relocation of non-primary shards is realized by recovering from the primary shard. `RoutingNodes` wrongly equates non-primary relocation as recovering a shard from the non-primary relocation source, however. This invalidates the checks in the `ThrottlingAllocationDecider` where the number of concurrent incoming/outgoing recoveries are bound using the settings `cluster.routing.allocation.node_concurrent_recoveries`, `cluster.routing.allocation.node_concurrent_incoming_recoveries` and `cluster.routing.allocation.node_concurrent_outgoing_recoveries`.
</description><key id="157516356">18640</key><summary>Recovery throttling does not properly account for relocating non-primary shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-05-30T14:46:29Z</created><updated>2016-06-03T12:11:34Z</updated><resolved>2016-06-03T12:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-30T14:53:23Z" id="222508802">+1 good find
</comment><comment author="jonaf" created="2016-06-01T19:55:12Z" id="223105909">@ywelsch What version(s) of Elasticsearch does this bug affect?
</comment><comment author="ywelsch" created="2016-06-02T14:24:22Z" id="223307645">@jonaf This only affects v5, previous versions used other mechanisms to throttle recoveries (see https://www.elastic.co/guide/en/elasticsearch/reference/2.3/recovery.html and in particular `indices.recovery.concurrent_streams`)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No need to inject the ClusterService into IndicesService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18639</link><project id="" key="" /><description>It's already available where it's needed. One less guice dependency.
</description><key id="157515664">18639</key><summary>No need to inject the ClusterService into IndicesService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-05-30T14:42:17Z</created><updated>2016-05-30T14:56:02Z</updated><resolved>2016-05-30T14:55:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-30T14:48:58Z" id="222507996">ok, can you actually remove it from the IndicesService constructor parameters?
</comment><comment author="bleskes" created="2016-05-30T14:51:30Z" id="222508459">@ywelsch i'm sleeping.  pushed an update.
</comment><comment author="s1monw" created="2016-05-30T14:52:13Z" id="222508593">LGTM
</comment><comment author="ywelsch" created="2016-05-30T14:54:48Z" id="222509057">LGTM2
</comment><comment author="bleskes" created="2016-05-30T14:56:02Z" id="222509291">thx @ywelsch @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Process dynamic templates in order.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18638</link><project id="" key="" /><description>When calling `findTemplateBuilder(context, currentFieldName, "text", null)`,
elasticsearch ignores all templates that have a `match_mapping_type` set since
no dynamic type is provided (the last parameter, which is null in that case).
So this should only be called _last_. Otherwise, if a path-based template
matches, it will have precedence over all type-based templates.

Closes #18625
Closes #2401
</description><key id="157514510">18638</key><summary>Process dynamic templates in order.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-05-30T14:35:41Z</created><updated>2016-05-31T10:13:08Z</updated><resolved>2016-05-31T07:01:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-30T19:23:02Z" id="222543916">I looked at it and I think I understand what's going on so LGTM but I think @rjernst should also look
</comment><comment author="rjernst" created="2016-05-30T20:51:38Z" id="222553083">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud azure plugin crashes on start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18637</link><project id="" key="" /><description>**Elasticsearch version**:
2.3

**JVM version**:
1.8.0_91-b14

**OS version**:
Ubuntu 16.04

**Description of the problem including expected versus actual behavior**:
I have the following config:

```
discovery:
    type: azure

cloud:
    azure:
        management:
             subscription.id: [...]
             cloud.service.name: [...]
             keystore:
                   path: /home/elasticsearch/azurekeystore.pkcs12
                   password: [...]
                   type: pkcs12
```

Now when I try to start elasticsearch it fails and I find the following error in the logs:

```
[2016-05-30 12:59:49,219][ERROR][bootstrap                ] Guice Exception: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "getClassLoader")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.ClassLoader.checkClassLoaderPermission(ClassLoader.java:1528)
        at java.lang.Thread.getContextClassLoader(Thread.java:1436)
        at com.microsoft.windowsazure.Configuration.load(Configuration.java:104)
        at com.microsoft.windowsazure.Configuration.getInstance(Configuration.java:90)
        at com.microsoft.windowsazure.management.configuration.ManagementConfiguration.configure(ManagementConfiguration.java:134)
        at org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl.&lt;init&gt;(AzureComputeServiceImpl.java:75)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:213)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:140)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

any idea what I'm doing wrong here?
If I leave out the pasted piece of the config, elastic fires up just fine.
</description><key id="157499065">18637</key><summary>Cloud azure plugin crashes on start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">Burgov</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-05-30T13:04:23Z</created><updated>2016-09-14T14:44:25Z</updated><resolved>2016-06-28T08:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-30T13:33:09Z" id="222493652">Just want to make sure: are you using elasticsearch 2.3.2 or another version?
</comment><comment author="Burgov" created="2016-05-30T13:38:35Z" id="222494652">@dadoonet  it's v2.3.3
</comment><comment author="dadoonet" created="2016-05-30T14:31:56Z" id="222504778">So I'm able to reproduce it. I'll create a PR for this and hopefully I'll give a workaround for it.
</comment><comment author="Burgov" created="2016-05-30T18:28:39Z" id="222538212">Thanks! Curious to see the PR to learn what causes it

(and good to know it's not me doing something stupid :) )
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Ansible playbook to Integrations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18636</link><project id="" key="" /><description>We have https://github.com/elastic/ansible-elasticsearch, but it's not listed in https://www.elastic.co/guide/en/elasticsearch/plugins/master/integrations.html#_supported_by_elasticsearch_2, although Puppet and Chef are.

It probably should be.
</description><key id="157498612">18636</key><summary>Add Ansible playbook to Integrations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwurm</reporter><labels><label>docs</label></labels><created>2016-05-30T13:01:27Z</created><updated>2016-06-06T12:31:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T16:15:18Z" id="223044778">Sure.  Want to send a PR?
</comment><comment author="cwurm" created="2016-06-06T12:31:22Z" id="223945185">@clintongormley sure, but I'm not sure: Does it fall under "Supported by Elasticsearch" or "Supported by community"?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Histogram aggregation causes OOM on 2.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18635</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.3.2

**JVM version**:
java version "1.7.0_75"
Java(TM) SE Runtime Environment (build 1.7.0_75-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.75-b04, mixed mode)

**OS version**:
Kernel: 3.10.0-229.20.1.el7.x86_64 
CentOS Linux release 7.1.1503 (Core)

**Description of the problem including expected versus actual behavior**:
When migrating our environment (staging first) from 1.7.2 to 2.3 - identical Elasticsearch aggregations query (used for facets) causes OOM on 2.3.

In particular when aggregating 'price' field using histogram aggregation, Elastic eats up all HEAP and ignores both fielddata.cache.size and indices.breaker.fielddata.limit. For fielddata.cache.size it makes sense since evaluation whether records will fit into limited cache are evaluated after loading them. For indices.breaker.fielddata.limit it should work not.

**Cluster stats**

``` text
/_cat/indices
green open sb-core-items_20160331085949                 1 0 40237 19961  35.2mb  35.2mb
```

Only 40k documents.

``` bash
[server ~]$  free -mh
              total        used        free      shared  buff/cache   available
Mem:           125G         76G        7.8G        3.9G         41G         43G
Swap:          2.0G         25M        2.0G
```

2 x Intel(R) Xeon(R) CPU E5-2680 0 @ 2.70GHz, cpu cores : 8

For 1.7.2 Elastic had ~5Gb of memory and was performing without slightest issue, after upgrading to 2.3. - 30Gb is not enough.

**Indice settings**

``` json
/sb-core-items/_settings
{
   "sb-core-items_20160331085949":{
      "settings":{
         "index":{
            "creation_date":"1459414789778",
            "persistent":{
               "indices":{
                  "breaker":{
                     "total":{
                        "limit":"40%"
                     },
                     "fielddata":{
                        "limit":"20%"
                     }
                  },
                  "fielddata":{
                     "cache":{
                        "size":"10%"
                     }
                  }
               }
            },
            "number_of_replicas":"0",
            "transient":{
               "indices":{
                  "store":{
                     "throttle":{
                        "type":"merge"
                     }
                  }
               }
            },
            "indices":{
               "fielddata":{
                  "cache":{
                     "size":"20%"
                  }
               }
            },
            "uuid":"zaPUAqq2Sku4GbQ3Yug9wA",
            "analysis":{},
            "number_of_shards":"1",
            "refresh_interval":"1s",
            "version":{
               "created":"1070499",
               "upgraded":"2030199"
            },
            "legacy":{
               "routing":{
                  "use_type":"false",
                  "hash":{
                     "type":"org.elasticsearch.cluster.routing.DjbHashFunction"
                  }
               }
            }
         }
      }
   }
}

```

**Steps to reproduce**:
1) Aggregate by similar query 

``` json
{
   "query":{
      "function_score":{
         "boost_mode":"sum",
         "functions":[

         ],
         "query":{
            "constant_score":{
               "query":{
                  "match":{
                     "_all":{
                        "query":"",
                        "analyzer":"default_search",
                        "operator":"and",
                        "zero_terms_query":"all"
                     }
                  }
               }
            }
         },
         "filter":{
            "bool":{
               "must":[

               ],
               "must_not":[

               ],
               "should":[

               ]
            }
         }
      }
   },
   "from":0,
   "size":0,
   "fields":[

   ],
   "aggs":{
      "catalogs":{
         "terms":{
            "field":"catalog_ids",
            "size":0
         },
         "aggs":{
            "sizes":{
               "terms":{
                  "field":"size_id",
                  "size":0
               }
            }
         }
      },
      "brands":{
         "terms":{
            "field":"brand_id",
            "size":100
         }
      },
      "colors":{
         "terms":{
            "field":"color_ids",
            "size":0
         }
      },
      "statuses":{
         "terms":{
            "field":"status_id",
            "size":0
         }
      },
      "disposal_conditions":{
         "terms":{
            "field":"disposal_conditions",
            "size":0
         }
      },
      "min_price":{
         "min":{
            "field":"price"
         }
      },
      "max_price":{
         "max":{
            "field":"price"
         }
      },
      "price":{
         "histogram":{
            "field":"price",
            "interval":1
         }
      },
      "price_ranges":{
         "range":{
            "field":"price",
            "ranges":[
               {
                  "from":0,
                  "to":1
               },
               {
                  "from":1,
                  "to":2
               },
               {
                  "from":2,
                  "to":3
               },
               {
                  "from":3,
                  "to":4
               },
               {
                  "from":4,
                  "to":5
               },
               {
                  "from":5,
                  "to":6
               },
               {
                  "from":6,
                  "to":7
               },
               {
                  "from":7,
                  "to":8
               },
               {
                  "from":8,
                  "to":9
               },
               {
                  "from":9,
                  "to":10
               },
               {
                  "from":10,
                  "to":12
               },
               {
                  "from":12,
                  "to":14
               },
               {
                  "from":14,
                  "to":16
               },
               {
                  "from":16,
                  "to":18
               },
               {
                  "from":18,
                  "to":20
               },
               {
                  "from":20,
                  "to":26
               },
               {
                  "from":26,
                  "to":32
               },
               {
                  "from":32,
                  "to":38
               },
               {
                  "from":38,
                  "to":44
               },
               {
                  "from":44,
                  "to":50
               },
               {
                  "from":50,
                  "to":60
               },
               {
                  "from":60,
                  "to":70
               },
               {
                  "from":70,
                  "to":80
               },
               {
                  "from":80,
                  "to":90
               },
               {
                  "from":90,
                  "to":100
               },
               {
                  "from":100,
                  "to":200
               },
               {
                  "from":200,
                  "to":300
               },
               {
                  "from":300,
                  "to":400
               },
               {
                  "from":400,
                  "to":500
               },
               {
                  "from":500,
                  "to":600
               }
            ]
         }
      },
      "videos_count":{
         "terms":{
            "field":"videos_count"
         }
      }
   }
}
```

2) This aggregation causes OOM.

``` json
      "price":{
         "histogram":{
            "field":"price",
            "interval":1
         }
      }
```

Logs 

``` text
[2016-04-27 12:56:36,558][DEBUG][action.admin.cluster.node.stats] [server.company.net] failed to execute on node [kLtivSu5RuKGo2zxhFeOPA]
ReceiveTimeoutTransportException[[server.company.net][ADDRESS:9300][cluster:monitor/nodes/stats[n]] request_id [320916] timed out after [19749ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:679)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2016-04-27 12:56:36,567][WARN ][transport                ] [server.company.net] Received response for a request that has timed out, sent [56327ms] ago, timed out [36578ms] ago, action [cluster:monitor/nodes/stats[n]], node [{server.company.net}{kLtivSu5RuKGo2zxhFeOPA}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id [320916]
[2016-04-27 12:56:36,568][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600780][18] duration [38.3s], collections [1]/[38.4s], total [38.3s]/[11.4m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [189.2mb]-&gt;[189.6mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 12:56:56,076][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600781][19] duration [19.4s], collections [1]/[19.4s], total [19.4s]/[11.7m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [189.6mb]-&gt;[191.3mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 12:57:31,607][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600782][20] duration [35.4s], collections [1]/[35.5s], total [35.4s]/[12.3m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 12:58:26,742][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600783][22] duration [55s], collections [2]/[19.7s], total [55s]/[13.3m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 12:59:38,785][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600784][24] duration [52.2s], collections [2]/[1.4m], total [52.2s]/[14.1m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.1mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 13:02:01,595][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600785][28] duration [1.7m], collections [4]/[1.7m], total [1.7m]/[15.9m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [191.1mb]-&gt;[191.3mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 13:05:45,925][DEBUG][action.search            ] [server.company.net] [sb-core-items_20160331085949][0]: Failed to execute [org.elasticsearch.action.search.SearchRequest@396f02fe] while moving to second phase
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:2219)
    at java.util.ArrayList.grow(ArrayList.java:242)
    at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:216)
    at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:208)
    at java.util.ArrayList.add(ArrayList.java:457)
    at java.util.ArrayList$ListItr.add(ArrayList.java:914)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.addEmptyBuckets(InternalHistogram.java:437)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.doReduce(InternalHistogram.java:466)
    at org.elasticsearch.search.aggregations.InternalAggregation.reduce(InternalAggregation.java:153)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:170)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:411)
    at org.elasticsearch.action.search.SearchCountAsyncAction.moveToSecondPhase(SearchCountAsyncAction.java:59)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.innerMoveToSecondPhase(AbstractSearchAsyncAction.java:374)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:171)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onResponse(AbstractSearchAsyncAction.java:147)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onResponse(AbstractSearchAsyncAction.java:144)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:41)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:819)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:803)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:793)
    at org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:58)
    at org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:134)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:369)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2016-04-27 13:06:21,761][WARN ][rest.suppressed          ] /sb-core-items/item/_search Params: {index=sb-core-items, query_cache=true, type=item, search_type=count, timeout=800ms}
Failed to execute phase [query], [reduce]
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:176)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onResponse(AbstractSearchAsyncAction.java:147)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onResponse(AbstractSearchAsyncAction.java:144)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:41)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:819)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:803)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:793)
    at org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:58)
    at org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:134)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:369)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:2219)
    at java.util.ArrayList.grow(ArrayList.java:242)
    at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:216)
    at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:208)
    at java.util.ArrayList.add(ArrayList.java:457)
    at java.util.ArrayList$ListItr.add(ArrayList.java:914)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.addEmptyBuckets(InternalHistogram.java:437)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.doReduce(InternalHistogram.java:466)
    at org.elasticsearch.search.aggregations.InternalAggregation.reduce(InternalAggregation.java:153)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:170)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:411)
    at org.elasticsearch.action.search.SearchCountAsyncAction.moveToSecondPhase(SearchCountAsyncAction.java:59)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.innerMoveToSecondPhase(AbstractSearchAsyncAction.java:374)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:171)
    ... 17 more
[2016-04-27 13:06:41,466][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600786][34] duration [2.7m], collections [6]/[5.2m], total [2.7m]/[18.7m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 13:07:17,357][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600787][36] duration [55.5s], collections [2]/[55.5s], total [55.5s]/[19.6m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[190.4mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 13:08:31,753][WARN ][monitor.jvm              ] [server.company.net] [gc][old][600788][38] duration [54.7s], collections [2]/[54.8s], total [54.7s]/[20.5m], memory [9.8gb]-&gt;[9.8gb]/[9.8gb], all_pools {[young] [1.4gb]-&gt;[1.4gb]/[1.4gb]}{[survivor] [190.4mb]-&gt;[191.1mb]/[191.3mb]}{[old] [8.1gb]-&gt;[8.1gb]/[8.1gb]}
[2016-04-27 13:11:19,646][INFO ][node                     ] [server.company.net] version[2.3.1], pid[43512], build[bd98092/2016-04-04T12:25:05Z]
[2016-04-27 13:11:19,647][INFO ][node                     ] [server.company.net] initializing ...
[2016-04-27 13:11:20,175][INFO ][plugins                  ] [server.company.net] modules [lang-groovy, reindex, lang-expression], plugins [], sites []
[2016-04-27 13:11:20,201][INFO ][env                      ] [server.company.net] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [381.1gb], net total_space [444gb], spins? [unknown], types [rootfs]
[2016-04-27 13:11:20,201][INFO ][env                      ] [server.company.net] heap size [9.8gb], compressed ordinary object pointers [true]
[2016-04-27 13:11:20,201][WARN ][env                      ] [server.company.net] max file descriptors [64000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2016-04-27 13:11:21,384][DEBUG][discovery.zen.elect      ] [server.company.net] using minimum_master_nodes [1]
[2016-04-27 13:11:21,385][DEBUG][discovery.zen.ping.unicast] [server.company.net] using initial hosts [ADDRESS], with concurrent_connects [10]
[2016-04-27 13:11:21,391][DEBUG][discovery.zen            ] [server.company.net] using ping.timeout [3s], join.timeout [30s], master_election.filter_client [true], master_election.filter_data [false]
[2016-04-27 13:11:21,392][DEBUG][discovery.zen.fd         ] [server.company.net] [master] uses ping_interval [30s], ping_timeout [30s], ping_retries [3]
[2016-04-27 13:11:21,394][DEBUG][discovery.zen.fd         ] [server.company.net] [node  ] uses ping_interval [30s], ping_timeout [30s], ping_retries [3]
[2016-04-27 13:11:22,078][INFO ][node                     ] [server.company.net] initialized
[2016-04-27 13:11:22,078][INFO ][node                     ] [server.company.net] starting ...
[2016-04-27 13:11:22,239][INFO ][transport                ] [server.company.net] publish_address {ADDRESS:9300}, bound_addresses {ADDRESS:9300}
[2016-04-27 13:11:22,244][INFO ][discovery                ] [server.company.net] core-sandbox/Mwvomz5uTlu7KX3F6xDJ1w
[2016-04-27 13:11:22,247][TRACE][discovery.zen            ] [server.company.net] starting to accumulate joins
[2016-04-27 13:11:22,247][TRACE][discovery.zen            ] [server.company.net] starting to ping
[2016-04-27 13:11:22,249][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-04-27 13:11:22,253][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[2], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-04-27 13:11:22,253][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-04-27 13:11:22,253][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[3], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-04-27 13:11:37,254][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-04-27 13:11:37,255][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[5], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-04-27 13:11:37,255][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-04-27 13:11:37,256][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[6], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-04-27 13:11:52,246][WARN ][discovery                ] [server.company.net] waited for 30s and no initial state was set by the discovery
[2016-04-27 13:11:52,257][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-04-27 13:11:52,258][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[7], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[8], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-04-27 13:11:52,258][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-04-27 13:11:52,259][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[7], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[7], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[9], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-04-27 13:11:52,259][TRACE][discovery.zen            ] [server.company.net] full ping responses: {none}
[2016-04-27 13:11:52,260][DEBUG][discovery.zen            ] [server.company.net] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2016-04-27 13:11:52,260][DEBUG][discovery.zen            ] [server.company.net] elected as master, waiting for incoming joins ([0] needed)
[2016-04-27 13:11:52,268][INFO ][cluster.service          ] [server.company.net] new_master {server.company.net}{Mwvomz5uTlu7KX3F6xDJ1w}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-04-27 13:11:52,272][INFO ][http                     ] [server.company.net] publish_address {185.68.39.13:9200}, bound_addresses {0.0.0.0:9200}
[2016-04-27 13:11:52,272][INFO ][node                     ] [server.company.net] started
[2016-04-27 13:11:52,273][TRACE][discovery.zen            ] [server.company.net] stopping join accumulation ([election closed])
[2016-04-27 13:11:52,273][TRACE][discovery.zen            ] [server.company.net] cluster joins counter set to [1] (elected as master)
[2016-04-27 13:11:52,473][INFO ][gateway                  ] [server.company.net] recovered [40] indices into cluster_state
[2016-04-27 13:11:53,533][DEBUG][action.search            ] [server.company.net] All shards failed for phase: [query_fetch]
[sb_it-core-items_20151217122807][[sb_it-core-items_20151217122807][0]] NoShardAvailableActionException[null]
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:129)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:115)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:47)
    at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:149)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:582)
    at org.elasticsearch.rest.action.search.RestSearchAction.handleRequest(RestSearchAction.java:85)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:205)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:449)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:61)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2016-04-27 13:11:53,536][WARN ][rest.suppressed          ] /sb_it-core-items/item/_search Params: {index=sb_it-core-items, type=item, timeout=800ms}
Failed to execute phase [query_fetch], all shards failed
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:129)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:115)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:47)
    at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:149)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:582)
    at org.elasticsearch.rest.action.search.RestSearchAction.handleRequest(RestSearchAction.java:85)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:205)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:449)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:61)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

3) This can be reproduced by generating up to 40k documents with price field and running above aggregation. 

**Logs**:

``` text
[2016-05-17 09:34:10,777][INFO ][node                     ] [server.company.net] version[2.3.2], pid[42797], build[b9e4a6a/2016-04-21T16:03:47Z]
[2016-05-17 09:34:10,777][INFO ][node                     ] [server.company.net] initializing ...
[2016-05-17 09:34:11,243][INFO ][plugins                  ] [server.company.net] modules [lang-groovy, reindex, lang-expression], plugins [], sites []
[2016-05-17 09:34:11,274][INFO ][env                      ] [server.company.net] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [294.5gb], net total_space [444gb], spins? [unknown], types [rootfs]
[2016-05-17 09:34:11,274][INFO ][env                      ] [server.company.net] heap size [30.8gb], compressed ordinary object pointers [true]
[2016-05-17 09:34:11,274][WARN ][env                      ] [server.company.net] max file descriptors [64000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2016-05-17 09:34:12,470][DEBUG][discovery.zen.elect      ] [server.company.net] using minimum_master_nodes [1]
[2016-05-17 09:34:12,471][DEBUG][discovery.zen.ping.unicast] [server.company.net] using initial hosts [ADDRESS], with concurrent_connects [10]
[2016-05-17 09:34:12,477][DEBUG][discovery.zen            ] [server.company.net] using ping.timeout [3s], join.timeout [30s], master_election.filter_client [true], master_election.filter_data [false]
[2016-05-17 09:34:12,478][DEBUG][discovery.zen.fd         ] [server.company.net] [master] uses ping_interval [30s], ping_timeout [30s], ping_retries [3]
[2016-05-17 09:34:12,481][DEBUG][discovery.zen.fd         ] [server.company.net] [node  ] uses ping_interval [30s], ping_timeout [30s], ping_retries [3]
[2016-05-17 09:34:13,190][INFO ][node                     ] [server.company.net] initialized
[2016-05-17 09:34:13,191][INFO ][node                     ] [server.company.net] starting ...
[2016-05-17 09:34:13,435][INFO ][transport                ] [server.company.net] publish_address {ADDRESS:9300}, bound_addresses {ADDRESS:9300}
[2016-05-17 09:34:13,440][INFO ][discovery                ] [server.company.net] core-sandbox/t5OCpCojSAadoVUVZJwbbg
[2016-05-17 09:34:13,445][TRACE][discovery.zen            ] [server.company.net] starting to accumulate joins
[2016-05-17 09:34:13,446][TRACE][discovery.zen            ] [server.company.net] starting to ping
[2016-05-17 09:34:13,447][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-05-17 09:34:13,451][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[2], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-05-17 09:34:13,451][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-05-17 09:34:13,452][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[3], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-05-17 09:34:28,452][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-05-17 09:34:28,454][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[5], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-05-17 09:34:28,454][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-05-17 09:34:28,454][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[6], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-05-17 09:34:43,444][WARN ][discovery                ] [server.company.net] waited for 30s and no initial state was set by the discovery
[2016-05-17 09:34:43,456][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-05-17 09:34:43,457][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[7], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[8], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-05-17 09:34:43,457][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] sending to {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}
[2016-05-17 09:34:43,457][TRACE][discovery.zen.ping.unicast] [server.company.net] [1] received response from {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}: [ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[1], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[4], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[7], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[7], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}, ping_response{node [{server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}], id[9], master [null], hasJoinedOnce [false], cluster_name[core-sandbox]}]
[2016-05-17 09:34:43,458][TRACE][discovery.zen            ] [server.company.net] full ping responses: {none}
[2016-05-17 09:34:43,458][DEBUG][discovery.zen            ] [server.company.net] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2016-05-17 09:34:43,458][DEBUG][discovery.zen            ] [server.company.net] elected as master, waiting for incoming joins ([0] needed)
[2016-05-17 09:34:43,468][INFO ][http                     ] [server.company.net] publish_address {ADDRESS:9200}, bound_addresses {0.0.0.0:9200}
[2016-05-17 09:34:43,469][INFO ][node                     ] [server.company.net] started
[2016-05-17 09:34:43,469][INFO ][cluster.service          ] [server.company.net] new_master {server.company.net}{t5OCpCojSAadoVUVZJwbbg}{ADDRESS}{ADDRESS:9300}{max_local_storage_nodes=1}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-05-17 09:34:43,472][TRACE][discovery.zen            ] [server.company.net] stopping join accumulation ([election closed])
[2016-05-17 09:34:43,473][TRACE][discovery.zen            ] [server.company.net] cluster joins counter set to [1] (elected as master)
[2016-05-17 09:34:43,596][INFO ][indices.breaker          ] [server.company.net] Updated breaker settings fielddata: [fielddata,type=MEMORY,limit=6623369625/6.1gb,overhead=1.03]
[2016-05-17 09:34:43,596][INFO ][indices.breaker          ] [server.company.net] Updated breaker settings parent: [parent,type=PARENT,limit=9935054438/9.2gb,overhead=1.0]
[2016-05-17 09:34:43,678][INFO ][gateway                  ] [server.company.net] recovered [40] indices into cluster_state
[2016-05-17 09:34:43,958][DEBUG][action.search            ] [server.company.net] All shards failed for phase: [query_fetch]
RemoteTransportException[[server.company.net][ADDRESS:9300][indices:data/read/search[phase/query+fetch]]]; nested: ShardNotFoundException[no such shard];
Caused by: [sb-core-items_20160331085949][[sb-core-items_20160331085949][0]] ShardNotFoundException[no such shard]
    at org.elasticsearch.index.IndexService.shardSafe(IndexService.java:197)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:639)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:620)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:463)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2016-05-17 09:34:43,964][WARN ][rest.suppressed          ] /sb-core-items/item/_search Params: {index=sb-core-items, type=item, timeout=800ms}
Failed to execute phase [query_fetch], all shards failed
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:855)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:833)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:387)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2016-05-17 09:34:46,597][INFO ][cluster.routing.allocation] [server.company.net] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[sb_nl-core-items_20151217122807][0]] ...]).
[2016-05-17 10:45:09,792][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4234][4] duration [2.2s], collections [1]/[2.3s], total [2.2s]/[2.6s], memory [1.1gb]-&gt;[665.5mb]/[30.8gb], all_pools {[young] [1.1gb]-&gt;[22.4mb]/[1.4gb]}{[survivor] [67.8mb]-&gt;[191.3mb]/[191.3mb]}{[old] [0b]-&gt;[452mb]/[29.1gb]}
[2016-05-17 10:45:11,945][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4235][5] duration [1.9s], collections [1]/[2.1s], total [1.9s]/[4.5s], memory [665.5mb]-&gt;[1.6gb]/[30.8gb], all_pools {[young] [22.4mb]-&gt;[28.9mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [452mb]-&gt;[1.4gb]/[29.1gb]}
[2016-05-17 10:45:13,922][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4236][6] duration [1.5s], collections [1]/[1.9s], total [1.5s]/[6.1s], memory [1.6gb]-&gt;[2.6gb]/[30.8gb], all_pools {[young] [28.9mb]-&gt;[70.2mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [1.4gb]-&gt;[2.3gb]/[29.1gb]}
[2016-05-17 10:45:15,685][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4237][7] duration [1.5s], collections [1]/[1.7s], total [1.5s]/[7.6s], memory [2.6gb]-&gt;[3.7gb]/[30.8gb], all_pools {[young] [70.2mb]-&gt;[143.8mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [2.3gb]-&gt;[3.3gb]/[29.1gb]}
[2016-05-17 10:45:17,504][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4238][8] duration [1.6s], collections [1]/[1.8s], total [1.6s]/[9.3s], memory [3.7gb]-&gt;[4.6gb]/[30.8gb], all_pools {[young] [143.8mb]-&gt;[32mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [3.3gb]-&gt;[4.3gb]/[29.1gb]}
[2016-05-17 10:45:19,463][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4239][9] duration [1.7s], collections [1]/[1.9s], total [1.7s]/[11.1s], memory [4.6gb]-&gt;[5.6gb]/[30.8gb], all_pools {[young] [32mb]-&gt;[22.4mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [4.3gb]-&gt;[5.4gb]/[29.1gb]}
[2016-05-17 10:45:21,703][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4240][10] duration [2s], collections [1]/[2.2s], total [2s]/[13.1s], memory [5.6gb]-&gt;[6.7gb]/[30.8gb], all_pools {[young] [22.4mb]-&gt;[28.8mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [5.4gb]-&gt;[6.5gb]/[29.1gb]}
[2016-05-17 10:45:23,719][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4241][11] duration [1.8s], collections [1]/[2s], total [1.8s]/[15s], memory [6.7gb]-&gt;[7.8gb]/[30.8gb], all_pools {[young] [28.8mb]-&gt;[119mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [6.5gb]-&gt;[7.5gb]/[29.1gb]}
[2016-05-17 10:45:25,694][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4242][12] duration [1.7s], collections [1]/[1.9s], total [1.7s]/[16.8s], memory [7.8gb]-&gt;[8.8gb]/[30.8gb], all_pools {[young] [119mb]-&gt;[29.4mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [7.5gb]-&gt;[8.5gb]/[29.1gb]}
[2016-05-17 10:45:27,908][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4243][13] duration [2s], collections [1]/[2.2s], total [2s]/[18.8s], memory [8.8gb]-&gt;[9.9gb]/[30.8gb], all_pools {[young] [29.4mb]-&gt;[35mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [8.5gb]-&gt;[9.7gb]/[29.1gb]}
[2016-05-17 10:45:29,963][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4244][14] duration [1.8s], collections [1]/[2s], total [1.8s]/[20.7s], memory [9.9gb]-&gt;[10.9gb]/[30.8gb], all_pools {[young] [35mb]-&gt;[3.7mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [9.7gb]-&gt;[10.7gb]/[29.1gb]}
[2016-05-17 10:45:31,685][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4245][15] duration [1.5s], collections [1]/[1.7s], total [1.5s]/[22.3s], memory [10.9gb]-&gt;[12.1gb]/[30.8gb], all_pools {[young] [3.7mb]-&gt;[28.3mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [10.7gb]-&gt;[11.9gb]/[29.1gb]}
[2016-05-17 10:45:33,713][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4246][16] duration [1.8s], collections [1]/[2s], total [1.8s]/[24.1s], memory [12.1gb]-&gt;[13.1gb]/[30.8gb], all_pools {[young] [28.3mb]-&gt;[3.7mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [11.9gb]-&gt;[12.9gb]/[29.1gb]}
[2016-05-17 10:45:35,747][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4247][17] duration [1.8s], collections [1]/[2s], total [1.8s]/[26s], memory [13.1gb]-&gt;[14.1gb]/[30.8gb], all_pools {[young] [3.7mb]-&gt;[23.1mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [12.9gb]-&gt;[13.9gb]/[29.1gb]}
[2016-05-17 10:45:38,973][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4248][18] duration [3s], collections [1]/[3.2s], total [3s]/[29.1s], memory [14.1gb]-&gt;[15.2gb]/[30.8gb], all_pools {[young] [23.1mb]-&gt;[35.7mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [13.9gb]-&gt;[15gb]/[29.1gb]}
[2016-05-17 10:45:42,194][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4249][19] duration [3s], collections [1]/[3.2s], total [3s]/[32.1s], memory [15.2gb]-&gt;[16.4gb]/[30.8gb], all_pools {[young] [35.7mb]-&gt;[22.2mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [15gb]-&gt;[16.2gb]/[29.1gb]}
[2016-05-17 10:45:44,310][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4250][20] duration [1.8s], collections [1]/[2.1s], total [1.8s]/[34s], memory [16.4gb]-&gt;[17.7gb]/[30.8gb], all_pools {[young] [22.2mb]-&gt;[267.3mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [16.2gb]-&gt;[17.2gb]/[29.1gb]}
[2016-05-17 10:45:47,816][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4251][21] duration [3.1s], collections [1]/[3.5s], total [3.1s]/[37.1s], memory [17.7gb]-&gt;[18.5gb]/[30.8gb], all_pools {[young] [267.3mb]-&gt;[4.7mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [17.2gb]-&gt;[18.3gb]/[29.1gb]}
[2016-05-17 10:45:49,779][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4252][22] duration [1.7s], collections [1]/[1.9s], total [1.7s]/[38.9s], memory [18.5gb]-&gt;[19.5gb]/[30.8gb], all_pools {[young] [4.7mb]-&gt;[30.4mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [18.3gb]-&gt;[19.3gb]/[29.1gb]}
[2016-05-17 10:45:51,769][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4253][23] duration [1.8s], collections [1]/[1.9s], total [1.8s]/[40.7s], memory [19.5gb]-&gt;[20.5gb]/[30.8gb], all_pools {[young] [30.4mb]-&gt;[4.3mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [19.3gb]-&gt;[20.3gb]/[29.1gb]}
[2016-05-17 10:45:53,754][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4254][24] duration [1.8s], collections [1]/[1.9s], total [1.8s]/[42.6s], memory [20.5gb]-&gt;[21.5gb]/[30.8gb], all_pools {[young] [4.3mb]-&gt;[9.5mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [20.3gb]-&gt;[21.3gb]/[29.1gb]}
[2016-05-17 10:45:56,828][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4255][25] duration [2.7s], collections [1]/[3s], total [2.7s]/[45.3s], memory [21.5gb]-&gt;[22.6gb]/[30.8gb], all_pools {[young] [9.5mb]-&gt;[22.7mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [21.3gb]-&gt;[22.4gb]/[29.1gb]}
[2016-05-17 10:45:59,824][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4256][26] duration [2.7s], collections [1]/[2.9s], total [2.7s]/[48s], memory [22.6gb]-&gt;[23.9gb]/[30.8gb], all_pools {[young] [22.7mb]-&gt;[422.7mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [22.4gb]-&gt;[23.3gb]/[29.1gb]}
[2016-05-17 10:46:03,930][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4257][27] duration [3.9s], collections [1]/[4.1s], total [3.9s]/[52s], memory [23.9gb]-&gt;[24.8gb]/[30.8gb], all_pools {[young] [422.7mb]-&gt;[401.2mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [23.3gb]-&gt;[24.2gb]/[29.1gb]}
[2016-05-17 10:46:06,760][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4258][28] duration [2.5s], collections [1]/[2.8s], total [2.5s]/[54.5s], memory [24.8gb]-&gt;[25.7gb]/[30.8gb], all_pools {[young] [401.2mb]-&gt;[3.6mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [24.2gb]-&gt;[25.5gb]/[29.1gb]}
[2016-05-17 10:46:09,785][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4259][29] duration [2.6s], collections [1]/[3s], total [2.6s]/[57.2s], memory [25.7gb]-&gt;[26.8gb]/[30.8gb], all_pools {[young] [3.6mb]-&gt;[29.4mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [25.5gb]-&gt;[26.6gb]/[29.1gb]}
[2016-05-17 10:46:11,980][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4260][30] duration [2s], collections [1]/[2.1s], total [2s]/[59.2s], memory [26.8gb]-&gt;[27.8gb]/[30.8gb], all_pools {[young] [29.4mb]-&gt;[4.5mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [26.6gb]-&gt;[27.6gb]/[29.1gb]}
[2016-05-17 10:46:14,136][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4261][31] duration [2s], collections [1]/[2.1s], total [2s]/[1m], memory [27.8gb]-&gt;[28.8gb]/[30.8gb], all_pools {[young] [4.5mb]-&gt;[34.7mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [27.6gb]-&gt;[28.6gb]/[29.1gb]}
[2016-05-17 10:48:35,045][WARN ][monitor.jvm              ] [server.company.net] [gc][old][4262][1] duration [2.3m], collections [1]/[2.3m], total [2.3m]/[2.3m], memory [28.8gb]-&gt;[25.4gb]/[30.8gb], all_pools {[young] [34.7mb]-&gt;[50.8mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[0b]/[191.3mb]}{[old] [28.6gb]-&gt;[25.4gb]/[29.1gb]}
[2016-05-17 10:48:36,816][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4263][33] duration [1.5s], collections [1]/[1.7s], total [1.5s]/[1m], memory [25.4gb]-&gt;[26.5gb]/[30.8gb], all_pools {[young] [50.8mb]-&gt;[4.5mb]/[1.4gb]}{[survivor] [0b]-&gt;[191.3mb]/[191.3mb]}{[old] [25.4gb]-&gt;[26.3gb]/[29.1gb]}
[2016-05-17 10:48:38,526][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4264][34] duration [1.5s], collections [1]/[1.7s], total [1.5s]/[1m], memory [26.5gb]-&gt;[27.5gb]/[30.8gb], all_pools {[young] [4.5mb]-&gt;[4.6mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [26.3gb]-&gt;[27.3gb]/[29.1gb]}
[2016-05-17 10:48:40,166][WARN ][monitor.jvm              ] [server.company.net] [gc][young][4265][35] duration [1.4s], collections [1]/[1.6s], total [1.4s]/[1m], memory [27.5gb]-&gt;[28.5gb]/[30.8gb], all_pools {[young] [4.6mb]-&gt;[5.6mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[191.3mb]/[191.3mb]}{[old] [27.3gb]-&gt;[28.3gb]/[29.1gb]}
[2016-05-17 10:51:18,668][WARN ][monitor.jvm              ] [server.company.net] [gc][old][4266][2] duration [2.6m], collections [1]/[2.6m], total [2.6m]/[4.9m], memory [28.5gb]-&gt;[29.5gb]/[30.8gb], all_pools {[young] [5.6mb]-&gt;[432.6mb]/[1.4gb]}{[survivor] [191.3mb]-&gt;[0b]/[191.3mb]}{[old] [28.3gb]-&gt;[29.1gb]/[29.1gb]}
[2016-05-17 10:54:15,758][WARN ][monitor.jvm              ] [server.company.net] [gc][old][4270][3] duration [2.8m], collections [1]/[2.9m], total [2.8m]/[7.8m], memory [30.8gb]-&gt;[30gb]/[30.8gb], all_pools {[young] [1.4gb]-&gt;[877.3mb]/[1.4gb]}{[survivor] [155.1mb]-&gt;[0b]/[191.3mb]}{[old] [29.1gb]-&gt;[29.1gb]/[29.1gb]}
[2016-05-17 10:56:15,286][WARN ][monitor.jvm              ] [server.company.net] [gc][old][4271][5] duration [1.9m], collections [2]/[1.9m], total [1.9m]/[9.8m], memory [30gb]-&gt;[30.2gb]/[30.8gb], all_pools {[young] [877.3mb]-&gt;[1.1gb]/[1.4gb]}{[survivor] [0b]-&gt;[0b]/[191.3mb]}{[old] [29.1gb]-&gt;[29.1gb]/[29.1gb]}
```

It is clearly seen how gc young is being eaten.
</description><key id="157498257">18635</key><summary>Histogram aggregation causes OOM on 2.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ernestas-poskus</reporter><labels /><created>2016-05-30T12:59:21Z</created><updated>2016-10-01T13:35:28Z</updated><resolved>2016-10-01T13:35:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wyukawa" created="2016-05-31T04:06:14Z" id="222588127">I encountered the same issue
</comment><comment author="wyhw" created="2016-06-28T05:53:25Z" id="228955175">I encountered the same issue!
On ES 2.3.2,  histogram aggregation will cause young gc(cms gc), then cause full gc,  JVM will keep 100% cpu usage for long time ... I have to kill it with signal 9!!
</comment><comment author="wyhw" created="2016-06-29T04:23:53Z" id="229252939">Because "min_doc_count" default value changed from 1 to 0 on ES 2.x,  then i set "min_doc_count" : 1 , it'is work!
</comment><comment author="ernestas-poskus" created="2016-09-26T08:04:39Z" id="249504652">&gt; Because "min_doc_count" default value changed from 1 to 0 on ES 2.x, then i set "min_doc_count" : 1 , it'is work!

With either value it mustn't cause OOM for whole cluster.
</comment><comment author="clintongormley" created="2016-10-01T13:35:28Z" id="250912836">&gt; With either value it mustn't cause OOM for whole cluster.

Fixed by https://github.com/elastic/elasticsearch/pull/19394
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix filtering of node ids for TransportNodesAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18634</link><project id="" key="" /><description>Don't mix up member variables  with local variables
in constructor.

closes #18618
</description><key id="157474723">18634</key><summary>Fix filtering of node ids for TransportNodesAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Internal</label><label>bug</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-05-30T10:22:24Z</created><updated>2016-05-30T13:26:38Z</updated><resolved>2016-05-30T13:26:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-30T11:29:30Z" id="222472647">Made some suggestions to improve readability of the code. If you don't agree, feel free to push. LGTM.
</comment><comment author="brwe" created="2016-05-30T12:03:42Z" id="222478023">@ywelsch I removed the local variables.
</comment><comment author="ywelsch" created="2016-05-30T13:18:38Z" id="222491013">LGTM
</comment><comment author="brwe" created="2016-05-30T13:26:38Z" id="222492481">Thanks for the review @ywelsch !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve phrase suggest test speed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18633</link><project id="" key="" /><description>There is no reason to read the entire marvel hero file to test the features,
it might take several seconds to do so which is unnecessary.
</description><key id="157467954">18633</key><summary>Improve phrase suggest test speed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label></labels><created>2016-05-30T09:42:45Z</created><updated>2016-06-01T16:25:23Z</updated><resolved>2016-05-30T15:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-30T13:09:43Z" id="222489349">There's one test still annotated with `@Nightly` (https://github.com/s1monw/elasticsearch/blob/b2788f01edc1a86fe9166a4ddd7d81c6c133fbfb/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java#L979) which maybe should run as well. Otherwise LGTM.
</comment><comment author="s1monw" created="2016-05-30T13:35:53Z" id="222494147">&gt; There's one test still annotated with @Nightly (https://github.com/s1monw/elasticsearch/blob/b2788f01edc1a86fe9166a4ddd7d81c6c133fbfb/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java#L979) which maybe should run as well. Otherwise LGTM.

I can remove that too
</comment><comment author="s1monw" created="2016-05-30T14:27:27Z" id="222503940">@cbuescher I split the test in 2 (main part in core / collate part in mustache), added `@Nighly` to forbidden APIs and removed all references to it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>S3 snapshot failure - files not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18632</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5

**JVM version**: 

```
java version "1.7.0_76"
Java(TM) SE Runtime Environment (build 1.7.0_76-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.76-b04, mixed mode)
```

**OS version**:

```
Distributor ID: Ubuntu
Description:    Ubuntu 12.04.5 LTS
Release:        12.04
Codename:       precise
```

**Description of the problem including expected versus actual behavior**:

I'm using elasticsearch-cloud-aws/2.7.1 to snapshot to s3 storage and restore to a backup cluster. This works as expected, but on some occasions the restore fails with the following exception :

```
[2016-05-30 01:30:09,837][WARN ][cluster.action.shard     ] [Node-02] [index_blah][4] received shard failed for [index_blah][4], node[fWO2LYbiTyaNGNjKWx0DXQ], [P], restoring[prod:snapshot_160530010001], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-30T01:30:09.722Z], details[shard failure [failed recovery][IndexShardGatewayRecoveryException[[index_blah][4] failed recovery]; nested: IndexShardRestoreFailedException[[index_blah][4] restore failed]; nested: IndexShardRestoreFailedException[[index_blah][4] failed to restore snapshot [snapshot_160530010001]]; nested: IndexShardRestoreFailedException[[index_blah][4] Failed to recover index]; nested: FileNotFoundException[Blob object [__idw] not found: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 660EA6BE976AF53A)]; ]]], indexUUID [KILcs5zETQmcm-zhsoLymQ], reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[[index_blah][4] failed recovery]; nested: IndexShardRestoreFailedException[[index_blah][4] restore failed]; nested: IndexShardRestoreFailedException[[index_blah][4] failed to restore snapshot [snapshot_160530010001]]; nested: IndexShardRestoreFailedException[[index_blah][4] Failed to recover index]; nested: FileNotFoundException[Blob object [__idw] not found: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 16AA079D6DCE946A)]; ]]
```

This seems to indicate that the snapshot did not complete successfully, but on the production cluster I can see the log line which indicates success :

```
[2016-05-30 01:01:28,009][INFO ][snapshots                ] [prod-03] snapshot [prod:snapshot_160530010001] is done
```

It seems like the snapshot is not completely transferred, or a could be a network failure. We also have another cluster in azure which works fine, so seems like an S3 specific issue.
</description><key id="157466446">18632</key><summary>S3 snapshot failure - files not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">raags</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2016-05-30T09:34:18Z</created><updated>2016-08-11T16:00:14Z</updated><resolved>2016-08-11T16:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-30T09:37:26Z" id="222453942">Thanks for reporting this.
Could you check manually if you can see the file on S3 and what are its properties?

If you run again the same restore command does it fail exactly the same way?

Cc @imotov 
</comment><comment author="raags" created="2016-05-30T09:40:13Z" id="222454475">Yes, I did. I don't see the files on S3; and yes it fails for that particular snapshot. Its basically a corrupt snapshot. It fails on the same file. If I try a later snapshot it works. So its some intermittent failure.
</comment><comment author="dadoonet" created="2016-05-30T09:51:13Z" id="222456548">Thanks @raags.

The snapshot operation should have raised an exception in that case. 
So I can see only 2 explanations:
- Bug on AWS/S3 side: could be a bug in their SDK also which may be swallow an exception while running the snapshot.
- Bug in AWS cloud plugin for elasticsearch which would swallow the exception while running the snapshot. I read again the code in https://github.com/elastic/elasticsearch-cloud-aws/blob/v2.7.1/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java and did not see anything obvious.

We are on the way of upgrading the SDK to 1.10.69 with this PR: https://github.com/elastic/elasticsearch-cloud-aws/pull/273 which might hopefully fix this behavior...

Will be also merged for 2.x series with #18048 and for 5.0 with #17784.

I'm unsure on how we can reproduce such an error though. Apart what we already do to simulate write or read errors: https://github.com/elastic/elasticsearch-cloud-aws/blob/v2.7.1/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java#L75

I'm leaving it open until @imotov comments on it as well.
</comment><comment author="imotov" created="2016-05-31T16:36:46Z" id="222745835">@dadoonet the restore service is trying to restore these files, which means that snapshot service thought that these files were successfully written to S3. But it looks like they either were not written or they somehow disappeared afterwards. 

@raags was it a one time issue or you can reproduce it in your setup? 
</comment><comment author="raags" created="2016-06-01T08:23:18Z" id="222925364">@imotov we have now correlated this occurrence with the snapshot clean up job that runs once a day. The snapshot that fails to restore happens during the cleanup. In cleanup we catch the ConcurrentExecution exception and wait/retry until the snapshot completes. Could this be causing removal of some snapshot files? Also, oddly this doesn't happen on other clusters that have the same setup (snapshot/cleanup schedules)
</comment><comment author="imotov" created="2016-06-01T19:32:12Z" id="223099998">@raags It looks like there is a tricky race condition in snapshot deletion. If a delete operation starts while a snapshot is being created, the delete operation indeed fails with ConcurrentSnapshotExecutionException. However, when a snapshot operation starts while a delete operation is in progress, the snapshot operation doesn't check for a running delete operation and as a result the delete operation and the snapshot operation can right simultaneously as long as a delete operation starts first . There are few more conditions that should take place sequence-wise for this condition to occur, but I can see how some files used by an in-progress snapshot might be deleted during a delete operation and cause the error that you've posted. I would guess, it might not be happening on other clusters because their cleanup and snapshot schedules are aligned slightly differently or because delete operations are slightly faster on these clusters for whatever reason and manage to finish before the next snapshot starts.

@abeyad I know you are collecting possible race conditions for snapshot/restore operations. Did we capture this condition somewhere else already or should we use this issue to track it?
</comment><comment author="abeyad" created="2016-06-01T21:35:19Z" id="223132110">Yes, this is a race condition that we encountered amongst others in the past.  However, we should keep this issue open as I hadn't gotten around to capturing this one yet.  Another issue that also presents race conditions is found here: https://github.com/elastic/elasticsearch/issues/18551

Though its not the same, it relates in the sense that it reveals race conditions as a result of a separation between using cluster state and the repository to maintain snapshots without added synchronization around constructs around those. 

Another simple example is if we call the get snapshots API, first we look in the cluster state, get the current snapshot, then look in the repository for the rest of the snapshots. In between looking in the cluster state for running snapshots and looking in the repository, that running snapshot could have completed and gotten stored in the repository, so it would be counted twice.  This is a benign example but is a manifestation of this class of race conditions. 

We have discussed these and been capturing them along the way and are in the process of formulating ideas to fix them. 
</comment><comment author="raags" created="2016-06-02T10:24:41Z" id="223253454">Thanks for the quick turn around. The once a day clean up task used to take a long time, which is why the snapshot conflict happened. We have changed the cleanup interval to every other hour now, so its faster. 
</comment><comment author="abeyad" created="2016-08-11T16:00:14Z" id="239206525">The issue with restores mentioned in this issue will largely go away with the following fix: https://github.com/elastic/elasticsearch/pull/19853.  In this PR, we check if a restore is in progress before attempting to execute a snapshot deletion.

There is still a slight possibility for race conditions, which is a larger issue that needs to be tackled and I opened an issue for it: https://github.com/elastic/elasticsearch/issues/19957

Closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoHashGridIT.filtered failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18631</link><project id="" key="" /><description>This test fails on 2.x branch since 55513946827b5061be6d431da5671711ec489d6c has been committed.

I'm not sure of the fix, maybe just adapt the test is enough.
</description><key id="157457710">18631</key><summary>GeoHashGridIT.filtered failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Geo</label><label>test</label><label>v2.4.0</label></labels><created>2016-05-30T08:41:02Z</created><updated>2016-07-11T15:48:21Z</updated><resolved>2016-07-11T15:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-05-30T08:43:56Z" id="222443520">@nknize Can you please have a look?
</comment><comment author="nknize" created="2016-05-31T20:46:19Z" id="222815032">Opened a PR on master, will back port the fix once committed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove extra bin/ directory in bin folder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18630</link><project id="" key="" /><description>Recent changes adds an extra bin/ directory that contains Windows bat files in the normal Bin folder of elasticsearch. For exemple the script elasticsearch.bat is located in bin/bin/elasticsearch.bat instead of bin/elasticsearch.bat.
</description><key id="157445430">18630</key><summary>Remove extra bin/ directory in bin folder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-05-30T07:20:36Z</created><updated>2016-05-30T07:30:36Z</updated><resolved>2016-05-30T07:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-30T07:28:45Z" id="222429553">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable compression when talking to a local node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18629</link><project id="" key="" /><description>We are testing re-indexing data on an Elasticsearch 2.3.3 instance (latest at the time).

I'm trying to find all bottlenecks to improve indexing and noticed the attached exception (see below)

Compression shouldn't be used when talking to the local node.  There's no real benefit in having this as there's no speedup due to very high bandwidth.  Additionally, it doesn't even speed up switch throughput as switches aren't involved.

It would be ideal to disable compression when talking to the local host and would avoid wasted compression/decompression for next to no benefit.  

```
        at java.util.zip.Deflater.deflateBytes(Native Method)
        at java.util.zip.Deflater.deflate(Deflater.java:444)
        - locked &lt;0xXXXXXXXXXXXXXXXX&gt; (a java.util.zip.ZStreamRef)
        at java.util.zip.Deflater.deflate(Deflater.java:366)
        at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:251)
        at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        - locked &lt;0xXXXXXXXXXXXXXXXX&gt; (a java.io.BufferedOutputStream)
        at org.elasticsearch.common.io.stream.OutputStreamStreamOutput.writeBytes(OutputStreamStreamOutput.java:42)
        at org.elasticsearch.common.io.stream.StreamOutput.write(StreamOutput.java:299)
        at org.jboss.netty.buffer.HeapChannelBuffer.getBytes(HeapChannelBuffer.java:111)
        at org.elasticsearch.common.bytes.ChannelBufferBytesReference.writeTo(ChannelBufferBytesReference.java:64)
        at org.elasticsearch.common.io.stream.StreamOutput.writeBytesReference(StreamOutput.java:127)
        at org.elasticsearch.action.index.IndexRequest.writeTo(IndexRequest.java:732)
        at org.elasticsearch.action.bulk.BulkRequest.writeTo(BulkRequest.java:546)
        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:863)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:329)
        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:51)
        at org.elasticsearch.client.transport.support.TransportProxyClient$1.doWithNode(TransportProxyClient.java:58)
        at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:212)
        at org.elasticsearch.client.transport.support.TransportProxyClient.execute(TransportProxyClient.java:55)
        at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:288)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:86)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:56)

```
</description><key id="157409028">18629</key><summary>Disable compression when talking to a local node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">burtonator</reporter><labels /><created>2016-05-29T22:14:30Z</created><updated>2017-06-28T03:19:59Z</updated><resolved>2016-05-30T00:13:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2017-06-28T03:19:59Z" id="311545747">I'm looking into why 2.4 is recovering shards slowly and hot threads report:

```
::: {mynode}{MFzED4YTQuy-24wahqqeow}{10.36.19.17}{10.36.19.17:9305}{rack_id=111, master=false}
   Hot threads at 2017-06-28T03:13:37.323Z, interval=5s, busiestThreads=3, ignoreIdleThreads=true:

   24.4% (1.2s out of 5s) cpu usage by thread 'elasticsearch[mynode][[http_server_worker.default]][T#52]{New I/O worker #117}'
     7/10 snapshots sharing following 7 elements
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   23.5% (1.1s out of 5s) cpu usage by thread 'elasticsearch[mynode][[http_server_worker.default]][T#54]{New I/O worker #119}'
     2/10 snapshots sharing following 38 elements
       java.util.zip.Inflater.inflateBytes(Native Method)
       java.util.zip.Inflater.inflate(Inflater.java:259)
       java.util.zip.InflaterInputStream.read(InflaterInputStream.java:152)
       java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
       java.io.BufferedInputStream.read(BufferedInputStream.java:345)
       org.elasticsearch.common.io.Streams.readFully(Streams.java:216)
       org.elasticsearch.common.io.stream.InputStreamStreamInput.readBytes(InputStreamStreamInput.java:51)
       org.elasticsearch.common.io.stream.FilterStreamInput.readBytes(FilterStreamInput.java:44)
       org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:98)
       org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:86)
       org.elasticsearch.indices.recovery.RecoveryFileChunkRequest.readFrom(RecoveryFileChunkRequest.java:109)
       org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:222)
       org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:116)
       org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
       org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
       org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
       org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
       org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
       org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
       org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
       org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
       org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
       org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
       org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     5/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
       org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
       org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   19.8% (991.4ms out of 5s) cpu usage by thread 'elasticsearch[mynode][[http_server_worker.default]][T#64]{New I/O worker #129}'
     2/10 snapshots sharing following 38 elements
       java.util.zip.Inflater.inflateBytes(Native Method)
       java.util.zip.Inflater.inflate(Inflater.java:259)
       java.util.zip.InflaterInputStream.read(InflaterInputStream.java:152)
       java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
       java.io.BufferedInputStream.read(BufferedInputStream.java:345)
       org.elasticsearch.common.io.Streams.readFully(Streams.java:216)
       org.elasticsearch.common.io.stream.InputStreamStreamInput.readBytes(InputStreamStreamInput.java:51)
       org.elasticsearch.common.io.stream.FilterStreamInput.readBytes(FilterStreamInput.java:44)
       org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:98)
       org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:86)
       org.elasticsearch.indices.recovery.RecoveryFileChunkRequest.readFrom(RecoveryFileChunkRequest.java:109)
       org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:222)
       org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:116)
       org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
       org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
       org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
       org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
       org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
       org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
       org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
       org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
       org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
       org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
       org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     7/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
       org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
       org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       org.jboss.netty.buffer.HeapChannelBuffer.&lt;init&gt;(HeapChannelBuffer.java:42)
       org.jboss.netty.buffer.BigEndianHeapChannelBuffer.&lt;init&gt;(BigEndianHeapChannelBuffer.java:34)
       org.jboss.netty.buffer.ChannelBuffers.buffer(ChannelBuffers.java:134)
       org.jboss.netty.buffer.HeapChannelBufferFactory.getBuffer(HeapChannelBufferFactory.java:68)
       org.jboss.netty.buffer.AbstractChannelBufferFactory.getBuffer(AbstractChannelBufferFactory.java:48)
       org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:80)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
       org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
       org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```

ES itself is saturating 1 CPU when there are 32 idle CPUs in the system. With 2x10G network it makes very little sense to compress.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.3.3 reindex api - InvalidIndexNameException[Invalid index name [_reindex]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18628</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.8.0_91

**OS version**: Amazon Linux AMI release 2016.03

**Description of the problem including expected versus actual behavior**:

I'm attempting to use the new [reindex api](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html) but am receiving an error `[_reindex] failed to create
[_reindex] InvalidIndexNameException[Invalid index name [_reindex], must not start with '_']`

I could be overlooking something simple, but I'm just not seeing what I'm doing wrong if that's the case.

**Steps to reproduce**:
1.  Executed the following line: 

```
curl -XPOST 'http://es1:9200/_reindex?pretty' -d '{"source":{"index":"medr-discovery-index-v2-20160320"},"dest":{"index":"medr-discovery-index-v2-20160528","version_type":"external"}}'
```

**Provide logs (if relevant)**:

I'm running 2.3.3:

```
$ curl -XGET 'http://es1:9200'
{
  "name" : "es1-dev",
  "cluster_name" : "medr_elz_dev",
  "version" : {
    "number" : "2.3.3",
    "build_hash" : "218bdf10790eef486ff2c41a3df5cfa32dadcfde",
    "build_timestamp" : "2016-05-17T15:40:04Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
```

I execute the command above and receive this error:

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "invalid_index_name_exception",
      "reason" : "Invalid index name [_reindex], must not start with '_'",
      "index" : "_reindex"
    } ],
    "type" : "invalid_index_name_exception",
    "reason" : "Invalid index name [_reindex], must not start with '_'",
    "index" : "_reindex"
  },
  "status" : 400
}
```

These are the logs from the single node in this cluster:

```
[2016-05-28 17:04:43,943][INFO ][node                     ] [es1-dev] version[2.3.3], pid[9628], build[218bdf1/2016-05-17T15:40:04Z]
[2016-05-28 17:04:43,943][INFO ][node                     ] [es1-dev] initializing ...
[2016-05-28 17:04:44,928][INFO ][plugins                  ] [es1-dev] modules [], plugins [analysis-phonetic, license, analysis-kuromoji, cloud-aws, lang-fields-native, analysis-icu, marvel-agent, analysis-smartcn, hq], sites [hq]
[2016-05-28 17:04:44,958][INFO ][env                      ] [es1-dev] using [1] data paths, mounts [[/data (/dev/xvdb)]], net usable_space [476gb], net total_space [503.8gb], spins? [no], types [ext4]
[2016-05-28 17:04:44,958][INFO ][env                      ] [es1-dev] heap size [3.9gb], compressed ordinary object pointers [true]
[2016-05-28 17:04:44,958][WARN ][env                      ] [es1-dev] max file descriptors [65535] for elasticsearch process likely too low, consider increasing to at least [65536]
[2016-05-28 17:04:48,000][INFO ][node                     ] [es1-dev] initialized
[2016-05-28 17:04:48,000][INFO ][node                     ] [es1-dev] starting ...
[2016-05-28 17:04:48,157][INFO ][transport                ] [es1-dev] publish_address {10.0.2.10:9300}, bound_addresses {10.0.2.10:9300}
[2016-05-28 17:04:48,162][INFO ][discovery                ] [es1-dev] medr_elz_dev/vFYAS4mxSd68SCnF1NVY6g
[2016-05-28 17:04:58,011][INFO ][marvel.agent.exporter    ] [es1-dev] skipping exporter [default_local] as it isn't ready yet
[2016-05-28 17:05:08,011][INFO ][marvel.agent.exporter    ] [es1-dev] skipping exporter [default_local] as it isn't ready yet
[2016-05-28 17:05:18,012][INFO ][marvel.agent.exporter    ] [es1-dev] skipping exporter [default_local] as it isn't ready yet
[2016-05-28 17:05:18,165][WARN ][discovery                ] [es1-dev] waited for 30s and no initial state was set by the discovery
[2016-05-28 17:05:18,171][INFO ][http                     ] [es1-dev] publish_address {10.0.2.10:9200}, bound_addresses {10.0.2.10:9200}
[2016-05-28 17:05:18,171][INFO ][node                     ] [es1-dev] started
[2016-05-28 17:05:18,187][INFO ][cluster.service          ] [es1-dev] new_master {es1-dev}{vFYAS4mxSd68SCnF1NVY6g}{10.0.2.10}{10.0.2.10:9300}{max_local_storage_nodes=1, master=true}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-05-28 17:05:18,436][INFO ][license.plugin.core      ] [es1-dev] license [ac1d0ccd-9fab-43b2-8906-33b26a9b38e6] - valid
[2016-05-28 17:05:18,561][INFO ][gateway                  ] [es1-dev] recovered [13] indices into cluster_state
[2016-05-28 17:05:21,840][INFO ][cluster.routing.allocation] [es1-dev] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[indexing-metrics-v1-index-20151012][0]] ...]).
[2016-05-28 17:06:43,360][DEBUG][action.admin.indices.create] [es1-dev] [_reindex] failed to create
[_reindex] InvalidIndexNameException[Invalid index name [_reindex], must not start with '_']
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:150)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:473)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$100(MetaDataCreateIndexService.java:97)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$1.execute(MetaDataCreateIndexService.java:192)
    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:468)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

```
</description><key id="157349673">18628</key><summary>2.3.3 reindex api - InvalidIndexNameException[Invalid index name [_reindex]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">natelapp</reporter><labels /><created>2016-05-28T17:51:23Z</created><updated>2016-05-29T23:05:01Z</updated><resolved>2016-05-29T11:00:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-29T08:18:41Z" id="222348579">It seems that the `reindex` module is missing from your ES installation:

```
[2016-05-28 17:04:44,928][INFO ][plugins                  ] [es1-dev] modules [], plugins [analysis-phonetic, license, analysis-kuromoji, cloud-aws, lang-fields-native, analysis-icu, marvel-agent, analysis-smartcn, hq], sites [hq]
```

On a fresh installation of ES the log line looks as follows:

```
[2016-05-29 10:07:51,699][INFO ][plugins                  ] [Android Man] modules [reindex, lang-expression, lang-groovy], plugins [], sites []
```

We bundle reindex and the other listed modules by default (in a directory `modules` under your ES installation). How did you install ES?
</comment><comment author="natelapp" created="2016-05-29T11:00:53Z" id="222354736">Ah.  I didn't realize that those were handled separately and packaged under a `modules` directory, which we aren't accounting for in our installation.   Sorry for the misreport -- I appreciate your help with figuring this out.
</comment><comment author="nik9000" created="2016-05-29T23:05:01Z" id="222388164">Sorry for the confusion!
On May 29, 2016 7:01 AM, "natelapp" notifications@github.com wrote:

Ah. I didn't realize that those were handled separately and packaged under
a modules directory, which we aren't accounting for in our installation.
Sorry for the misreport -- I appreciate your help with figuring this out.

&#8212;
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub
https://github.com/elastic/elasticsearch/issues/18628#issuecomment-222354736,
or mute the thread
https://github.com/notifications/unsubscribe/AANLotKUwX-Q0kn4WxVMxGy15Gs0ot9fks5qGXHsgaJpZM4IpIAC
.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Never trip circuit breaker in liveness request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18627</link><project id="" key="" /><description>We don't want transport clients to disconnect due to circuit breakers
preventing the liveness request from executing.

Relates to #17951

@danielmitterdorfer can you take a look I think it causes these [failures](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=fedora/494/console)
</description><key id="157336070">18627</key><summary>Never trip circuit breaker in liveness request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Circuit Breakers</label><label>:Translog</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-05-28T12:17:19Z</created><updated>2016-06-06T06:58:21Z</updated><resolved>2016-05-31T09:31:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-31T08:28:00Z" id="222624054">LGTM. 

For future record - the liveness action is supposed to indicate that a node is alive not necessarily  able to process requests.
</comment><comment author="danielmitterdorfer" created="2016-06-06T06:57:54Z" id="223881945">@bleskes Thanks for chiming in. @s1monw I was on vacation last week. Thanks for the change, makes sense to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic Scripting With Watcher - JSON Key-Value Inversed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18626</link><project id="" key="" /><description>**Elasticsearch version**:
1.5

**JVM version**:
1.8.0_91

**OS version**:
Ubuntu 14.04 LTS Desktop

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. Create an index `PUT some_type`
2. Create a watcher:
   
   ```
    {
      "trigger": {
        "schedule": {
          "interval": "5s"
        }
      },
      "input" : {
        "search" : {
          "request" : {
            "indices" : [ "&lt;some_index&gt;" ],
            "types" : [ "&lt;some_type&gt;" ],
            "body" : {
              "query" : {
                "range": {
                  "_timestamp": {
                    "gte": "now-5s"
                  }
                }
              }
            }
          }
        }
      },
      "transform" : {
        "script" : "return [ json: groovy.json.JsonOutput.toJson(ctx.payload.hits.hits)]"
      },
      "actions" : {
        "some_webhook" : {
          "webhook" : {
            "method" : "POST",
            "host" : "&lt;some_ip&gt;",
            "port" : &lt;some_port&gt;,
            "path": "&lt;some_endpoint&gt;",
            "body" : "{{ctx.payload.json}}"
          }
        }
      }
    }
   ```
3. When nothing matches the search, the output is:
   
   `{ '0': '' }`
4. When a document matches the search, the output is:

`{ '{"_index":"&lt;some_index&gt;","_type":"&lt;some_type&gt;","_source":{"&lt;some_key&gt;":"&lt;some_value&gt;", ...},"_id":"&lt;doc_id&gt;","_score":1.0}': '' }`

Where `ctx.payload.json` should already be a valid JSON without any need for a key, since `ctx.payload.hits.hits` should be a valid JSON array.

Basically, the data is being posted as the key of the JSON, if a document matches the search, and again '0' as the key of the JSON when no documents match the search. I would appreciate any help, thanks.
</description><key id="157335971">18626</key><summary>Dynamic Scripting With Watcher - JSON Key-Value Inversed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hasancansaral</reporter><labels /><created>2016-05-28T12:14:35Z</created><updated>2016-05-30T18:23:10Z</updated><resolved>2016-05-30T18:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hasancansaral" created="2016-05-30T18:23:10Z" id="222537622">My bad. I was forgetting:

```
"headers" {
    "Content-type": "application/json"
}
```

so nothing was able to parse (even print) it correctly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Templates defined in Dynamic Mapping are NOT processed in order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18625</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
1.7 or 2.2 

**JVM version**:
1.8.0_66

**OS version**:
OSX 10.11.5

**Description of the problem including expected versus actual behavior**:
According to the [dynamic-templates](https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-templates.html)

&gt; Templates are processed in order&#8201;&#8212;&#8201;the first matching template wins.

in the test case below, it's expected to hit only the first template definition, **path_match_str_def**, but it seemed hit the one next and got the exceptions attached. 

**Steps to reproduce**:
 1.mapping template 

``` javascript
"mappings": {
    "sdr": {
      "dynamic_templates": [
        {
          "path_match_str_def": {
            "path_match": "mypath.*",
            "match_mapping_type": "string",
            "mapping": {
              "type": "string",
              "index": "not_analyzed",
              "doc_values": true
            }
          }
        },
        {
          "path_match_str_int": {
            "path_match": "mypath.var_i*",
            "mapping": {
              "type": "long",
              "index": "not_analyzed",
              "doc_values": true
            }
          }
        }
      ],
      "properties": {
        "mypath": {
          "type": "object",
          "properties": {
            "root": {
              "type": "string",
              "index": "not_analyzed",
              "doc_values": true
            }
          }
        }
      }
    }
  }
```

 2.document
`{
  "mypath": {
    "var_istring": "abcdef"
  }
}`
 3.got exceptions below

``` java
RemoteTransportException[[Living Laser][127.0.0.1:9300][indices:data/write/index[p]]]; nested: MapperParsingException[failed to parse [mypath.var_istring]]; nested: NumberFormatException[For input string: "abcdef"];
Caused by: MapperParsingException[failed to parse [mypath.var_istring]]; nested: NumberFormatException[For input string: "abcdef"];
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:343)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:318)
    at org.elasticsearch.index.mapper.DocumentParser.parseAndMergeUpdate(DocumentParser.java:765)
    at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:652)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:451)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:271)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:315)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:335)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:261)
    at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:131)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:304)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:547)
    at org.elasticsearch.index.shard.IndexShard.prepareIndexOnPrimary(IndexShard.java:529)
    at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:211)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:223)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:157)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:65)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:595)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:263)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:260)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "abcdef"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Long.parseLong(Long.java:589)
    at java.lang.Long.parseLong(Long.java:631)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:145)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.innerParseCreateField(LongFieldMapper.java:275)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:241)
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:335)
    ... 26 more
```
</description><key id="157296043">18625</key><summary>Templates defined in Dynamic Mapping are NOT processed in order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">androidkencai</reporter><labels /><created>2016-05-27T21:45:00Z</created><updated>2016-05-31T20:10:49Z</updated><resolved>2016-05-31T07:01:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-30T10:35:19Z" id="222464167">I can reproduce on master too. The issue is due to the fact that in some cases elasticsearch will apply templates that do not define a `match_mapping_type` before those that do.
</comment><comment author="jpountz" created="2016-05-30T15:01:53Z" id="222510337">I am surprised nobody found it before as this bug seems to be almost 5 years old!
</comment><comment author="jpountz" created="2016-05-31T07:58:17Z" id="222617956">Thanks @androidkencai for reporting this bug. It will be addressed in Elasticsearch 2.4.0 and 5.0.0.
</comment><comment author="androidkencai" created="2016-05-31T20:07:03Z" id="222804214">@jpountz you're welcome. We're still on Kibana3. Will it be addressed in 1.7.x?
</comment><comment author="s1monw" created="2016-05-31T20:10:49Z" id="222805236">@androidkencai I am almost certain that there won't be another `1.7.x` release. you should look into upgradeing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Reorder evil jarhell test property setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18624</link><project id="" key="" /><description>We've seen some failures on java 9 where classes internally are not
found while building the dummy zip for testing. This change moves the
property setting to just before calling jarhell, to try to avoid java
pontentially lazily loading something that then sees the bogus overrides
we set for testing.
</description><key id="157289979">18624</key><summary>Test: Reorder evil jarhell test property setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-05-27T21:01:34Z</created><updated>2016-06-30T18:01:34Z</updated><resolved>2016-06-30T18:01:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-01T10:58:54Z" id="222959876">i think the change is fine, but i also think these tests might be too evil? i think we should anticipate that changing these properties will confuse the JDK completely and cause strange things to happen. 
</comment><comment author="rjernst" created="2016-06-01T13:31:40Z" id="222992734">@rmuir I agree, and that is why I did #18646 as a follow up. Evil jarhell tests are gone! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>InternalEngineTests failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18623</link><project id="" key="" /><description>I have found two test failures in `InternalEngineTests` that consistently fail:

1) `testDocStats`

Reproduces with:
`gradle :core:test -Dtests.seed=19501DA878395CCF -Dtests.class=org.elasticsearch.index.engine.InternalEngineTests -Dtests.method="testDocStats" -Dtests.security.manager=true -Dtests.locale=ar-SY -Dtests.timezone=Europe/Helsinki`

Seems related to the change done in https://github.com/elastic/elasticsearch/pull/18587

2) `testTranslogReplay`

I have not been able to get this to reproduce, though it has failed numerous times on the date of the creation of this issue, for example see: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/668/console

The reproduce line, FWIW:

`gradle :core:test -Dtests.seed=95E0E2968BF16DDB -Dtests.class=org.elasticsearch.index.engine.InternalEngineTests -Dtests.method="testTranslogReplay" -Dtests.security.manager=true -Dtests.locale=da -Dtests.timezone=US/Pacific`
</description><key id="157281176">18623</key><summary>InternalEngineTests failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Engine</label><label>:Stats</label><label>bug</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T20:04:07Z</created><updated>2016-05-28T10:48:28Z</updated><resolved>2016-05-28T06:15:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-28T10:48:28Z" id="222302338">The `testTranslogReplay` failure was addressed by #18611.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FieldSortBuilder.DOC_FIELD_NAME not available &lt; 5.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18622</link><project id="" key="" /><description>[The 2.3 docs](https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.3/java-search-scrolling.html) suggest using `FieldSortBuilder.DOC_FIELD_NAME` for `"_doc"`, but that constant doesn't seem to exist in [the 2.3 FieldSortBuilder class](https://github.com/elastic/elasticsearch/blob/2.3/core/src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java)
</description><key id="157274766">18622</key><summary>FieldSortBuilder.DOC_FIELD_NAME not available &lt; 5.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Java API</label><label>:Scroll</label><label>docs</label></labels><created>2016-05-27T19:23:47Z</created><updated>2016-05-30T11:02:34Z</updated><resolved>2016-05-30T11:01:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-30T10:45:02Z" id="222465671">@PhaedrusTheGreek it appears the code snippet was backported in https://github.com/elastic/elasticsearch/pull/17579 to 2.x and 2.3 without changing the constant which should be `SortParseElement.DOC_FIELD_NAME` on the 2.x branches. I will correct that in the docs for 2.3 and 2.x accordingly
</comment><comment author="cbuescher" created="2016-05-30T11:01:51Z" id="222468403">Corrected on 2.x and 2.3 via 3011dfd561 and 732be802514.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add java.time packages to painless whitelist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18621</link><project id="" key="" /><description>This api is horrible, but there is nothing unsafe about it, and it gives another way to manipulate dates (other than Date/Calendar). It is mapped directly other than a few exceptions (registering timezone provider stuff etc)
</description><key id="157272408">18621</key><summary>add java.time packages to painless whitelist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T19:09:14Z</created><updated>2016-06-01T14:03:59Z</updated><resolved>2016-05-28T10:51:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-27T21:02:42Z" id="222252503">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move ingest processors from core to ingest-common module.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18620</link><project id="" key="" /><description>Also all the rest tests have been moved to ingest-common module as well (rest tests can't be ran without processor implementations), because these tests don't run in the rest-api-spec module but in the distribution:integ-test-zip module and adding a test plugin with a mock plugin there felt just wrong to me. I think this is fine. I left a tiny ingest rest test behind in that tests with an empty pipeline.

Closes #18490
</description><key id="157257826">18620</key><summary>Move ingest processors from core to ingest-common module.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T17:45:57Z</created><updated>2016-07-01T19:08:06Z</updated><resolved>2016-06-07T15:33:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-05-30T10:42:22Z" id="222465253">Made a couple of updates:
- Moved reindex ingest tests to `smoke-test-ingest-with-all-dependencies` qa module.
- Extracted test plugin from core to test infra, so that the reindex module can reuse that.
- Removed two messy tests from `smoke-test-ingest-with-all-dependencies` qa module. What these tests tested was already covered by the rest tests in this qa module.
</comment><comment author="talevy" created="2016-05-31T19:18:08Z" id="222791690">Looks good. will make testing way nicer!
</comment><comment author="martijnvg" created="2016-06-01T08:17:59Z" id="222924238">@talevy I've also moved the grok processor to the ingest-commons module and dropped the ingest-grok module like we discussed yesterday.
</comment><comment author="rjernst" created="2016-06-01T17:34:46Z" id="223067565">I like the idea of moving this to a module, but why collapse in the grok module?
</comment><comment author="talevy" created="2016-06-01T18:23:43Z" id="223081572">@rjernst the only reason we moved Grok into its own module was to isolate its dependencies from core. Now that all the other processors moved into a module, we can combine Grok. There is no strong reason to isolate grok's dependencies from the other processors
</comment><comment author="javanna" created="2016-06-01T19:27:47Z" id="223098892">Not that it should block this PR and sorry if I am late to the party... I am genuinely afraid that we will end up with too many modules just because they make it easy to run a subset of the tests. I don't find this reason compelling enough and I wonder if there isn't a better solution to make development easier. Am I the only one having this concern? or maybe there are other reasons to have this as a module other than executing specific tests? I feel like the reasons why something should become a module are shifting from the initial "to isolate dependencies and crazy security permissions" to something else where many more things should become modules. Do modules have a cost in our build, or can we just have as many as we want? Maybe I am simply worrying too much though :)
</comment><comment author="rjernst" created="2016-06-01T19:36:05Z" id="223101028">&gt; Do modules have a cost in our build, or can we just have as many as we want?

The only cost is a slightly longer configuration time in gradle, but this continues to improve with new versions of gradle.

&gt;  I feel like the reasons why something should become a module are shifting from the initial "to isolate dependencies and crazy security permissions" to something else where many more things should become modules

To me, in addition to dependencies and security, it has to do with "keeping core small": #10368. It also has the added benefit that it forces us to ensure these things are actually pluggable, and the tests in core can be very generic for that exact purpose (does the infrastructure work, vs does a specific implementation work).
</comment><comment author="talevy" created="2016-06-01T19:41:41Z" id="223102424">Regarding the easy testing comment, maybe this warrants a separate issue?

This is how I currently test ingest changes

```
gradle :core:check -Dtests.class=org.elasticsearch.action.ingest.*
gradle :core:check -Dtests.class=org.elasticsearch.ingest.*
gradle :modules:ingest-grok:check
gradle :plugins:ingest-geoip:check
gradle :qa:smoke-test-ingest-with-all-dependencies:check
gradle :distribution:integ-test-zip:integTest -Dtests.class=org.elasticsearch.test.rest.RestIT
```

these don't take too long and they avoid running tests for all of core.

we will still have to run core tests since the core ingest apis and Pipeline features are still in core. 
</comment><comment author="javanna" created="2016-06-01T20:15:51Z" id="223111234">I like the "keep core small" and "verify pluggability" reasons much more than "make testing faster" alone. I am convinced this makes sense, thanks @rjernst for your thoughts. 
</comment><comment author="martijnvg" created="2016-06-02T07:04:46Z" id="223212157">&gt; I am genuinely afraid that we will end up with too many modules just because they make it easy to run a subset of the tests. 

@javanna Agreed, we should end up with too many modules (like a module per processor).  It should be balanced and I think we have the right balance here by adding all processors that don't require additional security permissions (grok, set, date, etc) into a single module. This way we benefit from a lean and mean ES core and testing ingest processors being quick.

&gt; we will still have to run core tests since the core ingest apis and Pipeline features are still in core.

Yes, changes to the ingest infra in core require testing several modules. However changing just a processor will require to only test a single plugin or module and the `smoke-test-ingest-with-all-dependencies` qa module.
</comment><comment author="dadoonet" created="2016-07-01T17:11:51Z" id="230000033">So apparently modules are not published to maven central?

Here is my use case. I'm building an ingest plugin for elasticsearch using Maven.

I depend on:

``` xml
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;5.0.0-alpha4&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
```

Then, I want to run a simple test where I'm building a pipeline aggregation using:
- rename processor
- my own processor

Was working well with alpha3 because rename was in core. Now it moved to a module which is not available on maven central so I can't use the code in my tests.
I'm now getting `No processor type exists with name [rename]"`.

This is a similar concern as I explained at #18131.

Any chance we will publish `ingest-common` JAR on maven central?
</comment><comment author="martijnvg" created="2016-07-01T19:08:06Z" id="230025544">&gt; So apparently modules are not published to maven central?

Correct, modules aren't published. I think the reason is because they are considered internal to ES, but @rjernst can confirm this.

Ideally any of your (unit and integration) tests shouldn't depend on other processors. If you do want to test an use case that requires multiple processors, then it is better to add such a test to a qa module. If your processor test depends on some really basic processor operation, then you can also use a mock test processor in your test (See TestProcessor in test framework). 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 5.0 alpha 2 does not starts and hangs on at heap size and quits after some time </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18619</link><project id="" key="" /><description>I downloaded the tar.gz files of elasticsearch and when running the same as ./bin/elasticsearch I am not able to start the same as it gets terminated after some time : 
Below is the stack trace for same:

&gt; [ec2-user@ip-172-31-56-201 ELK_5.alpha2]$ ./elasticsearch-5.0.0-alpha2/bin/elasticsearch
&gt; [2016-05-27 12:59:44,538][INFO ][node                     ] [Impulse] version[5.0.0-alpha2], pid[30414], build[e3126df/2016-04-26T12:08:58.960Z]
&gt; [2016-05-27 12:59:44,540][INFO ][node                     ] [Impulse] initializing ...
&gt; [2016-05-27 12:59:45,427][INFO ][plugins                  ] [Impulse] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
&gt; [2016-05-27 12:59:45,470][INFO ][env                      ] [Impulse] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [19.6gb], net total_space [24.9gb], spins? [unknown], types [rootfs]
&gt; [2016-05-27 12:59:45,471][INFO ][env                      ] [Impulse] heap size [1015.6mb], compressed ordinary object pointers [true]
&gt; Killed
&gt; [ec2-user@ip-172-31-56-201 ELK_5.alpha2]$
&gt; ![capture](https://cloud.githubusercontent.com/assets/6836305/15615270/e978037a-245a-11e6-99a8-f3b4fd866de4.JPG)
</description><key id="157249869">18619</key><summary>Elasticsearch 5.0 alpha 2 does not starts and hangs on at heap size and quits after some time </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prashanttct07</reporter><labels /><created>2016-05-27T17:02:45Z</created><updated>2016-06-01T15:19:30Z</updated><resolved>2016-05-27T17:30:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-27T17:30:53Z" id="222206677">Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.

A vanilla install of Elasticsearch 5.0.0-alpha2 is known to start. You might find help working through this issue if you post on the [Elastic Discourse forum](https://discuss.elastic.co).
</comment><comment author="jasontedor" created="2016-06-01T14:04:36Z" id="223002426">@prashanttct07 Were you able to get this resolved? I looked for a topic on Discuss but couldn't find one. Let me know, I would be happy to help. :smile:
</comment><comment author="prashanttct07" created="2016-06-01T14:09:46Z" id="223003982">Hi jason,
I was not able to get the same run on my aws instance... As I got busy with
2.3 for some task so was not able to get it posted on forum.. Also didn't
get any forum question over there for the same.

Just a question is there any memory requirement for 5.0 as I am running on
aws micro instance with 1 GB memory, so there I can run the 2.3 but not 5.0

Thanks,
Prashant Agrawal
Sent from my Mobile.
On Jun 1, 2016 7:35 PM, "Jason Tedor" notifications@github.com wrote:

&gt; @prashanttct07 https://github.com/prashanttct07 Were you able to get
&gt; this resolved? I looked for a topic on Discuss but couldn't find one. Let
&gt; me know, I would be happy to help. &#128516;
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18619#issuecomment-223002426,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AGhQUcwsqvLzCkDALlXRHHHmGNzRMI1Nks5qHZHFgaJpZM4IosAQ
&gt; .
</comment><comment author="jasontedor" created="2016-06-01T14:27:09Z" id="223009461">@prashanttct07 The defaults out of the box for alpha2 are to use a 1 GB heap. You can tune this in the jvm.options file down to 512 MB (just change `-Xmx1g` to `-Xmx512m`).
</comment><comment author="prashanttct07" created="2016-06-01T15:15:23Z" id="223025496">Ohh Oki,

I was considering the same as 256 as default . Thanks for the info I will
check the same and let you know in case of any issue.

Regards:
Prashant Agrawal
+91 8097606642
Skype : prashanttct07
www.futurepathfinder.com

On Wed, Jun 1, 2016 at 7:58 PM, Jason Tedor notifications@github.com
wrote:

&gt; @prashanttct07 https://github.com/prashanttct07 The defaults out of the
&gt; box for alpha2 are to use a 1 GB heap. You can tune this in the jvm.options
&gt; file down to 512 MB (just change -Xmx1g to -Xmx512m).
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18619#issuecomment-223009461,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AGhQUUI36hPzTJ7d6iFX9WjsLlYRL3E4ks5qHZcUgaJpZM4IosAQ
&gt; .
</comment><comment author="jasontedor" created="2016-06-01T15:19:30Z" id="223026878">@prashanttct07 Depending on what you're doing, 256 MB might be fine, or it might be a little light. We typically recommend half of the machine RAM for the Elasticsearch heap leaving the other half for the filesystem cache. This is assuming that you're only running Elasticsearch on the machine. If you're running other applications, you will have to adjust accordingly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtered list of nodes IDs in TransportNodesAction isn't actually considered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18618</link><project id="" key="" /><description>In the 2.x branch, the `TransportNodesAction` class defines two methods:

```
    protected String[] filterNodeIds(DiscoveryNodes nodes, String[] nodesIds) {
        return nodesIds;
    }

    protected String[] resolveNodes(NodesRequest request, ClusterState clusterState) {
        return clusterState.nodes().resolveNodesIds(request.nodesIds());
    }
```

which basically return all the nodes in the cluster (`resolveNodes`) and just the nodes IDs received as parameter (`filterNodeIds`), respectively.

The constructor, though, does the following:

```
            String[] nodesIds = resolveNodes(request, clusterState);
            this.nodesIds = filterNodeIds(clusterState.nodes(), nodesIds);
            ImmutableOpenMap&lt;String, DiscoveryNode&gt; nodes = clusterState.nodes().nodes();
            this.nodes = new DiscoveryNode[nodesIds.length];
            for (int i = 0; i &lt; nodesIds.length; i++) {
                this.nodes[i] = nodes.get(nodesIds[i]);
            }
            this.responses = new AtomicReferenceArray&lt;&gt;(this.nodesIds.length);
        }
```

So, the list of nodes - `this.nodes[i] = nodes.get(nodesIds[i]);` - is being built considering the result of `resolveNodes()` method (meaning all the nodes), not on the filtered list of nodes (the result of `filterNodeIds` method), which should be the desired behavior.
</description><key id="157232699">18618</key><summary>Filtered list of nodes IDs in TransportNodesAction isn't actually considered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">astefan</reporter><labels><label>:Internal</label><label>bug</label><label>v2.3.4</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T15:31:07Z</created><updated>2016-05-30T13:26:20Z</updated><resolved>2016-05-30T13:26:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add timeout and retry support for Azure storage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18617</link><project id="" key="" /><description>[This TODO](https://github.com/elastic/elasticsearch/blob/c4d3bf472bc5eaa18a1f1ffac727110a33f89b42/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java#L153-L159) shows an old comment about implementing or not timeout and retry options like this:

``` java
BlobRequestOptions options = new BlobRequestOptions();
options.setTimeoutIntervalInMs(1000);
options.setRetryPolicyFactory(new RetryNoRetry());
blobContainer.deleteIfExists(options, null);
```

This issue is just a reminder to check if it's still doable with the latest versions of the Azure storage SDK and if we need to implement this (and how).
</description><key id="157222879">18617</key><summary>Add timeout and retry support for Azure storage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>enhancement</label></labels><created>2016-05-27T14:46:24Z</created><updated>2016-07-25T14:33:02Z</updated><resolved>2016-07-25T14:33:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update repository-gcs.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18616</link><project id="" key="" /><description>Fixed link
</description><key id="157222849">18616</key><summary>Update repository-gcs.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>:Plugin Repository GCS</label><label>docs</label></labels><created>2016-05-27T14:46:17Z</created><updated>2016-05-27T14:49:42Z</updated><resolved>2016-05-27T14:49:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-27T14:49:42Z" id="222166520">Thanks a lot! Merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up RoutingNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18615</link><project id="" key="" /><description>This PR contains a number of cleanups related to recent changes in RoutingNodes:
- PR #17821 (Immutable ShardRouting) changed RoutingNode to use a map indexed by ShardId to manage ShardRouting elements. This means that we can directly select the right ShardRouting without iterating over all elements. This lets us get rid of RoutingNodeIterator and all kind of iterations all over the place.
- Second cleanup is an extension of #18390 (Expose cluster state before reroute in RoutingAllocation instead of RoutingNodes). We should not reexpose RoutingTable in RoutingNodes and only use it in the constructor. This makes it clear that the RoutingTable is only used to construct the RoutingNodes and can diverge from it afterwards (only RoutingNodes is mutable).
- Remove AllocationService.applyNewNodes() (that is already done as part of construction of RoutingNodes)
</description><key id="157222408">18615</key><summary>Clean up RoutingNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T14:44:14Z</created><updated>2016-05-31T14:37:58Z</updated><resolved>2016-05-31T14:37:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-30T15:00:04Z" id="222509995">@s1monw can I interest you in reviewing this? :-)
</comment><comment author="s1monw" created="2016-05-30T15:15:33Z" id="222512754">left small comments - look awesome
</comment><comment author="ywelsch" created="2016-05-31T13:12:10Z" id="222683637">@s1monw I've updated the PR with your suggestions. Please have another look.
</comment><comment author="s1monw" created="2016-05-31T14:03:02Z" id="222698089">LGTM thanks for the cleanups
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement ctx.op = "delete" on _update_by_query and _reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18614</link><project id="" key="" /><description>This pull request adds a new operation type `delete` when using scripts in the Reindex and Update-By-Query APIs.

Related to #18043
</description><key id="157221227">18614</key><summary>Implement ctx.op = "delete" on _update_by_query and _reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T14:38:43Z</created><updated>2016-06-06T09:22:38Z</updated><resolved>2016-06-06T09:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-05-30T07:23:08Z" id="222428544">@nik9000 Would you like to review this one?
</comment><comment author="nik9000" created="2016-06-03T21:10:05Z" id="223695316">Left some minor comments but LGTM.
</comment><comment author="tlrx" created="2016-06-06T09:22:38Z" id="223908849">@nik9000 Thanks! Merged with your suggestions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve thread pools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18613</link><project id="" key="" /><description>This issue is a meta-issue for collapsing several sub-issues for improving thread pools in core Elasticsearch:
- [x] register all thread pool settings #18674
- [x] provide hook for plugins to register a thread pool #18674
- [x] thread pool settings should be node-level settings #18674
- [ ] remove unbounded queues #18491 
- [ ] place a hard bound of 256 \* max number of threads in pool for a bound on the queue #14448 

The proposal here is that core Elasticsearch will no longer provide support for custom executors. Of course, a plugin can always create an executor on its own and so core Elasticsearch will provide an extension point for such plugins to provide stats back to core Elasticsearch for reading in node stats API and cat thread pools API.

Relates #17915, relates #18491, relates #14448, relates #15866
</description><key id="157219976">18613</key><summary>Improve thread pools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>Meta</label></labels><created>2016-05-27T14:32:47Z</created><updated>2016-06-07T02:13:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add query functions to function_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18612</link><project id="" key="" /><description>Allows to combine query scores with mult, sum etc
by wrapping individual queries in a `query_function`
of a `function_score like` so:

[Edit, see discussion below]

```
{
  "query": {
    "function_score": {
      "score_mode": "multiply",
      "functions": [
        {
          "query": {
            "match": {
              "text": "cat"
            }
          }
        },
        {
          "query": {
            "match": {
              "text": "dog"
            }
          }
        }
      ],
      "boost_mode": "replace"
    }
  }
}
```

[Was initially: ]

```
{
  "query": {
    "function_score": {
      "score_mode": "multiply",
      "functions": [
        {
          "query_function": {
            "query": {
              "match": {
                "text": "cat"
 ...
```

relates to #17116
</description><key id="157218539">18612</key><summary>Add query functions to function_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>feature</label><label>review</label><label>stalled</label></labels><created>2016-05-27T14:25:58Z</created><updated>2017-04-07T23:14:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-05-27T14:26:25Z" id="222160349">I am a little unsure about this implementation and added three TODOs where I am particularly unsure.
</comment><comment author="JnBrymn-EB" created="2016-05-27T16:02:52Z" id="222186079">Per https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html, the current `function_score` syntax is 

```
        {
            "filter": {},
            "FUNCTION": {}, 
            "weight": number
        }
```

Rather than adding a new type of function, would it be more consistent to add a parameter at this level called `query` (similar to the existing `filter`). This would require one less level of nesting if the function was omitted.
</comment><comment author="brwe" created="2016-05-27T16:51:20Z" id="222197303">So you mean the whole thing would more look like this? :

```
{
  "query": {
    "function_score": {
      "score_mode": "multiply",
      "functions": [
        {
          "filter": {
            "match": {
              "some_other_field": "foo"
            }
          }, 
          "query": {
            "match": {
              "text": "cat"
            }
          }, 
          "weight": 2
        },
       ...
      ],
      "boost_mode": "replace"
    }
  }
}

```

That might indeed be easier to read.

Or do you mean make the query a general option for all functions, like:

```
{
  "query": {
    "function_score": {
      "score_mode": "multiply",
      "functions": [
        {
          "filter": {
            "match": {
              "some_other_field": "foo"
            }
          },
          "field_value_factor": {
            "field": "num_field"
          },
          "query": {
            "match": {
              "text": "dog"
            }
          },
          "weight": 2
        }
        ...
      ],
      "boost_mode": "replace"
    }
  }
}
```

That was the original idea in #17116 but we discarded it because then we would need to know how to combine query score with the other function (in this case the output of `field_value_factor`). 
</comment><comment author="JnBrymn-EB" created="2016-05-27T19:01:25Z" id="222227931">I had in mind something that looks like your first example, but I was also wondering if it was sensible to combine this with the existing function types - probably doesn't. So yes, the first option. Effectively I think you're implementing this as a function_score function type called `query`, yes?

So far I'm ok with all of the above.
</comment><comment author="brwe" created="2016-05-30T11:00:20Z" id="222468177">Added a new commit to change syntax accordingly (f9876fd).
</comment><comment author="clintongormley" created="2016-06-01T14:29:43Z" id="223010360">@brwe sorry for the late notice but I'm not sure I agree with the latest changed.  I don't understand what output you'd expect from that construct.  With the original form (with `query` as a type of function) you understand that the result for each doc is its score.

I think this only really becomes useful once we have the second part (combining scores from each function with a script)
</comment><comment author="clintongormley" created="2016-06-01T16:12:45Z" id="223044013">&gt;  I'm not sure I agree with the latest changed.

Sorry I misread the history. I DO agree with the current format :)  The only thing I'd do is to change the name from `query` to `query_score`.  We already have `filter`, so `query` sounds too much like it can be used like a filter, while `query_score` is closer to `script_score`.
</comment><comment author="brwe" created="2016-06-02T10:38:51Z" id="223256188">OK, changed the syntax to be:

```
{
  "query": {
    "function_score": {
      "score_mode": "multiply",
      "functions": [
        {
          "filter": {
            "match": {
              "some_other_field": "foo"
            }
          }, 
          "query_score": {
            "match": {
              "text": "cat"
            }
          }, 
          "weight": 2
        },
       ...
      ],
      "boost_mode": "replace"
    }
  }
}

```

I need help with the TODOs. @jpountz when you have a minute, could you take a look? If not let me know and I will find someone else.
</comment><comment author="jpountz" created="2016-06-02T13:25:27Z" id="223290344">@brwe I left some comments!
</comment><comment author="brwe" created="2016-06-03T16:25:41Z" id="223626021">@jpountz thanks for the review! Addressed all comments except for the IndexSearcher in constructor of QueryFunction because I am not sure where to get it from, see https://github.com/elastic/elasticsearch/pull/18612#discussion_r65726645 . Will try to find a different way on Monday.
</comment><comment author="mckinnovations" created="2016-07-18T11:27:40Z" id="233304263">Any timeline for this feature ? when is it going to be released?
</comment><comment author="dakrone" created="2016-09-12T21:34:25Z" id="246501595">@jpountz I think this needs a re-review?
</comment><comment author="elasticmachine" created="2017-02-23T18:14:48Z" id="282074278">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="dakrone" created="2017-04-07T23:14:34Z" id="292673337">@jpountz you were reviewing this, is this something we should bring up to date?</comment><comment author="elasticmachine" created="2017-04-07T23:14:36Z" id="292673340">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix translog replay multiple operations same doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18611</link><project id="" key="" /><description>Modifying the translog replay to not replay again into the translog
introduced a bug for the case of multiple operations for the same
doc. Namely, since we were no longer updating the version map for each
operation, the second operation for a doc would be treated as a creation
instead of as an update. This commit fixes this bug by placing these
operations into version map. This commit includes a failing test case.

Relates #18547
Relates #18623
</description><key id="157201033">18611</key><summary>Fix translog replay multiple operations same doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Translog</label><label>bug</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T12:57:34Z</created><updated>2016-06-28T09:34:01Z</updated><resolved>2016-05-27T13:43:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-27T13:25:38Z" id="222145511">LGTM left a small comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve percolator query term extraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18610</link><project id="" key="" /><description>Don't bail query term extraction when a must clause contains an unsupported query, if there is at least one other must clause in a boolean query that can be extracted.
</description><key id="157197365">18610</key><summary>Improve percolator query term extraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T12:37:32Z</created><updated>2016-05-27T18:00:34Z</updated><resolved>2016-05-27T18:00:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-27T14:27:22Z" id="222160604">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade joda-time to 2.9.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18609</link><project id="" key="" /><description>This commit upgrades joda-time to version 2.9.4 to integrate a bug fix
there into Elasticsearch.

Closes #14524, closes #18017
</description><key id="157172198">18609</key><summary>Upgrade joda-time to 2.9.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Dates</label><label>upgrade</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-05-27T10:02:23Z</created><updated>2016-05-31T10:03:01Z</updated><resolved>2016-05-27T12:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-27T12:38:09Z" id="222135623">LGTM
</comment><comment author="gmoskovicz" created="2016-05-27T12:53:26Z" id="222138544">&#128512; 
</comment><comment author="deanobarnett" created="2016-05-27T13:11:49Z" id="222142388">Awesome!
</comment><comment author="s1monw" created="2016-05-27T13:30:32Z" id="222146628">good! thanks @jasontedor 
</comment><comment author="jasontedor" created="2016-05-27T13:38:33Z" id="222148569">Backports:
- 2.3: a9642baffd5e50c51f4b82e2ed6be7645cc7bf64
- 2.x: 37583581db166fb9c3cd779c779cb5105e9ad66e
</comment><comment author="gmoskovicz" created="2016-05-27T13:40:25Z" id="222149003">@jasontedor will it be included in a 2.2.x release?
</comment><comment author="jasontedor" created="2016-05-27T14:09:22Z" id="222156042">&gt; will it be included in a 2.2.x release?

No, the 2.2.x series will not see a maintenance release for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed nested query in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18608</link><project id="" key="" /><description /><key id="157138653">18608</key><summary>Fixed nested query in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grogy</reporter><labels><label>docs</label></labels><created>2016-05-27T06:26:44Z</created><updated>2016-06-01T13:39:05Z</updated><resolved>2016-06-01T13:38:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T13:39:00Z" id="222994798">thanks @grogy - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed typo "field".</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18607</link><project id="" key="" /><description>Field was used twice: "field value of a field".
</description><key id="157112677">18607</key><summary>Fixed typo "field".</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acareaga</reporter><labels><label>docs</label></labels><created>2016-05-27T01:34:11Z</created><updated>2016-06-01T13:27:49Z</updated><resolved>2016-06-01T13:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-01T13:27:49Z" id="222991666">thanks @acareaga - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add note regarding Windows service heap size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18606</link><project id="" key="" /><description>This commit adds a note regarding the difference in configuration for
the Windows service heap size from any other installation of
Elasticsearch.
</description><key id="157076123">18606</key><summary>Add note regarding Windows service heap size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label></labels><created>2016-05-26T20:50:48Z</created><updated>2016-06-01T20:31:16Z</updated><resolved>2016-06-01T20:31:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add bootstrap check docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18605</link><project id="" key="" /><description>This commit adds documentation for the bootstrap checks and provides
either links or inline guidance for setting the necessary settings to
pass the bootstrap checks.
</description><key id="157072697">18605</key><summary>Add bootstrap check docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T20:32:45Z</created><updated>2016-05-29T09:59:19Z</updated><resolved>2016-05-27T10:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-26T21:30:24Z" id="222000600">LGTM
</comment><comment author="clintongormley" created="2016-05-29T09:59:19Z" id="222352475">@jasontedor this PR doesn't mention the escape hatch added in https://github.com/elastic/elasticsearch/pull/18088
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updating CustomBoostFactorScorer to fix must_not + function score issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18604</link><project id="" key="" /><description>When using a function score query within a must_not clause, the query was returning the wrong results.

Note that this PR is pointed at 1.7 for two reasons:
1. ES 2.0 upgraded to Lucene 5, where the `ReqExclScorer` class was rewritten in such a way that this bug doesn't manifest in 2.0, 2.1, or the 2.2 branches (though as I'm investigating, there may be a different bug that excludes all results...)
2. ES 2.3 removed the `CustomBoostFactorScorer` class

Closes #18315
</description><key id="157064546">18604</key><summary>Updating CustomBoostFactorScorer to fix must_not + function score issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rusnyder</reporter><labels><label>:Search</label><label>bug</label><label>v1.7.6</label></labels><created>2016-05-26T19:51:50Z</created><updated>2016-05-27T12:51:41Z</updated><resolved>2016-05-27T12:51:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2016-05-26T19:51:51Z" id="221975828">Hi @rusnyder, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git [commit](https://github.com/elastic/elasticsearch/pull/18604.patch). Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?
</comment><comment author="rusnyder" created="2016-05-26T19:57:09Z" id="221977207">Done - sorry about that!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simple Query String: NOT operator causes all documents in index to be returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18603</link><project id="" key="" /><description>**Elasticsearch version**:
2.3.3

**JVM version**:
8u91

**OS version**:
Debian Jessie and CentOS 7

**Description of the problem including expected versus actual behavior**:

Including a simple query string query with the NOT operator (hyphen) causes all documents to be returned.

**Steps to reproduce**:
1. Start from a fresh ElasticSearch instance
   
   ```
   docker run -d -p 9200:9200 elasticsearch -Des.network.host=0.0.0.0
   ```
2. Load some data
   
   ```
   curl -XPUT localhost:9200/test_index -d '{"settings": {"index" {"number_of_shards": 3, "number_of_replicas": 0}}}'
   curl -XPOST localhost:9200/test_index/test_doc -d '{"content": "Buffalo"}'
   curl -XPOST localhost:9200/test_index/test_doc -d '{"content": "Buffalo, New York"}'
   curl -XPOST localhost:9200/test_index/test_doc -d '{"content": "Canada"}'
   ```
3. Verify normal querying works:
   
   ```
   curl -XGET localhost:9200/test_index/test_doc/_search -d'
   {
   "query": {
       "simple_query_string": {
           "query": "Buffalo"
       }
   }
   }'
   # The two, correct results are returned
   ```
4. Demonstrate queries with the NOT operator don't work:
   
   ```
   {
   "query": {
       "simple_query_string": {
           "query": "Buffalo -New"
       }
   }
   }'
   # Here, all three documents are returned, including one without terms specified in the query.
   ```
</description><key id="157044533">18603</key><summary>Simple Query String: NOT operator causes all documents in index to be returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dsem</reporter><labels /><created>2016-05-26T18:08:39Z</created><updated>2016-06-01T13:08:44Z</updated><resolved>2016-06-01T13:08:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dsem" created="2016-05-26T18:12:55Z" id="221950559">Possibly related to #9633
</comment><comment author="clintongormley" created="2016-06-01T13:08:43Z" id="222986677">Duplicate of #4707
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Acknowledge index deletion requests based on standard cluster state acknowledgment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18602</link><project id="" key="" /><description>Index deletion requests currently use a custom acknowledgement mechanism that wait for the data nodes to actually delete the data before acknowledging the request to the client. This was initially put into place as a new index with same name could only be created if the old index was wiped as we used the index name as data folder on the data nodes. With PR #16442, we now use the index uuid as folder name which avoids collision between indices that are named the same (deleted and recreated). This allows us to get rid of the custom acknowledgment mechanism altogether and rely on the standard cluster state-based acknowledgment instead.

Closes #18558
</description><key id="157034529">18602</key><summary>Acknowledge index deletion requests based on standard cluster state acknowledgment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>bug</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T17:16:31Z</created><updated>2016-06-01T13:25:56Z</updated><resolved>2016-06-01T13:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-30T09:09:03Z" id="222448429">@ywelsch and I talked about it offline, discussing the fact that this change uses two ack mechanism layered on top of each other (with two futures, two timeouts etc.) which makes things complex. We are currently evaluating whether specific index store deletion acks are needed now that we have uuids as folder names and based on that decide whether we want to invest more time here or just move to the standard cluster state based ack-ing.
</comment><comment author="ywelsch" created="2016-05-31T16:44:37Z" id="222747938">@bleskes I've updated the PR by removing the custom ack mechanism and relying only on the standard cluster state ack.
</comment><comment author="bleskes" created="2016-06-01T12:06:08Z" id="222972635">LGTM. Best stats ever. Can you update the PR description/title? 
</comment><comment author="ywelsch" created="2016-06-01T13:25:56Z" id="222991180">Thanks @bleskes! I've updated title/description.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use license, notice and readme files from root for distributions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18601</link><project id="" key="" /><description>The distributions had their own copies of these extra files, which was a
carry over from maven. This change removes the duplicate files and
copies them from the root of the project.

closes #18597
</description><key id="157023582">18601</key><summary>Use license, notice and readme files from root for distributions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T16:21:40Z</created><updated>2016-05-26T16:27:34Z</updated><resolved>2016-05-26T16:27:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-26T16:25:34Z" id="221922389">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>replace ScriptException with a better one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18600</link><project id="" key="" /><description>Exceptions from scripts are not handled well today:
- There is a lot of repetitive unnecessary boxing and wrapping of exceptions at multiple layers. 
- Despite this verbosity, no stacktrace information is given by default (!). Users of this are like "developers" and need to know _where_ in their script something went wrong.
- If the user passes the rest parameter (which is very long) to enable stacktrace output, they get a massive verbose toString, containing all kinds of irrelevant ES elements, and formatted as a terribly ugly massive string.
- ScriptException is meaningless, it doesn't provide any more structure over a RuntimeException!
- json is the worst possible way to transmit source code. We can count on having no line numbers (which always makes stacktraces meaningful), instead just everything shoved on one massive line.

The current situation just encourages even more bad exception handling. People complain if you get NullPointerException when you dereference a null pointer, but that is exactly what you deserve if you do this! And it tells you all you need to figure it out: its right there in the stacktrace with line numbers!

We should just "box" the exception a single time, with all the stuff you need to figure out the problem by default. That means telling you what script hit the problem, what language it was in, a "simple" reason of what happened, and a relevant "stacktrace" to the script. The original root cause is always preserved, if someone asks for the stacktrace or looks at the logs, they will see all that really happened, this is more about the horror of whats happening today at the REST layer.

In order to contend with "everything shoved on a single line", we have to do some highlighting. Normally, line numbers and existing lines work great for code, but since we don't have those, we have to work with what we've got. 

For painless that means, encoding offsets as line numbers into the bytecode, wherever an exception can strike. It also means breaking down the (probably massive) script into sentences that have enough meaningful context: we use leaf S\* nodes as "sentences".

For expressions it means when a "variable" goes wrong (e.g. not in mappings), we just highlight what went wrong with that variable definition, etc.

Other scripting engines can be adapted to use this, e.g. you can get some of this kind of info from at least some groovy/javascript/python exceptions. For now, they still keep using the deprecated "GeneralScriptException" and do the same as before. If someone else wants to improve them, great.

Here are some example runtime errors:

```
{
  "type" : "script_exception",
  "reason" : "runtime error",
  "caused_by" : {
    "type" : "unsupported_operation_exception",
    "reason" : null
  },
  "script_stack" : [
    "java.util.AbstractList.add(AbstractList.java:148)"
    "java.util.AbstractList.add(AbstractList.java:108)"
    "return x.add(5);",
    "        ^---- HERE"
  ],
  "script" : "def x = Collections.emptyList(); return x.add(5);",
  "lang" : "painless"
}

{
  "type" : "script_exception",
  "reason" : "runtime error",
  "caused_by" : {
    "type" : "null_pointer_exception",
    "reason" : null
  },
  "script_stack" : [
    "x = Math.log(Math.abs(ctx.sometypo.getSomething())); ",
    "                                  ^---- HERE"
  ],
  "script" : "double y = ctx.thing; double x = Math.log(Math.abs(ctx.sometypo.getSomething())); return x * 5 + y;",
  "lang" : "painless"
}
```

It helps also for compile-time errors (and the "link time" of expressions where it binds against the mappings):

```
{
  "type" : "script_exception",
  "reason" : "compile error",
  "caused_by" : {
    "type" : "parse_exception",
    "reason" : "invalid sequence of tokens near '*' on line (1) position (17)",
    "caused_by" : {
      "type" : "no_viable_alt_exception",
      "reason" : null
    }
  },
  "script_stack" : [
    "doc['d'].value * *@#)(@$*@#$ + 4",
    "                 ^---- HERE"
  ],
  "script" : "doc['d'].value * *@#)(@$*@#$ + 4",
  "lang" : "expression"
}

{
  "type" : "script_exception",
  "reason" : "link error",
  "caused_by" : {
    "type" : "parse_exception",
    "reason" : "Field [e] does not exist in mappings"
  },
  "script_stack" : [
    "doc['e'].value",
    "     ^---- HERE"
  ],
  "script" : "doc['e'].value * 5",
  "lang" : "expression"
}
```
</description><key id="157020019">18600</key><summary>replace ScriptException with a better one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Expressions</label><label>:Plugin Lang Painless</label><label>:Scripting</label><label>bug</label><label>PITA</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T16:04:45Z</created><updated>2016-05-26T21:51:35Z</updated><resolved>2016-05-26T21:51:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-26T16:29:33Z" id="221923566">LGTM.  Thanks for making these changes, exceptions just got a whole lot better  &#128077; !  Left one minor comment.
</comment><comment author="s1monw" created="2016-05-26T18:31:11Z" id="221955521">Looked through the exception related changes and LGTM. I left a nitpick :)

this is a fantastic improvement! thanks for doing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove extra mostly duplicate readme file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18599</link><project id="" key="" /><description>It looks like the readme was duplicated when plugins were merged back
into the repo. We removed all these extra files from the plugins, this
removes the remaining duplicate from core.

closes #18597
</description><key id="157017833">18599</key><summary>Remove extra mostly duplicate readme file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>docs</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T15:55:34Z</created><updated>2016-05-26T16:12:32Z</updated><resolved>2016-05-26T16:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-26T15:56:47Z" id="221914039">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[feautre request]_recovery API add some userful info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18598</link><project id="" key="" /><description>**Describe the feature**:
GET _recovery?active_only
"name": {
    "shards": [
      {
        "stage": "INDEX",
        "translog": {
          "total_on_start": 0,
          "recovered": 0,
          "total": 0,
          "total_time_in_millis": 0,
          "percent": "100.0%"
        },
        "total_time_in_millis": 802265,
        "verify_index": {
          "total_time_in_millis": 0,
          "check_index_time_in_millis": 0
        },
        "index": {
          "size": {
            "total_in_bytes": 10796323018,
            "reused_in_bytes": 0,
            "percent": "4.6%",
            "recovered_in_bytes": 499692769
          },
          "total_time_in_millis": 802265,
          "files": {
            "total": 147,
            "recovered": 114,
            "percent": "77.6%",
            "reused": 0
          },
          "target_throttle_time_in_millis": 0,
          "source_throttle_time_in_millis": 18
        },
        "start_time_in_millis": 1464274726291,
        "id": 14,
        "source": {
          "transport_address": "ip:9323",
          "ip": "ip",
          "host": "ip",
          "name": "name:9223",
          "id": "id"
        },
        "type": "RELOCATION",
        "primary": true,
        "target": {
          "transport_address": "ip2:9323",
          "ip": "ip2",
          "host": "ip2",
          "name": "name2",
          "id": "id2"
        }
      }

is it possible to add max_bytes_per_sec size so that we can get the transfer speed?
</description><key id="157008685">18598</key><summary>[feautre request]_recovery API add some userful info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-05-26T15:16:57Z</created><updated>2016-05-26T15:23:04Z</updated><resolved>2016-05-26T15:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-26T15:23:04Z" id="221903676">@makeyang We don't provide rates like max-bytes-per-sec in our stats as these numbers can easily be derived from the totals we do provide, with a simple monitoring tool.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Instructions from README.textile fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18597</link><project id="" key="" /><description>**Elasticsearch version**: 
elasticsearch-5.0.0-alpha2

**JVM version**: 
jdk1.8.0_65

**OS version**: 
Red Hat Enterprise Linux Workstation release 6.8 (Santiago)

**Description of the problem including expected versus actual behavior**:
 I follow instructions in README.textile and get an error.

**Steps to reproduce**:
1. Just follow instructions 
   1. Fails with: curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '

**Provide logs (if relevant)**:

```
[chojnasm@beechams elasticsearch-5.0.0-alpha2]$ curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
     }
 }'
```

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "parsing_exception",
      "reason" : "no [query] registered for [matchAll]",
      "line" : 4,
      "col" : 22
    } ],
    "type" : "parsing_exception",
    "reason" : "no [query] registered for [matchAll]",
    "line" : 4,
    "col" : 22
  },
  "status" : 400
}
```
</description><key id="157007783">18597</key><summary>Instructions from README.textile fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sajmmon</reporter><labels><label>build</label></labels><created>2016-05-26T15:13:07Z</created><updated>2016-05-26T16:27:34Z</updated><resolved>2016-05-26T16:27:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-26T15:16:05Z" id="221901428">&gt; matchAll

I don't believe that is in the README any more.
</comment><comment author="sajmmon" created="2016-05-26T15:19:10Z" id="221902394">I go forward with README And also get:

curl -XPUT http://127.0.0.1:9200/another_user/ -d '

&gt; {
&gt;     "index" : {
&gt;         "numberOfShards" : 1,
&gt;         "numberOfReplicas" : 1
&gt;     }
&gt; }'
&gt; {"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"unknown setting [index.numberOfReplicas] did you mean [index.number_of_replicas]?"}],"type":"illegal_argument_exception","reason":"unknown setting [index.numberOfReplicas] did you mean [index.number_of_replicas]?"},"status":400}
</comment><comment author="clintongormley" created="2016-05-26T15:26:08Z" id="221904627">It looks like we have two READMEs:
- https://github.com/elastic/elasticsearch/blob/master/README.textile
- https://github.com/elastic/elasticsearch/blob/master/core/README.textile

It's the latter which is out of date.  Can gradle use the top level one instead?
</comment><comment author="rjernst" created="2016-05-26T16:18:27Z" id="221920304">Doh, I kept misunderstanding what you meant by "can gradle use" because all I saw was this extra readme file. Until I realized there were _THREE_ readme files, because the distribution also has its own copy. I'll open a PR shortly to make that use the files from the root (including notice and license).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Record number of buckets removed via bucket_selector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18596</link><project id="" key="" /><description>It could be useful/convenient to know how many buckets the `bucket_selector` pipeline agg removes during runtime.

I'm not sure if/how/where the response would go, however, since the `bucket_selector` doesn't actually "enrich" the response...just prune it.  Adding to the parent agg would violate the internal immutability of element, and appending a new child bucket seems like a poor solution.
</description><key id="157007562">18596</key><summary>Record number of buckets removed via bucket_selector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2016-05-26T15:12:11Z</created><updated>2016-08-22T11:02:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="michaelbaamonde" created="2016-05-26T22:43:22Z" id="222015984">The only other place that springs to mind is the aggregation metadata. That's still tricky, since it's a client-provided object. Aside from breaking the current contract ("metadata's yours and yours alone, user!"), it would also introduce the need for blacklisting a key (`buckets_removed` or whatever) to guard against the scenario in which user-provided metadata contains that same key already. 

It does have the benefit of not messing with the semantics of the aggregation itself, however, as would be the case if we just added another child bucket.

Alternatively, we could add an altogether new metadata object for providing "internal" metadata at the aggregation level. That's probably a better, more flexible approach, as it would also allow us to add other kinds of metadata in the future without worrying about the problems I mentioned above re: user-provided metadata.
</comment><comment author="jpountz" created="2016-05-30T15:15:38Z" id="222512768">This does not feel clean to me in terms of API. If this is something clients need to know, maybe they should do the pruning on client side?
</comment><comment author="colings86" created="2016-08-22T11:02:00Z" id="241380240">currently the bucket_selector agg itself actually has no output (since all it does is prune a sibling agg) so we could add this information as it's own output. I am a little worried though that we will and up with users wanting to be able to turn this output off so we may need to expose it as an option on the bucket selector if we do implement it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update minimum gradle version and remove assumption around target compatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18595</link><project id="" key="" /><description>Up until now we assumed that we always compile with java 8 or java 9 and never with targetCompatibility lower than 1.8. This will be necessary with the upcoming java http client as it will be 1.7 compatible. We should then apply specific options only when using 1.8 or 1.9.

Also by upgrading the minimum required gradle version to 2.13 we better support subprojects that need to compile against different target versions as there were previously issues with the intellij integration around that.
</description><key id="157007426">18595</key><summary>Update minimum gradle version and remove assumption around target compatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-26T15:11:39Z</created><updated>2016-05-27T10:35:01Z</updated><resolved>2016-05-27T10:35:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-05-26T15:12:11Z" id="221900251">@rjernst can you have a look please? This is working in my branch but I may have missed something :)
</comment><comment author="rjernst" created="2016-05-27T09:01:50Z" id="222096819">LGTM, I left one more minor request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18594</link><project id="" key="" /><description>This pull request is a cleanup of two issues for plugins: 
- the plugin command specified in an error message for the remove
  plugin command is incorrect
- remove the ability to specify a custom path for plugins

Closes #18588
</description><key id="156996161">18594</key><summary>Plugins cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugins</label><label>breaking</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T14:24:51Z</created><updated>2016-09-16T10:28:55Z</updated><resolved>2016-05-26T18:46:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-26T14:35:51Z" id="221889312">&gt; remove the ability to specify a custom path for plugins

So long as we still have `readlink` on the directory then I'm ok with it. My good friend @ebernhardson uses that. Plugins are deployed to `/srv/elasticsearch/plugins` (I think) and symlinked into `/usr/share/elasticsearch/plugins` and I suspect something like that is reasonably common, especially for folks that develop their own plugins.
</comment><comment author="nik9000" created="2016-05-26T15:10:18Z" id="221899693">Left a halfhearted suggestion about telling folks about symlinks in the migration docs in case they feel left out without their fancy setting.

Maybe instead of deleting the test you can change it to use a symlink for the directory? As I said I think that is reasonably common. I'm happy to come back later and do that myself if you'd prefer to merge as is.
</comment><comment author="nik9000" created="2016-05-26T15:10:54Z" id="221899851">LGTM modulo those two things. Whatever you decide just let me know but I don't think this needs another review.
</comment><comment author="nik9000" created="2016-05-26T18:18:04Z" id="221951895">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ip fields backward-compatible at query time.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18593</link><project id="" key="" /><description>The fact that ip fields used a different doc values representation in 2.x causes
issues when querying 2.x and 5.0 indices in the same request. This changes 2.x
doc values on ip fields/2.x to be hidden behind binary doc values that use the
same encoding as 5.0. This way the coordinating node will be able to merge shard
responses that have different major versions.

One known issue is that this makes sorting/aggregating slower on ip fields for
indices that have been generated with elasticsearch 2.x.

Relates to #17971
</description><key id="156994279">18593</key><summary>Make ip fields backward-compatible at query time.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T14:17:07Z</created><updated>2016-06-28T09:28:37Z</updated><resolved>2016-05-31T12:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-05-31T09:12:43Z" id="222634020">@jpountz I left a couple of comments
</comment><comment author="jpountz" created="2016-05-31T12:29:22Z" id="222673871">@colings86 I fixed the comments.
</comment><comment author="colings86" created="2016-05-31T12:43:51Z" id="222677029">@jpountz thanks, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query string query and analyze_wildcard: true =&gt; tokenizer doesn't respect default_operator: AND</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18592</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3
**JVM version**: openjdk 8
**OS version**: debian 8

Hi,

i'm using a custom pattern tokenizer for an email field:

```
index:
  analysis:
    tokenizer:
      alnum:
        type: pattern
        pattern: '[^a-zA-Z0-9_/]+'
```

I search via:

```
{
  "query": {
    "query_string": {
      "query":"email:email@example.com*",
      "analyze_wildcard":true,
      "default_operator":"AND"
    }
  }, "from":0,"size":30
}
```

This returns all docs having ANY of the tokens "email", "example" or "com", even though default_operator: AND is specified. It should return only docs having ALL the tokens.

This issue is only present when using the wildcard plus analyze_wildcard: true, as

```
{
  "query": {
    "query_string":{
      "query":"email:email@example.com",
      "default_operator":"AND"
    }
  },"from":0,"size":30
}
```

returns correct results. However, i of course need the wildcard for my use case (autocomplete of email addresses).
</description><key id="156993494">18592</key><summary>Query string query and analyze_wildcard: true =&gt; tokenizer doesn't respect default_operator: AND</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrkamel</reporter><labels /><created>2016-05-26T14:13:43Z</created><updated>2016-05-26T14:33:02Z</updated><resolved>2016-05-26T14:29:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-26T14:29:19Z" id="221887354">Fixed by https://github.com/elastic/elasticsearch/pull/17711
</comment><comment author="mrkamel" created="2016-05-26T14:33:02Z" id="221888501">ah, perfect!

... and sry, didn't find that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade of index to Lucene 4 fails.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18591</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.4

**JVM version**: 1.8.0_25

**OS version**: OSX 10.11.5

**Description of the problem including expected versus actual behavior**: When upgrading an index from version 0.2 -&gt; 1.7 -&gt; 2.3. It appears the upgrade fails and I get the following error when loading my index in 2.3:

`The index [policy] was created before v0.90.0 and wasn't upgraded. This index should be open using a version before 2.0.0 and upgraded using the upgrade API.`

I would expect that running the upgrade process while running ES 1.7.4 would upgrade my index to a version that is compatible with ES 2.3.

**Steps to reproduce**:

I started with an index created with version 0.2. I then switched to version 1.7.4 and performed an upgrade of my index `policy`:

```
$ curl -XPOST 'http://localhost:9200/policy/_upgrade'
{"_shards":{"total":10,"successful":5,"failed":0},"upgraded_indices":{"policy":"3.6.0"}}
```

Then I start up ES 2.3.3 (using the same data location) and I see the error:

`Exception in thread "main" java.lang.IllegalStateException: The index [policy] was created before v0.90.0 and wasn't upgraded. This index should be open using a version before 2.0.0 and upgraded using the upgrade API`

Note how after doing the "upgrade", the policy version is 3.6.0. I would expect this version to be at least 4.0.0 as that is what is required by ES &gt;2.0.
</description><key id="156987816">18591</key><summary>Upgrade of index to Lucene 4 fails.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">alexspurling</reporter><labels><label>:Upgrade API</label><label>bug</label><label>feedback_needed</label></labels><created>2016-05-26T13:48:16Z</created><updated>2016-11-18T15:00:15Z</updated><resolved>2016-11-18T15:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-26T15:04:42Z" id="221898081">Do you have some empty shards?  I tried your recreation and had the same problem, until I reduced the number of shards to one and indexed a document.

As a workaround, and AFTER upgrading the index, you could close the index and change that setting:

```
POST policy/_close
PUT policy/_settings
{
  "index.version.minimum_compatible": "4.0.0"
}
POST policy/_open
```
</comment><comment author="alexspurling" created="2016-05-26T15:34:52Z" id="221907408">Could you point me to some documentation for how to check the number of shards I have and how to reduce the number to 1?
</comment><comment author="clintongormley" created="2016-06-01T13:04:44Z" id="222985738">@alexspurling you can just look at the index settings for `number_of_shards`:

```
GET policy/_settings
```

You can't change this value though - it is fixed at index creation time.  

If you want to see if you have any empty shards in the index, try:

```
GET policy/_stats/docs?level=shards
```
</comment><comment author="clintongormley" created="2016-11-18T15:00:15Z" id="261552087">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document the hard limits from on index and bulk thread pool sizes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18590</link><project id="" key="" /><description>In #15585 we put hard limits on the maximum thread pool size for bulk and index queue ... but we didn't document the limits.
</description><key id="156986864">18590</key><summary>Document the hard limits from on index and bulk thread pool sizes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>docs</label></labels><created>2016-05-26T13:43:59Z</created><updated>2016-05-26T14:06:59Z</updated><resolved>2016-05-26T14:06:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-26T13:47:05Z" id="221875345">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve TimeUnitRounding for edge cases and DST transitions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18589</link><project id="" key="" /><description>Our current testing for TimeUnitRoundings `rounding()` and `nextRoundingValue()` methods that are used especially for date histograms lacked proper randomization for time zones. We only did randomized tests for fixed offset time tones (e.g. +01:00, -05:00) but didn't account for real world time zones with DST transitions. 

Adding those tests revealed a couple of problems with our current rounding logic. In some cases, usually happening around those transitions, rounding a value down could land on a value that itself is not a proper rounded value. Also sometimes the nextRoundingValue would not line up properly with the rounded value of all dates in the next unit interval.

This change improves the current rounding logic in TimeUnitRounding in two ways: it makes sure that calling round(date) always returns a date that when rounded again won't change (making round() idempotent) by handling special cases happening during dst transitions by performing a second rounding. It also changes the `nextRoundingValue()` method to itself rely on the round method to make sure we always return rounded values for the unit interval boundaries.

Also adding tests for randomized TimeUnitRounding that assert important basic properties the rounding values should have. For better understanding and readability a few of the pathological edge cases are also added as a special test case.
</description><key id="156970273">18589</key><summary>Improve TimeUnitRounding for edge cases and DST transitions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Dates</label><label>enhancement</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T12:16:47Z</created><updated>2016-05-31T10:06:54Z</updated><resolved>2016-05-27T13:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-26T12:18:21Z" id="221855132">@jpountz do you have time to take a look at this?
</comment><comment author="jpountz" created="2016-05-26T13:36:16Z" id="221872440">LGTM!
</comment><comment author="cbuescher" created="2016-05-27T11:26:02Z" id="222123422">@jpountz thanks for the review. After talking directly to @colings86 about concerns that buckets on dst transitions that are not unit sized might lead to wrong derivative pipeline aggregation normalization on date histograms, I also added tests that show that 23h/25h day buckets on CET dst transitions and weird edge cases like in the case of Asia/Kathmandu are correctly normalized (using the diverging bucket width). Could you take another look at the tests added in the follow up commit before I merge?
</comment><comment author="jpountz" created="2016-05-27T12:40:12Z" id="222136031">LGTM^2
</comment><comment author="cbuescher" created="2016-05-27T16:18:34Z" id="222189835">Again, forgot to ask if you think this should also go to 2.x like https://github.com/elastic/elasticsearch/pull/18415? I'd say yes since the fix in that previous PR had its problems that were revealed by extending the tests.
</comment><comment author="jpountz" created="2016-05-30T10:51:55Z" id="222466798">+1 to backport
</comment><comment author="cbuescher" created="2016-05-30T10:56:24Z" id="222467523">Merged with 2.x via a4a2e26d49f058 and 238dccd9b2a3fe.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RemovePluginCommand.java reports older plugin command name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18588</link><project id="" key="" /><description>**Elasticsearch version**:
master/alpha-3

**Description of the problem including expected versus actual behavior**:
RemovePluginCommand.java error message still [reports](https://github.com/elastic/elasticsearch/blob/c257e2c51f235853c4453a86e10e463813140fc9/core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java#L74) `plugin` as the command name,  instead of `elasticsearch-plugin`

**Steps to reproduce**:

```
abonuccelli@w530 /opt/elk/PROD/node1/elasticsearch-5.0.0-alpha3/bin $ ./elasticsearch-plugin  remove x-pack
-&gt; Removing x-pack...
A tool for managing installed elasticsearch plugins

Commands
--------
list - Lists installed elasticsearch plugins
install - Install a plugin
remove - Removes a plugin from elasticsearch

Non-option arguments:
command              

Option         Description        
------         -----------        
-h, --help     show help          
-s, --silent   show minimal output
-v, --verbose  show verbose output
ERROR: Plugin x-pack not found. Run 'plugin list' to get list of installed plugins.
```
</description><key id="156967455">18588</key><summary>RemovePluginCommand.java reports older plugin command name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>:Plugins</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T11:59:14Z</created><updated>2016-05-26T18:46:09Z</updated><resolved>2016-05-26T18:46:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Move DocStats under Engine to get more accurate numbers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18587</link><project id="" key="" /><description>Today we pull doc stats from an index reader which might not reflect reality.
IndexWriter might have merged all deletes away but due to a missing refresh
the stats are completely off. This change pulls doc stats from the IndexWriter
directly instead of relying on refreshes to run regularly. Note: Buffered deletes
are still not visible until the segments are flushed to disk.
</description><key id="156922049">18587</key><summary>Move DocStats under Engine to get more accurate numbers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Stats</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T07:32:51Z</created><updated>2016-06-01T13:47:42Z</updated><resolved>2016-05-27T13:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-05-26T09:24:42Z" id="221821543">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding status field in _msearch error request bodies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18586</link><project id="" key="" /><description>Closes #18013 
This might be too naive of a solution. @HonzaKral is this along the lines of what you expected?
</description><key id="156907892">18586</key><summary>Adding status field in _msearch error request bodies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">a2lin</reporter><labels><label>:REST</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-26T05:42:30Z</created><updated>2016-06-17T14:21:17Z</updated><resolved>2016-06-16T12:31:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HonzaKral" created="2016-05-26T23:26:35Z" id="222022964">Thanks @a2lin, I think the `status` should always be present, however, even for a successful responses. What do you think?
</comment><comment author="a2lin" created="2016-05-27T05:07:29Z" id="222061248">@HonzaKral Are you referring to the _msearch endpoint? I think it makes some sense, but I don't think _search currently includes the status field for successful responses. Should I add it to _msearch anyway, or just leave it at parity?
</comment><comment author="clintongormley" created="2016-06-01T13:32:07Z" id="222992856">@a2lin I think we should add the `status` field to each _msearch entry regardless of success or failure.  It's not the same as for _search as _search can use HTTP headers instead.
</comment><comment author="a2lin" created="2016-06-02T08:15:23Z" id="223225226">@clintongormley looks like this: http://paste.ubuntu.com/16916354/
</comment><comment author="clintongormley" created="2016-06-02T09:30:08Z" id="223241703">thanks @a2lin - I'll leave this for somebody to review from a code perspective
</comment><comment author="tlrx" created="2016-06-14T13:47:51Z" id="225885977">I left some comments. It would be nice to document this additional field in `multi-search.asciidoc`.
</comment><comment author="a2lin" created="2016-06-15T07:34:26Z" id="226110669">@tlrx Thanks for reviewing this! I think I've fixed the issues.
</comment><comment author="tlrx" created="2016-06-16T06:52:58Z" id="226402708">Thanks for your contribution! LGTM

Can you please squash your commits in 1 and add details to your commit message (including "closes #18013")?
</comment><comment author="a2lin" created="2016-06-16T07:33:14Z" id="226409695">@tlrx I squashed and rebased on top of latest master branch. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex from remote</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18585</link><project id="" key="" /><description>This adds a `remote` option to reindex that looks like

```
curl -POST 'localhost:9200/_reindex?pretty' -d'{
  "source": {
    "remote": {
      "host": "otherhost:9200"
    },
    "index": "target",
    "query": {
      "match": {
        "foo": "bar"
      }
    }
  },
  "dest": {
    "index": "target"
  }
}'
```

This reindex has all of the features of local reindex:
- Using queries to filter what is copied
- Retry on rejection
- Throttle/rethottle

The big advantage of this version is that it goes over the HTTP API which can be made backwards compatible. I have yet to test it against any version of Elasticsearch other than the modern one but it should be fairly easy to retrofit it to work with anything.

Some things are different:
- The `query` field is sent directly to the other node rather than parsed on the coordinating node. This should allow it to support constructs that are invalid on the coordinating node for whatever reason.

Lots of things still need to be sorted out:
- [x] A bunch of NOCOMMITS
- [x] This uses httpclient directly and doesn't support tls or basic auth at all. I'd like to replace it with Elasticsearch's currently-being-built HTTP based java client.
- [x] At this point there isn't a whitelist for the `host`. This doesn't turn Elasticsearch into an arbitrary `curl`ing machine because reindex will only allow three commands - the one to start the scroll, the one to pull the next scroll, and the one clear the scroll. The URLs aren't arbitrary and neither are the methods and it only does them in that order. But, still, I'm going to have to add a whitelist.
- [x] I have to test it against more Elasticsearch versions.
- [x] We have to decide if we're going to integration test against real elasticsearch versions or just use mocks of their returned json and call that good enough. It is obviously much more work to setup the integration tests and much slower to test. 50ms per test vs 30 seconds per test kind of slower.
- [x] Building this involved a lot of refactoring. That refactoring needs to inform some unit test changes. Probably.
  More?

Closes https://github.com/elastic/elasticsearch/issues/17447
</description><key id="156888587">18585</key><summary>Reindex from remote</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>feature</label><label>release highlight</label><label>v5.0.0-alpha5</label></labels><created>2016-05-26T02:06:55Z</created><updated>2016-07-05T20:16:47Z</updated><resolved>2016-07-05T20:15:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-26T02:13:19Z" id="221759883">&gt; This uses httpclient directly and doesn't support tls or basic auth at all. I'd like to replace it with Elasticsearch's currently-being-built HTTP based java client.

Along those lines:
- I'm just slamming strings together to build the URLs to send to elasticsearch which is almost certainly as SQL-injection style goldmine. My hope is that the fancy new HTTP based Java client for Elasticsearch will have a pattern I can use for that.
</comment><comment author="clintongormley" created="2016-05-26T13:18:31Z" id="221867869">&gt; At this point there isn't a whitelist for the host. This doesn't turn Elasticsearch into an arbitrary curling machine because reindex will only allow three commands - the one to start the scroll, the one to pull the next scroll, and the one clear the scroll. The URLs aren't arbitrary and neither are the methods and it only does them in that order. But, still, I'm going to have to add a whitelist.

I'm not sure we need this.  What are you trying to protect against by adding a whitelist?
</comment><comment author="nik9000" created="2016-05-26T13:35:50Z" id="221872317">&gt; What are you trying to protect against by adding a whitelist?

It is to prevent Elasticsearch from being a jumping off point in the case of a partially compromised network. I don't trust us to prevent a sufficiently determined individual from figuring out how to do something nasty with the http calls that Elasticsearch issues on their behalf. I don't think of reindex-from-remote as being anything like dynamic scripting in terms of exploitability but I really like the idea of limiting it to a whitelist anyway just for extra paranoia.
</comment><comment author="nik9000" created="2016-05-27T13:57:28Z" id="222153090">Yesterday I ran some tests that reindexed from Elasticsearch nodes running on my laptop rather than the system under test. I found and fixed two issues:
- I was checking if the source index existed in the destination cluster even if we were pulling from remote. I never noticed because of the way I was testing.
- Before 2.1.0 Elasticsearch doesn't support `sort=_doc` which is the default sort used by reindex because it makes the search process more efficient. This required another round trip to the source cluster to check its version and required that we handle scan's habit of not returning any results in the first round.

With those two things solved I was able to verify that this works against the following versions:
- master
- 2.3.3
- 1.7.5
- 0.90.13
- 0.20.5

I hope no one has to use this so far back, but, at least with the basic test that I did, it works.
</comment><comment author="nik9000" created="2016-05-30T14:21:10Z" id="222502755">I've played with this a little more locally. I indexed 10,000 documents into the same cluster and then did both a few normal and a remote reindexes. The first ones were slow because, like a second. After that the "local" reindex hovers around ~270ms and the remote one around ~420ms. I'd like to play with it some and see where the bottleneck is. It might just be the network, http, and json overhead incurred by going remote. The "local" reindex can take advantage of lots of nice work Elasticsearch does to make requests more efficient, like local requests skipping the transport layer entirely.
</comment><comment author="tlrx" created="2016-05-31T09:58:47Z" id="222644657">I have a first look and it looks good! Most of my comment are really minor and concern naming. 

I do like how it fits into the current reindex infra but I'm wondering if we could reuse SearchHit/InternalSearchHit/SearchResponse/InternalSearchResponse instead of using the ScrollableHitSource.Hit, Response, BasicHit, ClientHit stuff... It looks like a big part of the code is declaring these objects and convert them around only because of the remote reindex use case. I'd rather see the remote reindex makes use of the current objects, even partially. It would also help later once the built-in HTTP client will be available, what do you think?
</comment><comment author="nik9000" created="2016-05-31T14:22:48Z" id="222704030">&gt; I'd rather see the remote reindex makes use of the current objects, even partially.

I really don't _like_ the objects we expose though! I think they are way overcomplicated to build. All kinds of weird internals leak out of them like `type` being `Text`. I think it makes the reindex testing simpler not to build them. ClientHit, with its handling of source, parent, routing, ttl, and timestamp I think simplifies the rest of the code in nice ways.

I don't think the java http client will help much with this, but I'm happy to wait and see!
</comment><comment author="djschny" created="2016-06-05T14:01:55Z" id="223814698">Most likely not for first round, but something to consider for future iterations is parallelizing of the scrolls. I often would do this when manually re-indexing prior to 2.3. You find a incremental ID field or date field in your data and then break it up evenly filtering on that field so that you can have 4 or however many scrolls happening in parallel.
</comment><comment author="nik9000" created="2016-06-05T17:14:09Z" id="223824951">&gt; parallelizing of the scrolls

I've recommended to some folks to do that. For the most part I think that is on them to launch the parallel reindexes. I was doing that in https://github.com/wikimedia/mediawiki-extensions-CirrusSearch years ago to make PHP play nice. I think, though, that #18237 is a thing we should think about when it gets in. I'm not sure how hard we'll want to work on that vs restart-able reindex though. I think if we could only pick faster or restart-able, we should pick restart-able. If they sort of come together than awesome!
</comment><comment author="djschny" created="2016-06-05T17:46:12Z" id="223826696">&gt; I think, though, that #18237 is a thing we should think about when it gets in.

Oh yeah totally! Thanks for calling that out, I've subscribed

&gt; I think if we could only pick faster or restart-able, we should pick restart-able.

For sure.
</comment><comment author="clintongormley" created="2016-06-07T14:15:53Z" id="224293896">&gt;  I'm not sure how hard we'll want to work on that vs restart-able reindex though. I think if we could only pick faster or restart-able, we should pick restart-able. If they sort of come together than awesome!

The beauty of #18237 is that it will enable both.
</comment><comment author="nik9000" created="2016-06-13T20:05:45Z" id="225692489">I'm going to squash and rebase this before picking it up again. I might do that a few times in the next few days so that I can use the http client when it is merged. The only pending discussion that is on a line is about my use of `myself` as a special case for the whitelist. No one likes the name. I don't like that I need to special case the whitelist, but our tests start Elasticsearch on a random port and the whitelist is a static setting for security's sake.
</comment><comment author="nik9000" created="2016-06-20T19:46:49Z" id="227248701">I'm squashed the last nocommit by moving to URIBuilder.

We still have to figure out what to do about `myself`. The problem is that for testing we want to whitelist hitting the local node remotely. We don't want this in general, just for testing. And when testing we don't know the port we bind to....

I'm going to have a look at what other unit test changes the refactoring asks for next. At this point I think this is just waiting more review and @javanna's HTTP client.
</comment><comment author="nik9000" created="2016-06-24T14:25:02Z" id="228359854">@tlrx I think this is finally ready for another round! I've got it using the Elasticsearch client.
</comment><comment author="nik9000" created="2016-06-28T14:40:17Z" id="229069789">Since there isn't active review right now I'll rebase to remove the conflicts....
</comment><comment author="nik9000" created="2016-06-28T14:44:06Z" id="229071085">&gt; Since there isn't active review right now I'll rebase to remove the conflicts....

It isn't worth it. The conflicts are minor.
</comment><comment author="dakrone" created="2016-06-28T19:01:18Z" id="229148973">Left comments, this is an exciting feature @nik9000!
</comment><comment author="nik9000" created="2016-06-28T20:20:31Z" id="229170427">Thanks for all the review @dakrone ! I'll go through the comments one more time and push an update.
</comment><comment author="nik9000" created="2016-06-28T20:53:23Z" id="229179814">@dakrone Ok - I believe I've either fixed or replied to all of your comments. Thanks for slogging through this one.
</comment><comment author="dakrone" created="2016-06-28T21:40:09Z" id="229192507">&gt; I'd like to wait and implement anything but non-authorized http in another PR.

How about changing this currently so the remote option supports `scheme` which can only be `http` right now? Right now if someone passes `http://localhost:9200` as the URL (which I imagine a _lot_ of people will use that format), they'll get an error like "`[host] can either be of the form [host] or [host:port] but was [http]`" which is a confusing message
</comment><comment author="nik9000" created="2016-06-29T12:32:16Z" id="229342002">&gt; How about changing this currently so the remote option supports scheme which can only be http right now? Right now if someone passes http://localhost:9200 as the URL (which I imagine a lot of people will use that format), they'll get an error like "[host] can either be of the form [host] or [host:port] but was [http]" which is a confusing message

Sure!
</comment><comment author="nik9000" created="2016-06-29T14:53:39Z" id="229381683">@dakrone, I pushed a commit that handles scheme. I'd still prefer to do the auth stuff in another PR, but this is nicer.
</comment><comment author="nik9000" created="2016-06-29T14:55:23Z" id="229382253">And I'll push an update to the docs in a second....
</comment><comment author="nik9000" created="2016-06-29T17:42:00Z" id="229432262">@dakrone I pushed another round of docs.
</comment><comment author="clintongormley" created="2016-06-30T11:37:04Z" id="229634005">@nik9000 I've only looked at the docs. Instead of having default port, scheme, etc, why not just require a FQDN?  It removes all the ambiguity about what scheme to use, what port etc.  The user should just specify the node they want to connect to.

Also, I'd add a note about the query and search params being passed directly to the remote cluster, so should use the syntax accepted by that version of Elasticsearch.
</comment><comment author="nik9000" created="2016-06-30T13:14:11Z" id="229653992">&gt; Instead of having default port, scheme, etc, why not just require a FQDN

I could require `http://otherhost:9200` every time. I could also require:

```
{
  "remote": {
    "host": "otherhost",
    "scheme": "http",
    "port": 9200
  }
}
```

Every time. I'm ok either way. I picked this way because it felt the most true to your original proposal on the issue.
</comment><comment author="nik9000" created="2016-06-30T13:42:42Z" id="229661642">&gt; I picked this way because it felt the most true to your original proposal on the issue.

In defense of the way i have it working now: the vast majority of the time you'll never have to specify a scheme or a port. It'll just "do the right thing" with the scheme and if you want to do something whacky like use https without a password or http with a password you can make it to do without too much trouble. 

But I'm ambivalent. So long as the scheme, host, and port are all available I'm ok with any API.
</comment><comment author="clintongormley" created="2016-06-30T14:39:40Z" id="229678559">&gt; I could require `http://otherhost:9200` every time. 

This would be my preference
</comment><comment author="nik9000" created="2016-06-30T17:00:53Z" id="229721915">OK - I've got that implemented. The rest test infrastructure doesn't support it so I've reached out to @javanna to talk about the right way to get it into the test infrastructure.
</comment><comment author="dakrone" created="2016-06-30T17:16:11Z" id="229726398">Okay, on the code side I think this LGTM, I played with it reindexing 40k docs from a 1.7.5 cluster running locally and it took ~35 seconds, so pretty nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build Failure: org.elasticsearch.search.aggregations.bucket.IpRangeIT.testUnmapped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18584</link><project id="" key="" /><description>Unable to reproduce with

gradle :core:integTest -Dtests.seed=E6A306CD108A9C82 -Dtests.class=org.elasticsearch.search.aggregations.bucket.IpRangeIT -Dtests.method="testUnmapped" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=da -Dtests.timezone=Antarctica/Syowa

Build failure on windows: (http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/3179/testReport/junit/org.elasticsearch.search.aggregations.bucket/IpRangeIT/testUnmapped/)
</description><key id="156848146">18584</key><summary>Build Failure: org.elasticsearch.search.aggregations.bucket.IpRangeIT.testUnmapped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-05-25T20:55:08Z</created><updated>2016-06-28T09:27:24Z</updated><resolved>2016-06-23T15:59:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-23T13:07:37Z" id="228043987">@dakrone this looks like a timing or test issue with shadow replicas?  Could you have a look please
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use resource files for list of modules and plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18583</link><project id="" key="" /><description>This adds modules.txt and plugins.txt to the core jar resource files,
which the install plugin command statically loads, in place of the
previously hardcoded lists (which have often gone out of date).
</description><key id="156845746">18583</key><summary>Use resource files for list of modules and plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-25T20:43:10Z</created><updated>2016-05-26T11:44:13Z</updated><resolved>2016-05-25T21:42:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-25T20:55:31Z" id="221704643">I like it a lot, great improvement. I left a comment about maybe adding some tests for some of the behavior though?
</comment><comment author="rjernst" created="2016-05-25T21:24:48Z" id="221712272">@jasontedor I pushed some more commits adding the tests you asked for.
</comment><comment author="jasontedor" created="2016-05-25T21:41:18Z" id="221716621">The tests are great. Thank you for adding them. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecation suppression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18582</link><project id="" key="" /><description>Failing the build on deprecation warnings was removed in
19b3ec88af95166fd654ce2f8d62a1f9b473ba8f. This commit removes the
suppressed deprecation warnings so that their use is surfaced in the
build now.

Relates #18579 
</description><key id="156844225">18582</key><summary>Remove deprecation suppression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-25T20:35:29Z</created><updated>2016-05-26T09:19:22Z</updated><resolved>2016-05-25T21:15:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-25T21:06:12Z" id="221707525">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Changing staging property to be the hash instead of a boolean</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18581</link><project id="" key="" /><description>With the unified release, we will have staged releases based on a
unified hash (hash of all the hashes), so using the elasticsearch hash
for plugins staging will no longer work. This change makes the
`es.plugins.staging` property take the staging hash it should use.
</description><key id="156832990">18581</key><summary>Plugins: Changing staging property to be the hash instead of a boolean</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-25T19:38:40Z</created><updated>2016-05-26T10:07:13Z</updated><resolved>2016-05-25T22:07:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-25T19:42:46Z" id="221684779">LGTM.
</comment><comment author="jasontedor" created="2016-05-25T20:04:36Z" id="221690586">There are a few places where this is used so needs to be updated: `prepare_release_candidate.py` and `smoke_test_rc.py`
</comment><comment author="rjernst" created="2016-05-25T21:48:23Z" id="221718292">I pushed an update for `smoke_test_rc.py`. `prepare_release_candidate.py` is not currently used and will be going away soon.
</comment><comment author="jasontedor" created="2016-05-25T21:58:00Z" id="221720476">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Remove buildSrc as a project in IDEs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18580</link><project id="" key="" /><description>We currently have a virtual build-tools project which pulls in all the same
IDE settings we have for other projects, so there is no longer a need
for hacking buildSrc into our IDEs.
</description><key id="156827421">18580</key><summary>Build: Remove buildSrc as a project in IDEs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-25T19:10:45Z</created><updated>2016-05-26T09:19:22Z</updated><resolved>2016-05-25T20:55:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-25T19:28:20Z" id="221680901">I'll add this to my list of things to play with so you have an Eclipse user test it. I don't mean to claim it: if someone gets to it before I do then feel free to review.

Code looks sane, I just want to give it a shot inside the IDE to be sure!
</comment><comment author="rjernst" created="2016-05-25T19:28:51Z" id="221681042">I did try it in eclipse, it worked for me.
</comment><comment author="rjernst" created="2016-05-25T19:29:38Z" id="221681235">But please do test yourself too, I can't even crawl in eclipse, so I don't know that things weren't completely broken (but I don't think they were).
</comment><comment author="rmuir" created="2016-05-25T20:45:29Z" id="221701850">+1 to fix this, i have to manually disable this project every time I rebuild (which is every time i run tests!)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: ignore deprecation warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18579</link><project id="" key="" /><description>We currently fail on any deprecations found during the build. However,
this includes things deprecated within ES, which adds a heavy burden in
order to deprecate apis (requring to add suppressions to all internal
callers of the API).

This change adds `-deprecation` to xlint. We should consider in the
future having a task to "see" what deprecated apis we currently use for
analysis.
</description><key id="156822032">18579</key><summary>Build: ignore deprecation warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-25T18:44:07Z</created><updated>2016-05-26T09:19:22Z</updated><resolved>2016-05-25T18:46:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-25T18:45:59Z" id="221669328">+1, if we consider deprecations "serious enough to fail the build" then they should be banned in forbidden apis. It explicitly supports this, for e.g. jdk deprecations.

But the whole purpose of deprecation is to have a transition mechanism, so we should be able to use it properly for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Function Reference Stub to Painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18578</link><project id="" key="" /><description>Added a function reference rule to the ANTLR grammar and stubbed out an E-node.  Throws UnsupportedOperationException if it gets used right now.
</description><key id="156808712">18578</key><summary>Add Function Reference Stub to Painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-25T17:39:04Z</created><updated>2016-05-26T15:32:34Z</updated><resolved>2016-05-26T15:32:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-26T01:51:10Z" id="221757272">+1
</comment><comment author="jdconrad" created="2016-05-26T15:32:34Z" id="221906726">@rmuir: Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify cloud.service.name entry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18577</link><project id="" key="" /><description>_From @ppf2 on June 19, 2015 0:3_

This is more of an Azure thing, but it will be helpful to clarify that cloud service name is the cloud service name/DNS but without the cloudapp.net part, so if the DNS name is "abc.cloudapp.net" then the cloud.service.name to use is just `abc`.

```
cloud.azure.management.cloud.service.name: your_azure_cloud_service_name
```

_Copied from original issue: elastic/elasticsearch-cloud-azure#95_
</description><key id="156788710">18577</key><summary>Clarify cloud.service.name entry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>docs</label></labels><created>2016-05-25T16:03:20Z</created><updated>2016-07-21T12:10:59Z</updated><resolved>2016-07-21T12:10:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Move useful messages from debug/trace level to warn/error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18576</link><project id="" key="" /><description>_From @ppf2 on June 18, 2015 23:56_

The following is an example of log entries that are very useful in troubleshooting azure discovery issues but are only available at the TRACE level:

```
[2015-06-18 23:18:57,662][TRACE][discovery.azure          ] [Baron Strucker] starting to ping
[2015-06-18 23:18:57,692][DEBUG][discovery.azure          ] [Baron Strucker] start building nodes list using Azure API
[2015-06-18 23:19:00,383][WARN ][discovery.azure          ] [Baron Strucker] can not get list of azure nodes: [can not get list of azure nodes]. Returning empty list of nodes.
[2015-06-18 23:19:00,383][TRACE][discovery.azure          ] [Baron Strucker] AzureServiceRemoteException caught
org.elasticsearch.cloud.azure.AzureServiceRemoteException: can not get list of azure nodes
    at org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl.getServiceDetails(AzureComputeServiceImpl.java:100)
    at org.elasticsearch.discovery.azure.AzureUnicastHostsProvider.buildDynamicNodes(AzureUnicastHostsProvider.java:176)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:313)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:219)
    at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:146)
    at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:124)
    at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:996)
    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:360)
    at org.elasticsearch.discovery.zen.ZenDiscovery.access$6100(ZenDiscovery.java:85)
    at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1373)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: com.microsoft.windowsazure.exception.ServiceException: BadRequest: The hosted service name is invalid.
    at com.microsoft.windowsazure.exception.ServiceException.createFromXml(ServiceException.java:216)
    at com.microsoft.windowsazure.management.compute.HostedServiceOperationsImpl.getDetailed(HostedServiceOperationsImpl.java:1740)
    at org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl.getServiceDetails(AzureComputeServiceImpl.java:98)
    ... 12 more
```

Can we move these to warn or error level?

And also move this one to INFO level ([2015-06-18 23:18:41,554][DEBUG][cloud.azure              ] [Baron Strucker] starting azure services?  Just to show that it is actually trying to use azure discovery? 

thx!

_Copied from original issue: elastic/elasticsearch-cloud-azure#94_
</description><key id="156788465">18576</key><summary>Move useful messages from debug/trace level to warn/error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2016-05-25T16:02:18Z</created><updated>2016-07-25T14:54:13Z</updated><resolved>2016-07-25T14:54:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-25T16:02:19Z" id="221623383">I think that at a WARN level we should probably also print the inner message: `BadRequest: The hosted service name is invalid.`
About moving to INFO level `starting azure services` or such message, I think it makes sense.

I'm going to copy this issue to elasticsearch repo.
</comment><comment author="dadoonet" created="2016-07-25T14:54:13Z" id="234977171">As Azure Classic plugin is deprecated in 5.0, I'm closing this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug: org.elasticsearch.test.ESIntegTestCase has randomly failing shards when using excludes in terms aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18575</link><project id="" key="" /><description>In one of our `ESIntegTestCase`s, we set up an index, add a couple of documents and run a number of queries against these indices.
In most cases this works fine, with 1 exception: when using `exclude(String[] terms)` in a `TermsBuilder`, every now and then (not always!), some of the shards fail.
The shard settings of our `ESIntegTestCase` are not changed; so it sets up 1-10 shards.
Whether these aggregations aim at a field on root level or a nested document does not matter - we could reproduce the error in either case (just running the test often enough).
What seems to make a difference: It only happens when applying `exclude`; with `include` we have not seen failing shards yet.
We have not tested the regular expressions; only `String[]`.

two failure messages that we saw on the `SearchResponse` in `.getShardFailures()` (nested document / root document):

```
shard [[DT3n-RvATnO4NDMUrDo4BA][visit][1]], reason [RemoteTransportException[[node_s0][local[1]][indices:data/read/search[phase/query/id]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AssertionError[startIndex=0, numBits=0]; ], cause [java.lang.AssertionError: startIndex=0, numBits=0
    at org.apache.lucene.util.LongBitSet.set(LongBitSet.java:335)
    at org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude$TermListBackedOrdinalsFilter.acceptedGlobalOrdinals(IncludeExclude.java:179)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.getLeafCollector(GlobalOrdinalsStringTermsAggregator.java:97)
    at org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:132)
    at org.elasticsearch.search.aggregations.AggregatorFactory$1$1.collect(AggregatorFactory.java:204)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator$1.collect(ReverseNestedAggregator.java:91)
    at org.elasticsearch.search.aggregations.LeafBucketCollector$3.collect(LeafBucketCollector.java:73)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator$1.collect(FilterAggregator.java:67)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator$1.collect(NestedAggregator.java:118)
    at org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
    at org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:174)
    at org.apache.lucene.search.AssertingLeafCollector.collect(AssertingLeafCollector.java:52)
    at org.apache.lucene.search.AssertingLeafCollector.collect(AssertingLeafCollector.java:52)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:221)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:172)
    at org.apache.lucene.search.AssertingBulkScorer.score(AssertingBulkScorer.java:79)
    at org.apache.lucene.search.AssertingBulkScorer.score(AssertingBulkScorer.java:63)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:821)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:433)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:376)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:373)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

and 

```
shard [[3TcVXShdQq6NoMcYQCwbtQ][visit][5]], reason [RemoteTransportException[[node_s0][local[1]][indices:data/read/search[phase/query]]]; nested: UncheckedExecutionException[QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AssertionError[startIndex=0, numBits=0];]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AssertionError[startIndex=0, numBits=0]; ], cause [com.google.common.util.concurrent.UncheckedExecutionException: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AssertionError[startIndex=0, numBits=0];
    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
    at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
    at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
    at org.elasticsearch.indices.cache.request.IndicesRequestCache.loadIntoContext(IndicesRequestCache.java:263)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:364)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:378)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AssertionError[startIndex=0, numBits=0];
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113)
    at org.elasticsearch.indices.cache.request.IndicesRequestCache$Loader.call(IndicesRequestCache.java:302)
    at org.elasticsearch.indices.cache.request.IndicesRequestCache$Loader.call(IndicesRequestCache.java:283)
    at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
    at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
    at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
    ... 14 more
Caused by: java.lang.AssertionError: startIndex=0, numBits=0
    at org.apache.lucene.util.LongBitSet.set(LongBitSet.java:335)
    at org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude$TermListBackedOrdinalsFilter.acceptedGlobalOrdinals(IncludeExclude.java:179)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.getLeafCollector(GlobalOrdinalsStringTermsAggregator.java:97)
    at org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:132)
    at org.elasticsearch.search.aggregations.AggregatorFactory$1$1.collect(AggregatorFactory.java:204)
    at org.elasticsearch.search.aggregations.LeafBucketCollector$3.collect(LeafBucketCollector.java:73)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator$1.collect(FilterAggregator.java:67)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator$1.collect(NestedAggregator.java:118)
    at org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
    at org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:174)
    at org.apache.lucene.search.AssertingLeafCollector.collect(AssertingLeafCollector.java:52)
    at org.apache.lucene.search.AssertingLeafCollector.collect(AssertingLeafCollector.java:52)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:221)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:172)
    at org.apache.lucene.search.AssertingBulkScorer.score(AssertingBulkScorer.java:79)
    at org.apache.lucene.search.AssertingBulkScorer.score(AssertingBulkScorer.java:63)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:821)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384)
    ... 22 more
```

The format of the response is always the same and correct; so we can still retrieve results; however, if one or two shards fail, the numbers within the response are incorrect; some documents are missing. The easiest way to check whether it worked is to check the number of failed shards.

FYI: We are using **ElasticSearch 2.3.2**
</description><key id="156777164">18575</key><summary>Bug: org.elasticsearch.test.ESIntegTestCase has randomly failing shards when using excludes in terms aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">DavidHauger</reporter><labels><label>:Aggregations</label></labels><created>2016-05-25T15:17:23Z</created><updated>2016-07-18T09:21:11Z</updated><resolved>2016-07-18T09:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-30T09:53:28Z" id="222456960">It looks to me that it can happen when all documents with a value are deleted on a shard and then merged away. Can you describe what your test case is doing?
</comment><comment author="DavidHauger" created="2016-05-30T12:20:36Z" id="222480673">it creates an index, adds a couple of documents (these two steps are done in the setup which is used for a couple of tests), creates a `TermsBuilder` aggregation, sets the `exclude()` with an array of Strings, and performs a query with this `TermsBuilder` aggregation added (and processes the result for further testing).
Well, the query is a bit more complex, but we saw this "shard failing exception" only with the exclude option set; the same test with an include option always worked (we include some values in one case and excluded the others in another test - so the results should be exactly the same).
Within this test we are not deleting (or overriding) any documents. Our test case extends `ESIntegTestCase`; the failing shards are definitely not necessarily empty; because there are 1-2 documents missing in the results, that should lie on these failing shard(s).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only fail relocation target shard if failing source shard is a primary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18574</link><project id="" key="" /><description>If the relocation source fails during the relocation of a shard from one node to another, the
relocation target is currently failed as well. For replica shards this is not necessary,
however, as the actual shard recovery of the relocation target is done via the primary shard.

Closes #16144
</description><key id="156776619">18574</key><summary>Only fail relocation target shard if failing source shard is a primary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-25T15:14:18Z</created><updated>2016-05-31T10:00:42Z</updated><resolved>2016-05-27T13:28:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-26T07:54:22Z" id="221802255">@s1monw can you have a look at this one?
</comment><comment author="s1monw" created="2016-05-26T08:11:16Z" id="221805685">I left one comment @ywelsch - looks good otherwise
</comment><comment author="s1monw" created="2016-05-27T13:24:26Z" id="222145216">much easier to understand - LGTM thanks @ywelsch 
</comment><comment author="ywelsch" created="2016-05-27T13:36:24Z" id="222148039">Thanks for reviewing @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce that minimum_master_nodes is set to a quorum of master-eligible nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18573</link><project id="" key="" /><description>The setting minimum_master_nodes is crucial to avoid split brains in a cluster. With PR #15625 we log a warning if minimum_master_nodes is set to less than a quorum of master-eligible nodes.

In the future we could not only log a warning but also add some checks to enforce that minimum_master_nodes is properly used. Updating the setting to a value that brings it down to less than a quorum of master-eligible nodes could for example be prohibited. (Note that this might need special care for the case where the cluster is down-scaled from two to one master-eligible node).

Long term we could even think about preventing a node from joining a cluster and fail elections if it will bring minimum_master_nodes to an illegal value (and we're in production mode).
</description><key id="156775546">18573</key><summary>Enforce that minimum_master_nodes is set to a quorum of master-eligible nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Discovery</label><label>enhancement</label></labels><created>2016-05-25T15:10:01Z</created><updated>2016-05-25T15:10:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Compile each Groovy script in its own classloader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18572</link><project id="" key="" /><description>From @rmuir: 

Creating many groovy scripts effectively creates tons of java class instances that will never be released (cached classloader in GroovyScriptEngineService = disaster).

The only way this classloader ever gets "released" is if you close() the entire GroovyScriptEngineService and then that gets garbage collected.

On the other hand, lucene expressions don't work this way. Instead we create a classloader for each script, that just has that one generated java class and nothing else, ensuring sane garbage collection, and no problems if you do this :) 

But for groovy? I think the current way we integrate groovy basically equates to a permanent memory leak for every single unique script that ever gets compiled.
</description><key id="156768190">18572</key><summary>Compile each Groovy script in its own classloader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2016-05-25T14:40:42Z</created><updated>2016-09-19T11:10:08Z</updated><resolved>2016-06-20T08:32:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2016-09-19T11:10:08Z" id="247965823">&gt; The only way this classloader ever gets "released" is if you close() the entire GroovyScriptEngineService and then that gets garbage collected.

You can unload Groovy classes from a `GroovyClassLoader` by

```
for (Class&lt;?&gt; groovyClass : groovyClassLoader.getLoadedClasses()) {
    GroovySystem.getMetaClassRegistry().removeMetaClass(c);
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix remove of azure files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18571</link><project id="" key="" /><description>Probably when we updated Azure SDK, we introduced a regression.
Actually, we are not able to remove files anymore.

For example, if you register a new azure repository, the snapshot service tries to create a temp file and then remove it.
Removing does not work and you can see in logs:

```
[2016-05-18 11:03:24,914][WARN ][org.elasticsearch.cloud.azure.blobstore] [azure] can not remove [tests-ilmRPJ8URU-sh18yj38O6g/] in container {elasticsearch-snapshots}: The specified blob does not exist.
```

This fix deals with that. It now list all the files in a flatten mode, remove in the full URL the server and the container name.

As an example, when you are removing a blob which full name is `https://dpi24329.blob.core.windows.net/elasticsearch-snapshots/bar/test` you need to actually call Azure SDK with `bar/test` as the path, `elasticsearch-snapshots` is the container.

Related to #16472.
Related to #18436.

Backport of #18451 in 2.x branch

To test it, I ran some manual tests:

On my laptop, create a file `/path/to/azure/config/elasticsearch.yml`:

``` yml
cloud.azure.storage.default.account: ACCOUNT
cloud.azure.storage.default.key: KEY
```

Run `AzureRepositoryF#main()` with `-Des.cluster.routing.allocation.disk.threshold_enabled=false -Des.path.home=/path/to/azure/` options.

Then run:

``` sh
curl -XDELETE localhost:9200/foo?pretty
curl -XDELETE localhost:9200/_snapshot/my_backup1?pretty
curl -XPUT localhost:9200/foo/bar/1?pretty -d '{
 "foo": "bar"
}'
curl -XPOST localhost:9200/foo/_refresh?pretty
curl -XGET localhost:9200/foo/_count?pretty
curl -XPUT localhost:9200/_snapshot/my_backup1?pretty -d '{
   "type": "azure"
}'

curl -XPOST "localhost:9200/_snapshot/my_backup1/snap1?pretty&amp;wait_for_completion=true"
curl -XDELETE localhost:9200/foo?pretty
curl -XPOST "localhost:9200/_snapshot/my_backup1/snap1/_restore?pretty&amp;wait_for_completion=true"
curl -XGET localhost:9200/foo/_count?pretty
```

Then check files we have on azure platform using the console.
Then run:

``` sh
curl -XDELETE localhost:9200/_snapshot/my_backup1/snap1?pretty
```

Then check files we have on azure platform using the console and verify that everything has been cleaned.
</description><key id="156766376">18571</key><summary>Fix remove of azure files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>bug</label><label>v2.4.0</label></labels><created>2016-05-25T14:33:22Z</created><updated>2016-05-27T15:27:53Z</updated><resolved>2016-05-27T15:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-25T14:34:34Z" id="221595586">@imotov I backported #18451 for 2.x branch. Would you mind giving a quick look?
</comment><comment author="imotov" created="2016-05-27T14:42:06Z" id="222164507">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>span_first gives unexpected results on a multi-value field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18570</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.3.1

**JVM version**:
1.8.0_77

**OS version**:
Ubuntu 14.04.1 LTS

**Description of the problem including expected versus actual behavior**:
When performing a `span_first` query on a string field containing multiple strings, it will only match on the first element in the string array and not on any of the following strings.

Most probably this is due to [an implementation detail](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/position-increment-gap.html).

As a user, I expect span_first to work on each element of a multi-value field and not on the concatenation of all these values.

**Steps to reproduce**:
https://www.found.no/play/gist/5f69f3eea73502a78d407764f7dbb23b
</description><key id="156753028">18570</key><summary>span_first gives unexpected results on a multi-value field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mcuelenaere</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2016-05-25T13:38:29Z</created><updated>2016-05-25T13:49:35Z</updated><resolved>2016-05-25T13:44:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mcuelenaere" created="2016-05-25T13:39:44Z" id="221578972">A workaround is to create multiple string fields, one for each entry in the array of strings. However this would only work for a relatively fixed amount of array entries.

Is there another way of achieving this?
</comment><comment author="clintongormley" created="2016-05-25T13:44:53Z" id="221580447">&gt; Most probably this is due to an implementation detail.

Yes and no...  without the position increment gap the first word of each value would be indexed as synonyms (ie both in position 0), and same for the second words etc. It's just how Lucene works.

I think the only way to work around this is to use `nested` fields.  
</comment><comment author="mcuelenaere" created="2016-05-25T13:49:35Z" id="221581845">Yes, I just came to the same conclusion. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change `include_global_state` default to `false`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18569</link><project id="" key="" /><description>It is trappy to restore an index and suddenly have your global state change, eg see https://github.com/elastic/elasticsearch/issues/18559

I think state should only be restored if specifically requested.  We should change the `include_global_state` default to `false`
</description><key id="156737972">18569</key><summary>Change `include_global_state` default to `false`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label><label>v5.0.0-alpha4</label></labels><created>2016-05-25T12:28:35Z</created><updated>2016-06-08T17:15:16Z</updated><resolved>2016-06-08T17:15:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>I couldn't install ICU plugin .</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18568</link><project id="" key="" /><description> curl -XGET 'localhost:9200' 
{
  "name" : "D'Spayre",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.3.1",
    "build_hash" : "bd980929010aef404e7cb0843e61d0665269fc39",
    "build_timestamp" : "2016-04-04T12:25:05Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
webonise@webonise-Desktop:/usr/share/elasticsearch$ sudo /etc/init.d/elasticsearch stop
[sudo] password for webonise: 
- Stopping Elasticsearch Server                                                                                                                                   [ OK ] 
  Desktop:/usr/share/elasticsearch$ sudo bin/plugin install elasticsearch/elasticsearch-analysis-icu/2.7.0
  -&gt; Installing elasticsearch/elasticsearch-analysis-icu/2.7.0...
  Trying https://download.elastic.co/elasticsearch/elasticsearch-analysis-icu/elasticsearch-analysis-icu-2.7.0.zip ...
  Downloading ..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
  Verifying https://download.elastic.co/elasticsearch/elasticsearch-analysis-icu/elasticsearch-analysis-icu-2.7.0.zip checksums if available ...
  NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
  ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip
  -Desktop:/usr/share/elasticsearch$ sudo bin/plugin --install elasticsearch/elasticsearch-analysis-icu/2.7.0
  ERROR: unknown command [--install]. Use [-h] option to list available commands
</description><key id="156706245">18568</key><summary>I couldn't install ICU plugin .</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Tenzil</reporter><labels /><created>2016-05-25T09:41:40Z</created><updated>2016-05-25T09:43:54Z</updated><resolved>2016-05-25T09:43:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-25T09:43:54Z" id="221524483">Please prefer https://discuss.elastic.co/ for questions.

read https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-icu.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested inner hits shouldn't use relative paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18567</link><project id="" key="" /><description>Like on other places in the query dsl the full field name should be used.

Before this change this wasn't the case for nested inner hits when source filtering was used.
Highlighting had a workaround, which is now removed as the source of nested inner hits can only be referred by the full name.

PR for #16653
</description><key id="156692503">18567</key><summary>Nested inner hits shouldn't use relative paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>breaking</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-25T08:34:39Z</created><updated>2016-05-27T11:57:21Z</updated><resolved>2016-05-27T11:57:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-26T14:25:07Z" id="221886054">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy script throws java.lang.NoClassDefFoundError: org/codehaus/groovy/runtime/wrappers/Wrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18566</link><project id="" key="" /><description>On my ES cluster running the latest 2.3.3 release (with groovy scripting enabled), I get the following error...

```
{
  "error": {
    "root_cause": [
      {
        "type": "script_exception",
        "reason": "failed to run inline script [def rparts = ctx._source.myDate.split('T'); def rtime = rparts[1].split(':'); ctx._source.myMinute = ((rtime[0] as int) * 60) + (rtime[1] as int)] using lang [groovy]"
      }
    ],
    "type": "script_exception",
    "reason": "failed to run inline script [def rparts = ctx._source.myDate.split('T'); def rtime = rparts[1].split(':'); ctx._source.myMinute = ((rtime[0] as int) * 60) + (rtime[1] as int)] using lang [groovy]",
    "caused_by": {
      "type": "bootstrap_method_error",
      "reason": "java.lang.NoClassDefFoundError: org/codehaus/groovy/runtime/wrappers/Wrapper",
      "caused_by": {
        "type": "no_class_def_found_error",
        "reason": "org/codehaus/groovy/runtime/wrappers/Wrapper",
        "caused_by": {
          "type": "class_not_found_exception",
          "reason": "org.codehaus.groovy.runtime.wrappers.Wrapper"
        }
      }
    }
  },
  "status": 500
}
```

...when I execute the following update-by-query

```
POST my-index/_update_by_query
{
  "script": {
    "inline": "def rparts = ctx._source.myDate.split('T'); def rtime = rparts[1].split(':'); ctx._source.myMinute = ((rtime[0] as int) * 60) + (rtime[1] as int)"
  },
  "query": {
    "bool": {
      "filter": {
        "missing": {
          "field": "myMinute"
        }
      }
    }
  }
}
```

It is worth noting that if I add the following class permission to the grants of the `/modules/lang-groovy/plugin-security.policy` file and restart my cluster, the script executes fine.

`permission org.elasticsearch.script.ClassPermission "org.codehaus.groovy.runtime.wrappers.Wrapper";`

It is very similar to what's been reported [here](https://github.com/docker-library/elasticsearch/issues/90) and to some extent [this issue](https://github.com/elastic/elasticsearch/issues/16657).
</description><key id="156683411">18566</key><summary>Groovy script throws java.lang.NoClassDefFoundError: org/codehaus/groovy/runtime/wrappers/Wrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">consulthys</reporter><labels /><created>2016-05-25T07:43:15Z</created><updated>2016-08-11T13:07:36Z</updated><resolved>2016-05-25T11:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="consulthys" created="2016-05-25T11:47:51Z" id="221549637">I've figured out that the `as int` type coercion is what's causing the error.

If I change the script to use the explicit `new Integer(...)` constructor instead of using type coercions, then it works flawlessly.

```
"inline": "def rparts = ctx._source.myDate.split('T'); def rtime = rparts[1].split(':'); ctx._source.myMinute = (new Integer(rtime[0]) * 60) + new Integer(rtime[1])"
```

I still think the class permission should be added to the policy file of the `lang-groovy` module, since type coercion is a perfectly valid groovy construct.
</comment><comment author="rmuir" created="2016-05-25T11:48:36Z" id="221549788">These classes are not whitelisted, because groovy "wrappers" are extremely insecure. For example they augment any List with an `execute` method.
</comment><comment author="consulthys" created="2016-05-25T11:51:42Z" id="221550381">That's fair enough, no big deal since I have an alternative.
Thanks much for your input, Robert. 
</comment><comment author="cfuerst" created="2016-08-11T13:07:35Z" id="239155332">thnx alot @consulthys for sharing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop 1.x BWC and cut over to Writeable for Translog.Operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18565</link><project id="" key="" /><description>We still maintain BWC for the translog operations back to [1.1](https://github.com/elastic/elasticsearch/pull/4993) which is not
supported in the current version anyway. This commit drops the bwc and moves
the operations to the Writeable interface enforcing immutability.
</description><key id="156682940">18565</key><summary>Drop 1.x BWC and cut over to Writeable for Translog.Operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-25T07:40:14Z</created><updated>2016-05-26T09:19:22Z</updated><resolved>2016-05-25T09:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-25T09:47:54Z" id="221525420">LGTM. Good to clean this up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix simulate spec test for new exception handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18564</link><project id="" key="" /><description>test is currently failing, this will fix that
</description><key id="156666254">18564</key><summary>fix simulate spec test for new exception handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>test</label></labels><created>2016-05-25T05:28:31Z</created><updated>2016-05-25T05:32:50Z</updated><resolved>2016-05-25T05:32:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-25T05:32:03Z" id="221477069">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed NPE when percolator filter option is "empty".</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18563</link><project id="" key="" /><description>Any "empty" (null) percolator filter option resulted in an NPE:
`GET /index/type/doc/_percolate { "filter": { "or": { "filters": [ {} ] } } }`
`GET /index/type/_percolate { "filter": {} }`

This PR updates the logic to ignore the filter option if it is `null` (all percolators executed).
Not an issue on current master (percolator was rewritten).

Not possible to integration test since [PercolateRequestBuilder](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java) and [PercolateSourceBuilder](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java) had the `filter` option removed in this commit: [Query DSL: Remove filter parsers.](https://github.com/elastic/elasticsearch/commit/a0af88e99630b9d3f0c2cf4997f2e82f1f834d41) Also these filters are deprecated.

Closes #6172 
</description><key id="156663840">18563</key><summary>Fixed NPE when percolator filter option is "empty".</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qwerty4030</reporter><labels><label>:Percolator</label><label>bug</label><label>v2.3.4</label><label>v2.4.0</label></labels><created>2016-05-25T05:02:31Z</created><updated>2016-05-25T09:56:41Z</updated><resolved>2016-05-25T09:56:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-05-25T08:57:30Z" id="221513667">@qwerty4030 Thanks! I'll merge shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>error message for heap size bootstrap check does not guide user</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18562</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha3 internal RC 1

**JVM version**: 1.8.0_74-b02

**OS version**: OSX 10.11.4

**Description of the problem including expected versus actual behavior**: 
After binding elasticsearch to a public facing network interface the bootstrap checks kick in. They are great and usually very directly and immediately allow the user to know what setting to set immediately. For example with the `discovery.zen.minimum_master_nodes`. However the heap size check states there is a problem but does not guide the user towards an example command line argument or property to set to remediate the problem as clear as the other bootstrap checks do. Not the end of the world, but definitely I think some quick wording could help remediate and provide a better user experience.

**Steps to reproduce**:
1. Startup elasticsearch binding to `0.0.0.0`
2. Notice the message for heap size does not directly guide the user on how to set them to be the same.

**Provide logs (if relevant)**:

```
djschny:elasticsearch-5.0.0-alpha3 djschny$ bin/elasticsearch -E network.host=0.0.0.0 -E discovery.zen.minimum_master_nodes=1
[2016-05-24 21:49:34,540][INFO ][node                     ] [Jim Hammond] version[5.0.0-alpha3], pid[43525], build[b3a8c54/2016-05-24T18:55:09.209Z]
[2016-05-24 21:49:34,541][INFO ][node                     ] [Jim Hammond] initializing ...
[2016-05-24 21:49:35,141][INFO ][plugins                  ] [Jim Hammond] modules [percolator, lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-05-24 21:49:35,172][INFO ][env                      ] [Jim Hammond] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [148.2gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2016-05-24 21:49:35,172][INFO ][env                      ] [Jim Hammond] heap size [1.9gb], compressed ordinary object pointers [true]
[2016-05-24 21:49:37,310][INFO ][node                     ] [Jim Hammond] initialized
[2016-05-24 21:49:37,311][INFO ][node                     ] [Jim Hammond] starting ...
[2016-05-24 21:49:37,388][INFO ][transport                ] [Jim Hammond] publish_address {192.168.43.85:9300}, bound_addresses {[::]:9300}
Exception in thread "main" java.lang.RuntimeException: bootstrap checks failed
initial heap size [268435456] not equal to maximum heap size [2147483648]; this can cause resize pauses and prevents mlockall from locking the entire heap
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:125)
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:85)
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:65)
    at org.elasticsearch.bootstrap.Bootstrap$5.validateNodeBeforeAcceptingRequests(Bootstrap.java:183)
    at org.elasticsearch.node.Node.start(Node.java:323)
    at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:198)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:257)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
    at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
Refer to the log for complete error details.
```

Ultimately I did the following to make this work:

`ES_JAVA_OPTS="-Xms1024m -Xmx1024m" bin/elasticsearch -E network.host=0.0.0.0 -E discovery.zen.minimum_master_nodes=1`

So perhaps guidance towards the `ES_JAVA_OPTS` example I think would go a long way. Once again minor in regards to technical, but I believe huge in regards to getting to a running state quicker.
</description><key id="156654882">18562</key><summary>error message for heap size bootstrap check does not guide user</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Exceptions</label><label>enhancement</label></labels><created>2016-05-25T03:19:13Z</created><updated>2016-10-22T07:28:57Z</updated><resolved>2016-05-27T22:46:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-25T03:26:12Z" id="221463958">Thanks for reporting. I think that all the bootstrap checks could use additional guidance. I will work to improve. 

Do note though that the goal isn't to get to a running state quickly but to get to a viable production state. 
</comment><comment author="jasontedor" created="2016-05-27T22:46:30Z" id="222269413">I pushed bootstrap check docs in #18605.
</comment><comment author="DreadPirateRob" created="2016-08-02T17:21:16Z" id="236976362">For others finding this via google search, I fixed this by editing `/etc/elasticsearch/jvm.options` file from this: 

```
-Xms1024m
-Xmx2g
```

to this: 

```
-Xms2g
-Xmx2g
```
</comment><comment author="ebuildy" created="2016-09-19T21:42:36Z" id="248136303">That means the release doesnot work without this fix ? (I had this "bug", @DreadPirateRob "fix" did the trick, thanks you).
</comment><comment author="jasontedor" created="2016-09-19T21:44:45Z" id="248136826">The next pre-release of Elasticsearch will have the defaults configured such that the default configuration will no longer run into this issue.
</comment><comment author="vanga" created="2016-10-22T06:56:04Z" id="255511709">These bootstrap checks are big big hindrance in getting a cluster up.
</comment><comment author="dadoonet" created="2016-10-22T07:28:57Z" id="255512821">@vanga I'd recommend reading https://www.elastic.co/blog/bootstrap_checks_annoying_instead_of_devastating
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow Other methods of s3 server-side encryption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18561</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**: 

Currently, by setting server_side_encryption to true, the only method that is allowed is aes256. It would be great if we could specify other s3 encryption methods, e.g. aws:kms.

For aws:kms, if the bucket has a default key, it's just a matter of including the correct header, so one possibility may be an additional parameter to specify the value of the `x-amz-server-side-encryption` header to get sent with requests. But the ability to specify an ams key to use would also be great.
</description><key id="156653095">18561</key><summary>Allow Other methods of s3 server-side encryption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tylerfontaine</reporter><labels><label>:Plugin Cloud AWS</label><label>adoptme</label><label>enhancement</label></labels><created>2016-05-25T02:59:42Z</created><updated>2017-05-08T15:20:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-25T08:34:11Z" id="221508236">Related doc: http://docs.aws.amazon.com/AmazonS3/latest/dev/client-side-using-kms-java.html
</comment><comment author="smarks" created="2017-05-08T15:20:09Z" id="299897644">I would like to work on this. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Make painless score tests unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18560</link><project id="" key="" /><description>This change simply removes the need for single node tests from score
tests.
</description><key id="156652721">18560</key><summary>Tests: Make painless score tests unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugin Lang Painless</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-25T02:55:33Z</created><updated>2016-05-26T09:19:22Z</updated><resolved>2016-05-25T15:44:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-25T02:55:42Z" id="221460606">/cc @jdconrad 
</comment><comment author="jdconrad" created="2016-05-25T15:15:39Z" id="221608906">LGTM.  Thanks, @rjernst!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index restore overwrites index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18559</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0_91

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:
Moving from a 1.7.1 cluster to a 2.3.1 cluster I set up a new index template in 2.3.1 through the API with the intention of restoring indexes from the 1.7.1 cluster and reindexing them to use the new template. However, whenever I restore an index from a snapshot it clobbers my new index template and replaces it with what seems to be the template of the index that was restored. I would expect that the template that I set up would be maintained instead of being overwritten. When I re-update the template it works fine (until I restore another index).

**Steps to reproduce**:
1. Back up an index on a 1.7 cluster
2. Add an index template with some modification of that index's template to a 2.3 cluster (change shard count, set doc_values on a field, etc.)
3. Restore 1.7 index onto 2.3 cluster
4. Check index template on 2.3 cluster

**Provide logs (if relevant)**:
This was in the logs at about the same time that the index finished restoring:
[_node-id_] [_index-name_] re-syncing mappings with cluster state because of types [[_type_]]
</description><key id="156648988">18559</key><summary>Index restore overwrites index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dhaworth</reporter><labels /><created>2016-05-25T02:17:32Z</created><updated>2016-05-25T13:24:10Z</updated><resolved>2016-05-25T12:29:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dhaworth" created="2016-05-25T04:43:08Z" id="221471736">As an update to this, if I rename the index template before restoring the index it does not get overwritten, but instead I end up with two templates. I guess I would expect the settings and mappings to travel with the index through the snapshot/restore process, but I wouldn't expect a template to.
</comment><comment author="clintongormley" created="2016-05-25T12:29:19Z" id="221559534">@dhaworth A template is a completely distinct entity from an index.  The index has no link to the template, it just uses it as a template when it is first created.  If you want to preserve the templates that you have in the new cluster, you should set `include_global_state` to `false`.

In fact, I think `include_global_state` should default to `false`, so I've opened https://github.com/elastic/elasticsearch/issues/18569
</comment><comment author="dhaworth" created="2016-05-25T13:16:32Z" id="221572465">@clintongormley - Ah, I see. I was not aware of that setting. Thank you for pointing me in the right direction.

I was just looking into it and found this case as well, which also recommends what you're suggesting for the reasons I encountered:
https://github.com/elastic/elasticsearch/issues/15260
</comment><comment author="clintongormley" created="2016-05-25T13:24:10Z" id="221574640">ah thanks @dhaworth 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] org.elasticsearch.indices.IndicesServiceTests.testDeleteIndexStore fails due to state file not existing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18558</link><project id="" key="" /><description>https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/636/console

The failure is:

```
ERROR   0.14s J1 | IndicesServiceTests.testDeleteIndexStore &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: ElasticsearchException[java.io.IOException: failed to read [id:1, legacy:false, file:/var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-intake/core/build/testrun/test/J1/temp/org.elasticsearch.indices.IndicesServiceTests_678304BAB99F89CE-001/tempDir-011/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[3730156636843346220]-HASH=[2CC2E218290088]/nodes/0/_state/global-1.st]]; nested: IOException[failed to read [id:1, legacy:false, file:/var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-intake/core/build/testrun/test/J1/temp/org.elasticsearch.indices.IndicesServiceTests_678304BAB99F89CE-001/tempDir-011/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[3730156636843346220]-HASH=[2CC2E218290088]/nodes/0/_state/global-1.st]]; nested: NoSuchFileException[/var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-intake/core/build/testrun/test/J1/temp/org.elasticsearch.indices.IndicesServiceTests_678304BAB99F89CE-001/tempDir-011/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[3730156636843346220]-HASH=[2CC2E218290088]/nodes/0/_state/global-1.st];
   &gt;    at __randomizedtesting.SeedInfo.seed([678304BAB99F89CE:D34003C59E7578EF]:0)
   &gt;    at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:164)
   &gt;    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:321)
   &gt;    at org.elasticsearch.gateway.MetaStateService.loadGlobalState(MetaStateService.java:109)
   &gt;    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:54)
   &gt;    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:102)
   &gt;    at org.elasticsearch.indices.IndicesServiceTests.testDeleteIndexStore(IndicesServiceTests.java:128)
   &gt;    at java.lang.Thread.run(Thread.java:745)
   &gt; Caused by: java.io.IOException: failed to read [id:1, legacy:false, file:/var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-intake/core/build/testrun/test/J1/temp/org.elasticsearch.indices.IndicesServiceTests_678304BAB99F89CE-001/tempDir-011/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[3730156636843346220]-HASH=[2CC2E218290088]/nodes/0/_state/global-1.st]
   &gt;    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:316)
   &gt;    ... 41 more
   &gt; Caused by: java.nio.file.NoSuchFileException: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-intake/core/build/testrun/test/J1/temp/org.elasticsearch.indices.IndicesServiceTests_678304BAB99F89CE-001/tempDir-011/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[3730156636843346220]-HASH=[2CC2E218290088]/nodes/0/_state/global-1.st
   &gt;    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
   &gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
   &gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
   &gt;    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
   &gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
   &gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
   &gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
   &gt;    at org.apache.lucene.mockfile.HandleTrackingFS.newByteChannel(HandleTrackingFS.java:240)
   &gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
   &gt;    at org.apache.lucene.mockfile.HandleTrackingFS.newByteChannel(HandleTrackingFS.java:240)
   &gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
   &gt;    at java.nio.file.Files.newByteChannel(Files.java:361)
   &gt;    at java.nio.file.Files.newByteChannel(Files.java:407)
   &gt;    at org.apache.lucene.store.SimpleFSDirectory.openInput(SimpleFSDirectory.java:77)
   &gt;    at org.elasticsearch.gateway.MetaDataStateFormat.read(MetaDataStateFormat.java:182)
   &gt;    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:311)
   &gt;    ... 41 more
```

However, it does not reproduce, here's the reproduction line:

```
gradle :core:test -Dtests.seed=678304BAB99F89CE -Dtests.class=org.elasticsearch.indices.IndicesServiceTests -Dtests.method="testDeleteIndexStore" -Dtests.security.manager=true -Dtests.locale=sv-SE -Dtests.timezone=Asia/Macau
```
</description><key id="156603941">18558</key><summary>[CI] org.elasticsearch.indices.IndicesServiceTests.testDeleteIndexStore fails due to state file not existing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2016-05-24T20:40:51Z</created><updated>2016-06-01T13:22:55Z</updated><resolved>2016-06-01T13:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-24T20:55:34Z" id="221398381">Could be related (though not the same) as: https://github.com/elastic/elasticsearch/issues/17767
</comment><comment author="ywelsch" created="2016-05-26T11:37:14Z" id="221847588">The lines that lead to the failure are

```
assertAcked(client().admin().indices().prepareDelete("test"));
meta = gwMetaState.loadMetaState();
```

To understand why this fails, it is important to know that acknowledgment of deletion can happen before the new cluster state containing the deletion has been written to disk, even if the cluster is only single-node.
Now, the way `loadMetaState` works is by listing the directory contents to find the _latest_ meta state file.  Latest means state file with highest version identifier attached to it. In a second step, it tries to read the file. If the meta state file has been replaced in the meanwhile by an updated version (and the old version deleted), we try to read the file that has been deleted.

loadMetaState() is only safe to call from the cluster state update thread, the same thread where we write the state files.

To fix the test, we could add the line

```
assertBusy(() -&gt; assertThat(client().admin().cluster().preparePendingClusterTasks().get().getPendingTasks(), empty()));
```

between the index deletion and loadMetaState(). @bleskes what do you think?
</comment><comment author="bleskes" created="2016-05-26T11:50:26Z" id="221849910">I think there are two things that are wrong here:

1) Delete index should wait for both node ack and cluster publishing to complete.
2) loadMetaState() and state writign should be thread safe. It's very trappy now.  We could do through in memory locks, but file based RW locks will be better. @s1monw any thoughts here?
</comment><comment author="ywelsch" created="2016-05-26T17:18:14Z" id="221936070">I've opened #18602 for the first issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log OS and JVM on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18557</link><project id="" key="" /><description>This commit adds the OS and JVM to the initial logline on startup.
</description><key id="156593581">18557</key><summary>Log OS and JVM on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-24T19:48:26Z</created><updated>2016-05-26T09:19:22Z</updated><resolved>2016-05-24T20:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-24T19:49:02Z" id="221381973">This produces logging like:

`[2016-05-24 15:44:44,602][INFO ][node                     ] [Astronomer] version[5.0.0-alpha3-SNAPSHOT], pid[21656], build[f210605/2016-05-24T19:43:08.002Z], OS[Linux/4.2.3-300.fc23.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_92/25.92-b14]`

The additional pieces over what is already there being:

`OS[Linux/4.2.3-300.fc23.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_92/25.92-b14]`
</comment><comment author="s1monw" created="2016-05-24T20:35:15Z" id="221393255">LGTM +1 good change
</comment><comment author="jasontedor" created="2016-05-24T20:56:01Z" id="221398497">Thanks @s1monw.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Use dash for deb package version and underscore for rpm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18556</link><project id="" key="" /><description>This change tweaks the metadata version associated with the deb and rpm
packages. For rpm, dashes are allowed in versions, so we can use the
version as it already exists. For rpm, dashes are not allowed, so this
uses underscores instead. This only affects prerelease versions (eg
alpha/beta/rc), and usually someone doesn't specify the version when installing, so the
inconsistency doesn't matter that much.
</description><key id="156586879">18556</key><summary>Build: Use dash for deb package version and underscore for rpm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-24T19:14:38Z</created><updated>2016-05-26T09:19:22Z</updated><resolved>2016-05-24T20:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-24T19:51:05Z" id="221382467">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename ThreadContext Persistent Headers to Request Headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18555</link><project id="" key="" /><description>This change will be exposed to consumers of the `ThreadContext` after https://github.com/elastic/elasticsearch/pull/17804 is merged.

The change:
- put/get Header methods will include `Request` in the name so that the type of header is clear.

Splitting the two makes #17804 easier to backport.
</description><key id="156582597">18555</key><summary>Rename ThreadContext Persistent Headers to Request Headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.4.4</label><label>v6.0.0</label></labels><created>2016-05-24T18:53:05Z</created><updated>2017-06-27T10:28:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-10-18T07:44:32Z" id="254431667">@pickypg is this still relevant?
</comment><comment author="pickypg" created="2016-10-18T15:20:19Z" id="254541462">@clintongormley Yeah, I'll push out a PR soon for this one. It's definitely a minor change though (just function names for clarity).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove cluster name from data path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18554</link><project id="" key="" /><description>Previously Elasticsearch used $DATA_DIR/$CLUSTER_NAME/nodes for the path
where data is stored, this commit changes that to be $DATA_DIR/nodes.

On startup, if the old folder structure is detected it will be upgraded
to the new folder structure.

Resolves #17810
</description><key id="156577498">18554</key><summary>Remove cluster name from data path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Core</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha4</label></labels><created>2016-05-24T18:29:32Z</created><updated>2016-06-07T16:35:07Z</updated><resolved>2016-06-07T16:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-25T12:12:36Z" id="221555020">@dakrone What happens if the user has multiple clusters in the same data path?  We shouldn't upgrade just one cluster, we should throw an exception instead.

In https://github.com/elastic/elasticsearch/issues/17810#issuecomment-220568052 we came to the conclusion that the safest thing was to complain if the data path contained a folder other than `nodes`, and let the user fix it.  But I agree that by far the commonest case is a single cluster, which can be upgraded safely.  
</comment><comment author="dakrone" created="2016-05-25T13:34:23Z" id="221577432">&gt; What happens if the user has multiple clusters in the same data path? We shouldn't upgrade just one cluster, we should throw an exception instead.

This still uses the cluster name to look for the folder, so if the cluster is named `elasticsearch` then it looks for `$DATA_DIR/elasticsearch` to upgrade, if there are other clusters it leaves them alone.

Let's assume someone has the following, `/tmp/data` is configured as `path.data`:

```
/tmp/data/elasticsearch/nodes/0/&lt;data&gt;
/tmp/data/mycluster/nodes/0/&lt;data&gt;
```

So they have two cluster configured, `elasticsearch` and `mycluster`.

Let's say they start ES 5.0 with the cluster name as `elasticsearch`, ES will "upgrade" (move) the `/tmp/data/elasticsearch/nodes` directory to `/tmp/data/nodes`, so now you will have:

```
/tmp/data/nodes/0/&lt;data&gt;
/tmp/data/mycluster/nodes/0/&lt;data&gt;
```

Now, if you shut down ES and start it back up with the cluster name `mycluster`, ES will see that it's already been upgraded because there's already a `nodes` folder and won't do anything.

That was me clarifying the current behavior (for anyone who reads this at a later time).

@clintongormley I'll change this to throw an exception if there exists any folder other than the cluster name or `nodes` in the `data.path` directory (though I think that's pretty strict, for someone who may have something like `nodes_backup` in the same directory when swapping data in and out)
</comment><comment author="clintongormley" created="2016-05-25T13:49:11Z" id="221581727">&gt; @clintongormley I'll change this to throw an exception if there exists any folder other than the cluster name or nodes in the data.path directory 

thanks

&gt; (though I think that's pretty strict, for someone who may have something like nodes_backup in the same directory when swapping data in and out)

Sure, but it is an easy thing for the user to fix.
</comment><comment author="dakrone" created="2016-05-25T14:53:08Z" id="221601692">@clintongormley I pushed a commit that throws this exception, this now looks like:

```
&#955; bin/elasticsearch
[2016-05-25 08:50:50,838][INFO ][node                     ] [Lupa] version[5.0.0-SNAPSHOT], pid[25769], build[aaf2d49/2016-05-25T14:47:22.057Z]
[2016-05-25 08:50:50,839][INFO ][node                     ] [Lupa] initializing ...
[2016-05-25 08:50:51,269][INFO ][plugins                  ] [Lupa] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
Exception in thread "main" ElasticsearchException[path.data [/home/hinmanm/scratch/elasticsearch-5.0.0-SNAPSHOT/data] contains unknown file [/home/hinmanm/scratch/elasticsearch-5.0.0-SNAPSHOT/data/es]]
    at org.elasticsearch.env.NodeEnvironment.upgradeDataPath(NodeEnvironment.java:296)
    at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:196)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:193)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
    at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:180)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:180)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:255)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
    at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
```
</comment><comment author="s1monw" created="2016-06-01T07:37:30Z" id="222916181">@clintongormley @dakrone @bleskes I am still concerned what happens if somebody is still using the data directory from another nodes installation? There might be a node running from version 2.x and we will basically wipe it's data directory entirely? I am not sure we can automatically upgrade this thing unless we acquire all possible locks in all the clusters directories but even that is just best effort?
</comment><comment author="bleskes" created="2016-06-01T07:50:26Z" id="222918644">I agree we should be very careful here. IMO we should only auto upgrade in the most straightforward case - there is 1 folder under the data path, it's named exactly like the cluster name and it has a nodes folder underneath it. All the rest should fail and clearly communicate why and what people need to do.  The PR right now is too lenient as looks for any folder with a `nodes` sub folder. 

@s1monw I think that side steps the concern about multiple nodes/cluster  sharing the same data folder?

PS We can potentially allow hidden folders like .DS_Store. 
</comment><comment author="dakrone" created="2016-06-01T15:31:35Z" id="223030897">&gt;  IMO we should only auto upgrade in the most straightforward case - there is 1 folder under the data path, it's named exactly like the cluster name and it has a nodes folder underneath it. All the rest should fail and clearly communicate why and what people need to do. The PR right now is too lenient as looks for any folder with a nodes sub folder.

The PR right now bails if there is any other directory when upgrading (with the exception that I pasted previously "`path.data [/tmp/data] contains unknown directory [/tmp/data/foo]`"), is that not what you are talking about?
</comment><comment author="bleskes" created="2016-06-01T21:45:32Z" id="223134595">@dakrone here is a test simulating what I meant:

```
    public void testSingleWrongClusterFolder() throws IOException {
        String clusterName = randomAsciiOfLengthBetween(10, 15);
        String clusterName2 = randomAsciiOfLengthBetween(10, 15);
        Path tempPath = createTempDir().toAbsolutePath();
        Files.createDirectories(tempPath.resolve("data").resolve(clusterName).resolve("nodes"));
        Settings settings = Settings.builder()
            .put("cluster.name", clusterName2)
            .put(Environment.PATH_HOME_SETTING.getKey(), tempPath).build();
        expectThrows(Exception.class, () -&gt;
            new NodeEnvironment(settings, new Environment(settings))
        );
    }
```

It simulates what happens when someone starts one node with one cluster name, then starts another node with another name (I think this can be done without shutting down the first node, but the point stays). The second will happily conclude it has nothing to do and start leaving us in the following situation:

```
lanhost446:elasticsearch boaz$ ls -l tempDir-001/data/
total 0
drwxr-xr-x  3 boaz  staff  102 Jun  1 23:35 fHcjgcryKGxM &lt;-- first clusterName
drwxr-xr-x  3 boaz  staff  102 Jun  1 23:37 nodes &lt;-- created when started with clusterName2
-rw-r--r--  1 boaz  staff    0 Jun  1 23:36 upgrade.lock &lt;-- should we clean up and remove this?
```

I hope this clarifies things
</comment><comment author="dakrone" created="2016-06-01T22:36:42Z" id="223145771">@bleskes that's because there's no data to upgrade!

If I change the test to:

``` java
    public void testSingleWrongClusterFolder() throws IOException {
        String clusterName = randomAsciiOfLengthBetween(10, 15);
        String clusterName2 = randomAsciiOfLengthBetween(10, 15);
        Path tempPath = createTempDir().toAbsolutePath();
        Files.createDirectories(tempPath.resolve("data").resolve(clusterName).resolve("nodes"));
        Files.createDirectories(tempPath.resolve("data").resolve(clusterName2).resolve("nodes"));
        Settings settings = Settings.builder()
            .put("cluster.name", clusterName2)
            .put(Environment.PATH_HOME_SETTING.getKey(), tempPath).build();
        expectThrows(Exception.class, () -&gt;
            new NodeEnvironment(settings, new Environment(settings))
        );
    }
```

Then it passes. The line I added was:

```
        Files.createDirectories(tempPath.resolve("data").resolve(clusterName2).resolve("nodes"));
```

It then fails as expected.

It behaves like this because if there is no data, it doesn't matter for the upgrade!

&gt; -rw-r--r--  1 boaz  staff    0 Jun  1 23:36 upgrade.lock &lt;-- should we clean up and remove this?

We don't clean up `node.lock` files, and this uses the same Lucene behavior for locking, so I'm not sure if we want to add cleanup things?
</comment><comment author="bleskes" created="2016-06-02T09:34:45Z" id="223242694">&gt; The line I added was

Sure , but I was talking about the situation where there is one folder belonging to a different cluster name. IMO we should not ignore it. Just adding our own top level nodes folder creates a mess.

&gt; We don't clean up node.lock files, and this uses the same Lucene behavior for locking, so I'm not sure if we want to add cleanup things?

I remember now that there was issues with locking semantics when deleting the file.  Thinking about this more I think we should protect against trying to rename a folder which is currently being used by a running node - i.e., lock the node folder lock.  I'm not sure how helpful the upgrade lock is - if two concurrent upgrades are running, one of them will succeed and the other will potentially fail, which is fine, right?
</comment><comment author="s1monw" created="2016-06-02T15:05:10Z" id="223320398">I thought about this and I think we shouldn't do this upgrade at all. I think we should support both and for new installs we should not write the clustername but for old we should read from it. Deprecate that an fail hard in 6 or whatever. it seems too risky to me.
</comment><comment author="dakrone" created="2016-06-02T15:11:18Z" id="223322304">&gt; I thought about this and I think we shouldn't do this upgrade at all. I think we should support both and for new installs we should not write the clustername but for old we should read from it. Deprecate that an fail hard in 6 or whatever. it seems too risky to me.

Okay, I will change this to read from the old path if it exists and has data (ie, a "nodes" folder) and if not, do the new one. I'll also add deprecation logging about it going away in 6.0
</comment><comment author="dakrone" created="2016-06-02T16:00:53Z" id="223337995">@s1monw okay, I changed this to simply check and use the old behavior if data exists, otherwise it will use the new behavior
</comment><comment author="s1monw" created="2016-06-07T09:37:30Z" id="224229857">left one comment - LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Relocation and indexing concurrently can lead to a deadlock</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18553</link><project id="" key="" /><description>**Elasticsearch version**: master (f210605af82ab0642b6294c08163c81b10e84969)

Edit: pushed a test to master with an AwaitsFix, see below. Reproduces for me on spinning disk.

This is what happens:

four nodes: node_t0, node_t1, node_t2, node_t3

node_t1 has P0 and R1 and node_t0 has P1 and R0

Index request for P0 comes in, executes on node_t1 and sends replication request to node_t0.
Index request for P1 comes in, executes on node_t0 and sends replication request to node_t1.
Both will hold one shard reference (by which I mean a permit in SuspendableRefContainer of the shard).

Before replication requests arrives at target (trapped in the network, queued in thread pool, ...):

P0 starts relocating from node_t1 to node_t2
P1 starts relocating from node_t0 to node_t3

Before relocation finishes we try to set the state of IndexShard to relocated and block all further incoming requests by trying to acquire acquiring all shard references.
We do this on both nodes. Both do not succeed in acquiring all counters because the two primary requests are still waiting for the replica to succeed. 
In the meanwhile, new indexing requests come in and try to acquire shard references but they have to wait too because the relocation is queued before them.
Hence, they block the indexing threadpool.

Now, the replication requests arrive at their respective targets. But because the indexing thread pool is full on both nodes they too will have to wait.

Therefore the primary requests on node_t0 and node_t1 will never release their shard reference, the relocation can never be finished and the new indexing requests never finish too.
</description><key id="156570007">18553</key><summary>Relocation and indexing concurrently can lead to a deadlock</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Recovery</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-05-24T17:54:44Z</created><updated>2016-09-14T14:44:25Z</updated><resolved>2016-07-02T07:36:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-05-24T18:09:03Z" id="221355925">Pushed the test to master with an AwaitsFix : https://github.com/elastic/elasticsearch/commit/b3a8c54928f85fd5511efb0bb8bef085e0e09dad
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix a leftover tests.logger.level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18552</link><project id="" key="" /><description>This is a leftover spot that wasn't changed. It was breaking
ClusterSettingsIT#ClusterSettingsIT because that test expected
the test's log level to default to the default logger level for
the nodes.
</description><key id="156564131">18552</key><summary>Fix a leftover tests.logger.level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-24T17:27:03Z</created><updated>2016-05-24T17:29:03Z</updated><resolved>2016-05-24T17:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-24T17:28:15Z" id="221344686">LGTM.
</comment><comment author="nik9000" created="2016-05-24T17:28:59Z" id="221344877">Thanks for looking!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Potential race condition with snapshot naming</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18551</link><project id="" key="" /><description>In very rare cases, it is possible to have two snapshots of the same name exist in a repository, one found in the cluster state as a snapshot in progress, and the other found in the repository.  This can happen when a snapshot (lets call it `snap`) exists in the repository, and we try to create another snapshot with the same name `snap`.  When trying to create the second `snap`, `SnapshotsService#createSnapshot` first updates the cluster state to include the second 'snap' snapshot as a snapshot in progress, and then calls `SnapshotsService#beginSnapshot`, which checks if a snapshot by the same name already exists.  If a snapshot by the same name does exist, an exception is thrown, which results in calling `SnapshotsService#removeSnapshotFromClusterState`.  However, this call to remove the snapshot from the cluster state simply submits a cluster state update task for the cluster service to execute in its own update thread, and so there is a tiny window when the cluster state could have a snapshot in progress with the name `snap` and the repository could have the original snapshot named 'snap' in its storage.

This affects the `SnapshotsService#deleteSnapshot` call, which looks through both the current snapshots and the snapshots in the repository storage, and it also affects `TransportGetSnapshotsAction` and `TransportSnapshotsStatusAction`, which have a very tiny window of time in which its possible to return two snapshots of the same name, or to return the current (soon-to-be-removed from the cluster state) snapshot when the snapshot in the repository storage is what was meant.

For those wondering why we don't check if the snapshot name already exists before adding the new snapshot to the cluster state's snapshots in progress, the reason is because it can present race conditions of its own.  If two requests arrive to create a snapshot of the same name at the same time, because neither has yet to be added to the cluster state, it is possible that both pass the check and subsequently both eventually get created by the snapshots service.
</description><key id="156550008">18551</key><summary>Potential race condition with snapshot naming</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2016-05-24T16:18:36Z</created><updated>2017-07-20T18:37:39Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2017-01-12T10:42:59Z" id="272131297">@abeyad is this still valid?</comment><comment author="abeyad" created="2017-01-12T14:23:01Z" id="272175273">The `deleteSnapshot` call is now safe (https://github.com/elastic/elasticsearch/pull/22313), but we still have potential issues with getting snapshots and snapshot status as outlined above.  I believe we can solve the issue of create snapshots temporarily creating a duplicate entry in the cluster state by maintaining some internal data structures on the master node containing the current snapshots, and that list is updated in a synchronized manner.  During master failover, the new master will populate its own internal data structures with the snapshot info.  This will be tricky to get right with various edge cases, but its an option for solving this issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set version to 5.0.0-alpha3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18550</link><project id="" key="" /><description>Set version to 5.0.0-alpha3
</description><key id="156520576">18550</key><summary>Set version to 5.0.0-alpha3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Core</label><label>non-issue</label><label>review</label></labels><created>2016-05-24T14:20:51Z</created><updated>2016-05-24T14:46:05Z</updated><resolved>2016-05-24T14:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-24T14:22:27Z" id="221286855">LGTM.
</comment><comment author="spinscale" created="2016-05-24T14:32:58Z" id="221290287">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not automatically close XContent objects/arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18549</link><project id="" key="" /><description>Issue like #18433 appeared because [it's easy to forgot ](https://github.com/elastic/elasticsearch/commit/d7a31c8cf7f411aadc7202a4a2326679ed820e88) a `.endObject()` or `.endArray()` when building `XContent`objects using a `XContentBuilder`. 

And issues like this are even harder to detect because Jackson automatically closes unclosed objects and arrays in its default configuration, suggesting that all is well and the object has been correctly built while an object like 

``` json
{
  "a": {}, 
  "b": {}
}
```

should have been rendered like

``` json
{
  "a": {}, 
  "b": {
```

instead because of missing `.endObject()` calls.

This pull request disables the  `AUTO_CLOSE_JSON_CONTENT` feature so that json/cbor/smile objects are rendered _as is_ with missing `}` or `]` if necessary, and throws an exception if there are missing `endObject` or `endArray` calls when closing the `XContentBuilder`. It also fixes various tests that do not correctly close objects.

It is really a best effort to detect that `XContent`objects are not correctly built but it does not check every `ToXContent` object neither it replaces good REST tests.
</description><key id="156518168">18549</key><summary>Do not automatically close XContent objects/arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-24T14:11:09Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-25T14:47:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-24T14:18:38Z" id="221285685">@tlrx It looks great. Thank you so much for doing this! I left a few comments about using the `expectThrows` idiom on the new tests (no need to change the existing tests). 
</comment><comment author="tlrx" created="2016-05-25T08:11:47Z" id="221503309">Thanks @jasontedor. I just updated the code according to your comments.Would you please have another look?
</comment><comment author="jasontedor" created="2016-05-25T10:09:33Z" id="221530367">LGTM.
</comment><comment author="tlrx" created="2016-05-25T14:49:01Z" id="221600311">@jasontedor thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix misplaced cast when parsing seconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18548</link><project id="" key="" /><description>This commit fixes a misplaced cast when parsing seconds. Namely, the
input seconds are parsed as a double and then cast to a long before
multiplying by the scale. This can lead to truncation to zero before
multiplying by the scale thus leading to "0.1s" being parsed as zero
milliseconds. Instead, the cast should occur after multiplying by the
scale so as to not prematurely truncate.

Closes #18546
</description><key id="156515126">18548</key><summary>Fix misplaced cast when parsing seconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v2.3.4</label><label>v2.4.0</label></labels><created>2016-05-24T13:59:31Z</created><updated>2016-05-24T15:36:08Z</updated><resolved>2016-05-24T15:19:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-24T14:11:38Z" id="221283385">This bug is only present in 2.x but not master due to an earlier refactoring there. When I integrate this into to 2.x, I will forward port the test to master so that we have coverage there too.
</comment><comment author="nik9000" created="2016-05-24T14:23:23Z" id="221287175">LGTM
</comment><comment author="jasontedor" created="2016-05-24T15:36:08Z" id="221311007">Thanks @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not replay into translog on local recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18547</link><project id="" key="" /><description>When performing a local recovery, the engine replays operations
recovered from the translog into the translog again. These duplicated
operations are eventually cleared after successful recovery on flush,
but there is no need to play these operations into the translog at
all. This commit modifies the local recovery process so as to not replay
these operations into the translog.
</description><key id="156508941">18547</key><summary>Do not replay into translog on local recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Translog</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-24T13:35:39Z</created><updated>2016-05-27T10:04:11Z</updated><resolved>2016-05-27T10:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-24T14:30:00Z" id="221289330">left 2 comments LGTM otherwise - good change
</comment><comment author="bleskes" created="2016-05-25T09:06:20Z" id="221515823">LGTM2 - I'm going with how way the discussion around Translog.Position falls out. Thx @jasontedor 
</comment><comment author="jasontedor" created="2016-05-26T04:12:14Z" id="221773003">@s1monw @bleskes I pushed more commits.
</comment><comment author="s1monw" created="2016-05-26T07:36:20Z" id="221798978">@jasontedor This is not what I meant, I think IMC relies on a trappy implementation detail of InternalEngine to set the translog location. Yet if there isn't one like in your case we should not set it. Instead I think Engine#Operation should either have a `sizeInBytes` method or implement `Accountable` such that we always get this information and it's part of the operation contract. We either do that in another PR before this one goes in or here.
</comment><comment author="bleskes" created="2016-05-26T08:55:29Z" id="221814968">@s1monw trying to clarify your comment to understand better - I agree it will be good to have an explicit sizeInBytes on Engine#Operation, but are you OK with powering it using the current solution in the case we recover from translog? We need to store that size somewhere and it seems adding the location to the translog operation interface and then setting it on the engine op is the least amount of effort to do so. 
</comment><comment author="s1monw" created="2016-05-26T10:26:27Z" id="221834820">&gt; @s1monw trying to clarify your comment to understand better - I agree it will be good to have an explicit sizeInBytes on Engine#Operation, but are you OK with powering it using the current solution in the case we recover from translog? We need to store that size somewhere and it seems adding the location to the translog operation interface and then setting it on the engine op is the least amount of effort to do so.

there is no reason to pull a location from that snapshot iterator. I don't get why we can't put a simple estimation into the operation just as we have on Translog.Operation
</comment><comment author="jasontedor" created="2016-05-26T23:41:03Z" id="222025025">@simonw I pushed another commit. Please let me know what you think.
</comment><comment author="s1monw" created="2016-05-27T07:30:55Z" id="222079530">LGTM thanks @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parser raises an exception if interval is set to "0.1s" but works fine with "100ms"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18546</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: `2.3.2`

**JVM version**: 

```
openjdk version "1.8.0_45-internal"
OpenJDK Runtime Environment (build 1.8.0_45-internal-b14)
OpenJDK 64-Bit Server VM (build 25.45-b02, mixed mode)
```

**OS version**: `Ubuntu 12.04.5 LTS (GNU/Linux 3.2.0-80-virtual x86_64)`

**Description of the problem including expected versus actual behavior**: date_histogram aggregations fail if interval is set to `0.1s` but work fine if set to `100ms`. I would say the problem comes from the parser and it doesn't have anything to do with date_histograms.

**Steps to reproduce**:
1. Following query fails

```
curl -XGET 'http://localhost:9200/logstash-2016.05.23/_search?pretty' -d '
{
   "size" : 0,
   "aggs" : {
      "0" : {
         "aggs" : {
            "0" : {
               "date_histogram" : {
                  "field" : "@timestamp",
                  "interval" : "0.1s"
               }
            }
         },
         "filter" : {
            "query" : {
               "filtered" : {
                  "query" : {
                     "query_string" : {
                        "query" : "*"
                     }
                  },
                  "filter" : {
                     "bool" : {
                        "must" : [
                           {
                              "range" : {
                                 "@timestamp" : {
                                    "to" : 1464034121806,
                                    "from" : 1464034119663
                                 }
                              }
                           }
                        ]
                     }
                  }
               }
            }
         }
      }
   }
}
'
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_argument_exception",
      "reason" : "Zero or negative time interval not supported"
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "logstash-2016.05.23",
      "node" : "z3wh9XcOQjCIyVjT2H-8qg",
      "reason" : {
        "type" : "illegal_argument_exception",
        "reason" : "Zero or negative time interval not supported"
      }
    } ]
  },
  "status" : 400
}
```

but it works fine if I change the `interval` field to the equivalent value `100ms` 

```
curl -XGET 'http://eln02.useb.cartodb.net:9200/logstash-2016.05.23/_search?pretty' -d '
{
   "size" : 0,
   "aggs" : {
      "0" : {
         "aggs" : {
            "0" : {
               "date_histogram" : {
                  "field" : "@timestamp",
                  "interval" : "100ms"
               }
            }
         },
         "filter" : {
            "query" : {
               "filtered" : {
                  "query" : {
                     "query_string" : {
                        "query" : "*"
                     }
                  },
                  "filter" : {
                     "bool" : {
                        "must" : [
                           {
                              "range" : {
                                 "@timestamp" : {
                                    "to" : 1464034121806,
                                    "from" : 1464034119663
                                 }
                              }
                           }
                        ]
                     }
                  }
               }
            }
         }
      }
   }
}
'
{
  "took" : 304,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },

(...)
```

**Provide logs (if relevant)**:

```
2016-05-24 11:25:06,144][DEBUG][action.search            ] [eln02] All shards failed for phase: [query]
RemoteTransportException[[eln01][10.0.2.81:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [
{
   "size" : 0,
   "aggs" : {
      "0" : {
         "aggs" : {
            "0" : {
               "date_histogram" : {
                  "field" : "@timestamp",
                  "interval" : "0.1s"
               }
            }
         },
         "filter" : {
            "query" : {
               "filtered" : {
                  "query" : {
                     "query_string" : {
                        "query" : "*"
                     }
                  },
                  "filter" : {
                     "bool" : {
                        "must" : [
                           {
                              "range" : {
                                 "@timestamp" : {
                                    "to" : 1464034121806,
                                    "from" : 1464034119663
                                 }
                              }
                           }
                        ]
                     }
                  }
               }
            }
         }
      }
   }
}
]]; nested: IllegalArgumentException[Zero or negative time interval not supported];
Caused by: SearchParseException[failed to parse search source [
{
   "size" : 0,
   "aggs" : {
      "0" : {
         "aggs" : {
            "0" : {
               "date_histogram" : {
                  "field" : "@timestamp",
                  "interval" : "0.1s"
               }
            }
         },
         "filter" : {
            "query" : {
               "filtered" : {
                  "query" : {
                     "query_string" : {
                        "query" : "*"
                     }
                  },
                  "filter" : {
                     "bool" : {
                        "must" : [
                           {
                              "range" : {
                                 "@timestamp" : {
                                    "to" : 1464034121806,
                                    "from" : 1464034119663
                                 }
                              }
                           }
                        ]
                     }
                  }
               }
            }
         }
      }
   }
}
]]; nested: IllegalArgumentException[Zero or negative time interval not supported];
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:855)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:654)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:620)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:371)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Zero or negative time interval not supported
    at org.elasticsearch.common.rounding.TimeZoneRounding$Builder.&lt;init&gt;(TimeZoneRounding.java:62)
    at org.elasticsearch.common.rounding.TimeZoneRounding.builder(TimeZoneRounding.java:40)
    at org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramParser.parse(DateHistogramParser.java:184)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:198)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:176)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:103)
    at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:838)
```
</description><key id="156484117">18546</key><summary>Parser raises an exception if interval is set to "0.1s" but works fine with "100ms"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rporres</reporter><labels><label>:Core</label><label>bug</label></labels><created>2016-05-24T11:30:19Z</created><updated>2016-05-24T15:21:38Z</updated><resolved>2016-05-24T15:21:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rporres" created="2016-05-24T11:38:20Z" id="221242955">As an aside fact, I've tested it against Elasticsearch 1.5.2 and it works fine
</comment><comment author="jasontedor" created="2016-05-24T13:52:46Z" id="221276600">It's a bug in the time value parsing that only impacts seconds parsing due to a misplaced parenthesis and thus a premature cast. I'll open a pull request. This bug is present in 2.x but not master.
</comment><comment author="jasontedor" created="2016-05-24T13:59:39Z" id="221279147">I opened #18548.
</comment><comment author="rporres" created="2016-05-24T14:02:52Z" id="221280363">Wow, that was fast! Thanks for taking a look, @jasontedor 
</comment><comment author="jasontedor" created="2016-05-24T15:21:35Z" id="221306115">Thanks for reporting @rporres!
</comment><comment author="jasontedor" created="2016-05-24T15:21:37Z" id="221306126">Closed by #18548
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expand wildcards to closed indices in /_cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18545</link><project id="" key="" /><description>Closed indices are already displayed when no indices are explicitly selected. This commit ensures that closed indices are also shown when wildcard filtering is used (fixes #16419). It also addresses another issue that is caused by the fact that the cat action is based internally on 3 different cluster states (one when we query the cluster state to get all indices, one when we query cluster health, and one when we query indices stats). We currently fail the cat request when the user specifies a concrete index as parameter that does not exist. The implementation works as intended in that regard. It checks this not only for the first cluster state request, but also the subsequent indices stats one. This means that if the index is deleted before the cat action has queried the indices stats, it rightfully fails. In case the user provides wildcards (or no parameter at all), however, we fail the indices stats as we pass the resolved concrete indices to the indices stats request and fail to distinguish whether these indices have been resolved by wildcards or explicitly requested by the user. This means that if an index has been deleted before the indices stats request gets to execute, we fail the overall cat request (see #17395). The fix is to let the indices stats request do the resolving again and not pass the concrete indices.

Closes #16419
Closes #17395
</description><key id="156479444">18545</key><summary>Expand wildcards to closed indices in /_cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:CAT API</label><label>bug</label><label>review</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-24T11:03:25Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-25T08:02:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-05-24T11:54:55Z" id="221246200">left a couple of comments, thanks @ywelsch 
</comment><comment author="javanna" created="2016-05-24T12:27:00Z" id="221252942">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back doc execution to query dsl.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18544</link><project id="" key="" /><description>Relates to #18211

This fixes what was reverted in 20aafb1196192d4f9f7faea8ce9a36b278e501a1. 
</description><key id="156476627">18544</key><summary>Add back doc execution to query dsl.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-24T10:47:10Z</created><updated>2016-05-24T10:47:21Z</updated><resolved>2016-05-24T10:47:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Include size of snapshot in snapshot metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18543</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

Feature: add snapshot size to snapshot metadata.

While the snapshot size (as a snapshot) on disk may not have much relevance because of the additive nature of snapshots (segments may already exist in the repository, so data transferred may be less than the total, and deleting a snapshot may not release all of the disk space because some segments may be retained for other snapshots...), it should be easy to capture the total size of the snapshot as it will require disk space to be restored, and this is an information which can be very useful when trying to determine before executing a restore, whether there is enough disk space on the target cluster.
</description><key id="156460928">18543</key><summary>Include size of snapshot in snapshot metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>feature</label></labels><created>2016-05-24T09:29:40Z</created><updated>2016-05-24T09:56:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Missing path.repo config in default elasticsearch.yml config file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18542</link><project id="" key="" /><description>Hi,

reporting live from lab of Operation course :)

The lab instructs me to setup path.repo, so I would expect that the [Path section of elasticsearch.yml](https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/config/elasticsearch.yml#L29-L39)  already contains it with short explanation  and commented out as it is for all others main configuration.
What do you think ?
</description><key id="156458803">18542</key><summary>Missing path.repo config in default elasticsearch.yml config file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wiibaa</reporter><labels /><created>2016-05-24T09:20:01Z</created><updated>2016-05-24T09:42:43Z</updated><resolved>2016-05-24T09:42:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-24T09:42:43Z" id="221218456">We intentionally keep the config file limited to the most important options only.  The instructions for setting `path.repo` are in the docs which explain how to setup a file system repo.

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upsert with automatic id generation failed with exception : Validation Failed: 1: id is missing;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18541</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.8

**OS version**:windows

**Description of the problem including expected versus actual behavior**:
When I try an upsert with java client without id in the insert the exception occured, but i want to create an automatic id generation.

**Steps to reproduce**:

I want to not place id in the prepareIndex method

`prepareIndex(indexName, indexType);`

The full method is :

```
public UpdateRequest createUpsertRequest(byte[] content, Offer offer){
        IndexRequestBuilder indexRequestBuilder = getElasticsearchClient().prepareIndex(indexName, indexType);
        indexRequestBuilder.setSource(content);
        IndexRequest indexRequest = indexRequestBuilder.request();

        UpdateRequestBuilder updateRequestBuilder = getElasticsearchClient().prepareUpdate(indexName, indexType, offer.getGuid());
        updateRequestBuilder.setDoc(content);
        UpdateRequest updateRequest = updateRequestBuilder.request();
        updateRequest.upsert(indexRequest);

        return updateRequest;
    }
```
</description><key id="156457713">18541</key><summary>Upsert with automatic id generation failed with exception : Validation Failed: 1: id is missing;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bhernout</reporter><labels /><created>2016-05-24T09:14:38Z</created><updated>2016-05-24T10:05:35Z</updated><resolved>2016-05-24T09:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-24T09:41:18Z" id="221218123">You can't run an update with an automatically generated ID.  That's just a create request.
</comment><comment author="bhernout" created="2016-05-24T09:51:44Z" id="221220760">Thank you for your response, but for me the problem is on the insert.

If I understand the process, the Upsert request will try to do the insert request first. And after if the insert does not work the update request will be launched.

I don't understand why I could do the insert with automatic id generation, and it does not work with inside the upsert ?
</comment><comment author="clintongormley" created="2016-05-24T09:55:34Z" id="221221665">It tries to retrieve the document first, in order to update it.  If it doesn't find a document with that ID, then it inserts it (with the same ID). Using autogenerated IDs make no sense here.
</comment><comment author="bhernout" created="2016-05-24T10:05:35Z" id="221224030">Thank you for you responses
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Classloader Isolation for Integration Tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18540</link><project id="" key="" /><description>Non-constant, static fields are the bane of testing.

With that out of the way, it seems like it might be worthwhile to introduce classloader isolation for our integration test nodes, so that they cannot share _any_ state, whether it's by accident or not. It's a simple statement with _a lot_ of potential ramifications.

/cc @jaymode
</description><key id="156423945">18540</key><summary>Use Classloader Isolation for Integration Tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>enhancement</label><label>high hanging fruit</label><label>test</label></labels><created>2016-05-24T05:39:52Z</created><updated>2016-05-24T05:39:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Jackson databind Exception when creating repository/sending snapshot to s3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18539</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 25.91-b14

**OS version**: 

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:

``` sh
curl -XPUT 'http://localhost:9200/_snapshot/test-snapshot' -d '{
    "type": "s3",
    "settings": {
        "bucket": "test-snapshot",
         "access_key": "XXXXX", 
         "secret_key": "XXXX"    }
}'
```

**Provide logs (if relevant)**:

```
[2016-05-24 00:12:38,425][WARN ][repositories             ] [54.210.62.226 (m3.xlarge) - i-4e4e7ac9] [awsmt-deviceprofile-es-snapshot] failed to verify repository
BlobStoreException[failed to check if blob exists]; nested: AmazonClientException[Error while loading partitions file from com/amazonaws/partitions/endpoints.json]; nested: JsonMappingException[Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")]; nested: IllegalArgumentException[Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")];
        at org.elasticsearch.cloud.aws.blobstore.S3BlobContainer.blobExists(S3BlobContainer.java:65)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.verify(BlobStoreIndexShardRepository.java:240)
        at org.elasticsearch.repositories.VerifyNodeRepositoryAction.doVerify(VerifyNodeRepositoryAction.java:121)
        at org.elasticsearch.repositories.VerifyNodeRepositoryAction.verify(VerifyNodeRepositoryAction.java:86)
        at org.elasticsearch.repositories.RepositoriesService.verifyRepository(RepositoriesService.java:214)
        at org.elasticsearch.repositories.RepositoriesService$VerifyingRegisterRepositoryListener.onResponse(RepositoriesService.java:436)
        at org.elasticsearch.repositories.RepositoriesService$VerifyingRegisterRepositoryListener.onResponse(RepositoriesService.java:421)
        at org.elasticsearch.cluster.AckedClusterStateUpdateTask.onAllNodesAcked(AckedClusterStateUpdateTask.java:63)
        at org.elasticsearch.cluster.service.InternalClusterService$SafeAckedClusterStateTaskListener.onAllNodesAcked(InternalClusterService.java:733)
        at org.elasticsearch.cluster.service.InternalClusterService$AckCountDownListener.onNodeAck(InternalClusterService.java:1013)
        at org.elasticsearch.cluster.service.InternalClusterService$DelegetingAckListener.onNodeAck(InternalClusterService.java:952)
        at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:637)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: com.amazonaws.AmazonClientException: Error while loading partitions file from com/amazonaws/partitions/endpoints.json
        at com.amazonaws.partitions.PartitionsLoader.loadPartitionFromStream(PartitionsLoader.java:99)
        at com.amazonaws.partitions.PartitionsLoader.build(PartitionsLoader.java:88)
        at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)
        at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:66)
        at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:54)
        at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:104)
        at com.amazonaws.services.s3.AmazonS3Client.resolveServiceEndpoint(AmazonS3Client.java:4195)
        at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1006)
        at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:991)
        at org.elasticsearch.cloud.aws.blobstore.S3BlobContainer.blobExists(S3BlobContainer.java:60)
        ... 17 more
Caused by: com.fasterxml.jackson.databind.JsonMappingException: Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:269)
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)
        at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)
        at com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:461)
        at com.fasterxml.jackson.databind.ObjectMapper._findRootDeserializer(ObjectMapper.java:3833)
        at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3727)
        at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2794)
        at com.amazonaws.partitions.PartitionsLoader.loadPartitionFromStream(PartitionsLoader.java:96)
        ... 26 more
Caused by: java.lang.IllegalArgumentException: Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
        at com.fasterxml.jackson.databind.util.ClassUtil.checkAndFixAccess(ClassUtil.java:513)
        at com.fasterxml.jackson.databind.deser.impl.CreatorCollector._fixAccess(CreatorCollector.java:280)
        at com.fasterxml.jackson.databind.deser.impl.CreatorCollector.verifyNonDup(CreatorCollector.java:327)
        at com.fasterxml.jackson.databind.deser.impl.CreatorCollector.addPropertyCreator(CreatorCollector.java:184)
        at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._addDeserializerConstructors(BasicDeserializerFactory.java:493)
        at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._constructDefaultValueInstantiator(BasicDeserializerFactory.java:324)
        at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory.findValueInstantiator(BasicDeserializerFactory.java:254)
        at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.buildBeanDeserializer(BeanDeserializerFactory.java:222)
        at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.createBeanDeserializer(BeanDeserializerFactory.java:142)
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2(DeserializerCache.java:403)
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer(DeserializerCache.java:352)
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:264)
        ... 33 more
```
</description><key id="156417953">18539</key><summary>Jackson databind Exception when creating repository/sending snapshot to s3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">yamap77</reporter><labels><label>:Plugin Repository S3</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-05-24T04:32:55Z</created><updated>2017-04-28T11:19:41Z</updated><resolved>2016-07-01T08:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-24T09:36:50Z" id="221217073">@dadoonet could you take a look please?
</comment><comment author="jsnod" created="2016-06-22T17:36:38Z" id="227819661">Is this blocking the release of v2.3.4?
</comment><comment author="dadoonet" created="2016-06-23T20:45:36Z" id="228178210">@yamap77 Did you set any specific metadata for the S3 bucket: `test-snapshot`?
Could you also try to explicitly set a region like `"region": "us-west"`?
</comment><comment author="yamap77" created="2016-06-23T21:03:52Z" id="228183191">Hi,

I had fixed this issue. It turned out that I have to disable the security manager. After I disable it, I can sent data to s3 now.
</comment><comment author="dadoonet" created="2016-06-23T21:24:50Z" id="228188613">I'm reopening it. You should never disable the security manager. We need to understand what is happening.

I'll try to reproduce but if you have any details which would help to understand I'd appreciate a lot!
</comment><comment author="mahesh-maney" created="2016-11-03T07:16:22Z" id="258076615">Hi,
Curious to know if this issue is resolved in 2.3.5? 
</comment><comment author="dadoonet" created="2016-11-03T07:31:24Z" id="258078234">@mahesh-maney I don't think so. It's marked as 2.4.0 and 5.0.0 and according to the commits it has not been back ported in 2.3 branch.
</comment><comment author="smarks" created="2017-04-28T11:19:41Z" id="297973303">FWIW, I am hacking the 2.4 branch with the ultimate goal of creating a S3 plugin that uses KMS encryption. 

I can add a single line to` org.elasticsearch.cloud.aws.InternalAwsS3Service`

`        Region region1 = Region.getRegion(Regions.US_EAST_1);
`
and reproduce what I think is this issue. 

```
Caused by: java.lang.IllegalArgumentException: Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
	at com.fasterxml.jackson.databind.util.ClassUtil.checkAndFixAccess(ClassUtil.java:505)
	at com.fasterxml.jackson.databind.deser.impl.CreatorCollector._fixAccess(CreatorCollector.java:271)

```

I did verify that the patch mentioned here is in code I am working with.  

If anyone has some ideas for a work around I'd appreciate it. (Also, apologies if this is wrong place for this kind of info but I thought it was relevant.) 

</comment></comments><attachments /><subtasks /><customfields /></item><item><title>painless: add trap tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18538</link><project id="" key="" /><description>This part of the grammar currently doesn't have tests. We should add some basic ones.
</description><key id="156413481">18538</key><summary>painless: add trap tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>test</label></labels><created>2016-05-24T03:37:52Z</created><updated>2016-05-24T04:11:31Z</updated><resolved>2016-05-24T04:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-24T03:44:53Z" id="221159007">LGTM!  Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score query functions needs offset param</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18537</link><project id="" key="" /><description>The current implementation of [function_value_factor](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-field-value-factor) accepts `factor` and `modifier` to shape and scale the resulting function, but missing from this is the ability to offset the value of the function.

Consider the following scenario: Documents represent events that one might attend. For a given query the total document score should be a function of the base query score, the distance, and the popularity. The query score is based solely upon the text match; distance uses a geo-based decay function; and popularity is based upon a function_value_factor function with `modifier: "sqrt"`. The function score query allows us to create a boost by either adding or multiplying the distance and popularity values. For our purposes it doesn't make sense to sum popularity and distance values -- you can have a distant event that no one can attend, but that will nonetheless rank highly based solely upon its high popularity. So instead we will multiply the popularity and distance boosts.

But there's a problem - popularity is based upon number of tickets already sold and if 0 tickets are sold, then the popularity will be 0. Since we are multiplying the boosts together and since the query score is multiplied by the boosts, this means that the total score for those events will also be 0. Thus new events with no tickets sold are all but eliminated from the search results.

We are prepared to resolve this issue with a script_score function, but this is not ideal. I propose introducing an `offset` parameter to be included in the function_value_factor so that the value of the function would be `factor*modifier(field) + offset`. This would ensure that popularity could never be zero.

The problem here also exists with decay functions. When using functions multiplicatively there are time when it would be beneficial to have a non-zero minimum value.
</description><key id="156409663">18537</key><summary>Function score query functions needs offset param</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JnBrymn-EB</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2016-05-24T02:55:28Z</created><updated>2016-05-25T22:26:25Z</updated><resolved>2016-05-25T14:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-24T09:34:10Z" id="221216389">@brwe what do you think about this?  I'm loathe to add more parameters unless they genuinely generically useful, given that this can all be achieved with a script  (and will be improved with https://github.com/elastic/elasticsearch/issues/17116)
</comment><comment author="JnBrymn-EB" created="2016-05-24T14:56:15Z" id="221297922">@clintongormley your prior issue #6955 is relevant here. There you needed a weight for all of the functions - but when using the functions multiplicatively, just having weights does not make sense. Consider that in the multiplicative use case the total score of a document is `query_score*(weight1*field_value1)*(weight2*field_value2)` - you can see that final score with weighting is equal to the original non-weighted score times `weight1*weight2`.

Because weight doesn't have any effect when the the function values are multiplied together I think there is a need for an offset parameter.

Consider also that the most basic definition for a linear function is: `y = m*x + b`. I think we're missing the `b`.
</comment><comment author="rjernst" created="2016-05-24T19:23:27Z" id="221375945">I don't see the point, given, as Clint said, that this can be done with a script, and even with an `expression` script, which will be very fast (I benchmarked this when adding expressions, specifically comparing to `function_value_factor` and the perf was identical).
</comment><comment author="brwe" created="2016-05-25T12:17:04Z" id="221556194">I also do not think that we can or should cover too many score combinations with the functions we provide so far. The idea of function score originally was to allow basic functionality out of the box and leave more sophisticated stuff to script_score. I somewhat agree that we are missing the b and that the weights do not make sense when the functions are multiplied  in the end. But on the other hand the offset does not make sense either when the functions are summed up...not sure. I am more inclined to work on #17116 and leave the factor function as is. 
</comment><comment author="JnBrymn-EB" created="2016-05-25T14:21:28Z" id="221591508">Ok - presuming I'm on Elasticsearch 2.3.x which scripting approach should I use? Groovy? Painless? Lucene Expression Script? My impression is that Groovy is insecure (which actually probably doesn't matter for my case); Painless is not available yet; Lucene Expression Script is marked as undergoing development.
</comment><comment author="clintongormley" created="2016-05-25T14:27:35Z" id="221593442">Expressions are very fast and stable, I'll remove that warning from the docs.  I'd definitely use expressions if it does what you need (which it sounds like it will).  The only downside is it may not support all the syntax you need, in which case your only option for the moment is Groovy.  Painless will be the lang to move to once 5.0 is out.
</comment><comment author="clintongormley" created="2016-05-25T14:28:56Z" id="221593864">Removed in https://github.com/elastic/elasticsearch/commit/cf7b13d11217ca11e765bf924f6c36418e94fee8
</comment><comment author="JnBrymn-EB" created="2016-05-25T22:26:25Z" id="221726496">thanks, All

-John

On Wed, May 25, 2016 at 9:31 AM, Clinton Gormley notifications@github.com
wrote:

&gt; Removed in cf7b13d
&gt; https://github.com/elastic/elasticsearch/commit/cf7b13d11217ca11e765bf924f6c36418e94fee8
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18537#issuecomment-221593864
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to insert document using alias in Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18536</link><project id="" key="" /><description>Inserting document using alias works in CLI but not from Java API

Below is the example.

From CLI:
**Create index:**
curl -XPUT 'http://localhost:9200/test1'

**Add alias:**
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "add" : { "index" : "test1", "alias" : "alias1" } }
    ]
}'

curl -XPOST 'http://localhost:9200/_cat/indices'
yellow open test1                                                     5 1      1  0   4.1kb   4.1kb  

curl -XGET 'http://localhost:9200/test1?pretty'

{
  "test1" : {
    "aliases" : {
      "alias1" : { }
    },
    "mappings" : {
      "indexType" : {
        "properties" : {
          "message" : {
            "type" : "string"
          },
          "post_date" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "user" : {
            "type" : "string"
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1464054657698",
        "number_of_shards" : "5",
        "number_of_replicas" : "1",
        "uuid" : "ZcUjvMkGSUS9FkaYjvlnxw",
        "version" : {
          "created" : "2020099"
        }
      }
    },
    "warmers" : { }
  }
}

**Add document using alias**

curl -XPUT 'http://localhost:9200/alias1/indexType/1' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}'

This will create a document inside the index.

But if I repeat the same steps from Java API it will create its own index with alias name rather than inserting document into the corresponding index.

**1. Create index**

CreateIndexRequest indexRequest = new CreateIndexRequest("test1");
        CreateIndexResponse response = client.admin().indices().create(indexRequest).actionGet();
        if (response.isAcknowledged()) {
            System.out.println("Index Created: {" + indexName + "}");
        } else {
            System.out.println("Failed to create Index: {" + indexName + "}");
        }

**Add alias**
IndicesAliasesResponse indAliasesResponse = getClient().admin().indices().prepareAliases().addAlias("test1" "alias1").execute().actionGet();

**Add document** 
JsonObject object = {}

 IndexResponse response = client
                .prepareIndex("alias1", + "testtype", 1)
                .setSource(object.toString()).execute().actionGet();

But this creates separate index with name alias1.

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="156404113">18536</key><summary>Unable to insert document using alias in Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pradeepg-telsiz</reporter><labels /><created>2016-05-24T01:58:39Z</created><updated>2016-05-25T12:28:58Z</updated><resolved>2016-05-25T12:28:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-05-24T09:02:20Z" id="221208744">Hi @pradeepg-telsiz this sounds weird given that the REST layer uses the java api internally. That means that if the java api cannot add a document to an alias, it is very likely that the REST layer doesn't work either. Are you working against a single cluster? Is it possible that those java api commands get sent to different clusters, one has the alias but the cluster where you index the document doesn't have it?

Your code snippets have some problems (e.g. a missing comma when you add the alias). Could you please post a complete recreation with exactly the code you are using?
</comment><comment author="s1monw" created="2016-05-24T09:35:30Z" id="221216728">we have a test that verifies this works from the java API https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java#L86

I can see you use `getClient()` and `client` in your code maybe you talk to different clusters?
</comment><comment author="pradeepg-telsiz" created="2016-05-24T13:56:19Z" id="221277899">I am able to figure out the problem. Below is the program to reproduce.

```
public class AliasTest {

    private TransportClient client;
    String indexName = "testindex";
    String aliasName = "testIndex_alias"; // Create different index
    //String aliasName = "testindex_write_alias"; // Create record in same

    public void createTransportClient() throws UnknownHostException {

        Settings.Builder settings = Settings.settingsBuilder();
        settings.put("cluster.name", "elastic-cluster").put("client.transport.nodes_sampler_interval", "30s")
                .put("client.transport.ping_timeout", "30s").put("client.transport.ignore_cluster_name", false)
                .put("client.transport.sniff", true).put("client.transport", "TRACE");
        TransportClient client = TransportClient.builder().settings(settings).build();
        client = client.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));
        this.client = client;

    }

    public static void main(String[] args) throws UnknownHostException {
        AliasTest aliasTest = new AliasTest();
        aliasTest.createTransportClient();
        aliasTest.buildIndex();
        aliasTest.addAlias();
        aliasTest.addDocumnetUsingAlias();
    }

    private String source(String id, String nameValue) {
        return "{ \"id\" : \"" + id + "\", \"name\" : \"" + nameValue + "\" }";
    }

    private void addDocumnetUsingAlias() {

        IndexResponse response = client.prepareIndex(aliasName.toLowerCase(), "sometype", "1")
                .setSource(source("1", "test")).execute().actionGet();
        if (!response.isCreated())
            client.admin().indices().prepareRefresh().execute().actionGet();
        if (response.isCreated()) {
            System.out.println("Document Created");
        } else {
            System.out.println("Failed to create document");
        }
    }

    private void buildIndex() {
        CreateIndexRequest indexRequest = new CreateIndexRequest(indexName);
        CreateIndexResponse response = client.admin().indices().create(indexRequest).actionGet();
        if (response.isAcknowledged()) {
            System.out.println("Index Created: {" + indexName + "}");
        } else {
            System.out.println("Failed to create Index: {" + indexName + "}");
        }
    }

    private void addAlias() {
        IndicesAliasesResponse indAliasesResponse = client.admin().indices().prepareAliases()
                .addAlias(indexName, aliasName).execute().actionGet();
        if (indAliasesResponse.isAcknowledged()) {
            System.out.println("Alias Created: {" + aliasName + "}");
        } else {
            System.out.println("Failed to create alias: {" + aliasName + "}");
        }
    }

}
```

So index name is not case sensitive. I added the alias with camel case but while inserting I was using lower case. This was causing new index creation. Is this the expected behavior? 
</comment><comment author="s1monw" created="2016-05-25T09:37:00Z" id="221522877">index names are case-sensitive!
</comment><comment author="javanna" created="2016-05-25T12:28:52Z" id="221559421">seems like you have found the problem @pradeepg-telsiz , this is the expected behaviour, index names are case sensitive.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support custom value coercion for boolean field mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18535</link><project id="" key="" /><description>Similar to how you are able to specify the [`format`](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html) for parsing date types, I would like to see support for custom true/false values for the boolean field mapping.

Currently `false`, `"false"`, `"off"`, `"no"`, `"0"`, `""`, `0`, and `0.0` are all treated as falsey out of the box, but things like `"No"` and `"Off"` are not. The ability to customize this behavior would be really handy.

The actual mapping might look something like this:

``` json
{
  "type": "boolean",
  "false_values": [ false, "No" ]
}
```
</description><key id="156392518">18535</key><summary>Support custom value coercion for boolean field mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marshall007</reporter><labels><label>discuss</label></labels><created>2016-05-24T00:01:41Z</created><updated>2016-05-25T22:56:18Z</updated><resolved>2016-05-25T22:56:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-24T09:24:20Z" id="221214002">this is a parsing feature and nothing we do per field or per index. I am also on the fence here since I think we should rather go into the other direction and only accept `true` or `false` and  nothing else going forward. I am still putting a discuss lable on it and we will discuss how we move forward
</comment><comment author="rjernst" created="2016-05-24T19:25:45Z" id="221376545">I don't think this should be customizable, in the same way we should not have custom formatting for numbers (for example, trying to parse numbers from text like "ten"). The fact that we currently allow many different values for false, and "not false" is true is something we have been slowly working towards removing. We should allow `true`, or `false`, and fail otherwise.
</comment><comment author="marshall007" created="2016-05-24T20:47:46Z" id="221396485">@s1monw @rjernst thanks for the feedback and that totally makes sense. Just to provide a bit of background on this: in refactoring our logic for replicating into ES, I started to prefer doing as little transformation on the documents as possible prior to indexing. Since the `_source` then remains largely untouched, it's easier to reason about and compare documents coming from the search API with those fetched directly from the database.

The `transform` API, which was deprecated in 2.0.0, worked really well for this use-case despite being a bit clunky and hard to test. I totally get why that API was deprecated, but I was hoping we might see it eventually replaced with more discrete/structured transformations like what I was proposing here.
</comment><comment author="rjernst" created="2016-05-25T22:56:18Z" id="221732222">@marshall007 You can use the `ingest` node feature which is coming in 5.0. See https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove inaccurate Javadoc on Setting constructor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18534</link><project id="" key="" /><description>The `Setting` constructor has some outdated Javadoc that suggested that it would automatically apply `Property.NodeScope` if no scope is supplied, but no scope is added in that case.
- Also fixed minor grammar issue: can not -&gt; cannot
</description><key id="156391668">18534</key><summary>Remove inaccurate Javadoc on Setting constructor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>non-issue</label></labels><created>2016-05-23T23:54:33Z</created><updated>2016-05-25T14:30:00Z</updated><resolved>2016-05-25T14:30:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-24T09:20:02Z" id="221212968">however you decide for the grammen nitpick LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve painless whitelist coverage of java api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18533</link><project id="" key="" /><description>This gives us a better foundation. Still TODO is java.time (I need a break), but this is most of the basic stuff. 

I also made very minor changes to reduce the ram usage of Definition.INSTANCE: we don't truly need to hang on to stuff from the reflection api (just 'int modifiers').
</description><key id="156389914">18533</key><summary>improve painless whitelist coverage of java api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T23:39:15Z</created><updated>2016-05-24T09:31:03Z</updated><resolved>2016-05-24T00:40:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-23T23:52:43Z" id="221129286">LGTM!  @rmuir Thank you so much for doing this!  We'll have an actual API now.  I seriously owe you a case or two of top-shelf beer.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Remove unnecessary Callable variant of assertBusy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18532</link><project id="" key="" /><description>The assertBusy method currently has both a Runnable and Callable
version. This has caused confusion with type inference and lambdas
sometimes, in particular with java 9. This change removes the callable
version as nothing was actually using it.
</description><key id="156387701">18532</key><summary>Tests: Remove unnecessary Callable variant of assertBusy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T23:19:52Z</created><updated>2016-05-24T00:11:54Z</updated><resolved>2016-05-24T00:11:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-23T23:21:57Z" id="221124488">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Grammar Ambiguities</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18531</link><project id="" key="" /><description>Title plus a bit of clean up.
</description><key id="156380370">18531</key><summary>Remove Grammar Ambiguities</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T22:25:02Z</created><updated>2016-05-23T23:42:40Z</updated><resolved>2016-05-23T23:42:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-23T23:18:15Z" id="221123946">I love that this is fixed, thanks for doing all this work to do it the clean way, it will be worth it! I left minor comments but looks great. With antlr 4 this is the best we can do, fail on this stuff in tests, and test everything. See this explanation: https://groups.google.com/d/msg/antlr-discussion/z8otgVzqB7I/IAHEU9keZLgJ
</comment><comment author="jdconrad" created="2016-05-23T23:37:43Z" id="221126979">@rmuir Thanks for the review and all the help debugging!  Glad you caught the issue.
</comment><comment author="jdconrad" created="2016-05-23T23:42:40Z" id="221127791">This was merged, but squashed due to 30 unimportant commit messages.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BlobContainer's deleteBlob method should not be lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18530</link><project id="" key="" /><description>Currently, several implementations of `BlobContainer#deleteBlob(String blobName)` are lenient - if the blob does not exist, it silently does nothing.  Instead, we should throw an IOException if the blob does not exist.  This is in accordance with `BlobContainer`'s contract as stated in https://github.com/elastic/elasticsearch/pull/18157.

Current implementations that do not conform (and silently do nothing if the blob does not exist):
1. `FsBlobContainer`
2. `HdfsBlobContainer`
3. `S3BlobContainer`
4. `AzureBlobContainer`
</description><key id="156378275">18530</key><summary>BlobContainer's deleteBlob method should not be lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-05-23T22:11:15Z</created><updated>2016-08-04T09:12:46Z</updated><resolved>2016-08-01T16:04:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gfyoung" created="2016-06-10T12:00:35Z" id="225163982">`S3BlobContainer`conforms &lt;a href="https://github.com/elastic/elasticsearch/blob/e7eb664c78dce2451dbeb55db0c0b761a65b295b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java#L114"&gt;here&lt;/a&gt;, and `AzureBlobContainer`conforms &lt;a href="https://github.com/elastic/elasticsearch/blob/c4d3bf472bc5eaa18a1f1ffac727110a33f89b42/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java#L123"&gt;here&lt;/a&gt;.
The other two implementations do not conform though as you pointed out.
</comment><comment author="abeyad" created="2016-06-10T14:46:14Z" id="225202315">@gfyoung Actually, the `S3BlobContainer` does not yet conform, because this [line](https://github.com/elastic/elasticsearch/blob/e7eb664c78dce2451dbeb55db0c0b761a65b295b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java#L112) calls `S3BlobStore.client()#deleteObject`.  The `S3BlobStore.client()` returns an instance of `AmazonS3`.  If you look at the javadoc for `AmazonS3#deleteObject`, they say that the absence of the object will not throw an exception:

http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#deleteObject(java.lang.String,%20java.lang.String)

The exception handling [line](https://github.com/elastic/elasticsearch/blob/e7eb664c78dce2451dbeb55db0c0b761a65b295b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java#L114) you linked to only gets triggered if there are actual errors during deleting the object in S3, not if the object itself does not exist.

I believe its the same with `AzureBlobContainer`.  

Instead, in both S3 and Azure, we should first check for the presence of the object and if it does not exist, throw an `IOException` before deleting.
</comment><comment author="jasontedor" created="2016-06-10T14:46:52Z" id="225202491">&gt; `S3BlobContainer`conforms &lt;a href="https://github.com/elastic/elasticsearch/blob/e7eb664c78dce2451dbeb55db0c0b761a65b295b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java#L114"&gt;here&lt;/a&gt;, and `AzureBlobContainer`conforms &lt;a href="https://github.com/elastic/elasticsearch/blob/c4d3bf472bc5eaa18a1f1ffac727110a33f89b42/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java#L123"&gt;here&lt;/a&gt;.
&gt; The other two implementations do not conform though as you pointed out.

This is not correct. The documentation for `AmazonS3#deleteObject` specifies:

```
     * If attempting to delete an object that does not exist,
     * Amazon S3 returns
     * a success message instead of an error message.
```

It's not clear what the Azure service does as it's not specified in the docs, it would have to be tested.
</comment><comment author="jasontedor" created="2016-06-10T14:48:31Z" id="225202971">Also, it's important to note that some exceptions that are being wrapped into `IOException`s probably should not be, for example, general client exceptions and URI syntax exceptions.
</comment><comment author="gfyoung" created="2016-06-10T14:55:44Z" id="225205087">@abeyad : Hmmm...okay I see, so the correct fix would to call `blobExists` first **before** attempting the actual deletion.

@jasontedor : With regards to exceptions, where do we draw the line with regards to the statement &lt;a href="https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java#L98"&gt;here&lt;/a&gt; in the contract?  I agree that URI syntax errors should be excluded, but others are not as clear in my mind (e.g. exceptions associated with `Files.deleteIfExists`).

Also, I do agree that testing needs to be done, but I'm not sure how to do it as I stated &lt;a href="https://github.com/elastic/elasticsearch/pull/18815#issuecomment-225190213"&gt;here&lt;/a&gt; in the PR.
</comment><comment author="abeyad" created="2016-06-10T15:07:50Z" id="225208606">&gt; Hmmm...okay I see, so the correct fix would to call blobExists first before attempting the actual deletion.

@gfyoung yes that's correct
</comment><comment author="tlrx" created="2016-06-10T18:51:38Z" id="225265662">&gt; It's not clear what the Azure service does as it's not specified in the docs, it would have to be tested.

Same for Google Cloud Storage, I don't see any documentation about what it does.
</comment><comment author="javanna" created="2016-07-01T09:12:41Z" id="229898211">Reopening as the commit was reverted, see https://github.com/elastic/elasticsearch/pull/18815#issuecomment-229898122
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove multiple variations of deleting blobs from BlobContainer interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18529</link><project id="" key="" /><description>Currently in the `BlobContainer` interface, there are three types of delete blob methods:
1. void deleteBlob(String blobName)
2. void deleteBlobs(Collection&lt;String&gt; blobNames)
3. void deleteBlobsByPrefix(String blobNamePrefix)

These methods are again redundant and present confusing semantics.  Having just one `deleteBlob(String blobName)` is the only method that can be truly atomic (can not atomically delete a collection of blobs) and it avoids complex exception handling (what do we do if some of the blobs exist, but not others?).
</description><key id="156372219">18529</key><summary>Remove multiple variations of deleting blobs from BlobContainer interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-05-23T21:35:31Z</created><updated>2016-09-14T14:44:25Z</updated><resolved>2016-07-13T18:36:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-05-24T07:33:45Z" id="221189052">I agree it's confusing. The `deleteBlobs(Collection blobNames)` can be useful for backends that support batched deletions (like Google Cloud Storage or S3) but there's still some complex exception handling to do... so +1, maybe we should move everything to `deleteBlob(String blobName)`.
</comment><comment author="s1monw" created="2016-05-24T09:14:36Z" id="221211672">&gt; so +1, maybe we should move everything to deleteBlob(String blobName).

++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove redundant writeBlob methods in BlobContainer interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18528</link><project id="" key="" /><description>The `BlobContainer` interface, used by snapshots, contains two `writeBlob` methods:
1. `void writeBlob(String blobName, InputStream inputStream, long blobSize)`
2. `void writeBlob(String blobName, BytesReference bytes)`

There is no reason for the second one that takes a `BytesReference`, as a `BytesReference` can easily be converted to an `InputStream` to be used by the first method.  We should look into removing the second method signature for `writeBlob` in favor of a simpler API.
</description><key id="156370582">18528</key><summary>Remove redundant writeBlob methods in BlobContainer interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-05-23T21:26:29Z</created><updated>2016-08-04T09:12:46Z</updated><resolved>2016-08-02T13:21:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-24T09:13:31Z" id="221211400">can we maybe trash it altogether and make it a reasonable interface ie `public IndexOutput createOutput()`? 
</comment><comment author="abeyad" created="2016-05-26T22:38:42Z" id="222015188">@s1monw (sorry, prematurely clicked without completing the comment), I think so eventually but I viewed this as low hanging fruit to quickly make the API easier to work with.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Use -release with java 9 instead of -source/-target</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18527</link><project id="" key="" /><description>This uses the new -release flag for javac which does not require setting
the bootclasspath as -source/-target did.
</description><key id="156348333">18527</key><summary>Build: Use -release with java 9 instead of -source/-target</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T19:30:16Z</created><updated>2016-05-23T19:52:19Z</updated><resolved>2016-05-23T19:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-23T19:44:20Z" id="221074650">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add retry test case for delete-by-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18526</link><project id="" key="" /><description>Tests that we retry failed searches, scrolls, and bulks.
</description><key id="156339840">18526</key><summary>Add retry test case for delete-by-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-05-23T18:47:34Z</created><updated>2016-05-26T16:08:40Z</updated><resolved>2016-05-26T16:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-05-23T19:06:29Z" id="221065153">LGTM, thanks!
</comment><comment author="nik9000" created="2016-05-26T16:08:40Z" id="221917596">Closed by 5e8127050971296247b14dac7fe3af6d5686c016
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove travis configuration file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18525</link><project id="" key="" /><description>This commit removes the Travis configuration file as it's out-of-date.
</description><key id="156336422">18525</key><summary>Remove travis configuration file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T18:31:48Z</created><updated>2016-05-23T18:55:33Z</updated><resolved>2016-05-23T18:55:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-05-23T18:40:59Z" id="221058113">LGTM
</comment><comment author="nik9000" created="2016-05-23T18:52:08Z" id="221061208">LGTM. I tried to get travis working a long, long time ago but it was ultimately fruitless. The build just takes too long for it to work well with travis.
</comment><comment author="jasontedor" created="2016-05-23T18:55:33Z" id="221062127">Thanks @pickypg and @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup settings and system properties entanglement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18524</link><project id="" key="" /><description>This commit cleans up some additional places where system properties
were being used to pass settings to Elasticsearch.

Relates #18198
</description><key id="156332711">18524</key><summary>Cleanup settings and system properties entanglement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T18:14:36Z</created><updated>2016-05-23T18:47:28Z</updated><resolved>2016-05-23T18:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-23T18:23:24Z" id="221053445">LGTM
</comment><comment author="jasontedor" created="2016-05-23T18:47:27Z" id="221059901">Thanks @s1monw.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for -Des.* system properties in integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18523</link><project id="" key="" /><description>This now requires that system properties passed to Gradle must be in the form of `-Dtests.es.*` instead of `-Des.*`.

It then chops off "tests.es." and passes that as a "-E" property to Elasticsearch. I don't expect this to be used very frequently, but it makes it explicit and it gets us further away from `-Des.*`.
</description><key id="156328337">18523</key><summary>Remove support for -Des.* system properties in integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T17:56:29Z</created><updated>2016-05-23T23:38:50Z</updated><resolved>2016-05-23T23:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-23T18:43:10Z" id="221058749">The most common use that I know of for this is setting the transport mode via `es.node.mode`. This is documented in the [testing docs](https://github.com/elastic/elasticsearch/blob/555de4256e137f0829af810c53549e6f0a22b392/TESTING.asciidoc) so that will need to be corrected as well.
</comment><comment author="jasontedor" created="2016-05-23T20:20:11Z" id="221084095">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SnapshotsStatusRequest should take ignoreUnavailable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18522</link><project id="" key="" /><description>The `GetSnapshotsRequest` has the option to ignore unavailable, which means that if a requested snapshot is missing, an exception won't get thrown.  A similar option should be added to `SnapshotsStatusRequest` for consistency.
</description><key id="156320859">18522</key><summary>SnapshotsStatusRequest should take ignoreUnavailable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>low hanging fruit</label><label>v5.0.0-beta1</label></labels><created>2016-05-23T17:21:52Z</created><updated>2016-09-14T14:44:25Z</updated><resolved>2016-08-19T20:19:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Enable Scrolling from a defined position</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18521</link><project id="" key="" /><description>**Describe the feature**:

At the moment, adding a scroll parameter will ignore the From parameter. This means that you cannot start scrolling from a specific offset. This could be useful in scenarios where you have a large data set and you want to jump to a specific position and load a big chunk of data.
Examples:
1. Paginated results where the scroll context expired
2. Data exports with predefined windows
</description><key id="156307298">18521</key><summary>Enable Scrolling from a defined position</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bbeda</reporter><labels /><created>2016-05-23T15:59:38Z</created><updated>2016-05-24T09:07:24Z</updated><resolved>2016-05-24T09:07:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-24T09:07:24Z" id="221209907">Hi @bbeda 

We've discussed this already here: https://github.com/elastic/elasticsearch/issues/13494#issuecomment-217368934

Closing in favour of #13494
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query_string query with wildcard not working when searching within nested objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18520</link><project id="" key="" /><description>**Elasticsearch version**: 2.3

**JVM version**: 1.8.0_25

**OS version**: OSX 10.11.5 

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. Define a nested object type in your mapping
2. Index some documents 
3. Perform a nested query with a query_string on on of the field of the nested object using a wildcard

I am indexing the following document with a nested type locations:

```
curl -X PUT 'http://localhost:9201/test_nested?pretty' -d '{
  "settings": {
    "index": {
      "number_of_shards": 5,
      "number_of_replicas": 1
    }
  },
  "mappings": {
    "loc": {
      "properties": {
        "id": {
          "type": "string",
          "index": "not_analyzed"
        },
        "locations": {
          "type": "nested",
          "properties": {
            "input": {
              "type": "string",
              "index": "not_analyzed"
            },
            "country_code": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}'

curl -X PUT 'http://localhost:9201/test_nested/loc/location_1' -d '{
    id: "location_1",
    "locations": [
      {
        input: "xxx",
        country_code: "BE"
      },
      {
        input: "yyy",
        country_code: "NL"
      }
    ]
}'

curl -X PUT 'http://localhost:9201/test_nested/loc/location_2' -d '{
    id: "location_2",
    "locations": [
      {
        input: "zzz",
        country_code: "BR"
      },
      {
        input: "vvv",
        country_code: "US"
      }
    ]
}'
```

When i try todo an extact match using query_string it works:

```
curl -X POST 'http://localhost:9201/test_nested/_search' -d '
{
 "query": {
       "nested": {
           "path": "locations",
           "query": {
               "query_string": {
                   "fields": [
                       "locations.country_code"
                   ],
                   "query": "BE"
               }
           }
       }
   }
}
'

RESULT:

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":2.098612,"hits":[{"_index":"test_nested","_type":"loc","_id":"location_1","_score":2.098612,"_source":{
    id: "location_1",
    "locations": [
      {
        input: "xxx",
        country_code: "BE"
      },
      {
        input: "yyy",
        country_code: "NL"
      }
```

When i try todo a wildcard it does not return anything:

```
curl -X POST 'http://localhost:9201/test_nested/_search' -d '
{
 "query": {
       "nested": {
           "path": "locations",
           "query": {
               "query_string": {
                   "fields": [
                       "locations.country_code"
                   ],
                   "query": "B*"
               }
           }
       }
   }
}
'

RESULT:

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

It seems to only not be working for nested datatypes because if i do a query_string with a wildcard on a property of the main document it works as expected:

```
curl -X POST 'http://localhost:9201/test_nested/_search' -d '
{
   "query": {
       "query_string": {
           "fields": [
               "id"
           ],
           "query": "location_*"
       }
   }
}
'

RESULT:

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"test_nested","_type":"loc","_id":"location_1","_score":1.0,"_source":{
    id: "location_1",
    "locations": [
      {
        input: "xxx",
        country_code: "BE"
      },
      {
        input: "yyy",
        country_code: "NL"
      }
    ]
}},{"_index":"test_nested","_type":"loc","_id":"location_2","_score":1.0,"_source":{
    id: "location_2",
    "locations": [
      {
        input: "zzz",
        country_code: "BR"
      },
      {
        input: "vvv",
        country_code: "US"
      }
    ]
}}]}}

```

So is this a limitation of the nested datatype that it does not work with wildcard when using query_string? And if yes where is it documented? The weird thing is as well if i do a wildcard query it seems to work as well.

```
curl -X POST 'http://localhost:9201/test_nested/_search' -d '
{
 "query": {
       "nested": {
           "path": "locations",
           "query": {
               "wildcard": {
                   "locations.country_code": "B*"
               }
           }
       }
   }
}
'

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"test_nested","_type":"loc","_id":"location_1","_score":1.0,"_source":{
    id: "location_1",
    "locations": [
      {
        input: "xxx",
        country_code: "BE"
      },
      {
        input: "yyy",
        country_code: "NL"
      }
    ]
}},{"_index":"test_nested","_type":"loc","_id":"location_2","_score":1.0,"_source":{
    id: "location_2",
    "locations": [
      {
        input: "zzz",
        country_code: "BR"
      },
      {
        input: "vvv",
        country_code: "US"
      }
    ]
}}]}}
```
</description><key id="156301803">18520</key><summary>query_string query with wildcard not working when searching within nested objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">butsjoh</reporter><labels /><created>2016-05-23T15:17:35Z</created><updated>2016-05-26T07:56:39Z</updated><resolved>2016-05-24T09:05:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-24T09:05:57Z" id="221209587">Hi @butsjoh 

Nothing to do with nested queries.  The issue is that the field is not analyzed, so is indexed as `BE`, but the `lowercase_expanded_terms` param (which applies to wildcards) in the query string query defaults to `true`.

Duplicate of https://github.com/elastic/elasticsearch/issues/9978
</comment><comment author="butsjoh" created="2016-05-24T12:09:55Z" id="221249183">@clintongormley Sorry but i do not fully understand your response. How could i then change my situation in order to make it work. You have to agree that the id field in the mapping i posted is also not analyzed but there the wildcard (location_*) works and for a field in the nested object it does not? Do you then recommend setting lowercase_expanded_terms to false for the nested query?
</comment><comment author="clintongormley" created="2016-05-24T12:16:55Z" id="221250701">&gt; You have to agree that the id field in the mapping i posted is also not analyzed but there the wildcard (location_*) 

Yes, the `id` field is `not_analyzed`, but the value you're indexing is already lower case, so it matches.  If you change the ID value to `LOCATION_FOO` and search that field for `LOC*`, it won't find anything either.

&gt; Do you then recommend setting lowercase_expanded_terms to false for the nested query?

As long as you're only planning on using wildcards on `not_analyzed` fields, then yes :)  If you want to use it on analyzed fields too, then you'll have the same problem but in reverse.

This is not an easy problem to solve, which is why #9978 is marked as high hanging fruit. It requires a big rewrite of our analysis framework.
</comment><comment author="butsjoh" created="2016-05-24T12:19:22Z" id="221251225">Ok i think i got it. So in order for my to avoid setting lowercase_expanded_terms to false i could analyse the field i am searching on with applying a lowercase filter on it in the mapping. Correct?
</comment><comment author="butsjoh" created="2016-05-26T07:56:39Z" id="221802704">So i can indeed confirm if you apply a lowercase filter on the country_code field (by setting up a analyzer in the mapping) i don't need to set lowercase_expanded_terms to false anymore and a query_string search for B\* or b\* will return both documents.

thnx for this clarification
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add console to docs for inner hits, explain, and friends</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18519</link><project id="" key="" /><description>This adds CONSOLE to inner hits, explain, field-stats, request body, scroll docs.

Relates to #18160 

@nik9000 Care to take a look?
</description><key id="156282336">18519</key><summary>Add console to docs for inner hits, explain, and friends</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-05-23T13:36:06Z</created><updated>2016-08-04T09:10:17Z</updated><resolved>2016-08-01T10:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-23T17:53:15Z" id="221045310">Left two suggestions but LGTM.
</comment><comment author="nik9000" created="2016-07-22T14:49:59Z" id="234564733">@MaineC could you finish this one up? I might start making more of these sorts of PRs if you are willing to review them.
</comment><comment author="MaineC" created="2016-07-27T12:52:40Z" id="235575434">@nik9000 I updated the PR; will merge if CI is green (only needed a few IMHO minor changes from what you already LGTM'ed earlier to make things work again).

TODO left for later: In explain.asciidoc I had to add a "\n" to the expected response as this is what the response seems to be looking like. My searching skills left me trying to figure out where in our code this is coming from.
</comment><comment author="MaineC" created="2016-08-01T08:36:53Z" id="236521267">jenkins, test it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `*_as_string` from the output of aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18518</link><project id="" key="" /><description>Many aggregations provide two representations of aggregated values, such as `key` and `key_as_string` in terms/histogram aggregations. This is annoying because this is duplicate information and the `key` return value leaks the internal representation of data in doc values. For instance this forced us to break the response of terms or range aggregations on `ip` fields when we added ipv6 support since we do not store ip addresses in a number anymore. We should return a single `key` parameter and drop `key_as_string`.
</description><key id="156264865">18518</key><summary>Remove `*_as_string` from the output of aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>breaking</label><label>bug</label><label>v6.0.0</label></labels><created>2016-05-23T12:05:20Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-23T12:37:06Z" id="220968099">As a note, `key_as_string` however is currently used for formatting utc keys for date histograms according to the time zone specified for rounding in the query. We introduced this back in #9710, if we remove this every bucket key needs to be converted to the appropriate time zone on the user side to display the correct local time on the buckets.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Duplicate value/getValue in Max aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18517</link><project id="" key="" /><description>`org.elasticsearch.search.aggregations.metrics.max.Max` has both a `value` and a `geValue()` methods. It should only have one of them. Other single-metric aggs seem to be impacted as well.

cc @colings86 
</description><key id="156263934">18517</key><summary>Duplicate value/getValue in Max aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-05-23T11:59:51Z</created><updated>2017-03-01T16:04:16Z</updated><resolved>2017-03-01T16:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kartiksomani" created="2016-06-09T16:55:35Z" id="224958052">I was thinking of changing value to getValue in SingleValue interface. But the metrics Cardinality and ValueCount return long, while others including SingleValue interface. 

any suggestions?
</comment><comment author="sudheesh001" created="2017-02-23T11:39:29Z" id="281969874">I don't think this issue exists anymore and can be closed. `Max` only contains `getValue()` now,</comment><comment author="clintongormley" created="2017-03-01T16:04:16Z" id="283383097">thanks @sudheesh001 </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Delete-By-Query plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18516</link><project id="" key="" /><description>This pull request removes the Delete-By-Query plugin and document the breaking changes. It has been replaced by the Delete-By-Query API implemented in the Reindex module.

It also replaces a wrong documentation link in the JSON REST spec.

Closes #18469
</description><key id="156261412">18516</key><summary>Remove Delete-By-Query plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-05-23T11:43:55Z</created><updated>2016-05-26T11:36:48Z</updated><resolved>2016-05-24T11:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-23T16:36:35Z" id="221025659">I looked over the docs and left a few comments but none of them are a big deal. I just skimmed the "delete the plugin" part but it looks right. LGTM.
</comment><comment author="s1monw" created="2016-05-23T18:24:36Z" id="221053776">I love the stats  - trash it!
</comment><comment author="nik9000" created="2016-05-23T18:49:58Z" id="221060625">Oh! You should probably check the vagrant/bats tests if you haven't already. They install all the plugins and verify that they installed. I think you'll have to nuke a few test cases there too.
</comment><comment author="tlrx" created="2016-05-23T19:11:26Z" id="221066409">&gt; Oh! You should probably check the vagrant/bats tests if you haven't already. They install all the plugins and verify that they installed. I think you'll have to nuke a few test cases there too.

Hum, I removed the `delete-by-query` mentions in `module_and_plugin_test_cases.bash` in this PR... did I miss something else?
</comment><comment author="nik9000" created="2016-05-23T19:41:56Z" id="221074018">&gt; Hum, I removed the delete-by-query mentions in module_and_plugin_test_cases.bash in this PR... did I miss something else?

That should be good enough. I didn't see it because I was searching the page for "bats" instead of "bash". Silly me!
</comment><comment author="tlrx" created="2016-05-24T11:29:18Z" id="221241294">Thanks @nik9000 and @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Non descriptive enough error when reindexing incorrect data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18515</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1
**JVM version**: 1.7.0_95
**OS version**: Amazon Linux 2015.09

**Description of the problem including expected versus actual behaviour**:

Reindexing an index fails (when reindexing for the second time) because inserted types are wrong from the mapping (Field value is `""` (`string` type) when it should be `null`(`date` type), error thrown is: `mapper_parsing_exception`. The thrown error should be something like: _Failed inserting `""` into a `date` field_, I spend quite some time until I realised what was wrong, full error is:

``` json
{
  "took": 294944,
  "timed_out": false,
  "total": 16852802,
  "updated": 0,
  "created": 40,
  "batches": 7190,
  "version_conflicts": 718959,
  "noops": 0,
  "retries": 0,
  "failures": [
    {
      "index": "logstash-2016.05.06",
      "type": "syslog",
      "id": "AVSE8LgGhfFJ1TNSUrWP",
      "cause": {
        "type": "mapper_parsing_exception",
        "reason": "failed to parse [path.field_name]",
        "caused_by": {
          "type": "illegal_argument_exception",
          "reason": "Invalid format: """
        }
      },
      "status": 400
    }
  ]
}
```

The thing is, on the first instance, it should have told me this error (indexing time), then if that did not pop up, it should have let me know about the problem on the first reindex operator (from `logstash-2016.05.06` to `logstash-new` and thrown a more descriptive error) not on the 2nd time I reindexed (from `logstash-new` to `logstash-2016.05.06` which was deleted and recreated again with proper index templates and mappings and no data)

**Steps to reproduce**:
1. Have a `date` field with a `""` value
2. Reindex index A to index B
3. Reindex index B to index C

**`_reindex` operation run**

``` json
{
  "conflicts": "proceed",
  "source": {
    "index": "logstash-new",
    "type": "syslog"
  },
  "dest": {
    "index": "logstash-2016.05.06",
    "op_type": "create"
  }
}
```
</description><key id="156240409">18515</key><summary>Non descriptive enough error when reindexing incorrect data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marclop</reporter><labels><label>:Exceptions</label><label>:Mapping</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-05-23T09:45:31Z</created><updated>2017-03-15T05:49:26Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-27T09:10:04Z" id="222098421">this seems like a reasonable request, `Joda.java` does this and we should maybe put more context into the error message here!
</comment><comment author="mortonsykes" created="2017-03-12T22:46:24Z" id="285983544">Hi all , 
I am new to ElasticSearch and I am interested in picking this one up.</comment><comment author="nik9000" created="2017-03-13T00:04:29Z" id="285989248">Sure! I'm not 100% sure this is a reindexing issue as much as it is an
indexing issue. But you can figure that out as you work on it.

On Sun, Mar 12, 2017, 3:46 PM mortonsykes &lt;notifications@github.com&gt; wrote:

&gt; Hi all ,
&gt; I am new to ElasticSearch and I am interested in picking this one up.
&gt;
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/18515#issuecomment-285983544&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AANLooTCzv4XzGkPYAJ1zWAtFPaHuO04ks5rlHXDgaJpZM4IkWr7&gt;
&gt; .
&gt;
</comment><comment author="mortonsykes" created="2017-03-15T05:49:20Z" id="286647383">Hi i've been looking into this , It appears this issue has already been fixed? . I have been trying to replicate the bug however I always receive a mapper_parsing_exception , "type": "illegal_argument_exception",
"reason": "Invalid format: """

`PUT my_index
{
"mappings": {
"my_type": {
"properties": {
"date": {
"type": "date"
}
}
}
}
}`

then 

`PUT my_index/my_type/1
{ "date": "" }`



</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce dedicated master nodes in testing infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18514</link><project id="" key="" /><description>This PR changes the InternalTestCluster to support dedicated master nodes. The creation of dedicated master nodes can be controlled using a new `@numMasterNodes` annotation. If set to something &gt;0, dedicated master nodes will be created. If set to 0, no master nodes will be created and data nodes will also be allowed to become masters. When set to -1, it's default, dedicated master will be randomly created (50% to no master nodes, 50% to choosing between 1 to 3 dedicated master nodes).

The nodes' data path default configuration is changed to separate nodes based on their roles. This is to avoid the situation where a restart of a master node will cause it to pick up on the path. 

I also fixed all tests that were failing by this change. For everything that wasn't easy to fix I just rolled back to the previous behavior by setting `numMasterNodes=0` on the test. 

There was also a concern regarding test speeds, therefore we will use 0 or 1 dedicated master most of the time. 
</description><key id="156223545">18514</key><summary>Introduce dedicated master nodes in testing infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-05-23T08:17:27Z</created><updated>2016-06-28T09:51:46Z</updated><resolved>2016-05-27T06:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-23T16:30:24Z" id="221024169">&gt; 50% to choosing between 1 to 3

Is 2 an ok choice? Like, I wouldn't chose it for a real deploy so maybe it should just be 1 or 3?
</comment><comment author="javanna" created="2016-05-23T16:56:15Z" id="221030585">I left a bunch of comments, good change, wanted for a long time ;)
</comment><comment author="s1monw" created="2016-05-23T18:36:00Z" id="221056815">thanks for doing this boaz &#128077; 
</comment><comment author="bleskes" created="2016-05-24T10:23:07Z" id="221227872">&gt; Is 2 an ok choice? Like, I wouldn't chose it for a real deploy so maybe it should just be 1 or 3?

@nik9000  2 is an OK choice for testing purposed, but given where we ended up with the randomization logic, I simplified things and just made it choose between 1 and 3
</comment><comment author="bleskes" created="2016-05-24T10:36:24Z" id="221230867">@javanna @s1monw @nik9000 thx for the comments. I pushed more commits, merged from master and responded.
</comment><comment author="bleskes" created="2016-05-24T15:04:58Z" id="221300807">@javanna one boolean for you. PR updated.
</comment><comment author="javanna" created="2016-05-24T15:58:01Z" id="221318192">left two minors, LGTM though. Should we add something to `testing-framework.asciidoc` also?
</comment><comment author="bleskes" created="2016-05-25T11:26:39Z" id="221545407">@javanna I pushed some docs
</comment><comment author="javanna" created="2016-05-25T12:23:43Z" id="221558079">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.mapper.dynamic: false prevents automatic index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18513</link><project id="" key="" /><description>Elasticsearch version: 2.3.3

JVM version:
java version "1.8.0_92"
Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)

OS version: Arch Linux kernel 4.5.4

**Description of the problem including expected versus actual behavior**:
if you set `index.mapper.dynamic: false` in `elasticsearch.yml` automatic index creation no longer works. That was not the case in ES 2.1, broke the upgrade from 2.1 to 2.3.

**Steps to reproduce**:
1. Add `index.mapper.dynamic: false` to your config
2. Add the following index template:

``` json
{
  "order": 0,
  "template": "trace_*",
  "settings": {},
  "mappings": {
    "trace": {
      "dynamic": "false",
      "_source": {
        "includes": [
          "@timestamp",
          "host",
          "file",
          "message"
        ]
      },
      "properties": {
        "@timestamp": {
          "type": "date"
        },
        "file": {
          "index": "not_analyzed",
          "type": "string"
        },
        "host": {
          "index": "not_analyzed",
          "type": "string"
        },
        "message": {
          "type": "string"
        }
      }
    }
  },
  "aliases": {}
}
```
1. Attempt to index a document `POST trace_20160524/trace`

``` json
{
  "message": "test"
}
```

Receive the following error:

``` json
{
  "error": {
    "root_cause": [
      {
        "type": "index_not_found_exception",
        "reason": "no such index",
        "resource.type": "index_expression",
        "resource.id": "trace_20160524",
        "index": "trace_20160524"
      }
    ],
    "type": "index_not_found_exception",
    "reason": "no such index",
    "resource.type": "index_expression",
    "resource.id": "trace_20160524",
    "index": "trace_20160524"
  },
  "status": 404
}
```

If I remove `index.mapper.dynamic: false` from config file, then index is created automatically. However, I want to keep the option, so that index is automatically created only if the mapping template was defined.
</description><key id="156217900">18513</key><summary>index.mapper.dynamic: false prevents automatic index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">n3ziniuka5</reporter><labels /><created>2016-05-23T07:42:17Z</created><updated>2016-05-23T11:14:14Z</updated><resolved>2016-05-23T11:14:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-23T08:43:12Z" id="220920851">maybe I misunderstand, you want the index to be created even if you send an invalid document?
</comment><comment author="n3ziniuka5" created="2016-05-23T09:34:18Z" id="220932512">What do you mean? How is the document invalid? It has a single field `message` and the index mapping was defined before inserting that document, but instead I get an error that the index does not exist when it should be created automatically. If I remove the `index.mapper.dynamic: false` option, then it creates the index.
</comment><comment author="clintongormley" created="2016-05-23T11:14:14Z" id="220952351">Actually, the setting in the config file has never worked correctly, see https://github.com/elastic/elasticsearch/issues/17561#issuecomment-217098396

Also, this setting can no longer be set in the config file as of 5.0, so I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to forbiddenapis 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18512</link><project id="" key="" /><description>Forbiddenapis v2.1 was released a few minutes ago.

The new version supports:
- Java 9 b119 support (updated ASM - support for new bytecode version; no dependency on bytecode version of JDK internals anymore)
- internalRuntimeForbidden deprecated, new bundled sigantures "jdk-non-portable"
- new bundled signatures: jdk-reflection (forbids setAccessible())
- Bugfixes, the most important one is about not detecting violations if a superclass was forbidden, but method was overriden in subclass. This is the reason for a fix together with the commit

This version also contains the changes suggested by @rmuir about a separate bundled signatures 'jdk-internal'. Those are hardcoded to disallow packages marked as internal. The 'jdk-non-portable' is a superset of them. The first one is useful for stuff like thirdPartyAudit, while the latter is good for guaranteening that only portable APIs are used. See PRs and issues about that.

I did not add 'jdk-reflection' because thats already in the resources provided by ES.
</description><key id="156173537">18512</key><summary>Update to forbiddenapis 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Internal</label><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-22T21:17:48Z</created><updated>2016-05-26T11:42:42Z</updated><resolved>2016-05-22T22:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-22T22:43:54Z" id="220861185">Thanks Uwe!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move the percolator from core to its own module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18511</link><project id="" key="" /><description>The most significant changes I had to make was moving `AbstractQueryTestCase` to the test framework module and add support the register custom plugins. Moving this base test class to the test framework was required because the `percolate` query is the first query outside of the core module and its unit test `PercolateQueryBuilderTests` extends from this test base class. The `AbstractQueryTestCase` didn't have support the register queries outside of core, so added support to the `AbstractQueryTestCase` class to register a plugin which then register its custom queries if the plugin contains that. I'm not super happy with the way needed to expose this, the entire initialization happens in [a static method](https://github.com/martijnvg/elasticsearch/blob/percolator_module/test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java#L207), so tests that want to test custom query builders now need to have a [static initializer](https://github.com/martijnvg/elasticsearch/blob/percolator_module/modules/percolator/src/test/java/org/elasticsearch/percolator/PercolateQueryBuilderTests.java#L54) that registers a plugin. If someone has a simpler/cleaner solution for this then that would be great.

Other changes:
- Lift the restriction that only one percolator could be added per index. This validation existed in MapperService, but because the percolator moved to a module it could no longer exist there. Instead of bringing it back it was removed. This validation existed since the percolator cache only supported one percolator query per document, since the percolator cache has been removed this restriction could removed as well.
- While moving percolator tests to the new module, also removed a couple of tests for the deprecated percolate and mpercolate api. These APIs are now sugar APIs for bwc support and redirect to the search and msearch APIs. Some tests were still testing as if percolate and mpercolate API did the percolation, but this no longer the case and these tests could be removed.

Note: The breaking part is that the percolate methods have been removed from the `Client` class in the Java api.
</description><key id="156173392">18511</key><summary>Move the percolator from core to its own module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-22T21:15:39Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-05-24T10:00:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-05-23T16:53:30Z" id="221029891">@s1monw @cbuescher I've changed the initialization method to be a member method and introduced this the `Holder` clause that gets initialized only once per test suite.
</comment><comment author="s1monw" created="2016-05-23T18:21:25Z" id="221052869">left some minors LGTM otherwise &#128077; 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18510</link><project id="" key="" /><description>$ MacOS 10.9

$ elasticsearch -v
Version: 1.7.5, Build: 00f95f4/2016-02-02T09:55:30Z, JVM: 1.8.0_25

What is it?

```
Elasticsearch request failed with code 500.
Error Info: Array
(
    [requestMethod] =&gt; GET
    [requestUrl] =&gt; http://127.0.0.1:9200/1vse/product/_search
    [requestBody] =&gt; {"size":10,"query":{"bool":{"must":[{"term":{"category.id":"1078"}}]}},"aggregations":{"price_max":{"max":{"field":"price.min"}},"price_min":{"min":{"field":"price.min"}}}}
    [responseCode] =&gt; 500
    [responseHeaders] =&gt; Array
        (
            [content-type] =&gt; application/json; charset=UTF-8
            [content-length] =&gt; 323
        )

    [responseBody] =&gt; Array
        (
            [error] =&gt; SearchPhaseExecutionException[
               Failed to execute phase [query_fetch], all shards failed; shardFailures {[cnOdbQjYTHawZ-iRiLsn1g][1vse][0]: ClassCastException[
               org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}]
            [status] =&gt; 500
        )

)
```
</description><key id="156172639">18510</key><summary>org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mirocow</reporter><labels /><created>2016-05-22T21:02:37Z</created><updated>2016-05-23T10:45:51Z</updated><resolved>2016-05-23T10:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-23T10:45:51Z" id="220947222">Hi @Mirocow 

You either have (1) two fields with the same name `price.min` (or `min` in type `price`) in different types which have different mappings or (2) a corrupt mapping on one shard which has been writing `price.min` as a `string` instead of a numeric field.

I highly recommend upgrading to 2.x - the situation with mappings is greatly improved.  Either way, it looks like you will have to reindex this index (and ensure that you don't have fields with the same name in different types with conflicting mappings).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support full range of Java Long for epoch DateTime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18509</link><project id="" key="" /><description>Addresses #17936.

Remove the arbitrary limit on epoch_millis and epoch_seconds of 13 and 10
characters, respectively. Instead allow any character combination that can
be converted to a Java Long.

Update the docs to reflect this change.
</description><key id="156170310">18509</key><summary>Support full range of Java Long for epoch DateTime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rbellamy</reporter><labels><label>:Dates</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-22T20:13:36Z</created><updated>2016-05-23T10:27:57Z</updated><resolved>2016-05-23T10:27:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>failed to rollback writer on close</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18508</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.1.2
**JVM version**:
jdk1.8.0_60
**OS version**:
CentOS release 6.6 (Final)
**Description of the problem including expected versus actual behavior**:

running a cluster for a long time, rolling update from 2.1.0 to 2.1.2 and one node keeps throw below exception:
[2016-05-22 22:34:58,678][WARN ][index.engine             ] [d_172.20.122.110:9204] [op_logs_2016-05-26][7] failed to read latest segment infos on flush
java.nio.file.FileSystemException: /data6/elasticsearch/jiesi-59/jiesi-59/nodes/0/indices/op_logs_2016-05-26/7/index: Input/output error
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
        at java.nio.file.Files.newDirectoryStream(Files.java:457)
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:179)
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:191)
        at org.elasticsearch.index.store.FsDirectoryService$1.listAll(FsDirectoryService.java:127)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:567)
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:563)
        at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:146)
        at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:783)
        at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:732)
        at org.elasticsearch.index.engine.Engine.flushAndClose(Engine.java:1128)
        at org.elasticsearch.index.shard.IndexShard.close(IndexShard.java:829)
        at org.elasticsearch.index.IndexService.closeShardInjector(IndexService.java:443)
        at org.elasticsearch.index.IndexService.removeShard(IndexService.java:416)
        at org.elasticsearch.index.IndexService.close(IndexService.java:253)
        at org.elasticsearch.indices.IndicesService.removeIndex(IndicesService.java:413)
        at org.elasticsearch.indices.IndicesService.access$000(IndicesService.java:108)
        at org.elasticsearch.indices.IndicesService$1.run(IndicesService.java:174)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="156155272">18508</key><summary>failed to rollback writer on close</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-05-22T14:42:55Z</created><updated>2017-03-15T00:19:14Z</updated><resolved>2016-05-22T15:04:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2016-05-22T14:47:04Z" id="220836254">one more thing to add up:
I find below logs in dmesg:
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.
XFS (sdh1): xfs_log_force: error 5 returned.

is it cause by file system or disk broken?
dose ES have ability to detected this and make the cluster recovery from this?
</comment><comment author="jasontedor" created="2016-05-22T14:48:53Z" id="220836333">&gt; java.nio.file.FileSystemException: /data6/elasticsearch/jiesi-59/jiesi-59/nodes/0/indices/op_logs_2016-05-26/7/index: Input/output error

This is almost always indicative of a filesystem error (usually hardware), not an application error.
</comment><comment author="makeyang" created="2016-05-22T14:50:19Z" id="220836411">ok. got it. 
so I guess the last question is: can ES detected this and make the cluster health under this condition happen?
</comment><comment author="jasontedor" created="2016-05-22T15:04:53Z" id="220837118">&gt; XFS (sdh1): xfs_log_force: error 5 returned.

You definitely have a hardware issue. The engine should fail itself and that should trigger a recovery but the best course of action here is to completely take that node out of the cluster. 

Since this is a hardware issue and not an Elasticsearch issue, I'll close this issue but let me know if you think otherwise. 
</comment><comment author="makeyang" created="2016-05-22T15:16:19Z" id="220837712">@jasontedor  pay attention to the very first log I post, actually it is one of many disks in that node have issue, so the engine should not fail completely but exclude allocation shard on that one, right?
</comment><comment author="jasontedor" created="2016-05-22T15:37:07Z" id="220838749">Engines are per shard, and there is not an allocation decider for excluding a disk that experienced an exception. If you want to exclude that disk you have to remove it from `path.data`. But I wouldn't trust that node even removing that failing disk until you run checks (SMART status etc.) on all the disks on that node and even a memory test.

Relates #18279
</comment><comment author="makeyang" created="2016-05-22T15:40:36Z" id="220838953">I get what u are talling about engine. 
The engine should fail itself and that should trigger a recovery but the best course of action here is to completely take that node out of the cluster.
is this the current action or is it the future action?
I bet it is the future action, right? otherwise, the es server shouldn't keep throw exception.
</comment><comment author="jasontedor" created="2016-05-22T15:44:23Z" id="220839163">I mean that _you_ should take the node out of the cluster. 
</comment><comment author="makeyang" created="2016-05-22T15:49:33Z" id="220839462">sure. it has been done. 
my question is: can ES handle this situation by itself or it must be recoveried manually?
</comment><comment author="jasontedor" created="2016-05-22T16:31:00Z" id="220841788">Removing a node from a cluster is a course of action that should only be done by an operator that is aware of end-user SLAs, maintenance windows, etc.
</comment><comment author="makeyang" created="2016-05-23T03:13:29Z" id="220879165">@jasontedor I am sorry I don't make myself clear. what I concerned is below:
 The engine should fail itself and that should trigger a recovery but the best course of action here is to completely take that node out of the cluster.
so ES can do this or not currently?
</comment><comment author="s1monw" created="2016-05-23T07:16:14Z" id="220903974">&gt; The engine should fail itself and that should trigger a recovery but the best course of action here is to completely take that node out of the cluster.
&gt; so ES can do this or not currently?

it won't take the node our of the cluster currently.
</comment><comment author="wongder" created="2017-03-14T18:54:24Z" id="286524450">Hey guys, can you pls point me to some info or links on elasticsearch rollback to undo the last updates....maybe this can be done via the _version ??  Thank you.</comment><comment author="jasontedor" created="2017-03-14T20:40:12Z" id="286552622">&gt; can you pls point me to some info or links on elasticsearch rollback to undo the last updates....maybe this can be done via the _version ?

This is not possible. If you have additional questions, please ask them on the [forum](https://discuss.elastic.co).</comment><comment author="wongder" created="2017-03-15T00:19:14Z" id="286602923">Thank you Jason, i actually did enter this question in the forum but will make sure to use your forum link in your reply next time, thanks again!


On 03-14-2017, at 4:40 PM, Jason Tedor &lt;notifications@github.com&lt;mailto:notifications@github.com&gt;&gt; wrote:


can you pls point me to some info or links on elasticsearch rollback to undo the last updates....maybe this can be done via the _version ?

This is not possible. If you have additional questions, please ask them on the forum&lt;https://discuss.elastic.co/&gt;.

&#8212;
You are receiving this because you commented.
Reply to this email directly, view it on GitHub&lt;https://github.com/elastic/elasticsearch/issues/18508#issuecomment-286552622&gt;, or mute the thread&lt;https://github.com/notifications/unsubscribe-auth/AC6Zb1kfrSBVnlAoxVuZmdaBMq_Vo0fzks5rlvtTgaJpZM4IkADH&gt;.

</comment></comments><attachments /><subtasks /><customfields /></item><item><title>epoch_millis should accept more than 13 character digits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18507</link><project id="" key="" /><description>**Describe the feature**:
Currently `epoch_millis` has what appears to be an arbitrary 13 character upper limit on what can be parsed. The [date format documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html) indicates that the year range is between 1653 and 2286, and that you should use a different formatter if your values exceed that range.

Because `epoch_seconds` is a subset of the `epoch_millis` format, it too has a strange upper limit of 10 characters.

We commonly receive (incorrect) data with epoch millisecond values with more than 13 characters.

For `epoch_millis`, one might assume this low number of digits is an attempt to prevent 32 signed int overflow, but that doesn't compute.

Max signed 32 bit int is 2147483647, which is 10 digits.

The underlying library used by Elasticsearch for Date parsing is Joda Time, and uses `DateTime(millis, DateTimeZone.UTC)` to construct an immutable DateTime value from `epoch_millis` , where `millis` is a Java Long value. `millis` can certainly accept more than 13 chars of digits.

Cross-post alert:
I've asked this question in the [discussion forums](https://discuss.elastic.co/t/epoch-millis-has-a-maximum-of-13-digits-joda-time-accepts-a-long/50508) as well.

I have a PR ready to submit, pending the outcome of this discussion.
</description><key id="156130628">18507</key><summary>epoch_millis should accept more than 13 character digits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rbellamy</reporter><labels /><created>2016-05-22T01:59:48Z</created><updated>2016-05-22T08:26:57Z</updated><resolved>2016-05-22T08:26:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-22T08:26:57Z" id="220820453">Duplicates #17936 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge pull request #1 from elastic/master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18506</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

Syncing with the parent repo
</description><key id="156113900">18506</key><summary>Merge pull request #1 from elastic/master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scorpionvicky</reporter><labels /><created>2016-05-21T17:53:56Z</created><updated>2016-05-21T17:57:04Z</updated><resolved>2016-05-21T17:57:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Merge pull request #1 from elastic/master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18505</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

Syncing with the parent repo
</description><key id="156113197">18505</key><summary>Merge pull request #1 from elastic/master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scorpionvicky</reporter><labels /><created>2016-05-21T17:36:46Z</created><updated>2016-05-21T17:50:26Z</updated><resolved>2016-05-21T17:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-21T17:50:26Z" id="220791471">This appears to have been opened in error. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>prevent registration of duplicated rest spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18504</link><project id="" key="" /><description>Rather than having one win against the other, reject duplicated apis. Also enforce the convention that see the api name have the same name as the name of the rest spec file that defines it.
</description><key id="156062291">18504</key><summary>prevent registration of duplicated rest spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T22:27:44Z</created><updated>2016-05-23T10:17:42Z</updated><resolved>2016-05-23T10:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-21T02:52:04Z" id="220754734">LGTM.  `?w=1` is your friend for changes like this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't mkdir directly in deb init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18503</link><project id="" key="" /><description>Elasticsearch will create the log and data path directories when it
starts, there is no need to do this in the init script

Resolves #18307
</description><key id="156058681">18503</key><summary>Don't mkdir directly in deb init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T21:58:31Z</created><updated>2016-05-23T07:59:40Z</updated><resolved>2016-05-21T05:04:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-21T00:21:42Z" id="220746748">LGTM.
</comment><comment author="dakrone" created="2016-05-21T05:05:06Z" id="220758933">Thanks @jasontedor!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ScriptMode class in favor of boolean true/false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18502</link><project id="" key="" /><description>This removes the ScriptMode class entirely, which was an enum with two
options (ON and OFF) which essentially boiled down to true and false.
Now the boolean values are used instead.
</description><key id="156050505">18502</key><summary>Remove ScriptMode class in favor of boolean true/false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T21:03:20Z</created><updated>2016-05-20T21:22:33Z</updated><resolved>2016-05-20T21:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-20T21:07:49Z" id="220718846">LGTM.
</comment><comment author="dakrone" created="2016-05-20T21:22:33Z" id="220721709">Thanks Jason!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix env. var placeholder test so it's reproducible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18501</link><project id="" key="" /><description>This commit modifies the settings test for environment variables
placeholders so that it is reproducible. The underlying issue is that
the set of environment variables from system to system can vary, and
this means that's we can never be sure that a failing test will be
reproducible. This commit simplifies this test to not rely on external
forces that could influence reproducbility.
</description><key id="156050367">18501</key><summary>Fix env. var placeholder test so it's reproducible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>non-issue</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T21:02:24Z</created><updated>2016-05-20T21:11:46Z</updated><resolved>2016-05-20T21:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-20T21:05:00Z" id="220718255">LGTM, thanks for fixing this!
</comment><comment author="jasontedor" created="2016-05-20T21:11:46Z" id="220719610">@dakrone Thanks for the quick review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Netty request/response tracer should wait for send</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18500</link><project id="" key="" /><description>We write to Netty channels in an async fashion, but notify listeners via
a transport service adapter before we are certain that the channel write
succeeded. In particular, the tracer logs are implemented via a
transport service adapter and this means that we can write tracer logs
before a write was successful and in some cases the write might fail
leading to misleading logs. This commit attaches the transport service
adapters to channel writes as a listener so that the notification occurs
only after a successful write.
</description><key id="156042722">18500</key><summary>Netty request/response tracer should wait for send</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T20:15:43Z</created><updated>2016-05-24T21:12:23Z</updated><resolved>2016-05-20T20:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-05-20T20:20:00Z" id="220708689">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect results for nested cardinality aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18499</link><project id="" key="" /><description>**Elasticsearch version**: 
2.3.0

**JVM version**: 
1.8.0_40
1.8.0_91

**OS version**: 
Windows 10

**Description of the problem including expected versus actual behavior**:
When nesting aggregations of term cardinality (unique counts of terms in Kibana), the results are often incorrect.  Depending on the data, sometimes the result is higher than it should be, other times it is lower than it should be.  When the nesting is removed, the results for a single aggregation of term cardinality is correct.  

In the following example, the results show 8 distinct values for field3, but there are only 7 unique values for field3.  

**Steps to reproduce**:
1. Index the following data:
   `POST test/_bulk
   {"index":{"_index":"test","_type":"test"}}
   {"field1":"A","field2":"B","field3":"1"}
   {"index":{"_index":"test","_type":"test"}}
   {"field1":"A","field2":"B","field3":"2"}
   {"index":{"_index":"test","_type":"test"}}
   {"field1":"A","field2":"B","field3":"3"}
   {"index":{"_index":"test","_type":"test"}}
   {"field1":"A","field2":"B","field3":"4"}
   {"index":{"_index":"test","_type":"test"}}
   {"field1":"A","field2":"B","field3":"5"}
   {"index":{"_index":"test","_type":"test"}}
   {"field1":"A","field2":"B","field3":"6"}
   {"index":{"_index":"test","_type":"test"}}
   {"field1":"A","field2":"B","field3":"7"}`
2. Run the following aggregation query:
   `POST test/_search
   {
   "query": {
     "filtered": {
       "query": {
         "query_string": {
           "analyze_wildcard": true,
           "query": "*"
         }
       },
       "filter": {
         "bool": {
           "must": [
             {
               "query": {
                 "query_string": {
                   "analyze_wildcard": true,
                   "query": "*"
                 }
               }
             }
           ],
           "must_not": []
         }
       }
     }
   },
   "size": 0,
   "aggs": {
     "3": {
       "terms": {
         "field": "field1",
         "size": 0,
         "order": {
           "_term": "asc"
         }
       },
       "aggs": {
         "4": {
           "terms": {
             "field": "field2",
             "size": 0,
             "order": {
               "_term": "asc"
             }
           },
           "aggs": {
             "1": {
               "cardinality": {
                 "field": "field3"
               }
             }
           }
         }
       }
     }
   }
   }`

**Provide logs (if relevant)**:
Nothing in the logs.
</description><key id="156035922">18499</key><summary>Incorrect results for nested cardinality aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrisdrusso</reporter><labels /><created>2016-05-20T19:35:31Z</created><updated>2016-06-01T16:11:05Z</updated><resolved>2016-05-23T08:24:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-23T08:24:53Z" id="220916916">This is because the precision threshold depends on the number of parent aggregations. We expect to have less values in the child aggregations so the threshold is lower:
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html
Of course you can control it:
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html#_precision_control

Bottom line is that the counts are approximate to save memory and that you can control how approximate they are by setting precision_threshold.
</comment><comment author="chrisdrusso" created="2016-06-01T16:11:05Z" id="223043498">Worked perfectly.  Thank you jimferenczi.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Reindex cancellation tests more uniform</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18498</link><project id="" key="" /><description>This pull request makes all the Reindex cancellation tests more uniform. They now all inherit from the same abstract class `ReindexCancelTestCase` and follow the same behaviour.

It also fixes a small bug where the `Task` status was not updated if the task was cancelled even if the operations were proceeded.

This relates to https://github.com/elastic/elasticsearch/pull/18329#discussion_r63392207
</description><key id="156033901">18498</key><summary>Make Reindex cancellation tests more uniform</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T19:24:08Z</created><updated>2016-05-23T08:45:49Z</updated><resolved>2016-05-23T08:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-05-20T19:25:07Z" id="220696737">@nik9000 Can you have a look? Thanks
</comment><comment author="nik9000" created="2016-05-20T19:35:26Z" id="220699011">I left a one comment about squashing the tests into a single file but otherwise LGTM.
</comment><comment author="tlrx" created="2016-05-23T08:45:49Z" id="220921413">Thanks @nik9000 ! I merged all 3 tests in a `CancelTests` class.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove use of a Fields class in snapshot responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18497</link><project id="" key="" /><description>Remove use of a Fields class in snapshot responses that contains x-content keys in favor of declaring/using the keys directly.
</description><key id="156030697">18497</key><summary>Remove use of a Fields class in snapshot responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T19:06:49Z</created><updated>2016-05-20T19:12:04Z</updated><resolved>2016-05-20T19:12:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-20T19:09:26Z" id="220693209">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make java9 work again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18496</link><project id="" key="" /><description>This change makes ES compile with java9 again, build 118.
- There are a handful of changes due to failure to determine types during compile.
- The attachment plugins which use tika needed to have tika upgraded in order to pickup fixes there for java 9.
- azure discovery and s3 repository indirectly depend on jaxb, which is no longer in the default modules. They now add a jaxb dependency externally, and make JarHell allow for this package.

Note: We shouldn't need to ignore the `options xlint class, I will handle this in a follow up to use java 9's new`-release` argument (which is a better -target/-source and actually works correctly, instead of the problem we get the warning about here).
</description><key id="156026719">18496</key><summary>Make java9 work again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T18:44:55Z</created><updated>2016-05-21T22:52:00Z</updated><resolved>2016-05-21T21:22:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-20T20:07:04Z" id="220706007">LGTM in general @rjernst I think let move here and get this in. thanks for working on this
</comment><comment author="jasontedor" created="2016-05-20T20:20:01Z" id="220708699">I'm with @s1monw, let's just keep moving on this and get to a point quickly where we can get CI turned on for Java 9 now that they are nearing FC and the JDK 9 umbrella JSR draft has been released.
</comment><comment author="uschindler" created="2016-05-21T08:42:50Z" id="220766447">In general: For real cross-compiling you should no longer use javac "source/target" on Java 9 and instead use the new "release" switch. Lucene already switched to this: https://issues.apache.org/jira/browse/LUCENE-7292

This ensures real cross-compilation, because it uses the method signatures and classes of Java 8 for compile.
</comment><comment author="rmuir" created="2016-05-21T12:22:29Z" id="220774826">i'm -1 to the changes because they are not correct. I see others trying to rush this change in but i do not care.
</comment><comment author="rjernst" created="2016-05-21T16:40:03Z" id="220787698">@uschindler I plan to switch to `-release` in a followup, as I noted in the PR description.
</comment><comment author="rjernst" created="2016-05-21T16:41:59Z" id="220787799">@rmuir I removed the changes to JarHell. These were unintentional. I originally thought they were necessary, before realizing the failures I was seeing were from the thirdPartyAudit jarhell checks. But after adding all the classes to be ignored there, I never went back and removed the incorrect changes to JarHell.
</comment><comment author="rjernst" created="2016-05-21T21:21:12Z" id="220800962">I pushed some new commits which fix a log4j issue with java 9.
</comment><comment author="rmuir" created="2016-05-21T21:21:44Z" id="220800989">+1 for the current patch!
</comment><comment author="uschindler" created="2016-05-21T22:24:15Z" id="220803521">Hi,
thanks for fixing. I tested compiling painless with Gradle 2.13 using Java 9 build 118: works

FYI: There is a fix for the setAccessible problem around the "nebula something jar" (after hamster scanning for projects). The problem is a bug in Groovy 2.4.4 that was fixed in 2.4.6 (I reported it last year: https://issues.apache.org/jira/browse/GROOVY-7587). Problem is Gradle 2.13 still uses 2.4.4.

To make Gradle 2.13 working with Java 9 do the following:
- download groovy-all-2.4.6.jar
- rename it to groovy-all-2.4.4.jar (jaja, you have to do this!)
- replace the grovy-all-2.4.4.jar in $GRADLE/lib dir
</comment><comment author="uschindler" created="2016-05-21T22:52:00Z" id="220804581">@rjernst @rmuir I have an idea how to remove the useless JAXB dependency: Is far as I know, JAXB is only used for the stupid Base64 decoder/encoder. How about the following: Remove JAXB.jar and jarhell fixes. Place a 3-liner java class named javax.xml.bind.DatatypeConverter which just implements the stupid printBase64Binary and parseBase64Binary using java.util.Base64. Just add a Jarhell exception for this single class with 2 static methods. As its part of the plugin, it is isolated by classloader and may not conflict with other stuff using a real JAXB.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skip all geo point queries in plain highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18495</link><project id="" key="" /><description>Geo queries and plain highlighter do not seem to work well
together (https://issues.apache.org/jira/browse/LUCENE-7293)
so we need to skip all geo related queries when we highlight.

hopefully closes #17537

Only plain highlighter seems to be affected.
I am worried that we might need to take care of GeoPointInBBoxQueryImpl too although I do not see how a rewritten query can be used for highlighting, see TODO below. 

@nik9000 since this was your idea, maybe you want to take a look?
</description><key id="156019922">18495</key><summary>Skip all geo point queries in plain highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Highlighting</label><label>bug</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T18:09:13Z</created><updated>2016-05-31T09:29:01Z</updated><resolved>2016-05-25T09:35:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-20T18:11:56Z" id="220679139">LGTM. Highlighter is all hacks so another one won't hurt. Much.
</comment><comment author="mikemccand" created="2016-05-20T18:25:11Z" id="220682479">I left one comment, else LGTM.  Thanks @brwe!
</comment><comment author="s1monw" created="2016-05-23T18:36:54Z" id="221057030">LGTM too
</comment><comment author="brwe" created="2016-05-31T09:27:44Z" id="222637505">Backported to 2.x here https://github.com/elastic/elasticsearch/commit/d2d670d96b3cbb054f5e4160bbc96345fdeb80ba and here https://github.com/elastic/elasticsearch/commit/13c3dd8f76787fc1c94f7edd09bf27879ac83dbc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes multiple toXContent entry points for SnapshotInfo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18494</link><project id="" key="" /><description>SnapshotInfo had a toXContent and an externalToXContent, the former for
writing snapshot info to the snapshot blob and the latter for writing the
snapshot info to the APIs. This commit unifies writing x-content to one
method, toXContent, which distinguishes which format to write the
snapshot info in based on the Params parameter.  In addition, it makes
use of the already existing snapshot specific params found in the
BlobStoreFormat.
</description><key id="156008911">18494</key><summary>Removes multiple toXContent entry points for SnapshotInfo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T17:06:49Z</created><updated>2016-05-24T19:47:43Z</updated><resolved>2016-05-24T19:47:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-05-24T19:42:39Z" id="221380441">LGTM
</comment><comment author="abeyad" created="2016-05-24T19:46:01Z" id="221381308">@imotov thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow empty on_failure processor definitions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18493</link><project id="" key="" /><description>If an empty `on_failure` definition is defined it should basically ignore any processor related failures and just continue with next processor.

For example if not all documents have an `ip` field then these documents fail in ingest and are not be indexed. In many cases this isn't the desired behaviour. Adding an empty `on_failure` definition should fix this:

```
{
  "processors" : [
    ...
    {
      "geoip" : {
        "field" : "ip",
        "on_failure" : []
      }
    }
    ....
  ]
}
```

Currently an empty `on_failure` definition behaves the same as no `on_failure` definition and index requests with no `ip` fields fail.
</description><key id="155996579">18493</key><summary>Allow empty on_failure processor definitions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>adoptme</label><label>enhancement</label></labels><created>2016-05-20T16:02:37Z</created><updated>2016-06-01T08:29:57Z</updated><resolved>2016-06-01T08:29:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-20T16:45:55Z" id="220657823">I think this is really trappy. It would be better to have an explicit `ignore` value, or something like that.
</comment><comment author="bleskes" created="2016-05-27T09:11:45Z" id="222098737">+1 to make an explicit "ignore" command.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for MatchNoDocsQuery in percolator's query terms extract service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18492</link><project id="" key="" /><description>A `percolate` query that should have no matches was taking too long to execute. After some debugging I found that the 900k `match` query only contained stopwords. If a `match` query's text analysis returns no tokens it returns a MatchNoDocsQuery Lucene query. So the percolator was 'evaluating' 900k MatchNoDocsQuery queries... This PR fixes that.
</description><key id="155991633">18492</key><summary>Add support for MatchNoDocsQuery in percolator's query terms extract service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T15:38:48Z</created><updated>2016-05-22T20:48:49Z</updated><resolved>2016-05-22T20:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-20T19:25:10Z" id="220696753">LGTM.  Very small nit comment
</comment><comment author="martijnvg" created="2016-05-22T20:48:49Z" id="220855439">thx @abeyad!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unbounded queues from thread pool executors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18491</link><project id="" key="" /><description>Today, the scaling thread pool executors have unbounded queues by default, and the fixed thread pool executors can have unbounded queues if their size is set to a negative value. Unbounded queues are dangerous and should be eliminated. I think we should remove the ability for any of the thread pool executors to have unbounded queues but maybe (some of?) the scaling thread pool executors should get a caller-runs rejection policy?

Relates #14448
</description><key id="155982850">18491</key><summary>Remove unbounded queues from thread pool executors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>discuss</label><label>enhancement</label></labels><created>2016-05-20T14:59:26Z</created><updated>2016-05-27T14:33:36Z</updated><resolved>2016-05-27T14:33:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-27T13:15:15Z" id="222143147">We discussed this in Fix-it-Friday and arrived at the following proposal:
- [ ] remove custom thread pools #17915 
- [ ] register proper settings for thread pools
- [ ] remove unbounded queues #18491
- [ ] place a hard bound of 256 \* max number of threads in pool for a bound on the queue #14448 
</comment><comment author="jasontedor" created="2016-05-27T14:33:07Z" id="222162102">Closed in favor of #18613
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move all ingest processors in core to a module?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18490</link><project id="" key="" /><description>This module should only contain the common processors that have external dependencies or require additional security permissions.

Not sure what the name should be. Maybe `ingest-common` or `ingest-processors`.

The main benefit would be that during development of common ingest processors, running tests becomes much quicker. Like @nik9000 mentioned [here](https://github.com/elastic/elasticsearch/issues/17317#issuecomment-200809490) as a good reason to move the percolator to a module.
</description><key id="155978545">18490</key><summary>Move all ingest processors in core to a module?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2016-05-20T14:40:53Z</created><updated>2016-06-07T15:34:00Z</updated><resolved>2016-06-07T15:34:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-27T09:27:47Z" id="222101937">I think that's fine as long as we don't build modules for 20 lines of code? I think it's worth moving things out like this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Passthrough test logger level to nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18489</link><project id="" key="" /><description>This commit passes the system property tests.logger.level down to the
external nodes launched in integration tests. Specific tests that want
to override the default logging level should push down a setting to
the nodes using cluster configuration instead of pushing down a system
property to the nodes using cluster configuration.
</description><key id="155977704">18489</key><summary>Passthrough test logger level to nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T14:36:53Z</created><updated>2016-05-21T15:02:16Z</updated><resolved>2016-05-20T14:40:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-05-20T14:38:15Z" id="220623675">LGTM
</comment><comment author="jasontedor" created="2016-05-20T14:40:45Z" id="220624356">Thanks @jaymode.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document source.size in in the _reindex documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18488</link><project id="" key="" /><description /><key id="155977555">18488</key><summary>Document source.size in in the _reindex documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roidelapluie</reporter><labels /><created>2016-05-20T14:36:07Z</created><updated>2016-05-23T10:12:02Z</updated><resolved>2016-05-23T10:12:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roidelapluie" created="2016-05-20T14:37:05Z" id="220623348">@clintongormley This is for the 2.x branch
</comment><comment author="clintongormley" created="2016-05-23T10:12:02Z" id="220940685">This one will be handled by https://github.com/elastic/elasticsearch/pull/18340 once it is backported to 2.x

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date math not correctly rounding down to nearest week</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18487</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.1

**JVM version**: 
java version "1.8.0_77"
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

When performing a query using date math documents are returned outside my specified range.  

**Steps to reproduce**:

Create a test index: 

```
curl -X POST -d '{
  "mappings": {
    "testdoc": {
      "properties": {
        "date": {
          "type": "date",
          "format": "epoch_second" 
        }
      }
    }
  }
}' "http://localhost:9200/testindex2"
```

Add Test Documents: 

```
curl -X POST -d '{
"name": "02/05 12:00",
"date":  1462190400
}' "http://localhost:9200/testindex2/testdoc"

curl -X POST -d '{
"name": "09/05 12:00",
"date":  1462795200
}' "http://localhost:9200/testindex2/testdoc"

curl -X POST -d '{
"name": "10/05 12:00",
"date":  1462881600
}' "http://localhost:9200/testindex2/testdoc"

curl -X POST -H "Cache-Control: no-cache" -H "Postman-Token: bf19a167-d4da-c5fc-77b3-c127c697b27c" -d '{
"name": "11/05 12:00",
"date":  1462968000
}' "http://localhost:9200/testindex2/testdoc"
```

Perform Query: 

```
curl -X POST -d '{
    "size": 10,
    "query": {

                "range": {
                    "date": {
                        "from": "2016-05-20||-2w/w",
                        "to": "2016-05-20||-1w/w",
                        "format": "yyyy-MM-dd"
                    }
                }

    }

}' "http://localhost:9200/testindex2/_search"
```

Which results in: 

``` json
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 4,
    "max_score": 1,
    "hits": [
      {
        "_index": "testindex2",
        "_type": "testdoc",
        "_id": "AVTOfD2XAfCW5pJcjlpV",
        "_score": 1,
        "_source": {
          "name": "11/05 12:00",
          "date": 1462968000
        }
      },
      {
        "_index": "testindex2",
        "_type": "testdoc",
        "_id": "AVTOe8HXAfCW5pJcjlpT",
        "_score": 1,
        "_source": {
          "name": "09/05 12:00",
          "date": 1462795200
        }
      },
      {
        "_index": "testindex2",
        "_type": "testdoc",
        "_id": "AVTOfAgMAfCW5pJcjlpU",
        "_score": 1,
        "_source": {
          "name": "10/05 12:00",
          "date": 1462881600
        }
      },
      {
        "_index": "testindex2",
        "_type": "testdoc",
        "_id": "AVTOe24dAfCW5pJcjlpS",
        "_score": 1,
        "_source": {
          "name": "02/05 12:00",
          "date": 1462190400
        }
      }
    ]
  }
}
```

I expected only two documents to be returned and that the two returned should be named "02/05 12:00" and "09/05 12:00"
</description><key id="155976654">18487</key><summary>Date math not correctly rounding down to nearest week</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jamiecuthill</reporter><labels /><created>2016-05-20T14:32:17Z</created><updated>2016-05-23T10:06:06Z</updated><resolved>2016-05-23T10:06:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-23T10:06:05Z" id="220939394">This is working correctly.  `from` is inclusive, so it rounds up.  This is why we recommend using `gt`, `gte`, `lt`, `lte` instead - it is more obvious what you are doing.

Use the validate-query API to understand how your query is being rewritten:

```
POST testindex2/_validate/query?explain&amp;rewrite
{
  "query": {
    "range": {
      "date": {
        "gte": "2016-05-20||-2w/w",
        "lt": "2016-05-20||-1w/w",
        "format": "yyyy-MM-dd"
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing example in the _reindex documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18486</link><project id="" key="" /><description /><key id="155976056">18486</key><summary>Add missing example in the _reindex documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roidelapluie</reporter><labels><label>docs</label></labels><created>2016-05-20T14:29:33Z</created><updated>2016-05-23T10:10:57Z</updated><resolved>2016-05-23T10:10:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roidelapluie" created="2016-05-20T14:37:21Z" id="220623432">@clintongormley This is for the 2.3 branch
</comment><comment author="clintongormley" created="2016-05-23T10:10:57Z" id="220940453">thanks @roidelapluie 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve adding clauses to `span_near` and `span_or` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18485</link><project id="" key="" /><description>Currently the `span_near` and `span_or` query builders expose their clauses as a modifiable list. Instead we should make the the `clauses()` getter return an unmodifiable list. Also renaming the method used to add a clause from `clause(spanQuery)` to `addClause(spanQuery)`, because this created some confusion for users.

Closes #18478
</description><key id="155947074">18485</key><summary>Improve adding clauses to `span_near` and `span_or` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Java API</label><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T11:57:04Z</created><updated>2016-05-20T12:10:28Z</updated><resolved>2016-05-20T12:10:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-20T12:06:26Z" id="220588567">LGTM
</comment><comment author="cbuescher" created="2016-05-20T12:10:28Z" id="220589254">@jimferenczi thanks for the review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a note about the source size parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18484</link><project id="" key="" /><description /><key id="155941053">18484</key><summary>Add a note about the source size parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roidelapluie</reporter><labels><label>docs</label></labels><created>2016-05-20T11:16:55Z</created><updated>2016-05-20T12:40:52Z</updated><resolved>2016-05-20T12:27:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-20T11:56:07Z" id="220586828">thanks for the PR @roidelapluie - just one fix, the batch size has been changed to 1000 in https://github.com/elastic/elasticsearch/pull/18340
</comment><comment author="roidelapluie" created="2016-05-20T12:02:29Z" id="220587963">What do you think of this?
</comment><comment author="nik9000" created="2016-05-20T12:12:47Z" id="220589656">LGTM
</comment><comment author="clintongormley" created="2016-05-20T12:24:08Z" id="220591772">thanks @roidelapluie - i'll use this as a base and add different commits to 2.3, 2.x, and master
</comment><comment author="roidelapluie" created="2016-05-20T12:28:04Z" id="220592525">Thanks.
</comment><comment author="roidelapluie" created="2016-05-20T12:34:36Z" id="220593795">@clintongormley wait, this was already present in the docs. I did not see it because it is not present in 2.3 docs.

Backporting the doc part of #18340 to 2.3 looks better than this.
</comment><comment author="clintongormley" created="2016-05-20T12:37:14Z" id="220594327">ah doh!  good spot
</comment><comment author="clintongormley" created="2016-05-20T12:40:52Z" id="220595025">Done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix _only_nodes preferences</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18483</link><project id="" key="" /><description>- Handle multiple attributes/names (coma separated): _only_nodes:a,b,c
- Shuffle the nodes that match the preferences.

Fix #12546
Fix #12700
</description><key id="155939151">18483</key><summary>Fix _only_nodes preferences</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T11:05:18Z</created><updated>2016-05-23T08:03:44Z</updated><resolved>2016-05-23T08:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-20T18:35:57Z" id="220685228">LGTM thanks for fixing this
</comment><comment author="jasontedor" created="2016-05-20T19:33:35Z" id="220698601">I left a few minor comments, otherwise LGTM. Fire at will.
</comment><comment author="jimczi" created="2016-05-23T08:03:38Z" id="220912586">Thanks @jasontedor @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RestController.dispatchRequest should not stash threadcontext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18482</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/blob/v5.0.0-alpha2/core/src/main/java/org/elasticsearch/rest/RestController.java#L165   RestController.dispatchRequest stashes the current threadcontext during resthandler execution. This makes it IMHO impossible to access transient context values from a RestHandler.
</description><key id="155931820">18482</key><summary>RestController.dispatchRequest should not stash threadcontext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">salyh</reporter><labels /><created>2016-05-20T10:20:27Z</created><updated>2017-01-16T20:09:12Z</updated><resolved>2017-01-16T20:09:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-20T17:28:00Z" id="220668128">you want to access the transient context from the caller of the RestHandler? I don't really understand why since the caller is the netty worker thread and nothing sits on top of this. It the beginning of the request so I think it has to stash to make sure everything is cleaned up afterwards?
</comment><comment author="salyh" created="2016-05-31T13:31:23Z" id="222688749">I have a plugin which defines its own `HttpServerAdapter` implementation and i try to pass through some transient parameters to the rest controller. In ES 2.x this worked and i wonder why the stash does not happen in `HttpServer`?
</comment><comment author="s1monw" created="2016-05-31T13:55:38Z" id="222695725">Well that is the place where we copy over relevant headers from the http request which is nothing by default. I think what we should do is to copy headers form the context rather than the request and filter them with the `relevantHeaders` set. That way you can pass in whatever you want but you have to register that header via `RestController#registerRelevantHeaders`? What do you think? Wanna open a PR for this?
</comment><comment author="salyh" created="2016-07-06T09:28:02Z" id="230723102">Will prepare a PR
</comment><comment author="s1monw" created="2017-01-16T10:41:41Z" id="272827849">@salyh note that #22636 will remove the infrastructure you are using. it would be great to get more feedback what you are doing to make sure  we have the right extension points.</comment><comment author="s1monw" created="2017-01-16T20:09:03Z" id="272951743">#22636 removed `HttpServerAdapter` this issue is obsolet</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshotting and sync could cause a dead lock TranslogWriter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18481</link><project id="" key="" /><description> #18360 introduced an extra lock in order to allow writes while syncing the translog. This caused a potential deadlock (see [1])  with snapshotting code where we first acquire the instance lock, followed by a sync (which acquires the syncLock). However, the sync logic acquires the syncLock first, followed by the instance lock.

I considered solving this by not syncing the translog on snapshot - I think we can get away with just flushing it. That however will create subtleties around snapshoting and whether operations in them are persisted. I opted instead to have slightly uglier code with nest synchronized, where the scope of the change is contained to the TranslogWriter class alone. If people feel differently, I can take the other approach.

[1]

```
  2&gt; "elasticsearch[node_s1][index][T#1]" ID=8144 BLOCKED on org.elasticsearch.index.translog.TranslogWriter@7527aa29 owned by "elasticsearch[node_s1][generic][T#2]" ID=8079
  2&gt;    at org.elasticsearch.index.translog.TranslogWriter.syncUpTo(TranslogWriter.java:225)
  2&gt;    - blocked on org.elasticsearch.index.translog.TranslogWriter@7527aa29
  2&gt;    - locked java.lang.Object@46936c8a

  2&gt; "elasticsearch[node_s1][generic][T#2]" ID=8079 BLOCKED on java.lang.Object@46936c8a owned by "elasticsearch[node_s1][index][T#1]" ID=8144
  2&gt;    at org.elasticsearch.index.translog.TranslogWriter.syncUpTo(TranslogWriter.java:219)
  2&gt;    - blocked on java.lang.Object@46936c8a
  2&gt;    at org.elasticsearch.index.translog.TranslogWriter.sync(TranslogWriter.java:151)
  2&gt;    at org.elasticsearch.index.translog.TranslogWriter.newSnapshot(TranslogWriter.java:201)
  2&gt;    - locked org.elasticsearch.index.translog.TranslogWriter@7527aa29
```
</description><key id="155913528">18481</key><summary>Snapshotting and sync could cause a dead lock TranslogWriter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-20T08:41:31Z</created><updated>2016-05-20T10:56:25Z</updated><resolved>2016-05-20T10:56:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-20T08:41:44Z" id="220549102">@s1monw can you take a look?
</comment><comment author="jasontedor" created="2016-05-20T10:51:02Z" id="220575748">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added Type name for DFI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18480</link><project id="" key="" /><description>I hope it was not missing on purpose
</description><key id="155906734">18480</key><summary>Added Type name for DFI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eratio08</reporter><labels><label>docs</label></labels><created>2016-05-20T08:02:22Z</created><updated>2016-05-20T09:04:13Z</updated><resolved>2016-05-20T09:01:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-20T09:04:13Z" id="220554063">thanks @eratio08 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Range Query Illegal Argument Exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18479</link><project id="" key="" /><description>Elasticsearch version 5.0 Alpha-2

JVM version 1.8.0_91

OS version Windows 7

**Description of the problem including expected versus actual behavior**:
Lucene throws the following error when running the Date Range query below. Files to reproduce are attached.

IllegalArgumentException[field="inceptionDate" is unrecognized];
Caused by: java.lang.IllegalArgumentException: field="inceptionDate" is unrecognized
at org.apache.lucene.codecs.lucene60.Lucene60PointsReader.getBKDReader(Lucene60PointsReader.java:120).  
1. Schema included in mapping.txt file
2. ES dump included in dump.txt
3. stack trace in log.txt
   {
   "query" : {
     "range" : {
   [log.txt](https://github.com/elastic/elasticsearch/files/273652/log.txt)
   [mapping.txt](https://github.com/elastic/elasticsearch/files/273651/mapping.txt)
   [dump.txt](https://github.com/elastic/elasticsearch/files/273653/dump.txt)
   
     "inceptionDate" : {
       "from" : "10/1/1980",
       "to" : "12/31/2015",
       "include_lower" : true,
       "include_upper" : true,
       "boost" : 1.0
     }
   }
   },
   "fields" : [ "orgType", "inceptionDate", "name", "id", "qualityRating", "version", "parentId", "descp" ]
   }
   **Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="155860596">18479</key><summary>Date Range Query Illegal Argument Exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cashel1</reporter><labels><label>:Mapping</label><label>bug</label><label>feedback_needed</label></labels><created>2016-05-19T23:53:46Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2017-03-31T13:22:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-20T08:54:50Z" id="220551986">I'm unable to replicate this with the following:

```
PUT security1
{
  "mappings": {
    "Org": {
      "dynamic": "false",
      "properties": {
        "certOrgId": {
          "type": "text"
        },
        "descp": {
          "type": "text"
        },
        "effectiveFrom": {
          "type": "date"
        },
        "effectiveThru": {
          "type": "date"
        },
        "externalId": {
          "type": "text"
        },
        "id": {
          "type": "text"
        },
        "inceptionDate": {
          "type": "date"
        },
        "indexDate": {
          "type": "date"
        },
        "latLon": {
          "type": "geo_point"
        },
        "name": {
          "type": "text"
        },
        "networkId": {
          "type": "text"
        },
        "orgType": {
          "type": "text"
        },
        "parentId": {
          "type": "text"
        },
        "qualityRating": {
          "type": "integer"
        },
        "routeId": {
          "type": "text"
        },
        "searchRoutes": {
          "type": "text"
        },
        "type": {
          "type": "text"
        },
        "version": {
          "type": "long"
        }
      }
    }
  }
}

PUT security1/Org/_bulk
{"index":{"_id":"a"}}
{"name":"Acme","parentId":null,"externalId":null,"networkId":"NC Public Cloud","inceptionDate":1422684000000,"certOrgId":null,"orgType":"Parent","geoHash":"9rv25grh5tgu","routeId":"R2","searchRoutes":"R2,R3,R4,R5","descp":"Acme Health System","qualityRating":4,"id":"a","type":null,"version":0,"indexDate":1463695280759,"effectiveFrom":1463695280759,"effectiveThru":1463695280759}
{"index":{"_id":"ae"}}
{"name":"Acme East","parentId":"a","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1422684000000,"certOrgId":null,"orgType":"Region","geoHash":"dr782k465uvc","routeId":"R2","searchRoutes":"R4,R5","descp":"Acme Eastern Region","qualityRating":3,"id":"ae","type":null,"version":0,"indexDate":1463695280847,"effectiveFrom":1463695280847,"effectiveThru":1463695280847}
{"index":{"_id":"aw"}}
{"name":"Acme West","parentId":"a","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1425103200000,"certOrgId":null,"orgType":"Region","geoHash":"9rv25grh5tgu","routeId":"R2","searchRoutes":"R2,R3","descp":"Acme Western Region","qualityRating":4,"id":"aw","type":null,"version":0,"indexDate":1463695280904,"effectiveFrom":1463695280904,"effectiveThru":1463695280904}
{"index":{"_id":"aae"}}
{"name":"Acme Acute East","parentId":"ae","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1425103200000,"certOrgId":null,"orgType":"Division","geoHash":"dr782k465uvc","routeId":"R2","searchRoutes":"R4","descp":"Acme Acute - Eastern Region","qualityRating":3,"id":"aae","type":null,"version":0,"indexDate":1463695280962,"effectiveFrom":1463695280962,"effectiveThru":1463695280962}
{"index":{"_id":"abe"}}
{"name":"Acme Behav East","parentId":"ae","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1427778000000,"certOrgId":null,"orgType":"Division","geoHash":"dr782k465uvc","routeId":"R2","searchRoutes":"R5","descp":"Acme Behavioral - Eastern Region","qualityRating":3,"id":"abe","type":null,"version":0,"indexDate":1463695281111,"effectiveFrom":1463695281111,"effectiveThru":1463695281111}
{"index":{"_id":"aaw"}}
{"name":"Acme Acute West","parentId":"aw","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1427778000000,"certOrgId":null,"orgType":"Division","geoHash":"9rv25grh5tgu","routeId":"R2","searchRoutes":"R2","descp":"Acme Acute - Western Region","qualityRating":5,"id":"aaw","type":null,"version":0,"indexDate":1463695281242,"effectiveFrom":1463695281242,"effectiveThru":1463695281242}
{"index":{"_id":"abw"}}
{"name":"Acme Behav West","parentId":"aw","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1427778000000,"certOrgId":null,"orgType":"Division","geoHash":"9rv25grh5tgu","routeId":"R2","searchRoutes":"R3","descp":"Acme Behavioral - Western Region","qualityRating":4,"id":"abw","type":null,"version":0,"indexDate":1463695281308,"effectiveFrom":1463695281308,"effectiveThru":1463695281308}
{"index":{"_id":"boi"}}
{"name":"Boise","parentId":"aaw","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1427778000000,"certOrgId":null,"orgType":"Hospital","geoHash":"9rv25grh5tgu","routeId":"R3","searchRoutes":"","descp":"Boise Acute Care","qualityRating":4,"id":"boi","type":null,"version":0,"indexDate":1463695281366,"effectiveFrom":1463695281366,"effectiveThru":1463695281366}
{"index":{"_id":"por"}}
{"name":"Portland","parentId":"abw","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1430370000000,"certOrgId":null,"orgType":"Hospital","geoHash":"c20dyhxgpq5w","routeId":"R4","searchRoutes":"","descp":"Portlandia Behavioral System","qualityRating":2,"id":"por","type":null,"version":0,"indexDate":1463695281424,"effectiveFrom":1463695281424,"effectiveThru":1463695281424}
{"index":{"_id":"ny"}}
{"name":"NewYork","parentId":"aae","externalId":null,"networkId":"NC Public Cloud","inceptionDate":1430370000000,"certOrgId":null,"orgType":"Hospital","geoHash":"dr782k465uvc","routeId":"R5","searchRoutes":"","descp":"New York Community Care","qualityRating":3,"id":"ny","type":null,"version":0,"indexDate":1463695281481,"effectiveFrom":1463695281481,"effectiveThru":1463695281481}

GET security1/_search
{
  "query": {
    "range": {
      "inceptionDate": {
        "from": "1980-10-01",
        "to": "2015-12-31",
        "include_lower": true,
        "include_upper": true,
        "boost": 1
      }
    }
  }
}
```

That said, i think this is a duplicate of https://github.com/elastic/elasticsearch/issues/18010 which has been fixed in master
</comment><comment author="clintongormley" created="2016-05-20T08:55:19Z" id="220552100">Any chance you can build from master to check? Or send me a recreation that fails on alpha2?
</comment><comment author="cashel1" created="2016-05-20T17:44:44Z" id="220672337">Can't get it to fail.  I'll try the master approach.  Thanks
</comment><comment author="jpountz" created="2016-05-23T11:57:01Z" id="220960301">@clintongormley Probably a duplicate of #18010 indeed.
</comment><comment author="cbuescher" created="2017-03-31T13:12:56Z" id="290708244">@jpountz Given that this isn't reproducible any more and #18010 was likely the cause, should we close this?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multiple Clauses in Span Near Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18478</link><project id="" key="" /><description>Elasticsearch 5.0 Alpha2

JVM version 1.8.0_91

OS Version Windows 7

Multiple clause addition in a 'span near query' produces an  java.lang.IndexOutOfBoundsException: Index: 3, Size: 1
The 'span or query' does not produce the problem.  

**Steps to reproduce**:
 SpanNearQueryBuilder qb = QueryBuilders.spanNearQuery(spanTermQuery("name","jack"), 1F);
 qb.clauses().add(1F, spanTermQuery("name", "jill"));

 qb = QueryBuilders.spanOrQuery(spanTermQuery("name", "jack"));
 qb.clauses().add(spanTermQuery(atb.getName(), termVal));
</description><key id="155857698">18478</key><summary>Multiple Clauses in Span Near Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cashel1</reporter><labels><label>:Java API</label><label>:Query DSL</label></labels><created>2016-05-19T23:28:30Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-05-20T12:10:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-20T08:42:29Z" id="220549269">@cbuescher could you take a look at this please?
</comment><comment author="cbuescher" created="2016-05-20T10:37:56Z" id="220573417">@cashel1 thanks for reporting, the problem seems to be that you are trying to add a new clause directly to the current list of clauses at a specific index (using javas List#add(int index, E element). This syntax will produce IndexOutOfBoundsException if you are trying to add element outside of the current lists length. Instead you should add new clauses using `SpanNearQueryBuilder clause(SpanQueryBuilder clause)`.

As a side note, I couldn't reproduce with your code snippet above, this doesn't seem to be using the Java-API since those methods wouldn't accept floats (`1F`) as argument for slop and list index. Are you using another client by any chance?

As far as I can see this is not a bug in ES but rather wrong usage of the API. We should not allow adding directly to the clauses list obtained by `SpanNearQueryBuilder#clauses()` but rather make that one a read-only unmodifiable list. Also we should probably add documentation and rename `clause(SpanQueryBuilder clause)` to `addClause(SpanQueryBuilder clause)` to avoid future confusions like this.
</comment><comment author="cashel1" created="2016-05-20T16:20:03Z" id="220651572">Thanks, on the slop, this was a lazy attempt on my part to eliminate the variable from the post.    
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not decode path when sending error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18477</link><project id="" key="" /><description>Today when sending a REST error to a client, we send the decoded
path. But decoding that path can already be the cause of the error in
which case decoding it again will just throw an exception leading to us
never sending an error back to the client. It would be better to send
the entire raw path to the client and that is what we do in this commit.

Closes #18476
</description><key id="155812554">18477</key><summary>Do not decode path when sending error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T19:10:56Z</created><updated>2016-05-20T16:15:38Z</updated><resolved>2016-05-20T16:15:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-19T20:04:57Z" id="220436746">LGTM but can we add a test for this that breaks with the old behavior so it doesn't get changed by on accident?
</comment><comment author="jasontedor" created="2016-05-19T20:26:46Z" id="220442204">@dakrone I pushed a test 998a93dbdb1c16c2a4ab2af7a937d737ffd55d04.
</comment><comment author="dakrone" created="2016-05-19T20:40:21Z" id="220445567">still looks good, thanks for adding the test!
</comment><comment author="jasontedor" created="2016-05-20T03:37:02Z" id="220509609">&gt; still looks good, thanks for adding the test!

@dakrone There was an issue with the previous test. Namely, since it constructed an anonymous class that inherited from `ElasticsearchException`, it violated an assumption in another test suite that all classes inheriting from `ElasticsearchException` are registered. Of course, we shouldn't register a silly class used for tests. I've modified the test to not need such an anonymous class. Would you mind taking another look before I merge (`gradle check` passes now)? &#128519; 
</comment><comment author="dakrone" created="2016-05-20T15:48:07Z" id="220643223">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest URL path parsing fails with IAE when trying to send an error response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18476</link><project id="" key="" /><description>**Elasticsearch version**: master @ c257e2c51f235853c4453a86e10e463813140fc9

**JVM version**:  Oracle Corporation 1.8.0_91 [OpenJDK 64-Bit Server VM 25.91-b14]

**OS version**: Linux 4.4.9-300.fc23.x86_64 (amd64)

**Description of the problem including expected versus actual behavior**:
ES fails to respond with an error for an invalid URL but doesn't disconnect the client.

The REST layer should fail with an appropriate error and close the connection immediately.

**Steps to reproduce**:
`curl 'localhost:9200/%a`'

**Provide logs (if relevant)**:

```
[elasticsearch] [2016-05-19 14:35:52,468][ERROR][rest                     ] [Nuke - Squadron Supreme Member] failed to send failure response for uri [/%a]
[elasticsearch] java.lang.IllegalArgumentException: partial escape sequence at end of string: /%a
[elasticsearch]     at org.elasticsearch.rest.support.RestUtils.decode(RestUtils.java:182)
[elasticsearch]     at org.elasticsearch.rest.support.RestUtils.decodeComponent(RestUtils.java:143)
[elasticsearch]     at org.elasticsearch.rest.support.RestUtils.decodeComponent(RestUtils.java:107)
[elasticsearch]     at org.elasticsearch.rest.RestRequest.path(RestRequest.java:62)
[elasticsearch]     at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:126)
[elasticsearch]     at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:90)
[elasticsearch]     at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:81)
[elasticsearch]     at org.elasticsearch.rest.RestController.sendErrorResponse(RestController.java:184)
[elasticsearch]     at org.elasticsearch.http.HttpServer.dispatchRequest(HttpServer.java:116)
[elasticsearch]     at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:490)
[elasticsearch]     at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:65)
[elasticsearch]     at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
[elasticsearch]     at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
[elasticsearch]     at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
[elasticsearch]     at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:85)

```
</description><key id="155809785">18476</key><summary>Rest URL path parsing fails with IAE when trying to send an error response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">camilojd</reporter><labels><label>:REST</label><label>bug</label></labels><created>2016-05-19T18:57:24Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-05-20T16:15:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-19T19:13:59Z" id="220424088">Thanks for reporting. I opened #18477.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement tests similar to bats tests for windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18475</link><project id="" key="" /><description>We test basic installing and options in our [vagrant tests](https://github.com/elastic/elasticsearch/tree/master/qa/vagrant/src/test/resources/packaging/scripts) on [some systems](https://github.com/elastic/elasticsearch/blob/master/Vagrantfile#L24) but so far not on windows. We need that though because every now and then we break elasticserch for windows and then only find out via bug reports (see for example https://github.com/elastic/elasticsearch/pull/18473, https://github.com/elastic/elasticsearch/issues/16086). We need something similar to the bats tests for windows. It might not be possible to provide a public windows image but at least there needs to be a way to test basic functionality for people that have access to one.
</description><key id="155809065">18475</key><summary>Implement tests similar to bats tests for windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2016-05-19T18:53:58Z</created><updated>2017-01-12T10:46:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2016-05-20T10:07:30Z" id="220567411">cc @elastic/microsoft it'd be good if we take ownership of this one. We need to write similar vagrant tests for our msi in the coming weeks too.
</comment><comment author="StefanScherer" created="2016-06-02T20:10:54Z" id="223407290">I did some Cucumber tests running against Vagrant boxes with the vagrant-cucumber plugin https://github.com/scalefactory/vagrant-cucumber

With linked clone support in Vagrant 1.8 it should be really fast to reset the VM between each test to have a clean base environment to install MSI package etc. again and again.

Here is a sample for a Linux box: https://github.com/StefanScherer/cups-cucumber-test/blob/master/features/lpr.feature

And this works for Windows VM's as well. I've some (sorry closed source) Windows environments running with VirtualBox Vagrant Boxes.

Just my two cent.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>pkg remove fails: bogus prerm script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18474</link><project id="" key="" /><description>**Elasticsearch version**:
2.3.0

**OS version**:
Ubuntu 15.04

**Description of the problem including expected versus actual behavior**:
`apt-get purge elasticsearch` fails because the used `prerm` script is bogus: The pure existence of the `systemctl` does not imply, that any message bus/systemd instance is installed or even running! So that's clearly a bug - if the script wanna use a certain service, it should make sure, that the service is up and running - in case of systemd `[[ -d /run/systemd/system ]]` might be a cheap test - but I'm not sure (using systemd) - please read the related documentation!

**Steps to reproduce**:
Use a ubuntu systemd "in theory capable" distro like vivid or wily, make it use upstart, i.e. install upstart,upstart-sysv and purge systemd-sysv packages, (perhaps reboot to see that upstart works wrt. to its parameters) and finally run `apt-get purge elasticsearch`.

**Provide logs (if relevant)**:

```
&gt; + apt-get purge elasticsearch
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be REMOVED:
  elasticsearch*
0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.
1 not fully installed or removed.
After this operation, 30.8 MB disk space will be freed.
Do you want to continue? [Y/n] 
(Reading database ... 35831 files and directories currently installed.)
Removing elasticsearch (2.3.0) ...
Stopping elasticsearch service...Failed to get D-Bus connection: Operation not permitted
dpkg: error processing package elasticsearch (--purge):
 subprocess installed pre-removal script returned error exit status 1
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service
Errors were encountered while processing:
 elasticsearch
E: Sub-process /usr/bin/dpkg returned an error code (1)
```

And as you see, even the suggested actions are plain wrong!
</description><key id="155808809">18474</key><summary>pkg remove fails: bogus prerm script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jelmd</reporter><labels /><created>2016-05-19T18:52:42Z</created><updated>2016-05-20T08:57:44Z</updated><resolved>2016-05-19T18:58:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-19T18:58:35Z" id="220420224">We do not support upstart, so uncleanly switching away from one of the two supported init daemons (systemd or System V init) is of course going to break that assumption. We do have tests that removal on these two init systems works.
</comment><comment author="jelmd" created="2016-05-19T20:27:44Z" id="220442451">Nope - you are plain wrong! Just installing upstart packages does **NOT** mean, the traditional init.d stuff isn't working anymore, it just adjusts the systemd crap, so that it doesn't any harm to the system anymore and makes the traditional init.d stuff beside upstart work.

And having a look at the mentioned script should have given you a clear hint, that the bug has nothing to do with upstart!
</comment><comment author="jasontedor" created="2016-05-19T21:11:45Z" id="220453500">By installing upstart and _not_ removing systemd, you are violating the assumption that we make that only one of systemd or System V init is installed on the system. This is not an unreasonable assumption. If you remove systemd, the script is fine.

``` bash
$ sudo apt-get -y install upstart upstart-sysv &gt; /dev/null
$ sudo shutdown -r now
[ after reboot ]
$ sudo apt-get purge -y systemd &gt; /dev/null
$ sudo apt-get install -y elasticsearch &gt; /dev/null
$ sudo apt-cache policy elasticsearch | grep Installed
  Installed: 2.3.3
$ sudo apt-get purge elasticsearch
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages were automatically installed and are no longer required:
  cgmanager systemd-shim
Use 'apt-get autoremove' to remove them.
The following packages will be REMOVED:
  elasticsearch*
0 upgraded, 0 newly installed, 1 to remove and 97 not upgraded.
After this operation, 30.8 MB disk space will be freed.
Do you want to continue? [Y/n] y
(Reading database ... 63497 files and directories currently installed.)
Removing elasticsearch (2.3.3) ...
Stopping elasticsearch service... * Stopping Elasticsearch Server
   ...done.
 OK
Purging configuration files for elasticsearch (2.3.3) ...
$ echo $?
0
```
</comment><comment author="jelmd" created="2016-05-19T22:35:12Z" id="220471055">That's what I'm saying, your assumption is plain wrong, buggy!
You can't say, the car is red and thus the driver of it is a firefighter, always an every time!
You can't say, just because there is a /bin/ls, the OS is Linux.
You can't say, just because there is a /bin/mysqld, a MySQL DB is running on the system.
You need more examples?
 Again, what the script assumes/deduces, is _plain wrong_ and needs to be fixed!
</comment><comment author="jasontedor" created="2016-05-19T22:49:25Z" id="220473392">Your analogies are poor. You didn't cleanly remove systemd, that is the mistake that is violating a reasonable assumption that only one of systemd or System V init is installed.
</comment><comment author="jelmd" created="2016-05-19T23:29:10Z" id="220479652">Pardon? I don't want to remove the systemd package at all! And your conclusion, that systemd wasn't cleanly removed, is wrong again. And again, your assumption is plain wrong! Don't you have an expert, which is able to understand this issue?
</comment><comment author="jasontedor" created="2016-05-20T01:44:51Z" id="220497349">I've tried to explain to you what we support, and the assumptions that we make. You're falling outside of that, and if you choose to say outside of that then that is your prerogative.

I like helping our users, I know that is true of the vast majority of our community, I've tried helping you and I would still like to help you, but you're making it difficult.

I take this very seriously, and despite your poor choice of language, I've read through various [sources](http://unix.stackexchange.com/questions/196166/) [tonight](http://unix.stackexchange.com/questions/18209/detect-init-system-using-the-shell) and I've been thinking about the problem. I remain unconvinced that we should do anything differently than we are doing. Having both systemd and upstart installed so that it is compatible with System V init is just not common enough of a problem that we are going to engineer around it. And we are not going to start supporting upstart, [that ship has sailed](http://www.markshuttleworth.com/archives/1316).

&gt; Don't you have an expert, which is able to understand this issue?

Most communities, especially open communities and _especially_ open-source communities, are funded in &#10084;&#65039; not &#128545;. You're more likely to be heard if you're constructive, which you haven't been. _Hugs_.
</comment><comment author="jelmd" created="2016-05-20T03:49:26Z" id="220510858">No, what you have done is to defend the buggy implementation by repeating lame non-sense, right from the start and being disrespectful by closing this issue without waiting for an answer. You also have shown with every comment incl. with the last one, that you obviously have no clue from what you are talking about and that improving the software, reading documentation, learning new things or to investigate a little bit deeper, even so hints were already given!!!, is not your intention at all.

Sorry, but I didn't expect to have to discuss such a trivial issue with a windows user, who is obviously not qualified to work on it and thus to waste a lot of my valuable time. So you can be sure, I'll not make this mistake again ...
</comment><comment author="clintongormley" created="2016-05-20T08:57:44Z" id="220552678">&gt; So you can be sure, I'll not make this mistake again ...

Appreciated
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter client/server VM options from jvm.options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18473</link><project id="" key="" /><description>The default jvm.options file ships with the -server flag to force the
server VM on systems where the server VM is not the default. However,
the method of starting the JVM via the Windows service does not support
the command-line flags for selecting the VM because it starts from a
DLL that is specific to the server or client VM. Thus, we need to
filter these options from the jvm.options configuration file when
installing the Windows service.
</description><key id="155805453">18473</key><summary>Filter client/server VM options from jvm.options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T18:36:36Z</created><updated>2016-05-20T19:25:45Z</updated><resolved>2016-05-20T19:25:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-20T16:58:55Z" id="220660998">LGTM really wish we had a test for this (not this PRs fault)
</comment><comment author="jasontedor" created="2016-05-20T19:25:18Z" id="220696784">&gt; LGTM

Thanks for reviewing.

&gt; really wish we had a test for this (not this PRs fault)

Agree. Issue #18475 was opened for this reason. :smile:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Help Aggregations Elasticsearch Curl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18472</link><project id="" key="" /><description>Hi,

I need to build the query below in ElasticSearch but I can not. Can someone help me? I need to use curl or other linux tool.

**Elasticsearch version**: 2.3

![image](https://cloud.githubusercontent.com/assets/19476780/15404635/cf05ae8e-1dd4-11e6-913f-d84e06864328.png)
</description><key id="155802007">18472</key><summary>Help Aggregations Elasticsearch Curl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gleberrl</reporter><labels /><created>2016-05-19T18:19:31Z</created><updated>2016-05-19T18:38:45Z</updated><resolved>2016-05-19T18:38:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-19T18:38:45Z" id="220414866">Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the preserve_original option from the FingerprintAnalyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18471</link><project id="" key="" /><description>The `preserve_original` option to the ASCIIFoldingFilter doesn't play well with the FingerprintFilter, as it ends up producing fingerprints like:

```
"and consistent godel g&#246;del is said sentence this yes"
```

The goal of the OpenRefine algorithm is to product a small normalized ASCII fingerprint. There's no need to expose `preserve_original`.
</description><key id="155791520">18471</key><summary>Remove the preserve_original option from the FingerprintAnalyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T17:26:29Z</created><updated>2016-05-19T17:37:13Z</updated><resolved>2016-05-19T17:37:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-19T17:26:57Z" id="220394758">Marking it as non-issue as this hasn't been released yet
</comment><comment author="polyfractal" created="2016-05-19T17:35:03Z" id="220397029">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up named queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18470</link><project id="" key="" /><description>Named queries have a performance bug when they are used with expensive queries
that need to perform a lot of work up-front like fuzzy or range queries
(including with points). The reason is that they currently re-create the weight
and scorer for every hit. Instead we should create weights exactly once and
use a single Scorer for all documents that are on the same segment.
</description><key id="155786735">18470</key><summary>Speed up named queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T17:01:55Z</created><updated>2016-05-23T08:32:34Z</updated><resolved>2016-05-23T07:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-05-19T21:29:13Z" id="220457486">Nice speedup! LGTM
</comment><comment author="s1monw" created="2016-05-20T07:09:00Z" id="220533578">left some minors but LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the Delete-By-Query plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18469</link><project id="" key="" /><description>Now the Delete-By-Query feature has been re implemented in core (in the reindex module actually) we must decide what to do with the plugin we created for 2.0.

I'm in favor of removing it completely for 5.0 and document it as a breaking change but one might have a different opinion.

As @nik9000 commented in https://github.com/elastic/elasticsearch/pull/18329#issuecomment-220160894 :

&gt; Remove the delete-by-query plugin. Should we just remove it in 5.0.0, call it a breaking change, and tell people to use the version in the reindex module? Is that ok because we're kind of just moving the API? It is losing some small functionality (timeouts) but it is gaining cancel, throttle, and rethrottle.
</description><key id="155778599">18469</key><summary>Remove the Delete-By-Query plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T16:22:12Z</created><updated>2016-05-25T11:19:09Z</updated><resolved>2016-05-24T11:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-19T16:32:50Z" id="220380308">+1
</comment><comment author="clintongormley" created="2016-05-20T08:30:10Z" id="220546660">Remove!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use default methods in Client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18468</link><project id="" key="" /><description>Use default methods in Client instead of dumb overrides in AbstractClient.
If there is interest in this PR, I can do the same for ClusterAdminClient and IndicesAdminClient.

This would remove a lot of code from AbstractClient, that was close to 1800 lines and move this code to several interfaces.
Also, it also may become easier to implement Client because you would need to implement a lot less methods
</description><key id="155778102">18468</key><summary>Use default methods in Client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">obourgain</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-05-19T16:19:47Z</created><updated>2016-07-01T17:02:58Z</updated><resolved>2016-05-23T17:53:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-19T17:14:07Z" id="220391312">I'm not sure if we should do this, this assumes behavior on all `Client` implementations (which exist outside of core elasticsearch). `AbstractClient`, though, is exactly for those that do want the behavior here.

I think of `default` as "it's okay if implementors do not implement this method because this default implementation should work for all of them if they maintain the invariants assumed by this interface" but I don't think that we can safely say that that is the case here.
</comment><comment author="javanna" created="2016-05-23T14:11:07Z" id="220991315">Is there anywhere a `Client` implementation that doesn't subclass `AbstractClient` as well? I cannot think of any, which makes me think this change is ok. Probably if java had default methods in the first place we would have used them from the beginning?
</comment><comment author="jasontedor" created="2016-05-23T14:18:24Z" id="220993397">@javanna There exist [implementations](https://github.com/jprante/elasticsearch-server/blob/7a1e5da71b36ae2bc50eef9c7a34707300466155/elasticsearch-server-node/src/main/java/org/elasticsearch/client/support/DecoratingClient.java) outside of core Elasticsearch. I  think this is the wrong use of `default`.
</comment><comment author="obourgain" created="2016-05-23T14:29:13Z" id="220996400">I am still not sure my PR is of interest, because on one hand, the current status makes it mandatory for third party implementations to do new versions to follow ES versions adding/removing method, with the risk of fragmenting the ecosystem if some people are stuck with older ES version or with forked and not maintained libs. On the other hand, default methods may cause those implementations to miss new methods 

If you think it is a bad idea, feel free to close the PR.
</comment><comment author="jasontedor" created="2016-05-23T14:35:24Z" id="220998150">&gt; On the other hand, default methods may cause those implementations to miss new methods

Yes, that is exactly my concern which I tried to express in my first [comment](https://github.com/elastic/elasticsearch/pull/18468#issuecomment-220391312).

&gt; If you think it is a bad idea, feel free to close the PR.

I do not think that it's a bad idea, just one that requires care and caution. We truly welcome the PRs, please keep them coming! :smile:
</comment><comment author="javanna" created="2016-05-23T14:46:26Z" id="221001471">thanks for the link @jasontedor I was not aware of that implementation.
</comment><comment author="khatchad" created="2016-06-30T15:26:47Z" id="229694522">@javanna Out of curiosity, did you perform this refactoring manually or via an automated refactoring tool?
</comment><comment author="obourgain" created="2016-06-30T16:30:45Z" id="229713507">I did it by hand, with a few shortcuts from the IDE
</comment><comment author="khatchad" created="2016-07-01T01:37:00Z" id="229833433">Thanks, @javanna!

On Thu, Jun 30, 2016 at 09:31:48AM -0700, Olivier Bourgain wrote:

&gt; I did it by hand, with a few shortcuts from the IDE
&gt; Le 30 juin 2016 5:31 PM, "Raffi Khatchadourian" notifications@github.com
&gt; a &#233;crit :
&gt; 
&gt; &gt; @javanna https://github.com/javanna Out of curiosity, did you perform
&gt; &gt; this refactoring manually or via an automated refactoring tool?
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; You are receiving this because you authored the thread.
&gt; &gt; Reply to this email directly, view it on GitHub
&gt; &gt; https://github.com/elastic/elasticsearch/pull/18468#issuecomment-229694522,
&gt; &gt; or mute the thread
&gt; &gt; &lt;https://github.com/notifications/unsubscribe/
&gt; &gt; AA7j0rUjdvRZxfnDOlCfpAxcbmuoUGIPks5qQ-B3gaJpZM4IibpG&gt;
&gt; &gt; .
&gt; 
&gt; &#8212;
&gt; You are receiving this because you commented.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.*

## 

Raffi Khatchadourian, PhD
Assistant Professor and Open Educational Resources (OER) Fellow
Computer Systems Technology
New York City College of Technology
City University of New York
300 Jay St., Room N913
Brooklyn, NY 11201-1909
United States
+1 (718) 260-5325
rkhatchadourian@citytech.cuny.edu
http://cuny.is/khatchad
</comment><comment author="obourgain" created="2016-07-01T08:11:55Z" id="229884365">i am not @javanna ;)
</comment><comment author="khatchad" created="2016-07-01T17:02:57Z" id="229998027">@obourgain Oh sorry about that! The mentions are a bit difficult to do via email.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit retries of failed allocations per index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18467</link><project id="" key="" /><description>Today if a shard fails during initialization phase due to misconfiguration, broken disks,
missing analyzers, not installed plugins etc. elasticsaerch keeps on trying to initialize
or rather allocate that shard. Yet, in the worst case scenario this ends in an endless
allocation loop. To prevent this loop and all it's sideeffects like spamming log files over
and over again this commit adds an allocation decider that stops allocating a shard that
failed more than N times in a row to allocate. The number or retries can be configured via
`index.allocation.max_retry` and it's default is set to `5`. Once the setting is updated
shards with less failures than the number set per index will be allowed to allocate again.

Internally we maintain a counter on the UnassignedInfo that is reset to `0` once the shards
has been started.

Relates to #18417
</description><key id="155765302">18467</key><summary>Limit retries of failed allocations per index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T15:26:07Z</created><updated>2016-05-20T18:37:45Z</updated><resolved>2016-05-20T18:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-19T15:27:21Z" id="220360279">@ywelsch can you take a look? 
@clintongormley what do you think how should we document that and where?
</comment><comment author="dakrone" created="2016-05-19T15:43:24Z" id="220365449">I know you didn't ask for my review, but I left some comments regardless :)
</comment><comment author="bleskes" created="2016-05-19T15:59:16Z" id="220370445">LGTM. Thx @s1monw 
</comment><comment author="ywelsch" created="2016-05-19T16:12:07Z" id="220374232">Left minor comments but LGTM o.w. 
For the docs, I wonder if we should put some words on how a sysadmin is supposed to get this shard assigned again after fixing the issue? Closing the index and reopening will work. Is that what we would recommend?
</comment><comment author="clintongormley" created="2016-05-20T08:28:54Z" id="220546403">@ywelsch Retrying allocation could be triggered by raising the value of `index.allocation.max_retry`, but my preference would be to have it obey the same override flag that is being added in #18321.  That makes it more consistent.
</comment><comment author="s1monw" created="2016-05-20T09:09:33Z" id="220555182">@clintongormley @ywelsch @bleskes I pushed a new commit that adds a `retry_failed` flag to the reroute API. This is also explained in the allocation explain output and in the documentation. I think we are ready here but please take another look
</comment><comment author="ywelsch" created="2016-05-20T10:06:25Z" id="220567219">Found a small issue. LGTM after fixing this. Thanks @s1monw!
</comment><comment author="bleskes" created="2016-05-20T10:55:55Z" id="220576643">LGTM2 . I wonder how we can rest test this. It's tricky and I'm not sure it's worth it to have a simple call with true to retry_after. I'm good with pushing as is - unless someone has a good idea.
</comment><comment author="s1monw" created="2016-05-20T18:28:00Z" id="220683204">@bleskes I thought about it and I think the major problem is to wait for state. I think we should rather try to unittest this so I added a unittest for serialization (found a bug) and for the master side of things on the reroute command. I think we are ready, will push soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Makes DeleteByQueryRequest implements IndicesRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18466</link><project id="" key="" /><description>@nik9000 Can you please have a look? Thanks
</description><key id="155762912">18466</key><summary>Makes DeleteByQueryRequest implements IndicesRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T15:16:34Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-25T07:37:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-20T11:35:45Z" id="220583256">Left a small comment, otherwise LGTM.
</comment><comment author="javanna" created="2016-05-24T14:47:11Z" id="221294919">LGTM @tlrx would it make sense to add a new test to IndicesRequestTests?
</comment><comment author="tlrx" created="2016-05-25T07:38:00Z" id="221496482">Thanks @javanna. I think it make sense, I'll give it a try in a follow up PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve random DateTimeZone creation in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18465</link><project id="" key="" /><description>We often require a random joda DateTimeZone in our tests. Currently there are a few options for generating such a random DateTimeZone from the set of available ids. Currently most random picks are not really reproducable across different jvms because they rely on order in the ids set implementation. 
The helper in DateProcessorFactoryTests already performs a sort on the set of ids before random picking from the result, so I moved this to ESTestCase to make it publicly available and changed all other tests to use that method.
</description><key id="155759901">18465</key><summary>Improve random DateTimeZone creation in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T15:04:43Z</created><updated>2016-05-26T11:42:03Z</updated><resolved>2016-05-19T16:13:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-19T15:53:32Z" id="220368652">LGTM
</comment><comment author="cbuescher" created="2016-05-19T16:13:44Z" id="220374698">@jpountz thanks for the review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce MatchQueryBuilder#maxExpansions() to be strictly positive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18464</link><project id="" key="" /><description>As a small follow-up to e2691d7, we should make sure that MatchQueryBuilder#maxExpansions(0) already throws an exception.
Also shortening a few of the illegal values tests.
</description><key id="155740914">18464</key><summary>Enforce MatchQueryBuilder#maxExpansions() to be strictly positive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T13:47:33Z</created><updated>2016-05-19T15:00:34Z</updated><resolved>2016-05-19T15:00:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-05-19T13:51:31Z" id="220330183">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Definition cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18463</link><project id="" key="" /><description>Painless creates its own class hierarchy backed by java classes and methods. But this is all in code and has become cumbersome (i dont want to add more methods in the current state because of it).

So, it would be good to move this definition to a file, where its easier to maintain. Also all the information about a class should be together.

This moves it out to a file with a simple format, easy to parse with low-tech, just defines class and superclasses and then signatures.

```
class Integer -&gt; java.lang.Integer extends Number,Object {
  Integer &lt;init&gt;(int)
  int MIN_VALUE
  int MAX_VALUE
  int compare(int,int)
  int compareTo(Integer)
  String toHexString(int)
  ...
}
```

Other problems were hardcoding every class inside Definition and then referring to those elsewhere. I changed all these to be a proper lookup (Definition.getType). We have to start making impl details impl details for Definition and instead define operations like this. This change caused a lot of noise, but we need to start separating the stuff. i think in the future if we cleanup Definition, we could e.g. define static final constants for the primitive types and it would look nicer (this requires more refactoring).

primitive types are in our definition file, which may look strange, but we may want to define methods for them. transforms are still hardcoded but those need separate cleanups and I think a lot can be removed. the "ghetto generics" are removed.

The difficulty with cleaning up this (and why i did a half-ass job) is that it takes many hours of slave labor to make a single change. That is why we have to start cleaning it up, it gets easier as we go through it. But its still kinda painful. I just stopped from exhaustion but not because I consider it done or even in a great state, just better.
</description><key id="155738974">18463</key><summary>Definition cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>PITA</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T13:39:00Z</created><updated>2016-05-23T10:19:06Z</updated><resolved>2016-05-20T18:26:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-19T16:53:11Z" id="220385746">LGTM!  Awesome, thank you for making this change!
</comment><comment author="uschindler" created="2016-05-19T17:07:08Z" id="220389477">I will more closely look into this later. Looks good as a first step!
</comment><comment author="rmuir" created="2016-05-19T18:02:11Z" id="220404629">thanks @uschindler . There are many more steps. I don't care how we do it, if you want just push improvements to my branch or we can make more PRs. 

I do think we should avoid cleaning up the grammar, just to be practical. @jdconrad is working heavily on this now and i dont want to conflict with that. From what I see, the generics syntax does not cause any of the problems the grammar has today but it will still simplify to get rid of it later.
</comment><comment author="uschindler" created="2016-05-19T22:12:19Z" id="220466860">Hi, thanks for the invitation to colaborate in this PR. I like the first step, but I am not happy with removing the "constants" and replacing them by getters with strings ("def", "int",...). I know why you did this, so I would take care of changing this:

Remove the copy constructor. I would make the inner part that holds the maps completely private and allocate one singleton instance using a completely private ctor (the instance field would be static final and also private). The maps inside do not necessarily be unmodifiable (we can do this later), but instead of the copy constructor, we should add static getters and static final constants that delegate to the private singleton instance, which is completely private. For consumers of definition clas,s they only see static final constants and some static, read-only getter methods. I will think about that this night and start to implement this tomorrow.
</comment><comment author="uschindler" created="2016-05-20T09:27:25Z" id="220558860">I made the public API of definition completely static. The `INSTANCE` field is temporarily still there, because I have to first remove the `Definition` parameter passed everywhere.
</comment><comment author="uschindler" created="2016-05-20T10:35:56Z" id="220573087">OK, I remved the definition parameter on all methods around analyze and write.
</comment><comment author="rmuir" created="2016-05-20T13:39:27Z" id="220607987">Thanks @uschindler for the cleanups here!!!! I pushed a couple minor cleanups, now that those internal-used types are constants, they are capitalized. And we don't need to pass CompilerSettings to every method in tree nodes.

I think this is good for now? I can think of quite a few followup cleanups but lets just get these simplifications in.

Thanks again for helping with cleanups!
</comment><comment author="uschindler" created="2016-05-20T14:26:11Z" id="220620352">Hi @rmuir, all fine from my side. Thanks for also cleaning up the compiler settings. This was the next on my TODO list. :-)

I think you should merge that now. If we do further improvements, those should be more focused on certain classes.
</comment><comment author="rmuir" created="2016-05-20T15:03:25Z" id="220630740">@jdconrad pinged me last night as he did his cleanup to boxed types in another branch. I pulled it in as its related: now there is no more transforms map, no more confusing Def vs def at all, no Utility stuff in whitelist, much better! 

I agree we should stop now and get some of this in :)
</comment><comment author="jdconrad" created="2016-05-20T15:46:50Z" id="220642871">@rmuir and @uschindler Thanks for all the help with cleaning this stuff up guys.  I know it can be massively time consuming.
</comment><comment author="jdconrad" created="2016-05-20T16:32:09Z" id="220654564">@uschindler An ASM question -- I noticed that for int -&gt; Integer boxing the javac compiler will issue the instruction for Integer.valueOf, but using the GeneratorAdapter to do boxing, the instructions issued use new Integer(...), dup_x1, swap.  Is this anything to worry about, should we be using Integer.valueOf here instead?

Edit: @rmuir has pointed out that I meant valueOf instead of parseInt.  Oops!
</comment><comment author="rmuir" created="2016-05-20T17:00:23Z" id="220661405">+1 to fix that, that is wrong to do (i dont care about java 1.4 here).

Can we override that method in our adapter? http://websvn.ow2.org/filedetails.php?repname=asm&amp;path=%2Ftrunk%2Fasm%2Fsrc%2Forg%2Fobjectweb%2Fasm%2Fcommons%2FGeneratorAdapter.java
</comment><comment author="rmuir" created="2016-05-20T17:01:39Z" id="220661750">I would just override this one for now: `public void box(final Type type) {`. We can do the right thing. Their unbox is fine.
</comment><comment author="uschindler" created="2016-05-20T17:36:15Z" id="220670130">We should not trust everything ASM is doing...
</comment><comment author="uschindler" created="2016-05-20T17:53:01Z" id="220674443">&gt; I would just override this one for now: public void box(final Type type) {. We can do the right thing. Their unbox is fine.

+1 (use the code from the svn link)

``` java
    /**
     * Generates the instructions to box the top stack value using Java 5's
     * valueOf() method. This value is replaced by its boxed equivalent on top
     * of the stack.
     * 
     * @param type
     *            the type of the top stack value.
     */
    public void valueOf(final Type type) {
        if (type.getSort() == Type.OBJECT || type.getSort() == Type.ARRAY) {
            return;
        }
        if (type == Type.VOID_TYPE) {
            push((String) null);
        } else {
            Type boxed = getBoxedType(type);
            invokeStatic(boxed, new Method("valueOf", boxed,
                    new Type[] { type }));
        }
    }
``
```
</comment><comment author="jdconrad" created="2016-05-20T18:14:35Z" id="220679799">@uschindler Thanks for the pointing that out.  I've updated the branch to use valueOf instead of box.
</comment><comment author="rmuir" created="2016-05-20T18:18:18Z" id="220680726">Can we override box to just call valueOf though? I dont like the method being there with inefficient code, waiting for something to call it.
</comment><comment author="uschindler" created="2016-05-20T18:20:15Z" id="220681217">&gt; Can we override box to just call valueOf though?

+1 to remove the trap. Then code can call box() or valueOf()
</comment><comment author="rmuir" created="2016-05-20T18:24:26Z" id="220682268">ok i am happy, trap is gone. thanks @jdconrad for discovering this trap
</comment><comment author="uschindler" created="2016-05-20T18:25:45Z" id="220682613">Thanks!
</comment><comment author="rmuir" created="2016-05-20T18:27:03Z" id="220682951">thanks guys for all the cleanup help! It is worth it here...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Heap_size error when network.host is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18462</link><project id="" key="" /><description>Hello everyone,

**Elasticsearch version**:

```
  "version" : {
    "number" : "5.0.0-alpha2"
```

**JVM version**:
`openjdk version "1.8.0_91"`

**OS version**:

```
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.3 LTS
Release:    14.04
Codename:   trusty
```

**Description of the problem including expected versus actual behavior**:

I'm trying to set up a cluster of 2 Elasticsearch nodes.
I'm experimenting a bug when I try to set the network.host setting in elasticsearch.yml.
When I let it commented no problem, Elasticsearch starts normally, but I believe that I need to set it in the config file of each node in order to make the nodes visible.

So the problem is that the log message concerns the HEAP_SIZE, (parameter that I tried to modify in /etc/default/elasticsearch), whereas my action is to set the network.host parameter.

**Steps to reproduce**:
1. Given 2 VMs with the same configuration (Java / Elasticsearch). Each machine can ping the other one. The problem happens when I try to start Elasticsearch on a node (each of both, same problem).
2. /etc/elasticsearch/elasticsearch.yml on machine 1 :

```
cluster.name: graylog
node.name: elasticsearch1
#network.host: ["IP.OF.NODE.1", "127.0.0.1"]
discovery.zen.ping.unicast.hosts: ["IP.OF.NODE.2"]
discovery.zen.minimum_master_nodes: 1
```
1. Elasticsearch starts just fine. Now if we uncomment network.host

```
cluster.name: graylog
node.name: elasticsearch1
network.host: ["IP.OF.NODE.1", "127.0.0.1"]
discovery.zen.ping.unicast.hosts: ["IP.OF.NODE.2"]
discovery.zen.minimum_master_nodes: 1
```
1. Now Elasticsearch refuses to start properly, with the following error message in the logs.

**Provide logs (if relevant)**:

```
[2016-05-19 14:58:16,436][INFO ][node                     ] [elasticsearch1] version[5.0.0-alpha2], pid[5762], build[e3126df/2016-04-26T12:08:58.960Z]
[2016-05-19 14:58:16,437][INFO ][node                     ] [elasticsearch1] initializing ...
[2016-05-19 14:58:16,819][INFO ][plugins                  ] [elasticsearch1] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-05-19 14:58:16,836][INFO ][env                      ] [elasticsearch1] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [21.9gb], net total_space [27.4gb], spins? [possibly], types [ext4]
[2016-05-19 14:58:16,837][INFO ][env                      ] [elasticsearch1] heap size [1007.3mb], compressed ordinary object pointers [true]
[2016-05-19 14:58:18,701][INFO ][node                     ] [elasticsearch1] initialized
[2016-05-19 14:58:18,702][INFO ][node                     ] [elasticsearch1] starting ...
[2016-05-19 14:58:18,808][INFO ][transport                ] [elasticsearch1] publish_address {IP.OF.NODE.1:9300}, bound_addresses {127.0.0.1:9300}, {IP.OF.NODE.1:9300}
[2016-05-19 14:58:18,812][ERROR][bootstrap                ] [elasticsearch1] Exception
java.lang.RuntimeException: bootstrap checks failed
initial heap size [268435456] not equal to maximum heap size [1073741824]; this can cause resize pauses and prevents mlockall from locking the entire heap
at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:93)
at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:66)
at org.elasticsearch.bootstrap.Bootstrap$5.validateNodeBeforeAcceptingRequests(Bootstrap.java:191)
at org.elasticsearch.node.Node.start(Node.java:323)
at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:206)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:269)
at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
at org.elasticsearch.cli.Command.main(Command.java:53)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Suppressed: java.lang.IllegalStateException: initial heap size [268435456] not equal to maximum heap size [1073741824]; this can cause resize pauses and prevents mlockall from locking the entire heap
at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:94)
... 11 more
[2016-05-19 14:58:18,814][INFO ][node                     ] [elasticsearch1] stopping ...
[2016-05-19 14:58:18,827][INFO ][node                     ] [elasticsearch1] stopped
[2016-05-19 14:58:18,827][INFO ][node                     ] [elasticsearch1] closing ...
[2016-05-19 14:58:18,842][INFO ][node                     ] [elasticsearch1] closed

```

**Describe the feature**:
Any help or enlightenment would be appreciated :)

Thanks !
</description><key id="155734356">18462</key><summary>Heap_size error when network.host is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">TonyRode</reporter><labels /><created>2016-05-19T13:17:40Z</created><updated>2016-07-15T18:31:14Z</updated><resolved>2016-05-19T13:26:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-19T13:26:08Z" id="220323219">Starting in Elasticsearch 5.0.0, there are bootstrap checks to ensure that common missed settings that would effect a production installation of Elasticsearch are configured. These bootstrap checks are enforced if the node is bound to a non-loopback interface, or if the node publishes to a non-loopback interface. Otherwise, these bootstrap checks appear as warning in the logs (check the logs from when you did not have `network.host` uncommented and you will see a warn-level log message). 

```
initial heap size [268435456] not equal to maximum heap size [1073741824]; this can cause resize pauses and prevents mlockall from locking the entire heap
```

To configure the heap size, you need to modify the `jvm.options` file. This is a new configuration file. There are two lines: `-Xms` and `-Xmx` to set the min and max heap size. These values should be equal, and must be equal if you trip the bootstrap checks.
</comment><comment author="jasontedor" created="2016-05-19T13:32:02Z" id="220324783">There is [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/master/heap-size.html) regarding the heap size that expands on my previous [comment](https://github.com/elastic/elasticsearch/issues/18462#issuecomment-220323219) regarding how to set the heap size.
</comment><comment author="TonyRode" created="2016-05-19T13:54:14Z" id="220331003">Okay thank you sir for the fast answer !

edit: Setting the parameter -Xms to the same value of -Xmx in /etc/elasticsearch/jvm.options solved the problem :) Thanks
</comment><comment author="jasontedor" created="2016-05-19T14:05:11Z" id="220334317">&gt; Okay thank you sir for the fast answer !

You're very welcome.
</comment><comment author="jaredmcqueen" created="2016-07-15T16:56:13Z" id="233007126">I'm having the same issue on elasticsearch 5.0.0-alpha4

setting anything other than `_lo_`, `127.0.0.1`, or `_local_` results in an initial heap size error.
</comment><comment author="nik9000" created="2016-07-15T17:00:30Z" id="233008260">This is only an issue if the error isn't telling you to set -Xms and -Xmx to the same value. @jasontedor already explained what is up in https://github.com/elastic/elasticsearch/issues/18462#issuecomment-220323219
</comment><comment author="jaredmcqueen" created="2016-07-15T17:10:13Z" id="233010609">@nik9000 my jvm.options were set to be the same.

I found a solution though -- I had to do the following to bind anything other than localhost:

run `sysctl -w vm.max_map_count=262144`

why would setting my `vm.max_map_count` allow me to set `network.host`?
</comment><comment author="nik9000" created="2016-07-15T17:15:27Z" id="233011869">It is another one of the preflight checks. There are a half dozen of them. Things like "can I write to the data directory" and "am I likely to run out of memory mapped files". If the error message told you the right thing to do then I don't think this is a bug. If it was talking about heap and you had to set max_map_count, then we have a problem.
</comment><comment author="jaredmcqueen" created="2016-07-15T17:17:52Z" id="233012442">@nik9000 yeah I think we have a problem.  The elasticsearch service starts just fine with the line `network.host` commented out, or set to anything that binds to localhost (`_lo_`, `127.0.0.1`, or `_local_`).  If you try to set `network.host` to something other than localhost, it won't start at all, throwing an `initial heap size` error until I set `vm.max_map_count` to 262144.

FYI, using CENTOS 7 minimum build 1511
</comment><comment author="jasontedor" created="2016-07-15T17:21:10Z" id="233013255">&gt; it won't start at all, throwing an `initial heap size` error until I set `vm.max_map_count` to 262144.

Note that failing to start if the initial heap size is not equal to the maximum heap size and you're bound to an external interface is intentional. The same thing is true for `vm.max_map_count`. However, what should not be the case is that you get a log message for the initial heap size error if it is in fact equal to the max heap size, and that the actual problem is `vm.max_map_count`.

Can you provide corroborating log messages and relevant configuration and command line parameters?
</comment><comment author="jaredmcqueen" created="2016-07-15T17:22:27Z" id="233013565">Sure, I'll build a new VM and step through the process again step by step and post relevant logs and info. 
</comment><comment author="jasontedor" created="2016-07-15T18:02:43Z" id="233024074">&gt; Sure, I'll build a new VM and step through the process again step by step and post relevant logs and info.

Thanks.
</comment><comment author="jasontedor" created="2016-07-15T18:31:14Z" id="233031509">Also, note that it's possible for both heap size configuration and `vm.max_map_count` to be a problem, and you will get an error message for both.

```
[2016-07-15 13:30:35,878][WARN ][bootstrap                ] [Locust] uncaught exception in thread [main]
org.elasticsearch.bootstrap.StartupError: java.lang.RuntimeException: bootstrap checks failed
initial heap size [268435456] not equal to maximum heap size [2147483648]; this can cause resize pauses and prevents mlockall from locking the entire heap
max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change BlobPath.buildAsString() method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18461</link><project id="" key="" /><description>This pull request changes the `BlobPath.buildAsString()` behavior so that extra checks in calling methods are not needed anymore.

This a follow up from https://github.com/elastic/elasticsearch/pull/13578#discussion_r59062851
</description><key id="155732827">18461</key><summary>Change BlobPath.buildAsString() method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-19T13:10:41Z</created><updated>2016-05-23T09:29:23Z</updated><resolved>2016-05-23T09:28:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-20T19:17:52Z" id="220695004">LGTM
</comment><comment author="tlrx" created="2016-05-23T09:29:17Z" id="220931376">Thanks @abeyad !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to start elasticsearch 2.3.2 as a service in Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18460</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2 and 2.3.3

**JVM version**: 1.7.0_55

**OS version**: Linux Mint 15 MATE 64-bit

**Description of the problem including expected versus actual behavior**:
Unable to start elasticsearch as a service. In service.bat file, the jvm parameter to executable is configured with %%JAVA_HOME%% which does not resolve the path properly. As a result, the service gets registered with invalid JAVA_HOME and fails to start elasticsearch.

"%EXECUTABLE%" //IS//%SERVICE_ID% --Startup %ES_START_TYPE% --StopTimeout %ES_STOP_TIMEOUT% --StartClass org.elasticsearch.bootstrap.Elasticsearch --StopClass org.elasticsearch.bootstrap.Elasticsearch --StartMethod main --StopMethod close --Classpath "%ES_CLASSPATH%" --JvmMs %JVM_MS% --JvmMx %JVM_MX% --JvmSs %JVM_SS% --JvmOptions %ES_JAVA_OPTS% ++JvmOptions %ES_PARAMS% %LOG_OPTS% --PidFile "%SERVICE_ID%.pid" --DisplayName "%SERVICE_DISPLAY_NAME%" --Description "%SERVICE_DESCRIPTION%" --Jvm "%%JAVA_HOME%%%JVM_DLL%" --StartMode jvm --StopMode jvm --StartPath "%ES_HOME%" %SERVICE_PARAMS%

**Steps to reproduce**:
1. Install elasticsearch. &gt;service.bat install
2. Open service manager. &gt;service.bat manager
3. Goto JAVA tab and check the value under "Java Virtual Machine" field. The path to jvm.dll is not correct.

**Provide logs (if relevant)**:
[error] [11372] Failed creating java %JAVA_HOME%\jre\bin\server\jvm.dll
[error] [11372] The system cannot find the path specified.
[error] [11372] ServiceStart returned 1
[error] [11372] The system cannot find the path specified.
</description><key id="155710377">18460</key><summary>Unable to start elasticsearch 2.3.2 as a service in Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sureshkm</reporter><labels /><created>2016-05-19T11:04:06Z</created><updated>2016-05-19T12:03:03Z</updated><resolved>2016-05-19T12:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-19T12:02:49Z" id="220304275">You need to set a system-level environment variable named `JAVA_HOME` that points to your Java installation. Please see the discussion on #18373 for why it is `%JAVA_HOME%` and not the value of `JAVA_HOME` when you installed the service, and for discussion therein of basically the same issue and how to resolve it.

Please note that Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions (should you have additional questions).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Creating an index on ES 2.2.0 with mapping containing "version": { "created": "1070299" } leads to odd results (records disappear)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18459</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: java version "1.8.0_92"

**OS version**: 4.2.0-35-generic #40~14.04.1-Ubuntu SMP x86_64

**Description of the problem including expected versus actual behavior**:
A mapping was uploaded to ES.
This mapping contained the following settings:
`"settings": {
                "index": {
                        "creation_date": "1447768297037",
                        "number_of_shards": "6",
                        "uuid": "1vDn3klzQ6uWj3mxLmXxaA",
                        "version": {
                                "created": "1070299"
                        },
                        "number_of_replicas": "0"
                }
        }
`

Then the index was created by bulk inserting records using LogStash.

At the end of the insertion process, a quick look at the "settings" for the index showed what was expected, but after stopping and restarting ES, the "settings" were updated and contained the following:
`settings: {
index: {
legacy: {
routing: {
hash: {
type: "org.elasticsearch.cluster.routing.DjbHashFunction"
},
use_type: "false"
}
},
number_of_shards: "6",
creation_date: "1447768297037",
number_of_replicas: "0",
uuid: "LaCy5xaQRAyvtgC9WHLpBw",
version: {
created: "1070299",
upgraded: "2020099"
}
}
}`

If we searched for a specific record prior to restarting ES, using
`http://ourhost/ourindex/outype/d0f1124e2a98`
we got the correct record returned.

After ES restart, the very same request returns
`{
_index: "ourindex",
_type: "ourtype",
_id: "d0f1124e2a98",
found: false
}`

If we issue the following request:
`http://ourhost/ourindex/outype/d0f1124e2a98?routing=5`
we get the correct record.

**Steps to reproduce**: read above

**Provide logs (if relevant)**: none
</description><key id="155691808">18459</key><summary>Creating an index on ES 2.2.0 with mapping containing "version": { "created": "1070299" } leads to odd results (records disappear)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">almico</reporter><labels /><created>2016-05-19T09:27:46Z</created><updated>2016-05-19T12:45:36Z</updated><resolved>2016-05-19T12:45:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-19T12:45:35Z" id="220313383">@almico So what happens here is:
- You create an index with the incorrect version setting (you should never set the version yourself and won't be able to do so in ES 5.0 onwards)
- The new index uses the current routing scheme
- You index docs into it
- When your restart the cluster, it finds what it thinks is an old index, which it then upgrades and adds the setting to use the old hash function.

Never set the version yourself.  Like I say, you won't be able to do this in 5.0, but it's much harder to prevent in 2.x.

To fix your existing index, first close it, then set the following setting, then reopen it:

```
PUT ourindex/_settings
{
  "index.legacy.routing.hash.type": "org.elasticsearch.cluster.routing.Murmur3HashFunction"
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_bounding_box queries failing with "Illegal shift value, must be 32..63; got shift=0"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18458</link><project id="" key="" /><description>**Elasticsearch version**:2.3.2

**JVM version**: 1.8.0 25.0-b70

**OS version**: Mac OS X 10.10.3

**Description of the problem including expected versus actual behavior**:
Indexing documents with geo_point attributes and then finding them in a bounding_box results in an illegal_argument_exception with the reason "Illegal shift value, must be 32..63; got shift=0"

**Steps to reproduce**:
1. 
`curl -XPUT 'http://localhost:9200/test/'`

2.

```
curl -XPUT 'http://localhost:9200/test/a/_mapping' -d '
{

    "properties" : {
        "location" : {"type": "geo_point", "store": true, "lat_lon": true},
        "name": {"type" : "string", "store" : true }
    }
}
'
```

 3.

```
curl -XPOST 'http://localhost:9200/test/a/1/' -d '
{
    "location" : {"lat": 44, "lon": 55},
    "name": "John Smith"
}
'
```
1. 

```
curl -XPOST 'http://localhost:9200/test/a/_search' -d '
{
 query: {
    bool: {
      must: {
        match_all: {}
      },
      filter: {
        geo_bounding_box: {
        location : {
              top_left : [56,43],
              bottom_right : [54,45]
          }
        }
      }
    }   
  }
}
'
```

**Stacktrace**:

```
[2016-05-19 11:11:51,705][DEBUG][action.search            ] [Crimson Dynamo V] [test][3], node[qR1OS2SgRkKv5G-g7z43Tw], [P], v[2], s[STARTED], a[id=43cmhMAPSBCaD8gU5b88Hg]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3ccd252e] lastShard [true]
RemoteTransportException[[Crimson Dynamo V][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: IllegalArgumentException[Illegal shift value, must be 32..63; got shift=0];
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: IllegalArgumentException[Illegal shift value, must be 32..63; got shift=0];
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:366)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:378)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IllegalArgumentException: Illegal shift value, must be 32..63; got shift=0
    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCodedBytes(GeoEncodingUtils.java:114)
    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCoded(GeoEncodingUtils.java:92)
    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum$Range.fillBytesRef(GeoPointPrefixTermsEnum.java:234)
    at org.apache.lucene.spatial.geopoint.search.GeoPointTermsEnum.nextRange(GeoPointTermsEnum.java:71)
    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextRange(GeoPointPrefixTermsEnum.java:169)
    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextSeekTerm(GeoPointPrefixTermsEnum.java:188)
    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:212)
    at org.apache.lucene.spatial.geopoint.search.GeoPointTermQueryConstantScoreWrapper$1.scorer(GeoPointTermQueryConstantScoreWrapper.java:103)
    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:628)
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:263)
    at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:389)
    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:628)
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:263)
    at org.apache.lucene.search.ConstantScoreQuery$1.scorer(ConstantScoreQuery.java:134)
    at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:389)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:370)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:818)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384)
    ... 12 more

```
</description><key id="155636687">18458</key><summary>geo_bounding_box queries failing with "Illegal shift value, must be 32..63; got shift=0"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">daamsie</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2016-05-19T01:32:40Z</created><updated>2016-08-26T13:23:34Z</updated><resolved>2016-05-27T20:01:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-05-19T07:53:11Z" id="220252776">I think `top_left` and `bottom_right` need to be swapped in the query. This query works:

```
POST /test/a/_search
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "geo_bounding_box": {
          "location": {
            "bottom_right": [
              56,
              43
            ],
            "top_left": [
              54,
              45
            ]
          }
        }
      }
    }
  }
}
```

A better error message would certainly help here.
</comment><comment author="daamsie" created="2016-05-19T09:41:58Z" id="220276428">Interesting. But why would 54 be the top of the box and 56 be bottom? And why would 45 be right while 43 is left? That doesn't make any sense.

For now I am working around this bug by using a geo_polygon, but I understand the performance is not as good. 

```
curl -XPOST 'http://localhost:9200/test/a/_search' -d '
{
 query: {
    bool: {
      must: {
        match_all: {}
      },
      filter: {
        geo_polygon: {
        location : {
            points: [
                {lat: 43, lon: 54},
                {lat: 43, lon: 56},
                {lat: 45, lon: 56},
                {lat: 45, lon: 54}
            ]
          }
        }
      }
    }   
  }
}
'
```
</comment><comment author="s1monw" created="2016-05-19T09:56:44Z" id="220279737">@nknize can you please look at this issue. I think it has something todo with your geov2 changes. Maybe we trying to pull a double from a float field or something like this. It seems like the wrong decoding for prefix coded terms is used on the lucene level.
</comment><comment author="brwe" created="2016-05-19T10:49:48Z" id="220290678">&gt;  But why would 54 be the top of the box and 56 be bottom? And why would 45 be right while 43 is left? That doesn't make any sense.

When you define a geo coordinate with [x, y] then x is the longitude and y the latitude, see [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-geo-bounding-box-query.html#_lat_lon_as_array_2). 

So [56, y] will be "right" (east, unless you want to span 56-54 once around earth) of  [54, y]  and [x, 45] would be "above" (north) of [x, 43]. 
At least this is how I understood it.
</comment><comment author="brwe" created="2016-05-19T10:54:34Z" id="220291588">Although now that I think about it I am assuming here that  +10 longitude means we are going east and I am not sure anymore this is correct. @nknize  will hopefully be able to explain.
</comment><comment author="daamsie" created="2016-05-19T11:10:15Z" id="220294484">+10 longitude does mean you're headed east. 

But I see what you're saying now... I did know that geoJSON format has lon, then lat, but I didn't notice that top_left also follows this. The naming is _really_ confusing in that instance since it strongly implies that top (latitude) comes before left (longitude). It's like naming it latLon and then having the values be [lon, lat]

Ah well, in that case I agree that some better error messaging would be great here, but at least this thread now exists to help future people googling for an answer (I spent a while hunting for one on this! :))
</comment><comment author="nknize" created="2016-05-19T13:33:19Z" id="220325160">@daamsie I see how one could misinterpret `top_left` `bottom_right` json field names. For this reason we recommend expressing a geo point as an object so there's no ambiguity as to which value is lat and lon.

Still, the error message indicates a bug with pole crossing. I would expect this query be split into 2 bboxes: [56, 43] , [180, -90] and [-180, 90] , [54, 45]  The fact this wasn't caught also indicates missing test coverage so thank you for pointing this out.
</comment><comment author="jweber" created="2016-05-26T19:54:12Z" id="221976428">I just wanted to note that we started seeing this same issue after moving to 2.3.3.

If it helps, here are a few of the _geo_bounding_box_ values that were generating this message:

`{
    "top_left": "48.65, -123.4805",
    "bottom_right": "48.7, -123.409"
}`

`{
    "top_left": "54, 10.5",
    "bottom_right": "54.1, 10.6"
}`

`{
    "top_left": "37.3, -112.6",
    "bottom_right": "37.4, -112.5"
}`
</comment><comment author="nknize" created="2016-05-26T22:20:09Z" id="222011676">Hi @jweber, the coordinates should be in lon, lat order.
</comment><comment author="jweber" created="2016-05-26T22:24:12Z" id="222012380">[The docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-geo-bounding-box-query.html#_lat_lon_as_string_2) say that it should be in lat,lon order when used as a string. Is that incorrect?
</comment><comment author="nknize" created="2016-05-27T19:31:12Z" id="222234380">Sorry, you're correct. But in each of your examples the top `latitude` value is less than the bottom. So the bounding boxes will be crossing the pole. It looks like the check for this didn't make it into the 2.3 branch so you're getting the scary Lucene exception. I'll back port the friendlier exception. Thanks for the heads up!
</comment><comment author="nknize" created="2016-05-27T20:01:53Z" id="222240422">I added the friendlier exception handling in https://github.com/elastic/elasticsearch/commit/55513946827b5061be6d431da5671711ec489d6c Its already fixed on master so I'm going to go ahead and close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dead BloomFilter code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18457</link><project id="" key="" /><description>We don't use this class for a quite a while. lets trash it.
</description><key id="155600815">18457</key><summary>Remove dead BloomFilter code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T21:02:12Z</created><updated>2016-05-19T12:11:48Z</updated><resolved>2016-05-19T07:19:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-18T21:03:18Z" id="220157351">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rewrite reindex's RetryTests to force rejections</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18456</link><project id="" key="" /><description>The retry test has failed a couple of times in CI because it wasn't able
to cause any retries. Putting it in a bash `while` loop shows that it
eventually does fail that way. The seed "4F6477A9C999CA20" seems especially
good at failing to get retries. It doesn't fail all the time, but more
than most.

This rewrites the test to get the exceptions every time by carefully blocking the executors.

Failing CI build that triggered this:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/852/console

Edit:
The second paragraph used to say:
This adds a retry to each test case, retrying a maximum of 10 times or
until it causes the retries. I've seen it fail to get retries 7 times
in a row but not go beyond that. Retrying doesn't seem to really hurt
the test runtime all that much. Most of the time is in the startup
cost.
</description><key id="155596682">18456</key><summary>Rewrite reindex's RetryTests to force rejections</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T20:42:27Z</created><updated>2016-05-23T18:39:08Z</updated><resolved>2016-05-23T18:39:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-18T20:42:42Z" id="220151832">@dakrone and @tlrx want to look at this?
</comment><comment author="dakrone" created="2016-05-19T15:36:33Z" id="220363311">Looking at this, I don't think I like the way the test is just repeated until a rejection occurs, it looks too much like `@Repeat` to force it to pass. I'd much rather try to solve the problem in the code so it could always be repeatable (much easier to debug that way too).

I see you set the queue_size to 1, what about setting it to 0, would that cause it not to fail?
</comment><comment author="nik9000" created="2016-05-19T16:39:20Z" id="220382069">&gt; I see you set the queue_size to 1, what about setting it to 0, would that cause it not to fail?

I expect that'll help some but might not be enough. The other option is to simply not assert that the test triggered the retries at all. Or to remove the test entirely and rely on the unit tests.

I don't like relying entirely on the unit tests because I think we could break this feature without breaking the unit tests. They are too mock-dance-ish.

I'm not a huge fan of just not asserting that we triggered retries but it is better than not having the test at all. It just means the test might become out of date without us noticing it.
</comment><comment author="dakrone" created="2016-05-19T19:40:14Z" id="220430457">&gt; I expect that'll help some but might not be enough.

What if you make this a single node case (if it's not already), and then you can get the instance for the threadpools, then you can register a `Runnable` that counts down on a latch blocking the bulk threadpool until you release the latch? That would allow you to force retries since you could occupy the single bulk thread for a while right?
</comment><comment author="nik9000" created="2016-05-19T20:50:17Z" id="220448125">&gt; blocking the bulk threadpool until you release the latch

I'll give that a shot in the morning! That sounds like a much better way to do it than this.
</comment><comment author="nik9000" created="2016-05-20T16:11:55Z" id="220649559">@dakrone I rewrote it.
</comment><comment author="dakrone" created="2016-05-20T17:28:56Z" id="220668381">LGTM, thanks for rewriting it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: better error message when analyzer created without tokenizer or&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18455</link><project id="" key="" /><description>This should address #15492.  I'm not sure if I put the test case in the correct place, so I can move that if needed.  Also, I wasn't entirely sure how to exercise the other code path (AnalysisRegistry:338) so any tips on that would be appreciated.
</description><key id="155568006">18455</key><summary>Core: better error message when analyzer created without tokenizer or&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jeff303</reporter><labels><label>:Analysis</label><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T18:17:35Z</created><updated>2016-05-19T13:47:34Z</updated><resolved>2016-05-19T13:47:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2016-05-18T18:17:36Z" id="220113426">Hi @jeff303, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git [commit](https://github.com/elastic/elasticsearch/pull/18455.patch). Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?
</comment><comment author="jeff303" created="2016-05-18T18:20:24Z" id="220114219">@karmi , apologies, I forgot to update user.email for this particular commit.  I have done so, and force-pushed.
</comment><comment author="s1monw" created="2016-05-18T19:19:41Z" id="220130451">this looks great, I left one comment on the test! 

thanks for improving this!
</comment><comment author="karmi" created="2016-05-19T06:13:01Z" id="220236445">@jeff303, great, the CLA check is now passing correctly!
</comment><comment author="s1monw" created="2016-05-19T13:07:56Z" id="220318574">looks awesome! I am running tests and will pull this in afterwards
</comment><comment author="s1monw" created="2016-05-19T13:47:34Z" id="220329086"> merged thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scrolling in Multi-Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18454</link><project id="" key="" /><description>**Describe the feature**:

We would like to perform multiple searches at once that may contain more than 1 page of results.  From and Size are not reliable methods of paging results.

The [Multi-Search](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html) feature currently does not support [scrolling](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html).    If it did we would be able to page the results properly.
</description><key id="155554955">18454</key><summary>Scrolling in Multi-Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">collinsauve</reporter><labels /><created>2016-05-18T17:10:05Z</created><updated>2016-05-19T11:54:17Z</updated><resolved>2016-05-19T11:54:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-18T20:35:33Z" id="220149961">I don't think this would play nicely given that all requests would not have the same number of pages. I think it is ok to require users to use the regular search API when scrolling.
</comment><comment author="clintongormley" created="2016-05-19T11:54:17Z" id="220302702">Agreed with @jpountz- multi-search doesn't add anything useful here except complexity
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: Failing org.elasticsearch.messy.tests.IndicesRequestTests on 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18453</link><project id="" key="" /><description>**Elasticsearch version**: 2.x

**JVM version**: Oracle Corporation 1.8.0_77 (64-bit)

**OS version**: Linux 3.12.51-52.31-default amd64

**Description of the problem including expected versus actual behavior**:

Failing CI tests on 2.x branch:

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-intake/205/console
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-intake/204/console
http://build-us-00.elastic.co/job/es_core_2x_windows-2012-r2/973
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-intake/203/console

**Provide logs (if relevant)**:

```
Suite: org.elasticsearch.messy.tests.IndicesRequestTests
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=A693FA69435937FD -Dtests.class=org.elasticsearch.messy.tests.IndicesRequestTests -Dtests.method="testClearCache" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fr-CH -Dtests.timezone=Europe/Belfast
FAILURE 0.95s J0 | IndicesRequestTests.testClearCache &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: no internal requests intercepted for action [indices:admin/cache/clear[n]]
   &gt; Expected: a value greater than &lt;0&gt;
   &gt;      but: &lt;0&gt; was equal to &lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([A693FA69435937FD:684CC0A5C1CA78BD]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:736)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:725)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.testClearCache(IndicesRequestTests.java:426)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=A693FA69435937FD -Dtests.class=org.elasticsearch.messy.tests.IndicesRequestTests -Dtests.method="testForceMerge" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fr-CH -Dtests.timezone=Europe/Belfast
FAILURE 0.57s J0 | IndicesRequestTests.testForceMerge &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: no internal requests intercepted for action [indices:admin/optimize[n]]
   &gt; Expected: a value greater than &lt;0&gt;
   &gt;      but: &lt;0&gt; was equal to &lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([A693FA69435937FD:FBEBABFB4BDD3AA5]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:736)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:725)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.testForceMerge(IndicesRequestTests.java:401)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=A693FA69435937FD -Dtests.class=org.elasticsearch.messy.tests.IndicesRequestTests -Dtests.method="testRecovery" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fr-CH -Dtests.timezone=Europe/Belfast
FAILURE 1.64s J0 | IndicesRequestTests.testRecovery &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: no internal requests intercepted for action [indices:monitor/recovery[n]]
   &gt; Expected: a value greater than &lt;0&gt;
   &gt;      but: &lt;0&gt; was equal to &lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([A693FA69435937FD:676383C56E09FD5A]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:736)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:725)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.testRecovery(IndicesRequestTests.java:438)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=A693FA69435937FD -Dtests.class=org.elasticsearch.messy.tests.IndicesRequestTests -Dtests.method="testIndicesStats" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fr-CH -Dtests.timezone=Europe/Belfast
FAILURE 0.80s J0 | IndicesRequestTests.testIndicesStats &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: no internal requests intercepted for action [indices:monitor/stats[n]]
   &gt; Expected: a value greater than &lt;0&gt;
   &gt;      but: &lt;0&gt; was equal to &lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([A693FA69435937FD:B7CE8FFC016D7FCB]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:736)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:725)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.testIndicesStats(IndicesRequestTests.java:462)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=A693FA69435937FD -Dtests.class=org.elasticsearch.messy.tests.IndicesRequestTests -Dtests.method="testSegments" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fr-CH -Dtests.timezone=Europe/Belfast
FAILURE 1.55s J0 | IndicesRequestTests.testSegments &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: no internal requests intercepted for action [indices:monitor/segments[n]]
   &gt; Expected: a value greater than &lt;0&gt;
   &gt;      but: &lt;0&gt; was equal to &lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([A693FA69435937FD:73BCFB86802FF5E7]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:736)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:725)
   &gt;    at org.elasticsearch.messy.tests.IndicesRequestTests.testSegments(IndicesRequestTests.java:450)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+2.x+multijob-intake/modules/lang-groovy/target/J0/temp/org.elasticsearch.messy.tests.IndicesRequestTests_A693FA69435937FD-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene54): {field=PostingsFormat(name=Asserting), _field_names=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), _timestamp=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting)}, docValues:{_type=DocValuesFormat(name=Lucene54), _version=DocValuesFormat(name=Lucene54), _timestamp=DocValuesFormat(name=Lucene54)}, sim=RandomSimilarity(queryNorm=false,coord=crazy): {}, locale=fr-CH, timezone=Europe/Belfast
  2&gt; NOTE: Linux 3.12.51-52.31-default amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=4,threads=1,free=342974464,total=513802240
  2&gt; NOTE: All tests run in this JVM: [GroovyScriptTests, IndicesRequestTests]
```
</description><key id="155536606">18453</key><summary>CI: Failing org.elasticsearch.messy.tests.IndicesRequestTests on 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>jenkins</label><label>test</label><label>v2.4.0</label></labels><created>2016-05-18T15:47:47Z</created><updated>2016-05-19T08:21:51Z</updated><resolved>2016-05-19T08:21:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-18T15:49:17Z" id="220070402">Unfortunately cannot reproduce this one so far with the provided REPRODUCE line.
</comment><comment author="cbuescher" created="2016-05-18T16:41:26Z" id="220086348">This reproduces on 2.x when running the whole test suit. I think it might have started with 40452f0, the rest of the changes since then look harmless. @danielmitterdorfer could you take a look at this? I will put an @AwaitsFix in for this test for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Swift support for snapshot/restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18452</link><project id="" key="" /><description>**Describe the feature**:
There has been a community plugin for this (https://github.com/wikimedia/search-repository-swift) but it's become out-of-date (last update: 1.7)
</description><key id="155531535">18452</key><summary>Swift support for snapshot/restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">eskibars</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>feedback_needed</label></labels><created>2016-05-18T15:27:45Z</created><updated>2016-06-01T13:51:12Z</updated><resolved>2016-06-01T13:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-18T15:44:39Z" id="220068829">@nik9000 is this something we can support and test? you were involved with the wikimedia plugin
</comment><comment author="s1monw" created="2016-05-27T09:31:13Z" id="222102634">The plugin has been updated to 2.3.3 I think we are good here?
</comment><comment author="eskibars" created="2016-05-27T14:09:02Z" id="222155950">It certainly helps as a stop-gap.  The second question is whether we want to bring this type of functionality into Elasticsearch (so we can support it, as we have people asking for that) or be happy with it as a community plugin (which we can't support and which will certainly always lag behind)
</comment><comment author="clintongormley" created="2016-06-01T13:51:12Z" id="222998400">Given that there are only 14 watchers on the github repo, perhaps it isn't yet worth us supporting this in core.  I think I'm going to close for now, but we can reopen if demand grows.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix azure files removal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18451</link><project id="" key="" /><description>Probably when we updated Azure SDK, we introduced a regression.
Actually, we are not able to remove files anymore.

For example, if you register a new azure repository, the snapshot service tries to create a temp file and then remove it.
Removing does not work and you can see in logs:

```
[2016-05-18 11:03:24,914][WARN ][org.elasticsearch.cloud.azure.blobstore] [azure] can not remove [tests-ilmRPJ8URU-sh18yj38O6g/] in container {elasticsearch-snapshots}: The specified blob does not exist.
```

This fix deals with that. It now list all the files in a flatten mode, remove in the full URL the server and the container name.

As an example, when you are removing a blob which full name is `https://dpi24329.blob.core.windows.net/elasticsearch-snapshots/bar/test` you need to actually call Azure SDK with `bar/test` as the path, `elasticsearch-snapshots` is the container.

To run the test, you need to pass some parameters: `-Dtests.thirdparty=true -Dtests.config=/path/to/elasticsearch.yml`

Where `elasticsearch.yml` contains something like:

```
cloud.azure.storage.default.account: account
cloud.azure.storage.default.key: key
```

Related to #16472
Closes #18436.

This PR also fixes a missing default value for setting `repositories.azure.container` and adds more logs.
</description><key id="155531376">18451</key><summary>Fix azure files removal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T15:27:07Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-25T09:21:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-18T15:28:02Z" id="220063466">@imotov Could you give a look at this?
</comment><comment author="imotov" created="2016-05-20T12:15:02Z" id="220590085">Left a few minor comments. Otherwise LGTM.
</comment><comment author="dadoonet" created="2016-05-20T18:10:55Z" id="220678868">@imotov I pushed some other changes. Do you mind giving a final review for it?

Thanks! 
</comment><comment author="dadoonet" created="2016-05-20T18:11:56Z" id="220679140">@clintongormley I believe I should backport this to 2.x branch right?
And probably to 1.7 as well as I believe latest 1.7 versions are also broken.
</comment><comment author="clintongormley" created="2016-05-23T10:18:23Z" id="220941973">&gt; @clintongormley I believe I should backport this to 2.x branch right? And probably to 1.7 as well as I believe latest 1.7 versions are also broken.

Yes please
</comment><comment author="imotov" created="2016-05-23T17:11:29Z" id="221034306">Left a minor comment about a comment. I still think `blob_container` doesn't match our naming convention and should be renamed to `blobContainer`. 
</comment><comment author="dadoonet" created="2016-05-25T08:50:31Z" id="221512027">&gt; Left a minor comment about a comment. I still think blob_container doesn't match our naming convention and should be renamed to blobContainer.

I do agree. I just missed the comment! Sorry!
</comment><comment author="dadoonet" created="2016-05-25T16:18:06Z" id="221628054">For the record:
- 2.x: https://github.com/elastic/elasticsearch/issues/18571
- 1.7: https://github.com/elastic/elasticsearch-cloud-azure/issues/117
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.3.1 accept a field both as a string and as an object on the same index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18450</link><project id="" key="" /><description>As I understand, it should be impossible to use the same field name with more then one field type on the same index, regardless of the document type. 

But, on 2.3.1 we sometimes see that the index accepts a field with both string and object types (easy to reproduce - see bellow).

After this point the behaviour is not that predictive as some docs gets in and other don't with IllegalArgumentException (see log samples bellow).

Can you please confirm this is a bug or make it clear when a field can be used with more then one field-type?

Thanks!

**Elasticsearch version**: 2.3.1

**JVM version**: 1.7.0_95

**OS version**: Linux AMD64 3.13.0-48-generic

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. POST es-host:9200/index-name/type1  { "field-name" : "a string value" }
2. POST es-host:9200/index-name/type2  
   { "field-name" : { "inner-field" : "now an object"} }
3. GET es-host:9200/index-name/_mappings 

```
{
    "index-name": {
        "mappings": {
            "type1": {
                "properties": {
                    "field-name": {
                        "type": "string"
                    }
                }
            },
            "type2": {
                "properties": {
                    "field-name": {
                        "properties": {
                            "inner-field": {
                                "type": "string"
                            }
                        }
                    }
                }
            }
        }
    }
}
```
1. On this point indexing to the index fails on some cases - either when trying to use the same field ('field-name') on new types and on the existing types and even (hard to reproduce) on docs without this specific field.

*_Provide logs *_:

From the master node:

```
[2016-05-11 08:39:18,659][DEBUG][action.admin.indices.mapping.put] [master-xxx] failed to put mappings on indices [[index1]], type [client]
java.lang.IllegalArgumentException: Field [os] is defined as a field in mapping [elb] but this name is already used for an object in other types
        at org.elasticsearch.index.mapper.MapperService.checkObjectsCompatibility(MapperService.java:473)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:336)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:289)
        at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:223)
        at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:468)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

From the data node:

[2016-05-11 08:42:58,381][DEBUG][action.bulk              ] [PROD-data-elasticsearch2-4-i-9cbda518] [index1][0] failed to execute bulk item (index) index {[index1][iis][AVSe-7uvn-xxxxxxxxx], source[{"cs-method":"GET","cs-uri-stem":"/deploy/General/CheckHealth","cs-uri-query":"-","sc-substatus":0,"type":"iis","cs-username":"-","os_iis":"Other","host":"IP-xxxx","timestamp":"2016-05-11 08:41:56","app":"player","s-sitename":"0.0.0.0","message":"2016-05-11 08:41:56 0.0.0.0 GET /deploy/General/CheckHealth - 80 - 1.1.1.1 ELB-HealthChecker/1.0 - 200 0 0 0","env":"dev","time-taken":0,"cs":["ELB-HealthChecker/1.0","-"],"@timestamp":"2016-05-11T08:41:56.000Z","response":200,"s-port":"80","name":"Other","os_name":"Other","c-ip":"1.1.1.1","device":"Other"}]}
java.lang.IllegalArgumentException: Field [os] is defined as a field in mapping [elb] but this name is already used for an object in other types
        at org.elasticsearch.index.mapper.MapperService.checkObjectsCompatibility(MapperService.java:473)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:336)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:289)
        at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:223)
        at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:468)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="155531301">18450</key><summary>Elasticsearch 2.3.1 accept a field both as a string and as an object on the same index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">segalziv</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-05-18T15:26:49Z</created><updated>2016-05-18T16:08:55Z</updated><resolved>2016-05-18T15:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-18T15:32:25Z" id="220064877">This is indeed a bug, and is already fixed in master.  @jpountz @rjernst is this fix backportable?
</comment><comment author="segalziv" created="2016-05-18T15:35:03Z" id="220065691">ok. thanks for the fast response!

I really hope this is indeed going to get back ported.
</comment><comment author="rjernst" created="2016-05-18T15:50:06Z" id="220070694">@clintongormley I think you mean #17568, which was already backported and released in 2.3.2?
</comment><comment author="clintongormley" created="2016-05-18T15:52:18Z" id="220071448">Ahhhh thanks @rjernst - I missed that.  Tried the above recreation in 2.3.2 and it fails (as it should).

thanks.  Fixed by #17568
</comment><comment author="segalziv" created="2016-05-18T16:08:55Z" id="220076648">Thanks guys!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: RepositoryS3SettingsTests.testInvalidChunkBufferSizeRepositorySettings fails on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18449</link><project id="" key="" /><description>**JVM version**: Oracle Corporation 1.8.0_72 (64-bit)

**OS version**: Mac OS X 10.10.5 x86_64

**Description of the problem including expected versus actual behavior**:
CI Failure on Revision: 27b65e90ca03678a9f7019f6887f1b12ef1e6fb0 
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/850/console

**Steps to reproduce**:
gradle :plugins:repository-s3:test -Dtests.seed=F10111D46D737C83 -Dtests.class=org.elasticsearch.cloud.aws.RepositoryS3SettingsTests -Dtests.method="testInvalidChunkBufferSizeRepositorySettings" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=be -Dtests.timezone=Mexico/BajaNorte

**Provide logs (if relevant)**:

```
Suite: org.elasticsearch.cloud.aws.RepositoryS3SettingsTests
  2&gt; REPRODUCE WITH: gradle :plugins:repository-s3:test -Dtests.seed=F10111D46D737C83 -Dtests.class=org.elasticsearch.cloud.aws.RepositoryS3SettingsTests -Dtests.method="testInvalidChunkBufferSizeRepositorySettings" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=be -Dtests.timezone=Mexico/BajaNorte
FAILURE 0.43s J1 | RepositoryS3SettingsTests.testInvalidChunkBufferSizeRepositorySettings &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: a string containing "Failed to parse value [6tb] for setting [chunk_size] must be =&lt; 5tb"
  2&gt; NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+master+periodic/plugins/repository-s3/build/testrun/test/J1/temp/org.elasticsearch.cloud.aws.RepositoryS3SettingsTests_F10111D46D737C83-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {}, docValues:{}, maxPointsInLeafNode=1537, maxMBSortInHeap=5.799909680151539, sim=ClassicSimilarity, locale=be, timezone=Mexico/BajaNorte
  2&gt; NOTE: Linux 3.12.51-52.31-default amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=4,threads=1,free=436672664,total=514850816
   &gt;      but: was "Failed to parse value [6tb] for setting [chunk_size] must be &lt;= 5tb"
  2&gt; NOTE: All tests run in this JVM: [S3ProxiedSnapshotRestoreOverHttpsTests, RepositoryS3SettingsTests]
   &gt;    at __randomizedtesting.SeedInfo.seed([F10111D46D737C83:55C7A59262B5911]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.cloud.aws.RepositoryS3SettingsTests.internalTestInvalidChunkBufferSizeSettings(RepositoryS3SettingsTests.java:344)
   &gt;    at org.elasticsearch.cloud.aws.RepositoryS3SettingsTests.testInvalidChunkBufferSizeRepositorySettings(RepositoryS3SettingsTests.java:318)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="155526660">18449</key><summary>CI: RepositoryS3SettingsTests.testInvalidChunkBufferSizeRepositorySettings fails on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Plugin Repository S3</label><label>jenkins</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T15:08:53Z</created><updated>2016-05-18T16:16:32Z</updated><resolved>2016-05-18T16:16:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove last vestiges of /bin/sh shebangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18448</link><project id="" key="" /><description>This commit removes the remaining /bin/sh shebangs in favor of
/bin/bash.
</description><key id="155521152">18448</key><summary>Remove last vestiges of /bin/sh shebangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T14:48:44Z</created><updated>2016-05-26T11:44:23Z</updated><resolved>2016-05-18T15:03:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-18T14:51:59Z" id="220051212">LGTM, thanks for switching everything!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Transport Profile configuration for TransportClients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18447</link><project id="" key="" /><description>When using the TransportClient with Sniffing, the Java application is using any of the profiles that you configure internally in the nodes. That way, is hard to separate the ports that the TransportClient uses to connect to the cluster, and the connections between the node.

Using a static pool resolves the issue, because you just use a fixed list of Ips+Hosts.

We should consider adding a new setting to the TransportClient to define a (set) of profiles (or default) so we don't use non-permitted endpoints when sniffing.
</description><key id="155516531">18447</key><summary>Add Transport Profile configuration for TransportClients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Java API</label><label>discuss</label><label>enhancement</label></labels><created>2016-05-18T14:31:16Z</created><updated>2016-05-23T14:13:55Z</updated><resolved>2016-05-20T09:14:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-20T09:14:05Z" id="220556117">Discussed in FixItFriday. Transport profiles were added as a way to distinguish internal traffic from external traffic for security purposes, however the code introduces a lot of complexity.  Given that our goal is to remove the transport client entirely and to expose just the HTTP interface, we've decided against fixing this (and adding even more complexity).
</comment><comment author="alehane" created="2016-05-20T10:08:41Z" id="220567624">Hi Clinton,

Given that the aim is to remove the transport client, does that mean that java clients will only be able to use the Node client?

Will this have the same issues with the network separation? (i.e. one network for data node interconnect connections and another for client connections).

Given that one of the stated advantages of using the Transport Client over the Node Client is that it's more adept at handling large numbers of connections, does this mean that the node client is going to be refactored? (although that page I got this information from states that it's currently being reworked :)

Thanks in advance,

Andy
</comment><comment author="javanna" created="2016-05-20T10:13:45Z" id="220568670">Hi @alehane , 
as Clint said our goal is to remove the transport client entirely and to expose just the HTTP interface. That means that the node client will no longer be exposed either. In fact the whole java api will be replaced with a new REST client whici will become the only way to connect to elasticsearch in Java, as it already is the case with any other language. See #7743 .
</comment><comment author="alehane" created="2016-05-20T11:03:20Z" id="220577871">Hi @javanna, 

Apologies, I read that initially, then starting doing a little more research and completely forgot that statement!

So, will elastic provide a java client, which uses the underlying REST interface but looks like the existing java api?

We make heavy usage of the java api and this would mean that we would need to make changes to our application. It would also have implications on any possible migration path, Although neither of these issues are insurmountable, they are inconvenient and they do present us with cost and planning implications.

We would also need to figure out how this would work with our custom shield realm, where the we use the java api to pass a kerberos style token and optional environment id to elastic (although, I assume this would be via the http headers) for authorisation and authentication.

Cheers,

Andy
</comment><comment author="javanna" created="2016-05-23T14:13:54Z" id="220992054">Hi @alehane we will take all this into account but changes will for sure be required to migrate. Yet looking at all the changes to keep up with when using the java api (due to the fact that we use it internally), that would be the last time that we require changes, cause the REST layer comes with much stronger backwards compatibility guarantees.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow adding additional child types that point to an existing parent type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18446</link><project id="" key="" /><description>From 2.0 adding child types to existing types was forbidden because the`_parent` field stores the join between parent and child at index time. This is to protect from the fact that types that weren't a parent before become a parent while previously indexed documents would not have a join field. This would break the parent/child feature.

The restriction was a bit too strict in the sense that also if a type was a parent type the restriction would also forbid adding child types that point to a parent type (so child types already point to it). This change make sure that the restriction only applies if that type isn't a parent type already.

PR for #17956
</description><key id="155516318">18446</key><summary>Allow adding additional child types that point to an existing parent type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>enhancement</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T14:30:25Z</created><updated>2016-05-19T09:36:13Z</updated><resolved>2016-05-19T09:36:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-18T19:13:54Z" id="220129008">LGTM, one minor suggestion
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enabling http.compression breaks cors on ES 2.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18445</link><project id="" key="" /><description>Hi guys,

ES 2.3.2 with Oracle Java 1.8.0_91 on Ubuntu 14.04 amd64 here.

I have the following in my `elasticsearch.yaml`:

```
http.cors.enabled: true
http.cors.allow-origin : "*"
http.cors.allow-methods : OPTIONS, HEAD, GET, POST, PUT, DELETE

http.compression: true
```

The following curl OPTIONS request against ES fails:

```
$ curl -vv 'http://localhost:9200/volume_groups_demo/doc/_search?q=k2_id:5583&amp;sort=name' -X OPTIONS -H 'Access-Control-Request-Method: POST' -H 'Origin: http://localhost:9000' -H 'Referer: http://localhost:9000/' -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'
* Hostname was NOT found in DNS cache
*   Trying 127.0.0.1...
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; OPTIONS /volume_groups_demo/doc/_search?q=k2_id:5583&amp;sort=name HTTP/1.1
&gt; Host: localhost:9200
&gt; Accept: */*
&gt; Access-Control-Request-Method: POST
&gt; Origin: http://localhost:9000
&gt; Referer: http://localhost:9000/
&gt; User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36
&gt; 
* Empty reply from server
* Connection #0 to host localhost left intact
curl: (52) Empty reply from server
```

**However**, if I disable `http.compression` - it works just fine:

```
$ curl -i 'http://localhost:9200/volume_groups_demo/doc/_search?q=k2_id:5583&amp;sort=name' -X OPTIONS -H 'Access-Control-Request-Method: POST' -H 'Origin: http://localhost:9000' -H 'Referer: http://localhost:9000/' -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'
HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: HEAD
Access-Control-Allow-Methods: DELETE
Access-Control-Allow-Methods: POST
Access-Control-Allow-Methods: GET
Access-Control-Allow-Methods: OPTIONS
Access-Control-Allow-Methods: PUT
Access-Control-Allow-Headers: X-Requested-With
Access-Control-Allow-Headers: Content-Length
Access-Control-Allow-Headers: Content-Type
Access-Control-Max-Age: 1728000
date: Wed, 18 May 2016 12:16:10 GMT
content-length: 0
```

The failing request issues the below exception in ES logs:

```
[2016-05-18 15:19:13,016][WARN ][http.netty               ] [Rintrah] Caught exception while handling client http traffic, closing connection [id: 0x68e7081e, /127.0.0.1:49883 =&gt; /127.0.0.1:9200]
java.lang.IllegalStateException: cannot send more responses than requests
        at org.jboss.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:101)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:105)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.jboss.netty.channel.Channels.write(Channels.java:704)
        at org.jboss.netty.channel.Channels.write(Channels.java:671)
        at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:348)
        at org.elasticsearch.http.netty.cors.CorsHandler.handlePreflight(CorsHandler.java:123)
        at org.elasticsearch.http.netty.cors.CorsHandler.messageReceived(CorsHandler.java:80)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="155509907">18445</key><summary>Enabling http.compression breaks cors on ES 2.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haizaar</reporter><labels /><created>2016-05-18T14:05:11Z</created><updated>2016-05-18T14:51:08Z</updated><resolved>2016-05-18T14:25:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-18T14:25:38Z" id="220042635">Duplicates #18089
</comment><comment author="haizaar" created="2016-05-18T14:51:07Z" id="220050927">Great! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>All-numeric query prefix causes NullPointerException with SimpleQueryAnalyzer and analyze_wildcard: true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18444</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2 (also seen in 2.2.0)

**JVM version**: OpenJDK Runtime Environment (IcedTea 2.6.6) (7u101-2.6.6-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)

**OS version**: Ubuntu 14.04.4 LTS

**Description of the problem including expected versus actual behavior**:
When issuing a `simple_query_string` query containing a numeric-prefix term (e.g. "1234*") against a field that uses the `SimpleQueryAnalyzer` using `analyze_wildcard: true`, Elasticsearch throws a `NullPointerException`.

**Steps to reproduce**:

```
#!/bin/bash

export ELASTICSEARCH_ENDPOINT="http://localhost:9200"

# Create indexes

curl -XPUT "$ELASTICSEARCH_ENDPOINT/play" -d '{
    "mappings": {
        "type": {
            "properties": {
                "prop1": {
                    "type": "string",
                    "analyzer": "simple"
                },
                "prop2": {
                    "type": "string",
                    "analyzer": "whitespace"
                }
            }
        }
    }
}'

# Index documents
curl -XPOST "$ELASTICSEARCH_ENDPOINT/_bulk?refresh=true" -d '
{"index":{"_index":"play","_type":"type"}}
{"prop1":"value1","prop2":"value2"}
'

# Do searches

curl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '
{
    "query": {
        "simple_query_string": {
            "query": "1234*",
            "fields": [
                "prop1"
            ],
            "analyze_wildcard": true
        }
    }
}
'

curl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '
{
    "query": {
        "simple_query_string": {
            "query": "1234*",
            "fields": [
                "prop2"
            ],
            "analyze_wildcard": true
        }
    }
}
'
```

**Provide logs (if relevant)**:

```
[2016-05-18 14:20:53,093][DEBUG][action.search.type       ] [Amphibius] [play2][4], node[HgT_hAH2RC6wTntYZy7rPQ], [P], v[2], s[STARTED], a[id=L9qtfcf9SyyVKb-HZxap9A]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7873a5ad] lastShard [true]
RemoteTransportException[[Amphibius][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [
{
    "query": {
        "simple_query_string": {
            "query": "1234*",
            "fields": [
                "prop1"
            ],
            "analyze_wildcard": true
        }
    }
}
]]; nested: NullPointerException;
Caused by: SearchParseException[failed to parse search source [
{
    "query": {
        "simple_query_string": {
            "query": "1234*",
            "fields": [
                "prop1"
            ],
            "analyze_wildcard": true
        }
    }
}
]]; nested: NullPointerException;
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:853)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:652)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:618)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:369)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.query.SimpleQueryParser.newPrefixQuery(SimpleQueryParser.java:131)
        at org.apache.lucene.queryparser.simple.SimpleQueryParser.consumeToken(SimpleQueryParser.java:406)
        at org.apache.lucene.queryparser.simple.SimpleQueryParser.parseSubQuery(SimpleQueryParser.java:212)
        at org.apache.lucene.queryparser.simple.SimpleQueryParser.parse(SimpleQueryParser.java:152)
        at org.elasticsearch.index.query.SimpleQueryStringParser.parse(SimpleQueryStringParser.java:212)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:256)
        at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:303)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:206)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:201)
        at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:836)
        ... 10 more
```
</description><key id="155501727">18444</key><summary>All-numeric query prefix causes NullPointerException with SimpleQueryAnalyzer and analyze_wildcard: true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jamestait</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-05-18T13:30:48Z</created><updated>2016-05-18T14:44:26Z</updated><resolved>2016-05-18T14:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jamestait" created="2016-05-18T13:38:45Z" id="220028445">Also worth noting that floating-point numbers exhibit the same failure.
</comment><comment author="dakrone" created="2016-05-18T14:44:24Z" id="220048667">@jamestait this is actually caused by the same thing as https://github.com/elastic/elasticsearch/issues/18202 and is already fixed, it'll be out in the 2.3.3 release
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing builder.endObject() in FsInfo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18443</link><project id="" key="" /><description>This pull request fix a small regression introduced by #15915

Closes #18433 

@ywelsch Can you have a look please? Thanks
</description><key id="155499776">18443</key><summary>Add missing builder.endObject() in FsInfo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T13:21:47Z</created><updated>2016-05-18T13:41:31Z</updated><resolved>2016-05-18T13:40:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-18T13:25:12Z" id="220024764">LGTM, and sorry!
</comment><comment author="tlrx" created="2016-05-18T13:41:31Z" id="220029176">@jasontedor Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add CONSOLE to from/size docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18442</link><project id="" key="" /><description>Trivial change, other trivial CONSOLE additions see #18441

Relates to #18160
</description><key id="155498563">18442</key><summary>Add CONSOLE to from/size docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels /><created>2016-05-18T13:16:09Z</created><updated>2016-05-18T16:38:41Z</updated><resolved>2016-05-18T16:38:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-18T15:36:09Z" id="220066030">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs/add console to search request options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18441</link><project id="" key="" /><description>Add CONSOLE to several smaller search request docs.

Relates to #18160
</description><key id="155498250">18441</key><summary>Docs/add console to search request options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T13:14:39Z</created><updated>2016-05-19T07:54:44Z</updated><resolved>2016-05-19T07:54:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-18T18:33:23Z" id="220117909">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix parsing single `rescore` element in SearchSourceBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18440</link><project id="" key="" /><description>We are currently only parsing the array-syntax for the rescore part in SearchSourceBuilder (`"rescore" : [ {...}, {...} ]`) . 
We also used to support `"rescore" : {...}`, so adding this back to the parsing in SearchSourceBuilder.

Closes #18439
</description><key id="155490562">18440</key><summary>Fix parsing single `rescore` element in SearchSourceBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Search</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T12:37:34Z</created><updated>2016-05-19T09:39:12Z</updated><resolved>2016-05-18T13:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-18T12:39:06Z" id="220013408">@MaineC can you take a look at this and try this change with the query you used in #18439? I added a simplified version to the json parsing tests that I think will also work for the issue you reported.
</comment><comment author="MaineC" created="2016-05-18T13:03:07Z" id="220019057">LGTM - thanks for fixing
</comment><comment author="nik9000" created="2016-05-18T18:08:27Z" id="220110943">Should `SourceSoureBuilder#toXContent` spit out just the rescore object if there is only a single rescore? That is what we expect users to do when they build a rescore.
</comment><comment author="cbuescher" created="2016-05-19T09:39:12Z" id="220275818">@nik9000 I think the reason we didn't do this (and other similar things like writing default values even if they haven' been explicitely set by the user) so far is to avoid having different query rendering logic. In this case it would have helped with test coverage (no need for the explicit json tests added with this PR) but in the end the array syntax with one element is just parsed to the same object again. Given that `toXContent` should only be used in roundtriup tests and for logging I think we can leave that for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rescoring as documented in reference guide broken on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18439</link><project id="" key="" /><description>**Elasticsearch version**: 5.x/ current master

**JVM version**: Oracle Corporation 1.8.0_60 [Java HotSpot(TM) 64-Bit Server VM 25.60-b23]

**OS version**: Fedora release 23 (Twenty Three)

**Description of the problem including expected versus actual behavior**:

Running the rescore query as documented against a current master built fails to parse the query json. See #18424 for original discussion.

**Steps to reproduce**:
1. Clone repository, build the source and start elasticsearch
2. Submit the following query:
   
   `GET /_search{`
   
   ```
   "query" : {
         "match" : {
             "field1" : {
                 "operator" : "or",
                 "query" : "the quick brown",
                 "type" : "boolean"
              }
           }
        },
       "rescore" : {
         "window_size" : 50,
         "query" : {
            "rescore_query" : {
             "match" : {
                "field1" : {
                   "query" : "the quick brown",
                   "type" : "phrase",
                   "slop" : 2
                }
             }
          },
          "query_weight" : 0.7,
          "rescore_query_weight" : 1.2
       }}}
   ```

Which essentially is a plain copy of the example here: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-rescore.html

Error message I get:

```
{
   "error": {
      "root_cause": [
         {
            "type": "parsing_exception",
            "reason": "Unknown key for a START_OBJECT in [rescore].",
            "line": 11,
            "col": 16
         }
      ],
      "type": "parsing_exception",
      "reason": "Unknown key for a START_OBJECT in [rescore].",
      "line": 11,
      "col": 16
   },
   "status": 400
{
   "error": {
      "root_cause": [
         {
            "type": "parsing_exception",
            "reason": "Unknown key for a START_OBJECT in [rescore].",
            "line": 11,
            "col": 16
         }
      ],
      "type": "parsing_exception",
      "reason": "Unknown key for a START_OBJECT in [rescore].",
      "line": 11,
      "col": 16
   },
   "status": 400
}
```

Found thanks to the lovely doc snippet testing by @nik9000 - hunch about what's going wrong thanks to analysis by @cbuescher 
</description><key id="155483921">18439</key><summary>Rescoring as documented in reference guide broken on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T12:01:31Z</created><updated>2016-05-18T13:17:32Z</updated><resolved>2016-05-18T13:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-18T12:24:03Z" id="220010104">We are currently only parsing the array-syntax for the `rescore` part in SearchSourceBuilder (`"rescore" : [ {...}, {...} ]`) . We also need to support `"rescore" :  {...}`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the index-too-old exception more explicit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18438</link><project id="" key="" /><description>When starting Elasticsearch 5.x with an index created before ES  2, the exception now reads:

```
The index [[foo]] was created before v2.0.0.beta1. It should be reindexed in Elasticsearch 2.x before upgrading to 5.0.0.
```
</description><key id="155476868">18438</key><summary>Make the index-too-old exception more explicit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T11:20:22Z</created><updated>2016-05-18T11:33:50Z</updated><resolved>2016-05-18T11:33:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-18T11:29:22Z" id="219999072">LGTM
</comment><comment author="clintongormley" created="2016-05-18T11:33:50Z" id="219999997">thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] test command line options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18437</link><project id="" key="" /><description>This commit adds a test for command line options -p, --version and --help

closes #16129
</description><key id="155468671">18437</key><summary>[TEST] test command line options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label></labels><created>2016-05-18T10:34:15Z</created><updated>2016-05-20T08:33:39Z</updated><resolved>2016-05-19T18:42:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-18T18:36:21Z" id="220118728">I started looking at this this morning before going out for the morning and I'm just getting back to it. I get that testing these in groovy makes sense because we have all the dependencies, but I think this is usually something we want to do in junit tests, right? I mean, should we do it there? Do we just do it in groovy because it is easier there? I'm pretty ok with that but I want to be sure I'm not missing something.
</comment><comment author="rjernst" created="2016-05-18T18:41:48Z" id="220120280">This is a lot of duplication of existing infrastructure just to test output (which should be done in unit tests). In fact, we already have the ability, and do, test these in unit tests. And our integration tests start elasticsearch (and the plugin cli), so I'm not seeing the benefit of this added complexity to the build?
</comment><comment author="nik9000" created="2016-05-18T18:47:32Z" id="220121802">&gt; And our integration tests start elasticsearch (and the plugin cli), so I'm not seeing the benefit of this added complexity to the build?

Right. They use the pidfile pretty extensively irrc. We probably don't need to test that again? Is it that we aren't confident that the unit tests will catch errors with stuff like --version and --help? We have the vagrant tests for lots of that kind of thing but you are right that they don't cover windows at all.
</comment><comment author="rjernst" created="2016-05-18T19:02:37Z" id="220125967">&gt; you are right that they don't cover windows at all.

Can't we have windows vagrant images?
</comment><comment author="nik9000" created="2016-05-18T19:09:37Z" id="220127833">&gt; Can't we have windows vagrant images?

Maybe? I don't understand the licensing and I'm afraid to ask. Even if we did we'd have to jump through a zillion hoops because bats is, well, bash.
</comment><comment author="rjernst" created="2016-05-18T19:15:02Z" id="220129290">&gt; bats is, well, bash.

I think we could restructure the vagrant test infra to allow running something other than bash. Really it is just "ssh to the vagrant image and run something".
</comment><comment author="nik9000" created="2016-05-18T19:22:53Z" id="220131232">Sure! But maybe we're making the perfect the enemy of the good? I mean, if we have time to solve the windows vm problem now then we should do it, but maybe testing these parameters at all in windows is important and maybe something like this is the way to go until we do have the time? Or maybe just running the build with its use of `-p` on windows machines is good enough and we don't need to do any of this?
</comment><comment author="brwe" created="2016-05-19T09:39:13Z" id="220275824">I barely understand the discussion but will try to answer anyway.

&gt; In fact, we already have the ability, and do, test these in unit tests.

I added this test to check that the startup scripts `elasticsearch.bat` and `elasticsearch`  work when we add --help -p pid etc. and that they do not mess up parameter order, see https://github.com/elastic/elasticsearch/pull/15320 Sorry, I was not aware that we have unit test for the startup scripts. Can you point me to them?

&gt;  And our integration tests start elasticsearch (and the plugin cli), so I'm not seeing the benefit of this added complexity to the build?

We do not seem to use the -p option there. Instead we write that in the config if I understand correctly.

&gt; Or maybe just running the build with its use of -p on windows machines is good enough and we don't need to do any of this?

That is what I did on 2.x via hacking the various ant scripts. But I never liked it because then these options are just tested implicitly. I would have preferred a dedicated test on 2.x already but thought at least on master I make it right. 

&gt; I mean, if we have time to solve the windows vm problem now then we should do it

Definitely! I will try and find out where we are with the windows vms. I know there was some discussion about that. Will come back with an update. If that is possible I'd rather have a vagrant test.

Again, if I was just missing some smarter unit test option let me know. 
</comment><comment author="brwe" created="2016-05-19T18:42:31Z" id="220415918">We discussed and came to the conclusion it would be better to invest in a proper vagrant test like we do with other systems and then move this test to vagrant tests. It would make more sense because issues on windows pop up every now and then and this test is just one of many we need to implement anyway and we probably don't want to do all with gradle.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Azure repository does not remove temporary dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18436</link><project id="" key="" /><description>**Elasticsearch version**: tested with 2.4.0-SNAPSHOT

**Description of the problem including expected versus actual behavior**: 

When you create your azure repository, you get a message `The specified blob does not exist`

**Steps to reproduce**:
1. `bin/plugin install cloud-azure`
2. modify `config/elasticsearch.yml` to set `cloud.azure.storage.my_account.account: xxx` and `cloud.azure.storage.my_account.key: yyy`
3. `bin/elasticsearch`
4. Run the following script:

``` sh
# Clean existing repo if any
curl -XDELETE localhost:9200/_snapshot/my_backup1?pretty

# Create azure repo
# This call gives in log: The specified blob does not exist
curl -XPUT localhost:9200/_snapshot/my_backup1?pretty -d '{
  "type": "azure"
}'
```

**Provide logs (if relevant)**:

```
[2016-05-18 11:03:24,914][WARN ][org.elasticsearch.cloud.azure.blobstore] [azure] can not remove [tests-ilmRPJ8URU-sh18yj38O6g/] in container {elasticsearch-snapshots}: The specified blob does not exist.
```

Note that there is a trailing `/` at the end which might explain the issue.
</description><key id="155456220">18436</key><summary>Azure repository does not remove temporary dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>bug</label></labels><created>2016-05-18T09:32:52Z</created><updated>2016-05-27T15:28:56Z</updated><resolved>2016-05-27T15:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-25T09:23:48Z" id="221519808">This has been fixed in master but I'm reopening as it now needs to be backported in 2.x, 2.3 and for elasticsearch 1.7.
</comment><comment author="dadoonet" created="2016-05-27T15:28:56Z" id="222177240">Closed with:
- master (5.0.0): elastic/elasticsearch#18451 
- 2.x (2.4.0): elastic/elasticsearch#18571
- 1.7 (2.8.4 - old versioning): https://github.com/elastic/elasticsearch-cloud-azure/pull/117
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log IndexShard.refresh logs under trace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18435</link><project id="" key="" /><description>We log them every second...
</description><key id="155456036">18435</key><summary>Log IndexShard.refresh logs under trace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T09:31:53Z</created><updated>2016-05-19T15:12:37Z</updated><resolved>2016-05-19T15:12:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-18T10:22:35Z" id="219986069">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove percolator cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18434</link><project id="" key="" /><description>Before 5.0 it was required that the percolator queries were cached in jvm heap as Lucene queries for two reasons:
- Performance. The percolator evaluated all percolator queries all the time. There was no pre-selecting queries optimization that selects candidate matches like we have today.
- Updates made to percolator queries were visible in realtime, today these changes are visible in near realtime. So updating no longer requires the percolator to have the queries in jvm heap.

So having the percolator queries in jvm heap via the percolator cache is now less attractive. Especially when there are many percolator queries then these queries can consume many GBs of jvm heap. Removing the percolator cache does make the percolate query slower compared to how the execution time in 5.0.0-alpha1 and alpha2, but it is still significantly faster compared to 2.x and before.

I think removing the percolator cache is the right tradeoff. I've been running some tests with an index that contains ~35M queries. With this change this change the used heap space is ~250MB and without 24GB. The query time slowdown compared with this change depends on how many candidate queries need to be evaluated if they actually match. With some test queries I have seen a slow down up to 5 times. However if query time is compared with how things work in 2.x and before then query time is still significantly better with this change.

Also if this gets merged we can improve other things, so that the slowdown introduced by this change get smaller:
- For percolator queries that are a disjunction query we know that they will match if they are are a candidate match. So there is no need to evaluate these queries by the MemoryIndex.
- Add range query extract logic, so that we can intelligently pre-select percolator queries that contain range queries.
- Right now we store the xcontent representation of the percolator queries. We can instead store the binary representation which improve query time (parsing logic takes less time) for this change at the cost of bwc for stored percolator queries. The binary format of query builders has bwc within a major version as apposed for across a major version for the xcontent format. So that isn't too bad and with the reindex api, updating the percolator queries is now much more pleasant. So this could be explored as well.

Note the breaking part is the removal of percolator related statistics from the stats and cat APIs that are no longer needed now.
</description><key id="155449936">18434</key><summary>Remove percolator cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T09:00:27Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-05-20T12:54:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-18T11:48:05Z" id="220002729">&gt; With this change this change the used heap space is ~250MB and without 24GB.

Wow!
</comment><comment author="jpountz" created="2016-05-18T21:12:20Z" id="220159747">&gt; For percolator queries that are a conjunction query we know that they will match if they are are a candidate match.

I think you meant disjunction?

I just realized something else that we could easily improve: if the query is for instance `+name:martijn #age:[20 TO 40]`, then we currently fail the extraction since range queries are unsupported. However it would be correct to extract `name:martijn`: maybe we should improve term extraction fo catch `UnsupportedQueryException`s in the case of conjunctions, and only rethrow if no clause could be extracted?

Back to this PR: I agree this is the right direction and we should work on improving query pre-selection rather than making verification faster at the expense of memory usage. Maybe index sorting could also help if visiting all matches is not required.

Using the binary representation of queries is indeed appealing. Hopefully this will come for free if we ever support running clusters with mixed major versions.
</comment><comment author="nik9000" created="2016-05-18T21:28:53Z" id="220163832">&gt; I've been running some tests with an index that contains ~35M queries. With this change this change the used heap space is ~250MB and without 24GB. The query time slowdown compared with this change depends on how many candidate queries need to be evaluated if they actually match. With some test queries I have seen a slow down up to 5 times.

These are both huge swings.

I wonder if it makes sense to use a cache with a limited size rather than pre-building all the queries? You wouldn't pay the cost of building queries that are frequently candidates. It might just be an exercise in needless bit shuffling if the hit rate isn't good though.
</comment><comment author="martijnvg" created="2016-05-18T21:29:36Z" id="220164013">&gt; I think you meant disjunction?

Oops, yes. Fixed the description.

&gt; maybe we should improve term extraction fo catch UnsupportedQueryExceptions in the case of conjunctions, and only rethrow if no clause could be extracted?

Good point. This enhancement can be added via this PR, since it is a small change.

&gt; Using the binary representation of queries is indeed appealing. Hopefully this will come for free if we ever support running clusters with mixed major versions.

I'm hesitant to change to use the binary format, because we don't support this. So I think we should stick with using the xcontent format for now. I think bigger wins can be made improving the query preselecting better then changing the to use the binary format for storage.
</comment><comment author="martijnvg" created="2016-05-18T21:38:54Z" id="220166185">&gt; I wonder if it makes sense to use a cache with a limited size rather than pre-building all the queries? You wouldn't pay the cost of building queries that are frequently candidates. It might just be an exercise in needless bit shuffling if the hit rate isn't good though.

I think a limited sized cache won't help that much, since there will always be misses (lets say we limit this cache to 5% of the available heap space). I think if we take a look at entire percolator refactoring effort, the execution time improvements compared to 2.x are so large that this trade off for using the percolator with far less memory is justified.
</comment><comment author="nik9000" created="2016-05-18T21:40:59Z" id="220166701">&gt; since there will always be misses

Makes sense to me.
</comment><comment author="jpountz" created="2016-05-18T21:41:11Z" id="220166735">&gt; Good point. This enhancement can be added via this PR, since it is a small change.

Maybe a different PR would be better since this PR might be controversial, while the discussed improvement should not be?

&gt; I'm hesitant to change to use the binary format, because we don't support this. So I think we should stick with using the xcontent format for now. I think bigger wins can be made improving the query preselecting better then changing the to use the binary format for storage.

+1
</comment><comment author="martijnvg" created="2016-05-18T21:43:21Z" id="220167269">&gt; Maybe a different PR would be better since this PR might be controversial, while the discussed improvement should not be?

True, lets make this change in a different PR. This improvement doesn't depend on this PR at all.
</comment><comment author="jpountz" created="2016-05-18T22:21:51Z" id="220175844">I can handle the review, but I would like @s1monw and @clintongormley to confirm this is the direction we want to take as this change has important implications.
</comment><comment author="clintongormley" created="2016-05-19T12:07:41Z" id="220305183">I think removing the cache in favour of improved execution is the right direction to go. 

&gt; I'm hesitant to change to use the binary format, because we don't support this. So I think we should stick with using the xcontent format for now. I think bigger wins can be made improving the query preselecting better then changing the to use the binary format for storage.

I agree - this part is trickier.  It did occur to me that we could (a) store the binary format and (b) fall back to rebuilding the query from source if it is not parseable on a later version (where a reindex would rebuild it in the right format).  but agreed, this is more controversial and should be discussed on a different ticket.
</comment><comment author="jpountz" created="2016-05-19T13:59:03Z" id="220332466">I left some comments about corner cases but otherwise LGTM
</comment><comment author="martijnvg" created="2016-05-20T12:54:16Z" id="220597671">Pushed via: 80fee8666ff5dd61ba29b175857cf42ce3b9eab9
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: Failing RestIT on master (nodes.stats/30_discovery/Discovery stats) </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18433</link><project id="" key="" /><description>This Rest test started failing a few hours ago, I have seen it fail several times on master branch:

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/415/console
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/844/console
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/583/console

```
:distribution:integ-test-zip:integTest
Executing task ':distribution:integ-test-zip:integTest' (up-to-date check took 0.001 secs) due to:
  Task.upToDateWhen is false.
[ant:junit4] &lt;JUnit4&gt; says kaixo! Master seed: D34B10548736FEA
==&gt; Test Info: seed=D34B10548736FEA; jvm=1; suite=1
Started J0 PID(342@slave-886fdd4f.build.us-west-2a.elasticnet.co).
Suite: org.elasticsearch.test.rest.RestIT
IGNOR/A 0.01s | RestIT.test {p0=update/85_fields_meta/Metadata Fields}
   &gt; Assumption #1: [update/85_fields_meta/Metadata Fields] skipped, reason: [Update doesn't return metadata fields, waiting for #3259] 
  2&gt; REPRODUCE WITH: gradle :distribution:integ-test-zip:integTest -Dtests.seed=D34B10548736FEA -Dtests.class=org.elasticsearch.test.rest.RestIT -Dtests.method="test {p0=nodes.stats/30_discovery/Discovery stats}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.jvm.argline="-XX:-UseParNewGC -XX:-UseConcMarkSweepGC -XX:+UseG1GC" -Dtests.locale=el-CY -Dtests.timezone=America/Chihuahua
FAILURE 0.01s | RestIT.test {p0=nodes.stats/30_discovery/Discovery stats} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: field [nodes] doesn't have a true value
   &gt; Expected: not null
   &gt;      but: was null
   &gt;    at __randomizedtesting.SeedInfo.seed([D34B10548736FEA:85608EDFE68F0212]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.rest.section.IsTrueAssertion.doAssert(IsTrueAssertion.java:48)
   &gt;    at org.elasticsearch.test.rest.section.Assertion.execute(Assertion.java:69)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:399)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-05-18 04:56:46,232][ERROR][org.elasticsearch.test.rest.client] Adding header Accept
  1&gt;  with value application/yaml
IGNOR/A 0.00s | RestIT.test {p0=indices.put_settings/all_path_options/put settings in list of indices}
   &gt; Assumption #1: [indices.put_settings/all_path_options/put settings in list of indices] skipped, reason: [list of indices not implemented yet] 
  1&gt; [2016-05-18 04:57:21,422][ERROR][org.elasticsearch.test.rest.client] Adding header Accept
  1&gt;  with value application/yaml
  2&gt; NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+master+g1gc/distribution/integ-test-zip/build/testrun/integTest/J0/temp/org.elasticsearch.test.rest.RestIT_D34B10548736FEA-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60), sim=RandomSimilarity(queryNorm=true,coord=no): {}, locale=el-CY, timezone=America/Chihuahua
  2&gt; NOTE: Linux 2.6.32-573.el6.x86_64 amd64/Oracle Corporation 1.8.0_71 (64-bit)/cpus=4,threads=1,free=275366168,total=536870912
  2&gt; NOTE: All tests run in this JVM: [RestIT]
Completed [1/1] in 73.34s, 519 tests, 1 failure, 2 skipped &lt;&lt;&lt; FAILURES!

Tests with failures:
  - org.elasticsearch.test.rest.RestIT.test {p0=nodes.stats/30_discovery/Discovery stats}
```

Also reproduces locally when I run `gradle clean check` in `distribution/integ-test-zip`, but cannot reproduce with the above "REPRODUCE WITH" line.
</description><key id="155441859">18433</key><summary>CI: Failing RestIT on master (nodes.stats/30_discovery/Discovery stats) </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Discovery</label><label>jenkins</label><label>test</label></labels><created>2016-05-18T08:17:05Z</created><updated>2016-05-19T13:45:15Z</updated><resolved>2016-05-18T13:40:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-18T11:11:50Z" id="219995626">I temporarily disabled the test with de321fb159f4e9f764d3fe6f1c03c664bf15b384.
</comment><comment author="clintongormley" created="2016-05-18T12:03:25Z" id="220005880">@bleskes @ywelsch this is to do with node/stats/discovery - something you've touched recently?
</comment><comment author="bleskes" created="2016-05-19T13:29:53Z" id="220324182">@tlrx thx for fixing. I wonder if we need to add something to our rest failure messages/reporting as I know  @cbuescher spent some time trying to figure things out. Outputting/incorrect/unbalanced JSON should have been obvious...  How did you end up finding it?
</comment><comment author="tlrx" created="2016-05-19T13:45:15Z" id="220328420">@bleskes I reproduced the test in DEBUG mode so that it prints the JSON response, which showed that the `nodes` fields was missing (as stated in the test failure message). 

The thing to know with `filter_path` expressions like `nodes.*.discovery` is that the parent fields corresponding to `nodes` and `*` are written in the response only if at least one child field matches. So `nodes` is written only if `nodes.ABC.discovery` exists. So I ran a node and checks the ouput of `GET /_nodes/stats` and saw that Discovery stats were located in `nodes.ABC.fs.discovery` meaning that something messed up the object hierarchy.

I think we could detect malformed objects built with `XContentBuilder` and throws an exception. I'll try to have a look at this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clear all caches after testing parent breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18432</link><project id="" key="" /><description>With this commit we clear all caches after testing the parent circuit breaker.
This is necessary as caches hold on to circuit breakers internally. Additionally,
due to usage of `CircuitBreaker#addWithoutBreaking()` in caches, it's even possible
to go above the limit. As a consequence, all subsequent requests fall victim to
the limit.

Hence, right after the parent circuit breaker tripped, we clear all caches to
reduce these circuit breakers to 0 again. We also exclude the clear caches
transport request from limit check in order to ensure it will succeed. As this is
typically a very small and low-volume request, it is deemed ok to exclude it.

Closes #18325
</description><key id="155434358">18432</key><summary>Clear all caches after testing parent breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>test</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T07:32:25Z</created><updated>2016-05-26T11:42:18Z</updated><resolved>2016-05-18T13:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-18T11:11:18Z" id="219995519">LGTM. I'm not sure what is up with the CLA check. Usually for folks who've contributed before it comes from a bad email address on the patch but I dunno.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unable to install the logstash-filter-grok plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18431</link><project id="" key="" /><description>when I install the grok,it shows this error:
`ERROR: Installation Aborted, message: Bundler could not find compatible versions for gem "logstash-core-plugin-api":
In snapshot (Gemfile.lock):
logstash-core-plugin-api (= 1.10.0)

In Gemfile:
logstash-filter-geoip (&gt;= 0) java depends on
logstash-core-plugin-api (~&gt; 1.0) java
logstash-filter-geoip (&gt;= 0) java depends on
  logstash-core-plugin-api (~&gt; 1.0) java

logstash-filter-geoip (&gt;= 0) java depends on
  logstash-core-plugin-api (~&gt; 1.0) java

logstash-filter-geoip (&gt;= 0) java depends on
  logstash-core-plugin-api (~&gt; 1.0) java

logstash-filter-geoip (&gt;= 0) java depends on
  logstash-core-plugin-api (~&gt; 1.0) java

logstash-filter-grok (= 3.0.1) java depends on
  logstash-core-plugin-api (~&gt; 2.0) java

logstash-core-plugin-api (= 1.10.0) java
Running bundle update will rebuild your snapshot from scratch, using only
the gems in your Gemfile, which may resolve the conflict.
`

Anyone have any ideas ?
Thanks !
</description><key id="155409639">18431</key><summary>unable to install the logstash-filter-grok plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">superhobbit</reporter><labels /><created>2016-05-18T03:30:46Z</created><updated>2016-05-18T11:42:19Z</updated><resolved>2016-05-18T11:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-18T11:42:19Z" id="220001590">Please open this issue on the logstash issues list
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert search-after tests to // CONSOLE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18430</link><project id="" key="" /><description>Relates to #18160
</description><key id="155404576">18430</key><summary>Convert search-after tests to // CONSOLE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-18T02:37:22Z</created><updated>2016-05-18T10:42:28Z</updated><resolved>2016-05-18T10:42:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-18T02:37:32Z" id="219910090">Ping @jimferenczi and @MaineC to have a look?
</comment><comment author="jimczi" created="2016-05-18T07:01:14Z" id="219942136">LGTM, thank you for fixing this.
</comment><comment author="MaineC" created="2016-05-18T07:52:38Z" id="219952383">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Platform enablement request: Please add s390x to the supported platforms and build targets.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18429</link><project id="" key="" /><description /><key id="155379785">18429</key><summary>Platform enablement request: Please add s390x to the supported platforms and build targets.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vmorris</reporter><labels /><created>2016-05-17T22:45:23Z</created><updated>2016-05-18T13:36:06Z</updated><resolved>2016-05-18T11:16:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-18T11:16:42Z" id="219996568">Sorry but we don't have the hardware to test on, so we can't support it. On top of that, there isn't an OpenJDK based JVM for s390x
</comment><comment author="vmorris" created="2016-05-18T13:35:46Z" id="220027631">@clintongormley There are a few ways developers can gain access to s390x hardware for free or very little cost. One is through the [IBM Dallas Innovation Center](http://dtsc.dfw.ibm.com/MVSDS/%27HTTPD2.DSN01.PUBLIC.SHTML%28LNXTD%29%27).
Another is through the [LinuxONE Community Cloud](https://developer.ibm.com/linuxone/).

There are OpenJDK based JVM installation packages for RHEL, SUSE, and Ubuntu. Did you check?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove if/else ANTLR ambiguity.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18428</link><project id="" key="" /><description>Same as the title.
</description><key id="155378281">18428</key><summary>Remove if/else ANTLR ambiguity.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T22:34:22Z</created><updated>2016-05-17T23:41:35Z</updated><resolved>2016-05-17T23:41:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-17T22:42:19Z" id="219876080">I see this in my "make antlr picky branch" (https://github.com/rmuir/elasticsearch/tree/make_antlr_picky) as this:

```
2&gt; REPRODUCE WITH: gradle :modules:lang-painless:test -Dtests.seed=F88776F17F5FFFDB -Dtests.class=org.elasticsearch.painless.BasicStatementTests -Dtests.method="testIfStatement" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=ro -Dtests.timezone=Australia/North
FAILURE 0.02s | BasicStatementTests.testIfStatement &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: line: 1, offset: 33, symbol:[@14,33:36='else',&lt;13&gt;,1:33] reportAttemptingFullContext d=1 (statement), input='else'
```

I think we need two rules but the recursion needs to be constrained so the with-else case can't then recurse back to without-else
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"newbie mode" in API reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18427</link><project id="" key="" /><description>**Describe the feature**:

it would be great if the API documentation (the stuff on the web) had a "newbie" mode that would show query components within the context of a complete query.  for example, on https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-common-terms-query.html, there are examples of the "common" block, but it's not clear (to me at least) where the "common" block fits in the query.  

I realize that adding that information to every page would be noisy for experienced users, so I'd suggest a way for newbies to say "show me this in the context of the ENTIRE query".  That would also be helpful as a learning tool, because by seeing lots of different examples, the newbie learns the structure of queries by osmosis.
</description><key id="155374694">18427</key><summary>"newbie mode" in API reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ccurvey</reporter><labels /><created>2016-05-17T22:12:37Z</created><updated>2016-05-18T01:31:44Z</updated><resolved>2016-05-18T01:31:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-18T01:31:44Z" id="219901725">We're actively working to switch all the snippets in the docs to runnable snippets for a bunch of reasons, one of which is this. The other is that we've start to actually run those snippets as tests to make sure that the docs don't bit rot on us. So this is kind of a duplicate of #18160 so I'm going to close it. Thanks for the suggestion!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix race condition in snapshot initialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18426</link><project id="" key="" /><description>When a snapshot initialization fails, the create snapshot method may return before the snapshot metadata in the cluster state is removed. This can cause follow up snapshot-API related calls to fail due to a snapshot still running. This is causing CI failures when we try to delete indices that were participating in failed snapshot to a read-only repository.

Closes #18121
</description><key id="155353973">18426</key><summary>Fix race condition in snapshot initialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T20:29:16Z</created><updated>2016-05-20T14:54:22Z</updated><resolved>2016-05-20T14:54:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-17T20:37:18Z" id="219845884">LGTM, left one really minor comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made def variable casting consistent with invokedynamic rules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18425</link><project id="" key="" /><description>Title says it all.
</description><key id="155352134">18425</key><summary>Made def variable casting consistent with invokedynamic rules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T20:20:27Z</created><updated>2016-05-17T20:29:49Z</updated><resolved>2016-05-17T20:29:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-17T20:27:13Z" id="219843025">+1, this looks way simpler, and I like that there is no implicit narrowing of types. 
</comment><comment author="jdconrad" created="2016-05-17T20:29:43Z" id="219843774">@rmuir Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs/add console to highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18424</link><project id="" key="" /><description>This adds CONSOLE to the highlighting docs.

Caveat: I didn't manage to get the rescore snippet to work yet (probably something obvious, will dig deeper tomorrow).

Relates to #18160 
</description><key id="155341075">18424</key><summary>Docs/add console to highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label><label>WIP</label></labels><created>2016-05-17T19:25:42Z</created><updated>2016-05-24T10:14:36Z</updated><resolved>2016-05-24T10:14:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-05-18T10:59:53Z" id="219993326">@cbuescher maybe you can help with the rescore parsing error I run into:

`Unknown key for a START_OBJECT in [rescore]`
</comment><comment author="cbuescher" created="2016-05-18T11:08:56Z" id="219995083">@MaineC that usually means you are parsing on a START_OBJECT token (`{`) and the key before that is not supported by whatever parser is currently handling the thing you are parsing. What does the structure that is being parsed look like?
</comment><comment author="MaineC" created="2016-05-18T11:10:49Z" id="219995431">@cbuescher https://github.com/MaineC/elasticsearch/blob/611ece6127ada8efbd296f58df28199c7e377127/docs/reference/search/request/highlighting.asciidoc#highlight-query ... this query is the one that doesn't seem to parse.
</comment><comment author="cbuescher" created="2016-05-18T11:45:16Z" id="220002182">@MaineC After looking into that rescore part, I think the problem is somewhere in SearchSourceBuilder where we parse the `rescore` element (L1069) where we seem to expect an array. I think adding the snippet above to the SearchSourceBuilder tests will reveal where the problem is and if we also (need to) support `"rescore" : { ... }`.
</comment><comment author="nik9000" created="2016-05-18T11:47:19Z" id="220002588">&gt; if we also (need to) support "rescore" : { ... }.

I believe we've always supported that. The array syntax came later and is used much less frequently.
</comment><comment author="MaineC" created="2016-05-18T11:51:18Z" id="220003354">+1 to what @nik9000 said. Here's the original rescore proposal I bumbed into while trying to figure out what's going wrong: https://github.com/elastic/elasticsearch/issues/2640
</comment><comment author="MaineC" created="2016-05-18T13:47:21Z" id="220030797">@cbuescher thanks for your speedy fix for #18439 - now this doc snippet test magically works as well :)
</comment><comment author="MaineC" created="2016-05-23T13:30:39Z" id="220980565">@cbuescher @nik9000 Am I right in assuming this can go in? (I don't see a LGTM in the previous comments, that's why I'm double checking)
</comment><comment author="nik9000" created="2016-05-23T17:10:37Z" id="221034118">&gt; @cbuescher @nik9000 Am I right in assuming this can go in? (I don't see a LGTM in the previous comments, that's why I'm double checking)

I was waiting for @cbuescher. I'll give it one more quick glance.
</comment><comment author="nik9000" created="2016-05-23T17:11:29Z" id="221034307">It is as I remember it: LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add CONSOLE annotation to sort documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18423</link><project id="" key="" /><description>This adds CONSOLE to sort docs in order to automatically execute the doc
snippets. Fixes a few minor types along the way.

Relates to #18160
</description><key id="155340013">18423</key><summary>Add CONSOLE annotation to sort documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T19:20:40Z</created><updated>2016-05-18T09:20:41Z</updated><resolved>2016-05-18T09:20:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-17T20:15:45Z" id="219839760">LGTM. Left some minor suggestions but that shouldn't stop merging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Amended PR adding testing to query-dsl doc snippets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18422</link><project id="" key="" /><description>Accidentally merged #18211 without including the last commit that fixed occasional test failures for the parent-id docs on my side. Including that and everything else here.
</description><key id="155339808">18422</key><summary>Amended PR adding testing to query-dsl doc snippets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T19:19:42Z</created><updated>2016-05-25T09:03:05Z</updated><resolved>2016-05-17T19:19:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-05-24T13:18:37Z" id="221265457">@MaineC can we get rid of `QueryDSLDocumentationTests` now that we test the query dsl docs snippets?
</comment><comment author="MaineC" created="2016-05-25T09:03:05Z" id="221515043">@javanna The snippets check the REST endpoints, `QueryDSLDocumentationTests` has checks for https://www.elastic.co/guide/en/elasticsearch/client/java-api/master/java-query-dsl.html - I'm not sure if something like https://github.com/elastic/docs/issues/4 could help replace this.

One thing we could get rid of now is stuff like this: https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java#L287 introduced in #14249 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify description of search timeout parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18421</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.x

It will be nice to clarify the description of the search timeout parameter (https://www.elastic.co/guide/en/elasticsearch/guide/current/_search_options.html#_timeout_2):

&gt; The timeout parameter tells the coordinating node how long it should wait before giving up and just returning the results that it already has. It can be better to return some results than none at all.

The search timeout parameter actually gets pushed down to the shards and the coordinating node just async waits for responses from the shards. The shards themselves do a best-effort (since things like document collection can be aborted but not highlighting, re-writing, etc..) to abort the query phase if it runs out of time. 
</description><key id="155339571">18421</key><summary>Clarify description of search timeout parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2016-05-17T19:18:24Z</created><updated>2016-05-18T10:16:17Z</updated><resolved>2016-05-18T10:16:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-18T10:16:17Z" id="219984786">This issue was moved to elastic/elasticsearch-definitive-guide#536
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Revert "Add Autosense annotation for query dsl testing"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18420</link><project id="" key="" /><description>Reverts elastic/elasticsearch#18211 (missing one final fix)
</description><key id="155334823">18420</key><summary>Revert "Add Autosense annotation for query dsl testing"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels /><created>2016-05-17T18:56:12Z</created><updated>2016-05-17T18:56:34Z</updated><resolved>2016-05-17T18:56:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add GC overhead logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18419</link><project id="" key="" /><description>This commit adds simple GC overhead logging. This logging captures
intervals where the JVM is spending a lot of time performing GC but it
is not necessarily the case that each GC is large. For a start, this
logging is simple and does not attempt to incorporate whether or not the
collections were efficient (that is, we are only capturing that a lot of
GC is happening, not that a lot of useless GC is happening).
</description><key id="155329669">18419</key><summary>Add GC overhead logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T18:32:03Z</created><updated>2016-11-30T05:57:33Z</updated><resolved>2016-05-18T13:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-17T18:33:20Z" id="219811429">This produces logging like

```
[2016-05-17 14:32:52,240][INFO][monitor.jvm              ] [Novs] [gc][3] overhead, spent [350ms] collecting in the last [1s]
```
</comment><comment author="nik9000" created="2016-05-18T01:40:55Z" id="219902909">LGTM but it is probably worth another set of eyes because it is late.
</comment><comment author="small-tomorrow" created="2016-11-30T05:55:04Z" id="263789964">This problem troubled me so much ,when upgrading from ES 2.1 to ES 5.0 , bulk requests keep making OOM , looking forward to your help here's the discuss : https://discuss.elastic.co/t/did-i-get-to-the-upper-limit-of-bulk-upload-api/66620/3</comment><comment author="small-tomorrow" created="2016-11-30T05:57:33Z" id="263790310">the log keeps printing : 
`[2016-11-30T13:56:11,860][INFO ][o.e.m.j.JvmGcMonitorService] [es-nmg02-jpaas212] [gc][10] overhead, spent [316ms] collecting in the last [1s]
[2016-11-30T13:56:15,862][INFO ][o.e.m.j.JvmGcMonitorService] [es-nmg02-jpaas212] [gc][14] overhead, spent [278ms] collecting in the last [1s]
[2016-11-30T13:56:29,071][INFO ][o.e.m.j.JvmGcMonitorService] [es-nmg02-jpaas212] [gc][27] overhead, spent [283ms] collecting in the last [1s]`
...
and finally OOM</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot open 2.3.1 index in 5.0.0-alpha2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18418</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha2
**JVM version**:  1.8.0_92
**OS version**: MS Win 10
**Description of the problem including expected versus actual behavior**:
I cannot open the index created with ES 2.3.1 in ES 5.0.0-alpha2. According to [docs](https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-5.0.html) there is no need to upgrade the 2.x index but I get the following exception. The other kibana index was migrated successfully.

```
Exception in thread "main" java.lang.IllegalStateException: The index [[sdtwitter-20160417000000/23TYmyVRQxmn7VZ4Ih3Oug]] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 5.0.0-alpha2 and upgraded using the upgrade API.
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:98)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
        at org.elasticsearch.gateway.GatewayMetaState.upgradeMetaData(GatewayMetaState.java:229)
        at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:90)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:235)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```
</description><key id="155310927">18418</key><summary>Cannot open 2.3.1 index in 5.0.0-alpha2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pokorsky</reporter><labels /><created>2016-05-17T17:01:34Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-05-17T18:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-17T18:08:19Z" id="219804139">can you provide the index metadata by any chance it should be in the data path underneath `sdtwitter-20160417000000/_state` 
</comment><comment author="pokorsky" created="2016-05-17T18:18:29Z" id="219807096">This is the origin metadata file before migration.

[state-13.zip](https://github.com/elastic/elasticsearch/files/268859/state-13.zip)
</comment><comment author="s1monw" created="2016-05-17T18:24:35Z" id="219808872">this index was created with `1.7.3` according to that state file. You have to reindex indices on 2.x before moving to 5.x. Yet, I think the error message is confusing here and has to be fixed. @clintongormley I think we should be explicit here and say `has to be reindexed with 2.x`?
</comment><comment author="pokorsky" created="2016-05-17T18:32:29Z" id="219811158">The index was imported with elasticdump to ES 2.3.1 with bulk api. Is it really necessary to upgrade the index?
</comment><comment author="s1monw" created="2016-05-17T18:41:51Z" id="219813837">&gt; The index was imported with elasticdump to ES 2.3.1 with bulk api. Is it really necessary to upgrade the index?

I don't know what elasticdump is and how it works. If that tool blindly copied index metadata then it will essentially create broken indices. We rely on the metadata we use internally to make decisions. If you reindexed it into 2.3 with a tool that copies the created_version from the old index then you bascially have to do that again, sorry.
</comment><comment author="s1monw" created="2016-05-17T18:55:07Z" id="219817928">@pokorsky I opened an issue with elasticsearch-dump
</comment><comment author="pokorsky" created="2016-05-17T20:13:35Z" id="219839120">Sorry for confusion. I have investigated the data and it does not seem to be related to elasticdump. At least there is no mention of ES version in the imported dump file.
The current index was created in ES 2.3.1 with

```
PUT sdtwitter-20160417000000
{
  "settings":{
    ...
    "version": {"created": "1070399"},
    "uuid": "FhxSdp0YRNSyb9pztxyPSQ"}
    ...
  }
}
```

Does `1070399` stand for `1.7.3`?
But the index status shows now

``` json
"uuid": "23TYmyVRQxmn7VZ4Ih3Oug",
"version": {
    "created": "1070399",
    "upgraded": "2030199"
}
```

Does not `"upgraded": "2030199"` mean the version of the index is 2.3.1? Does it need to be reindexed yet?
</comment><comment author="clintongormley" created="2016-05-18T10:31:43Z" id="219987869">&gt; Does not "upgraded": "2030199" mean the version of the index is 2.3.1? Does it need to be reindexed yet?

Have a look at the warning on the upgrade API page: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/indices-upgrade.html

You need to reindex this index.  The reindex API in 2.3 makes this easy.  Try out the migration helper plugin which gives a simple UI for reindexing: https://github.com/elastic/elasticsearch-migration/tree/2.x
</comment><comment author="pokorsky" created="2016-05-18T14:38:17Z" id="220046808">I wanted to avoid several hours of reindexing as the index was created in 2.3.1 but the reindex definitely solved the issue. Thank you for your help on this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent allocating shards to broken nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18417</link><project id="" key="" /><description>Allocating shards to a node can fail for various reasons. When an allocation fails, we currently ignore the node for that shard during the next allocation round. However, this means that:
- subsequent rounds consider the node for allocating the shard again.
- other shards are still allocated to the node (in particular the balancer tries to put shards on that node with the failed shard as its weight becomes smaller).
  This is particularly bad if the node is permanently broken, leading to a never-ending series of failed allocations. Ultimately this affects the stability of the cluster.
</description><key id="155297371">18417</key><summary>Prevent allocating shards to broken nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label></labels><created>2016-05-17T16:00:32Z</created><updated>2016-05-20T09:14:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-17T18:38:03Z" id="219812807">@ywelsch I think we can approach this from multiple directions. 
- we can start bottom up and check that a data-path is writeable before we allocate a shard and skip it if possible (that would help if someone looses a disk and has multiple)
- we can also has a simple allocation_failed counter on UnassignedInfo to prevent endless allocation of a potentially broken index (metadata / settings / whatever is broken)
- we might also be able to use a simple counter of failed allocations per node that we can reset once we had a successful one on that node. We can then also have a simple allocation decider that throttles that node or takes it out of the loop entirely once the counter goes beyond a threshold?

I think in all of these cases simplicity wins over complex state... my $0.05
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Support for azure credential settings within repository settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18416</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
Allow the [Azure Cloud Repo](https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-azure-repository.html) to have configuration settings such as credentials added via an API call rather than in `elasticsearch.yml`. This will standardise on what is already in place for [Amazon S3](https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-aws-repository.html).
</description><key id="155296210">18416</key><summary> Support for azure credential settings within repository settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damianpfister</reporter><labels><label>:Plugin Repository Azure</label><label>adoptme</label><label>feature</label></labels><created>2016-05-17T15:55:46Z</created><updated>2016-10-02T07:39:58Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="damianpfister" created="2016-05-17T15:56:28Z" id="219764302">Example would be something like:

```
PUT _snapshot/my_backup2
{
    "type": "azure",
    "settings": {
        "credentials.account": "AZUREACCOUNT",
        "credentials.key": "AZUREKEY"
    }
}
```
</comment><comment author="siddharthlatest" created="2016-10-02T05:52:25Z" id="250955386">@damianpfister Second adding this. Does this have any ETA?
</comment><comment author="dadoonet" created="2016-10-02T07:39:58Z" id="250958557">It does not. Wanna contribute some code?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix TimeUnitRounding for hour, minute and second units</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18415</link><project id="" key="" /><description>Currently, rounding intervals obtained by `TimeUnitRounding#nextRoundingValue()` for hour, minute and second time units can include an extra hour when 
overlapping a DST transitions that adds an extra hour (eg CEST -&gt; CET). This changes the rounding logic for time units smaller or equal to an hour to fix this.

Internally when computing `nextRoundingValue()` we convert to time local to the time zone unsed for rounding before incrementing by one unit. This is necessary for day units and 
larger to make sure we again arive at a timestamp that is rounded according to the timezone/unit. When crossing DST changes, this might for example add 23h, or 25h to arive 
at a start-of-day date, so in this case the differences between values obtained by repeatedly calling `#nextRoundingValue()` is not supposed to always we the same.

For time units equal or smaller to an hour we can ommit the conversion to local time and directly add the DurtionField, since we are sure to land again on a rounded date.

Closes #18326 
</description><key id="155292846">18415</key><summary>Fix TimeUnitRounding for hour, minute and second units</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T15:41:57Z</created><updated>2016-11-01T14:44:09Z</updated><resolved>2016-05-18T15:30:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-17T15:44:30Z" id="219760325">@jpountz could you take a look at this please?
</comment><comment author="cbuescher" created="2016-05-17T15:49:33Z" id="219762014">So with this fix in place, we get your buckets for the CEST -&gt; CET transition that only resulted in three buckets before (see #18326) and the keys are all 1h apart:

```
DELETE /_all

PUT /test/type/1
{
  "dateField" : "2000-01-01"
}

GET /test/type/_search
{
  "query": {
    "match_none": {}
  },
  "aggs": {
    "events_by_date": {
      "date_histogram": {
        "field": "dateField",
        "interval": "1h",
        "time_zone": "Europe/Oslo",
        "min_doc_count": 0,
        "extended_bounds": {
          "min": "2015-10-25T02:00:00.000+02:00",
          "max": "2015-10-25T04:00:00.000+01:00"
        }
      }
    }
  }
}
```

```
      "buckets": [
        {
          "key_as_string": "2015-10-25T02:00:00.000+02:00",
          "key": 1445731200000,
          "doc_count": 0
        },
        {
          "key_as_string": "2015-10-25T02:00:00.000+01:00",
          "key": 1445734800000,
          "doc_count": 0
        },
        {
          "key_as_string": "2015-10-25T03:00:00.000+01:00",
          "key": 1445738400000,
          "doc_count": 0
        },
        {
          "key_as_string": "2015-10-25T04:00:00.000+01:00",
          "key": 1445742000000,
          "doc_count": 0
        }
      ]
    }
```
</comment><comment author="cbuescher" created="2016-05-17T15:50:28Z" id="219762323">This should probably also be ported back to the 2.x branch.
</comment><comment author="jpountz" created="2016-05-18T11:36:33Z" id="220000572">It looks good to me. I just left a suggestion about a potentially better way to check that the interval is greater than a day.
</comment><comment author="cbuescher" created="2016-05-18T13:59:32Z" id="220034564">@jpountz thanks, I moved the duration check for the DateTimeUnit into the enum class and added tests for it. This way we shouldn't miss anything if we happen to add new constants. Mind to take another look to see if this is okay?
</comment><comment author="jpountz" created="2016-05-18T14:27:44Z" id="220043336">LGTM
</comment><comment author="cbuescher" created="2016-05-18T15:31:27Z" id="220064558">@jpountz thanks, I think this should be ported back to the 2.x branch, wdyt?
</comment><comment author="jpountz" created="2016-05-18T16:42:17Z" id="220086609">@cbuescher +1
</comment><comment author="cbuescher" created="2016-05-18T17:26:34Z" id="220099165">Merged with 2.x via 8a1d76504f8667e63d6ea1f5807ade264306d5cf
</comment><comment author="mattdawson" created="2016-05-22T21:30:07Z" id="220857620">I honestly don't think this is what you want.  Not all timezones change by 1hr, some change by 30m (Lord Howe Island).  If I want to guarantee 24 buckets in day I would use GMT for the query, and post convert the times.  The only conistent answer is to generate a 2hr or 1.5hr interval bucket in the result.
</comment><comment author="dkulichkin" created="2016-11-01T11:16:13Z" id="257542270">Guys, what is the last status with this DST issue again for date histograms? It seems like version 2.3.2 which is supposed to have all the fixes already on board still has the issue, because of the recent winter daylight shift I started to get recent day-based buckets like following (Amsterdam offset):

request:

```
 {
  "date_histogram": {
    "field": "ts",
    "interval": "day",
    "time_zone": "+01:00",
    "min_doc_count": 0,
    "extended_bounds": {
        "min": 1477346400000,
        "max": 1477995604417
    }
  }
}
```

Gives the following key_as_string converted to javascript Date objects as:
2016-10-30T00:00:00.000+01:00 =&gt; Sun Oct 30 2016 01:00:00 GMT+0200 (CEST)
2016-10-31T00:00:00.000+01:00 =&gt; Tue Oct 31 2016 00:00:00 GMT+0100 (CET)

This makes for instance a problem of showing bullets before Noc 1st on google chats - them all are shown shifted to 1 am values
</comment><comment author="cbuescher" created="2016-11-01T12:03:10Z" id="257550123">&gt; "time_zone": "+01:00"

You are using a fixed time zone offset here, no DST involved whatsoever. Try using "Europe/Amsterdam". Small example using ES 2.4.1:

```
PUT /test/type/1
{
  "dateField" : "2016-11-01"
}

GET /test/type/_search
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "events_by_date": {
      "date_histogram": {
        "field": "dateField",
        "interval": "day",
        "time_zone": "Europe/Amsterdam",
        "min_doc_count": 0,
        "extended_bounds": {
          "min": "2016-10-30T02:00:00.000+02:00",
          "max": "2016-11-03T04:00:00.000+01:00"
        }
      }
    }
  }
}
```

Response:

```
"buckets": [
        {
          "key_as_string": "2016-10-30T00:00:00.000+02:00",
          "key": 1477778400000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-10-31T00:00:00.000+01:00",
          "key": 1477868400000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-11-01T00:00:00.000+01:00",
          "key": 1477954800000,
          "doc_count": 1
        },
        {
          "key_as_string": "2016-11-02T00:00:00.000+01:00",
          "key": 1478041200000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-11-03T00:00:00.000+01:00",
          "key": 1478127600000,
          "doc_count": 0
        }
      ]
```

Note that Sunday 2016-10-30 has 25h, thats because of the added hour when going to CET.
</comment><comment author="dkulichkin" created="2016-11-01T14:44:09Z" id="257584630">@cbuescher thanks a lot, I relied on moment-timezone's solution to get it determined in the browser
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds aggregation profiling to the profile API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18414</link><project id="" key="" /><description>This change refactors some of the query profiling classes to extract common functionality that can be used to profile other features and then adds profiling of aggregations.

Caveats:
- This is very early stage and definitely contains bugs
- No tests exist yet for agg profiling
- The reduce phase is not profiled yet and will always appear as 0. This is because we currently have no infrastructure on the coordinating node for profiling. This will probably be added in a later PR.

Relates to #10538
</description><key id="155291463">18414</key><summary>Adds aggregation profiling to the profile API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-17T15:36:26Z</created><updated>2016-06-13T08:50:43Z</updated><resolved>2016-06-10T08:41:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-06-07T08:47:22Z" id="224218205">@jpountz @polyfractal I've made some progress on this and it would be great to get your feedback on how close to ready you think it is.
</comment><comment author="jpountz" created="2016-06-07T09:44:15Z" id="224231399">I like that this change is not too intrusive, the only hack I could spot is the getWrappedClass on the multi-bucket aggregator, which I think is fine. We might want to check whether we can make the profile wrappers pkg-private. Before giving an LGTM I would just like to do another iteration to better understand how the profile tree is built, but in general this looks on the right path to me.
</comment><comment author="colings86" created="2016-06-07T12:51:57Z" id="224270681">@jpountz thanks for the review. I've updated this PR with your comments. Note that the profile tree is the same logic as used for the query profiling.
</comment><comment author="jpountz" created="2016-06-08T08:35:57Z" id="224524769">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove custom Base64 implementation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18413</link><project id="" key="" /><description>This replaces o.e.common.Base64 with java.util.Base64.
</description><key id="155290776">18413</key><summary>Remove custom Base64 implementation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T15:33:46Z</created><updated>2016-05-23T11:10:50Z</updated><resolved>2016-05-23T09:34:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-17T20:43:41Z" id="219847640">I ran a micro-benchmark just to make sure we aren't losing anything here:

```
Benchmark                                                 Mode  Cnt       Score      Error  Units
Base64Benchmark.javaUtilBase64EncodeToString             thrpt   15  139451.673 &#177; 1592.545  ops/s
Base64Benchmark.orgElasticsearchCommonBase64EncodeBytes  thrpt   15   92309.961 &#177; 2922.081  ops/s
```

It's not close.

Thanks for doing this; @jaymode and I discussed this in Lake Tahoe so pinging him in case he has any thoughts here.

Otherwise, LGTM.
</comment><comment author="jpountz" created="2016-05-17T20:47:41Z" id="219848825">@jasontedor Thanks for looking and benchmarking!
</comment><comment author="jaymode" created="2016-05-19T11:59:45Z" id="220303683">LGTM as well. Thanks for doing this @jpountz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarification on filtered nested aggregation operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18412</link><project id="" key="" /><description>Some clarification would be of use in mixing nested filters with nested aggregations.  

Here is an example of how the confusion might arise:

```
POST /my_index
{
   "mappings": {
      "my_type": {
         "properties": {
            "NESTED_FIELD": {
               "type": "nested",
               "include_in_parent": true,
               "properties": {
                  "SOME_FIELD": {
                     "type": "integer",
                     "doc_values": true
                  }
               }
            }
         }
      }
   }
}
```

Post some sample data:

```
POST /my_index/my_type/
{
   "NESTED_FIELD": [
      {
         "SOME_FIELD": 1718570
      }
   ]
}


POST /my_index/my_type/
{
   "NESTED_FIELD": [
      {
         "SOME_FIELD": 1718567
      },
      {
         "SOME_FIELD": 1718565
      },
      {
         "SOME_FIELD": 1718571
      },
      {
         "SOME_FIELD": 1718568
      },
      {
         "SOME_FIELD": 1718570
      },
      {
         "SOME_FIELD": 1718569
      },
      {
         "SOME_FIELD": 1718566
      }
   ]
}
```

This query looks like it should filter aggs by  `"NESTED_FIELD.SOME_FIELD": "1718565"`, but the filter has no effect.

```
GET my_index/_search?size=0
{
  "aggs": {
    "baseFilterAggs": {
      "filter": {
        "nested": {
          "path": "NESTED_FIELD",
          "filter": {
            "term": {
              "NESTED_FIELD.SOME_FIELD": "1718565"
            }
          }
        }
      },
      "aggs": {
        "nestedLocId": {
          "nested": {
            "path": "NESTED_FIELD"
          },
          "aggs": {
            "locIdTerms": {
              "terms": {
                "field": "NESTED_FIELD.SOME_FIELD",
                "size": 0
              }
            }
          }
        }
      }
    }
  }
}
```

When written like this, it works as expected:

```
GET my_index/_search?size=0
{
  "aggs": {
    "nestedAggs": {
      "nested": {
        "path": "NESTED_FIELD"
      },
      "aggs": {
        "hhhhhhh": {
          "filter": {
            "term": {
              "NESTED_FIELD.SOME_FIELD": "1718565"
            }
          },
          "aggs": {
            "locIdTerms": {
              "terms": {
                "field": "NESTED_FIELD.SOME_FIELD",
                "size": 0
              }
            }
          }
        }
      }
    }
  }
}
```

to quote @jpountz :

&gt; In the first query, the filter keeps root documents that have at leas one nested document that matches the query. Since the 2nd root document has one nested document that matches, it is selected. Then we run the nested aggregation, which can see all nested docs of this root document.
&gt; 
&gt; On the other hand the second query first goes to the nested documents, and then only keeps those that match the filter, so there is only one bucket: the one that matches the filter.
</description><key id="155287301">18412</key><summary>Clarification on filtered nested aggregation operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Aggregations</label><label>:Nested Docs</label><label>adoptme</label><label>docs</label></labels><created>2016-05-17T15:20:42Z</created><updated>2016-05-17T18:46:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>delete-by-query plugin triggers CircuitBreakingException / SearchContextMissingException on large queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18411</link><project id="" key="" /><description>**Elasticsearch version**:

5.0.0~alpha2

**JVM version**:

java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**:

Linux ip-10-10-155-12 3.13.0-74-generic #118-Ubuntu SMP Thu Dec 17 22:52:10 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. Create a large number of documents
2. Attempt to delete the documents using something like a match_all query and the delete-by-query plugin
3. Boom!

**Provide logs (if relevant)**:

You may see any combination of these:

```
[2016-05-15 00:38:29,675][ERROR][action.deletebyquery     ] [client] scroll request [...] failed, scrolling document(s) is stopped
Failed to execute phase [query], all shards failed; shardFailures {RemoteTransportException[[worker][10.10.155.231:9300][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [3894]]; }{RemoteTransportException[[worker][10.10.155.184:9300][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [3894]]; }{RemoteTransportException[[worker][10.10.155.83:9300][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [3897]]; }{RemoteTransportException[[worker][10.10.155.181:9300][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [3894]]; }{RemoteTransportException[[worker][10.10.155.192:9300][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [3897]]; }
        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.onQueryPhaseFailure(SearchScrollQueryThenFetchAsyncAction.java:155)
        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction$1.onFailure(SearchScrollQueryThenFetchAsyncAction.java:142)
        at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:51)
        at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:795)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:204)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:194)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:141)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[ERROR][action.deletebyquery     ] [client] scroll request [...] failed, scrolling document(s) is stopped
```

or:

```
2016-05-15 23:38:55,644WARNrest.suppressed /_bulk Params: {}
CircuitBreakingException[parent Data too large, data for &lt;http_request&gt; would be larger than limit of 2994274304/2.7gb]
at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:211)
at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128)
...
```

Yup, 2.7 gigs!

Since I had quite a few of these queries going in tandem, you can guess what happened next. Several of the worker nodes pegged at 200% CPU and started to use up all swap and ram (44gigs for the elasticsearch process in most cases). Searches would take 15 minutes. 

Looking at the task list I see a big backlog of scroll requests and delete-by-query tasks, none of them cancellable. Not sure what I can do here except kill the index and start again.

Looking quickly at https://github.com/elastic/elasticsearch/blob/master/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java I'm guessing that the plugin just builds a bulk delete request without considering how large it might get.

I think some first steps might be:
1. Make this plugin cancellable, not sure what this involves but there seems plenty of opportunity in between pages of the scroll request to check a cancellation flag.
2. At least error out before attempting to build queries of such a large size, the search loop could probably check the `BulkRequest#estimatedSizeInBytes` against the cluster's max request size limits.
3. Chunk bulk requests into smaller components and deliver them piecemeal
</description><key id="155284103">18411</key><summary>delete-by-query plugin triggers CircuitBreakingException / SearchContextMissingException on large queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">j16r</reporter><labels><label>:Plugin Delete By Query</label></labels><created>2016-05-17T15:07:40Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-07-27T12:25:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-17T15:17:34Z" id="219751109">The circuit breaker issue is a duplicate of https://github.com/elastic/elasticsearch/issues/18144 that will be fixed by https://github.com/elastic/elasticsearch/pull/18204 (in alpha3)
</comment><comment author="nik9000" created="2016-05-17T15:22:14Z" id="219752548">It is hard to tell without a curl recreation, but do you happen to be setting the `size` on the request? It looks like that sets the batch size.
</comment><comment author="nik9000" created="2016-05-17T15:24:12Z" id="219753268">&gt; It is hard to tell without a curl recreation, but do you happen to be setting the size on the request? It looks like that sets the batch size.

For what it is worth that won't be true after #18329 is merged and released. That'll make delete-by-query work just like update-by-query so `size` will actually be a size limit. And it'll default to unlimited.
</comment><comment author="j16r" created="2016-05-17T15:41:53Z" id="219759493">@nik9000 just reading: https://github.com/elastic/elasticsearch/blob/230697c20220937ae5ffed4de29af73b5124d56d/modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkByScrollAction.java it doesn't look like the plugins will attempt to avoid creating gigantic bulk requests, only truncate them if you specify a size. Is my reading correct? 

Thankfully that PR seems to add cancellation capability!
</comment><comment author="nik9000" created="2016-05-17T15:54:47Z" id="219763754">@j16r right. If you specify a massive batch_size parameter then it'll make a massive bulk request. I could certainly add validation for the batch size and forbid sizes above 50,000 or something just like we do with the result window on a regular query.

Do you know if your issue with delete-by-query was caused by setting a huge batch size?
</comment><comment author="j16r" created="2016-05-17T15:58:09Z" id="219764839">I have not explicitly set a batch size, so it seems that it's defaulting to something quite large (unlimited?). It does not appear to be specified with the API I'm using: https://github.com/olivere/elastic/blob/v3.0.37/delete_by_query.go
</comment><comment author="nik9000" created="2016-05-17T16:39:44Z" id="219777664">I had a look and I can't actually set a batch size over the REST api. Which means this is probably all the fault of #18329. It looks like delete-by-query is always using a batch size of 10, the default for searches. I expect porting it to reindex's infra will probably speed it up quite a bit then.

I think we're ok here - probably it is already fixed by #18204 which will come out with alpha 3. There isn't really a work around until then though.

Maybe we should add a preflight check to the _reindex family of requests to make sure the batch size is not crazy large?
</comment><comment author="clintongormley" created="2016-05-18T09:56:21Z" id="219980333">I tested this out with size 1000 and size 5000, with sort:_doc and with sort:_uid.  There was little difference in performance (while 1000 gives a huge performance boost over 10).  

&gt; Maybe we should add a preflight check to the _reindex family of requests to make sure the batch size is not crazy large?

Agreed.  Perhaps an upper limit of 10000?
</comment><comment author="nik9000" created="2016-07-27T12:25:59Z" id="235569648">&gt; Maybe we should add a preflight check to the _reindex family of requests to make sure the batch size is not crazy large?

We have an in-flight check for this in master (#19367), failing with batch sizes of greater than 10,000. I think we're ok here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix insanely slow compilation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18410</link><project id="" key="" /><description>The issue is caused by too much ambiguity in the grammar, particularly the fact that semicolons aren't needed to separate statements.

Instead, just make the semicolon optional at the very end (in other words, `EOF` is equivalent). This means that very simple one liner cases like `5` or `_score + log(doc['pagerank'].value)` or whatever do not need semicolons. But you can't do `int x = y int z = x + 5 return z` anymore. I really don't think we should be allowing that anyway.

Closes #18398
</description><key id="155282116">18410</key><summary>Fix insanely slow compilation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T14:59:14Z</created><updated>2016-05-18T09:39:44Z</updated><resolved>2016-05-17T16:18:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-17T15:00:24Z" id="219745581">Note that i'd really like to hook in stuff to fail on ambiguities in tests, but there are other ambiguity problems. Maybe dangling-else type stuff that is easily fixed. But that is not to do here: we have to keep the scope of this PR contained.
</comment><comment author="jdconrad" created="2016-05-17T15:46:41Z" id="219761012">LGTM.  Thanks!
</comment><comment author="rmuir" created="2016-05-17T16:18:07Z" id="219771097">Thanks @jdconrad. I am sure this can be improved if we wish, e.g. to allow newlines too. But I think most users will not even have more than 1 line with painless because it works inline. It may be good enough to keep it simple like this. I will look into the other ambiguity...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using nested agg after reverse_nested shows higher count than expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18409</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0
**JVM version**:
java version "1.8.0_05"
Java(TM) SE Runtime Environment (build 1.8.0_05-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)

**OS version**: OSX Yosemite 10.10.2

I am doing this:
1. Grouping by a nested field: `nested_path.nested_field`
2. Using a reverse_nested agg so I can apply this filter: `non_nested_field == "yay"`
3. Using a nested agg so I can then get a count of the nested field I am grouping by: `nested_path.nested_field`

Problem: By using the reverse_nested agg I am getting a higher doc_count than I would expect.

Here is the mapping and docs I am indexing:

```
PUT /my_index
{
   "mappings": {
      "my_type": {
         "properties": {
            "nested_path": {
               "type": "nested",
               "properties": {
                  "nested_field": {
                     "type": "string"
                  }
               }
            },
            "non_nested_field": {
               "type": "string"
            }
         }
      }
   }
}

POST /my_index/my_type/1
{
  "non_nested_field": "whoray",
  "nested_path": [
    {
      "nested_field": "yes"
    },
    {
      "nested_field": "yes"
    },
    {
      "nested_field": "no"
    }
  ]
}

POST /my_index/my_type/2
{
  "non_nested_field": "yay",
  "nested_path": [
    {
      "nested_field": "maybe"
    },
    {
      "nested_field": "no"
    }
  ]
}
```

Search request:

```
POST my_index/my_type/_search
{
   "aggs": {
      "nested_option": {
         "nested": {
            "path": "nested_path"
         },
         "aggs": {
            "group_list": {
               "terms": {
                  "field": "nested_path.nested_field",
                  "size": 100
               },
               "aggs": {
                  "level_1": {
                     "reverse_nested": {},
                     "aggs": {
                        "level_2": {
                           "filter": {
                              "term": {
                                 "non_nested_field": "yay"
                              }
                           },
                           "aggs": {
                              "level_3": {
                                 "nested": {
                                    "path": "nested_path"
                                 },
                                 "aggs": {
                                    "stat": {
                                       "value_count": {
                                          "field": "nested_path.nested_field"
                                       }
                                    }
                                 }
                              }
                           }
                        }
                     }
                  }
               }
            }
         }
      }
   },
   "size": 0
}
```

Part of the response I get is this:

```
{  
  "aggregations": {
    "nested_option": {
      "doc_count": 5,
      "group_list": {
        "buckets": [
          {
            "key": "no",
            "doc_count": 2,
            "level_1": {
              "doc_count": 2,
              "level_2": {
                "doc_count": 1,
                "level_3": {
                  "doc_count": 2,
                  "stat": {
                    "value": 2
                  }
                }
              }
            }
          }
          //.... 
        ]
      }
    }
  }
}
```

In the first element of the buckets array in the response, `level_1.level_2.doc_count` is 1, and this is correct, because there's only one of the two docs indexed where `nested_path.nested_field == "no"` and `non_nested_field == "yay"`. But `level_1.level_2.level_3.doc_count` in the response is 2. It should only be 1. This seems like a bug to me.
</description><key id="155281260">18409</key><summary>Using nested agg after reverse_nested shows higher count than expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chevin99</reporter><labels /><created>2016-05-17T14:55:52Z</created><updated>2016-05-17T18:44:22Z</updated><resolved>2016-05-17T18:44:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T18:44:21Z" id="219814608">This is working correctly. As soon as you step up with `reverse_nested`, you lose any context you had in the nested documents.  You can see this if you change your `stat` agg to repeat the terms agg:

```
POST my_index/my_type/_search
{
  "aggs": {
    "nested_option": {
      "nested": {
        "path": "nested_path"
      },
      "aggs": {
        "group_list": {
          "terms": {
            "field": "nested_path.nested_field",
            "size": 100
          },
          "aggs": {
            "level_1": {
              "reverse_nested": {},
              "aggs": {
                "level_2": {
                  "filter": {
                    "term": {
                      "non_nested_field": "yay"
                    }
                  },
                  "aggs": {
                    "level_3": {
                      "nested": {
                        "path": "nested_path"
                      },
                      "aggs": {
                        "group_list": {
                          "terms": {
                            "field": "nested_path.nested_field",
                            "size": 100
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations fix: support include/exclude strings for IP and dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18408</link><project id="" key="" /><description>Added calls to DocValueFormat.parse methods to handle parsing of user-supplied `include` and `exclude` strings in `terms` and `significant_terms` aggregations into a form that can be compared with doc values

Closes #17705
</description><key id="155265493">18408</key><summary>Aggregations fix: support include/exclude strings for IP and dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T13:54:06Z</created><updated>2016-05-18T15:24:42Z</updated><resolved>2016-05-18T15:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-17T14:52:17Z" id="219742836">Could you also test that regexp-based filtering on ips is not supported? I think `convertToStringFilter` and `convertToOrdinalsFilter` should fail when the format is not `DocValueFormat.RAW`? Otherwise it looks good to me.
</comment><comment author="jpountz" created="2016-05-18T14:29:59Z" id="220044126">LGTM
</comment><comment author="markharwood" created="2016-05-18T15:24:41Z" id="220062343">Pushed in https://github.com/elastic/elasticsearch/commit/a846ff93e91020b5a837c45a045191b1c58a86e7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add tests for URL Repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18407</link><project id="" key="" /><description>The `org.elasticsearch.repositories.uri.URLRepository` class does not have any tests written for it.  It needs tests to exercise its functionality, probably by connecting it to a webserver, so it will require some thought.

`SharedClusterSnapshotRestoreIT` contains two tests related to the URL repository:
1. `testUrlRepository`
2. `testReadOnlyRepository`

However, it would be helpful to have more comprehensive, unit level tests that prove the semantics and guarantees of the URL repository.
</description><key id="155265428">18407</key><summary>Add tests for URL Repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>test</label></labels><created>2016-05-17T13:53:48Z</created><updated>2016-05-20T13:39:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-17T14:51:01Z" id="219742414">We can use a fixture for this I think! See `:test:fixtures`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch fails to start when tmp directory defined</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18406</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2 

**JVM version**: java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)

**OS version**: Centos 7 SELinux enabled

**Description of the problem including expected versus actual behavior**:
We have hardened Centos 7 where /tmp is not writeable
We have elasticsearch homedir in custom location and we link that custom location into /var/lib/elasticsearch.
We define tmp dir path in /etc/sysconfig/elasticserach as 
# Additional Java OPTS

ES_JAVA_OPTS="-Djna.tmpdir=/var/lib/elasticsearch/tmp" (we also tried without "" and with java.io.tmpdir=)

**Steps to reproduce**:
1. change tmpdir path in sysconfig
2. restart application

**Provide logs (if relevant)**:
`[2016-05-17 12:31:30,875][WARN ][bootstrap                ] unable to load JNA native support library, native methods will be disabled.
java.lang.UnsatisfiedLinkError: /tmp/jna--1985354563/jna306419785920339930.tmp: /tmp/jna--1985354563/jna306419785920339930.tmp: failed to map segment from shared object: Operation not permitted
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(Unknown Source)
        at java.lang.ClassLoader.loadLibrary(Unknown Source)
        at java.lang.Runtime.load0(Unknown Source)
        at java.lang.System.load(Unknown Source)
        at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:761)
        at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:736)
        at com.sun.jna.Native.&lt;clinit&gt;(Native.java:131)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Unknown Source)
        at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45)
        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:89)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)`
[elasticsearch_dump.txt](https://github.com/elastic/elasticsearch/files/268267/elasticsearch_dump.txt)
</description><key id="155262411">18406</key><summary>elasticsearch fails to start when tmp directory defined</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bubo77</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2016-05-17T13:41:29Z</created><updated>2017-07-07T19:42:42Z</updated><resolved>2016-11-21T10:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T18:19:36Z" id="219807419">I tried setting the tmpdir with `ES_JAVA_OPTS="-Djna.tmpdir=/var/lib/elasticsearch/tmp"` and it worked just fine.

&gt; We have elasticsearch homedir in custom location and we link that custom location into /var/lib/elasticsearch.

Do you mean symlinked?  That's probably your problem.
</comment><comment author="bubo77" created="2016-05-17T18:28:06Z" id="219809882">Yeah i mean we have a symlink. 
But interestingly it can create read/write the indicies but can't write tmp

```
ls -alZ /var/lib/elasticsearch/
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       .
drwxr-xr-x. root          root          unconfined_u:object_r:usr_t:s0   ..
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       graylog2
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       tmp 
```

I have tried without a symlink fails the same way

Have you checked the kernel dump i've attached to the original post?
</comment><comment author="clintongormley" created="2016-05-20T09:40:47Z" id="220561788">Looks like a duplicate of https://github.com/elastic/elasticsearch/issues/18406

Closing
</comment><comment author="FlorianHeigl" created="2016-11-18T01:53:10Z" id="261427007">You closed this with reference to itself. :-)
</comment><comment author="clintongormley" created="2016-11-19T14:09:37Z" id="261715932">Whoops. Reopening.

@jasontedor any thoughts?
</comment><comment author="tlrx" created="2016-11-21T09:32:36Z" id="261887996">This looks similar to #18272</comment><comment author="FlorianHeigl" created="2016-11-21T10:24:37Z" id="261899146">Thanks! sounds like the right thing. Not sure why google liked the duplicate better.
I think you can re-close with that reference!

In summary that bug says:

- you cannot run ElasticSearch on a system that follows standard and basic security practices.
- The issue is not in ES, but in the JNA thing (but obviously testing does not include systems that are secure)
- The issue is related to the combination of JNA, noexec mounts and SELinux
- It will **not** help to set a different ElasticSearch tmpdir since the problem isn't in ES itself.
- There's no mention of a JNA issue for that. I'll try to find out about that. Expecting to end up in 10 year old stale code ;-)</comment><comment author="FlorianHeigl" created="2016-11-21T10:55:38Z" id="261905929">I think my last comment was lost. Sounds you found the right reference.
(Sounds also like a hopeless case right now) </comment><comment author="clintongormley" created="2016-11-21T10:57:08Z" id="261906248">Reclosing in favour of https://github.com/elastic/elasticsearch/issues/18272</comment><comment author="jasontedor" created="2016-11-22T05:15:52Z" id="262149807">&gt; you cannot run ElasticSearch on a system that follows standard and basic security practices.

This is false. It's not clear exactly what the poor interaction is because no one can provide a clear reproduction, but it does run fine on some systems with SELinux set to enforcing and `/tmp` mounted with `noexec`. 

&gt; The issue is not in ES, but in the JNA thing (but obviously testing does not include systems that are secure)

This is false. We have CI machines, VMs for our packaging tests, and my workstation (and likely other developer's workstations) that all run SELinux in enforcing mode. </comment><comment author="jasontedor" created="2016-11-22T05:16:36Z" id="262149915">&gt; (Sounds also like a hopeless case right now)

Yes, until someone can provide a 100% reliable reproduction. </comment><comment author="Fabian1976" created="2017-02-27T10:28:35Z" id="282683232">It does seem to be related to the noexec mount options. I've had the same issue on 1 of our servers. When the noexec mount option is active on the /tmp directory, I get this error:
```
[2017-02-27T11:27:17,588][WARN ][o.e.b.Natives            ] unable to load JNA native support library, native methods will be disabled.
java.lang.UnsatisfiedLinkError: /tmp/jna--1985354563/jna3776152293556606246.tmp: /tmp/jna--1985354563/jna3776152293556606246.tmp: failed to map segment from shared object: Operation not permitted
        at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_111]
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_111]
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[?:1.8.0_111]
        at java.lang.Runtime.load0(Runtime.java:809) ~[?:1.8.0_111]
        at java.lang.System.load(System.java:1086) ~[?:1.8.0_111]
        at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:851) ~[jna-4.2.2.jar:4.2.2 (b0)]
        at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:826) ~[jna-4.2.2.jar:4.2.2 (b0)]
        at com.sun.jna.Native.&lt;clinit&gt;(Native.java:140) ~[jna-4.2.2.jar:4.2.2 (b0)]
        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_111]
        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_111]
        at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:104) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:203) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:333) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:121) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:112) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.cli.Command.main(Command.java:88) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:89) [elasticsearch-5.2.1.jar:5.2.1]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:82) [elasticsearch-5.2.1.jar:5.2.1]
```

After that I removed the noexec mount option, and it works fine.
Setting de -Djna.tmpdir of -Djava.io.tmpdir to something different also didn't work on my server.</comment><comment author="jcmcken" created="2017-03-13T17:13:47Z" id="286176490">Just chiming in here to say we have the same problem. We use ``-Djna.tmpdir`` with other applications just fine. Only Elasticsearch seems not to work, requiring us to set ``exec`` on ``/tmp``.

**ES**: elasticsearch-5.2.0-1.noarch (RPM distribution)
**OS**: CentOS Linux release 7.3.1611 (Core)
**JVM**:

```
java version "1.8.0_121"
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)
```
</comment><comment author="Fabian1976" created="2017-03-16T07:00:26Z" id="286973809">@jasontedor Reproduction was quite simple for me. Place noexec mount options on /tmp, installations fails. Even if a seperate tmp dir is defined (it is not respected or used). Remove noexec mount options, installation works fine.</comment><comment author="tlrx" created="2017-03-16T11:36:01Z" id="287031542">@Fabian1976  @jcmcken Can you please share more about your OS, versions of elasticsearch and java, `/proc/mounts`, `/etc/fstab`, use of tmpfs etc please? That would help.

@jcmcken Do the other applications you mentioned also use JNA? Also, I can't reproduce on a fresh CentOS installation (see above) with `noexec`. 

```
$ cat /etc/centos-release
CentOS Linux release 7.3.1611 (Core) 
```
```
$ java -version
java version "1.8.0_121"
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)
```
```
$ cat /proc/mounts | grep /tmp
tmpfs /tmp tmpfs rw,noexec 0 0
```
```
$ sudo rpm -i elasticsearch-5.2.0.rpm 
warning: elasticsearch-5.2.0.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY
Creating elasticsearch group... OK
Creating elasticsearch user... OK
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service
```
```
$ sudo systemctl start elasticsearch

$ curl -XPOST 'http://localhost:9200/books/book' -d '{"title":"foo"}'
{"_index":"books","_type":"book","_id":"AVrW3iFUg6TNDnTr-o09","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},"created":true}

$ curl -XGET 'http://localhost:9200/_search?q=foo'
{"took":59,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.2876821,"hits":[{"_index":"books","_type":"book","_id":"AVrW3iFUg6TNDnTr-o09","_score":0.2876821,"_source":{"title":"foo"}}]}}
```</comment><comment author="jasontedor" created="2017-03-16T14:25:46Z" id="287073579">@Fabian1976 I know that this thread is quite long, but the stack trace that you provided is different than the one that triggered this issue. I think that we are looking at separate problems. It might be best if you open a separate issue.</comment><comment author="FlorianHeigl" created="2017-03-16T14:47:19Z" id="287080337">@tlrx did you look in your elasticsearch logs too or just run the tests?</comment><comment author="Fabian1976" created="2017-03-17T10:50:03Z" id="287323866">@tlrx 
My OS:
```
[root@elasticsearch01 ~]# cat /etc/redhat-release 
CentOS Linux release 7.3.1611 (Core) 
[root@elasticsearch01 ~]# 
```

Elasticsearch version:
```
[root@elasticsearch01 ~]# yum list installed | grep elasticsearch
elasticsearch.noarch       5.2.1-1               @CMC_Elastic_co_5_x            
[root@elasticsearch01 ~]# 
```

Java version:
```
[root@elasticsearch01 ~]# java -version
openjdk version "1.8.0_111"
OpenJDK Runtime Environment (build 1.8.0_111-b15)
OpenJDK 64-Bit Server VM (build 25.111-b15, mixed mode)
[root@elasticsearch01 ~]# 
```

Proc mounts:
```
[root@elasticsearch01 ~]# cat /proc/mounts | grep /tmp
/dev/mapper/VolGroup01-tmp /tmp ext4 rw,seclabel,nosuid,nodev,relatime,data=ordered 0 0
/dev/mapper/VolGroup01-tmp /var/tmp ext4 rw,seclabel,nosuid,nodev,relatime,data=ordered 0 0
[root@elasticsearch01 ~]# 
```

FStab:
```
[root@elasticsearch01 ~]# cat /etc/fstab | grep /tmp
/dev/mapper/VolGroup01-tmp	/tmp	ext4	nodev,nosuid	0	2
/tmp	/var/tmp	ext4	bind,nodev,nosuid,noexec	0	0
[root@elasticsearch01 ~]# 
```
</comment><comment author="jcmcken" created="2017-03-17T17:29:35Z" id="287419878">@tlrx 

Just to be clear, because I'm not sure we're talking about the same thing: ES launches just fine when ``/tmp`` is mounted ``noexec`` (which you've demonstrated). That's not my issue.

The issue is that it's falling back to NOT using JNA in this configuration, which you can see by examining the logs. But I *would* like to actually use whatever native extensions are available. And so I'm setting ``jna.tmpdir`` system prop to an alternate location on a filesystem that's mounted ``exec`` (e.g. ``ES_JAVA_OPTS="-Djna.tmpdir=/some/other/path"`` in ``/etc/sysconfig/elasticsearch``). In this configuration, ES launches, and then dies a few seconds later.

Jenkins CI is the example that comes to mind where we're setting ``jna.tmpdir`` to an alternate location.</comment><comment author="Balajiswz" created="2017-04-04T12:45:05Z" id="291488209">I am also hitting the same error when we try to define separate TMP folder for elasticsearch with similar configurations. </comment><comment author="candido1212" created="2017-05-09T12:28:42Z" id="300147665">Try this:

The problem can be solved download jars, and copy to elasticsearch lib folder

https://github.com/java-native-access/jna

Version 4.4.0
Maven Central   **jna.jar**
Maven Central   **jna-platform.jar**</comment><comment author="jasontedor" created="2017-05-09T13:06:56Z" id="300156540">@candido1212 Elasticsearch 5.4.0 ships with JNA 4.4.0 now. Dropping jars into the lib folder is fully unsupported.</comment><comment author="FlorianHeigl" created="2017-05-09T13:19:52Z" id="300160632">@candido1212 were you doing that on ES 5.4.0?
If dropping in the same version did in fact change the behaviour that is at least interesting information.
</comment><comment author="AverageJoe2" created="2017-05-27T02:33:45Z" id="304421267">@jasontedor
hi buddy how you make it with version Elasticsearch 5.4.0 now ?</comment><comment author="yami12376" created="2017-06-19T10:09:30Z" id="309397010">I have the same issue - tried what candido suggested but without success.
 (CentOS 5.2 (Final) x64) - i think i have to update to CentOs 6
ES 5.4.1
java version "1.8.0_112"
Installed and started as posted in https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html

```
 cat /proc/mounts | grep /tmp
&lt;empty&gt;
```

```
cat /etc/fstab | grep /tmp
&lt;empty&gt;

```

```
sudo service elasticsearch start
Starting elasticsearch:                                    [  OK  ]

```

after that:

```
starts elasticsearch

Option                Description
------                -----------
-E &lt;KeyValuePair&gt;     Configure a setting
-V, --version         Prints elasticsearch version information and exits
-d, --daemonize       Starts Elasticsearch in the background
-h, --help            show help
-p, --pidfile &lt;Path&gt;  Creates a pid file in the specified path on start
-q, --quiet           Turns off standard ouput/error streams logging in console
-s, --silent          show minimal output
-v, --verbose         show verbose output
ERROR: D is not a recognized option

```

```

[2017-06-19T12:04:30,595][WARN ][o.e.b.Natives            ] unable to load JNA native support library, native methods will be disabled.
java.lang.UnsatisfiedLinkError: /tmp/jna-3506402/jna8670561425318125531.tmp: /lib/libc.so.6: version `GLIBC_2.7' not found (required by /tmp/jna-3506402/jna8670561425318125531.tmp)
        at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_112]
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_112]
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[?:1.8.0_112]
        at java.lang.Runtime.load0(Runtime.java:809) ~[?:1.8.0_112]
        at java.lang.System.load(System.java:1086) ~[?:1.8.0_112]
        at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:947) ~[jna-4.4.0.jar:4.4.0 (b0)]
        at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:922) ~[jna-4.4.0.jar:4.4.0 (b0)]
        at com.sun.jna.Native.&lt;clinit&gt;(Native.java:190) ~[jna-4.4.0.jar:4.4.0 (b0)]
        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_112]
        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_112]
        at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:105) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:350) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.cli.Command.main(Command.java:88) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) [elasticsearch-5.4.1.jar:5.4.1]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) [elasticsearch-5.4.1.jar:5.4.1]
[2017-06-19T12:04:30,604][WARN ][o.e.b.Natives            ] cannot check if running as root because JNA is not available
[2017-06-19T12:04:30,604][WARN ][o.e.b.Natives            ] cannot install system call filter because JNA is not available
[2017-06-19T12:04:30,605][WARN ][o.e.b.Natives            ] cannot register console handler because JNA is not available
[2017-06-19T12:04:30,607][WARN ][o.e.b.Natives            ] cannot getrlimit RLIMIT_NPROC because JNA is not available
[2017-06-19T12:04:30,608][WARN ][o.e.b.Natives            ] cannot getrlimit RLIMIT_AS beacuse JNA is not available
[2017-06-19T12:04:30,802][INFO ][o.e.n.Node               ] [] initializing ...
[2017-06-19T12:04:30,924][INFO ][o.e.e.NodeEnvironment    ] [c51pvG3] using [1] data paths, mounts [[/ (/dev/mapper/VolGroup00-LogVol00)]], net usable_space [12.4gb], net total_space [31.1gb], spins? [possibly], types [ext3]
[2017-06-19T12:04:30,924][INFO ][o.e.e.NodeEnvironment    ] [c51pvG3] heap size [1.9gb], compressed ordinary object pointers [unknown]
[2017-06-19T12:04:30,934][INFO ][o.e.n.Node               ] node name [c51pvG3] derived from node ID [c51pvG3UT62JdbAwGWOJuQ]; set [node.name] to override
[2017-06-19T12:04:30,935][INFO ][o.e.n.Node               ] version[5.4.1], pid[21917], build[2cfe0df/2017-05-29T16:05:51.443Z], OS[Linux/2.6.18-92.el5/i386], JVM[Oracle Corporation/Java HotSpot(TM) Server VM/1.8.0_112/25.112-b15]
[2017-06-19T12:04:30,935][INFO ][o.e.n.Node               ] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+DisableExplicitGC, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/opt/install/elasticsearch-5.4.1]
[2017-06-19T12:04:32,221][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [aggs-matrix-stats]
[2017-06-19T12:04:32,221][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [ingest-common]
[2017-06-19T12:04:32,221][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [lang-expression]
[2017-06-19T12:04:32,221][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [lang-groovy]
[2017-06-19T12:04:32,221][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [lang-mustache]
[2017-06-19T12:04:32,222][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [lang-painless]
[2017-06-19T12:04:32,222][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [percolator]
[2017-06-19T12:04:32,222][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [reindex]
[2017-06-19T12:04:32,222][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [transport-netty3]
[2017-06-19T12:04:32,222][INFO ][o.e.p.PluginsService     ] [c51pvG3] loaded module [transport-netty4]
[2017-06-19T12:04:32,223][INFO ][o.e.p.PluginsService     ] [c51pvG3] no plugins loaded
[2017-06-19T12:04:34,468][INFO ][o.e.d.DiscoveryModule    ] [c51pvG3] using discovery type [zen]
[2017-06-19T12:04:35,703][INFO ][o.e.n.Node               ] initialized
[2017-06-19T12:04:35,703][INFO ][o.e.n.Node               ] [c51pvG3] starting ...
[2017-06-19T12:04:37,077][INFO ][o.e.t.TransportService   ] [c51pvG3] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
[2017-06-19T12:04:37,140][WARN ][o.e.b.BootstrapChecks    ] [c51pvG3] max file descriptors [1024] for elasticsearch process is too low, increase to at least [65536]
[2017-06-19T12:04:37,140][WARN ][o.e.b.BootstrapChecks    ] [c51pvG3] system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk
[2017-06-19T12:04:40,334][INFO ][o.e.c.s.ClusterService   ] [c51pvG3] new_master {c51pvG3}{c51pvG3UT62JdbAwGWOJuQ}{Xk4Yetg-Q3etoZ3YGo6Z5Q}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-elected-as-master ([0] nodes joined)
[2017-06-19T12:04:40,486][INFO ][o.e.h.n.Netty4HttpServerTransport] [c51pvG3] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
[2017-06-19T12:04:40,490][INFO ][o.e.n.Node               ] [c51pvG3] started
[2017-06-19T12:04:40,614][INFO ][o.e.g.GatewayService     ] [c51pvG3] recovered [1] indices into cluster_state
[2017-06-19T12:04:40,931][INFO ][o.e.c.r.a.AllocationService] [c51pvG3] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana][0]] ...]).

```
</comment><comment author="jasontedor" created="2017-06-19T15:32:04Z" id="309476780">@yami12376 As I [mentioned](https://github.com/elastic/elasticsearch/issues/18406#issuecomment-300156540), dropping jars in the lib folder is fully unsupported. The problem that you're running to here is that the official JNA jar from upstream does not support older distributions that do not support newer versions of glibc. This is why we ship with our own build of JNA that does support some of the older distributions that we still support. The only difference between our build and upstream is that the Linux native library is recompiled to support OS that are using older versions of glibc, everything else is identical to upstream so dropping a different version of the jar in the lib folder is not going to help with anything to begin with.

Please, stop following the advice to drop a different build into the lib folder. It is not a solution to the problem here, and it is fully unsupported.</comment><comment author="yami12376" created="2017-06-23T12:50:10Z" id="310656995">I updated to CentOs 7 - and it works fine</comment><comment author="thosfelt" created="2017-07-07T19:42:42Z" id="313775097">I'm currently running ES 2.4.0 running on CentOS 6.6 (PPC) and am haveing some of the same issues after our SA's have done some upgrades to the systems.  I can't get past the "because JNA is not available" and ES won't use ES_JAVA_OPTS to point to a different jna.tmpdir.  Also /tmp doesn't have the noexec option turned on and still won't start up after the system upgrade.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify recovery logic in IndicesClusterStateService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18405</link><project id="" key="" /><description>- Moves recovery logic into IndexShard
- Lets recoveries be cancelled by shard closing
- Ensures routingEntry is set on initialization of IndexShard
</description><key id="155255737">18405</key><summary>Simplify recovery logic in IndicesClusterStateService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T13:11:03Z</created><updated>2016-05-18T08:52:12Z</updated><resolved>2016-05-18T08:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-17T19:43:53Z" id="219831270">I like the change. Left some comments/ suggestions
</comment><comment author="ywelsch" created="2016-05-18T06:15:59Z" id="219934809">@bleskes I've reworked the bit where recovery source changed. Please have another look. I've also added a commit that I forgot to push as part of this PR that fixes an issue in IndexShardTests when `UnassignedInfo.Reason` is `INDEX_CREATED` (no recovery happens for shards with `ShardRouting.allocatedPostIndexCreate == false`).
</comment><comment author="bleskes" created="2016-05-18T07:18:17Z" id="219945717">LGTM. Left one minor request for docs.
</comment><comment author="ywelsch" created="2016-05-18T08:52:12Z" id="219965143">Thanks for the review @bleskes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force attach a fake jar with non "jar" artifacts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18404</link><project id="" key="" /><description>Follow up for #18396: which is reverted with this change

When you upload any `jar` artifact you have to provide the `jar` file, javadocs and sources.

For distribution, we changed from `pom` artifact to `jar` in order to be able to run integration tests.
But we never generate any jar, sources or javadoc because they don't exist.

This PR adds a FAKE file in `src/main/resources`. So Maven is able to generate the JAR file which is uploaded to Sonatype.
Sources are then automatically added.
To generate the javadoc file, we need also to tell maven that javadoc will be based on this `src/main/resources` dir.
As a file exists in it, javadoc is generated as well.

```
[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ elasticsearch ---
[INFO] Installing /Users/dpilato/Documents/Elasticsearch/dev/es-2x/elasticsearch/distribution/zip/target/elasticsearch-2.4.0-SNAPSHOT.jar to /Users/dpilato/.m2/repository/org/elasticsearch/distribution/zip/elasticsearch/2.4.0-SNAPSHOT/elasticsearch-2.4.0-SNAPSHOT.jar
[INFO] Installing /Users/dpilato/Documents/Elasticsearch/dev/es-2x/elasticsearch/distribution/zip/pom.xml to /Users/dpilato/.m2/repository/org/elasticsearch/distribution/zip/elasticsearch/2.4.0-SNAPSHOT/elasticsearch-2.4.0-SNAPSHOT.pom
[INFO] Installing /Users/dpilato/Documents/Elasticsearch/dev/es-2x/elasticsearch/distribution/zip/target/elasticsearch-2.4.0-SNAPSHOT-javadoc.jar to /Users/dpilato/.m2/repository/org/elasticsearch/distribution/zip/elasticsearch/2.4.0-SNAPSHOT/elasticsearch-2.4.0-SNAPSHOT-javadoc.jar
[INFO] Installing /Users/dpilato/Documents/Elasticsearch/dev/es-2x/elasticsearch/distribution/zip/target/elasticsearch-2.4.0-SNAPSHOT-sources.jar to /Users/dpilato/.m2/repository/org/elasticsearch/distribution/zip/elasticsearch/2.4.0-SNAPSHOT/elasticsearch-2.4.0-SNAPSHOT-sources.jar
[INFO] Installing /Users/dpilato/Documents/Elasticsearch/dev/es-2x/elasticsearch/distribution/zip/target/releases/elasticsearch-2.4.0-SNAPSHOT.zip to /Users/dpilato/.m2/repository/org/elasticsearch/distribution/zip/elasticsearch/2.4.0-SNAPSHOT/elasticsearch-2.4.0-SNAPSHOT.zip
```
</description><key id="155253322">18404</key><summary>Force attach a fake jar with non "jar" artifacts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>review</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-05-17T13:00:31Z</created><updated>2016-05-17T13:58:28Z</updated><resolved>2016-05-17T13:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tiny Missing Article</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18403</link><project id="" key="" /><description>Noticed a very very minor grammatical error of a missing article before the word "node".
</description><key id="155251032">18403</key><summary>Tiny Missing Article</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kylegoch</reporter><labels><label>docs</label></labels><created>2016-05-17T12:49:18Z</created><updated>2016-05-17T15:40:04Z</updated><resolved>2016-05-17T15:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kylegoch" created="2016-05-17T12:50:12Z" id="219707433">The CLA was signed moments before submitting this PR. If there are any issues let me know.
</comment><comment author="nik9000" created="2016-05-17T15:09:40Z" id="219748578">&gt; The CLA was signed moments before submitting this PR. If there are any issues let me know.

The CLA checker got a recent overhaul it is working much better! In a neat bit of engineering, it rechecks whenever someone posts a comment in a PR that hasn't passed. So just coming to the issue and saying "I've signed the CLA" is usually enough for it to recheck and pass.
</comment><comment author="nik9000" created="2016-05-17T15:40:04Z" id="219758897">Thanks @kylegoch ! I've merged to 2.3 and forward ported to:
2.x: e14f5a3c7ee168b1557c705d0a9fe6bf9a12a594
master: b12cabd2f55fd430f533be00378a4f984d1dc6ac
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `Character.MODIFIER_SYMBOL` to the list of symbol categories.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18402</link><project id="" key="" /><description>Closes #18388
</description><key id="155248584">18402</key><summary>Add `Character.MODIFIER_SYMBOL` to the list of symbol categories.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Analysis</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T12:36:53Z</created><updated>2016-05-23T08:28:09Z</updated><resolved>2016-05-23T08:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-17T15:26:31Z" id="219754067">LGTM. Left a suggestion for an extra test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Create javadoc for all projects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18401</link><project id="" key="" /><description>This ensures that javadocs are created for any projects by configuring
on the root pom.xml
</description><key id="155246129">18401</key><summary>Build: Create javadoc for all projects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-05-17T12:23:15Z</created><updated>2016-05-17T13:33:25Z</updated><resolved>2016-05-17T13:33:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use Java 9 Indy String Concats, if available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18400</link><project id="" key="" /><description>This PR adds support to painless for indyfied string concats, which is a new feature of Java 9. Instead of creating a StringBuilder in byte code and then appending all parts, the recommended way for bytecode in Java 9 is to use use `invokedynamic` on `java.lang.invoke.StringConcatFactory`. This allows many improvements in the JVM:
- Hotspot is better in optimizing this, as the code is just a single method call with many (primitive or reference) arguments
- It knows the size of the result array before (e.g. by summing up the lengths of all parts)
- It can directly create a "compact string", while `StringBuilder` still uses a `char[]` inbetween

See http://openjdk.java.net/jeps/280 and http://cr.openjdk.java.net/~shade/8085796/notes.txt for more details. Not all of the above optimizations are implemented in JDK/Hotspot yet, but the old code will no longer get improvements in JDK, as it is too complicated to analyze and breaks easily.

The new code checks if the `StringConcatFactory` is available at run time with correct signature. Otherwise the byte code is generated as before. If indy string concats are enabled, all parts are pushed with their original types to stack and the MethodWriter's concat method just records the types, so it can build the descriptor for the single method call afterwards.

As number of arguments to concat is limited to 200, the code calls the invokedyanmic always after 200 items and just leaves the return value on stack as first param for next round. While testing this I found a parser slowness bug in painless (see #18398).
</description><key id="155243592">18400</key><summary>Use Java 9 Indy String Concats, if available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T12:08:59Z</created><updated>2016-05-18T12:00:59Z</updated><resolved>2016-05-18T12:00:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-05-17T12:16:20Z" id="219700161">Here is the code gerenated from the following script: `String s = \"cat\"; return s + true + 'abc' + null;`

```
  public execute(Ljava/util/Map;Lorg/apache/lucene/search/Scorer;Lorg/elasticsearch/search/lookup/LeafDocLookup;Ljava/lang/Object;)Ljava/lang/Object;
   L0
    LINENUMBER 1 L0
    LDC "cat"
    ASTORE 5
   L1
    LINENUMBER 1 L1
    ALOAD 5
    ICONST_1
    LDC "abc"
    ACONST_NULL
    INVOKEDYNAMIC concat(Ljava/lang/String;ZLjava/lang/String;Ljava/lang/Object;)Ljava/lang/String; [
      // handle kind 0x6 : INVOKESTATIC
      java/lang/invoke/StringConcatFactory.makeConcat(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;
      // arguments: none
    ]
    ARETURN
    MAXSTACK = 4
    MAXLOCALS = 6
```
</comment><comment author="rmuir" created="2016-05-17T13:16:48Z" id="219714055">this looks fantastic. i will look into the parsing performance issue.
</comment><comment author="rmuir" created="2016-05-17T16:22:03Z" id="219772280">This looks fine to me: great to have the fastest strings possible :) 

The parsing performance issue is fixed but it caused a conflict because I ported your test to master already. If you merge up I will push this.
</comment><comment author="uschindler" created="2016-05-17T16:51:47Z" id="219781115">Hi @rmuir 
I merged: the test is fast - thanks!
I will later add some checks to the `visiteEnd` method of the MethodWariter class to ensure state machine is in sane state.
I will also change the test to also test the limits (exact number of 200 args,...).
I will inform you once I found

Maybe you can look at benchmarking the string concats. I don't know how optimized that all is, but it should _not_ be slower than without.

Maybe add a compile option to enable/disable indy string concats?
</comment><comment author="rmuir" created="2016-05-17T17:17:36Z" id="219788842">unfortunately i dont have a benchmark that can be easily used out of box. Even for the search case, our benchmark has at least 20% noise :(

Currently instead of benchmarking, when reviewing I simply look at whether its better bytecode or not. This is not perfect but better than any benchmark we have. You can of course make a "stupid" benchmark that just does a bunch of concat and runs the script millions of times to try to see (e.g. from a test) if you have concerns, but I don't have any concerns :)

As far as compile option, do you have concerns about correctness? From my perspective it is a task for jenkins and we don't need it.
</comment><comment author="uschindler" created="2016-05-17T22:32:19Z" id="219874188">Hi,
I added one more check in the visitEnd() method of MethodWriter to check that the concat stack is empty. I also added a new test for nested concats (a concat has another concat indirectly hidden behind a method call in one of the parts).

I also found out that there is another slowdown with the long string concats if you use a cast to def instead of toString() or plain string literal. I will add this to the other slowness issue!

I think this is ready.
</comment><comment author="uschindler" created="2016-05-17T22:35:36Z" id="219874834">&gt; As far as compile option, do you have concerns about correctness? From my perspective it is a task for jenkins and we don't need it.

No concerns. I just say: Java 9 is not yet finalized. The current code only activates concats if the method signature matches 100%, otherwise it uses string buffer, so risk is low. On Java 8 it will never be used, as StringConcatFactory does not exist there.

The configuration option was just meant as a way to disable it, if it really breaks with a later JDK9 EA release (e.g. if then change the magic number 200 shortly before release to say 150...). Currently its documented to be 200 in Javadocs, but those are not yet finalized ("Please note that the specifications and other information contained herein are not final and are subject to change"). I will suggest on the Mailing list to have a integer constant with that number in the StringConcatFactory.
</comment><comment author="rmuir" created="2016-05-17T22:54:32Z" id="219878255">&gt; No concerns. I just say: Java 9 is not yet finalized.

Yes it is a risk but painless is documented as experimental at the moment. Therefore we shall experiment :)
</comment><comment author="rmuir" created="2016-05-18T12:00:59Z" id="220005409">Thanks @uschindler !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update http client version to 4.5.2 and http-core 4.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18399</link><project id="" key="" /><description>This should allow us to remove StrictHostnameVerifier which was introduced to work around a bug in the apache http client, which is used in our tests.
</description><key id="155241365">18399</key><summary>update http client version to 4.5.2 and http-core 4.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T11:55:39Z</created><updated>2016-05-20T11:00:42Z</updated><resolved>2016-05-20T10:02:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-05-17T11:55:51Z" id="219696146">@jaymode can you please have a look?
</comment><comment author="jaymode" created="2016-05-19T14:21:27Z" id="220339048">LGTM. Thanks @javanna 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compilation horrible slow on some huge statements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18398</link><project id="" key="" /><description>While working on indify string concats for painless, I noticed that some statements take veeery long to compile. Some background:

Indy string concats only allow a maximum of 200 parts per invokedynamic. To test the code to allow more (using intermediate results), I added a simple statement with 211 items and let painless parse it:

```
String s = "cat"; return s + "000".toString() + "001".toString() + ... + "209".toString();
```

This takes on painless on my computer up to 300 seconds. If I remove the `.toString()` calls, it gets a bit faster, but the result is that strings are already merged while compiling (so the `.toString()` is just a workaround to prevent concats on compilation: I did not find a test for that, its cool that painless does this!). It gets even slower if you instead cast every single string to `(def)`, its still works and works and works since 1000s.

Looks like there is some exponential runtime problem.

This is my test code:

``` java
    public void testAppendMany() {
        StringBuilder script = new StringBuilder("String s = \"cat\"; return s");
        StringBuilder result = new StringBuilder("cat");
        for (int i = 0; i &lt; WriterConstants.MAX_INDY_STRING_CONCAT_ARGS + 10; i++) {
            final String s = String.format(Locale.ROOT,  "%03d", i);
            script.append(" + '").append(s).append("'.toString()");
            result.append(s);
        }
        System.out.println(Debugger.toString(script.toString()));
        //assertEquals(result.toString(), exec(script.toString()));
    }
```
</description><key id="155235775">18398</key><summary>Compilation horrible slow on some huge statements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label></labels><created>2016-05-17T11:21:20Z</created><updated>2016-05-17T16:20:35Z</updated><resolved>2016-05-17T16:18:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-05-17T11:27:13Z" id="219690755">FYI, the resulting bytecode with Java 9 and indyfied string concat looks like - (I will open a PR about that soon!):

```
  public execute(Ljava/util/Map;Lorg/apache/lucene/search/Scorer;Lorg/elasticsearch/search/lookup/LeafDocLookup;Ljava/lang/Object;)Ljava/lang/Object;
   L0
    LINENUMBER 1 L0
    LDC "cat"
    ASTORE 5
   L1
    LINENUMBER 1 L1
    ALOAD 5
    LDC "000"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "001"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "002"
[...]
    LDC "198"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    INVOKEDYNAMIC concat(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String; [
      // handle kind 0x6 : INVOKESTATIC
      java/lang/invoke/StringConcatFactory.makeConcat(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;
      // arguments: none
    ]
    LDC "199"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "200"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "201"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "202"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "203"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "204"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "205"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "206"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "207"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "208"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    LDC "209"
    INVOKEVIRTUAL java/lang/String.toString ()Ljava/lang/String;
    INVOKEDYNAMIC concat(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String; [
      // handle kind 0x6 : INVOKESTATIC
      java/lang/invoke/StringConcatFactory.makeConcat(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;
      // arguments: none
    ]
    ARETURN
    MAXSTACK = 200
    MAXLOCALS = 6
```
</comment><comment author="uschindler" created="2016-05-17T11:33:52Z" id="219691928">@clintongormley this is a bug not an enhancement!
</comment><comment author="clintongormley" created="2016-05-17T11:34:42Z" id="219692082">Dutifully updated :)
</comment><comment author="uschindler" created="2016-05-17T11:35:35Z" id="219692252">Thanks!
</comment><comment author="rmuir" created="2016-05-17T13:35:14Z" id="219718978">I ran profiler and it looks to me the issue is caused by the grammar. I added DiagnosticErrorListener and ambiguity detection and it prints:

```
line 1:18 reportAttemptingFullContext d=15 (statement), input='Strings="cat";return'
line 1:16 reportAmbiguity d=15 (statement): ambigAlts={5, 11}, input='Strings="cat";'
line 1:4016 reportAttemptingFullContext d=28 (expression), input='+'000'.toString()+'001'.toString()+'002'.toString()+...+'209.toString()
```

The "..." are mine but it shows all 210 strings.
</comment><comment author="rmuir" created="2016-05-17T13:47:21Z" id="219722411">The test also runs in 3-5 seconds instead of hundreds of seconds if i set SLL prediction mode. I don't think we should solve it that way (even though tests pass), instead we should figure out how to fix the ambiguity...
</comment><comment author="rmuir" created="2016-05-17T13:59:16Z" id="219725954">The issue is caused by making semicolons optional everywhere in the grammar. Fixing that makes the test run instantly.
</comment><comment author="rmuir" created="2016-05-17T14:02:53Z" id="219727052">Also by fixing the semicolon issue, i get no more ambiguity warnings with this hack. So this is the real bad guy. I will make a PR to fix this. We can make semicolons mandatory and be fast. If we want to make them optional in some way, we should do it in a way that is not a performance killer, or we don't do it. This isn't groovy.

```
diff --git a/modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java b/modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java
index 4f6e2f5..c776245 100644
--- a/modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java
+++ b/modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java
@@ -21,7 +21,9 @@ package org.elasticsearch.painless.antlr;

 import org.antlr.v4.runtime.ANTLRInputStream;
 import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.DiagnosticErrorListener;
 import org.antlr.v4.runtime.ParserRuleContext;
+import org.antlr.v4.runtime.atn.PredictionMode;
 import org.elasticsearch.painless.Operation;
 import org.elasticsearch.painless.Variables.Reserved;
 import org.elasticsearch.painless.antlr.PainlessParser.AfterthoughtContext;
@@ -139,8 +141,10 @@ public final class Walker extends PainlessParserBaseVisitor&lt;ANode&gt; {
         final PainlessParser parser = new PainlessParser(new CommonTokenStream(lexer));
         final ParserErrorStrategy strategy = new ParserErrorStrategy();

-        lexer.removeErrorListeners();
-        parser.removeErrorListeners();
+        //lexer.removeErrorListeners();
+        //parser.removeErrorListeners();
+        parser.addErrorListener(new DiagnosticErrorListener());
+        parser.getInterpreter().setPredictionMode(PredictionMode.LL_EXACT_AMBIG_DETECTION);
         parser.setErrorHandler(strategy);

         return parser.source();
```
</comment><comment author="rmuir" created="2016-05-17T16:20:35Z" id="219771803">Thanks for reporting this @uschindler !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Maybe we should make our REST test examples visible in our docs?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18397</link><project id="" key="" /><description>With #18075 merged we now have the option to execute our doc snippets through the REST test infrastructure.

Many of the small parts of original REST tests I've read look like great stories around how to use our REST API. I have no idea how much overlap there is between our docs and the REST test-suite, however I would imagine that one could convert several REST tests into nice pieces of documentation showing the complete story from indexing data to using the API under test.

I'm wondering if it would make sense to consolidate what we are now (or in the hopefully near future) testing from within our docs and what is covered in the REST tests. (Intentionally not adding any version label to this issue - currently this is really just a vague question instead of a hard and fast proposal to move any specific way.)
</description><key id="155231976">18397</key><summary>Maybe we should make our REST test examples visible in our docs?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label></labels><created>2016-05-17T10:59:10Z</created><updated>2016-05-20T09:15:25Z</updated><resolved>2016-05-20T09:15:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T11:05:42Z" id="219686618">I'm very wary of docs that are not optimally docs because they're trying to be tests instead.  I'd much rather write the best possible docs and test the snippets used within as a secondary goal.
</comment><comment author="nik9000" created="2016-05-17T15:32:03Z" id="219756008">&gt; I'd much rather write the best possible docs and test the snippets used within as a secondary goal.

I think any snippets in the docs that aren't part of a test are prone to bit rot. One day I'd like to fail the build if there are any docs that aren't tested.

I agree with @clintongormley that docs should be docs first though. If they happen to be tests second that is a bonus. There might be duplication in test coverage that way but that seems less bad than having bad docs because they are really tests. OTOH I bet we could do something. I just don't think it is as urgent as getting the snippets tested.
</comment><comment author="MaineC" created="2016-05-17T19:40:03Z" id="219830338">&gt; &gt; I'm very wary of docs that are not optimally docs because they're trying to be tests instead.

I think this is exactly the kind of input needed to put the original idea into perspective.

&gt; I think any snippets in the docs that aren't part of a test are prone to bit rot. 

+1 There's one more argument for having these under test:

When making changes to existing features (adding deprecations, changing meaning, deleting parameters etc.) right now I can think of at least five locations in our code base that should be touched:
- the original code itself
- the tests the code hopefully ships with
- the migration docs
- potentially the REST tests
- the example in the docs
  At least for myself I can state that it's fairly easy to forget one or more locations to update. Having the build tell me automatically instead of getting this feedback from a human reviewer and going through an added round of code review might save some time.

&gt; I agree with @clintongormley that docs should be docs first though.

+1

&gt; If they happen to be tests  second that is a bonus. There might be duplication in test coverage 
&gt; that way but that seems less bad than having bad docs because they are really tests. OTOH I 
&gt; bet we could do something. I just don't think it is as urgent as getting the snippets tested.

+1 Yeah sure - if github had some way of sorting issues by priority IMHO this one should be waaay below the one about putting existing snippets under test.

I guess my initial question is based on two thoughts:
- As we have support for both, request execution and result validation in the snippet testing, instead of writing both, a REST test and a comprehensive doc entry, maybe it would make sense to have only the doc (this assumes all that should be tested actually does fit nicely into the documentation itself).
- Because I know that at least I am myself prone to missing stuff if for any given change I need to touch more than a very limited number of locations in the code I'm leaning towards trying to eliminate duplication.
</comment><comment author="clintongormley" created="2016-05-20T09:15:25Z" id="220556363">I think there isn't anything more to discuss, so I'll close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force attach sources with non "jar" artifacts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18396</link><project id="" key="" /><description>Sonatype rule `sources-staging` force to provide a `*-sources.jar` artifact.

This PR generates the source jar even if there is no related java code by considering the `pom.xml` file as a source file.

We could set this globally on the project but I prefer being cautious here and just add this to the projects which are not generating source files archive for now.

When you run it on distribution packages or site-example plugin, you now get:

```
[INFO] --- maven-source-plugin:2.4:jar (attach-sources) @ site-example ---
[INFO] Building jar: /Users/dpilato/Documents/Elasticsearch/dev/es-2x/elasticsearch/plugins/site-example/target/site-example-2.4.0-SNAPSHOT-sources.jar
```

Was previously:

```
[INFO] --- maven-source-plugin:2.4:jar (attach-sources) @ site-example ---
[INFO] No sources in project. Archive not created.
```
</description><key id="155222629">18396</key><summary>Force attach sources with non "jar" artifacts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-05-17T10:07:48Z</created><updated>2016-05-17T10:13:18Z</updated><resolved>2016-05-17T10:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T10:09:12Z" id="219675132">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moves query profiler classes into their own package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18395</link><project id="" key="" /><description>The change also renames fields and methods in the Profilers class.

Note that I had to make ProfileResult a public class (it was package private before) because now classes that call it are in a different package.
</description><key id="155218256">18395</key><summary>Moves query profiler classes into their own package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T09:46:48Z</created><updated>2016-05-17T13:21:44Z</updated><resolved>2016-05-17T13:20:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-05-17T12:59:48Z" id="219709784">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Ensure javadocs are released for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18394</link><project id="" key="" /><description>In order to release javadocs for plugins as well, these need to
be generated and valid.
</description><key id="155217835">18394</key><summary>Release: Ensure javadocs are released for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-05-17T09:44:45Z</created><updated>2016-05-17T09:54:43Z</updated><resolved>2016-05-17T09:54:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T09:48:55Z" id="219670606">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unassigned shards after complete cluster failure (All nodes down)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18393</link><project id="" key="" /><description>ES version 2.3.1 

I have a 3 node cluster, all 3 nodes are master-eligible and data nodes. I have tested cases when any one node is down, or any two nodes are down. Everything works perfectly fine and single node which is up is able to serve requests and shows yellow cluster health status which is expected. 

However when i bring all nodes down and start only one node, cluster status stays "red" and shard allocation never happens (tried also with cluster.routing.allocation.enable: all) , however if i start any one of other two nodes it gets back to normal, i get this in logs :-

org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];

17 May 2016 14:37:24,778 DEBUG  gateway:82 - [myhostname.com] [my-index][1] found 1 allocations of [my-index][1], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-05-17T09:07:08.627Z]], highest version: [186]

17 May 2016 14:37:24,778 DEBUG  gateway:89 - [myhostname.com] [my-index][1]: not allocating, number_of_allocated_shards_found [1]

I am using unicast for node discovery, and 2 replicas.

index.number_of_replicas: 2

Not sure if its the expected behavior.
</description><key id="155212924">18393</key><summary>Unassigned shards after complete cluster failure (All nodes down)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet01</reporter><labels /><created>2016-05-17T09:20:50Z</created><updated>2016-05-17T11:46:10Z</updated><resolved>2016-05-17T09:53:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-17T09:53:07Z" id="219671552">Please ask questions like these on http://discuss.elastic.co

`index.number_of_replicas: 2` means that you have 3 copies of the data, i.e., each node has a copy. If you start only a single node, ES does not assign this shard as it wants to prevent allocating a stale shard copy. It therefore requires a quorum (2 copies) of the data to be available before deciding which one is the most recent one. This can be influenced by the setting `index.recovery.initial_shards` (see https://www.elastic.co/guide/en/elasticsearch/reference/2.3/index-modules.html#dynamic-index-settings ). Setting this to "1" will start recovery if only one copy is available but is dangerous as it can make a stale shard copy primary, leading to data loss.

Note: we have improved this in v5.0.0 with allocation ids #14739, so that we don't need a quorum of copies anymore to ensure allocating a non-stale copy.
</comment><comment author="vineet01" created="2016-05-17T10:46:23Z" id="219682879">https://discuss.elastic.co/t/50211
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updates the documentation for the recent changes in the profiler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18392</link><project id="" key="" /><description /><key id="155210352">18392</key><summary>Updates the documentation for the recent changes in the profiler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search</label><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T09:08:02Z</created><updated>2016-05-17T10:07:38Z</updated><resolved>2016-05-17T10:07:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T10:04:11Z" id="219674009">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ignoring deleted stopwords while generating shingles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18391</link><project id="" key="" /><description>Hi ! 
I have the same issue that the one exposed here : [#12819 ](https://github.com/elastic/elasticsearch/issues/12819) (before closing due to the lack of feedback asked by @clintongormley to the author @apanimesh061)

I discussed it here [on stackoverflow](http://stackoverflow.com/questions/37205478/how-to-prevent-duplicate-tokens-with-same-start-pos-but-different-end?noredirect=1#comment61964513_37205478) because I thought I made a mistake in my parameters

When using a shingle token filter after removal of stopwords why don't we have a parameter to completely ignore stopwords ?
There would be two huge benefits for me :
- Even with "filler_token":"" (or "filler_token":" " then a "trim" token filter) there are duplicated shingles generated, with the same text, "pos" and "start" but only differing by "end" that might tweak the queries scores for the duplicated words as seen [here :](http://i.stack.imgur.com/nboTe.png)
- If a word is surrounded by stopwords, no shingles would be generated associating it with other words around, several (deleted) stopwords away...

I shall add that those duplicated tokens are not removed by a "unique" token filter... and that I don't understand why.

Here is my analysis (I only kept the "test" analyzer which I used to generate examples but I have a few others ):

``` json
"settings": {
    "index": {
        "analysis": {
            "filter": {
                "fr_stop": {
                    "ignore_case": "true",
                    "remove_trailing": "true",
                    "type": "stop",
                    "stopwords": "_french_"
                },
                "fr_worddelimiter": {
                    "type": "word_delimiter",
                    "language": "french"
                },
                "fr_snowball": {
                    "type": "snowball",
                    "language": "french"
                },
                "custom_nGram": {
                    "type": "ngram",
                    "min_gram": "3",
                    "max_gram": "10"
                },
                "custom_shingles": {
                    "max_shingle_size": "4",
                    "min_shingle_size": "2",
                    "token_separator": " ",
                    "output_unigrams": "true",
                    "filler_token":"",
                    "type": "shingle"
                },
                "custom_unique": {
                    "type": "unique",
                    "only_on_same_position": "true"
                },
                "fr_elision": {
                    "type": "elision",
                    "articles": [
                        "l",
                        "m",
                        "t",
                        "qu",
                        "n",
                        "s",
                        "j",
                        "d",
                        "c",
                        "jusqu",
                        "quoiqu",
                        "lorsqu",
                        "puisqu",
                        "parce qu",
                        "parcequ",
                        "entr",
                        "presqu",
                        "quelqu"
                    ]
                }
            },
            "charfilter": "html_strip",
            "analyzer": {
                "test": {
                    "filter": [
                        "asciifolding",
                        "lowercase",
                        "fr_stop",
                        "fr_elision",
                        "custom_shingles"
                        "trim",
                        "custom_unique",
                    ],
                    "type": "custom",
                    "tokenizer": "standard"
                }
            },
            "tokenizer": {
                "custom_edgeNGram": {
                    "token_chars": [
                        "letter",
                        "digit"
                    ],
                    "min_gram": "3",
                    "type": "edgeNGram",
                    "max_gram": "20"
                },
                "custom_nGram": {
                    "token_chars": [
                        "letter",
                        "digit"
                    ],
                    "min_gram": "3",
                    "type": "nGram",
                    "max_gram": "20"
                }
            }
        }
    }
}
```
</description><key id="155206294">18391</key><summary>Ignoring deleted stopwords while generating shingles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vchalmel</reporter><labels><label>:Analysis</label><label>feedback_needed</label></labels><created>2016-05-17T08:46:59Z</created><updated>2016-05-17T11:44:31Z</updated><resolved>2016-05-17T10:45:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T10:02:09Z" id="219673552">Hi @vchalmel 

Could you provide a complete (and minimal) curl recreation showing exactly what you're doing, the results you're getting, and how they differ from what you expect?
</comment><comment author="clintongormley" created="2016-05-17T10:11:13Z" id="219675529">Just use the analyze API:
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/indices-analyze.html

On Tue, 17 May 2016 at 12:06 CHALMEL notifications@github.com wrote:

&gt; Hi @clintongormley https://github.com/clintongormley
&gt; I would be glad to... but I'm afraid I don't know how to get the detail of
&gt; generated token using curl, I always use kopf to test my analysis...
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18391#issuecomment-219674522
</comment><comment author="vchalmel" created="2016-05-17T10:21:34Z" id="219677650">Hi @clintongormley 

```
curl -XGET 'localhost:9200/test_marches_publics/_analyze' -d '
{
"analyzer":"test",
"text" : "Scandale au pays de Candy"
}
```

give me, with default "filler_token"

``` json
{"tokens":[{"token":"scandale","start_offset":0,"end_offset":8,"type":"&lt;ALPHANUM&gt;","position":0},
{"token":"scandale _","start_offset":0,"end_offset":12,"type":"shingle","position":0},
{"token":"scandale _ pays","start_offset":0,"end_offset":16,"type":"shingle","position":0},
{"token":"scandale _ pays _","start_offset":0,"end_offset":20,"type":"shingle","position":0},
{"token":"_ pays","start_offset":12,"end_offset":16,"type":"shingle","position":1},
{"token":"_ pays _","start_offset":12,"end_offset":20,"type":"shingle","position":1},
{"token":"_ pays _ candy","start_offset":12,"end_offset":25,"type":"shingle","position":1},
{"token":"pays","start_offset":12,"end_offset":16,"type":"&lt;ALPHANUM&gt;","position":2},
{"token":"pays _","start_offset":12,"end_offset":20,"type":"shingle","position":2},
{"token":"pays _ candy","start_offset":12,"end_offset":25,"type":"shingle","position":2},
{"token":"_ candy","start_offset":20,"end_offset":25,"type":"shingle","position":3},
{"token":"candy","start_offset":20,"end_offset":25,"type":"&lt;ALPHANUM&gt;","position":4}]}
```

and with "filler_token":""

``` json
{"tokens":[{"token":"scandale","start_offset":0,"end_offset":8,"type":"&lt;ALPHANUM&gt;","position":0},
{"token":"scandale  pays","start_offset":0,"end_offset":16,"type":"shingle","position":0},
{"token":"pays","start_offset":12,"end_offset":16,"type":"shingle","position":1},
{"token":"pays  candy","start_offset":12,"end_offset":25,"type":"shingle","position":1},
{"token":"pays","start_offset":12,"end_offset":16,"type":"&lt;ALPHANUM&gt;","position":2},
{"token":"pays  candy","start_offset":12,"end_offset":25,"type":"shingle","position":2},
{"token":"candy","start_offset":20,"end_offset":25,"type":"shingle","position":3},
{"token":"candy","start_offset":20,"end_offset":25,"type":"&lt;ALPHANUM&gt;","position":4}]}
```

I would want, for example (among other duplicates), only one token "pays", and a token "scandale pays candy" which currently overthrow the max shingle size due to deleted stopwords "ghosts".
Another effect of those deleted stopwords "ghosts",  the multiple spaces "  " (one for each deleted stopword) instead of " " in the shingles.
What I would want :

``` json
{"tokens":[{"token":"scandale","start_offset":0,"end_offset":8,"type":"&lt;ALPHANUM&gt;","position":0},
{"token":"scandale pays","start_offset":0,"end_offset":16,"type":"shingle","position":0},
{"token":"scandale pays candy","start_offset":0,"end_offset":25,"type":"shingle","position":0}
{"token":"pays","start_offset":12,"end_offset":16,"type":"shingle","position":1},
{"token":"pays candy","start_offset":12,"end_offset":25,"type":"shingle","position":1},
{"token":"candy","start_offset":20,"end_offset":25,"type":"&lt;ALPHANUM&gt;","position":4}]}
```
</comment><comment author="vchalmel" created="2016-05-17T10:32:08Z" id="219679932">N.b. I had a mistake (now edited) in my first post, in "custom_unique", "only_on_same_position" is set to true.
If this parameter is false, the "trim"/"custom_unique" get rid of the duplicated tokens, but I don't want to delete identical tokens from distinct parts of the original text and every other issue remains.
</comment><comment author="clintongormley" created="2016-05-17T10:45:44Z" id="219682733">@vchalmel positions are used for phrase queries, offsets are used for highlighting. It's a mistake to try to use a shingled field for either of these purposes.  A shingled field is used for finding associated words, and that's it.  You should use a shingled field in a bool.should clause to boost the relevance of docs with matching fields.

Also, having min=max shingle of 2 is usually all you need. Remove the unigrams.
</comment><comment author="vchalmel" created="2016-05-17T11:07:27Z" id="219686975">I don't understand your objection... Is that only a usecase scenario issue ? Or am I deeply wrong about the r&#244;le of those tokens differing only by offset or position ?
- If I remove the unigrams and search on a unique word, will it match ?
- If "pays" is duplicated three times here, are you saying that a search on "pays" won't get a higher score on this document than on a document where it is not surrounded by stopwords and hence is not duplicated because tokens differing only by positions and offset are only affecting highlighting and phrase queries ?

You seem also to consider that shingles should only be used in a complementary field to boost score in queries if associated words are found, but that is simply not my use case... I want an index with all these tokens, both unigrams and shingles up to three words at least, because i will use them in a text-mining algorithm... 

On the other hand if min=max shingle=2 in that case "fox to be quick" won't generate the shingle "fox quick"
And what about the issue with the extras spaces ?
</comment><comment author="clintongormley" created="2016-05-17T11:23:14Z" id="219690020">&gt; If I remove the unigrams and search on a unique word, will it match ?

No.  But if you're using shingles as a secondary field for boosting, then the single word will match in the primary field you're searching on.

&gt; If "pays" is duplicated three times here, are you saying that a search on "pays" won't get a higher score on this document than on a document where it is not surrounded by stopwords and hence is not duplicated because tokens differing only by positions and offset are only affecting highlighting and phrase queries ?

Yes it will get a higher score.  But then it is used more frequently, so it should get a higher score.  If you don't want it to match when it is combined with different stopwords, then leave the stopwords there (or use a different filler token).

&gt; You seem also to consider that shingles should only be used in a complementary field to boost score in queries if associated words are found, but that is simply not my use case... I want an index with all these tokens, both unigrams and shingles up to three words at least, because i will use them in a text-mining algorithm...

Ok...  then perhaps your requirements are different, but I don't understand what your exact requirements are.  All I'm saying is that this is working correctly as is.  Token filters are not allowed to change positions or offsets. That jobs belongs solely to the tokenizer.  This comes directly from Lucene and isn't something we can change in Elasticsearch.
</comment><comment author="vchalmel" created="2016-05-17T11:41:54Z" id="219693430">Thanks for your explanations !

&gt; Token filters are not allowed to change positions or offsets. That jobs belongs solely to the tokenizer.

Oh that is something I failed to take into account... 
That would prevent us to re-calculate the size of the shingle to ignore deleted stopwords with the current formula, am I right ?... But how about a parameter to prevent the creation of a shingle starting or ending with a deleted stopword like these : 

```
{"token":"pays","start_offset":12,"end_offset":16,"type":"shingle","position":1}, /*virtually start with deleted stopword "au"*/
{"token":"pays  candy","start_offset":12,"end_offset":25,"type":"shingle","position":1},/*virtually start with deleted stopword "au"*/
{"token":"pays","start_offset":12,"end_offset":16,"type":"&lt;ALPHANUM&gt;","position":2}, /*this unigram is duplicate with first token because due to stopwords I have shingles of size 2 containing only 1 word*/
{"token":"pays  candy","start_offset":12,"end_offset":25,"type":"shingle","position":2}, /*duplicate with second token*/
```

How could the size of a stopword being different of the count of words effectively existing in it be "working correctly as is" ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose cluster state before reroute in RoutingAllocation instead of RoutingNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18390</link><project id="" key="" /><description>Instead of re-exposing index metadata, blocks and customs in RoutingNodes (which is part of the cluster state), expose it as part of the RoutingAllocation which is known to be only temporarily used during reroute.
</description><key id="155201200">18390</key><summary>Expose cluster state before reroute in RoutingAllocation instead of RoutingNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T08:17:29Z</created><updated>2016-05-17T17:02:28Z</updated><resolved>2016-05-17T17:02:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-17T13:41:31Z" id="219720748">@dakrone can you have a look at this one?
</comment><comment author="dakrone" created="2016-05-17T15:31:24Z" id="219755774">LGTM, this is a nice cleanup
</comment><comment author="ywelsch" created="2016-05-17T17:00:26Z" id="219783688">Thanks for reviewing @dakrone!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return highlighted fragment doesn't work </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18389</link><project id="" key="" /><description>Elasticsearch version: 2.3.2
JVM version: 1.8.0_91
OS version: ubuntu 16.04 LTS

I want ES to return 150 characters of the highlighted parts of the field "recipe". Now there are no change  (I think) between when I wrote the "highlight" lines and before: 

(nb it is a Perl code but just replace the =&gt; by : and you have a typical ES request) 

```
 my $range_cooktime_filter   = { "range" =&gt; { "cooktime" =&gt; { "lte" =&gt; $full_recherche{cooktime_max} } } };

  my $range_difficulty_filter = { "range" =&gt; { 
                                  "difficulty_int"  =&gt; { 
                                  "lte"   =&gt; $full_recherche{difficulty_max}, 
                                  "gte"  =&gt; $full_recherche{difficulty_min} } } };

  my $range_price_filter      = { "range"       =&gt; { 
                                  "price_int"   =&gt; { 
                                  "lte"  =&gt; $full_recherche{price_max}, 
                                  "gte"  =&gt; $full_recherche{price_min} } } };

  my $must_not   = { "multi_match" =&gt; {
                     "query"       =&gt; $full_recherche{ingredients_neg},
                     "fields"      =&gt; [ qw/title recipe ingredients/ ], } };

  my $multimatch = { "multi_match" =&gt; {
                     "query"       =&gt; $full_recherche{general},
                     "operator"    =&gt; "and",
                     "fields"      =&gt; [ qw/title recipe ingredients/ ], } };

  my $type_filter = {"term" =&gt; { "type" =&gt; $full_recherche{type} } };

  my $matchall = { "match_all" =&gt; {} };

  my $unfiltered      = ($full_recherche{general} eq "") ? $matchall : $multimatch;
  my $type_filterF    = ($full_recherche{type} == undef) ? $matchall : $type_filter;

  my $results = $e-&gt;search(
    "size" =&gt; 10,
    "from" =&gt; 0,
    "body" =&gt; {
      "query" =&gt; {
        "bool" =&gt; {
          "must" =&gt; $unfiltered,
          "filter" =&gt; [ $range_cooktime_filter, 
                        $range_difficulty_filter, 
                        $range_price_filter,
                        $type_filterF
                        ],
          "must_not" =&gt; $must_not,
        }
      },
      "highlight" =&gt; {
         "fields" =&gt; {
            "recipe" =&gt; {"fragment_size" =&gt; 150, "number_of_fragments" =&gt; 1}
          }
        }
      }
    }
  );
```

Thank you very much ! 
</description><key id="155196998">18389</key><summary>Return highlighted fragment doesn't work </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">emaudoux</reporter><labels><label>:Highlighting</label><label>feedback_needed</label></labels><created>2016-05-17T07:52:13Z</created><updated>2016-05-18T15:41:07Z</updated><resolved>2016-05-18T15:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T09:45:16Z" id="219669767">@emaudoux please upload a complete recreation in curl syntax. You haven't given us the index settings, the mappings, or the document.  Btw, with the Perl client just do:

```
$e = Search::Elasticsearch-&gt;new(trace_to =&gt; 'Stdout')
```

and it will dump all your requests in curl syntax
</comment><comment author="clintongormley" created="2016-05-17T09:45:53Z" id="219669892">Also, please make the recreation as small as possible - I don't want to have to wade through lots of fields etc that don't form a part of the problem
</comment><comment author="emaudoux" created="2016-05-17T12:15:54Z" id="219700069">I'm not sure I understand everything you said above but still , here is my request : 

```

    "size" : 10,
    "from" : 0,
    "body"  : {
      "query" :{
        "bool" : {
          "must" : {
            "multi_match" : { "query" : $full_recherche{general},
                                        "operator" : "and",
                     "fields" : [ qw/title recipe ingredients/ ], 
            } 
         },
          "filter" : [ ...  ],
          "must_not" : $must_not,
        }
      },
      "highlight" : {
         "fields" : {
            "recipe" : {}
          }
        }
      }
    }
  );`
```

Here is the mapping : 

```
{
    "mappings": {
        "docs": {
            "properties": {
                ...
                "recipe": { "type": "string", "index": "analyzed", "analyzer": "french" },
                ...
            }
        }
    }
}
```
</comment><comment author="clintongormley" created="2016-05-17T17:24:36Z" id="219790916">@emaudoux Unless you provide a full recreation that we can copy and paste which demonstrates the issue (without having to guess about what values you're passing in), nobody is likely to look at this.

here's an example of what a full recreation looks like: https://github.com/elastic/elasticsearch/issues/11277#issuecomment-105249937
</comment><comment author="emaudoux" created="2016-05-18T14:53:08Z" id="220051612">I finally found the problem, it was a silly mistake, thank you for the time you spent on this. And sorry for the disturbance. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic search missing modifier symbol in symbol matcher.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18388</link><project id="" key="" /><description>Last week I got a problem that elastic search didn't support back quote and I found out the reason. It seems that elastic missed 'modifier symbol' in symbol matcher. I found code in CharMatcher.java:
`SYMBOL {`
`@Override`
            `public boolean isTokenChar(int c) {`
                `switch (Character.getType(c)) {`
                `case Character.CURRENCY_SYMBOL:`
                `case Character.MATH_SYMBOL:`
                `case Character.OTHER_SYMBOL:`
                    `return true;`
                `default:`
                     `return false;`
                `}`
           `}`
        `}`
Appearently Character.MODIFIER_SYMBOL is missing here. I think it should be a bug.
</description><key id="155183748">18388</key><summary>Elastic search missing modifier symbol in symbol matcher.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">qpily</reporter><labels><label>:Analysis</label><label>bug</label></labels><created>2016-05-17T06:09:22Z</created><updated>2016-05-23T08:27:25Z</updated><resolved>2016-05-23T08:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-17T09:37:55Z" id="219668119">@jpountz could you look at this please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unknown setting [discovery.ec2.ping_timeout]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18386</link><project id="" key="" /><description>about 5.0.0-alpha2.

My settings are as follows:

``` yml
discovery.type: ec2
discovery.ec2.ping_timeout: 60s
```

Logs are as follows:

``` java
Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [discovery.ec2.ping_timeout] did you mean any of [discovery.zen.ping_timeout, discovery.zen.fd.ping_timeout, discovery.zen.join_timeout, discovery.zen.publish_timeout, discovery.zen.commit_timeout]?
    at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:267)
    at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
    at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:120)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:235)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
    at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
```

`discovery.ec2.ping_timeout` is defined in [Docs](https://www.elastic.co/guide/en/elasticsearch/plugins/master/discovery-ec2-discovery.html)

Is this a bug ?
</description><key id="155171691">18386</key><summary>unknown setting [discovery.ec2.ping_timeout]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mats116</reporter><labels /><created>2016-05-17T03:52:17Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-05-17T05:07:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-17T05:07:13Z" id="219620411">You need to install the `discovery-ec2` plugin.
</comment><comment author="s1monw" created="2016-05-17T07:14:10Z" id="219637599">@rjernst hmm maybe we need to add some DYM like functionality for prefixes too and list at least build-in plugins. I think this can reduce the noise for stuff like this on the issue tracker?
</comment><comment author="rjernst" created="2016-05-27T04:46:19Z" id="222059310">@s1monw I think the only way to do that would be to include all plugin settings somehow inside ES itself (perhaps a resource file generated in the build from the plugin source)? But I don't think it should be something managed by hand, it would just get out of date quickly?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add method overloading based on arity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18385</link><project id="" key="" /><description>Currently painless only keys methods, constructors, static methods based on `name`. This means we cannot have multiple signatures with the same name, which makes it difficult to expand the whitelist.

For example `new Date()` vs `new Date(long)`. 

Here we just allow to overload based on number of parameters vs signatures: to keep everything still simple (including the user since much typing is dynamic and implicit conversions could get complex), but allow improvements for many cases where this is really needed.

For methods i added variants of `String.indexOf`, but in order to test the ctor and static method case I added a `FeatureTest` to the whitelist. This also contains getter/setters for loads and stores which we need to test but aren't yet exposed to the whitelist. It needs to be possible to divorce these two things, so we don't have to rush the api just to test.

We key methods on MethodKey (name + arity), but this only impacts the slow path for method lookups, the caching still works the same, so it does not have a performance impact.
</description><key id="155171105">18385</key><summary>Add method overloading based on arity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-17T03:45:31Z</created><updated>2016-05-17T13:41:13Z</updated><resolved>2016-05-17T12:03:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-17T03:53:03Z" id="219613062">LGTM!  Thanks @rmuir 
</comment><comment author="uschindler" created="2016-05-17T06:43:36Z" id="219632797">LGTM.

About MethodKey: Maybe don't use the MethodKey class at all and instead do it like forbidden: Forbidden creates a pure String key (e.g. 'name@arity').

Thats just an idea.
</comment><comment author="rmuir" created="2016-05-17T12:03:41Z" id="219697620">I don't think we should mix string concatenation here at all? I also don't plan to use tuple here, ever. that is horrible.
</comment><comment author="uschindler" created="2016-05-17T12:15:18Z" id="219699959">Sorry the Tuple was a suggestion. I will remove the comment, just for you!
</comment><comment author="rmuir" created="2016-05-17T12:18:14Z" id="219700543">I would rather painless not have this feature than use that horrible Tuple.

I am afraid if i commit this, that someone will refactor it to use Tuple later. 

Quality not quantity.
</comment><comment author="uschindler" created="2016-05-17T12:38:30Z" id="219704836">I won't do that, trust me. My initial suggestion was to just use simple strings as key, but this involves more overhead in the fallback case (you need to first concat a string + the integer, vs just creating an object for the lookup in the map). So nuke this suggestion, too. For forbiddenapis its fine, but here the solution you brought is better.
</comment><comment author="uschindler" created="2016-05-17T13:41:13Z" id="219720658">Thanks Robert for commiting! :100: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include script field even if it value is null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18384</link><project id="" key="" /><description>This is my first contribution, so any feedback is appreciated.  

Closes #16408.
</description><key id="155168618">18384</key><summary>Include script field even if it value is null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jeff303</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-17T03:20:23Z</created><updated>2016-06-17T20:41:40Z</updated><resolved>2016-06-17T20:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-03T21:26:23Z" id="223698721">@jeff303 Thanks for doing this! This is a great first contribution! I left a bunch of comments about the test. The short version is that you based the test on a very old test because that is where we were testing script fields. It was the right thing to do but the whole test can be about half the length with new constructs that we have now. I figure since this is a new test it'd be good to use the new constructs.
</comment><comment author="nik9000" created="2016-06-17T20:30:36Z" id="226873998">Looks good to me! I'll test locally and merge.
</comment><comment author="nik9000" created="2016-06-17T20:41:39Z" id="226876340">Thanks for fixing this @jeff303 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>more control shard allocation by node?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18383</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:  1.7 or 2.x

**JVM version**: 1.8

**OS version**: centos 6.5

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
can make some nodes  only allocation primary shard ,and other nodes only allow allocation replica shards-----just for backup

and more , if index have two replica shard,can config it some node allow allocation one sets replica,one some nodes allow allocation one sets&#65292;

for example :have a index name "demo",have 2 shards
node1,node2:   demo-0-p   demo-1-p
node3,node4:   demo-0-r1  demo-1-r1
node5,node6:   demo-0-r2  demo-1-r3
</description><key id="155160621">18383</key><summary>more control shard allocation by node?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sshling</reporter><labels /><created>2016-05-17T01:56:08Z</created><updated>2016-05-17T07:13:08Z</updated><resolved>2016-05-17T07:13:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-17T02:06:24Z" id="219600986">What do you think should happen if, for a given shard, all the nodes that you've marked as eligible for hosting a primary shard have died?
</comment><comment author="sshling" created="2016-05-17T06:49:22Z" id="219633684">yes .  
In Multi machine room &#65292; one for all normal use ,primary + replicate  process client request&#65292;but in a  accident  , this room all nodes power failure&#65292;so  another room with a sets replicate can be continue process client request , thanks .
</comment><comment author="s1monw" created="2016-05-17T07:13:08Z" id="219637430">this is what [allocation awareness](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-awareness.html) is for. 

please ask questions first on the [discuss list](https://discuss.elastic.co)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor WriterUtils to extend ASM GeneratorAdapter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18382</link><project id="" key="" /><description>...as "MethodWriter" for easy usage (and more coming later).

This makes usage a bit easier and does not need to import lots of static methods. WriterUtils was perfectly fine, so if you wnder why I did this refactoring: There comes more!

The idea is (will be separate PR) to also support Java 9's optimized string concats ("indify string concatenation"). Java 9 has still backwards compatibility of course for old-style StringBuilder concats, but the preferred way from Java 9 on is to collect all types (strings, primitive types,...) on the stack (up to 200 pieces) and finally call invokedynamic using http://download.java.net/java/jdk9/118/docs/api/java/lang/invoke/StringConcatFactory.html

The idea is to make the new MethodWriter support this. But as it has to collect the types of arguments during populating the stack for the string concat, it needs to keep state. Thats not possible with WriterUtility.

My new PR afterwards will use Class.forName &amp; Co. to check the JDK for StringConcatFactory and then use the new code, which is heavy optimized in Java 9.
</description><key id="155146496">18382</key><summary>Refactor WriterUtils to extend ASM GeneratorAdapter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T23:40:15Z</created><updated>2016-05-17T09:24:25Z</updated><resolved>2016-05-17T01:12:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-05-17T00:27:03Z" id="219587377">Sorry, I started again. I broke my local branch - now all is fine.
</comment><comment author="rmuir" created="2016-05-17T00:48:38Z" id="219590566">I really like it! I was thinking of a similar thing when wanting to add debug instructions with line numbers, just a two line helper at the moment, but i had to put it in a strange place: https://github.com/elastic/elasticsearch/blob/master/modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ANode.java#L49-L58

I can move this one to MethodWriter after this (i am thinking just a simple method taking 'int lineNumber')

I think this is better as we can both add assertions/checks if we need to existing writes but also "macros" like this more intuitively.
</comment><comment author="jdconrad" created="2016-05-17T01:05:40Z" id="219592871">@uschindler Thank you this looks like a great change to me!  Is this ready for commit?
</comment><comment author="uschindler" created="2016-05-17T01:10:08Z" id="219593448">Yes. All fine.
</comment><comment author="uschindler" created="2016-05-17T01:11:39Z" id="219593663">You may change the usual name "adapter" to "writer" in the local impl methods of Nodes, but thats not needed for this issue.

I want this in, so it does not go out of date because it touches many files.
</comment><comment author="jdconrad" created="2016-05-17T01:12:20Z" id="219593752">Done!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed unused AllocationService member in TransportClusterAllocationExplainAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18381</link><project id="" key="" /><description /><key id="155136403">18381</key><summary>Removed unused AllocationService member in TransportClusterAllocationExplainAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Allocation</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T22:25:59Z</created><updated>2016-05-16T22:42:45Z</updated><resolved>2016-05-16T22:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-16T22:35:07Z" id="219568985">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add note to contributing docs about force push</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18380</link><project id="" key="" /><description>This commit adds a note to the contributing docs regarding force pushing
during the review process.
</description><key id="155126443">18380</key><summary>Add note to contributing docs about force push</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label></labels><created>2016-05-16T21:29:09Z</created><updated>2016-05-16T21:34:33Z</updated><resolved>2016-05-16T21:34:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-16T21:32:55Z" id="219554920">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Bug in Painless Assignment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18379</link><project id="" key="" /><description>Added some tests for Def optimization and found a minor bug in assignment under a rare corner case.
</description><key id="155126175">18379</key><summary>Fix Bug in Painless Assignment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T21:27:43Z</created><updated>2016-05-16T23:41:53Z</updated><resolved>2016-05-16T23:41:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-16T21:39:48Z" id="219556609">Lets get @uschindler to look. I do like the little bytecode assert in the test for now :)
</comment><comment author="uschindler" created="2016-05-16T22:08:48Z" id="219563573">Looks OK. Sorry for the issue :-)
</comment><comment author="uschindler" created="2016-05-16T22:18:05Z" id="219565471">&gt; I do like the little bytecode assert in the test for now :)

Yes, very cool. Maybe add some utility method to the ScriptTester base class to do those asserts.
</comment><comment author="jdconrad" created="2016-05-16T22:51:01Z" id="219571970">@uschindler Thank you for the feedback.  Updated the changes a bit based on your feedback.  Everything look okay?
</comment><comment author="uschindler" created="2016-05-16T23:09:55Z" id="219575339">LGTM!
</comment><comment author="jdconrad" created="2016-05-16T23:41:48Z" id="219580459">@uschindler Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor JvmGcMonitorService for testing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18378</link><project id="" key="" /><description>This commit refactors the JvmGcMonitorService so that it can be
tested. In particular, hooks are added to verify that the
JvmMonitorService correctly observes slow GC events, and that the
JvmGcMonitorService logs the correct messages.
</description><key id="155119226">18378</key><summary>Refactor JvmGcMonitorService for testing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T20:52:13Z</created><updated>2016-05-17T17:05:36Z</updated><resolved>2016-05-17T17:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-17T01:51:30Z" id="219599029">Thanks @nik9000. I've responded to your feedback.
</comment><comment author="nik9000" created="2016-05-17T16:06:44Z" id="219767580">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename AggregatorBuilder to AggregationBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18377</link><project id="" key="" /><description>Rename AggregatorBuilder and all of its subclasses to AggregationBuilder, in keeping consistent with the Java APIs.

Closes #18367
</description><key id="155085275">18377</key><summary>Rename AggregatorBuilder to AggregationBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T18:04:02Z</created><updated>2016-05-19T13:29:54Z</updated><resolved>2016-05-19T13:29:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-17T12:36:57Z" id="219704493">@clintongormley I'm not sure if non-issue is the correct label, because if the rename doesn't happen, then the Java API docs need to be updated as they all still refer to `AggregationBuilder`.  See #18367 
</comment><comment author="jpountz" created="2016-05-19T07:29:45Z" id="220248290">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate soon-to-be-unsupported queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18376</link><project id="" key="" /><description>This is a follow-up to #18276.
</description><key id="155070607">18376</key><summary>Deprecate soon-to-be-unsupported queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>v2.4.0</label></labels><created>2016-05-16T16:48:17Z</created><updated>2016-05-23T08:32:11Z</updated><resolved>2016-05-23T08:32:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-16T17:02:00Z" id="219481650">LGTM
</comment><comment author="rjernst" created="2016-05-16T17:50:50Z" id="219494996">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>service install error on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18375</link><project id="" key="" /><description>fixed envionment variable setting for java_home while run service
install bat on windows OS
</description><key id="155045057">18375</key><summary>service install error on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhiaixuexi</reporter><labels /><created>2016-05-16T14:46:56Z</created><updated>2016-05-16T14:53:04Z</updated><resolved>2016-05-16T14:52:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-16T14:52:14Z" id="219445439">Thanks for the PR @zhiaixuexi. I left a [comment](https://github.com/elastic/elasticsearch/issues/18373#issuecomment-219444162) on #18373. There is no bug here.

Also, note that the `%` around `JAVA_HOME` are escaped for a reason: this is so that the service gets installed with the Java Virtual Machine having the path `%JAVA_HOME%\jre\bin\server\jvm.dll` instead of being tied to a specific Java Virtual Machine.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix response format of /customer/external/1?pretty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18374</link><project id="" key="" /><description>The response format is displaying elements "found" and "source" in the same lines and they should appear on their own separate lines.
</description><key id="155042005">18374</key><summary>Fix response format of /customer/external/1?pretty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rodol-fo</reporter><labels><label>docs</label></labels><created>2016-05-16T14:32:44Z</created><updated>2016-05-16T14:42:15Z</updated><resolved>2016-05-16T14:38:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rodol-fo" created="2016-05-16T14:35:01Z" id="219440920">I just signed the CLA --
</comment><comment author="nik9000" created="2016-05-16T14:35:38Z" id="219441084">Makes sense to me. You sent this PR against the 1.5 branch which is super out of date. Can you send it against master?
</comment><comment author="jasontedor" created="2016-05-16T14:35:43Z" id="219441104">The 1.5 branch is no longer maintained, but it looks like the same issue exists in master. I will integrate there.
</comment><comment author="jasontedor" created="2016-05-16T14:42:15Z" id="219442772">Thanks @rodol-fo.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can not install service correctly on windows using "service.bat"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18373</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**elasticsearch-2.3.2**:

**jdk1.8**:

**windows 7**:

*_Can not install service correctly on windows using "service.bat", startup service after execute "service install" , you will get an error "can not find %java_home%/jre/bin/server/jvm.dll" *_:

**Steps to reproduce**:
 1.unzip the elasticsearch, setting environment variables well
 2.run service install [yourservicename]
 3.service start [yourservicename]

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

*_Well, i guess the error is caused by a small mistake in service.bat, function - doInstall, --Jvm "%%JAVA_HOME%%%JVM_DLL%" , a couple of more percent sign added around JAVA-HOME here  *_:
</description><key id="155039896">18373</key><summary>can not install service correctly on windows using "service.bat"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhiaixuexi</reporter><labels /><created>2016-05-16T14:23:02Z</created><updated>2016-05-19T12:01:26Z</updated><resolved>2016-05-16T14:47:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-16T14:47:19Z" id="219444162">There is no bug here, you just don't have Java set up correctly. In particular, those forward slashes are probably causing trouble. Also, I just downloaded 2.3.2 onto a Windows VM and tested.

Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.
</comment><comment author="zhiaixuexi" created="2016-05-16T15:01:39Z" id="219448151">@jasontedor  Thanks a lot.
</comment><comment author="jasontedor" created="2016-05-16T15:03:22Z" id="219448643">@zhiaixuexi You're welcome.
</comment><comment author="zhiaixuexi" created="2016-05-16T15:28:20Z" id="219455769">@jasontedor 
I don't know where i made mistake but it still not work .by execute service manager command and i got this
![image](https://cloud.githubusercontent.com/assets/11385718/15294900/baaab206-1bc0-11e6-890b-5eef4e2613e9.png)
This is the error log below.
![error log](https://cloud.githubusercontent.com/assets/11385718/15294168/c1e5ea5c-1bbd-11e6-918b-7744db123bdb.png)
everything goes well untill i changed 
"%%JAVA_HOME%%%JVM_DLL%" to "%JAVA_HOME%%JVM_DLL%".
</comment><comment author="jasontedor" created="2016-05-16T15:41:33Z" id="219459476">That `Jdk1.8` looks out of place. You should have `JAVA_HOME` set to something like `C:\Program Files\Java\jdk1.8.0_92` (or whatever update you have) and that value of that "Java Virtual Machine" property should just be `%JAVA_HOME%\jre\bin\server\jvm.dll`.
</comment><comment author="zhiaixuexi" created="2016-05-16T16:00:51Z" id="219465061">@jasontedor 
oh, i am sorry i given an error information that mislead you,now env variable on my computer as below
![image](https://cloud.githubusercontent.com/assets/11385718/15294892/b61685e4-1bc0-11e6-83df-dc714fd074ed.png).
but it still shows the error.
But the strange thing is that, i found other services bind with env var likes `%JAVA_HOME%` on my PC are work well, i think i need to carefully check it again.
It is so kind of you to help me ^_^
</comment><comment author="jasontedor" created="2016-05-16T16:18:13Z" id="219469833">Your image shows that you have a system-level environment variable named `%JAVA_HOME%`. It should just be `JAVA_HOME`.
</comment><comment author="zhiaixuexi" created="2016-05-17T00:17:16Z" id="219585983">@jasontedor 
That's it!  everything worked well after i move the `%JAVA_HOME%` from user-level to system-level.
anyhow i start the service by command prompt or via windows services.msc.
This has already perplexed me for a long time. You&#8216;ve done me a big favor ^_^
</comment><comment author="jasontedor" created="2016-05-17T01:09:00Z" id="219593292">You're welcome; I'm happy to have helped.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Whitelist expansion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18372</link><project id="" key="" /><description>Most of these improvements focus on the basic types:
- add compare/compareTo/toHexString/parseX for basic types
- add Integer/Long/Float/Double min/max and remove the imax/lmax stuff
- add Character methods like isWhiteSpace and similar
- add Math E and PI constants.

We need to followup with docs improvements (its already out of date anyway). In general I think the way we present that needs to be re-organized completely. There is a lot more to do here, but these changes are all simple and easy.
</description><key id="155037774">18372</key><summary>Whitelist expansion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T14:12:44Z</created><updated>2016-05-17T12:23:17Z</updated><resolved>2016-05-16T21:08:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-16T17:31:28Z" id="219489545">LGTM.  Thanks for adding more!
</comment><comment author="uschindler" created="2016-05-16T20:08:37Z" id="219532321">Do we also have some real read/write on fields, so we can check painless DefField class? Static fields don't help... Currently there is no way to test the code!
</comment><comment author="rmuir" created="2016-05-16T20:15:54Z" id="219534234">This PR is really just to capture "easy wins". I know the problem there, but we should solve it differently IMO. I also omitted any date apis or similar here for the same reason. They are more complex.
</comment><comment author="jdconrad" created="2016-05-16T20:26:56Z" id="219537270">@uschindler So there were actually tests for this before, but they got removed when we decided to make Definition a singleton.  It should be possible to have a dummy dictionary again for field tests, but this should probably be a separate issue.
</comment><comment author="uschindler" created="2016-05-16T21:10:55Z" id="219549189">@jdconrad thanks for this. I think this is ok for now. I'd suggest to add some "undocumented" fake class for testing that is added to the Definition.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Small cleanup of Debugger class to use StringWriter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18371</link><project id="" key="" /><description>I just noticed this. The shuffling between Strings and byte arrays seems useless!
</description><key id="155033831">18371</key><summary>Small cleanup of Debugger class to use StringWriter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T13:53:13Z</created><updated>2016-05-17T12:23:23Z</updated><resolved>2016-05-16T13:58:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-16T13:59:00Z" id="219431624">thanks @uschindler much better
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor of query profile classes to make way for other profile implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18370</link><project id="" key="" /><description>To prepare for https://github.com/elastic/elasticsearch/issues/10538 this PR refactors some of the profiler classes to allow them to be reused by other profile types. It also renames other classes to make it clear they relate to Query profiling to avoid confusion when another profiler is added later.
</description><key id="155025949">18370</key><summary>Refactor of query profile classes to make way for other profile implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Search</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T13:07:56Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-05-16T15:16:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-05-16T13:08:16Z" id="219420492">@jpountz @polyfractal would you mind reviewing this?
</comment><comment author="jpountz" created="2016-05-16T13:33:17Z" id="219425614">LGTM
</comment><comment author="polyfractal" created="2016-05-16T14:02:29Z" id="219432483">LGTM!
</comment><comment author="polyfractal" created="2016-05-16T15:01:39Z" id="219448155">Rename looks good too.  

Not crazy about the placement of `buildShardResults()` into `Profilers`, but not quite sure where to put it.  That class only deals with query profiling bits (adding/removing to the current context, etc), whereas I'm expecting `buildShardResults()` will accumulate aggregation profiling bits too, since it is helping to construct the final result object.

Maybe it should go in `SearchProfileShardResults`, since it is a helper for assembling the final results?  Dunno.  Just a minor thing though, I think it's fine to merge without the change too.
</comment><comment author="polyfractal" created="2016-05-16T15:14:45Z" id="219451854">For posterity, chatted with @colings86 on slack: `Profilers` will be growing new methods to support agg profiling, so my comment was mostly moot :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query string and boolean operator (AND, OR) not working as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18369</link><project id="" key="" /><description>Hello everyone,

I try to run a simple query string like that :

```
{
   "query": {
      "query_string": {
         "fields": [
            "newsitem.itemdetails.fulltext",
            "newsitem.itemdetails.headline"
         ],
         "query": "(alzheimer AND apple) OR \"Bruno Dubois\"",
         "analyzer": "docAnalyzer"
      }
   },
   "highlight": {
      "fields": {
         "newsitem.itemdetails.fulltext": {
            "number_of_fragments": "0"
         },
         "newsitem.itemdetails.headline": {
            "number_of_fragments": "0"
         }
      },
      "pre_tags": [
         "&lt;test&gt;"
      ],
      "post_tags": [
         "&lt;/test&gt;"
      ]
   }
}
```

I expect to get only documents who alzheimer and apple is present and "Bruno Dubois" should be present.
What I'm getting, is highlights with alzheimer whereas apple is not present in the text ... It's quite confusing.

Here is my settings + mapping :

```
PUT _template/kmcloud_test
{
   "order": 0,
   "template": "kmcloud_test_*",
   "settings": {
      "index": {
         "number_of_replicas": "2",
         "analysis": {
            "filter": {
               "elision": {
                  "type": "elision",
                  "articles": [
                     "l",
                     "m",
                     "t",
                     "qu",
                     "n",
                     "s",
                     "j",
                     "d"
                  ]
               }
            },
            "analyzer": {
               "docAnalyzer": {
                  "type": "custom",
                  "filter": [
                     "lowercase",
                     "asciifolding",
                     "elision"
                  ],
                        "tokenizer": "doc_tokenizer"
               },
               "casesensitive_analyzer": {
                  "type": "custom",
                  "tokenizer": "standard"
               }
            },
            "tokenizer": {
               "doc_tokenizer": {
                  "type": "pattern",
                  "pattern": "\\s+|\\.$|\\.+\\s+|,|;|:|!|\\}|\\{|\\(|\\)|\\?|&#8217;|\\'|\\/|@"
               }
            }
         },
         "number_of_shards": "3",
         "index": {
            "mapper": {
               "dynamic": "false"
            }
         }
      }
   },
   "mappings": {
      "kmcloud_test": {
         "dynamic": "strict",
         "properties": {
            "indexingdate": {
               "format": "dateOptionalTime",
               "type": "date"
            },
            "sendingdate": {
               "format": "dateOptionalTime",
               "type": "date"
            },
            "newsitem": {
               "properties": {
                  "itemdetails": {
                     "properties": {
                        "translatedheadline": {
                           "properties": {
                              "value": {
                                 "index": "not_analyzed",
                                 "type": "string"
                              },
                              "languageisocode": {
                                 "index": "not_analyzed",
                                 "type": "string"
                              }
                           }
                        },
                        "headline": {
                           "type": "string",
                           "analyzer": "docAnalyzer",
                           "fields": {
                              "cs": {
                                 "analyzer": "casesensitive_analyzer",
                                 "type": "string"
                              }
                           }
                        },
                        "originalurl": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "wordcount": {
                           "type": "integer"
                        },
                        "articletype": {
                           "type": "integer"
                        },
                        "pagelist": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "author": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "imdurl": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "pagecount": {
                           "type": "integer"
                        },
                        "retriurl": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "tweetparentid": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "numberofretweet": {
                           "type": "integer"
                        },
                        "itemfilenames": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "itemkey": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "fulltext200": {
                           "type": "string",
                           "analyzer": "docAnalyzer",
                           "fields": {
                              "cs": {
                                 "analyzer": "casesensitive_analyzer",
                                 "type": "string"
                              }
                           }
                        },
                        "ave": {
                           "type": "integer"
                        },
                        "fulltext": {
                           "type": "string",
                           "analyzer": "docAnalyzer",
                           "fields": {
                              "cs": {
                                 "analyzer": "casesensitive_analyzer",
                                 "type": "string"
                              }
                           }
                        },
                        "languageisocode": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "abstract": {
                           "properties": {
                              "value": {
                                 "index": "not_analyzed",
                                 "type": "string"
                              },
                              "languageisocode": {
                                 "index": "not_analyzed",
                                 "type": "string"
                              }
                           }
                        },
                        "nlaobjectid": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "keywords": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "urls": {
                           "properties": {
                              "urltype": {
                                 "index": "not_analyzed",
                                 "type": "string"
                              },
                              "value": {
                                 "index": "not_analyzed",
                                 "ignore_above": 1000,
                                 "type": "string"
                              }
                           },
                           "type": "nested"
                        },
                        "fulltext100": {
                           "type": "string",
                           "analyzer": "docAnalyzer",
                           "fields": {
                              "cs": {
                                 "analyzer": "casesensitive_analyzer",
                                 "type": "string"
                              }
                           }
                        },
                        "fulltext300": {
                           "type": "string",
                           "analyzer": "docAnalyzer",
                           "fields": {
                              "cs": {
                                 "analyzer": "casesensitive_analyzer",
                                 "type": "string"
                              }
                           }
                        },
                        "header": {
                           "type": "string",
                           "analyzer": "docAnalyzer",
                           "fields": {
                              "cs": {
                                 "analyzer": "casesensitive_analyzer",
                                 "type": "string"
                              }
                           }
                        },
                        "headlinesearch": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "interviewtype": {
                           "type": "integer"
                        },
                        "geolocalisation": {
                           "index": "not_analyzed",
                           "type": "string"
                        },
                        "itemranking": {
                           "type": "integer"
                        },
                        "typeoftweet": {
                           "index": "not_analyzed",
                           "type": "string"
                        }
                     }
                  }
               }
            }
         },
         "_all": {
            "enabled": false
         }
      }
   },
   "aliases": {
      "kmcloud_test": {}
   }
}
```

Document :

```

{
    "indexingdate": "2016-05-13T07:28:39.6555239+02:00",
    "supplier": {
        "suppliername": "FRANCE ACTU REGION",
        "supplierkey": "FAR"
    },
    "newsitem": {
        "customerkey": {
            "disableindexation": false
        },
        "publicationdetails": {
            "media": "Press Publication",
            "publicationkey": "146571",
            "globalpublicationkey": "42398",
            "publicationdate": "2016-05-13T00:00:00",
            "publicationname": "Progr&#232;s [Le] - Oyonnax &#8211; L&#233;man &#8211; Bugey &#8211; Bas-Bugey",
            "supplementname": "",
            "countryisocode": "FR",
            "sector": "",
            "avatar": ""
        },
        "itemdetails": {
            "itemkey": "303512345",
            "headline": "Une &#233;cole de tennis",
            "header": "",
            "translatedheadline": {
                "languageisocode": "EN",
                "value": ""
            },
            "abstract": {
                "languageisocode": "fr",
                "value": ""
            },
            "fulltext": "Une &#233;cole de tennis existe aussi et Alzheimer une vingtaine d&#8217;enfants pratique cette discipline. &#171;&#160;&#8201;Concernant l&#8217;&#233;cole, nous minimisons le groupe car nous souhaitons former en privil&#233;giant la qualit&#233; 
de l&#8217;enseignement&#8201;&#160;&#187;, explique Bruno Dubois.",
            "fulltext100": "Une &#233;cole de tennis existe aussi et une vingtaine d&#8217;enfants pratique cette discipline. &#171;&#160;&#8201;Concernant l&#8217;&#233;cole, nous minimisons le groupe car nous souhaitons former en privil&#233;giant la 
qualit&#233; de l&#8217;enseignement&#8201;&#160;&#187;, explique Bruno Dubois.",
            "fulltext200": "Une &#233;cole de tennis existe aussi et une vingtaine d&#8217;enfants pratique cette discipline. &#171;&#160;&#8201;Concernant l&#8217;&#233;cole, nous minimisons le groupe car nous souhaitons former en privil&#233;giant la 
qualit&#233; de l&#8217;enseignement&#8201;&#160;&#187;, explique Bruno Dubois.",
            "fulltext300": "Une &#233;cole de tennis existe aussi et une vingtaine d&#8217;enfants pratique cette discipline. &#171;&#160;&#8201;Concernant l&#8217;&#233;cole, nous minimisons le groupe car nous souhaitons former en privil&#233;giant la 
qualit&#233; de l&#8217;enseignement&#8201;&#160;&#187;, explique Bruno Dubois.",
            "itemfilenames": [""],
            "originalurl": "",
            "keywords": [""],
            "languageisocode": "fr",
            "author": "",
            "pagelist": "",
            "typeoftweet": "",
            "tweetparentid": "",
            "numberofretweet": 0,
            "geolocalisation": "",
            "urls": [],
            "imdurl": "",
            "retriurl": "",
            "nlaobjectid": ""
        }
    }
}
```

Thank you.

Nicolas.
</description><key id="154996498">18369</key><summary>Query string and boolean operator (AND, OR) not working as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sterchelen</reporter><labels><label>:Highlighting</label></labels><created>2016-05-16T09:51:34Z</created><updated>2016-05-20T09:28:43Z</updated><resolved>2016-05-20T09:28:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-16T10:51:05Z" id="219398841">I have a feeling that this is just how highlighting works.  It extracts possible term matches from the query and looks for those terms in the best hits.  It doesn't try to apply the full bool logic of the query.
</comment><comment author="sterchelen" created="2016-05-16T11:29:15Z" id="219404578">Thank you for you quick response.
Do you know a workaround ? The principal characteristic of an highlight, it's to highlight the terms that match the query. 
</comment><comment author="markharwood" created="2016-05-20T09:28:35Z" id="220559129">&gt; The principal characteristic of an highlight, it's to highlight the terms that match the query.

It is also charged with summarizing a document, selecting the best sections and so it is not always possible to fulfil both of these goals (summarizing and accurately reflecting arbitrary Boolean logic that can span the whole text).

&gt; Do you know a workaround ?

Not a satisfactory one - highlighting is always about compromises. If your application takes responsibility for breaking the text into paragraphs e.g. as nested documents it would be possible to select only the paragraph docs that fully matched your query logic. This would be a more complex arrangement and complicate both the query and indexing logic.

Closing, not because it's not a legitimate complaint but because we've thought about this for a long time in Lucene land and don't think there's a good solution. Sorry :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AggregatorBuilder and PipelineAggregatorBuilder do not need generics.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18368</link><project id="" key="" /><description>Similar reasoning as #18133 but for the aggs API. One important change is that
I moved the base PipelineAggregatorBuilder class to the o.e.s.aggregations
package instead of o.e.s.aggregations.pipeline so that the create method does
not need to be public.
</description><key id="154987767">18368</key><summary>AggregatorBuilder and PipelineAggregatorBuilder do not need generics.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-16T08:56:54Z</created><updated>2016-06-01T08:23:45Z</updated><resolved>2016-06-01T08:23:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-05-31T09:22:42Z" id="222636391">@jpountz I left some minor comments but LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AggregatorBuilder vs. AggregationBuilder in the Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18367</link><project id="" key="" /><description>With the query refactoring, AggregationBuilder was renamed to AggregatorBuilder. While I get the intention since these objects do not create aggregations directly but rather aggregators, this is a bit confusing from the perspective of the Java API since aggregators are completely internal to elasticsearch.

We should either finish this renaming and also rename AggregationBuilders to AggregatorBuilders and fix the documentation of the Java API (which still uses AggregationBuilder), or rename it (and all sub classes) back to AggregationBuilder.

I have a slight preference for option 2 since I think it makes the Java API nicer.
</description><key id="154984356">18367</key><summary>AggregatorBuilder vs. AggregationBuilder in the Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>blocker</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T08:34:17Z</created><updated>2016-05-19T13:29:54Z</updated><resolved>2016-05-19T13:29:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-16T09:13:30Z" id="219382575">cc @colings86 
</comment><comment author="colings86" created="2016-05-16T09:21:25Z" id="219383951">I'm kind of +0 for both options in that I have no strong opinion either way but I would like us to keep AggregatorFactory the same name (rather than renaming it to AggregationFactory) since this is an internal class and we have factory objects on the Aggregation side of things for some aggregations (e.g. histogram) and I think renaming the AggregatorFactory would lead to confusion.
</comment><comment author="jpountz" created="2016-05-16T12:47:05Z" id="219416492">&gt; I would like us to keep AggregatorFactory the same name (rather than renaming it to AggregationFactory) since this is an internal class

+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Add generic packaging assemble task to have one dir with all packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18366</link><project id="" key="" /><description>This change adds an assemble task which puts the zip, tar.gz, deb and
rpm files all in one build directory. It will make it easier for
publishing these files in the unified release.
</description><key id="154961575">18366</key><summary>Build: Add generic packaging assemble task to have one dir with all packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label></labels><created>2016-05-16T05:10:21Z</created><updated>2016-05-16T10:31:24Z</updated><resolved>2016-05-16T06:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-16T06:00:02Z" id="219354878">Nevermind, I found a better way.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>EngineException[Couldn't resolve version] nested: NotSerializableExceptionWrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18365</link><project id="" key="" /><description>I got the following exception, when I tried to update some records, I never seen this exception before, please help:

```
2016-05-15 20:27:04,411][INFO ][rest.suppressed] /index1/aprelease/1122/_update Params: {id=aa22, index=index1, type=aprelease}

RemoteTransportException[[test-es15-a][indices:data/write/update[s]]]; nested: 
EngineException[Couldn't resolve version]; nested: NotSerializableExceptionWrapper[Input/output 
error: NIOFSIndexInput(path="/data/node-a/data/ts-data/nodes/0/indices/index1/18/index/_ypk3.cfs") 
[slice=_ypk3_Lucene50_0.tim]]; nested: NotSerializableExceptionWrapper[Input/output error];

Caused by: [index1][[index1][18]] EngineException[Couldn't resolve version]; nested: 
NotSerializableExceptionWrapper[Input/output error: NIOFSIndexInput(path="/data/node-a/data/ts-
data/nodes/0/indices/index1/18/index/_ypk3.cfs") [slice=_ypk3_Lucene50_0.tim]]; nested: 
NotSerializableExceptionWrapper[Input/output error];
    at org.elasticsearch.index.engine.Engine.getFromSearcher(Engine.java:256)
    at org.elasticsearch.index.engine.InternalEngine.get(InternalEngine.java:349)
    at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:606)
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:173)
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:86)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:76)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:170)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:164)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:65)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:249)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:245)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="154954313">18365</key><summary>EngineException[Couldn't resolve version] nested: NotSerializableExceptionWrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JackWang917</reporter><labels><label>:Exceptions</label><label>:Store</label><label>feedback_needed</label></labels><created>2016-05-16T03:30:37Z</created><updated>2016-05-16T17:56:12Z</updated><resolved>2016-05-16T17:56:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-16T10:29:02Z" id="219395375">What version of Elasticsearch? My initial guess is that you have a corrupt shard.
</comment><comment author="rmuir" created="2016-05-16T12:07:29Z" id="219410156">"input/output error" often points to an error with the filesystem or hardware.
</comment><comment author="JackWang917" created="2016-05-16T17:25:49Z" id="219487950">ES version is 2.1
</comment><comment author="JackWang917" created="2016-05-16T17:52:40Z" id="219495476">I found the reason: The disk was broken. But why ES didn't note that, it seems ES had no ware about the big problem. Is an ES bug?
</comment><comment author="jasontedor" created="2016-05-16T17:56:12Z" id="219496426">It did, when an I/O operation on the disk failed. There is no Elasticsearch bug here. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove LeafSearchScript.runAsFloat(): Nothing calls it.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18364</link><project id="" key="" /><description>Nothing calls this method, I think we should remove it?

See https://github.com/elastic/elasticsearch/pull/18359 for the background. 

The fact we don't know our script's return type until runtime is a big problem for us. It causes boxing on every value for painless right now, but prevents us from removing boxing internally too.

I would love future ideas on how to clearly separate the stuff that returns Object (e.g. ExecutableScript) vs the stuff returning a double for scoring or something like that, because it is something we really need to fix. We really need to know the return type at compile-time. But runAsFloat is clearly useless, so this is an easy step :)

cc @uschindler 
</description><key id="154953081">18364</key><summary>Remove LeafSearchScript.runAsFloat(): Nothing calls it.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>breaking-java</label><label>v5.0.0-alpha3</label></labels><created>2016-05-16T03:14:24Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-05-16T21:08:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-16T04:07:59Z" id="219344687">LGTM.

&gt; I would love future ideas on how to clearly separate the stuff that returns Object (e.g. ExecutableScript) vs the stuff returning a double for scoring or something like that

One idea I have been thinking about is having separate compile methods for each context a script can be used in. Right now, execution of scripts requires passing back to the script engine. Instead, if the object returned by compile was the thing that would be executed, and specific to the context, then we would have the info you seek (for example, search scripts will always return a double). Eventually we can improve the compile methods to take extra arguments specific to each context, for example search scripts or script fields taking mappings. The idea needs a lot more thought (like how this works with the current caching), but I think separating out each place a script can be used with separate requirements is the only way to get the proper info to script compilation.
</comment><comment author="uschindler" created="2016-05-16T09:27:13Z" id="219384968">About painless: I would make 2 or more abstract Executables: One that returns Object and one that returns double or the other types. Painless checks while compiling which one to implement and returns the double one if possible. I think this should be possible to implement with some logic in the Writer if the "actual" type of the root node is primitive.
</comment><comment author="rmuir" created="2016-05-16T11:06:16Z" id="219401230">&gt; About painless: I would make 2 or more abstract Executables: One that returns Object and one that returns double or the other types. Painless checks while compiling which one to implement and returns the double one if possible. I think this should be possible to implement with some logic in the Writer if the "actual" type of the root node is primitive.

yes of course. To me that part is easy :) The hard part is this scripting API. It is what holds us back constantly and harder to make changes to: that is why I made a small step here.
</comment><comment author="jdconrad" created="2016-05-16T17:30:29Z" id="219489281">LGTM as well. Nuke it all!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Flags Parameter for Char Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18363</link><project id="" key="" /><description>Title is self-explanatory.  Closes #18362 .

This is my first PR for the repository, so all comments and critiques are welcome!
</description><key id="154924608">18363</key><summary>Add Flags Parameter for Char Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gfyoung</reporter><labels><label>:Analysis</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-15T19:14:34Z</created><updated>2016-06-09T16:42:47Z</updated><resolved>2016-06-09T16:39:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gfyoung" created="2016-05-15T19:18:11Z" id="219304863">Tangentially related to this PR, but I had a hard time running individual test suites.  After reading the TESTING doc, I tried the following:

```
gradle test "-Dtests.class=*.CharFilterTests"
gradle test -Dtests.class=org.elasticsearch.index.analysis.CharFilterTests

gradle test "-Dtests.class=*.CharFilterTests" "-Dtests.method=testPatternReplaceCharFilter*"
gradle test -Dtests.class=org.elasticsearch.index.analysis.CharFilterTests -Dtests.method=testPatternReplaceCharFilter
```

However, all of these failed, with `gradle` complaining that there were no tests run.  What did I do wrong here with these commands?
</comment><comment author="rjernst" created="2016-05-15T19:21:03Z" id="219305013">&gt; However, all of these failed, with gradle complaining that there were no tests run. What did I do wrong here with these commands?

You need to `cd` into the directory of the project the tests are in (in this case, `core/`). If you look carefully at the failure, i would imagine it fails in a different subproject.
</comment><comment author="gfyoung" created="2016-05-15T19:26:23Z" id="219305310">Ah, I see.  Now it works.  Thanks, @rjernst !
</comment><comment author="nik9000" created="2016-05-16T16:05:21Z" id="219466330">I think rather than exposing flags you should allow for a couple of booleans:

```
"char_filter": {
  "test": {
    "type": "pattern_replace",
    "pattern": "a*a",
    "replacement": "a",
    "case_sensitive": false,
    "comments": true,
    "multiline": true,
    "dotall": true,
    ...
  }
}
```

My reasoning is that what you have works fine for java but is impossible outside of it. I'd prefer to _just_ do it that way so we don't have multiple ways of doing it. I also this it is going to be easier to read the configuration. The way you have it you'll have to mentally AND the flags out of the number which is fine for a few folks but I can't do that with base 10 numbers.

Sorry that that makes it more complicated!

For what it's worth you should be able to run those tests inside Eclipse or IntelliJ if you like. So long as you followed the instructions in CONTRIBUTING.md. **should** work.
</comment><comment author="jasontedor" created="2016-05-16T16:10:40Z" id="219467808">&gt; You need to `cd` into the directory of the project the tests are in (in this case, `core/`).

You can also just run `gradle :core:test` with the additional flags you already had to just run the specific test. The difference is that `gradle test` runs the task `test` across all sub-projects but `gradle :core:test` limits it to just core.
</comment><comment author="nik9000" created="2016-05-16T18:20:28Z" id="219503171">&gt; gradle test runs the task test across all sub-projects

All sub-projects of the directory you are in. You can also do `cd modules; gradle reindex:check` and that'll run `:modules:reindex`'s `check`. Leading the project name with `:` (like `:core:test` instead of `core:test`) means "don't use my path to resolve tasks. Gradle's resolution rules are fun!
</comment><comment author="jasontedor" created="2016-05-16T18:22:59Z" id="219503886">&gt; All sub-projects of the directory you are in.

Sure, I meant from the root since that seems to be where the user was when they encountered their issue. This is an important clarification though, thanks for making it. &#128516; 
</comment><comment author="gfyoung" created="2016-05-16T18:38:00Z" id="219508212">@nik9000 : Okay, but if you read the original issue, this PR is helping to standardize the interface across filters by adding a feature that is already supported in the other filters.  What you're suggesting is a much larger API change that would have to apply across all of the filters.  I would only give that more consideration if more people think that's a good idea.  Otherwise, I will leave it as is.

I might also add that having to set the flags for every single query can become tedious, especially if you use the same filters over and over again.  It's much easier to abstract those flags into a number in cases like that instead of having to remember every boolean that you set.
</comment><comment author="jasontedor" created="2016-05-16T18:57:58Z" id="219513970">It's best to not force push over your commits once your PR has been opened.
</comment><comment author="gfyoung" created="2016-05-16T19:01:29Z" id="219514991">@jasontedor : That's a practice I've carried over from previous work / open source contributions.  I re-run test suites before making such pushes FYI.
</comment><comment author="jasontedor" created="2016-05-16T19:05:19Z" id="219516084">&gt; That's a practice I've carried over from previous work / open source contributions.

Force pushing over publicly visible code is generally something to be avoided. Incremental commits are easier on the reviewers. I'll add a note to our contributing guide about it.

&gt; I generally rebase and re-run test suites before making such pushes.

It's better to push new commits. Rebasing can be replaced with merging which is generally not needed until the end of the review process unless you have to pick up some change from core or will see a merge conflict otherwise.
</comment><comment author="nik9000" created="2016-05-16T19:05:52Z" id="219516245">&gt; I would only give that more consideration if more people think that's a good idea.

Fair enough. I don't think we should keep going this way because flags parameters aren't readable without the constants.
</comment><comment author="jasontedor" created="2016-05-16T19:07:36Z" id="219516722">&gt; I don't think we should keep going this way because flags parameters aren't readable without the constants.

I agree, it's just leaking the Java `Pattern` API into our API.
</comment><comment author="clintongormley" created="2016-05-17T09:17:21Z" id="219663323">&gt; I think rather than exposing flags you should allow for a couple of booleans:

@nik9000 we expose this as a string `flags` parameter in other regex-related APIs, eg see https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-analyzer.html

&gt; Flags should be pipe-separated, eg "CASE_INSENSITIVE|COMMENTS". Check Java Pattern API for more details about flags options.
</comment><comment author="gfyoung" created="2016-05-23T21:08:21Z" id="221096761">&lt;b&gt;@Everyone&lt;/b&gt;: bump - any updates on this?
</comment><comment author="rmuir" created="2016-05-23T22:05:58Z" id="221110373">&gt; @nik9000 we expose this as a string flags parameter in other regex-related APIs, eg see https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-analyzer.html

Yes, but keep in mind when adding jdk-specific regex parameters that users complain about the java regex implementation and open bugs in elasticsearch/lucene about it (e.g. https://issues.apache.org/jira/browse/LUCENE-7256). These bugs will never get fixed:  there is nothing we can do about it: other than switching to a different regex impl, since those issues are jdk issues.
</comment><comment author="gfyoung" created="2016-05-30T22:35:28Z" id="222561988">&lt;b&gt;@Everyone&lt;/b&gt;: bump - any updates on this?
</comment><comment author="clintongormley" created="2016-06-01T16:36:28Z" id="223050978">&gt; I agree, it's just leaking the Java Pattern API into our API.

These flags are Java Pattern dependent.  Other regex engines (which we don't use today) will only support some of these flags, and may have others that they need adding.  My inclination is to stick with the current syntax used elsewhere until we have plans to change the regex engine or make it pluggable.
</comment><comment author="gfyoung" created="2016-06-08T10:00:28Z" id="224544593">&lt;b&gt;@Everyone&lt;/b&gt;: bump - any updates on this?  Would love to get feedback on how I can help get this merged.
</comment><comment author="nik9000" created="2016-06-08T17:39:27Z" id="224669945">&gt; @Everyone: bump - any updates on this? Would love to get feedback on how I can help get this merged.

Sorry! I think if you switched:

``` java
+        pattern = Pattern.compile(sPattern, settings.getAsInt("flags", 0));
```

with something more like the way the PatternAnalyzerProvider does it that'd be good enough for me. Like:

``` java
+        pattern = Regex.compile(sPattern, settings.get("flags"));
```

Then we can use the same string-based flags that we have for the pattern analyzer.

The problem with these being regex implementation specific is a problem, but we've already bound Elasticsearch to java's regex so I don't think this makes it any worse.
</comment><comment author="gfyoung" created="2016-06-09T05:25:46Z" id="224803080">@nik9000 : changed it to `Regex.compile` and modified test appropriately - everything still passes
</comment><comment author="nik9000" created="2016-06-09T16:37:54Z" id="224953017">OK! Looks good to me! I'll pull it locally, check it one last time so I'm 100% sure I don't break the build, and then merge it!
</comment><comment author="nik9000" created="2016-06-09T16:42:47Z" id="224954388">Merged, thanks @gfyoung! I've also pushed 09cc4c449aa16cbc0aa0cb14c3fac6c206f41382 with some docs for it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add regex flags to pattern-replace character filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18362</link><project id="" key="" /><description>In the `pattern` tokenizer, we allow setting regex flags via the `flags` parameter.  The same support should be added to the `pattern_replace` character filter.
</description><key id="154910929">18362</key><summary>Add regex flags to pattern-replace character filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-05-15T14:19:24Z</created><updated>2016-06-09T16:39:23Z</updated><resolved>2016-06-09T16:39:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gfyoung" created="2016-05-15T15:19:25Z" id="219291760">Hi!  I'm a first time contributor trying to find my way around.  Is &lt;a href="https://github.com/elastic/elasticsearch/blob/0f00c14afc8428a2a72c0b766d2171029dc8f6e1/core/src/main/java/org/elasticsearch/index/analysis/PatternReplaceCharFilterFactory.java"&gt;this&lt;/a&gt; the correct file to make the changes?
</comment><comment author="clintongormley" created="2016-05-15T17:41:38Z" id="219299659">@gfyoung yes, that's the one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>copy_to not working as expected with the mapper plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18361</link><project id="" key="" /><description>**Elasticsearch version**:
2.3.2

**JVM version**:
1.7.0_101 x64

**OS version**:
Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:
Hi,
i am using Elasticsearch 2.3.2 with the Mapper Attachments Plugin. I try to copy the extracted content to another field, as described [here](https://github.com/elastic/elasticsearch-mapper-attachments#copy-to-feature)
This is not working for me, though.

I also found a closed bug here #14946. But the discussion does it not make clear, whether the bug has been resolved or not.

Therefore i would be happy, if you could give me a definitive answer, whether this is possible or not.

**Steps to reproduce**:
curl -XPOST 'http://localhost:9200/test' -d '{
  "entry": {
    "properties": {
      "file": {
        "type": "attachment",
        "fields": {
          "content": {
            "type": "string",
            "copy_to": "fulltext_en"
          }
        }
      },
      "fulltext_en": {
        "type": "string",
        "analyzer": "english"
      }
    }
  }
}'

curl -XPOST 'http://localhost:9200/test/entry' -d '{
  "file": "TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ="
}'
// the base64 is "Lorem ipsum dolor sit amet"

curl -XPOST 'http://localhost:9200/test/_search' -d '
{
  "query": {
    "match": {
      "fulltext_en": "ipsum"
    }
  }
}'

the result of the search is then empty, which is wrong, as "ipsum" is contained in the base64.
</description><key id="154905402">18361</key><summary>copy_to not working as expected with the mapper plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sikron</reporter><labels><label>:Plugin Mapper Attachment</label><label>docs</label></labels><created>2016-05-15T12:05:36Z</created><updated>2016-05-16T09:35:12Z</updated><resolved>2016-05-15T17:33:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-15T17:27:25Z" id="219298832">Hi @sikron 

Yes, `copy_to` is no longer allowed on multi-fields as of https://github.com/elastic/elasticsearch/pull/10802.  In fact, for field types other than `attachment`, we throw an exception if you try to add `copy_to` to a multi-field (https://github.com/elastic/elasticsearch/pull/15213).  This isn't something we can fix in mappings, without introducing a lot of complexity which is likely to carry bugs with it.  

The mapper-attachment plugin is deprecated in favour of the [ingest-attachment plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest-attachment.html), so this is not something that we're going to fix, but I will remove the docs claiming that we support `copy_to`.
</comment><comment author="sikron" created="2016-05-16T09:35:11Z" id="219386393">hi, thx for clarification!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FSync translog outside of the writers global lock</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18360</link><project id="" key="" /><description>Today we aquire a write global lock that blocks all modification to the
translog file while we fsync / checkpoint the file. Yet, we don't necessarily
needt to block concurrent operations here. This can lead to a lot of blocked
threads if the machine has high concurrency (lot os CPUs) but uses slow disks
(spinning disks) which is absolutely unnecessary. We just need to protect from
fsyncing / checkpointing concurrently but we can fill buffers and write to the
underlying file in a concurrent fashion.

This change introduces an additional lock that we hold while fsyncing but moves
the checkpointing code outside of the writers global lock.

I had some conversation with @mikemccand who saw congestion on these locks. @mikemccand can you give this patch a go if you see better concurrency? I also think we need to run the powertester on this one again :)
</description><key id="154905081">18360</key><summary>FSync translog outside of the writers global lock</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-15T11:57:35Z</created><updated>2016-05-19T07:40:11Z</updated><resolved>2016-05-19T07:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-05-15T13:54:07Z" id="219287119">Thanks @s1monw I will test this!
</comment><comment author="mikemccand" created="2016-05-15T14:07:23Z" id="219287875">LGTM
</comment><comment author="mikemccand" created="2016-05-16T18:38:32Z" id="219508357">I think this change is potentially a biggish performance gain on highly concurrent CPUs with slowish IO.

I tested on 72 core box, on a spinning disk, and without the change (2 runs):

```
  67.3 M docs in 948.4 seconds
  67.3 M docs in 946.3 seconds
```

and 2 runs with the change:

```
  67.3 M docs in 858.1 seconds
  67.3 M docs in 884.7 seconds
```

I also ran the "random power loss tester" and there were no corruptions after 18 power loss events (at which point I hit disk full!).
</comment><comment author="s1monw" created="2016-05-17T07:38:19Z" id="219641999">@mikemccand I pushed a new commit
</comment><comment author="bleskes" created="2016-05-17T07:42:52Z" id="219642846">@mikemccand out of curiosity - how many concurrent threads did you use to index and how many shards are in the index?
</comment><comment author="mikemccand" created="2016-05-17T14:34:32Z" id="219737142">LGTM, thanks @s1monw 
</comment><comment author="mikemccand" created="2016-05-17T14:36:44Z" id="219737866">&gt;  how many concurrent threads did you use to index and how many shards are in the index?

6 shards, 12 client side threads, and I told ES to use 36 cores (though I did have a local mod to relax its current 32 core max).  I disabled auto-IO merge throttling, and ran with translog durability `async`.
</comment><comment author="mikemccand" created="2016-05-17T18:49:55Z" id="219816344">I re-ran perf test with default translog durability (`request`) fsync:

```
before.log.0: Indexer: 56952280 docs: 1139.35 sec [49986.8 dps, 16.0 MB/sec]
before.log.1: Indexer: 56952280 docs: 1133.59 sec [50240.5 dps, 16.1 MB/sec]
before.log.2: Indexer: 56952280 docs: 1156.01 sec [49266.4 dps, 15.8 MB/sec]

 after.log.0: Indexer: 56952280 docs: 1058.93 sec [53782.8 dps, 17.2 MB/sec]
 after.log.1: Indexer: 56952280 docs: 1064.17 sec [53518.0 dps, 17.2 MB/sec]
 after.log.2: Indexer: 56952280 docs: 1046.26 sec [54433.9 dps, 17.4 MB/sec]
```

This is powerful CPU (36 real cores) against slowish IO (single spinning 8 TB disk) ... I think there are real gains here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove boxing when loading and storing values in "def" fields/arrays, remove boxing onsimple method calls of "def" methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18359</link><project id="" key="" /><description>I finally managed to get the field and array stores to `def` fields or arrays no longer box. The problem to solve was to hack into `EChain.analyzeWrite` and promote the type the other way round:
- If it finds out that last node (the store link) in the chain accepts a `def` type, the code changes the return type of the expression (of which result should be stored) and disables casting the usual way. After that the return type of the expression gets promoted to directly to the store node.
- The code in field store and array store's `write()` was adapted to use the promoted type in the descriptor (this change was missing in my last PR already).

In addition in this PR I made all invokedynamic calls use the `GeneratorAdapter` method name instead on the underlying visitor's method name. This makes all invokes named the same throughout codebase.

Another change is removal of a useless List -&gt; array clone.
</description><key id="154902618">18359</key><summary>Remove boxing when loading and storing values in "def" fields/arrays, remove boxing onsimple method calls of "def" methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-15T11:01:19Z</created><updated>2016-05-17T09:19:21Z</updated><resolved>2016-05-16T19:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-05-15T11:06:48Z" id="219278885">I dumped the bytecode of several array tests (field stores are hard at the moment; they are also completely untested, as far as I know):

First example:
`def x = new int[4]; x[0] = 5; return x[0];`

This creates the following invokedynamic:

```
  public execute(Ljava/util/Map;Lorg/apache/lucene/search/Scorer;Lorg/elasticsearch/search/lookup/LeafDocLookup;Ljava/lang/Object;)Ljava/lang/Object;
   L0
    LINENUMBER 1 L0
    ICONST_4
    NEWARRAY T_INT
    ASTORE 5
   L1
    LINENUMBER 1 L1
    ALOAD 5
    ICONST_0
    ICONST_5
    INVOKEDYNAMIC arrayStore(Ljava/lang/Object;II)V [
      // handle kind 0x6 : INVOKESTATIC
      org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I)Ljava/lang/invoke/CallSite;
      // arguments:
      4
    ]
   L2
[...]
```

As you see there is no boxing involved anymore, the value is pushed unmodified to stack.

It also works if the value to Store is originally a String, although it does not really involves boxing, but you see that type passed to invokedyanmic is preserved:

```
def x = new String[4]; x[0] = 'foobar'; return x[0];

  public execute(Ljava/util/Map;Lorg/apache/lucene/search/Scorer;Lorg/elasticsearch/search/lookup/LeafDocLookup;Ljava/lang/Object;)Ljava/lang/Object;
   L0
    LINENUMBER 1 L0
    ICONST_4
    ANEWARRAY java/lang/String
    ASTORE 5
   L1
    LINENUMBER 1 L1
    ALOAD 5
    ICONST_0
    LDC "foobar"
    INVOKEDYNAMIC arrayStore(Ljava/lang/Object;ILjava/lang/String;)V [
      // handle kind 0x6 : INVOKESTATIC
      org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I)Ljava/lang/invoke/CallSite;
      // arguments:
      4
    ]
   L2
[...]
```
</comment><comment author="rmuir" created="2016-05-15T14:16:31Z" id="219288334">Thanks for killing more boxing! I will look when i get home.
</comment><comment author="uschindler" created="2016-05-15T15:46:46Z" id="219293253">Thanks @rmuir!
I am currently not sure if the code here is 100% correct, because it misuses the `before` field. I will change the PR to add a field `storeType` or similar. I can then also add an instanceof-check instead of the stupid "hack" I did now. The new code in `analyzeWrite` should really only be done for stores on DEF fields, so a superclass for `LDefField` and `LDefArray` should be introduced.
The current solutions is a hack and may break in future.
</comment><comment author="uschindler" created="2016-05-15T16:34:53Z" id="219295986">New version, I nuked the old one. This is now clean.

The new superclass of `LDef*` Link nodes is also useful to handle return values correctly. For this we can have a similar approach that makes method calls or "loads" get the type that is really expected. This is also the reason why I adapted the superclass of unrelated `LDefCall`, too.
</comment><comment author="uschindler" created="2016-05-15T17:53:38Z" id="219300278">I also was able to remove boxing from loads and method calls - as long as they are simple expressions. Still not solved are compound statements (increments,...). I have to first understand `EChain.analyzeCompound()`. But I am sure there is a solution to find the simplest possible type.

The new code works similar to before: It checks if the link node used for read is a `LDefLink` and the expression got a "expected" output type (which is set by parent node). If this is given it will adapt the output type of the last node to the expected type. Because of that no cast is needed.
</comment><comment author="uschindler" created="2016-05-15T17:56:51Z" id="219300461">Example: `def x = new HashMap(); x['abc'] = 5; int z = x.get('abc'); return z;`

Bytecode for the part `int z = x.get('abc')`:

```
    ALOAD 5
    LDC "abc"
    INVOKEDYNAMIC get(Ljava/lang/Object;Ljava/lang/String;)I [
      // handle kind 0x6 : INVOKESTATIC
      org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I)Ljava/lang/invoke/CallSite;
      // arguments:
      0
    ]
    ISTORE 6
```
</comment><comment author="uschindler" created="2016-05-15T21:02:20Z" id="219310363">I also fixed a bug with compound statements and made them work with `def` at all. Problem was: Size of `LDefArray` was missing (was 0 instead of 2 like for `LBrace`), which caused ASM error because stack dups were missing.

For compound statements, we can unfortunately make no type propagation, because we load from DEF and store from DEF, so there is no more type information. E.g., if you increment a DEF, it will always box. But that has nothing to do with this issue (invokedynamic).

The other thing not yet solved is missing type information for statements like `int x = a[0] + 1` (with `a` a DEF). Unfortunately the tree does not give enough information, because the expected type of the outer (`int x`) is not yet visible in the inner (`a[0] + 1`). I will think about that.

This PR now solves all the "simple" cases and also fixes a bug with def arrays and stack size.
</comment><comment author="rmuir" created="2016-05-16T00:45:48Z" id="219325260">&gt; I also fixed a bug with compound statements and made them work with def at all. Problem was: Size of LDefArray was missing (was 0 instead of 2 like for LBrace), which caused ASM error because stack dups were missing.

Thank you, I think i hit this one when experimenting the other day!
</comment><comment author="rmuir" created="2016-05-16T02:13:54Z" id="219334867">&gt; The other thing not yet solved is missing type information for statements like int x = a[0] + 1 (with a a DEF). Unfortunately the tree does not give enough information, because the expected type of the outer (int x) is not yet visible in the inner (a[0] + 1). I will think about that.

OK, this would be great to look into. Already this patch helps, for example our script in the nightly benchmark:

`(Math.log(Math.abs(doc['population'].value)) + doc['elevation'].value * doc['latitude'].value)/_score`

We see removal of boxing for the log(abs(population)), instead of: 

```
LDC "population"
INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
INVOKEDYNAMIC value(Ljava/lang/Object;)Ljava/lang/Object; [ ... ]
INVOKESTATIC org/elasticsearch/painless/Def.DefTodouble (Ljava/lang/Object;)D
INVOKESTATIC java/lang/Math.abs (D)D
INVOKESTATIC java/lang/Math.log (D)D
```

we now see:

```
LDC "population"
INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
INVOKEDYNAMIC value(Ljava/lang/Object;)D [ ... ]
INVOKESTATIC java/lang/Math.abs (D)D
INVOKESTATIC java/lang/Math.log (D)D
```

Unfortunately the optimization ends there for now (but this is progress!). I think part of the problem is that for the larger expression, painless has no idea that it needs to return a double in this case, so at the end of the day always returns `def`, and then `runAsDouble()` in the scripting api just unboxes that. The other part is the math operators, but its related :)

I will look into trying to clean this up, i hate that we don't know our return type when being compiled, i think its grossly unfair of the scripting api to do this. If we can fix it, then I think more type information would bubble up the tree and we'd kill more of the overhead.
</comment><comment author="uschindler" created="2016-05-16T08:12:39Z" id="219372198">&gt; I think part of the problem is that for the larger expression, painless has no idea that it needs to return a double in this case, so at the end of the day always returns def, and then runAsDouble() in the scripting api just unboxes that. The other part is the math operators, but its related

There is some parts missing in painless:
- The first issue is order of evaluation: It first evaluates the inner expressions in `EBinary`, but does not give `expected` to them - it does not know it at that time. The inner `EChain` then sees `expected==null` and cannot apply the type optimization. For the method call and assignment case, painless passes `expected` already, this is why it works. This is also the reason why I added the null check; maybe add a comment that this is not always given
- For the type propagation, painless only have it inner-&gt;outer. So AnalyzerCaster can only take the operands and calculated the type that comes out, e.g. for addition. But it has no code to do the same other way round.

One idea to fix this would be a two-phase analyze: In the first step of analyze the code is checked for inconsistenceies and every node would assign the "I'd be happy to get that input and output types". In a second pass of optimize the information collected in first phase would be used to adopt types also the other way round: if a sub node returns Def only after first phase, but the acceptor reported that it may accept double or float, make the inner node adopt its return type. The last step would then be writing of byte code.
</comment><comment author="uschindler" created="2016-05-16T08:15:33Z" id="219372638">&gt; I think part of the problem is that for the larger expression, painless has no idea that it needs to return a double in this case, so at the end of the day always returns def, and then runAsDouble() in the scripting api just unboxes that

Thats a separate issue. I would make 2 abstract Executables: One that returns Object and one that returns double. Painless checks while compiling which one to implement and returns the double one if possible. I think this should be possible to implement with some logic in the Writer if the "actual" type of the root node is primitive.
</comment><comment author="uschindler" created="2016-05-16T08:32:55Z" id="219375557">Also a bit unrelated to this issue, but we may investigate it in a separate issue: If boxing is needed (we cannot avoid it everywhere, like `def i = 0; [...]; i++`, the code boxes "constants" (in the increment case the constant 1) over and over. I know that hotspot optimizes that away in most cases, but it still looks bad and makes bytecode unreadable.

We already have a `EConstant` node, so we may optimize that to add all boxed constants it sees in code as `static final` field to the script's class and just refer to them with a static field load  (here it would be named, e.g. `BOXED_INT_1`) instead of the box instruction. We would just need some code to collect them and write all of them using visitField after we wrote the bytecode. We may be able to put the whole logic for that into `EConstant` and the pool of constants in the Variables or CompilerSettings!

This is not a cache - it is static, it just saves instances of all boxed constants found in code.
</comment><comment author="jdconrad" created="2016-05-16T17:19:05Z" id="219486153">At Uwe, thanks for tackling this!  I left a couple of comments.
</comment><comment author="jdconrad" created="2016-05-16T17:26:35Z" id="219488189">@uschindler I really like your idea of the two-passes to collect all known type information!
</comment><comment author="uschindler" created="2016-05-16T17:34:28Z" id="219490325">&gt; I really like your idea of the two-passes to collect all known type information!

Yes, I think this should be a separate issue. This issue is just to improve the situation, but a 2 pass analyze might be the best option, although this won't help for all cases. Sometimes it is impossible to catch all.
</comment><comment author="jdconrad" created="2016-05-16T17:37:31Z" id="219491146">Oh yeah, definitely a separate issue. :)
</comment><comment author="uschindler" created="2016-05-16T18:33:24Z" id="219506890">Hi Jack, I fixed the issue with the "this.actual" value so chained assignment works correct. So I need to change _this_ actual output to the (not) casted one. The next part in chained expression is then casting the duped value afterwards. Now it passes.
</comment><comment author="jdconrad" created="2016-05-16T18:42:15Z" id="219509461">@uschindler This looks great, if you're happy with it, I'm happy with it.  +1  I'm happy to commit as-is unless there was anything else you wanted to add or change.
</comment><comment author="jdconrad" created="2016-05-16T18:47:04Z" id="219510958">Sorry for the confusion on this.actual versus expression.actual.  Took a bit for the caffeine to kick in!
</comment><comment author="jdconrad" created="2016-05-16T18:53:56Z" id="219512846">@uschindler Are you ready for me to commit?
</comment><comment author="uschindler" created="2016-05-16T18:54:16Z" id="219512928">No problem :)

Maybe we should change the assignments tos use this as prefix also in source code. As EChain is also an expression, this makes clear if the inner expression or this is meant by the assignments. I can fix this, OK?
</comment><comment author="jdconrad" created="2016-05-16T18:56:16Z" id="219513487">@uschindler Yeah, please do.  Just let me know whenever you're ready.
</comment><comment author="uschindler" created="2016-05-16T19:07:47Z" id="219516765">OK, I am done. Thanks for help and finding the remaining issue with the cast.

We should really work on more tests for more corner cases.

In addition, I was talking with @rmuir about some way to check that "optimizations" in bytecode survive refactorings. One idea is to dump the bytecode to a String in tests using debugger class and look for patterns in it (e.g. for this issue make sure that it does not box by looking for "java/lang/Integer.valueOf" and fail test if it occurs in bytecode). Alternatively use forbiddenapis' API (as test dependency) and invoke `de.forbiddenapis.Checker` to forbid some method calls in the class :-)
</comment><comment author="jdconrad" created="2016-05-16T19:11:34Z" id="219517772">@uschindler  Thanks again for working on this issue!  I really like the idea of checking for the optimizations using the patterns from the bytecode.  Definitely need to ensure these changes stay in the future.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[spark] fields replacement in es.update.script.params fails for objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18358</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7+

**JVM version**: 1.7+

**OS version**: OS X, Linux

**Description of the problem including expected versus actual behavior**:

Parameter fields corresponding to a nested object are substituted by strings instead of collections.
Which means that collection operations can't succeed in scripts. A workaround is to convert the string to an object before using it in the script, but it should definitely be handled internally.

**Steps to reproduce**:
- Delete index in case it already exists

`curl -XDELETE 'http://localhost:9200/index2'`
- Add mapping

```
curl -XPOST 'http://localhost:9200/index2' -d '
{
  "mappings": {
    "contact": {
      "properties": {
        "id": {
          "type": "string"
        },
        "adresses": {
          "type": "nested", 
          "properties": {
            "id":    { "type": "string"  },
            "zipcode": { "type": "string"  }
          }
        }
      }
    }
  }
}
'
```
- Create new contact

`curl -XPOST 'http://localhost:9200/index2/contact/67861' -d '{ "id" : "67861","adresses": [] }'`
- Use Spark to try to add an entry to adresses nested object 

```
import org.apache.spark.{SparkConf, SparkContext}
import org.elasticsearch.spark._
sc.stop
val sparkConf = new SparkConf().set("es.write.operation", "upsert").set("es.input.json", "true").set("es.mapping.id","id")
val sc = new SparkContext(sparkConf)
val lines = sc.parallelize( List("""{"id":"67861","address":{"zipcode":"25381","id":"67861"}}"""))
val up_params = "new_address:address"
val up_script = "ctx._source.adresses+=new_address"
lines.saveToEs("index2/contact", Map("es.update.script.params" -&gt; up_params, "es.update.script" -&gt; up_script))
```

We then get the following error

```
failed to execute bulk item (index) index {[index2][contact][67861], source[{"id":"67861","adresses":["{\"zipcode\":\"25381\",\"id\":\"67861\"}"]}]}
MapperParsingException[object mapping for [adresses] tried to parse field [null] as object, but found a concrete value]
...
```

In the error message you can see that the issue is that what is inserted into adresses array is a string and not an object.

If we use an object in the script it works as expected :

```
val up_script = """ctx._source.adresses += [zipcode: '25381', id: '67861']"""
lines.saveToEs("index2/contact", Map("es.update.script" -&gt; up_script))
```

**Provide logs (if relevant)**: 

```
failed to execute bulk item (index) index {[index2][contact][67861], source[{"id":"67861","adresses":["{\"zipcode\":\"25381\",\"id\":\"67861\"}"]}]}
MapperParsingException[object mapping for [adresses] tried to parse field [null] as object, but found a concrete value]
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:213)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:306)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:436)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:262)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:580)
```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**: NA
</description><key id="154893228">18358</key><summary>[spark] fields replacement in es.update.script.params fails for objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cscetbon</reporter><labels /><created>2016-05-15T06:24:49Z</created><updated>2016-05-15T16:14:47Z</updated><resolved>2016-05-15T15:52:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-15T15:52:35Z" id="219293683">HI @cscetbon 

I suggest you open this issue on the es-hadoop list: https://github.com/elastic/elasticsearch-hadoop/issues
</comment><comment author="cscetbon" created="2016-05-15T16:14:47Z" id="219294888">@clintongormley &#128076; 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix concurrency bug in IMC that could cause it to check too infrequently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18357</link><project id="" key="" /><description>On each document indexed or deleted, IMC increments a total bytes counter, and when that crosses a threshold it polls all shards to see if it's time to move some indexed bytes to disk.

It uses crazy concurrency to try to avoid a per-index-op lock, but that concurrency is broken such that its running tally of total bytes indexed can (illegally) go negative, causing IMC to check too infrequently and possibly eventually leading to OOMEs.

I saw this happen when running indexing performance tests.

I think this change fixes it, but concurrent code is incredibly tricky
</description><key id="154879669">18357</key><summary>Fix concurrency bug in IMC that could cause it to check too infrequently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T22:51:05Z</created><updated>2016-05-15T14:18:29Z</updated><resolved>2016-05-15T14:18:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-15T11:22:52Z" id="219279571">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Improved tokenizer docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18356</link><project id="" key="" /><description>Added descriptions and runnable examples to all tokenizers
</description><key id="154868272">18356</key><summary>Docs: Improved tokenizer docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>docs</label><label>review</label></labels><created>2016-05-14T18:21:36Z</created><updated>2016-05-19T17:42:24Z</updated><resolved>2016-05-19T17:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-15T03:07:36Z" id="219263553">Left some points about wording. I'd love to be able to test those results arrays. We could probably build something to make that work, given the number of _analyze results that we have floating around the in the docs. It is nice to have such a short response form in the docs.
</comment><comment author="nik9000" created="2016-05-15T03:08:24Z" id="219263574">I think the "it'd be nice to test" part can come later. This already adds more tests by adding `// CONSOLE` snippets. I'd be happy to make a quick change to get the responses tested after this is merged.
</comment><comment author="clintongormley" created="2016-05-15T12:04:39Z" id="219281998">Thanks for the review @nik9000.  I addressed most of your comments in a9b3db65625b52664825ba2db18057f45c12a9c6

In e79d5a85afc6fde119c7ab58f04a1076a1456391 I added TESTRESPONSEs for all of the tokenizer examples, wrapping them in comment blocks so that they don't render in the docs, eg:

```
/////////////////////

[source,js]
----------------------------
{  response body }
----------------------------
// TESTRESPONSE

/////////////////////
```

There's just one test in the edge ngrams which I'm not sure how to fix, as the `took` value will be different on every test run
</comment><comment author="clintongormley" created="2016-05-15T14:57:52Z" id="219290536">I've added TESTRESPONSES for analyzers in https://github.com/elastic/elasticsearch/pull/18356/commits/38b89f9966fccf3433ed4b9b954bd49e9222ef34, and docs and TESTRESPONSES for character filters in https://github.com/elastic/elasticsearch/pull/18356/commits/89bbc7f861ea99c3af0f0db8718be64f7d62db79

The `pattern_replace` tests aren't working as `//TEST[continued]` isn't working as expected. 
</comment><comment author="clintongormley" created="2016-05-16T10:13:47Z" id="219392934">Thanks @nik9000 - fixed that.  There are two issues which remain:
- The `replacement` of `"$1"` is interpreted as a stash variable and so fails.  This would fail in the REST tests too
- The `"took"` value is always different.  I tried `TESTRESPONSE[s/"took".*//] but it seems to remove this from the provided response snippet, rather than from the received response.
</comment><comment author="nik9000" created="2016-05-16T16:32:56Z" id="219473783">&gt; The replacement of "$1" is interpreted as a stash variable and so fails. This would fail in the REST tests too

`\\$1` I think will do it, but I'm not sure. If not then maybe disable that test and merge and I'll hack on the rest infrastructure to make it work?

&gt; TESTRESPONSE[s/"took"._//]
&gt; `// TESTRESPONSE[s/"took"._/"took": "$body.took"/]

`s//` just changes the generated tests. "$body.took" gets it from the stash.
</comment><comment author="clintongormley" created="2016-05-17T08:37:37Z" id="219654671">&gt; `// TESTRESPONSE[s/"took"./"took": "$body.took"/]

this worked, thanks

&gt; \$1 I think will do it, 

That won't work because it changes what is displayed/sent to ES during the test. It's skipped for now.
</comment><comment author="nik9000" created="2016-05-17T15:38:37Z" id="219758404">&gt; That won't work because it changes what is displayed/sent to ES during the test. It's skipped for now.

LGTM. If you merge I'll have a look at the escape thing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Iterables.flatten should not pre-cache the first iterator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18355</link><project id="" key="" /><description>This only affects 5.0.0.

Today, `Iterables.flatten` pre-iterates its input and saves away a cached copy for later iteration, but for #18353 this is bad since it means we will never see new shards after the iterable was first created (on node startup).

I reviewed the few other places that use `Iterables.flatten` today and they all are fine with delaying iteration from creation time to when the flattened iterator is later consumed.

Closes #18353 
</description><key id="154866749">18355</key><summary>Iterables.flatten should not pre-cache the first iterator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T17:47:04Z</created><updated>2016-05-15T14:23:07Z</updated><resolved>2016-05-15T14:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-14T20:02:33Z" id="219248648">@mikemccand should we rather fix the one we pass to IMC instead? I think the behavior of copy the incoming list is good? We should maybe fix it like this:

``` Java
indexingMemoryController = new IndexingMemoryController(settings, threadPool, 
() -&gt; Iterables.flatten(this).iterator() // ensure we pull an iter with new shards - flatten makes a copy
);
```
</comment><comment author="mikemccand" created="2016-05-14T20:38:27Z" id="219250388">OK, I can do that instead, though I disagree that pre-caching is good here ;)

I think I makes the API quite trappy, since normally with an `Iterable` when you ask it for an iterator you normally would get the latest values.

Also, why not also pre-cache the values returned by the inner iterables?  It seems inconsistent.
</comment><comment author="mikemccand" created="2016-05-14T21:10:58Z" id="219251756">OK I pushed a new commit, putting `Iterables.flatten` back to pre-caching the outer iterable.
</comment><comment author="s1monw" created="2016-05-15T11:27:39Z" id="219279787">&gt; I think I makes the API quite trappy, since normally with an Iterable when you ask it for an iterator you normally would get the latest values.

I think that is the case now actually. if you modify one of the iterables that you passed to it it would reflect the change? What you are proposing is that changes to the list holding the individual _Iterable_ instances should be reflected in the flatten operation? ie. if I call `list.clear()` it should be an empty iterable afterwards?

``` Java
Iterable&lt;F&gt; x = Iterables.flatten(myList);
myList.clear();
for (F f : x) {
  // we never get here?
}
```

I think that would be trappy behavior no?
</comment><comment author="s1monw" created="2016-05-15T11:29:01Z" id="219279847">the change LGTM
</comment><comment author="mikemccand" created="2016-05-15T14:23:07Z" id="219288671">&gt; I think that would be trappy behavior no?

Well I'm not really sure which way is trappier anymore :)  But at least it has javadocs now explaining which way it goes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Slow Query Log - Ability to specify data to be added to log lines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18354</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

We would like to be able to link slow queries back to the user or application which caused the slow query.

Therefore we recommend adding a "logData" parameter to the search API which is simply copied from the request directly into the slow query log. We can then put identifiers (user ID, application ID, query ID, etc.) and what-not into this field to provide the linkages necessary to perform traceback to the application / systems which caused the slow query.

We realize that we could possibly do this with the "stats" parameter (stats groups), but we are concerned that there may be many IDs (thousands, millions) and that the stats-groups structure may not support such a large number of groups (i.e. it would likely use up memory maintaining stats for millions of entries).

Thank you.
Paul
</description><key id="154863135">18354</key><summary>Slow Query Log - Ability to specify data to be added to log lines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paullovessearch</reporter><labels /><created>2016-05-14T16:35:55Z</created><updated>2016-05-14T16:40:34Z</updated><resolved>2016-05-14T16:40:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-14T16:40:33Z" id="219230143">Duplicates #9669
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexingMemoryController doesn't see any active shards?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18353</link><project id="" key="" /><description>I was benchmarking ES indexing and noticed that IMC no longer asks shards to write indexing buffers to disk, which is horrible :)

I dug a bit, and I see that IMC is passed an `Iterable&lt;IndexShard&gt; indexServices` to its ctor, which it uses to find all active shards.  However, this iterable for some reason produces no shards.

Here's the full stack trace to IMC init, so I think somehow guice is not giving IMC the right iterable or something? 

```
node0: java.lang.Throwable
node0:  at org.elasticsearch.indices.IndexingMemoryController.&lt;init&gt;(IndexingMemoryController.java:104)
node0:  at org.elasticsearch.indices.IndexingMemoryController.&lt;init&gt;(IndexingMemoryController.java:97)
node0:  at org.elasticsearch.indices.IndicesService.&lt;init&gt;(IndicesService.java:184)
node0:  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
node0:  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
node0:  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
node0:  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
node0:  at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:49)
node0:  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
node0:  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
node0:  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
node0:  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:886)
node0:  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
node0:  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
node0:  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:50)
node0:  at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
node0:  at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
node0:  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
node0:  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
node0:  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
node0:  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:886)
node0:  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
node0:  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
node0:  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:50)
node0:  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:205)
node0:  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:197)
node0:  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:879)
node0:  at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:197)
node0:  at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:187)
node0:  at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
node0:  at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
node0:  at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
node0:  at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
node0:  at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
node0:  at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:235)
node0:  at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
node0:  at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
node0:  at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
node0:  at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:261)
node0:  at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
node0:  at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
node0:  at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
node0:  at org.elasticsearch.cli.Command.main(Command.java:53)
node0:  at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
node0:  at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```
</description><key id="154860466">18353</key><summary>IndexingMemoryController doesn't see any active shards?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>blocker</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T15:48:17Z</created><updated>2016-05-15T14:21:35Z</updated><resolved>2016-05-15T14:21:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-05-14T16:48:12Z" id="219230562">OK I see we are in fact passing the `IndicesService` to IMC ctor ... I'll dig.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18352</link><project id="" key="" /><description>Here are just some cleanups to code:
- GeneratorAdaptor.visitInvokeDyanmicInsn takes varargs, so the extra integer can be passed directly (more readable)
- Remove hardcoded descriptor in WriterConstants
</description><key id="154859263">18352</key><summary>Some cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T15:25:40Z</created><updated>2016-05-15T15:47:22Z</updated><resolved>2016-05-14T21:00:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-14T21:00:42Z" id="219251332">thanks @uschindler
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify delayed shard allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18351</link><project id="" key="" /><description>This PR simplifies the delayed shard allocation implementation by assigning clear responsibilities to the various components that are affected by delayed shard allocation:
- `UnassignedInfo` gets a boolean flag `delayed` which determines whether assignment of the shard should be delayed. The flag gets persisted in the cluster state and is thus available across nodes, i.e. each node knows whether a shard was delayed-unassigned in a specific cluster state. Before, nodes other than the current master were unaware of that information.
- This flag is initially set as `true` if the shard becomes unassigned due to a node leaving and the index setting `index.unassigned.node_left.delayed_timeout` being strictly positive. From then on, unassigned shards can only transition from delayed to non-delayed, never in the other direction.
- The reroute step is in charge of removing the delay marker (comparing timestamp when node left to current timestamp). 
- A dedicated service `DelayedAllocationService`, reacting to cluster change events, has the responsibility to schedule reroutes to remove the delay marker.

Relates to #18293
</description><key id="154858332">18351</key><summary>Simplify delayed shard allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-14T15:05:21Z</created><updated>2016-05-26T11:39:55Z</updated><resolved>2016-05-26T11:39:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-14T15:05:44Z" id="219225214">@bleskes can you have a look?
</comment><comment author="ywelsch" created="2016-05-23T13:53:38Z" id="220986618">@bleskes I've updated the PR. Please have another look.
</comment><comment author="bleskes" created="2016-05-25T14:30:07Z" id="221594244">Looking good! left comments here and there. Almost all very minor.
</comment><comment author="ywelsch" created="2016-05-25T16:43:02Z" id="221634897">@bleskes thanks for reviewing. I've updated the PR with your suggestions. Please have another look.
</comment><comment author="bleskes" created="2016-05-26T09:13:49Z" id="221818987">LGTM. @ywelsch thanks for the extra iterations
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use isAssignableFrom instead of relying on ClassCastException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18350</link><project id="" key="" /><description>Title says it all :-)
</description><key id="154855369">18350</key><summary>Use isAssignableFrom instead of relying on ClassCastException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T14:05:17Z</created><updated>2016-05-17T09:18:25Z</updated><resolved>2016-05-16T19:41:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-05-14T15:50:23Z" id="219227437">I found more of those, updating PR in a minute...
</comment><comment author="uschindler" created="2016-05-14T16:02:25Z" id="219228134">I added more changes around the static casting. It would be good if somebody cross-checks, because some of checks were really crazy (AnalyzerCaster was completely strange to me - I think the fix is right and much simpler)!
</comment><comment author="rmuir" created="2016-05-16T01:22:25Z" id="219328658">I agree catching the cast makes the logic hard to understand. personally i find asSubclass much easier to reason about than isAssignableFrom (i think the verb from in the method throws me for a loop every time), but given that we want to return our own exceptions anyway and that there is some "or" logic in here, I think this is better here. Thanks for cleaning it up! cc: @jdconrad 
</comment><comment author="uschindler" created="2016-05-16T07:57:22Z" id="219369869">&gt; personally i find asSubclass much easier to reason about than isAssignableFrom (i think the verb from in the method throws me for a loop every time)

I know those arguments - especially the "From" confuses. If you once understood how this method should be used, it is quite easy to understand:

If you have `left.isAssignableFrom(right)` then this only returns true if you could do the following assignment `leftInstance = rightInstance` - you see left stays left and right stays right. It returns false if you would need a cast and you would get a ClassCastException.

So don't think about super- or subclass, just think in terms of this assignment possible or not! :-)

The problem is similar to some people to understand like `compareTo`. It is the same: `left.compareTo(right)` is the same like `Comparator.compare(left, right)`, so its just the order. It also brought me into the same mental loop until you realize this "order of arguments".

For the code in painless, the huge trycatches confuse more than the mental loop :-)

P.S.: If you look at `Class.isSubclass()`'s source, it calls `isAssignableFrom` internally, because it is the one that the JVM implements. It just wraps the ClassCastException afterwards. So In our current code, it does the same thing 2 times and each unwrapping exception. And its unreadable, too.
</comment><comment author="uschindler" created="2016-05-16T08:00:14Z" id="219370293">&gt; and that there is some "or" logic in here

Thats the real reason why I did it!
</comment><comment author="jdconrad" created="2016-05-16T19:40:55Z" id="219525068">Thanks @uschindler!  LGTM.  If I'm being completely honest, asSubclass was the one I found in examples, so I used that, but this is much cleaner.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>painless: restore accidentally removed test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18349</link><project id="" key="" /><description>In my PR #18338 yesterday, I accidentally removed @rmuir's the test. This restores it. Sorry for that.
</description><key id="154845188">18349</key><summary>painless: restore accidentally removed test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T10:01:42Z</created><updated>2016-05-15T15:41:05Z</updated><resolved>2016-05-14T10:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-14T10:11:11Z" id="219212262">thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose `fuzzy_transpositions` in fuzzy queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18348</link><project id="" key="" /><description>The `fuzzy_transpositions` parameter is supported by the `match` query (`true` by default) but not by the `multi_match`, `query_string`, `simple_query_string`, or `fuzzy` queries, nor by the completion suggester.

We should add this parameter to all of the above.
</description><key id="154842501">18348</key><summary>Expose `fuzzy_transpositions` in fuzzy queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-05-14T08:51:40Z</created><updated>2017-07-28T07:50:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cjjojoba" created="2016-11-24T14:34:20Z" id="262789733">@clintongormley I see that `fuzzy_transpositions` is being mentioned in https://www.elastic.co/guide/en/elasticsearch/reference/2.4/query-dsl-match-query.html but not in https://www.elastic.co/guide/en/elasticsearch/reference/2.4/query-dsl-multi-match-query.html -- does MultiMatchQuery have this parameter too? If so, can the documents be updated please?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add not-null precondition check in BulkRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18347</link><project id="" key="" /><description>With this commit we add a precondition check to BulkRequest so
we fail early if users pass `null` for the request object.

For a more detailed discussion, see #12038.
This supersedes #12038.

Relates #12038.
</description><key id="154840757">18347</key><summary>Add not-null precondition check in BulkRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Bulk</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T08:02:25Z</created><updated>2016-05-18T10:35:56Z</updated><resolved>2016-05-14T09:25:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-14T09:10:25Z" id="219209648">LGTM
</comment><comment author="danielmitterdorfer" created="2016-05-14T09:24:57Z" id="219210205">Thanks for the review.
</comment><comment author="javanna" created="2016-05-17T12:03:58Z" id="219697663">@danielmitterdorfer sorry I am late to the party, but can we add a small test around these new null checks that we introduced?
</comment><comment author="danielmitterdorfer" created="2016-05-18T10:14:01Z" id="219984215">@javanna I have now added tests in de3e7d1. Can you please have a short look?
</comment><comment author="javanna" created="2016-05-18T10:35:56Z" id="219988657">looks great thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes unused methods in the o/e/common/Strings class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18346</link><project id="" key="" /><description /><key id="154835161">18346</key><summary>Removes unused methods in the o/e/common/Strings class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-14T05:19:12Z</created><updated>2016-05-14T12:09:30Z</updated><resolved>2016-05-14T12:09:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-05-14T08:15:30Z" id="219207696">LGTM
</comment><comment author="abeyad" created="2016-05-14T12:03:26Z" id="219216683">Thanks for the review @danielmitterdorfer !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18345</link><project id="" key="" /><description>This reverts commit ef421b6057d3b902501da3a3ca969161dd57153c.

The failure we tried to debug did not occur anymore. Maybe
a sideeffect of increasing the logger level?
</description><key id="154826767">18345</key><summary>2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">csy2013</reporter><labels /><created>2016-05-14T01:23:33Z</created><updated>2016-05-14T02:04:43Z</updated><resolved>2016-05-14T02:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-14T02:04:43Z" id="219194847">This appears to have been opened in error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Make run task you full zip distribution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18344</link><project id="" key="" /><description>This change makes `gradle run` use the full zip distribution. Arguably
we could make it do the same when run inside plugins, but I started out
with this simple change. I am open to moving it into the RunTask itself
so that plugins will do this as well.
</description><key id="154819722">18344</key><summary>Build: Make run task you full zip distribution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T23:38:12Z</created><updated>2016-05-14T00:00:41Z</updated><resolved>2016-05-14T00:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-13T23:58:15Z" id="219186085">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: More pom generation improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18343</link><project id="" key="" /><description>This adds a non empty description to all generated poms, as well as
fixing distributions so they actually have a generated pom.
</description><key id="154819303">18343</key><summary>Build: More pom generation improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T23:34:10Z</created><updated>2016-05-14T00:00:10Z</updated><resolved>2016-05-14T00:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-13T23:57:29Z" id="219186001">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose underlying processor to blame for thrown exception within CompoundProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18342</link><project id="" key="" /><description>Fixes #17823
</description><key id="154806912">18342</key><summary>Expose underlying processor to blame for thrown exception within CompoundProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T21:48:55Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-24T21:25:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-16T16:41:47Z" id="219476092">LGTM though maybe @javanna should have a look?
</comment><comment author="talevy" created="2016-05-16T18:29:45Z" id="219505819">thanks for the review @nik9000, I'll give @javanna or @martijnvg a little bit of time to check this out as well.

It was one of these problems that arose from refactoring, and this solution was kind of lost in the changes (seemed more difficult at the time).
</comment><comment author="martijnvg" created="2016-05-17T08:46:25Z" id="219656553">LGTM
</comment><comment author="talevy" created="2016-05-24T19:59:01Z" id="221384489">@nik9000 @martijnvg thank you for the reviews. Originally, I was missing an integration test that actually caught a big flaw in my original `CompoundProcessorException` design.

In the latest push, I wrap all CompoundProcessor related exceptions in an ElasticsearchException which relays the processor information in its headers.

let me know what you think!
</comment><comment author="martijnvg" created="2016-05-24T20:53:06Z" id="221397772">LGTM &#128077; 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Register `indices.query.bool.max_clause_count` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18341</link><project id="" key="" /><description>This commit registers `indices.query.bool.max_clause_count` as a node
level setting and removes support for its synonym setting
`index.query.bool.max_clause_count`.

Closes #18336
</description><key id="154804395">18341</key><summary>Register `indices.query.bool.max_clause_count` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T21:31:45Z</created><updated>2016-05-19T12:29:29Z</updated><resolved>2016-05-19T08:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-13T21:42:29Z" id="219166510">Lgtm
</comment><comment author="bleskes" created="2016-05-14T09:36:57Z" id="219210782">Can we maybe add a little explaination to the breaking change as to why it has to be a node level setting, despite of the fact that at face value it should be an index level and isn't really node dependent?  (i.e., Lucene has this as a static thing, so it has to be like this). Also, I see it is marked as deprecated, is there is any other better way to accomplish this/plans to make one? if so it will be nice to mention that as well. 
</comment><comment author="s1monw" created="2016-05-14T19:59:06Z" id="219248390">@bleskes I don't get what you mean it was never a index level setting. it just had the confusing prefix.
</comment><comment author="rjernst" created="2016-05-15T01:47:45Z" id="219261318">I wonder if we should (in addition to this good rename) move the settings validation check in SettingsModule before the check for index level settings existing in node settings? This improves the output here so we get "did you mean" behavior with the old setting like this:

```
[elasticsearch] Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [index.query.bool.max_clause_count] did you mean [indices.query.bool.max_clause_count]?
```

Otherwise with this change we get a confusing message stating the setting should be moved to indexes:

```
[elasticsearch] Please ensure all required values are updated on all indices by executing:
[elasticsearch] curl -XPUT 'http://localhost:9200/_all/_settings?preserve_existing=true' -d '{
[elasticsearch] "index.query.bool.max_clause_count" : "200"
[elasticsearch] }'
```
</comment><comment author="bleskes" created="2016-05-15T08:53:45Z" id="219273890">&gt; it was never a index level setting. it just had the confusing prefix.

Oh , you are of course right. Got confused by the confusing prefix. Sorry for the noise re that. 

My question re-deprecation still stands though - is there (a plan for) an alternative configuration option?
</comment><comment author="s1monw" created="2016-05-15T11:21:31Z" id="219279515">&gt; My question re-deprecation still stands though - is there (a plan for) an alternative configuration option?

I think there aren't any satisfying ways to fix this. For instance you would need to add a setter to anything that can rewrite to BQ which can set a setter on BQ to pass on the setting. One way of doing it would be a thread local which sucks too but I think that's the reason for the simple but global way of setting it.
</comment><comment author="s1monw" created="2016-05-15T12:28:23Z" id="219283016">&gt; I wonder if we should (in addition to this good rename) move the settings validation check in SettingsModule before the check for index level settings existing in node settings? This improves the output here so we get "did you mean" behavior with the old setting like this:

@rjernst I think the check it correct there otherwise you won't get past that point since the other leftover `index.*` settings you have in the node settings will cause failures otherwise. I think I will just special case that setting there?
</comment><comment author="rjernst" created="2016-05-15T14:04:33Z" id="219287710">&gt;  I think I will just special case that setting there?

Sure, sounds good.
</comment><comment author="bleskes" created="2016-05-17T08:58:14Z" id="219659182">&gt; I think there aren't any satisfying ways to fix this.

Sure. Does this mean we should not mark the settings as deprecated? 
</comment><comment author="s1monw" created="2016-05-18T18:43:05Z" id="220120616">&gt; Sure. Does this mean we should not mark the settings as deprecated?

I don't see where this PR deprecates anything? I am confused how is this related?
</comment><comment author="bleskes" created="2016-05-18T21:51:14Z" id="220169139">&gt; I don't see where this PR deprecates anything?

The reason why I mentioned is the usage of `Setting.Property.Deprecated` [here](https://github.com/elastic/elasticsearch/pull/18341/files#diff-cc9584700dd179d6f04f643b578a85f9R295).
</comment><comment author="s1monw" created="2016-05-19T07:43:11Z" id="220250819">&gt; The reason why I mentioned is the usage of Setting.Property.Deprecated here.

how the fuck.... not sure how that got there - not intended. That explains the confusion :) I will fix and push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch default batch size for reindex to 1000</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18340</link><project id="" key="" /><description>1000 is likely to perform much better.
</description><key id="154799987">18340</key><summary>Switch default batch size for reindex to 1000</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T21:04:35Z</created><updated>2016-05-27T14:16:56Z</updated><resolved>2016-05-16T12:26:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-13T21:04:58Z" id="219158897">@tlrx want to review this?

@clintongormley we discussed this earlier today.
</comment><comment author="clintongormley" created="2016-05-14T08:55:25Z" id="219209083">LGTM
</comment><comment author="tlrx" created="2016-05-14T10:20:39Z" id="219212642">LGTM too :)
</comment><comment author="nik9000" created="2016-05-16T12:47:53Z" id="219416629">Thanks for reviewing @clintongormley and @tlrx !
</comment><comment author="clintongormley" created="2016-05-20T12:19:37Z" id="220590913">@nik9000 Would you backport this to 2.x as well please?
</comment><comment author="nik9000" created="2016-05-20T12:28:32Z" id="220592634">Sure. I'll head back to the 2.x branch later today if possible.
</comment><comment author="clintongormley" created="2016-05-20T12:29:12Z" id="220592755">thanks - btw i've already doc'ed the batch size as 1000 in 2.x (see https://github.com/elastic/elasticsearch/pull/18484)
</comment><comment author="clintongormley" created="2016-05-20T12:41:21Z" id="220595115">@nik9000 scratch my previous comment - i've reverted that commit as I see you include the docs in this PR
</comment><comment author="nik9000" created="2016-05-23T16:03:46Z" id="221018408">2.x: 62bf6c89644e0a220d8e6a56902c76aab0566bcb
</comment><comment author="clintongormley" created="2016-05-24T09:07:41Z" id="221209985">thanks @nik9000 
</comment><comment author="persiarash" created="2016-05-24T23:22:58Z" id="221431303">Thanks @nik9000, but out of necessity, is there a way to configure the batch size, even if it isn't through the Java API? What I mean is that even something like a server-side configuration would do.
</comment><comment author="nik9000" created="2016-05-25T01:32:26Z" id="221450128">You can't change the default but you can set it on each request. Of hand I
believe it is called batch_size in update_by_query and it is the "size"
field in the "source" object of a redirect.
On May 24, 2016 7:23 PM, "persiarash" notifications@github.com wrote:

&gt; Thanks @nik9000 https://github.com/nik9000, but out of necessity, is
&gt; there a way to configure the batch size, even if it isn't through the Java
&gt; API? What I mean is that even something like a server-side configuration
&gt; would do.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/18340#issuecomment-221431303
</comment><comment author="persiarash" created="2016-05-25T02:28:38Z" id="221457347">@nik9000 thanks for the quick reply.

I'm aware of the ability to change the batch size via the HTTP REST API, I believe it is called "scroll_size" there, however I don't see a way to set the same parameter via the Java API. All I see through the Java API is a "size", which is actually the hard cap on the total number of objects to process, not the batch size.

That's why I was hoping that there might be a way to configure this value on the server itself, via the yaml properties file. Because I don't think as a Java consumer of ES I have any other option (unless of course I write my own Java HTTP client to interface w/ ES via its HTTP API).
</comment><comment author="nik9000" created="2016-05-25T02:54:16Z" id="221460454">In the Java API it is the size parameter on the search request.
On May 24, 2016 10:28 PM, "persiarash" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 thanks for the quick reply.
&gt; 
&gt; I'm aware of the ability to change the batch size via the HTTP REST API, I
&gt; believe it is called "scroll_size" there, however I don't see a way to set
&gt; the same parameter via the Java API. All I see through the Java API is a
&gt; "size", which is actually the hard cap on the total number of objects to
&gt; process, not the batch size.
&gt; 
&gt; That's why I was hoping that there might be a way to configure this value
&gt; on the server itself, via the yaml properties file. Because I don't think
&gt; as a Java consumer of ES I have any other option (unless of course I write
&gt; my own Java HTTP client to interface w/ ES via its HTTP API).
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/18340#issuecomment-221457347
</comment><comment author="persiarash" created="2016-05-25T02:57:00Z" id="221460736">@nik9000 Are you sure? I tested this earlier today by setting size to 1000, what I got was an update which updated exactly 1000 items, and did so in 19 batches (1000 updates + 835 conflicts = 1835 total / 19 batches = batch-size of 100, which is the default for ES 2.3)

For your reference this is how I'm building it:

UpdateByQueryRequestBuilder requestBuilder = UpdateByQueryAction.INSTANCE.newRequestBuilder(CLIENT)
                .source(toStringArray(indexIds))
                .filter(queryBuilder)
                .abortOnVersionConflict(false)
                .consistency(WriteConsistencyLevel.DEFAULT)
                        .size(1000) // This does NOT work, it sets maximum results count not batch size.
                .script(updateScript);
</comment><comment author="nik9000" created="2016-05-26T16:05:54Z" id="221916855">This ought to work:

```
UpdateByQueryRequestBuilder requestBuilder = UpdateByQueryAction.INSTANCE.newRequestBuilder(CLIENT)
.source(toStringArray(indexIds))
.filter(queryBuilder)
.abortOnVersionConflict(false)
.consistency(WriteConsistencyLevel.DEFAULT)
.script(updateScript);
requestBuilder.source().size(1000);
```
</comment><comment author="persiarash" created="2016-05-27T01:55:02Z" id="222041694">Thank you very much @nik9000, that (almost) did the trick! The solution for anyone else who is interested ended up being:
requestBuilder.source().setSize(1000);

Just FYI the Javadocs are VERY misleading about this, i.e.:
.setSize() =&gt; "The number of search hits to return. Defaults to 10."
Of course in this scenario it is actually the batch size, and it defaults to 100 (or 1000 in 2.4+)
</comment><comment author="nik9000" created="2016-05-27T14:16:56Z" id="222157948">&gt; Javadocs are VERY misleading

Yeah, part of the reason for that is that reindex is reusing SearchRequestBuilder to give you control over the search request it uses to start the scroll. So everything under `source()` from a scrolling perspective. But SearchRequestBuilder javadocs don't think about scrolling.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Add developer info to generated pom files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18339</link><project id="" key="" /><description /><key id="154798957">18339</key><summary>Build: Add developer info to generated pom files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T20:58:22Z</created><updated>2016-05-13T21:05:40Z</updated><resolved>2016-05-13T21:05:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-13T21:01:34Z" id="219158172">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build descriptor of array and field load/store in code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18338</link><project id="" key="" /><description>First step: fix array index to adapt type not DEF

This also makes the descriptor of the invokedyanmic in the code. As first step I removed the cast for array index. The operand for storing is not yet adapted - @rmuir will take care
</description><key id="154797394">18338</key><summary>Build descriptor of array and field load/store in code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T20:49:12Z</created><updated>2016-05-14T08:55:56Z</updated><resolved>2016-05-13T21:25:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-13T20:58:15Z" id="219157449">Just one minor comment, but this is great!  Thanks, @uschindler for fixing this up.
</comment><comment author="rmuir" created="2016-05-13T21:00:06Z" id="219157830">Nice step!

for a simple expression like this: `def x = new ArrayList(); return x[0];` you can see the boxing gets removed.

before:

```
ICONST_0
INVOKESTATIC java/lang/Integer.valueOf (I)Ljava/lang/Integer;
INVOKEDYNAMIC arrayLoad(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; [
 ...
]
```

after:

```
ICONST_0
INVOKEDYNAMIC arrayLoad(Ljava/lang/Object;I)Ljava/lang/Object; [
 ...
]
```

Maybe we should add a small test like this one for method calls, just to be sure if the user passes something crazy that the right stuff happens: https://github.com/uschindler/elasticsearch/blob/8281800b64c6ed06a8858aa46c6c016274bd0520/modules/lang-painless/src/test/java/org/elasticsearch/painless/WhenThingsGoWrongTests.java#L178-L182
</comment><comment author="uschindler" created="2016-05-13T21:04:42Z" id="219158833">I removed the obsolete line.
</comment><comment author="uschindler" created="2016-05-13T21:07:10Z" id="219159340">Should I add a test to something goes wrong?
I would test both arrays and also Lists
</comment><comment author="rmuir" created="2016-05-13T21:11:07Z" id="219160152">Yes, sounds great. 
</comment><comment author="uschindler" created="2016-05-13T21:14:48Z" id="219160965">I also have another small change:
when building the descriptor, I will change the return value to `after.type`. I also applied this for calls.
To fix return type, we would then only need to change `after`'s type inside analyze.
</comment><comment author="uschindler" created="2016-05-13T21:23:25Z" id="219162739">OK, I also added test. Should I combine the commits to 1? Otherwise its ready.
</comment><comment author="jdconrad" created="2016-05-13T21:25:43Z" id="219163182">@uschindler Thanks again!  We'll keep the 3 commits.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test docs for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18337</link><project id="" key="" /><description>We weren't doing it before because we weren't starting the plugins.
Now we are.

The hardest part of this was handling the files the tests expect
to be on the filesystem. extraConfigFiles was broken.
</description><key id="154792218">18337</key><summary>Test docs for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-05-13T20:18:08Z</created><updated>2016-06-14T18:32:57Z</updated><resolved>2016-06-14T18:32:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-16T12:46:23Z" id="219416379">@rjernst I pushed an update. I'd love to work through the ClusterFormationTask thing in another way if you've got any ideas. That took most of the time I spent on this, sadly.
</comment><comment author="nik9000" created="2016-05-19T14:51:34Z" id="220348390">@rjernst could you have another look? I haven't removed the groovy-ism that was required for an unknown reason.
</comment><comment author="nik9000" created="2016-05-31T14:32:26Z" id="222706990">@rjernst could you have a look at this at some point? I'd love to get these under the test umbrella one day.
</comment><comment author="nik9000" created="2016-06-13T22:17:41Z" id="225725650">@rjernst could you have a look at this at some point? I'd love to get these under the test umbrella one day.
</comment><comment author="rjernst" created="2016-06-14T18:04:47Z" id="225965591">I looked again, and fixed the gradle integ test issue with extra config files. My concern is still about requiring every doc test to add a "wait for the cluster to be ready". This should just be part of the setup.
</comment><comment author="nik9000" created="2016-06-14T18:13:25Z" id="225968161">&gt; My concern is still about requiring every doc test to add a "wait for the cluster to be ready". This should just be part of the setup.
- This is already how the rest of the doc tests work.
- We already wait for yellow between setup sections and other sections. This should just be adding it to places where we create the index and use it in the same snippet.
- We're fixing the need for it in #18759.
- Without the wait for yellow you can't actually copy and paste the snippets from the browser. They fail to run.
</comment><comment author="rjernst" created="2016-06-14T18:19:30Z" id="225970043">&gt; This is already how the rest of the doc tests work

I did not know that. LGTM then for now. Looking forward to #18759
</comment><comment author="nik9000" created="2016-06-14T18:28:48Z" id="225973013">&gt; Looking forward to #18759

amen
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.query.bool.max_clause_count can not be set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18336</link><project id="" key="" /><description>Index level setting is not allowed in elastic search.yml with 5.0.0-alpha2.

And with _setting or _template API I got:
{
"error": {
"root_cause": [
{
"type": "illegal_argument_exception",
"reason": "unknown setting [index.query.bool.max_clause_count]"
}
],
"type": "illegal_argument_exception",
"reason": "unknown setting [index.query.bool.max_clause_count]"
},
"status": 400
}

Is this a bug? Any work-around?
</description><key id="154776036">18336</key><summary>index.query.bool.max_clause_count can not be set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jimyoo1</reporter><labels><label>:Settings</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T18:43:40Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-05-19T08:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-13T19:35:29Z" id="219139349">this is a bug - this setting has not been registered! good catch thanks for reporting this!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename the dynamic call site factory to DefBootstrap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18335</link><project id="" key="" /><description>... and make the inner class name very short (PIC = Polymorphic Inline Cache)

The reason for this change is to make stack traces look nice if the bootstrapping of invokedynamic fails. The current class names were inconsistent and redundant. PIC (Polymorphic Inline Cache) is just an internal impl detail and the name was choosen to be short.
</description><key id="154768012">18335</key><summary>Rename the dynamic call site factory to DefBootstrap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T17:59:19Z</created><updated>2016-05-13T18:09:23Z</updated><resolved>2016-05-13T18:05:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-13T18:02:14Z" id="219116386">Looks good. The shorter name will give less noise in errors, and the bootstrap name is less confusing (its no callsite).
</comment><comment author="rmuir" created="2016-05-13T18:05:23Z" id="219117159">Thanks @uschindler for these cleanups!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_size doesn't support doc values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18334</link><project id="" key="" /><description>The documentation for the `mapper-size` plugin claims you can use `_size` in sorts, aggregations, and in scripts but I don't imagine any of those work because it doesn't support doc_values. Is it intentional that it doesn't?
</description><key id="154764733">18334</key><summary>_size doesn't support doc values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Mapper Size</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-05-13T17:41:43Z</created><updated>2016-09-14T14:44:25Z</updated><resolved>2016-07-05T12:53:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T18:07:58Z" id="219117816">This worked in 2.x because it fell back to loading fielddata.  @jpountz what do you think? should we at least allow enabling doc values on `_size`?

@dakrone how were you guys using `_size`?  Did you need to run aggs on them?
</comment><comment author="dakrone" created="2016-05-13T18:27:10Z" id="219122883">&gt; how were you guys using `_size`? Did you need to run aggs on them?

Yep, usually min/max/average were the metrics we cared about in terms of document size.
</comment><comment author="clintongormley" created="2016-05-20T09:30:26Z" id="220559553">Discussed in FixItFriday - we should add doc values support to the size field, with fallback to fielddata for older index versions
</comment><comment author="clintongormley" created="2016-06-23T13:10:56Z" id="228044780">@jimferenczi could you take this one please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid race while retiring executors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18333</link><project id="" key="" /><description>Today, a race condition exists when retiring executors. Namely, if an
executor is retired and then the thread pool is terminated, the retiring
of the executor and the termination of the thread pool can race to
remove the retired executor from the queue of retired executors. More
precisely, when the executor is initially retired, it is placed on a
queue of retired executors, and then removed when it is successfully
shutdown. When the pool is terminated, it will also drain the queue of
retired executors. This leads to a time-of-check-time-of-use race where
the draining can see a retired executor on the queue but that retired
executor can be removed upon successful shutdown of that executor. This
leads to the draining attempting to remove an element from the queue
when there is none. This commit addresses this race condition by instead
safely polling the queue.
</description><key id="154749382">18333</key><summary>Avoid race while retiring executors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T16:16:39Z</created><updated>2016-05-13T16:26:22Z</updated><resolved>2016-05-13T16:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-13T16:21:37Z" id="219091171">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow only a single extension for a scripting engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18332</link><project id="" key="" /><description>Previously multiple extensions could be provided, however, this can lead
to confusion with on-disk scripts (ie, "foo.js" and "foo.javascript")
having different content. Only a single extension is now supported.

The only language currently supporting multiple extensions was the
Javascript engine ("js" and "javascript"). It now only supports the
`.js` extension.

Relates to #10598
</description><key id="154744850">18332</key><summary>Allow only a single extension for a scripting engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T15:55:29Z</created><updated>2016-05-13T16:28:59Z</updated><resolved>2016-05-13T16:28:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-13T16:05:16Z" id="219087145">LGTM. I know you left a note in the migration docs about javascript's lang parameter changing but maybe another note about not being able to use `.javascript`? It seems weird that we supported it when `.js` is so common, but it is probably worth a note just in case?
</comment><comment author="dakrone" created="2016-05-13T16:06:26Z" id="219087445">Sure, I'll add that when I merge this. Thanks @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Teach reindex to retry on search failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18331</link><project id="" key="" /><description>This uses the same backoff policy we use for bulk and just retries until the request isn't rejected.

Closes #18059
</description><key id="154727261">18331</key><summary>Teach reindex to retry on search failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T14:39:29Z</created><updated>2016-05-17T17:59:37Z</updated><resolved>2016-05-17T17:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-13T20:24:58Z" id="219150180">@tlrx, want to have a look?
</comment><comment author="nik9000" created="2016-05-13T20:30:09Z" id="219151356">Or @dakrone !
</comment><comment author="dakrone" created="2016-05-13T21:57:41Z" id="219169334">Left a couple of comments but otherwise looks good!
</comment><comment author="nik9000" created="2016-05-16T14:31:33Z" id="219440050">@dakrone and @tlrx I pushed a few more commits to address some comments and try and tighten up RetryTests.
</comment><comment author="dakrone" created="2016-05-16T17:25:40Z" id="219487897">Left a few more comments, code looks good though
</comment><comment author="tlrx" created="2016-05-17T07:30:28Z" id="219640540">Besides @dakrone comments, LGTM
</comment><comment author="nik9000" created="2016-05-17T15:49:47Z" id="219762086">@dakrone and @tlrx: I pushed the logging fixes and breaking changes docs.
</comment><comment author="dakrone" created="2016-05-17T15:51:05Z" id="219762533">still LGTM
</comment><comment author="nik9000" created="2016-05-17T15:52:37Z" id="219763066">Cool. I'll squash a merge then!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update CONTRIBUTING.md</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18330</link><project id="" key="" /><description>Fix typo
</description><key id="154721142">18330</key><summary>Update CONTRIBUTING.md</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shubham391</reporter><labels><label>docs</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T14:12:50Z</created><updated>2016-05-13T14:27:48Z</updated><resolved>2016-05-13T14:27:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-13T14:27:27Z" id="219058911">Thanks. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Port Delete By Query to Reindex infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18329</link><project id="" key="" /><description>This commit ports the Delete By Query implementation on the same infrastructure as the Reindex and Update By Query APIs.

The Delete By Query shares most of its code with the Update By Query feature. Some code has been moved around so that there's no duplication. Reindex does not use it for now but the changes will help that. There's surely some improvements to do but I'd be happy to get some feedback from a review.. @nik9000 Can you please have a look? That'd be awesome.

Closes #16883
</description><key id="154713741">18329</key><summary>Port Delete By Query to Reindex infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>feature</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T13:38:23Z</created><updated>2016-06-06T14:40:01Z</updated><resolved>2016-05-19T14:14:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-16T17:52:59Z" id="219495563">I like it! I think you should change the way the metadata is copied somewhat - something with less instanceofs would make me much more happy.

I think there is probably a cleaner way to do the scripts so that the async actions don't have to implement BiFunction and apply.

I wonder if you can get `op_type=delete` to work for `update_by_query` and `reindex` now that these are there? It feels like that isn't much more than what you've already done. Maybe another PR after this one?

I think it is worth adding this to the breaking changes list somehow too. The plugin is gone and the output is different, right?
</comment><comment author="tlrx" created="2016-05-17T16:30:52Z" id="219775057">Thanks for this great review @nik9000 !

I updated the code according to your comments. Please let me know what you think.

&gt; I like it! I think you should change the way the metadata is copied somewhat - something with less instanceofs would make me much more happy.

I agree. I followed your suggestion and wrapped action requests and exposed an interface to manipulate them because `DocumentRequest` does not fit.

&gt; I think there is probably a cleaner way to do the scripts so that the async actions don't have to implement BiFunction and apply.

Passing a method reference (or Supplier) forces all classes to be `static` as well as `copy*` methods... So I added a `getScriptApplier()` method.

&gt; I wonder if you can get op_type=delete to work for update_by_query and reindex now that these are there? It feels like that isn't much more than what you've already done. Maybe another PR after this one?

That's exactly the idea :) but I think it's better if this is done in a separate PR.

&gt; I think it is worth adding this to the breaking changes list somehow too. The plugin is gone and the output is different, right?

I plan to do it when I'll remove the plugin, if that's OK for you.
</comment><comment author="nik9000" created="2016-05-17T16:35:26Z" id="219776346">&gt; &gt; I like it! I think you should change the way the metadata is copied somewhat - something with less instanceofs would make me much more happy.
&gt; &gt; I agree. I followed your suggestion and wrapped action requests and exposed an interface to manipulate them because DocumentRequest does not fit.

Cool! I'll have a look!

&gt; &gt; I wonder if you can get op_type=delete to work for update_by_query and reindex now that these are there? It feels like that isn't much more than what you've already done. Maybe another PR after this one?
&gt; &gt; That's exactly the idea :) but I think it's better if this is done in a separate PR.

Awesome.

&gt; I think it is worth adding this to the breaking changes list somehow too. The plugin is gone and the output is different, right?
&gt; I plan to do it when I'll remove the plugin, if that's OK for you.

Great.
</comment><comment author="nik9000" created="2016-05-17T18:19:08Z" id="219807290">I reviewed again and it looks pretty good! I think maybe rebase to resolve the conflicts I just created by merging #18331 and I'll have another look in my morning with fresh eyes?
</comment><comment author="tlrx" created="2016-05-18T09:00:56Z" id="219967148">@nik9000 Thanks again! I rebased and updated the code according to your comments. 
</comment><comment author="nik9000" created="2016-05-18T21:10:58Z" id="220159379">I'm getting a test failure with this:

```
==&gt; Test Info: seed=D21734D0466B8A8E; jvm=1; suite=1
Suite: org.elasticsearch.smoketest.SmokeTestReindexWithGroovyIT
  2&gt; REPRODUCE WITH: gradle :qa:smoke-test-reindex-with-groovy:integTest -Dtests.seed=D21734D0466B8A8E -Dtests.class=org.elasticsearch.smoketest.SmokeTestReindexWithGroovyIT -Dtests.method="test {yaml=update_by_query/10_script/Setting bogus ctx is an error}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.jvms=12 -Dtests.locale=es-PE -Dtests.timezone=Asia/Dushanbe
FAILURE 0.21s | SmokeTestReindexWithGroovyIT.test {yaml=update_by_query/10_script/Setting bogus ctx is an error} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: the error message was expected to match the provided regex but didn't
   &gt; Expected: Invalid fields added to ctx \[junk\]
   &gt;      but: was "{root_cause=[{type=illegal_argument_exception, reason=Invalid fields added to context [junk]}], type=illegal_argument_exception, reason=Invalid fields added to context [junk]}"
   &gt;    at __randomizedtesting.SeedInfo.seed([D21734D0466B8A8E:5A430B0AE897E776]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:117)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:399)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/manybubbles/Workspaces/Elasticsearch/elasticsearch/qa/smoke-test-reindex-with-groovy/build/testrun/integTest/J0/temp/org.elasticsearch.smoketest.SmokeTestReindexWithGroovyIT_D21734D0466B8A8E-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {}, docValues:{}, maxPointsInLeafNode=1342, maxMBSortInHeap=5.027602980016196, sim=ClassicSimilarity, locale=es-PE, timezone=Asia/Dushanbe
  2&gt; NOTE: Linux 3.16.0-4-amd64 amd64/Oracle Corporation 1.8.0_72-internal (64-bit)/cpus=12,threads=1,free=444417848,total=514850816
  2&gt; NOTE: All tests run in this JVM: [SmokeTestReindexWithGroovyIT]
Completed [1/1] in 8.49s, 21 tests, 1 failure &lt;&lt;&lt; FAILURES!
```

When I want to test "just" the reindex module I run `gradle modules:reindex:check qa:smoke-test-reindex-with-groovy:check docs:check`. Because all of those modules have reindex based tests, sadly.
</comment><comment author="nik9000" created="2016-05-18T21:16:49Z" id="220160894">I left a few more comment, including a failing test. I'm pretty sure we're close to the end on this. I expect those are the last changes I'm likely to have any cause to suggest. After this is merged we'll need to do a few more things to finish the "delete-by-query porting project". This is my guess of the list:
- Remove the delete-by-query plugin. Should we just remove it in 5.0.0, call it a breaking change, and tell people to use the version in the reindex module? Is that ok because we're _kind of_ just moving the API? It is losing some small functionality (timeouts) but it is gaining cancel, throttle, and rethrottle.
- Document all the breaking changes.
- Consolidate all those tests that have a version of reindex and a version for update by query into single tests.
  *\* Add a delete-by-query case to the new retry tests. That should probably wait until #18456 is merged.
</comment><comment author="nik9000" created="2016-05-18T21:20:35Z" id="220161780">Oh! More things:
- (optional) Implement `ctx.op = "delete"` on _update_by_query and _reindex.
- Cut all the script based cancel tests over to your listener based one. That is nicer.
</comment><comment author="tlrx" created="2016-05-19T12:04:12Z" id="220304532">@nik9000 Thanks again! I updated the code and also fixed the failing test.

Once this one is merged, I'll address the remaining points one after one :)
</comment><comment author="nik9000" created="2016-05-19T12:30:19Z" id="220309927">&gt; Once this one is merged, I'll address the remaining points one after one :)

I'm happy to grab a few!
</comment><comment author="nik9000" created="2016-05-19T12:40:38Z" id="220312303">LGTM
</comment><comment author="natelapp" created="2016-05-28T17:30:31Z" id="222319967">Will this change also support deleting `external` versioned document that was broken by the move to the plugin?  The details of that bug are found in https://github.com/elastic/elasticsearch/issues/16654.
</comment><comment author="tlrx" created="2016-06-06T14:40:01Z" id="223979076">@natelapp In the current state, no. Your issue is a specific case and I created #18750 to handle it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`DateTimeZone.forID` doesn't work in ES 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18328</link><project id="" key="" /><description>**Elasticsearch version**: 2.x

**JVM version**: 1.8

**OS version**: Any

**The following works in 1.x**:

```
PUT try
{
  "mappings": {
    "type": {
      "properties": {
        "time": {
          "type": "date"
        }
      }
    }
  }
}

POST try/type
{
  "time": "2016-05-09T14:33:36.202Z"
}

POST try/_search
{
  "script_fields": {
    "FIELD": {
      "script": "doc['time'].date.setZone(DateTimeZone.forID('America/New_York')); return doc['time'].date;"
    }
  }
}
```

**The following DOES NOT work in 2.x**

```
PUT try
{
  "mappings": {
    "type": {
      "properties": {
        "time": {
          "type": "date"
        }
      }
    }
  }
}

POST try/type
{
  "time": "2016-05-09T14:33:36.202Z"
}

POST try/_search
{
  "fields": [],
  "script_fields": {
    "FIELD": {
      "script": {
        "inline": "doc['time'].date.setZone(DateTimeZone.forID('America/New_York'))"
      }
    }
  }
}
```

The error is the following:

```
{
  "took": 429,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 4,
    "failed": 1,
    "failures": [
      {
        "shard": 4,
        "index": "try",
        "node": "YY36CprdRhq_oWPHloHqOA",
        "reason": {
          "type": "script_exception",
          "reason": "failed to run inline script [doc['time'].date.setZone(DateTimeZone.forID('America/New_York'))] using lang [groovy]",
          "caused_by": {
            "type": "illegal_argument_exception",
            "reason": "The datetime zone id 'America/New_York' is not recognised"
          }
        }
      }
    ]
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": []
  }
}
```

In the logs, i see the following:

```
[2016-05-13 10:02:00,471][DEBUG][action.search            ] [Karl Lykos] [4] Failed to execute fetch phase
RemoteTransportException[[Karl Lykos][127.0.0.1:9300][indices:data/read/search[phase/fetch/id]]]; nested: ScriptException[failed to run inline script [doc['time'].date.setZone(DateTimeZone.forID('America/New_York'))] using lang [groovy]]; nested: IllegalArgumentException[The datetime zone id 'America/New_York' is not recognised];
Caused by: ScriptException[failed to run inline script [doc['time'].date.setZone(DateTimeZone.forID('America/New_York'))] using lang [groovy]]; nested: IllegalArgumentException[The datetime zone id 'America/New_York' is not recognised];
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:320)
    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:85)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:592)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: The datetime zone id 'America/New_York' is not recognised
    at org.joda.time.DateTimeZone.forID(DateTimeZone.java:229)
    at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:228)
    at 5b2c7c46fa5b2546451451ee1ff497b431ee701d.run(5b2c7c46fa5b2546451451ee1ff497b431ee701d:1)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript$1.run(GroovyScriptEngineService.java:313)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:310)
    ... 12 more
```
</description><key id="154706756">18328</key><summary>`DateTimeZone.forID` doesn't work in ES 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels /><created>2016-05-13T13:02:49Z</created><updated>2016-05-16T16:49:25Z</updated><resolved>2016-05-13T13:05:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmoskovicz" created="2016-05-13T13:03:09Z" id="219036470">Looks related to #14524
</comment><comment author="jasontedor" created="2016-05-13T13:05:57Z" id="219037129">Duplicates #18017 
</comment><comment author="jasontedor" created="2016-05-16T16:49:25Z" id="219478180">Note that the reproduction provided here does _not_ reproduce if your machine is set to the `America/New_York` timezone; you have to change the script to a timezone that your machine is not set to.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed grammar in index-too-old exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18327</link><project id="" key="" /><description>`gradle tests` pass
</description><key id="154704265">18327</key><summary>Fixed grammar in index-too-old exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Exceptions</label><label>non-issue</label><label>review</label></labels><created>2016-05-13T12:49:52Z</created><updated>2016-05-13T13:08:15Z</updated><resolved>2016-05-13T13:08:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-13T12:52:20Z" id="219034133">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DST issues with DateHistogram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18326</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.8.0_92

**OS version**: Windows 7

**Description of the problem including expected versus actual behavior**:

It appears that the DateHistogram aggregation has some troubles when time zone is set, and the histogram with hourly interval passes a DST transition (eg CEST -&gt; CET). 

**Steps to reproduce**:

Tests reproducing the error in this diff against v2.3.2: https://github.com/nilsga/elasticsearch/commit/155e3081ca04cd63e6b973dbe4cebd13bb7972b8

**Provide logs (if relevant)**:

The error is 

```
Failed to execute phase [merge], [reduce] 

    at __randomizedtesting.SeedInfo.seed([71B41DDE1915B1DC:6476BA807BC1E239]:0)
    at org.elasticsearch.action.search.SearchQueryAndFetchAsyncAction$1.onFailure(SearchQueryAndFetchAsyncAction.java:76)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.AssertionError
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.addEmptyBuckets(InternalHistogram.java:442)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.doReduce(InternalHistogram.java:466)
    at org.elasticsearch.search.aggregations.InternalAggregation.reduce(InternalAggregation.java:153)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:170)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:411)
    at org.elasticsearch.action.search.SearchQueryAndFetchAsyncAction$1.doRun(SearchQueryAndFetchAsyncAction.java:64)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    ... 3 more
```

I think it is related to the implementation of `TimeZoneRounding.nextRoundingInterval`. I don't quite get why time is converted from UTC to local time, and then back to UTC. I would expect a certain point of time to be constant, regardless of timezone. Formatting of time, however, I can understand would require time zone specific behaviour.
</description><key id="154701594">18326</key><summary>DST issues with DateHistogram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nilsga</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2016-05-13T12:34:35Z</created><updated>2016-05-18T15:30:00Z</updated><resolved>2016-05-18T15:30:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-13T12:37:21Z" id="219031070">Relates #18310
</comment><comment author="nilsga" created="2016-05-14T16:20:28Z" id="219229109">I have not fully understood the logic yet, but It seems odd that the rounding logic is implemented by actually shifting the time according to local "calendar" time. All duration fields in `DateTimeUnit` seems to be UTC based. If one instead looked up the `DateTimeField` with the correct timezone of the chronology, it seems to me like the rounding should be provided "out of the box" by Joda.
</comment><comment author="cbuescher" created="2016-05-17T10:15:38Z" id="219676437">@nilsga great catch, I'm looking into this and think I found the glitch in the rounding logic of `TimeZoneRounding.nextRoundingInterval`. The conversion from UTC to local time is necessary there for day or month intervals, otherwise we would end up cases where `TimeZoneRounding.nextRoundingInterval` is not on the correct start of day/month when crossing dst transitions. But I think for smaller durations (hours in your case, minutes and seconds also) we should not do this. I will need to look a bit closer and add some test to be sure though.
</comment><comment author="cbuescher" created="2016-05-17T14:04:28Z" id="219727555">This is how this behaviour presents itself outside of unit-tests: create an empty index, add any document with a date field, then do a histogram query with extended bounds covering the last DST end (e.g. Oslo/Europe 2015-10-25, 2015-10-25T02:00:00.000+02:00):

```
DELETE /_all

PUT /test/type/1
{
  "dateField" : "2000-01-01"
}

GET /test/type/_search
{
  "query": {
    "match_none": {}
  },
  "aggs": {
    "events_by_date": {
      "date_histogram": {
        "field": "dateField",
        "interval": "1h",
        "time_zone": "Europe/Oslo",
        "min_doc_count": 0,
        "extended_bounds": {
          "min": "2015-10-25T02:00:00.000+02:00",
          "max": "2015-10-25T04:00:00.000+01:00"
        }
      }
    }
  }
}

"aggregations": {
    "events_by_date": {
      "buckets": [
        {
          "key_as_string": "2015-10-25T02:00:00.000+02:00",
          "key": 1445731200000,
          "doc_count": 0
        },
        {
          "key_as_string": "2015-10-25T03:00:00.000+01:00",
          "key": 1445738400000,
          "doc_count": 0
        },
        {
          "key_as_string": "2015-10-25T04:00:00.000+01:00",
          "key": 1445742000000,
          "doc_count": 0
        }
      ]
    }
  }
```

The problem is that when looking at the bucket keys (utc), the first bucket covers two hours (1445738400000 - 1445731200000 = 7200000) whereas the other buckets are one hour(3600000 ms) wide.

In the other direction (start of DST) we dont have the same problem. Since local time is advanced by one hour, we still have 1h buckets, but you can see the jump in "key_as_string":

```
GET /test/type/_search
{
  "query": {
    "match_none": {}
  },
  "aggs": {
    "events_by_date": {
      "date_histogram": {
        "field": "dateField",
        "interval": "1h",
        "time_zone": "Europe/Oslo",
        "min_doc_count": 0,
        "extended_bounds": {
          "min": "2016-03-27T01:00:00.000+01:00",
          "max": "2016-03-27T04:00:00.000+02:00"
        }
      }
    }
  }
}

"aggregations": {
    "events_by_date": {
      "buckets": [
        {
          "key_as_string": "2016-03-27T01:00:00.000+01:00",
          "key": 1459036800000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-27T03:00:00.000+02:00",
          "key": 1459040400000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-27T04:00:00.000+02:00",
          "key": 1459044000000,
          "doc_count": 0
        }
      ]
    }
  } 
```

I think we should have equally spaced date histogram buckets for hour and sub-hour time units and will open a PR for that.
</comment><comment author="nilsga" created="2016-05-17T20:41:01Z" id="219846854">Thanks for looking into it. I understand that you need to have correctly aligned buckets for all time units. But I don't understand why this could not be handled by the JodaTime library by using `ISOChronology.getInstance(timeZone)` instead of `ISOChronology.getInstanceUTC()`, which would yield time zone adjusted "floor" values for `DateTimeField.roundFloor`?

```
DateTime time = new DateTime(1445731200000L, DateTimeZone.UTC);
DateTimeField fieldUtc = ISOChronology.getInstanceUTC().dayOfMonth(); // As in DateTimeUnit enum
DateTimeField field = ISOChronology.getInstance(DateTimeZone.forID("Europe/Oslo")).dayOfMonth(); // Time zone "enabled" version of the same field.
DateTime rounded = new DateTime(field.roundFloor(time.getMillis()), DateTimeZone.forID("Europe/Oslo"));
DateTime roundedUtc = new DateTime(fieldUtc.roundFloor(time.getMillis()), DateTimeZone.UTC);
System.out.println(rounded);
System.out.println(roundedUtc);
System.out.println(rounded.withZone(DateTimeZone.UTC));
```

Output:

```
2015-10-25T00:00:00.000+02:00
2015-10-25T00:00:00.000Z
2015-10-24T22:00:00.000Z
```
</comment><comment author="cbuescher" created="2016-05-18T14:24:10Z" id="220042173">@nilsga thanks for pointing this out. As far as I see, this will do the same thing in ZonedChronology#roundFloor() that we are currently do in our TimeZoneRounding#roundKey(), namely converting UTC to local and back. It might be worth looking into whether using Joda directly for this would simplify our code base or not.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CircuitBreakerServiceIT.testParentChecking fails frequently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18325</link><project id="" key="" /><description>See for example: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=debian/405/consoleText

I can reproduce if I run this in a loop like so:

```
 gradle :core:integTest -Dtests.seed=FCF3BEA90D893CAF -Dtests.class=org.elasticsearch.indices.memory.breaker.CircuitBreakerServiceIT -Dtests.method="testParentChecking" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.jvm.argline="-XX:-UseParNewGC -XX:-UseConcMarkSweepGC -XX:+UseG1GC" -Dtests.locale=es-UY -Dtests.timezone=America/Menominee -Dtests.iters=100 -Dtests.failfast=true
```
</description><key id="154696984">18325</key><summary>CircuitBreakerServiceIT.testParentChecking fails frequently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>test</label></labels><created>2016-05-13T12:06:42Z</created><updated>2016-06-09T11:08:27Z</updated><resolved>2016-05-18T13:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add note regarding thread stack size on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18324</link><project id="" key="" /><description>This commit adds a note to the Windows service docs regarding the thread
stack size setting for the Windows service installer. As the Apache
Commons procrun daemon requires that this setting be explicitly set, we
need a value to be set when the service is installed. The right place
for this setting is the jvm.options file. We do not want to ship with a
hard-coded value here because we do not want to override the default
setting on other platforms, and the right default depends on whether or
not the end-user is on a 32-bit versus a 64-bit Windows system.
</description><key id="154691719">18324</key><summary>Add note regarding thread stack size on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T11:32:13Z</created><updated>2016-05-14T11:29:56Z</updated><resolved>2016-05-14T11:29:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="anhlqn" created="2016-05-13T14:38:31Z" id="219062103">Is there a new thread stack size value for ES 5.0.0 or just 256k as usual? I'm on Windows 64 bit.
</comment><comment author="jasontedor" created="2016-05-13T14:42:33Z" id="219063341">&gt; Is there a new thread stack size value for ES 5.0.0 or just 256k as usual? I'm on Windows 64 bit.

You should use `-Xss1m` in jvm.options file on 64-bit Windows.
</comment><comment author="clintongormley" created="2016-05-13T17:38:01Z" id="219110113">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup of DynamicCallSite</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18323</link><project id="" key="" /><description>This PR cleans up the DynamicCallSite class a bit:
- Move all logic into the MutableCallSite impl. This allows to simplify code, because the fallback() method does not need to be static, so it has access to all field directly via this.
- Move bootstrap code to ctor of call site.
- The outer class is now solely containing the bootstrap factory.
- Fix comment why class is public: It is because different classloader, package is the same!
</description><key id="154684313">18323</key><summary>Cleanup of DynamicCallSite</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T10:45:01Z</created><updated>2016-05-13T17:40:27Z</updated><resolved>2016-05-13T15:19:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-13T15:02:42Z" id="219069675">&gt; because the fallback() method does not need to be static

I'm concerned this is always the case. What about e.g. a bimorphic case where we have T1, fallback -&gt; T2, fallback -&gt; lookup. Will this now prohibit inlining?
</comment><comment author="rmuir" created="2016-05-13T15:04:05Z" id="219070066">OK i get it, need coffee. the important parts will still be static :)
</comment><comment author="rmuir" created="2016-05-13T15:19:35Z" id="219074398">Thanks @uschindler 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't allow `fuzziness` for `multi_match` types `cross_fields`, `phrase` and `phrase_prefix`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18322</link><project id="" key="" /><description>Currently `fuzziness` is not supported for the `cross_fields` type of the `multi_match` query since it complicates the logic that
blends the term queries that cross_fields uses internally (#6866). At the moment using this combination is silently ignored, which can lead to
confusions. Instead we should throw an exception in this case. The same is true for `phrase` and `phrase_prefix` type.

Closes #7764
</description><key id="154684224">18322</key><summary>Don't allow `fuzziness` for `multi_match` types `cross_fields`, `phrase` and `phrase_prefix`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T10:44:34Z</created><updated>2016-05-13T17:22:02Z</updated><resolved>2016-05-13T15:33:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-13T13:32:52Z" id="219043718">LGTM - should we add a note to the migration guide? as usual there is the percolator problem that if we have such a thing in the percolator we are pretty much dead in the water. I am not sure how we can fix that though. but lets move on here
</comment><comment author="cbuescher" created="2016-05-13T14:37:43Z" id="219061864">After trying `multi_match` with `fuzziness` on 2.3 I&#8217;m not sure anymore if we should immediately throw an exception for types `phrase` and `phrase_prefix`. For `cross_fields` using `fuzziness` doesn't work, so its better to error there instead of silently ignoring the parameter, but for `phrase` and `phrase_prefix` type using `fuzziness` doesn't error currently and produces different results. @clintongormley had some reasons why we should error here as well, can you re-check that this is what we want?
</comment><comment author="cbuescher" created="2016-05-13T15:18:26Z" id="219074075">Re-checked with @clintongormley, added a note to the migration docs.
</comment><comment author="clintongormley" created="2016-05-13T17:22:02Z" id="219105990">&gt; but for phrase and phrase_prefix type using fuzziness doesn't error currently and produces different results.

for context, the reason was that Christoph was running a phrase query with just one word, which was then being treated as a non-phrase query, which is why fuzziness worked.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't ignore allocation settings with cluster reroute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18321</link><project id="" key="" /><description>The cluster reroute command ignores the allocation deciders for index/cluster allocation settings, which makes it hard to debug why shards are not being assigned (see https://github.com/elastic/elasticsearch/issues/18294#issuecomment-218967378)
</description><key id="154668066">18321</key><summary>Don't ignore allocation settings with cluster reroute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-05-13T09:16:43Z</created><updated>2016-05-20T13:13:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-13T14:04:05Z" id="219052117">I believe it should be an option to disable the allocation deciders when running reroute commands. I've disabled allocation and then manually allocated shards to be where I want them to be before in order to reproduce a specific cluster allocation
</comment><comment author="clintongormley" created="2016-05-13T17:33:33Z" id="219108926">~~While that may be useful from a debugging perspective, it sounds very trappy for users~~ The least we should do is report them as ignored in the explain output

_Update_: I was missing the context.  I'm +1 on providing a flag to allow disabled allocation to be overridden with the cluster reroute command
</comment><comment author="s1monw" created="2016-05-13T19:41:00Z" id="219140524">&gt; Update: I was missing the context. I'm +1 on providing a flag to allow disabled allocation to be overridden with the cluster reroute command

+1 for now but in general we should really try to get away from providing this API alltogether
</comment><comment author="clintongormley" created="2016-05-20T08:27:11Z" id="220546054">The flag should also allow overriding of the `index.allocation.max_retry` counter, which is being added in https://github.com/elastic/elasticsearch/pull/18467
</comment><comment author="bleskes" created="2016-05-20T08:51:34Z" id="220551236">heads up to whoever implements this - some allocation deciders should never be overridden - for example `ReplicaAfterPrimaryActiveAllocationDecider` and `SameShardAllocationDecider`. These are so essential to how the code works, that I wonder if we should give them a special place/hard code them.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable transposition for fuzzy search in multi match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18320</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**: 
Apparently, `transpositions` are disabled for `fuzzy` `multi_match` queries. Is there any reason for the same? Can it be made configurable?
</description><key id="154664756">18320</key><summary>Enable transposition for fuzzy search in multi match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels /><created>2016-05-13T08:59:50Z</created><updated>2016-05-14T20:11:49Z</updated><resolved>2016-05-13T17:20:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T17:20:03Z" id="219105504">@bittusarkar It's not:

```
PUT t/t/1
{
  "foo": "one two three",
  "bar": "one two three"
}

GET t/_search 
{
  "query": {
    "multi_match": {
      "query": "trhee",
      "fields": ["foo","bar"],
      "fuzziness": "AUTO"
    }
  }
}
```
</comment><comment author="bittusarkar" created="2016-05-13T18:44:59Z" id="219127586">@clintongormley Thank you for your reply. Thing is I don't want to use `AUTO`. I want to limit the maximum value of `fuzziness` to `1` and also enable transpositions. Can that be achieved without changing Elasticsearch code?
</comment><comment author="s1monw" created="2016-05-13T19:47:46Z" id="219142030">transpositions are enabled by default but can't be disabled on multi match query @clintongormley should we add that switch? 
</comment><comment author="clintongormley" created="2016-05-14T08:53:25Z" id="219209026">&gt; Thing is I don't want to use AUTO. I want to limit the maximum value of fuzziness to 1 and also enable transpositions. Can that be achieved without changing Elasticsearch code?

Yes, transpositions are enabled by default so just set `fuzziness` to `1`.

&gt; transpositions are enabled by default but can't be disabled on multi match query @clintongormley should we add that switch?

I've opened  https://github.com/elastic/elasticsearch/issues/18348
</comment><comment author="clintongormley" created="2016-05-14T09:21:01Z" id="219210038">Added docs for `fuzzy_transposition` to `match` query in bfc826003bf7db97f852c8f74e74bc11946917b6
</comment><comment author="bittusarkar" created="2016-05-14T11:48:28Z" id="219216044">Sorry for the confusion guys. I should've mentioned I am using Elasticsearch 1.7.1 in which `transposition` is not enabled. Is that intentional? I see that in Elasticsearch 2.1.1, `transposition` is enabled by default. Looks like I need to upgrade Elasticsearch.
</comment><comment author="s1monw" created="2016-05-14T20:11:49Z" id="219249066">&gt; I see that in Elasticsearch 2.1.1, transposition is enabled by default. Looks like I need to upgrade Elasticsearch.

agreed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve exception stacktraces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18319</link><project id="" key="" /><description>We can improve our stackframes in exceptions to help with debuggability, errors, hot threads, whatever.
- Give painless the name of the script (stored scripts: the id, file scripts: the filename, inline: null) at compile-time.
- Reduce length of generated class name.
- For inline scripts, we have no "filename" but we include some of the script to help identify it, but truncating with ellipsis if its too big or spans multiple lines.
- Add line number debug information to bytecode. It was added to AST nodes in https://github.com/elastic/elasticsearch/pull/18298. We emit this for all leaf S-nodes.

example:

```
java.lang.NullPointerException
    at org.elasticsearch.painless.Executable$Script.execute(String x = null; boolean y = x.isEmpty(); ... @ &lt;inline script&gt;:1)
```
</description><key id="154646251">18319</key><summary>Improve exception stacktraces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T06:59:30Z</created><updated>2016-05-17T12:23:29Z</updated><resolved>2016-05-13T19:42:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-13T16:01:07Z" id="219086097">LGTM, much nicer looking.
</comment><comment author="jdconrad" created="2016-05-13T16:43:09Z" id="219096458">This is great!  Thanks for doing this. +1
</comment><comment author="rmuir" created="2016-05-13T16:52:03Z" id="219098625">By the way, later we can investigate putting character offsets as line numbers, so the jvm gives us back a more precise location. It is evil but we could do some magic to make it useful.

We gotta start somewhere though, so line numbers seem like walking before running to me.
</comment><comment author="uschindler" created="2016-05-13T18:00:43Z" id="219115984">Looks fine!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Min and max heap size must be equal to start ES 5.0.0-alpha2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18318</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha2

**JVM version**: jre1.8.0_91

**OS version**: Windows 10

**Description of the problem including expected versus actual behavior**:
ES 5.0.0 require same min and max heap size

``````
initial heap size [268435456] not equal to maximum heap size [1073741824]; this can cause resize pauses and prevents mlockall from locking the entire heap```
``````

Why don't we just set

```
-Xms1g
-Xmx1g
```

instead of having in the default options

```
-Xms256m
-Xmx1g
```

**Steps to reproduce**:
1. Run ES with default JVM options

**Provide logs (if relevant)**:
</description><key id="154643583">18318</key><summary>Min and max heap size must be equal to start ES 5.0.0-alpha2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anhlqn</reporter><labels /><created>2016-05-13T06:37:04Z</created><updated>2016-05-13T06:38:40Z</updated><resolved>2016-05-13T06:38:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-05-13T06:38:40Z" id="218963564">We think so too and this is already addressed in a couple of tickets, the latest one being #18309 and the related PR is #18311. Closing this now in favor of the already existing tickets.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"thread stack size not set" on Elasticsearch-5.0.0-alpha2 Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18317</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha2

**JVM version**: jre1.8.0_91

**OS version**: Windows 10

**Description of the problem including expected versus actual behavior**:
When installing ES service on Windows, I received the following error

```
thread stack size not set; configure via "E:\ELK\elasticsearch-5.0.0-alpha2\config\jvm.options" or ES_JAVA_OPTS"
```

I think -Xss256k or an optimal setting should exist in jvm.options by default

**Steps to reproduce**:
1. Run service.bat install

**Provide logs (if relevant)**:
</description><key id="154641893">18317</key><summary>"thread stack size not set" on Elasticsearch-5.0.0-alpha2 Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anhlqn</reporter><labels /><created>2016-05-13T06:21:54Z</created><updated>2017-05-09T08:16:01Z</updated><resolved>2016-05-13T10:09:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-13T10:09:36Z" id="219004176">This is a deliberate choice. On all systems except for running Elasticsearch as a service on Windows, the thread stack size setting is not needed and we can pick up the JVM defaults. The issue when running Elasticsearch as a service on Windows is that `procrun` requires min heap, max heap and thread stack size to be set. Thus, we ship with out-of-the-box settings that work for all cases except running Windows as a service. We use to set this in the service installer, but the issue there is that moves a piece of the JVM configuration out of the jvm.options file negating the point of the introduction of that configuration file.
</comment><comment author="jasontedor" created="2016-05-13T11:36:11Z" id="219019611">Thank you for reporting. While it was a deliberate choice, we should definitely communicate this situation better and your report prompted me to open #18324 to add a note to the documentation regarding this. There is an effort to have dedicated Windows packaging which will alleviate this issue but I can not give a timeframe for when that effort will be completed.

@clintongormley added you to the [Pioneer Program](https://www.elastic.co/blog/elastic-pioneer-program). Thanks again!
</comment><comment author="anhlqn" created="2016-05-13T14:36:15Z" id="219061422">Thanks, it would be helpful for new users of ES on windows as I don't see many documents on forum or Google about how to set a thread stack size for ES. I guess because it used to be set by default.
</comment><comment author="kellihall" created="2016-09-09T17:07:13Z" id="245976017">I agree.  I'm trying to install it as a service on Windows (as I did with earlier ES versions), but getting error message noted above.  Please be sure to document this issue and how to resolve.    

Thanks! 
- Kelli
</comment><comment author="jasontedor" created="2016-09-09T17:12:00Z" id="245977324">@kellihall It was documented in #18324 and is currently available in the [Installing Elasticsearch as a Service on Windows](https://www.elastic.co/guide/en/elasticsearch/reference/5.0/windows.html#windows-service) section of the docs. :smile:
</comment><comment author="kellihall" created="2016-09-09T17:20:10Z" id="245979599">That worked!  I added -Xss1m to the jvm.options file in the config folder and re-ran the 'service.bat install' command.  It is now installed and running as a service in windows.
Thanks so much!!
</comment><comment author="jasontedor" created="2016-09-09T17:23:15Z" id="245980468">@kellihall You're very welcome.
</comment><comment author="niemyjski" created="2016-10-31T13:17:15Z" id="257290830">I got this error on the rtm, as a long time user of elastic I know all the commands so I wouldn't read the docs. The error message lead me to the jvm.options... However, there is no mention of this. This setting should be talked about at the top of the file so those looking at the file know what to do without googling, finding this issue and then reading the install docs and finding a note (one of many) mentioning this.
</comment><comment author="nik9000" created="2016-10-31T13:40:33Z" id="257295867">&gt; finding this issue and then reading the install docs and finding a note (one of many) mentioning this.

If you open a PR to put the information in the file I'd be happy to merge it. I'm not really sure what to do about the other notes though. Ultimately we have to document breaking changes in one place and that is the [breaking changes](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-5.0.html) page. I'm happy to brainstorm better ways to get eyes to that page though.
</comment><comment author="jasontedor" created="2016-10-31T13:56:20Z" id="257299925">&gt; This setting should be talked about at the top of the file so those looking at the file know what to do without googling, finding this issue and then reading the install docs and finding a note (one of many) mentioning this.

I don't agree as this is specific to the Windows service only. Perhaps the error message should be changed to be more explicit though.
</comment><comment author="Smoothfu" created="2016-11-01T11:44:53Z" id="257547062">Open the file,\elasticsearch-5.0.0\elasticsearch-5.0.0\config\jvm.options,        add -Xsslm  after
 -Xms2g  -Xmx2g 
then save and run elasticsearch-service.bat again and it has been installed.Copy from http://blog.csdn.net/leo063/article/details/52994786
</comment><comment author="niemyjski" created="2016-11-01T19:34:50Z" id="257670344">So what is the census here? Should I open a pr or the error message be improved. This definitely isn't a good on boarding process on windows. Could this setting be inferred when running on windows?
</comment><comment author="jasontedor" created="2016-11-01T19:43:08Z" id="257672791">@niemyjski I integrated #21200 and #21201 already. The change to the script to provide a more helpful error message will be included in the 5.0.1 patch release, and the [doc update](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_packaging.html) is live as of some time yesterday.
</comment><comment author="jasontedor" created="2016-11-01T19:43:37Z" id="257672928">@Smoothfu It doesn't have to go below the `-Xms` and `-Xmx`, it can go on any otherwise line in the file and on 64-bit Windows it should be `-Xss1m` not `-Xsslm` as you have.
</comment><comment author="Smoothfu" created="2016-11-02T01:46:55Z" id="257750134">@jasontedor ,it may be wrong, This morning when I start the elasticsearch.failed and it showed that 
"Invalid thread stack size:-Xsslm
Error:Could not create the Java Virtual Machine.
Error:A fatal exception has occured.Program will exit." So I erase the _Xsslm in the cinfig file,save,start,OK.
#18317 occured when elasticsearch-service.bat install ,it would be server.But from the client side,it doesn't need  _Xsslm.
I'm a .net programmer,not familiar with Java including JVM
</comment><comment author="anhlqn" created="2016-11-02T02:19:54Z" id="257754346">How about putting some recommended values for `-Xss` inside the java.options file by default?

```
-Xms1g
-Xmx1g
# 64-bit Windows stack size
#-Xss1m

# 32-bit Windows stack size
#-Xss320k
```

Is there an assumption that not many people run production ES (meaning running ES as a service) on Windows?
</comment><comment author="jasontedor" created="2016-11-02T02:37:44Z" id="257756552">@Smoothfu It's a `1` (one) not an `l` (ell).
</comment><comment author="jasontedor" created="2016-11-02T02:41:04Z" id="257756931">&gt; How about putting some recommended values for `-Xss` inside the java.options file by default?

I'm opposed to this because it's OS specific, I'm hoping that the improved docs are sufficient here.

&gt; Is there an assumption that not many people run production ES (meaning running ES as a service) on Windows?

It's not an assumption, we know that this user base is not large.
</comment><comment author="gmarz" created="2016-11-02T02:58:43Z" id="257759181">I agree this isn't the best experience on Windows, and also caught me by surprise.  I think the service should just work out of the box.

Since it's Windows specific, can we not just supply a default value in the service bat itself instead of `jvm.options`?
</comment><comment author="jasontedor" created="2016-11-02T03:00:39Z" id="257759433">@gmarz I don't think we should, because then it's [moving JVM configuration out of the jvm.options file](https://github.com/elastic/elasticsearch/issues/18317#issuecomment-219004176).
</comment><comment author="gmarz" created="2016-11-02T03:18:24Z" id="257761515">True, but I think there's a valid argument here for treating `Xss` as a special case since it's OS specific.  We'd still give precedence to `Xss` in the jvm.options file, and only fallback to a default (provided in the script) if it isn't present.

Keep in mind, procrun already moves configuration out of the jvm.options file by storing the JVM options in the Windows registry after installing, which is exposed via the manager UI.
</comment><comment author="jasontedor" created="2016-11-02T03:22:33Z" id="257761958">&gt; True, but I think there's a valid argument here for treating `Xss` as a special case since it's OS specific.  We'd still give precedence to `Xss` in the jvm.options file, and only fallback to a default (provided in the script) if it isn't present.

I'm unconvinced, there are other OS-specific settings for which the same argument could be made.

&gt; Keep in mind, procrun already moves configuration out of the jvm.options file by storing the JVM options in the Windows registry after installing, which is exposed via the manager UI.

Yes, but it's configured by reading jvm.options.

It's terrible that `procrun` requires this, and doesn't just rely on JVM defaults.
</comment><comment author="niemyjski" created="2016-11-02T03:23:59Z" id="257762129">I agree with @gmarz.. please keep in mind a large share of prod users won't be using windows but I'm betting a ton of developers are running elastic as a service on windows. Do you have stats for this? This should just work on windows I mean you setup guide has always inferred juts launch the batch file...
</comment><comment author="gmarz" created="2016-11-02T03:42:59Z" id="257764326">&gt; I'm unconvinced, there are other OS-specific settings for which the same argument could be made.

Well, it's really procrun specific.

&gt; Yes, but it's configured by reading jvm.options.

I guess my point is if the argument against a default value in the bat file is that there should be a single source of truth for JVM options (e.g. the `jvm.options` file), then it's not a strong one because of the simple the fact that procrun uses the registry after it's installed, and thus it's configuration and `jvm.options` are already prone to diverging.

&gt; It's terrible that procrun requires this, and doesn't just rely on JVM defaults.

Agreed :/.  I'm also ok with just improving the docs, but I think a default value in the service.bat would be pragmatic.  I'd be happy to open a PR.
</comment><comment author="jasontedor" created="2016-12-01T19:42:59Z" id="264273238">I opened #21920.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how can I put parameters in ElasticSearch curl post</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18316</link><project id="" key="" /><description>The following is my linux shell code for insert one record to ElasticSearch:

username="Tom"
curl -XPOST 'http://192.168.0.1:9200/userdb/info/1' -d '{"user":"$username"}'
But it didn't work, it treated $username is a string not a variable. How can I fix this please?
</description><key id="154639774">18316</key><summary>how can I put parameters in ElasticSearch curl post</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JackWang917</reporter><labels /><created>2016-05-13T06:01:41Z</created><updated>2016-05-13T06:04:48Z</updated><resolved>2016-05-13T06:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-05-13T06:04:16Z" id="218959231">Hi @JackWang917, we have a discussion forum for these type of questions: https://discuss.elastic.co/c/elasticsearch Can you please post your question there? Closing now as this is not an issue with Elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score queries in must_not bool clauses producing incorrect results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18315</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5 (Tested against 2.0.2, 2.1.2, 2.2.2, and 2.3.2 as well and this is not an issue in any of those releases)

**JVM version**: 1.8.0_45

**OS version**: OSX 10.11.4

**Problem**:

When running a function score query as a must_not bool clause, approximately half of the expected documents are being returned. I've been playing around with replicas, number of shards, optimizing, and refreshing on insert which all have minimal to no impact, so for the sake of testing an demonstration and to eliminate variables, the example focuses on 1 shard with no replicas.  I also played around with different score functions and found this same behavior with all functions I tested.

**Steps to reproduce**:

``` shell
# Create the sample index with a single shard
curl -XPUT http://localhost:9200/fnscore-test-index -d '{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  }
}'

# Index some documents.  Two should be enough to demonstrate the issue 
# in a single-shard index, but I saw some inconsistencies around how many documents
# failed to match appropriately in smaller sample sizes, so crank up the sample size
# until you see the query count mismatches
for i in {1..2}; do
  curl -XPUT "http://localhost:9200/fnscore-test-index/event/$i" -d "{\"field\":\"value-$i\"}"
done

# Run the must_not function score query, sticking with a simple function
# score script that SHOULD match every document, therefore making the
# must_not match zero documents
curl -XPOST http://localhost:9200/fnscore-test-index/event/_count -d '{
   "query": {
      "bool": {
         "must_not": [
            {
               "function_score": {
                  "functions": [
                     {
                        "script_score": {
                           "script": "-1",
                           "lang": "expression"
                        }
                     }
                  ],
                  "score_mode": "sum",
                  "boost_mode": "replace",
                  "min_score": 0
               }
            }
         ]
      }
   }
}'
# ...which returns...
{
   "count": 1, # This should be 2 
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   }
}
```

Since the function score query should match zero documents, the must_not should (in theory) return ALL documents, but it's really only returning somewhere around half.  Note that small sample sizes have (greater sample sizes )

**Interesting (maybe) notes**:
- Adding a refresh after each insert changes how many documents fail to match the query criteria in small samples, but with an increased sample size, that difference diminishes as the sample size increases, more/less completely vanishing in the 80-100 document range
- Using a `not` filter instead of a bool query `must_not` "fixes" the problem and returns all expected results (this requires the function score query to be wrapped in a query filter, of course)
- Adding `must` and `should` clauses to the bool query has no impact on the result

Note that this is only an issue in 1.7.5 and below, which I totally understand makes it way lower priority.  I've tested with all of the most recent 2.x point releases and have been unable to reproduce with those releases.  I'm putting in this report more because I'm not super familiar w/ bool internals and wanted to make sure this wasn't something systemic that may cause future issues.
</description><key id="154638442">18315</key><summary>Function score queries in must_not bool clauses producing incorrect results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rusnyder</reporter><labels><label>:Search</label><label>bug</label><label>feedback_needed</label></labels><created>2016-05-13T05:46:55Z</created><updated>2016-07-19T08:26:44Z</updated><resolved>2016-07-19T08:26:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T17:08:35Z" id="219102637">Hi @rusnyder 

I've tried out your replication and it seems to work just fine on 1.7.5.  This is what I did:

```
PUT /fnscore-test-index
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  }
}

PUT /fnscore-test-index/event/1
{"field":"value-1"}
PUT /fnscore-test-index/event/2
{"field":"value-2"}

POST /fnscore-test-index/event/_count
{
  "query": {
    "bool": {
      "must_not": [
        {
          "function_score": {
            "functions": [
              {
                "script_score": {
                  "script": "-1",
                  "lang": "expression"
                }
              }
            ],
            "score_mode": "sum",
            "boost_mode": "replace",
            "min_score": 0
          }
        }
      ]
    }
  }
}
```

&gt; Adding a refresh after each insert changes how many documents fail to match the query criteria in small samples, but with an increased sample size, that difference diminishes as the sample size increases, more/less completely vanishing in the 80-100 document range

This makes me think you have changed the refresh interval on this index?  I think if you try this on a clean cluster with default settings, you'll find it working just fine.
</comment><comment author="rusnyder" created="2016-05-17T15:01:08Z" id="219745822">I'm seeing the same behavior as you (as in, not broken) when executing the same commands as you on a fresh cluster, but am still able to reproduce my issue by replacing the independent index requests with either a for loop in bash OR (newly discovered) with a bulk request:

_NOTE: I'll switch to using sense-like syntax for sharing queries - sorry for the bash!_

``` shell
DELETE /fnscore-test-index

PUT /fnscore-test-index
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  }
}

POST /fnscore-test-index/_bulk
{"index":{"_type":"event","_id":"1"}}
{"field":"value-1"}
{"index":{"_type":"event","_id":"2"}}
{"field":"value-2"}

POST /fnscore-test-index/event/_count
{
  "query": {
    "bool": {
      "must_not": [
        {
          "function_score": {
            "functions": [
              {
                "script_score": {
                  "script": "-1",
                  "lang": "expression"
                }
              }
            ],
            "score_mode": "sum",
            "boost_mode": "replace",
            "min_score": 0
          }
        }
      ]
    }
  }
}
```

Since both a for loop of individual index requests and a bulk request are able reproduce the issue (for me, anyway - the verdict is out for yourself!), it leads me to believe the lack-of-delay between insert statements is closely tied to the underlying issue.  It also makes me feel like it's not a fun one.
</comment><comment author="clintongormley" created="2016-05-17T18:45:44Z" id="219815038">And if you do:

```
POST fnscore-test-index/_refresh
```

before your count?
</comment><comment author="rusnyder" created="2016-05-17T20:41:58Z" id="219847107">Adding a refresh doesn't change the count.  I then also tried to optimize, just for kicks and giggles, since that fixed similar (yet unrelated) issues in another cluster to no avail:

```
POST /fnscore-test-index/_optimize?max_num_segments=1
```

I added the explain param to the "failing" query and even the explain results indicate that the hit that is being returned shouldn't, in fact, be returned since it's not actually a search hit (either that or I'm not reading the explain output correctly, which is possible):

``` javascript
POST /fnscore-test-index/event/_search?explain
{
  "query": {
    "bool": {
      "must_not": [
        {
          "function_score": {
            "functions": [
              {
                "script_score": {
                  "script": "-1",
                  "lang": "expression"
                }
              }
            ],
            "score_mode": "sum",
            "boost_mode": "replace",
            "min_score": 0
          }
        }
      ]
    }
  }
}

// Note that value/description indicate that this shouldn't have hit:

{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_shard": 0,
            "_node": "nvCXAPZoS3OImOn5X0l72A",
            "_index": "fnscore-test-index",
            "_type": "event",
            "_id": "1",
            "_score": 1,
            "_source": {
               "field": "value-1"
            },
            "_explanation": {
               "value": 0,
               "description": "Failure to meet condition(s) of required/prohibited clause(s)",
               "details": [
                  {
                     "value": 0,
                     "description": "match on prohibited clause (function score (ConstantScore(*:*),function=script[-1], params [null]))",
                     "details": [
                        {
                           "value": -1,
                           "description": "function score, product of:",
                           "details": [
                              {
                                 "value": -1,
                                 "description": "Math.min of",
                                 "details": [
                                    {
                                       "value": -1,
                                       "description": "script score function, computed with script:\"-1",
                                       "details": [
                                          {
                                             "value": 1,
                                             "description": "_score: ",
                                             "details": [
                                                {
                                                   "value": 1,
                                                   "description": "ConstantScore(*:*), product of:",
                                                   "details": [
                                                      {
                                                         "value": 1,
                                                         "description": "boost"
                                                      },
                                                      {
                                                         "value": 1,
                                                         "description": "queryNorm"
                                                      }
                                                   ]
                                                }
                                             ]
                                          }
                                       ]
                                    },
                                    {
                                       "value": 3.4028235e+38,
                                       "description": "maxBoost"
                                    }
                                 ]
                              },
                              {
                                 "value": 1,
                                 "description": "queryBoost"
                              }
                           ]
                        }
                     ]
                  },
                  {
                     "value": 1,
                     "description": "ConstantScore(*:*), product of:",
                     "details": [
                        {
                           "value": 1,
                           "description": "boost"
                        },
                        {
                           "value": 1,
                           "description": "queryNorm"
                        }
                     ]
                  }
               ]
            }
         }
      ]
   }
}
```
</comment><comment author="clintongormley" created="2016-05-18T11:06:26Z" id="219994647">&gt; I'm seeing the same behavior as you (as in, not broken) when executing the same commands as you on a fresh cluster, but am still able to reproduce my issue by replacing the independent index requests with either a for loop in bash OR (newly discovered) with a bulk request:

Do you mean that you're able to replicate this on a fresh cluster of 1.7.5? Or only on your existing cluster?

I've run your recreation many times without seeing it fail once.
</comment><comment author="rusnyder" created="2016-05-18T20:09:20Z" id="220142988">My apologies - I mean in a fresh cluster.  Perhaps it's something machine or JVM specific.  I'll try reproducing this with a few more combinations of machine + JVM version and get back with more info.

EDIT: To be clear, by "fresh cluster" I mean deleting the data directory entirely, then starting Elasticsearch and running the recreation script.
</comment><comment author="rusnyder" created="2016-05-20T18:01:28Z" id="220676576">I've gotten a few other devs here to reproduce the issue w/ fresh ES 1.7.5 installs and fresh clusters on some different OS's and Java versions (combinations of Fedora, Ubuntu, and OS X with Java 1.8 updates 45, 60, and 91).  I've also created a now-failing test that reproduces the issue (sorry I didn't lead with this!): https://github.com/rusnyder/elasticsearch/commit/554e36c5855ab5a314f1d32d968d57493f1f617a

I can inline a patch for that test, if desired, but otherwise will just spend some time this weekend trying to figure out what's happening.
</comment><comment author="rusnyder" created="2016-05-20T23:58:31Z" id="220744586">I think I figured out what the issue is, but I haven't quite worked out a solution yet.

During the query phase of the aforementioned queries (`must_not` wrapping a `function_score`), this internally gets scored using Lucene's [`ReqExclScorer`](https://github.com/apache/lucene-solr/blob/branch_4x/lucene/core/src/java/org/apache/lucene/search/ReqExclScorer.java) wrapping an effective match all as the required scorer (more specifically, a `ConstantScore(*:*)` and a [`CustomBoostFactorScorer`](https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java) as the exclusion scorer.  When using the `min_score` parameter of the function score query, the `CustomBoostFactorScorer` delegates advancement through the document set to its inner class, [`MinScoreNextDoc`](https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java#L92)

The problem is that `MinScoreNextDoc.advance(int)` and `ReqExclScorer.nextDoc` are stepping on each other's toes, because both are advancing (calling `nextDoc()`) on the underlying scorer but`ReqExclScorer` is more/less expecting to be the only thing doing the advancement while marking exclusions.

I'll walk through the code path of concern here (all info on variables was found by and can reproduced by stepping through the test case I linked in the previous comment):

[`org.apache.lucene.search.Weight.DefaultBulkScorer#scoreAll(Collector, Scorer)`](https://github.com/apache/lucene-solr/blob/branch_4x/lucene/core/src/java/org/apache/lucene/search/Weight.java#L190)

``` java
public abstract class Weight {
  ...
  static class DefaultBulkScorer extends BulkScorer {
    /* [rusnyder] This field, in our test case, is set to the ReqExclScorer of concern */
    private final Scorer scorer;
    ...
    static void scoreAll(Collector collector, Scorer scorer) throws IOException {
      int doc;
      /* [rusnyder] scorer.nextDoc() -&gt; see next snippet */
      while ((doc = scorer.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
        collector.collect(doc);
      }
    }
    ...
  }
}
```

[`org.apache.lucene.search.ReqExclScorer#nextDoc()`](https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.10.4/lucene/core/src/java/org/apache/lucene/search/ReqExclScorer.java#L46)

``` java
class ReqExclScorer extends Scorer {
  ...
  public int nextDoc() throws IOException {
    if (reqScorer == null) {
      return doc;
    }
    /* [rusnyder] This is the first of 2 document advancements in the ReqExclScorer */
    doc = reqScorer.nextDoc();
    if (doc == NO_MORE_DOCS) {
      reqScorer = null; // exhausted, nothing left
      return doc;
    }
    if (exclDisi == null) {
      return doc;
    }
    /* [rusnyder] toNonExcluded() -&gt; see next snippet */
    return doc = toNonExcluded();
  }
  ...
}
```

[`org.apache.lucene.search.ReqExclScorer#toNonExcluded()`](https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.10.4/lucene/core/src/java/org/apache/lucene/search/ReqExclScorer.java#L72)

``` java
class ReqExclScorer extends Scorer {
  ...
  private int toNonExcluded() throws IOException {
    int exclDoc = exclDisi.docID();
    int reqDoc = reqScorer.docID(); // may be excluded
    do {  
      if (reqDoc &lt; exclDoc) {
        return reqDoc; // reqScorer advanced to before exclScorer, ie. not excluded
      } else if (reqDoc &gt; exclDoc) {
        /* [rusnyder] exclDisi is an instance of 
           FunctionScoreQuery.FunctionFactorScorer here, but that class 
           inherits CustomBoostFactorScorer which is where the 
           pertinent logic is */
        /* [rusnyder] exclDisi.advance(reqDoc) -&gt; see next snippet */
        exclDoc = exclDisi.advance(reqDoc);
        if (exclDoc == NO_MORE_DOCS) {
          exclDisi = null; // exhausted, no more exclusions
          return reqDoc;
        }
        if (exclDoc &gt; reqDoc) {
          return reqDoc; // not excluded
        }
      }
    /* [rusnyder]: The function score query from the test should result in
        all documents being added to the exclusion set, whereas this nextDoc()
        is only invoked for sparse exclusion sets (i.e. - our test never hits this) */
    } while ((reqDoc = reqScorer.nextDoc()) != NO_MORE_DOCS);
    reqScorer = null; // exhausted, nothing left
    return NO_MORE_DOCS;
  }
  ...
}
```

[`org.elasticsearch.common.lucene.search.function.CustomBoostFactorScorer.MinScoreNextDoc#aadvance(int)`](https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java#L112)

``` java
abstract class CustomBoostFactorScorer extends Scorer {
    ...
    public class MinScoreNextDoc implements NextDoc {
        ...
        public int advance(int target) throws IOException {
            int doc = scorer.advance(target);
            if (doc == NO_MORE_DOCS) {
                return doc;
            }
            currentScore = innerScore();
            if (currentScore &lt; minScore) {
                /* [rusnyder] This is the advancement that conflicts with
                   the advancements made above */
                return scorer.nextDoc();
            }
            return doc;
        }
    }
    ...
}
```

The conflict is in between the call `scorer.nextDoc()` from within the `CustomBoostFactorScorer` (Elasticsearch) and the `reqScorer.nextDoc()` call from the ReqExclScorer (Lucene).  I tried increasing the number of documents that I indexed in my test, and what I saw made lots of sense: For a contiguous sequence of documents within a single segment that all match the "exclusion" criteria (the code path I outlined is within the context of a single `AtomicReader` on a single segment), every other document from such a sequence is being returned.  This makes plenty of sense when looking at the code because for each document that "hits" the exclusion set in the code path above, the `nextDoc()` function gets invoked twice which ultimately results in the following document within that sequence to be skipped.

I'm new to this part of the codebase and am still rationalizing where exactly the breakdown is and which part of the code is responsible for sorting this out.  My instinct leads me to believe that the onus is either on Elasticsearch's `CustomBoostFactorScorer` to stop advancing the underlying scorer or on Lucene's `ReqExclScorer` to better handle the case where the exclusion scorer needs to advance through multiple docs at a time to build the exclusion set.

Anyway, I'll see if I can pull something together that makes sense, and in the meantime, and if there's any useful info you have in the meantime, I'd love to hear it!
</comment><comment author="clintongormley" created="2016-05-23T10:36:25Z" id="220945449">Awesome work @rusnyder !!!  Thank you for diving into this.

@jpountz please could you take a look?
</comment><comment author="jpountz" created="2016-05-23T11:44:35Z" id="220957954">@rusnyder Thanks for digging! Base on the description of the issue, I think all that is needed to fix the bug is the following?

``` patch
diff --git a/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java b/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java
index bcc785a..ded96e8 100644
--- a/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java
+++ b/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java
@@ -116,7 +116,7 @@ abstract class CustomBoostFactorScorer extends Scorer {
             }
             currentScore = innerScore();
             if (currentScore &lt; minScore) {
-                return scorer.nextDoc();
+                return nextDoc();
             }
             return doc;
         }

```
</comment><comment author="rusnyder" created="2016-05-23T14:54:31Z" id="221003522">Wow @jpountz - that was easy.  It's like you guys know what you're doing...

That worked like a charm!  I'm happy to put out a PR w/ the test and the fix, but I'll leave that decision up to you!

Thanks again!!
</comment><comment author="rusnyder" created="2016-05-26T20:15:51Z" id="221981854">Of note, I went back and tested this in 2.0.2, 2.1.2, 2.2.2, and 2.3.3 now that I had a better understand of what was going on and the test case more honed in, and there is a similar issue in 2.0.2, 2.1.2, and 2.2.2, except that all three of those versions consistently return zero documents for the "must not(function score(match no docs))" query when they should be returning all of the docs.  2.3.3 returns the right results for the query.

I haven't had much time to dig in on those, but I took a look to see what would happen if I applied the same patch to those versions and this patch does NOT fix the 2.x versions.  I'll dig more later, but since the `ReqExclScorer` more/less received an overhaul in Lucene 5.x, it'll take a bit more exploring for me to track down.
</comment><comment author="jpountz" created="2016-05-27T13:16:56Z" id="222143495">Thanks for the PR!

&gt;  return zero documents for the "must not(function score(match no docs))" query when they should be returning all of the docs

I could not replicate this bug, can you share the query you are running?
</comment><comment author="rusnyder" created="2016-05-27T13:29:37Z" id="222146436">Sure thing.  It's the same as the original steps to reproduce the 1.7 bug, but I added an optimize to guarantee all documents are on the same segment first:

```
PUT /fnscore-test-index
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  }
}

POST /fnscore-test-index/_bulk
{"index":{"_type":"event","_id":"1"}}
{"field":"value-1"}
{"index":{"_type":"event","_id":"2"}}
{"field":"value-2"}

POST /fnscore-test-index/_refresh
POST /fnscore-test-index/_optimize?max_num_segments=1

POST /fnscore-test-index/event/_search?explain
{
  "query": {
    "bool": {
      "must_not": [
        {
          "function_score": {
            "functions": [
              {
                "script_score": {
                  "script": "-1",
                  "lang": "expression"
                }
              }
            ],
            "score_mode": "sum",
            "boost_mode": "replace",
            "min_score": 0
          }
        }
      ]
    }
  }
}
```

I hope I'm not going crazy, but that sequence is causing the query to return 0 events on fresh installs of the three aforementioned versions for me.
</comment><comment author="jpountz" created="2016-05-27T13:45:51Z" id="222150298">The default query for a function_score is a match_all query, so I think returning no hits in this case is the expected behoviour?
</comment><comment author="rusnyder" created="2016-05-27T15:53:43Z" id="222183676">I assumed that was the default behavior was a `match_all` if no function was specified, not if a function was specified that matched no events.  As per the [function score docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html):

&gt; If no query is given with a function this is equivalent to specifying `"match_all": {}`

Since we _are_ specifying a function here that should be matching zero events, I'd think the docs from that `match_all` would all get filtered out by that function.  To add to my expected behavior, with the index setup and documents as indexed in the previous comment, running just the function score query without the `must_not` _also_ provides 0 results, which is the behavior that I would expect:

```
POST /fnscore-test-index/_search
{
   "query": {
      "function_score": {
         "functions": [
            {
               "script_score": {
                  "script": "-1",
                  "lang": "expression"
               }
            }
         ],
         "score_mode": "sum",
         "boost_mode": "replace",
         "min_score": 0
      }
   }
}
```
</comment><comment author="jpountz" created="2016-05-30T09:48:43Z" id="222456075">@rusnyder Function score takes two kinds of queries: one that defines the matching docs, and then one per function that defines the documents to apply the function to (which is effectively just a filter since its score is never used). The sentence you are quoting is about the latter. Maybe we should reuse the parameter name and rewrite it to `If no _filter_ is given with a function this is equivalent to specifying "match_all": {}` to be clearer.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TopHitsIT and InnerHitsIT try to serialize compiled scripts over the wire</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18314</link><project id="" key="" /><description>These tests (or code) are buggy. Nothing can be assumed about the compiled form of a script.

The tests only work today because MockScriptEngine uses a String representation for compiled code.
</description><key id="154638418">18314</key><summary>TopHitsIT and InnerHitsIT try to serialize compiled scripts over the wire</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T05:46:39Z</created><updated>2016-10-18T08:49:35Z</updated><resolved>2016-05-13T18:12:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-13T18:12:33Z" id="219119045">I fixed this in the branch for #18319, it was a bug in the mock script engine.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add CONSOLE tests to aliases documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18313</link><project id="" key="" /><description>I tried to leave most of the examples the same and isolated, but there are a few examples that continue through multiple requests, so I used TEST[continued] a few times.

Towards the end, the existing documentation shuffles back and forth a few times, so my choices were to either have a lot of TEST[continued] strung together or to replace the examples so that they were a bit more isolated and I generally did the latter.

Also brought the examples up to year 2016.
</description><key id="154614571">18313</key><summary>Add CONSOLE tests to aliases documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>docs</label></labels><created>2016-05-13T01:18:39Z</created><updated>2016-05-13T03:50:59Z</updated><resolved>2016-05-13T03:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-13T01:52:46Z" id="218932784">LGTM
</comment><comment author="eskibars" created="2016-05-13T01:56:35Z" id="218933186">Thanks @nik9000 I was hoping you might take a look :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return number of documents in the result counts as HTTP header or in JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18312</link><project id="" key="" /><description>Elasticsearch version: 2.3.2

JVM version: 1.8.0_91

**OS version**: Linux dev 4.4.0-21-generic #37-Ubuntu SMP Mon Apr 18 18:33:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

**Describe the feature**:

We're a SaaS provider of social media data via Elasticsearch.  We have a large ES of about 150 machines and about 6 months of social media content.

You can check our our service at:

http://spinn3r.com

We need the ability to throttle customers based on the number of documents they have returned.  We have a piece of HTTP proxy middleware that sits between the Internet and our ES boxes which allows us to provide authorization, logging, etc.

We REALLY need the ability to easily keep track of the number of documents fetched.  

What we've done so far is developed an internal ES plugin with new _search and _msearch endpoints that exposes these as HTTP headers.

The headers just include the number of documents returned in the search request.

This prevents us from having to parse the documents to find out the number returned.

We would LOVE for this functionality to be integrated into stock elasticsearch.  
</description><key id="154614077">18312</key><summary>Return number of documents in the result counts as HTTP header or in JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">burtonator</reporter><labels><label>:REST</label><label>feedback_needed</label></labels><created>2016-05-13T01:12:49Z</created><updated>2017-03-31T14:49:16Z</updated><resolved>2017-03-31T14:48:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T15:49:12Z" id="219082804">Hi @burtonator 

I can definitely see the utility for you of doing this. There's a related request here https://github.com/elastic/elasticsearch/issues/16993

The questions this raises for me are:
- What if somebody else wants a different value returned in a header
- What if others don't want the headers

I'm struggling to come up with a different value for the search API that users might want as a header, and there could probably be a setting to disable headers, but i'll leave this here for discussion
</comment><comment author="burtonator" created="2016-05-18T18:59:33Z" id="220125124">Well the headers don't impose that much overhead.  Maybe 10 bytes per request.  There are already a ton of headers there anyway.  

X-result-count: 150 

would be fine.  
</comment><comment author="s1monw" created="2016-05-20T09:27:43Z" id="220558925">@burtonator I think we can work on making the search and msearch rest actions simpler such that you can override the response handler and set the headers yourself? This would make your plugin trivial and future proof? Would that help you as a first step?
</comment><comment author="clintongormley" created="2017-02-12T11:49:30Z" id="279213166">No further feedback. Closing</comment><comment author="burtonator" created="2017-02-12T19:10:38Z" id="279240527">Can we reopen this.  I think the header should just be a STANDARD part of Elasticsearch. It seems reasonable to have the document response count easily parsed.  Right now it's not.  </comment><comment author="clintongormley" created="2017-02-13T10:23:38Z" id="279347247">&gt;  I think the header should just be a STANDARD part of Elasticsearch.

You do, for your application.  I neither need it nor want it.  Hence, @s1monw's offer to extend the REST api to make your plugin simpler, which was waiting for a response.</comment><comment author="colings86" created="2017-03-31T14:48:24Z" id="290732962">No further feedback. @burtonator if you would like to see the feature that @s1monw described please comment on this issue and we can discuss.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase default heap size to 2g</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18311</link><project id="" key="" /><description>Benchmarking with the geonames data set shows that Elasticsearch truly
needs 2g of heap or the heap is a bottleneck for indexing rates. If our
goal is to satisfy the out-of-the-box-experience, we should prioritize
the out-of-the-box performance experience. Thus, the default heap should
be 2g until smaller heaps are not the bottleneck for indexing small data
sets.

Relates #16334, relates #17686, relates #18309 
</description><key id="154613568">18311</key><summary>Increase default heap size to 2g</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-13T01:07:11Z</created><updated>2016-05-26T20:51:18Z</updated><resolved>2016-05-14T11:52:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-13T01:08:20Z" id="218927622">+1, it makes the defaults honest. 512MB is just not enough.
</comment><comment author="dadoonet" created="2016-05-13T05:01:23Z" id="218952439">+1
</comment><comment author="s1monw" created="2016-05-13T07:14:24Z" id="218968755">lets do it! +1
</comment><comment author="s1monw" created="2016-05-13T07:15:00Z" id="218968843">since we now increase the heap size should we add benchmarks with lower heap size to track if stuff gets better / worse?
</comment><comment author="danielmitterdorfer" created="2016-05-13T07:17:44Z" id="218969275">@s1monw We can do this. I can pin one setup to 1gb but I'd say we don't include the (moving) default heap size in the benchmarks (i.e. now the 2gb setting) from now on. Wdyt?
</comment><comment author="s1monw" created="2016-05-13T07:55:58Z" id="218976032">yeah why not
</comment><comment author="danielmitterdorfer" created="2016-05-13T07:57:23Z" id="218976323">Provided nobody else objects, the default setup will run the benchmark candidate with 1GB heap as of the next nightly run (i.e. tomorrow).
</comment><comment author="jasontedor" created="2016-05-13T08:05:47Z" id="218977881">I think that we _should_ be benchmarking the 2g configuration because below that we are just hamstringing the benchmarks by the heap. Also, I've come to the view that we should be benchmarking the defaults so we can watch for regressions there. If this is the default, that's what users will be running if they do not change any settings. We see lots of benchmarks in the wild that do not modify the out-of-the-box settings, for better or for worse. 
</comment><comment author="mikemccand" created="2016-05-13T08:12:42Z" id="218979258">I think it's vital that we DO benchmark at the default settings: many users will do just that and draw conclusions.  In fact, I think it's the most important curve in the nightly benchmarks ;)

We can also benchmark at other fixed heap sizes so we know the impact, of either too little or too big heap sizes.
</comment><comment author="danielmitterdorfer" created="2016-05-13T08:14:40Z" id="218979626">Ok, then I'll revert.
</comment><comment author="s1monw" created="2016-05-13T08:28:13Z" id="218982219">I misunderstood - of course we have to benchmark defaults. I though you are saying you are not adding 2GB explicitly
</comment><comment author="jasontedor" created="2016-05-13T11:11:30Z" id="219015323">@clintongormley I pushed bc46517f7e941bd09336652338bce6396769b6f8 to set unequal defaults of 256m and 2g.
</comment><comment author="djschny" created="2016-05-17T23:46:34Z" id="219887063">While the change itself is obviously trivial, this change is a **big** deal IMO. There are tons of users who use elasticsearch in limited resource environments. Before they could use the defaults and not worry about consuming too many resources.

For these reasons I feel this should be labelled as a breaking change.
</comment><comment author="clintongormley" created="2016-05-18T11:39:56Z" id="220001170">@djschny nobody should be using the default heap size in production.  In fact, we fail if min heap != max heap in production mode
</comment><comment author="djschny" created="2016-05-18T14:08:27Z" id="220037260">@clintongormley what is this "production mode" spoken of? If it is the startup checks that happen when not bound to localhost, then those did not exist in 2.x so people could have been running in "production mode" with defaults. Or are you implying that since the new "production mode" stuff is a breaking change, it will force people to set ES_HEAP_SIZE so hence making it a moot point?
</comment><comment author="clintongormley" created="2016-05-18T14:52:18Z" id="220051330">&gt; Or are you implying that since the new "production mode" stuff is a breaking change, it will force people to set ES_HEAP_SIZE so hence making it a moot point?

Precisely
</comment><comment author="jasontedor" created="2016-05-18T15:01:47Z" id="220054662">&gt; set ES_HEAP_SIZE

Note that `ES_HEAP_SIZE` does not exist in &gt;= 5.0.0.
</comment><comment author="anhlqn" created="2016-05-20T14:14:19Z" id="220617092">Just a note about heap size settings on Windows that could be added to documentation. After installing ES as a service on Windows, if we want to change the heap size, changing the value in jvm.options won't work. Need the change the value after running `service.bat manager`
</comment><comment author="jasontedor" created="2016-05-20T14:50:04Z" id="220627057">@anhlqn That's always been the case, I'm surprised it's not documented but it definitely does not seem to be. Thanks for the comment, will add.
</comment><comment author="jasontedor" created="2016-05-26T20:51:17Z" id="221990743">@anhlqn I opened #18606.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Histogram with extended bounds misses daylight savings date when no records matched</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18310</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.8.0_40-b25

**OS version**: Windows 7 Home Premium

**Description of the problem including expected versus actual behavior**:
Doing histo query over range with no matched data with extended bounds returns only 1 bucket on the daylight savings cross over for 2 hours.  When this is done with matching data you get 2 buckets, 1 for pre-dst one for post dst both with same formatted time (0300hrs).  It should consistently return 2 buckets or 1 bucket at crossover, which ever is correct.

**Steps to reproduce**:
Do a histo query with extended bounds on an index over a date range with no data over a daylight savings boundary.  Then do one with data and compare results around April 3rd 3am, 2016

e.g.

```
curl -XPOST 'localhost:9200/read_5555605bab95c2765d65c3cc_201601/_search' -d '
{
   "query":{
      "filtered":{
         "filter":{
            "bool":{
               "must":[
           {
            "term": {
                "p":"doesntexist"
            }
          }
               ]
            }
         }
      }
   },
   "aggs":{
    "events_by_date":{
       "date_histogram":{
          "field":"tsr",
          "interval":"1h",
          "time_zone":"Pacific/Auckland",
          "min_doc_count":0,
          "extended_bounds":{
             "min":"2016-03-31T11:00:00.000Z",
             "max":"2016-05-02T12:00:00.000Z"
          }
       }
    }
    }
}'
```
</description><key id="154609588">18310</key><summary>Date Histogram with extended bounds misses daylight savings date when no records matched</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">mattdawson</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2016-05-13T00:23:24Z</created><updated>2016-10-24T14:36:58Z</updated><resolved>2016-10-24T14:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T12:06:20Z" id="219025309">@colings86 please could you take a look at this?
</comment><comment author="mattdawson" created="2016-05-15T22:10:34Z" id="219313848">Looking deeper in to this I realise that the intention is for one bucket for the hour covering the DST change over.
Essentially I have time series data with tsr = timestamp and tsrf = timestamp formatted, all stored against a channel denoted by 'c', with a sequence id of 'k', where if the data becomes non contiguous for some reason the 'k' value gets incremented.
I then do terms agg plus a nested histo query over the data and merge multiple streams into one.
But, at change over is happens.. A dual 2am record for the second term aggregation where k=1
Look for ***\* in the log to see the double.

```
...
rowk1.tsrf==rowk2.tsrf? true rowk1.tsrf= 03/04/2016 01:00 rowk2.tsrf= 03/04/2016 01:00 rowk1.k= undefined rowk2.k= 1
rowk1.tsrf==rowk2.tsrf? true rowk1.tsrf= 03/04/2016 02:00 rowk2.tsrf= 03/04/2016 **** 02:00 **** rowk1.k= undefined rowk2.k= 1
rowk1.tsrf==rowk2.tsrf? false rowk1.tsrf= 03/04/2016 03:00 rowk2.tsrf= 03/04/2016 **** 02:00 ****  rowk1.k= undefined rowk2.k= 1
rowk1.tsrf==rowk2.tsrf? false rowk1.tsrf= 03/04/2016 04:00 rowk2.tsrf= 03/04/2016 03:00 rowk1.k= undefined rowk2.k= 1
...
rowk1.tsrf==rowk2.tsrf? false rowk1.tsrf= 02/05/2016 13:00 rowk2.tsrf= 02/05/2016 12:00 rowk1.k= undefined rowk2.k= 1
rowk1.tsrf==rowk2.tsrf? false rowk1.tsrf= 02/05/2016 14:00 rowk2.tsrf= 02/05/2016 13:00 rowk1.k= 0 rowk2.k= 1
rowk1.tsrf==rowk2.tsrf? false rowk1.tsrf= 02/05/2016 15:00 rowk2.tsrf= 02/05/2016 14:00 rowk1.k= 0 rowk2.k= 1
rowk1.tsrf==rowk2.tsrf? false rowk1.tsrf= 02/05/2016 16:00 rowk2.tsrf= 02/05/2016 15:00 rowk1.k= 0 rowk2.k= undefined
...
```

What you see here is a two streams in a channel, one from 31/3/2016 11:00 to 02/05/2016 14:00, and one from 02/05/2016 13:00 to 02/05/2016 14:00:00.
I think the bug only occurs when there are two streams and thus two buckets for k values(and thus two date_histos).

Heres the full query I execute.

```
{
        "index": "read_5555605bab95c2765d65c3cc_201603,read_5555605bab95c2765d65c3cc_201604,read_5555605bab95c2765d65c3cc_201605",
        "type": "reading",
        "ignoreUnavailable": true,
        "allowNoIndices": true,
        "size": 0
}
{
        "query": {
                "filtered": {
                        "filter": {
                                "bool": {
                                        "must": [
                                                {
                                                        "term": {
                                                                "c": "57314b3f361be1ce3c55110d"
                                                        }
                                                },
                                                {
                                                        "term": {
                                                                "p": "energy_active_sum"
                                                        }
                                                },
                                                {
                                                        "range": {
                                                                "tsr": {
                                                                        "gte": "2016-03-30T11:00:00.000Z",
                                                                        "lt": "2016-05-02T12:00:00.000Z"
                                                                }
                                                        }
                                                }
                                        ]
                                }
                        }
                }
        },
        "aggs": {
                "by_k": {
                        "terms": {
                                "field": "k",
                                "size": 0,
                                "order": {
                                        "_term": "asc"
                                }
                        },
                        "aggs": {
                                "events_by_date": {
                                        "date_histogram": {
                                                "field": "tsr",
                                                "interval": "1h",
                                                "time_zone": "Pacific/Auckland",
                                                "min_doc_count": 0,
                                                "extended_bounds": {
                                                        "min": "2016-03-30T11:00:00.000Z",
                                                        "max": "2016-05-02T12:00:00.000Z"
                                                },
                                                "order": {
                                                        "_key": "asc"
                                                }
                                        },
                                        "aggs": {
                                                "maxtsr": {
                                                        "max": {
                                                                "field": "tsr"
                                                        }
                                                },
                                                "mintsr": {
                                                        "min": {
                                                                "field": "tsr"
                                                        }
                                                },
                                                "maxvd": {
                                                        "max": {
                                                                "field": "vd"
                                                        }
                                                },
                                                "minvd": {
                                                        "min": {
                                                                "field": "vd"
                                                        }
                                                }
                                        }
                                }
                        }
                }
        }
}
```
</comment><comment author="mattdawson" created="2016-05-15T22:39:10Z" id="219315379">Further thought leads me to think the k=0 stream is correct because it's dst values were created by extended bounds, and the second failed because it actually had data.
</comment><comment author="cbuescher" created="2016-05-19T11:50:13Z" id="220301919">@mattdawson can you take a look at #18326 and the fix provided in #18415 to see if this solves your problem? I have looked at the queries you are using and this looks very similar, however I'm not fully able to understand your use case or check this with the patch from #18415 without a better understanding of your data. If looking at the two issues I linked to doesn't help, can you provide a minimal example with a few datapoints?
</comment><comment author="mattdawson" created="2016-05-20T02:58:50Z" id="220505781">@cbuescher, The code change appears to add one non-DST adjusted unit inside TimeUnitRounding .nextRoundingValue for intervals less than 1 day.  Although the outcome is a consistent dual bucket for both extended and non-extended result sets, I believe it's the wrong approach.  What you want is a single bucket covering 2 hours for the DST change over.  The reason being because some DST change overs are only 30mins and this method would not be congruous between 1hour and 30min DST changes.
</comment><comment author="matsondawson" created="2016-05-30T04:01:54Z" id="222407440">@cbuescher,
That change breaks 15 min intervals.  The buckets seem to change one hour early.  And I'm getting overlapping data results.  In the data below the min of the next bucket should be more than the max of the previous bucket.
key_as_string is in Pacific/Auckland time.  It should change to +12:00 at 3am, but it's changing at 2am.

```
{ key_as_string: '2016-04-03T01:45:00.000+13:00', mintsr: '2016-04-02T12:45:04.523Z', maxtsr: '2016-04-02T12:50:00.728Z', minvd: 6127.3, maxvd: 6127.3 }
{ key_as_string: '2016-04-03T02:00:00.000+12:00', mintsr: '2016-04-02T13:00:00.755Z', maxtsr: '2016-04-02T14:10:00.814Z', minvd: 6127.4, maxvd: 6127.9 }
{ key_as_string: '2016-04-03T02:15:00.000+12:00', mintsr: '2016-04-02T13:15:00.682Z', maxtsr: '2016-04-02T14:25:00.722Z', minvd: 6127.5, maxvd: 6128.0 }
{ key_as_string: '2016-04-03T02:30:00.000+12:00', mintsr: '2016-04-02T13:30:00.789Z', maxtsr: '2016-04-02T14:40:00.684Z', minvd: 6127.6, maxvd: 6128.1 }
{ key_as_string: '2016-04-03T02:45:00.000+12:00', mintsr: '2016-04-02T13:45:05.004Z', maxtsr: '2016-04-02T14:55:05.685Z', minvd: 6127.7, maxvd: 6128.2 }
{ key_as_string: '2016-04-03T03:00:00.000+12:00', mintsr: '2016-04-02T15:00:06.389Z', maxtsr: '2016-04-02T15:10:00.729Z', minvd: 6128.3, maxvd: 6128.3 }
```
</comment><comment author="cbuescher" created="2016-05-30T09:13:26Z" id="222449265">@matsondawson This is a different problem unrelated to #18415 , although from a user perspective it appears related. If you use 15 min intevals, ES is using `TimeIntervalRounding` internally which wasn't affected by that change. I'm pretty sure there are edge cases with arbitrary interval lengths around dst changes have glitches in them, so can I ask you again for an example containing few data ponts and a query that doesn't work for you, then I will look into this again. 
</comment><comment author="jpountz" created="2016-10-24T14:36:58Z" id="255758780">Closing due to lack of feedback.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unable to index trivial dataset without entering GC hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18309</link><project id="" key="" /><description>Followup to https://github.com/elastic/elasticsearch/pull/16334

In previous releases, elasticsearch would use up to 1GB of ram by default. With this change, it will only use 512MB. The problem is, elasticsearch is not ready for this, and our own benchmarks show it: https://benchmarks.elastic.co/index.html#gc_time

You can easily see the ~ **25x** increase in old GC time with this change, for this trivial dataset.

There are two ways to fix this:
1. go back to using a 1GB heap by default
2. fix ES to not waste so much RAM.

I don't care which one we do, but the defaults should support indexing a trivial dataset. I've indexed this dataset enough times with lucene to know exactly how trivial it is, disputing that point will not be productive.

The way I see it, ES isn't ready for a 512mb default heap for a number of reasons:
- explosive bombs like fielddata still exist in some shape or form
- trappy RecylingXYZPools of this form or another hogging RAM
- use of netty as a library for networking
- creation of far too many threads, beyond the point of being wasteful.

This list could go on and on. Most of these are simply bad tradeoffs. It may be easier to just go back to a 1GB heap for 5.0 than to try to address these things. But lets not blame @jasontedor for trying to reduce the memory, it is a good goal.
</description><key id="154606585">18309</key><summary>unable to index trivial dataset without entering GC hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>blocker</label><label>Meta</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T23:54:11Z</created><updated>2016-05-15T11:04:03Z</updated><resolved>2016-05-15T11:04:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-13T01:28:32Z" id="218930087">&gt; You can easily see the ~ 25x increase in old GC time with this change, for this trivial dataset.

For what it's worth, I was only able to reproduce a 10x increase in GC time with the decrease from a 1g to a 512m heap. I do not want this comment to distract from the issue though.

&gt; go back to using a 1GB heap by default

I think we should go to 2g and I opened #18311. The work I did studying the impact of #16334 shows that at 2g we stop getting returns from increasing the heap. Namely: allocations in the TLABs, allocation rates and GC pause times do not change above 2g. 

&gt; fix ES to not waste so much RAM.

But of course, we should do this too, but that's a larger effort. :smile:
</comment><comment author="rmuir" created="2016-05-13T01:48:51Z" id="218932341">I am fine with your analysis. If you think 2GB is what we need, +1

We may not _like_ that it is an increase from 1GB, but reality is reality. I think we should size the defaults to be really functional defaults, ones that actually work, and then separately open an issue to reduce the ram usage so we can lower this number. I don't like the number myself, but that is the number we have.
</comment><comment author="jasontedor" created="2016-05-15T11:04:03Z" id="219278784">Closed by #18311
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch init.d scripts to use bash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18308</link><project id="" key="" /><description>This commit modifies the init.d scripts to use bash now that bash is a
required dependency.

Relates #18259
</description><key id="154592680">18308</key><summary>Switch init.d scripts to use bash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T22:07:54Z</created><updated>2016-05-12T22:13:09Z</updated><resolved>2016-05-12T22:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-12T22:12:33Z" id="218901130">LGTM
</comment><comment author="jasontedor" created="2016-05-12T22:13:09Z" id="218901232">Thanks @dakrone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Loop through comma-separated data directories and create them in init.d startup script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18307</link><project id="" key="" /><description>We run `mkdir` on the $DATA_DIR in the deb init.d script (and with https://github.com/elastic/elasticsearch/pull/17419 don't run it at all if it has commas in it), but it would be good to loop through all the directories and create them if there is a comma in the list.
</description><key id="154592577">18307</key><summary>Loop through comma-separated data directories and create them in init.d startup script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T22:07:12Z</created><updated>2016-05-21T05:04:34Z</updated><resolved>2016-05-21T05:04:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-17T20:53:30Z" id="219850467">So, ES itself will create the data directories if they don't exist when it starts, therefore, should the init script do this? I would say it's not necessary since it would just be duplicating the logic already in ES' bootstrapping.

Thoughts?
</comment><comment author="jasontedor" created="2016-05-17T21:04:55Z" id="219853604">&gt; So, ES itself will create the data directories if they don't exist when it starts, therefore, should the init script do this? I would say it's not necessary since it would just be duplicating the logic already in ES' bootstrapping.

Sounds reasonable.

&gt; Thoughts?

I wonder if we should remove the "optimization" for a single directory then?
</comment><comment author="dakrone" created="2016-05-17T21:07:15Z" id="219854194">&gt; I wonder if we should remove the "optimization" for a single directory then?

That might be a good idea, I'll leave this open for a little bit longer (overnight at least) and see if anyone has a reason to keep it around
</comment><comment author="clintongormley" created="2016-05-20T09:32:39Z" id="220560039">Discussed in FixItFriday - just rely on Elasticsearch to create the directories, so remove the `mkdir` completely.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing another superfluous 's'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18306</link><project id="" key="" /><description>Pretty sure we're not making a brand new `/var/logs` directory when everything else goes into `/var/log`
</description><key id="154574441">18306</key><summary>Removing another superfluous 's'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">untergeek</reporter><labels /><created>2016-05-12T20:31:34Z</created><updated>2016-05-12T20:34:17Z</updated><resolved>2016-05-12T20:34:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-12T20:34:14Z" id="218877501">Closing in favor of #18305.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing the superfluous 's'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18305</link><project id="" key="" /><description>Pretty sure we're not making a brand new `/var/logs` directory when everything else goes into `/var/log`
</description><key id="154572762">18305</key><summary>Removing the superfluous 's'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">untergeek</reporter><labels><label>docs</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T20:23:22Z</created><updated>2016-05-12T20:42:38Z</updated><resolved>2016-05-12T20:42:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-12T20:26:25Z" id="218875411">It looks like there are a few more places with the same mistake. Twice in [configuration.asciidoc](https://github.com/elastic/elasticsearch/blob/df43230844b5c16238103a2962c8b415377b30a2/docs/reference/setup/configuration.asciidoc) and once in [deb.asciidoc](https://github.com/elastic/elasticsearch/blob/df43230844b5c16238103a2962c8b415377b30a2/docs/reference/setup/install/deb.asciidoc). Can you get those too @untergeek?
</comment><comment author="untergeek" created="2016-05-12T20:26:41Z" id="218875490">Will take a look.
</comment><comment author="untergeek" created="2016-05-12T20:29:50Z" id="218876358">I did this edit right on github, so I will have to do other commits the same way, I think.
</comment><comment author="jasontedor" created="2016-05-12T20:31:30Z" id="218876785">&gt; I did this edit right on github, so I will have to do other commits the same way, I think.

We can just fetch the `superfluous-s` branch locally and push back to the branch.
</comment><comment author="untergeek" created="2016-05-12T20:32:17Z" id="218876999">Sure, after I just made a `superfluous-s-deb` branch :)
</comment><comment author="untergeek" created="2016-05-12T20:33:00Z" id="218877178">let me see about that local pull
</comment><comment author="untergeek" created="2016-05-12T20:40:53Z" id="218879230">That should cover the bases.
</comment><comment author="jasontedor" created="2016-05-12T20:42:31Z" id="218879714">Awesome. Thank you. LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting on query_string not returning expected results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18304</link><project id="" key="" /><description>Tested on Elasticsearch 2.1.1 and 2.3.2.

Highlighting isn't behaving as I'd expect (nor as I've seen in Elasticsearch 1.x) with query_string queries where a field or default_field is not specified:

```
$ curl localhost:9200/test-highlight -XPUT
$ curl localhost:9200/test-highlight/test/_mapping -d'{"test": {"properties": {"tags": {"type": "string"}, "name": {"type": "string"}}}}'
$ curl localhost:9200/test-highlight/test/1 -d'{"name": "web-thing-1", "tags": ["web", "apache"]}'
$ curl localhost:9200/test-highlight/test/2 -d'{"name": "web-thing-2", "tags": []}'

```

If I run a query on tags:apache I get expected highlighting:

```
$ curl localhost:9200/test-highlight/test/_search -d'{"query": {"query_string": {"query": "tags:apache"}}, "highlight": {"fields": {"*": {}}}}'

{"took":3,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.625,"hits":[{"_index":"test-highlight","_type":"test","_id":"1","_score":0.625,"_source":{"name": "web-thing-1", "tags": ["web", "apache"]},"highlight":{"tags":["&lt;em&gt;apache&lt;/em&gt;"]}}]}}

```

If I specify default_field, highlighting is applied:

```
$ curl localhost:9200/test-highlight/test/_search -d'{"query": {"query_string": {"query": "apache", "default_field": "tags"}}, "highlight": {"fields": {"*": {}}}}'

{"took":4,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.625,"hits":[{"_index":"test-highlight","_type":"test","_id":"1","_score":0.625,"_source":{"name": "web-thing-1", "tags": ["web", "apache"]},"highlight":{"tags":["&lt;em&gt;apache&lt;/em&gt;"]}}]}}
```

However, if I run the same query without specifying the "tags" field:

```
$ curl localhost:9200/test-highlight/test/_search -d'{"query": {"query_string": {"query": "apache"}}, "highlight": {"fields": {"*": {}}}}'

{"took":3,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.4375,"hits":[{"_index":"test-highlight","_type":"test","_id":"1","_score":0.4375,"_source":{"name": "web-thing-1", "tags": ["web", "apache"]}}]}}

```

Turning off requires_field_match works but doesn't give the behavior I want; I'm hoping to have highlighting match the query as far as possible. Am I misunderstanding something about how highlighting's expected to work? Under elasticsearch 1.7, all 3 queries return highlighted results.
</description><key id="154566024">18304</key><summary>Highlighting on query_string not returning expected results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sjmc7</reporter><labels /><created>2016-05-12T19:49:42Z</created><updated>2016-05-13T11:58:21Z</updated><resolved>2016-05-13T11:58:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T11:58:21Z" id="219023829">&gt; Turning off requires_field_match works but doesn't give the behavior I want; I'm hoping to have highlighting match the query as far as possible. 

The only thing that changed was that we defaulted `require_field_match` to `true` instead of `false`, so setting it to `true` restores the same behaviour we had in 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New lemma-tizer plugin for ukrainian language.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18303</link><project id="" key="" /><description>Hi all,

I wonder whether you are interested in supporting a plugin which provides a mapping between ukrainian word forms and their lemmas. Some tests and docs go out-of-the-box =) .

https://github.com/mrgambal/elasticsearch-ukrainian-lemmatizer

It's really simple but still works and generates some value for its users.

P.S: @HonzaKral we had a discussion rgd this one on PyCon UA. Could you by any chance help?
</description><key id="154557566">18303</key><summary>New lemma-tizer plugin for ukrainian language.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrgambal</reporter><labels><label>:Analysis</label><label>feedback_needed</label></labels><created>2016-05-12T19:04:47Z</created><updated>2016-07-14T10:53:42Z</updated><resolved>2016-05-24T10:38:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T11:49:47Z" id="219022159">Hi @mrgambal 

Thanks for the offer.  I wonder if it shouldn't go directly into Lucene instead - that's where all our other analysis components live.  Also, I don't see a license in the repository.  It would have to be a compatible license for us (or Lucene) to accept the code.
</comment><comment author="mrgambal" created="2016-05-13T12:38:23Z" id="219031286">Hi @clintongormley 

Thanks for your response! Good spot on the license: I'll add a MIT, if you don't mind (hopefully - you don't).
IMO it isn't worth to be included into Lucene's set of plugins because this one doesn't provide any sort of real linguistic analysis. The thing is, it is just a `TokenFilter` and a huge dictionary of words and their lemmas.
Does that make any sense?
</comment><comment author="mrgambal" created="2016-05-13T16:37:14Z" id="219094979">Added a license in https://github.com/mrgambal/elasticsearch-ukrainian-lemmatizer/commit/e76a042218092e12be1642632e6ca7ff45b403fc
</comment><comment author="clintongormley" created="2016-05-13T17:56:18Z" id="219114846">@rmuir what do you think?
</comment><comment author="dchaplinsky" created="2016-05-16T20:56:42Z" id="219545360">@arysin, hooray!
</comment><comment author="arysin" created="2016-05-16T22:11:53Z" id="219564207">@dchaplinsky Wow, great news!
</comment><comment author="clintongormley" created="2016-05-17T09:26:06Z" id="219665340">I just asked @mikemccand and he would love to see this in Lucene.  Would you mind submitting this there instead?  Once it is merged into Lucene we can expose it in Elasticsearch
</comment><comment author="mrgambal" created="2016-05-17T09:48:54Z" id="219670604">@clintongormley no objections from me. Where should I create a ticket?
I'm not sure whether I can put anythin' into their ASF tracker.
</comment><comment author="clintongormley" created="2016-05-17T10:06:32Z" id="219674543">@mrgambal you should be able to create an issue here: https://issues.apache.org/jira/browse/LUCENE
</comment><comment author="mrgambal" created="2016-05-17T22:00:26Z" id="219867441">@clintongormley here we go: https://issues.apache.org/jira/browse/LUCENE-7287
cc: @mikemccand 

Bear with me: it's my very first ASF ticket. If I did anything wrong - please let me know)
</comment><comment author="clintongormley" created="2016-05-24T10:38:38Z" id="221231326">Closing in favour of https://issues.apache.org/jira/browse/LUCENE-7287
</comment><comment author="dchaplinsky" created="2016-07-13T23:49:08Z" id="232519377">Hello @clintongormley . Thanks to @arysin now we have proper support for Ukrainian language in Lucene itself.

What should be done to propagate it to elasticsearch?
</comment><comment author="clintongormley" created="2016-07-14T10:53:42Z" id="232631993">nice work @arysin 

@dchaplinsky we'll need to wait for Lucene 6.2 to be released.  I've opened this issue to track it: https://github.com/elastic/elasticsearch/issues/19433
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Record method counts while profiling query components</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18302</link><project id="" key="" /><description>Records the number of times each Lucene method is invoked.  These counts can be used to help judge the selectivity of individual query components in the context of the entire query.  

E.g. a query may not look selective when run by itself (matches most of the index), but when run in context of a full search request, is evaluated only rarely due to execution order.

It seemed like a relatively minor overhead to profiling, and could provide useful information in the future.

/cc @jpountz thoughts?
</description><key id="154549062">18302</key><summary>Record method counts while profiling query components</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-05-12T18:24:01Z</created><updated>2016-07-14T13:46:46Z</updated><resolved>2016-07-14T13:46:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-05-16T16:12:34Z" id="219468337">/cc @colings86 too, since you're doing profile stuff now as well.

~~Note: this needs to be updated because of #18370~~
</comment><comment author="colings86" created="2016-05-17T09:12:20Z" id="219662258">I like the idea. Having number of invocations on each timing type can also help see bottlenecks a bit clearer. For example, if you have two parts of your query that take 2 seconds on the profiler, but one has 1,000,000 invocations and the other has 4 then it is a good indication that the place to start looking is the query that is called 4 times
</comment><comment author="colings86" created="2016-07-04T09:59:36Z" id="230253430">@polyfractal are you still planning on progressing with this? it would be good to get this in IMO
</comment><comment author="polyfractal" created="2016-07-05T13:24:41Z" id="230476917">Em, yes, sorry.  Got lost in the wedding/travel frenzy.  Will update this today or tomorrow.
</comment><comment author="polyfractal" created="2016-07-12T19:28:33Z" id="232153270">Rebased, and updated the responses for the aggregation profiles since they include counts now too.

@colings86 mind taking a quick look at this?
</comment><comment author="colings86" created="2016-07-13T07:42:21Z" id="232280262">@polyfractal LGTM, think this change will be really useful
</comment><comment author="jpountz" created="2016-07-13T09:19:50Z" id="232302608">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test for fixed executor rejected count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18301</link><project id="" key="" /><description>This commit adds a test that a fixed executors rejected count behaves as
expected. In particular, we test that if we consume the executor, then
stuff the executor queue, further tasks will be rejected and the
rejected stats are updated appropriately. This test also asserts that if
we resize the queue the rejected count is reset to zero.
</description><key id="154546568">18301</key><summary>Add test for fixed executor rejected count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T18:12:19Z</created><updated>2016-05-13T15:27:20Z</updated><resolved>2016-05-13T15:27:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-12T18:13:04Z" id="218840373">&gt; This test also asserts that if we resize the queue the rejected count is reset to zero.

Whether or not this is the behavior that we ultimately want, it is the behavior that we have today. If we do want to change this, that can be addressed in a follow up but now we will have a test in place that will verify and maintain that behavior (or the zeroing behavior if that is what we want to keep).
</comment><comment author="jasontedor" created="2016-05-13T15:26:54Z" id="219076487">Thanks @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New Matrix Stats Aggregation module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18300</link><project id="" key="" /><description>This PR adds a new aggs-matrix-stats module. The module introduces a new class of aggregations called `Matrix Aggregations`. Matrix aggregations work on multiple fields and produce a matrix as output. The matrix aggregation provided by this module is the `matrix_stats` aggregation which computes the following descriptive statistics over a set of fields:
- Covariance
- Correlation

For completeness (and interpretation purposes) the following per-field statistics are also provided:
- sample count
- population mean
- population variance
- population skewness
- population kurtosis

Example request:

``` javascript
"aggs": {
    "matrixstats": {
        "matrix_stats": {
            "field": ["poverty", "income"]
        }
     }
}
```

Example response:

``` javascript
{
    "aggregations": {
        "matrixstats": {
            "fields": [{
                "name": "income",
                "count": 50,
                "mean": 51985.1,
                "variance": 7.383377037755103E7,
                "skewness": 0.5595114003506483,
                "kurtosis": 2.5692365287787124,
                "covariance": {
                    "income": 7.383377037755103E7,
                    "poverty": -21093.65836734694
                },
                "correlation": {
                    "income": 1.0,
                    "poverty": -0.8352655256272504
                }
            }, {
                "name": "poverty",
                "count": 50,
                "mean": 12.732000000000001,
                "variance": 8.637730612244896,
                "skewness": 0.4516049811903419,
                "kurtosis": 2.8615929677997767,
                "covariance": {
                    "income": -21093.65836734694,
                    "poverty": 8.637730612244896
                },
                "correlation": {
                    "income": -0.8352655256272504,
                    "poverty": 1.0
                }
            }]
        }
    }
}
```

This PR replaces #16826. The differences include:
- renames aggregation from `multifield_stats` to `matrix_stats`
- adds aggregation code as a module instead of directly to core
- refactors Integration Tests as REST Tests
</description><key id="154543965">18300</key><summary>New Matrix Stats Aggregation module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>feature</label><label>v5.0.0-alpha4</label></labels><created>2016-05-12T17:59:15Z</created><updated>2016-06-01T21:41:36Z</updated><resolved>2016-06-01T21:41:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-05-13T18:46:19Z" id="219127887">This PR has been updated to include MultiValuesSource classes (per comment in #18285)

@colings86, @jpountz could one (or both) of you have a look if you get a chance?
</comment><comment author="colings86" created="2016-05-16T09:17:47Z" id="219383340">@nknize I left some comments on the aggregation type stuff but I don't know enough about the actual statistics we are calculating here to review those bits. Maybe @brwe or @polyfractal would be able to review those bits?
</comment><comment author="polyfractal" created="2016-05-16T15:42:33Z" id="219459785">I tried, and largely failed, to grok the stats calculations.  My math skills are pretty rusty and the streaming formulas look different from what I'm used to.  I'd try to get a second opinion from @brwe :)
</comment><comment author="nknize" created="2016-05-23T19:00:03Z" id="221063343">Great suggestions! I incorporated all feedback and the PR is ready for another review. Here are the main changes:
- Added `MultiValueMode` support for handling multi value fields (it was easier adding it here instead of postponing to a later PR)
- Added a `MultiValuesSource` class to encapsulate field names and ValuesSource arrays, this is now used to retrieve document field values instead of creating a Map inside of `collect`
- Updated `doReduce` to merge all shard results in an empty `RunningStats` object.
- Added Rest testing for `MultiValueMode` support

/cc @colings86, @jpountz 
</comment><comment author="nknize" created="2016-05-24T14:55:33Z" id="221297709">Module documentation has been added and is ready for review /cc @clintongormley.  I think this feature is ready to go - pending a final review from @jpountz and/or @colings86. 
</comment><comment author="nknize" created="2016-05-24T14:59:31Z" id="221299022">@rashidkpc or @spalger, the response format changed from the old #16826 PR. This was done to make it easier to parse (specifically for Kibana). Does this format look good to y'all?
</comment><comment author="spalger" created="2016-05-24T17:20:56Z" id="221342630">Thanks @nknize 

I appreciate that, but if I was going to be using this response I would probably transform it into something like this before using it:

``` js
"aggregations": {
  "matrixstats": {
    "fields": [
      {
        "name": "income",
        "count": 50,
        "mean": 51985.1,
        "variance": 7.383377037755103E7,
        "skewness": 0.5595114003506483,
        "kurtosis": 2.5692365287787124,
        "covariance": {
          "poverty": -21093.65836734694
        },
        "correlation": {
          "poverty": -0.8352655256272504
        }
      },
      {
        "name": "poverty",
        "count": 50,
        "mean": 12.732000000000001,
        "variance": 8.637730612244896,
        "skewness": 0.4516049811903419,
        "kurtosis": 2.8615929677997767,
        "covariance": {
          "income": -21093.65836734694,
        },
        "correlation": {
          "income": -0.8352655256272504
        }
      }
    ]
  }
}
```
</comment><comment author="spalger" created="2016-05-24T17:23:38Z" id="221343388">Though honestly I'm not certain what all those values are... so guessing how I would use them it tricky. Either way, I don't see any point in optimizing this specific api response too much.

If response size is a concern then I think the current proposal makes sense, if not I would probably use a more hash-based style (rather than array's of values mapped to an array of keys by index)
</comment><comment author="spalger" created="2016-05-24T17:25:43Z" id="221343952">Something I said in #16826 may have lead you down this path?

&gt; We'll have to transform it from object to array, but as long as the fields aren't in a meaningful order this is fine.

What I meant here was simply making an array of the fields so they could more easily be iterated over, as in the example response I posted. The fact that the fields all had their values assigned as properties was preferred 
</comment><comment author="nknize" created="2016-05-24T18:06:32Z" id="221355231">Thanks @spalger!

&gt; Something I said in #16826 may have lead you down this path?

Ah. I think it was a combination of that and a separate discussion.

&gt; The fact that the fields all had their values assigned as properties was preferred

@clintongormley, thoughts? Looks like its more useful to cut back to the first response format? It will get quite large with a lot of fields (this is only 2) but parsing is easier.

&gt; guessing how I would use them it tricky

From a visualization perspective I envision a heatmap like in #16826 where you can visualize covariance or correlation. The rest of those values are just like `extended_stats`, likely displayed in a descriptive data table (or combined with a histogram).
</comment><comment author="georgebridgeman" created="2016-05-24T18:22:40Z" id="221359612">Hi all,

We visualise the correlation results using a [chord diagram](https://bost.ocks.org/mike/uberdata/), and also in a matrix (normally only showing the upper triangular). We also show ad-hoc correlation coefficients if the user or report is only interested in a few fields. Being able to key directly from one field to another to get the correlation coefficient like @spalger says saves having to figure array indexes.

I pulled @nknize 's branch a couple of weeks back (based on #16826) and the response format looked perfect. The array format is still workable but there's an extra step involved if picking out certain fields. 

Side note... I've been watching this PR for a while as it solves a major performance issue we've been having for a while. Currently, we effectively do what this aggregation does in code (using Apache Commons Math) after fetching all the documents. Having elasticsearch do this for us saves transporting the documents to the application server, making the visualisations much, much quicker. Thanks for all the work put into this PR!
</comment><comment author="nknize" created="2016-05-25T16:15:41Z" id="221627271">Thanks for the feedback @georgebridgeman! Wonderful to hear how this provides a performance boost for your use case. A chord diagram is another fantastic way to visualize the correlation/covariance results. We may want to consider adding it as a visualization option in kibana.

I'm going to go ahead and change the response format to look like the one posted by @spalger. It is quite a bit bigger but I think its more usable. If size becomes an issue we can consider alternatives in a future PR?
</comment><comment author="nknize" created="2016-05-25T19:14:55Z" id="221677482">Response format changed to the following:

``` json
{
    "aggregations": {
        "matrixstats": {
            "fields": [{
                "name": "income",
                "count": 50,
                "mean": 51985.1,
                "variance": 7.383377037755103E7,
                "skewness": 0.5595114003506483,
                "kurtosis": 2.5692365287787124,
                "covariance": {
                    "income": 7.383377037755103E7,
                    "poverty": -21093.65836734694
                },
                "correlation": {
                    "income": 1.0,
                    "poverty": -0.8352655256272504
                }
            }, {
                "name": "poverty",
                "count": 50,
                "mean": 12.732000000000001,
                "variance": 8.637730612244896,
                "skewness": 0.4516049811903419,
                "kurtosis": 2.8615929677997767,
                "covariance": {
                    "income": -21093.65836734694,
                    "poverty": 8.637730612244896
                },
                "correlation": {
                    "income": -0.8352655256272504,
                    "poverty": 1.0
                }
            }]
        }
    }
}
```
</comment><comment author="jpountz" created="2016-05-25T21:00:45Z" id="221706110">Even though I think it is more readable too, it might be more challenging to support scripts in the future since we would not have obvious keys. I don't think it is a blocker, this is experimental anyway so we'll be free to change the response format but I wanted to mention this potential annoyance with this format.
</comment><comment author="nknize" created="2016-05-26T21:43:37Z" id="222003735">Thanks for the latest round of review @jpountz. I went ahead and made the minor code changes, added support for missing values, and updated documentation. Let me know what you think.
</comment><comment author="jpountz" created="2016-05-31T09:12:00Z" id="222633839">I'd like @colings86 to have a final look, at least for the core refactorings that you did. But otherwise this looks good to me for a first iteration!
</comment><comment author="colings86" created="2016-05-31T09:35:33Z" id="222639347">@jpountz @nknize I took a look at the core changes and they LGTM. I also took a brief look at the module, there are unit and REST tests but we are missing integration tests for this. Is there a reason we have not done an integration tests for this aggregation? All other aggregations have integration tests.
</comment><comment author="nknize" created="2016-05-31T16:07:51Z" id="222737532">@colings86 I had asked @rjernst about integration testing in modules. I could be wrong but I think to avoid the overhead of starting/stopping integration test clusters in plugins and modules the `ESIntegTestCase` tests should be replaced by simple Unit and Rest tests? It was a bit tricky but what I did was refactor the integration tests (defined by `AbstractNumericTestCase`) from the original PR (#16826) to the yaml Rest definitions included in this PR. I also split out testing of `RunningStats` and `MatrixStatsResults` into a separate `RunningStatsTests` unit test class to ensure the RunningStats (and corresponding results) are computed as expected.
</comment><comment author="rjernst" created="2016-06-01T19:47:29Z" id="223103913">&gt; there are unit and REST tests but we are missing integration tests for this

REST tests are real integration tests. No need for fantasy land tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to delete snapshot &#8220;index and alias names need to be unique&#8221;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18299</link><project id="" key="" /><description>I already searched for a similar issue and posted about this in [here](https://discuss.elastic.co/t/unable-to-delete-snapshot-index-and-alias-names-need-to-be-unique/49676) and got no reply.
I looks like a bug so I'm opening this issue.

**Elasticsearch version**:
2.1.2

**JVM version**:
java version "1.8.0_72"
Java(TM) SE Runtime Environment (build 1.8.0_72-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.72-b15, mixed mode)

**OS version**:
Red Hat Enterprise Linux Server release 6.7 (Santiago)

**Description of the problem including expected versus actual behavior**:
I am unable to delete snapshots created in a previous version of **elasticsearch** containing indices and aliases with the same name.
I am currently using 2.1.2 and I have created the snapshots back in 1.7.2.

When I try to delete a snapshot using:

`curl -s -XDELETE localhost:9200/_snapshot/repoX/snapY?pretty`

I get the following error message:

```
{
  "error" : {
  "root_cause" : [ {
  "type" : "remote_transport_exception",
  "reason" : "[cluster:admin/snapshot/delete]"
  } ],
  "type" : "illegal_state_exception",
  "reason" : "index and alias names need to be unique, but alias [indexZ] and index [indexZ] have the same name"
  },
  "status" : 500
}
```

This seems to be due to the restriction that I think was introduced in 2.0.0 that index and alias names must be unique in a cluster.

I would expect deleting a snapshot to always work, despite it containing invalid indices and aliases names, since I am disposing of it.

This is preventing me from releasing valuable disk space in a production system.

**Steps to reproduce**:
1. Create an index and alias with the same name in a version of elasticsearch where this is allowed (e.g. 1.7.2).
2. Create a snapshot including that index and that alias.
3. Upgrade to a version of elasticsearch where index and alias names must be unique (e.g. 2.1.2), first deleting either the index or the alias to ensure uniqueness
4. Try to delete the snapshot created in the previous elasticsearch version.

**Provide logs (if relevant)**:

```
[2016-05-12 18:25:52,981][INFO ][rest.suppressed          ] /_snapshot/repoX/snapY Params: {pretty=, repository=repoX, snapshot=snapY}
RemoteTransportException[[****][****:9300][cluster:admin/snapshot/delete]]; nested: IllegalStateException[index and alias names need to be unique, but alias [indexZ] and index [indexZ] have the same name];
Caused by: java.lang.IllegalStateException: index and alias names need to be unique, but alias [indexZ] and index [indexZ] have the same name
        at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1037)
        at org.elasticsearch.repositories.blobstore.BlobStoreRepository.readSnapshotMetaData(BlobStoreRepository.java:490)
        at org.elasticsearch.repositories.blobstore.BlobStoreRepository.deleteSnapshot(BlobStoreRepository.java:310)
        at org.elasticsearch.snapshots.SnapshotsService$8.run(SnapshotsService.java:997)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="154540749">18299</key><summary>Unable to delete snapshot &#8220;index and alias names need to be unique&#8221;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdlourenco</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>discuss</label></labels><created>2016-05-12T17:43:17Z</created><updated>2016-10-25T01:09:14Z</updated><resolved>2016-05-13T11:42:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-13T11:42:46Z" id="219020816">Hi @jdlourenco 

Yes, it's a problem.  Fortunately, the answer is easy: set up a single node of 1.7.5, add the repository in question and delete the snapshot.
</comment><comment author="jdlourenco" created="2016-05-13T12:21:15Z" id="219027976">I understand your hack but why did you close the issue?
Shouldn't it be fixed?

I don't think having to go back to a previous version of elasticsearch a very neat solution.

Perhaps deleting snapshots should work even if they contain invalid data since it won't be restored.
</comment><comment author="clintongormley" created="2016-05-13T17:25:30Z" id="219106867">@jdlourenco the fact that an index and an alias could have the same name was a bug, which has been fixed.  unfortunately it resulted in you getting into a tricky position.  i don't think there is anything left to fix here, especially as there is a workaround (even though it may be hacky) for the situation you found yourself in.
</comment><comment author="baileyspace" created="2016-10-25T01:09:14Z" id="255908961">I'm getting a similar error when I try to start elasticsearch:
`java.lang.IllegalStateException: index and alias names need to be unique, but alias [people] and index [people] have the same name`
 I tried  `curl -XDELETE 'http://localhost:9200/_all` but I'm still getting the same error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Line Number Available in Painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18298</link><project id="" key="" /><description>Added a line number to Painless nodes for debugging purposes.
</description><key id="154539082">18298</key><summary>Make Line Number Available in Painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T17:34:43Z</created><updated>2016-05-13T11:21:39Z</updated><resolved>2016-05-12T17:41:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-12T17:37:50Z" id="218830503">+1! This should make it easy to now add it to the bytecode
</comment><comment author="jdconrad" created="2016-05-12T17:41:13Z" id="218831415">Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> How to enable script.groovy on es 5.0.0 alpha2 for ubuntu Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18297</link><project id="" key="" /><description>i added lines to config/elasticsearch.yml

```
script.groovy: true
script.inline: true
script.indexed: true
```

restated and got this

```
 * Stopping Elasticsearch Server                                                                                               [ OK ] 
 * Starting Elasticsearch Server                                                                                               [fail]
```

i checked log file, it is empty, no errors

can you help me with it

https://discuss.elastic.co/t/how-to-enable-script-groovy-on-es-5-0-0-alpha2-for-ubuntu/49846
https://discuss.elastic.co/t/groovy/49836
</description><key id="154533435">18297</key><summary> How to enable script.groovy on es 5.0.0 alpha2 for ubuntu Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maksymseleznov</reporter><labels /><created>2016-05-12T17:05:48Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-05-12T17:12:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-12T17:12:47Z" id="218823559">Someone from the community should be able to answer your question on Discuss, but Elastic reserves GitHub for verified bug reports and feature requests.
</comment><comment author="maksymseleznov" created="2016-05-12T17:16:36Z" id="218824629">but it looks like bug, why log file without error, where i can see why elasticsearch didnt start? @jasontedor 
</comment><comment author="jasontedor" created="2016-05-12T20:14:15Z" id="218872256">There's no bug here. You're not setting the right settings, and it should be telling you that in the logs:

``` bash
16:09:11 [jason:~/src/elastic/elasticsearch] master+ 1 &#177; cat &gt; ~/elasticsearch/elasticsearch-5.0.0-alpha2/config/elasticsearch.yml 
script.groovy: true
script.inline: true
script.indexed: true
16:09:41 [jason:~/src/elastic/elasticsearch] master+ &#177; ~/elasticsearch/elasticsearch-5.0.0-alpha2/bin/elasticsearch
[2016-05-12 16:09:47,002][INFO ][node                     ] [Lady Lotus] version[5.0.0-alpha2], pid[53827], build[e3126df/2016-04-26T12:08:58.960Z]
[2016-05-12 16:09:47,003][INFO ][node                     ] [Lady Lotus] initializing ...
[2016-05-12 16:09:47,269][INFO ][plugins                  ] [Lady Lotus] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-05-12 16:09:47,289][INFO ][env                      ] [Lady Lotus] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [114.3gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-05-12 16:09:47,289][INFO ][env                      ] [Lady Lotus] heap size [989.8mb], compressed ordinary object pointers [true]
Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [script.groovy]
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:267)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
        at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:120)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:235)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
```

But if I remove the offending settings:

``` bash
16:10:39 [jason:~/src/elastic/elasticsearch] master+ 130 &#177; cat &gt; ~/elasticsearch/elasticsearch-5.0.0-alpha2/config/elasticsearch.yml 
script.inline: true
script.stored: true   
16:10:48 [jason:~/src/elastic/elasticsearch] master+ &#177; ~/elasticsearch/elasticsearch-5.0.0-alpha2/bin/elasticsearch
[2016-05-12 16:10:52,773][INFO ][node                     ] [Ruby Thursday] version[5.0.0-alpha2], pid[54205], build[e3126df/2016-04-26T12:08:58.960Z]
[2016-05-12 16:10:52,773][INFO ][node                     ] [Ruby Thursday] initializing ...
[2016-05-12 16:10:53,044][INFO ][plugins                  ] [Ruby Thursday] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-05-12 16:10:53,064][INFO ][env                      ] [Ruby Thursday] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [114.3gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-05-12 16:10:53,064][INFO ][env                      ] [Ruby Thursday] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-05-12 16:10:54,430][INFO ][node                     ] [Ruby Thursday] initialized
[2016-05-12 16:10:54,430][INFO ][node                     ] [Ruby Thursday] starting ...
[2016-05-12 16:10:54,506][INFO ][transport                ] [Ruby Thursday] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2016-05-12 16:10:54,510][WARN ][bootstrap                ] [Ruby Thursday] bootstrap checks failed
[2016-05-12 16:10:54,510][WARN ][bootstrap                ] [Ruby Thursday] initial heap size [268435456] not equal to maximum heap size [1073741824]; this can cause resize pauses and prevents mlockall from locking the entire heap
[2016-05-12 16:10:54,510][WARN ][bootstrap                ] [Ruby Thursday] please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
[2016-05-12 16:10:57,550][INFO ][cluster.service          ] [Ruby Thursday] new_master {Ruby Thursday}{IEy7H74WSYqzukx8oEZaag}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-05-12 16:10:57,569][INFO ][http                     ] [Ruby Thursday] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2016-05-12 16:10:57,569][INFO ][node                     ] [Ruby Thursday] started
[2016-05-12 16:10:57,574][INFO ][gateway                  ] [Ruby Thursday] recovered [0] indices into cluster_state
```

And I can use Groovy against it:

``` bash
16:12:18 [jason:~] $ curl -XPOST localhost:9200/test/doc/1?pretty=true -d '{"foo": "quick brown fox jumped over the lazy dog", "bar": 1}'
{
  "_index" : "test",
  "_type" : "doc",
  "_id" : "1",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}
16:12:25 [jason:~] $ curl -XPOST localhost:9200/test/doc/2?pretty=true -d '{"foo": "fast jumping spiders", "bar": 2}'
{
  "_index" : "test",
  "_type" : "doc",
  "_id" : "2",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}
16:12:45 [jason:~] $ curl -XPOST localhost:9200/test/doc/3?pretty=true -d '{"foo": "dog spiders that can eat a dog", "bar": 3}'
{
  "_index" : "test",
  "_type" : "doc",
  "_id" : "3",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}
16:13:04 [jason:~] $ curl -XPOST localhost:9200/test/_search?pretty=true -d '
{ "query": {
    "function_score": {
      "query": {
        "match": {
          "foo": "dog"
        }
      },
      "script_score": {
        "script": "_score &gt; 0.0 ? _score : 0"
      }
    }
  }
}'
{
  "took" : 10,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.53033006,
    "hits" : [ {
      "_index" : "test",
      "_type" : "doc",
      "_id" : "3",
      "_score" : 0.53033006,
      "_source" : {
        "foo" : "dog spiders that can eat a dog",
        "bar" : 3
      }
    }, {
      "_index" : "test",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 0.3125,
      "_source" : {
        "foo" : "quick brown fox jumped over the lazy dog",
        "bar" : 1
      }
    } ]
  }
}
```
</comment><comment author="maksymseleznov" created="2016-05-12T21:04:24Z" id="218885425">i found problem

i added on the end of file two this lines

```
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
[__]cluster.name: tapt
#
# ------------------------------------ Other ------------------------------------

script.inline: true
script.stored: true
```

[__]cluster.name: tapt has two spaces
script.inline: true hasnt space

when i added two spaces for script.inline: true and script.stored: true

```


# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
[__]cluster.name: tapt
#
# ------------------------------------ Other ------------------------------------

[__]script.inline: true
[__]script.stored: true
```

`[__] - two spaces`

elasticsearch started work, thanks for help
</comment><comment author="jasontedor" created="2016-05-12T21:08:54Z" id="218886525">Your logs should still be telling you _exactly_ where the problem is:

``` bash
17:07:19 [jason:~/src/elastic/elasticsearch] master+ 130 &#177; cat &gt; ~/elasticsearch/elasticsearch-5.0.0-alpha2/config/elasticsearch.yml 
  cluster.name: foo
 node.name: bar 
17:07:40 [jason:~/src/elastic/elasticsearch] master+ &#177; ~/elasticsearch/elasticsearch-5.0.0-alpha2/bin/elasticsearch
Exception in thread "main" SettingsException[Failed to load settings from [elasticsearch.yml]]; nested: ElasticsearchParseException[malformed, expected end of settings but encountered additional content starting at line number: [2], column number: [2]]; nested: ParserException[expected '&lt;document start&gt;', but found BlockMappingStart
 in 'reader', line 2, column 2:
     node.name: bar
     ^
];
Likely root cause: expected '&lt;document start&gt;', but found BlockMappingStart
 in 'reader', line 2, column 2:
     node.name: bar
     ^

        at com.fasterxml.jackson.dataformat.yaml.snakeyaml.parser.ParserImpl$ParseDocumentStart.produce(ParserImpl.java:225)
        at com.fasterxml.jackson.dataformat.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)
        at com.fasterxml.jackson.dataformat.yaml.snakeyaml.parser.ParserImpl.getEvent(ParserImpl.java:168)
        at com.fasterxml.jackson.dataformat.yaml.YAMLParser.nextToken(YAMLParser.java:346)
        at org.elasticsearch.common.xcontent.json.JsonXContentParser.nextToken(JsonXContentParser.java:52)
        at org.elasticsearch.common.settings.loader.XContentSettingsLoader.load(XContentSettingsLoader.java:77)
        at org.elasticsearch.common.settings.loader.XContentSettingsLoader.load(XContentSettingsLoader.java:50)
        at org.elasticsearch.common.settings.loader.YamlSettingsLoader.load(YamlSettingsLoader.java:50)
        at org.elasticsearch.common.settings.Settings$Builder.loadFromStream(Settings.java:948)
        at org.elasticsearch.common.settings.Settings$Builder.loadFromPath(Settings.java:935)
        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:94)
        at org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.java:202)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:240)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
```
</comment><comment author="maksymseleznov" created="2016-05-12T21:48:31Z" id="218895963">sorry but it was really empty

Linux ip-10-0-2-253 3.13.0-86-generic #130-Ubuntu SMP Mon Apr 18 18:27:15 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

```

apt-get update &amp;&amp; apt-get upgrade --quiet --assume-yes  &amp;&amp; apt-get dist-upgrade --quiet --assume-yes &amp;&amp; apt-get autoremove --quiet --assume-yes &amp;&amp; apt-get autoclean

sudo add-apt-repository ppa:webupd8team/java

sudo apt-get update

sudo apt-get install oracle-java8-installer

java -version

wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -

echo "deb http://packages.elastic.co/elasticsearch/5.x/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-5.x.list

sudo apt-get update &amp;&amp; sudo apt-get install elasticsearch

sudo update-rc.d elasticsearch defaults 95 10

service elasticsearch restart  

# uncomment couple lines on /etc/elasticsearch/elasticsearch.yml  replace [#] to [space]

echo "script.inline: true" &gt;&gt; /etc/elasticsearch/elasticsearch.yml
echo "script.stored: true" &gt;&gt; /etc/elasticsearch/elasticsearch.yml

service elasticsearch restart  
```

and small fix on jvm.options from  -Xms256m  to  -Xms1g
</comment><comment author="jasontedor" created="2016-05-12T22:11:15Z" id="218900844">What are you showing here? Those settings are not offending, and anyway the logs do not dump to the console when you start the service. You have to check the actual log files.
</comment><comment author="maksymseleznov" created="2016-05-12T22:15:28Z" id="218901634">i showed what i did before checked log file, ok np, now it works, thanks
</comment><comment author="jasontedor" created="2016-05-12T22:16:55Z" id="218901940">&gt; i showed what i did before checked log file, ok np, now it works, thanks

You're welcome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace [source,json] with [source,js] in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18296</link><project id="" key="" /><description>The syntax highlighter only supports [source,js].

Also adds a check to the rest test generator that runs during
the build that'll fail the build if it sees `[source,json]`.
</description><key id="154529235">18296</key><summary>Replace [source,json] with [source,js] in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T16:44:42Z</created><updated>2016-05-24T15:22:01Z</updated><resolved>2016-05-24T15:21:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-12T16:45:00Z" id="218815962">@clintongormley this is will fail the build for `[source,json]`.
</comment><comment author="nik9000" created="2016-05-12T16:45:32Z" id="218816107">It shouldn't conflict with #18211 either....
</comment><comment author="nik9000" created="2016-05-23T18:53:23Z" id="221061543">@clintongormley can you have a look at this when you get a chance?
</comment><comment author="clintongormley" created="2016-05-24T09:14:06Z" id="221211561">LGTM
</comment><comment author="nik9000" created="2016-05-24T15:22:01Z" id="221306268">Thanks for looking @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add wait_for_health=yellow to reindex snippets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18295</link><project id="" key="" /><description>This should help the tests pass more consistently. Should.

This also removes from `?pretty` from the docs. It isn't a thing
with `// CONSOLE`.

Relates to
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=oraclelinux/399/console
</description><key id="154524599">18295</key><summary>Add wait_for_health=yellow to reindex snippets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>jenkins</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T16:22:37Z</created><updated>2016-05-12T16:33:30Z</updated><resolved>2016-05-12T16:33:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-12T16:22:57Z" id="218809973">@ywelsch, this is my reaction to the failed build you sent me.
</comment><comment author="ywelsch" created="2016-05-12T16:27:59Z" id="218811372">LGTM (in the future, we might not need these `wait_for_health=yellow`, see #9126)
</comment><comment author="nik9000" created="2016-05-12T16:32:18Z" id="218812498">Oh cool! I'm glad we are working on that!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster will not automatically assign shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18294</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.1

**JVM version**:  java version "1.8.0_31"
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)

**OS version**: CentOS release 6.7 (Final)

**Description of the problem including expected versus actual behavior**:

Thanks in advance for your help.

I inherited a poorly-behaving Elasticsearch cluster that I've been tuning on and off for a few months. Some things have improved, others have not.

About a month ago, another team needed to reboot the (AWS hosted) nodes. I turned off shard allocation in the cluster with an API call and got a success response. However, it didn't take. So I also turned off allocation in kopf. But, rebooting a node caused shards to be allocated anyway. So we rebooted one box per day and let the cluster rebalance each time.

Some time after the reboots, I noticed the cluster was still yellow. It had stopped allocating shards. I figured the API call had finally taken, so I turned allocation back on (and got another success message). According to kopf, allocation is enabled. But the cluster did not automatically assign shards.

I tried toggling shard allocation on and off with both kopf and the API. But the cluster would not allocate shards.

I complained about this on Twitter and an Elastic employee recommended upgrading. So, I upgraded the cluster from 1.5.2 to 2.2.1. However, the behavior persists.

If I assign shards manually with the API, they allocate just fine.

**Steps to reproduce**:
1. Turn off shard allocation with API call and kopf
2. Turn on shard allocation with API call and kopf
3. Cry a lot

**Provide logs (if relevant)**: I didn't see any relevant logs, but if you tell me what to look for I can grep. I'm happy to gather diagnostics for you, but please note that I no longer own this cluster as of 3pm PDT Friday.
</description><key id="154523629">18294</key><summary>Cluster will not automatically assign shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alicegoldfuss</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2016-05-12T16:18:02Z</created><updated>2016-05-13T16:01:04Z</updated><resolved>2016-05-12T23:23:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-12T16:34:58Z" id="218813224">Hi @alicegoldfuss 

Could you provide the actual command that you use to enable/disable shard allocation, along with the output of:

```
GET _cluster/settings
```

and (assuming you have unassigned shards atm):

```
GET _shard_stores
```

Also, would be good to take an unassigned shard and try to assign it to a particular node with the [cluster-reroute API](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/cluster-reroute.html), adding the `?explain` flag to figure out why the shard isn't being assigned.

thanks
</comment><comment author="alicegoldfuss" created="2016-05-12T17:41:27Z" id="218831464">Hi Clinton!

```
$ curl -XPUT http://node-1.com:9200/_cluster/settings -d '{"transient":{"cluster.routing.allocation.enable": "all"}}'
{"acknowledged":true,"persistent":{},"transient":{"cluster":{"routing":{"allocation":{"enable":"all"}}}}}
```

```
$ curl -XGET http://node-1.com:9200/_cluster/settings
{"persistent":{"cluster":{"routing":{"allocation":{"enable":"all"}}}},"transient":{"cluster":{"routing":{"allocation":{"enable":"all"}}}}}
```

Shard stores returned a lot, so I cleaned up one of the replies:

```
{"indices":{"index-1":{"shards":{"2":{"stores":[{"urZEQshBQourcmigNqeG0g":{"name":"node-1","transport_address":"127.0.0.1:9300","attributes":{"ec2az":"us-east-1a","datacenter":"use1v"}},"version":40,"allocation":"primary"},{"uWmawPeUQ2KYNR1sLKvxfA":{"name":"node-2","transport_address":"127.0.0.1:9300","attributes":{"ec2az":"us-east-1d","datacenter":"use1v"}},"version":25,"allocation":"unused"}]}}},
```

The explain query dumped a bunch of stuff. Can I give it to you privately?
</comment><comment author="alicegoldfuss" created="2016-05-12T17:46:37Z" id="218832859">Also I should add that assigning the shard via the API worked. That has always worked. But the cluster doesn't assign automatically.
</comment><comment author="clintongormley" created="2016-05-12T18:14:11Z" id="218840646">&gt; The explain query dumped a bunch of stuff. Can I give it to you privately?

Sure. clinton at elastic dot co

&gt; Also I should add that assigning the shard via the API worked. That has always worked. But the cluster doesn't assign automatically.

Do you have ongoing recoveries? What is the output of:

```
GET _cat/recovery?v
```

I also see you're using node attributes, and possibly multiple data centres?  (usually this is a no-no, unless they're like ec2 availability zones, ie as fast as a lan). Could you paste the settings you're using for shard awareness/allocation?
</comment><comment author="alicegoldfuss" created="2016-05-12T18:19:33Z" id="218842094">I will send you the query dump and recovery dump.

We are using multiple AWS availability zones, not physical data centers.
</comment><comment author="clintongormley" created="2016-05-12T18:57:19Z" id="218852762">@alicegoldfuss It looks like you tried to reroute an already assigned shard.  Could you try one that is unassigned, eg shard 1 of index `logstash-2016.02.13`?  (with `?explain`)
</comment><comment author="clintongormley" created="2016-05-12T18:59:08Z" id="218853247">Also, could you paste the settings you're using for shard awareness/allocation?
</comment><comment author="alicegoldfuss" created="2016-05-12T19:04:47Z" id="218854745">@clintongormley I assigned a shard that was unassigned. But when I reran the command to create that file, yes it was already assigned.

I will send you the new shard assignment.

Can you tell me where to find the settings for shard awareness/allocation?
</comment><comment author="clintongormley" created="2016-05-12T19:06:34Z" id="218855201">@alicegoldfuss Either in index settings or in node settings, depending on what is set.  Try:

```
GET _nodes/settings
GET logstash-2016.02.13/_settings
```
</comment><comment author="alicegoldfuss" created="2016-05-12T19:11:16Z" id="218856432">Okay, emailed those settings to you.
</comment><comment author="s1monw" created="2016-05-12T19:26:43Z" id="218860346">@alicegoldfuss apparently those shards can be allocated so if you just call `_reroute` without any body or arguments (now that I think about it you might need to specify an empty body) will shard get allocated? if you allocate one shard manually will others follow once that shards is started?
</comment><comment author="alicegoldfuss" created="2016-05-12T20:10:44Z" id="218871332">Hello!

If I manually allocate one shard, the other shards will not follow suit.

I ran the following:

`curl -XPOST 'http://node-1.com:9200/_cluster/reroute'`

That returned a large response, full of shard states:

```
{"state":"STARTED","primary":false,"node":"XXXXXXXXXXX","relocating_node":null,"shard":4,"index":"index-1","version":24,"allocation_id":{"id":"XXXXXXXXXXXX"}}
```

But no signs of actual allocation.

I ran it with an empty body and got the same.
</comment><comment author="s1monw" created="2016-05-12T21:07:57Z" id="218886290">well I think from here on I don't have many more ideas than enabling trace logging for allocation for `cluster.routing.allocation.decider` as well as `cluster.routing.allocation` @bleskes any ideas?
</comment><comment author="alicegoldfuss" created="2016-05-12T21:15:28Z" id="218888133">That would require either putting in a Puppet PR (I don't think I could deploy in time) or disabling Puppet and shoving the settings into the config on the boxes (not my favorite choice). So if there's a Plan B I would prefer it.
</comment><comment author="jasontedor" created="2016-05-12T21:19:38Z" id="218889146">&gt;  So if there's a Plan B I would prefer it.

You can do it from the settings API:

```
PUT /_cluster/settings
{
    "transient" : {
        "logger.cluster.routing.allocation.decider": "TRACE",
        "logger.cluster.routing.allocation": "TRACE"
    }
}
```
</comment><comment author="alicegoldfuss" created="2016-05-12T21:27:02Z" id="218891023">Thanks! So, put those in place and try to allocate a shard with explain enabled? Or will those settings generate log files?
</comment><comment author="jasontedor" created="2016-05-12T21:28:59Z" id="218891492">&gt; Thanks! So, put those in place and try to allocate a shard with explain enabled? Or will those settings generate log files?

It will probably start generating log lines, but try to do an allocation too and share the master logs from the time you enabled the traces.
</comment><comment author="alicegoldfuss" created="2016-05-12T21:49:48Z" id="218896289">```
$ curl -XPUT 'http://node-1.com:9200/_cluster/settings' -d '{"transient" : {"logger.cluster.routing.allocation.decider": "TRACE","logger.cluster.routing.allocation": "TRACE"}}'
{"acknowledged":true,"persistent":{},"transient":{"logger":{"cluster":{"routing":{"allocation":"TRACE","allocation.decider":"TRACE"}}}}}
```

I successfully allocated two shards with the API. Elasticsearch generated no additional logs. I captured the explain dumps if you would like to see them.
</comment><comment author="jasontedor" created="2016-05-12T21:57:12Z" id="218897958">&gt; Elasticsearch generated no additional logs.

Are you sure that you extracted the logs from the master node? You can check the master with `GET /_cat/master` against any node in the cluster.
</comment><comment author="alicegoldfuss" created="2016-05-12T22:16:05Z" id="218901780">Okay! I thought ES logs would be the same across the cluster. I have a 37 MB log file for you @jasontedor where can I send it?
</comment><comment author="jasontedor" created="2016-05-12T22:21:26Z" id="218902814">&gt; Okay! I thought ES logs would be the same across the cluster.

Each node is a special snowflake, and the master is a very special snowflake and in particular is the only node making allocation decisions.

&gt; I have a 37 MB log file for you @jasontedor where can I send it?

Can you gzip compress that (if not already) and send it to my first name at the same domain as @clintongormley's email address from earlier?
</comment><comment author="alicegoldfuss" created="2016-05-12T22:32:18Z" id="218904807">Done :)
</comment><comment author="jasontedor" created="2016-05-12T22:52:33Z" id="218908315">@alicegoldfuss Can you hit `GET /_cluster/settings` against the master node and share here?
</comment><comment author="dakrone" created="2016-05-12T22:54:57Z" id="218908741">Do you have the timestamps from when you performed the allocation? I see things like:

```
[2016-05-12 21:47:20,469][TRACE][cluster.routing.allocation.decider] [...] Can not allocate [...], at[2016-05-12T05:48:08.074Z], details[failed recovery, failure RecoveryFailedException[index: Recovery failed from {node}{...}{...}{...}{...} into {...}{...}{...}{...}{...} (no activity after [30m])]; nested: ElasticsearchTimeoutException[no activity after [30m]]; ]]] on node [.....] due to [DisableAllocationDecider]
```

Especially the `DisableAllocationDecider` part, it looks like allocation is still disabled at this point, has it been turned back on? I noticed in your command for enabling allocation that you have enabled the `EnableAllocationDecider`, but perhaps the `DisableAllocationDecider` (which is [deprecated in 2.x](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java#L51-L64) ) is still disabling allocation?

In 2.x, both are around since DisableAllocationDecider is deprecated so people can move away from it, however, they both have effect, so if allocation is disabled by the DisableAllocationDecider then it will still be disabled.
</comment><comment author="dakrone" created="2016-05-12T22:56:24Z" id="218908994">Additionally, I _only_ see logs from the `cluster.routing.allocation.decider` and `cluster.routing.allocation.allocator` packages in this file, what happened to the logs from the other packages?
</comment><comment author="alicegoldfuss" created="2016-05-12T23:08:03Z" id="218910982">Ran settings against the master node:

```
$ curl -XGET http://node-master.com:9200/_cluster/settings
{"persistent":{"cluster":{"routing":{"allocation":{"enable":"all"}}}},"transient":{"cluster":{"routing":{"allocation":{"enable":"all"}}},"logger":{"cluster":{"routing":{"allocation":"TRACE","allocation.decider":"TRACE"}}}}}
```

I don't have specific timestamps, but I did give you the entire log file. Everything you see in that file is what I can see on the master. The other nodes are full of the 30 minute timeout notifications from when I was allocating hundreds of shards yesterday.
</comment><comment author="alicegoldfuss" created="2016-05-12T23:10:56Z" id="218911476">How can I toggle the `DisableAllocatiorDecider`? Give me a command and I'll try running it.

I know this is probably in your docs somewhere, but I'm pretty swamped, so sorry in advance.
</comment><comment author="dakrone" created="2016-05-12T23:13:59Z" id="218911949">Okay, it's still possible that the indices have the disable allocation decider enabled on them as I don't have the settings you emailed Clint earlier (he's offline), you can try:

```
curl -XPUT 'node:9200/_cluster/settings' -d'{"transient": {"cluster.routing.allocation.disable_allocation": false}}'
```

And additionally (in case it is set on the indices themselves):

```
curl -XPUT 'node:9200/_all/_settings' -d'{"index.routing.allocation.disable_allocation": false}'
```
</comment><comment author="alicegoldfuss" created="2016-05-12T23:22:12Z" id="218913330">YOU DID IT!

It was set on the indices themselves! I issued the second command you provided and BAM shards are initializing automatically!

No idea how that thing got toggled in the first place, but I will never touch it again.

Thank you everyone!

![](https://media.giphy.com/media/iPTTjEt19igne/giphy.gif)
</comment><comment author="dakrone" created="2016-05-12T23:23:34Z" id="218913588">Glad to hear that!

I'm going to close this issue, thanks for debugging with us :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decreasing delayed allocation timeout while shard fetching can lead to longer delay</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18293</link><project id="" key="" /><description>Test failure:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/518/testReport/junit/org.elasticsearch.cluster.routing/DelayedAllocationIT/testDelayedAllocationChangeWithSettingTo100ms/

Scenario that explains situation:
- Due to node failing, shard allocation is delayed for one minute
- 30 seconds later, user updates delayed shard allocation to 40 second for this index -&gt; This should allocate the shard in 10 seconds, BUT:
- While the reroute step is called during the update of the settings, shard fetching is still happening (or any other kind of reason that makes ReplicaShardAllocator call removeAndIgnore)
- This means that the setting is updated, but routing table is not (as we only update routing table if shard is delayed (which is not in this case, as the shard-fetching check comes first).
- The delay in the UnassignedInfo is still marked as 1 minute.
- RoutingService.clusterChanged checks findSmallestDelayedAllocationSettingNanos which returns 40 seconds. As this is smaller than the previous setting (which was 1 minute), it cancels existing delayed reroute, and schedules a new one (it sets minDelaySettingAtLastSchedulingNanos to 40 seconds). To determine the delay it looks at the delay stored in the shards, and only finds 1 minute delays (as UnassignedInfo was not updated), so it schedules next reroute in 1 minute (this means that the original delay is even extended by 30 seconds).
- Shard fetching is finished (2 seconds later), and a reroute is done. Here we update the delay in UnassignedInfo to 8 seconds. The routing table is now correctly updated, BUT RoutingService does not react properly to it. It compares minDelaySettingAtLastSchedulingNanos (previously set to 40 seconds with current value of findSmallestDelayedAllocationSettingNanos which still returns 40 seconds). As such it will not reschedule.
- This means that the shard will only be reallocated one minute and a half after node crashed unless the user updates delayed shard allocation for the index to a shorter time.
</description><key id="154515821">18293</key><summary>Decreasing delayed allocation timeout while shard fetching can lead to longer delay</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label></labels><created>2016-05-12T15:44:04Z</created><updated>2016-05-26T11:40:11Z</updated><resolved>2016-05-26T11:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add tests that packages depend on bash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18292</link><project id="" key="" /><description>This commit adds bats tests that the RPM and Debian packages depend on
bash.

Relates #18259
</description><key id="154473493">18292</key><summary>Add tests that packages depend on bash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T12:45:21Z</created><updated>2016-05-12T13:04:40Z</updated><resolved>2016-05-12T13:04:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-12T13:02:53Z" id="218750117">LGTM
</comment><comment author="jasontedor" created="2016-05-12T13:04:40Z" id="218750530">Thanks @dakrone!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sense UI - Playbutton can move whole frames</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18291</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**OS version**: 
Win7

**Description of the problem including expected versus actual behavior**:
Sense UI is no more fixed inside its browser window.

**Steps to reproduce**:
1. Write some requests into request frame, that you'll get the frames scoll bar.
2. Select the request on the bottom
3. Scroll up
4. The Play-Button will shift your whole screen up
5. Below the request and response frames you'll have some empty useless space

Testet in Mozilla Firefox as well Google Chrome

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

![bug_sense](https://cloud.githubusercontent.com/assets/19325590/15213272/8dc7d67e-1846-11e6-8502-7232802f6167.JPG)
</description><key id="154461297">18291</key><summary>Sense UI - Playbutton can move whole frames</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Norlo</reporter><labels /><created>2016-05-12T11:36:51Z</created><updated>2016-05-12T11:39:57Z</updated><resolved>2016-05-12T11:39:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-12T11:39:13Z" id="218732637">There is a [dedicated repository](https://github.com/elastic/sense) for Sense.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Combining a score plugin with a filter results in empty documents in the java plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18290</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5

**JVM version**:
java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)

**OS version**: OS X 10.11.4

**Description of the problem including expected versus actual behavior**:
When combining our score plugin with a filter we have empty documents in the java plugin.

```
  "query": {
        "function_score": {
            "functions": [{
                "script_score": {
                    "script": "native-score",
                    "lang": "native",
                    "params": {}
                }
            }],
            "query": {
                "bool": {
                    "must_not": [{
                        "term": {
                            "field1": "A"
                        }
                    }, {
                        "term": {
                            "field1": "B"
                        }
                    }]
                }
            },
            "boost_mode": "replace",
            "min_score": 0
        }
    }
```

And then in the Script

```
  @Override
  public double runAsDouble() {
    if (source().size() == 0) {
      logger.warn("empty document"); 
    }
}
```

Then in the log files we have this

```
...
[2016-05-12 11:33:55,633][WARN ][com.enrise...Script$Factory] [Yuriko Oyama] empty document
```

This message disappears when we remove the filter. It seems that instead of filtering the documents they are somehow emptied and then passed to the script

**Steps to reproduce**:
1. create a simple plugin
2. add some data
3. filter part of the data
4. see the empty documents
</description><key id="154441049">18290</key><summary>Combining a score plugin with a filter results in empty documents in the java plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roelandvanbatenburg</reporter><labels /><created>2016-05-12T09:48:28Z</created><updated>2016-05-12T11:17:46Z</updated><resolved>2016-05-12T11:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-12T11:13:24Z" id="218727935">You are replacing (`boost_mode: replace`) the score from the query with nothing, which is returned by your script.  Also, accessing the source field in a query will result in horrendous performance.  Don't do that.
</comment><comment author="roelandvanbatenburg" created="2016-05-12T11:17:46Z" id="218728799">I know source() is bad, but this is just to show that the doc is empty.
The runAsDouble actually returns a number in our code, but for these cases it can not determine the score as it has no fields to work with.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed to upgrade from 1.7.4 to 2.3.2, initializing primary shards stuck in TRANSLOG stage.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18289</link><project id="" key="" /><description>when I restarted cluster, some of the primaries are properly restored, but some recovery task tasks hang

they are in the 'TRANSLOG' stage and are staying there for more than an hour, which is not expected since I run _flush before restart meaning there should be no remaining translog.

hot threads api shows below

```
0.0% (97.7micros out of 500ms) cpu usage by thread 'elasticsearch[IP][transport_client_timer][T#1]{Hashed wheel timer #1}'
 10/10 snapshots sharing following 5 elements
   java.lang.Thread.sleep(Native Method)
   org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
   org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
   org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
```

while /_cat/recovery shows a lot of lines just like below

```
INDEX_NAME 9 1688129 store translog 10.96.250.224 10.96.250.224 n/a n/a 0 100.0% 0 100.0% 111 5087612156 0 -1.0% -1
```

some of the relevant error logs in data nodes are as below.

``````
[2016-05-12 11:54:30,666][WARN ][cluster.service          ] [IP] failed to connect to node [{IP}{P9z8Kl0SSV6d8VaZWViNLg}{IP}{IP:9300}{box_type=hdd, river=_none, master=false, node_id=IP}]
ConnectTransportException[[IP][IP:9300] connect_timeout[30s]]; nested: SocketException[&#50672;&#44208;&#51060; &#65533;~A&#65533;~@&#54200;&#65533;~W~P &#65533;~X&#54644; &#65533;~J&#50612;&#51664;];
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:940)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:855)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:828)
        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:243)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:474)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:225)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:188)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: &#50672;&#44208;&#51060; &#65533;~A&#65533;~@&#54200;&#65533;~W~P &#65533;~X&#54644; &#65533;~J&#50612;&#51664;
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        ... 3 more
[2016-05-12 11:59:37,265][INFO ][indices.breaker          ] [IP] Updated breaker settings fielddata: [fielddata,type=MEMORY,limit=644245094/614.3mb,overhead=1.03]
[2016-05-12 11:59:37,266][INFO ][cluster.routing.allocation.decider] [IP] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
[2016-05-12 12:00:01,034][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
        at java.lang.Integer.valueOf(Integer.java:832)
        at sun.nio.ch.EPollSelectorImpl.updateSelectedKeys(EPollSelectorImpl.java:106)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:84)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)```
``````

```
[2016-05-12 12:00:54,273][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
```

```
2016-05-12 12:00:54,940][DEBUG][action.admin.indices.stats] [IP] [indices:monitor/stats] failed to execute operation for shard [[log-2016-04-13][3], node[kji2YtUgQ_O6MCh40Tb26A], [P], v[15], s[INITIALIZING], a[id=ZggyMnC7Sfm3ZHhYv2YxUg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-05-12T02:59:33.040Z]]]
[log-2016-04-13][[log-2016-04-13][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:399)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:376)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:365)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [log-2016-04-13][[log-2016-04-13][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:957)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:791)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:612)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:395)
        ... 7 more
```

I tried again to upgrade from 1.7.4 to 2.0.0, just out of curiosity, but similar problem occurred. Do you have any idea on why this is happening?
</description><key id="154401225">18289</key><summary>failed to upgrade from 1.7.4 to 2.3.2, initializing primary shards stuck in TRANSLOG stage.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels /><created>2016-05-12T05:37:52Z</created><updated>2016-05-12T07:16:16Z</updated><resolved>2016-05-12T07:16:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-12T07:16:16Z" id="218678521">please ask question on the [discuss list](http://discuss.elastic.co) - this is tracker is only for bugs / features / improvements. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add debugging tool to tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18288</link><project id="" key="" /><description>Today this is done with commented out code... and its painful.

Instead we can add a simple Debugger.toString() for debugging.
</description><key id="154392063">18288</key><summary>Add debugging tool to tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>test</label></labels><created>2016-05-12T03:58:21Z</created><updated>2016-05-17T12:23:34Z</updated><resolved>2016-05-12T05:16:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-12T04:00:58Z" id="218654738">Looks great.  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove input, support params instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18287</link><project id="" key="" /><description>Users don't need to go thru a `input` map anymore to do things with painless. They can access `_score`, `doc`, `ctx`, `_value` etc directly, and those are first-class citizens and optimized to one degree or another to be fast, with any typing (e.g. map) we can use.

So typically this map in most cases only used to access `params` from the user Because of that I think we should name it exactly that, it makes usage intuitive:

```
script: "ctx._source.foo = params.bar"
lang: "painless"
params: { 
    bar: 'xxx' 
}
```

Also I updated some outdated syntax in various places to try to make things better.
</description><key id="154378670">18287</key><summary>Remove input, support params instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-12T01:36:18Z</created><updated>2016-05-17T12:23:38Z</updated><resolved>2016-05-12T01:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-12T01:39:07Z" id="218637551">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decouple ANTLR AST from Painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18286</link><project id="" key="" /><description>Created a new Painless Tree for analysis/writing to decouple the ANTLR AST and to allow new forms of input to be developed.
</description><key id="154364718">18286</key><summary>Decouple ANTLR AST from Painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T23:21:56Z</created><updated>2016-05-12T11:07:01Z</updated><resolved>2016-05-12T07:50:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-12T04:45:49Z" id="218659272">Overall looks good. +1 to push once its synced up with master!!!!!

I find it easier to understand than the current organization. Thanks for the package docs and other things that make it easier to navigate the code, if you are not so familiar with it.
</comment><comment author="jdconrad" created="2016-05-12T07:50:56Z" id="218684865">Thanks @rmuir for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add MultiValuesSource support to Aggregation Framework</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18285</link><project id="" key="" /><description>This PR refactors core ValuesSourceAggregatorBuilder, Factory, and Parser as base classes to new {Single | Multi} ValuesSourceAggregatorBuilder, Factory, and Parser classes. This provides support for developing new aggregations that operate on multiple fields as is needed by the MultiFieldStats aggregation discussed in #16817.

Existing single field aggregations are refactored to extend SingleValuesSourceAggregator and a follow on PR will add a new matrix aggregation module which includes the MultiFieldStats aggregation reviewed in PR #16826. 
</description><key id="154344548">18285</key><summary>Add MultiValuesSource support to Aggregation Framework</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>discuss</label><label>feature</label><label>review</label></labels><created>2016-05-11T21:14:12Z</created><updated>2016-05-13T18:45:15Z</updated><resolved>2016-05-13T18:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-05-11T21:15:50Z" id="218592675">@colings86, would you mind taking a look at this if you get a chance? It takes the initial work you did with MultiValuesSource but removes the code duplication.
</comment><comment author="colings86" created="2016-05-12T14:10:24Z" id="218768860">@nknize I left some comments but I like what you have so far
</comment><comment author="nknize" created="2016-05-12T22:34:10Z" id="218905103">Thanks @colings86. First round review changes have been made.

It looks like there are a lot of duplicate ParseField static instances like: `ParseField("field)` I went ahead and put some of the common ones as static variables in the ParseField class, but haven't removed the duplicates throughout the code. I figured this could be done in a separate PR if this is the preferred approach?
</comment><comment author="colings86" created="2016-05-13T08:05:21Z" id="218977804">@nknize thanks for making the changes. This looks pretty good now. I left a comment about where the ParseField constants are put because I don't think that they are in the right place.

Also, I know we are going to have an implementation of a MultiValuesSourceAgg in a module but since the infrastructure is in core I think we should at least do basic tests on the infrastructure to make sure it works in core rather than relying on the module. Maybe we should have a Unit test (that extends BaseAggregationTestCase) and an integration test that register a mock or very simple MultiValueSource aggregation and then tests to make sure all the bits of the multi source agg infrastructure works? So in the unit test it checks that the MultiValuesSourceAggregatorBuilder parses and renders to the same thing, hashCode and equals works and the wire format works correctly, and for the integration test it makes sure the options set in the builder are propogated properly through to the aggregator and it returns something sensible? Alternatively to the integration test maybe we just need unit tests for the Parser and Factory?
</comment><comment author="nknize" created="2016-05-13T18:45:07Z" id="219127620">After talking with @colings86 and @jpountz we've decided to close this PR in favor of the following:
- Leave ValuesSource classes in core alone (regardless of code duplication)
- Put MultiValuesSource support classes in the aggs-matrix-stats module (PR #18300) until another module or plugin requires them
- Remove script support from MultiValuesSource aggregations (until the design can be thought through)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_value support in painess?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18284</link><project id="" key="" /><description>this makes aggregations per-document _value fast (bypass hash put, hash get, etc) for painless.

but i have no clue how to test it, it seems this feature never worked via REST? If i try to pass `script` at all, terms aggregation says GFY.

Should we drop the feature instead? Did it ever work????

@clintongormley @s1monw 

Some guidance please, I'm kinda frustrated. I just want to make this stuff fast.
</description><key id="154325719">18284</key><summary>_value support in painess?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>:Scripting</label><label>enhancement</label><label>PITA</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T19:41:59Z</created><updated>2016-05-12T15:42:10Z</updated><resolved>2016-05-12T00:37:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-11T19:42:27Z" id="218567659">this is what i refer to: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-terms-aggregation.html#_value_script_8
</comment><comment author="rmuir" created="2016-05-11T20:11:24Z" id="218575446">Seems a lot of the docs were out of date here. I have concerns this feature is actually used...
</comment><comment author="s1monw" created="2016-05-11T20:21:48Z" id="218578234">@rmuir and I spoke offline and the examples seem to be outdated in current master but work in 2.x so at least that is resolved. The question why we have this special value is still open and if we can / should move away from it?
</comment><comment author="rmuir" created="2016-05-11T20:29:32Z" id="218580320">Right thats the thing, we should optimize for common cases. This PR optimizes this `_value` so that its not going to do a bunch of per-document hash gets and puts, etc. Lucene expressions also already optimizes for it.

It was my understanding that this is how scripts interact with aggregations. So if this is true, then I want things to be performant, and i want the syntax to be easy.
</comment><comment author="jdconrad" created="2016-05-11T20:34:28Z" id="218581672">LGTM on the Painless portion.
</comment><comment author="rjernst" created="2016-05-12T00:09:16Z" id="218625934">LGTM too
</comment><comment author="jdconrad" created="2016-05-12T00:09:19Z" id="218625941">LGTM.  Let's move forward.
</comment><comment author="clintongormley" created="2016-05-12T10:55:14Z" id="218724546">@rmuir this feature works in 5.0.0-alpha2 and in master:

```
PUT t/t/1
{
  "foo": [5,10]
}

GET t/_search?size=0
{
  "aggs": {
    "foo": {
      "terms": {
        "field": "foo",
        "script": {
          "inline": "_value",
          "lang": "groovy"
        }
      }
    }
  }
}
```

There are two forms of scripts here: One where you don't specify the `field`, which is called once per doc, and one where you DO specify the `field`, which is called once per field `_value`.
</comment><comment author="rmuir" created="2016-05-12T15:25:07Z" id="218792304">&gt; @rmuir this feature works in 5.0.0-alpha2 and in master:

The confusion here was caused by the 5.0 docs not having `script/inline` but having what @s1monw called "1.x style scripting". So when i chased down this `_value` to see what it was all about: none of the examples worked. And there was not a lone rest test in the system using the feature (second problem)!

I think we can fix the docs side, I would like to convert them all to painless anyway and also make them use the special notation so they are tested.
</comment><comment author="clintongormley" created="2016-05-12T15:42:10Z" id="218797640">ah ok - thanks for the explanation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CORS handling triggered whether User-Agent is a browser or not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18283</link><project id="" key="" /><description>This commit ensures that if CORS is enabled, then Origin headers are
checked regardless of whether the request came from a browser or not.
In the past, we only proceeded with CORS checks if the User-Agent was a
browser.
</description><key id="154295235">18283</key><summary>CORS handling triggered whether User-Agent is a browser or not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label><label>v2.3.3</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T17:10:33Z</created><updated>2016-07-14T12:53:23Z</updated><resolved>2016-05-11T19:42:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-11T19:38:36Z" id="218566725">LGTM.
</comment><comment author="abeyad" created="2016-05-11T19:42:59Z" id="218567810">Closed by 189341da101adf75e542a118021ef618857b59db
</comment><comment author="sheilj" created="2016-07-14T12:42:12Z" id="232654325">Hi, we are calling the elasticsearch (2.3.3.) from javascript code, using http methods. What in the headers do we need to set in the http request to get rid of the error:

Request header content-type was not present in the Access-Control-Allow-Headers list.

Any help is appreciated, struggling with this issue since some time now.
</comment><comment author="jasontedor" created="2016-07-14T12:44:03Z" id="232654756">@sheilj Sorry you're struggling, but please ask questions like that on the [Elastic Discourse forum](https://discuss.elastic.co).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Long priority over Float</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18282</link><project id="" key="" /><description>Def math operations will now check for Long before Float.  Note that type promotion still works correctly because we make sure both ops are Long.  (This does require an extra branch in some cases, but I believe the trade off is worth it as Float should be a very rarely used type.)
</description><key id="154287779">18282</key><summary>Long priority over Float</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T16:35:17Z</created><updated>2016-05-12T10:57:42Z</updated><resolved>2016-05-12T04:08:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-11T16:46:03Z" id="218518648">LGTM
</comment><comment author="rmuir" created="2016-05-11T17:07:39Z" id="218524853">maybe we can be a little more conservative and reorder only where it won't cost us an additional branch?

I think its helpful to keep the cases minimal too in case we implement them differently (e.g. methodhandle).
</comment><comment author="rmuir" created="2016-05-11T18:28:18Z" id="218547588">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move AsciiFolding earlier in FingerprintAnalyzer filter chain</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18281</link><project id="" key="" /><description>Rearranges the FingerprintAnalyzer so that AsciiFolding comes earlier in the chain (after lowercasing, before stop removal, for maximum deduping power)

/cc @markharwood 

Closes #18266
</description><key id="154283642">18281</key><summary>Move AsciiFolding earlier in FingerprintAnalyzer filter chain</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T16:15:55Z</created><updated>2016-05-12T13:34:34Z</updated><resolved>2016-05-12T13:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-12T06:15:19Z" id="218669313">LGTM
</comment><comment author="clintongormley" created="2016-05-12T11:00:46Z" id="218725549">Perhaps add a test like the one in https://github.com/elastic/elasticsearch/issues/18266?
</comment><comment author="clintongormley" created="2016-05-12T12:12:53Z" id="218738898">&gt; Perhaps add a test like the one in #18266?

Sorry, missed that you already had. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_timestamp and _ttl still use legacy numerics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18280</link><project id="" key="" /><description>I just noticed that `_timestamp` and `_ttl` still use legacy numerics. We need to either forbid them on new indices (since they were deprecated in 2.0) or make them use points.

It is not strictly required to do it before 5.0 but that would be much better if 5.0 never created fields with legacy numerics.
</description><key id="154281150">18280</key><summary>_timestamp and _ttl still use legacy numerics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha4</label></labels><created>2016-05-11T16:04:30Z</created><updated>2016-06-21T16:06:20Z</updated><resolved>2016-06-21T16:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-12T06:17:53Z" id="218669675">For the record, we had a quick chat with @rjernst about moving those to plugins, but it would require adding pluggability to the parsing of request parameters and the translog, which is probably something that we want to avoid (especially the latter).
</comment><comment author="clintongormley" created="2016-05-12T12:55:14Z" id="218748343">The `_timestamp` and `_ttl` fields were deprecated in 2.0.0-beta1 because they are problematic:
- what does the `_timestamp` represent? a created date or a last-modified date or some other timestamp value?
- should the `_ttl` be extended on update? what if an update happens after expiry? 

etc etc.  There are no right answers to these questions - they depend entirely on use case.  Instead of providing these "opinionated features", we'd be better of just providing the primitives required for users to implement the logic they need.

We have these primitives in 5.0:  ingest pipelines allow injecting a new timestamp at index time, delete-by-query is being moved back into core (https://github.com/elastic/elasticsearch/issues/16883) for deletion of expired documents, and the update API allows updating the ttl and timestamp as the user sees fit.

We still need to be able to support indices created in 2.x in 5.x, but we should prevent new indices from enabling `_ttl` and `_timestamp`.
</comment><comment author="clintongormley" created="2016-05-12T12:58:02Z" id="218748986">Would be good to add deprecation logging in 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hot swappable path.data disks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18279</link><project id="" key="" /><description>It seems that when making use of `path.data` over multiple physical disks, that when a disk is removed, the system should recover automatically.   Currently, searches and or indexing requests over missing shards throw exceptions, and no allocation/recovery occurs.   The only way to bring the data back online is to restart the node, or to reinsert the original disk with existing data.  

It would be great if Elasticsearch could:
- Automatically recover when disks are removed
- Automatically make use of a newly returned empty disk

Steps to Test / Reproduce:

1) Set up `path.data` over 2 disks, and start 2 elasticsearch nodes locally

```
path.data: ["/Volumes/KINGSTON", "/Volumes/SDCARD"]
```

2) Index some data over 5 shards.

```
index    shard prirep state   docs  store ip        node
test1003 4     r      STARTED    2 10.1kb 127.0.0.1 Jacqueline Falsworth
test1003 4     p      STARTED    2 10.1kb 127.0.0.1 Vindicator
test1003 3     r      STARTED    6 24.4kb 127.0.0.1 Jacqueline Falsworth
test1003 3     p      STARTED    6 24.5kb 127.0.0.1 Vindicator
test1003 1     r      STARTED   10 40.6kb 127.0.0.1 Jacqueline Falsworth
test1003 1     p      STARTED   10 45.5kb 127.0.0.1 Vindicator
test1003 2     r      STARTED    2 10.1kb 127.0.0.1 Jacqueline Falsworth
test1003 2     p      STARTED    2 10.1kb 127.0.0.1 Vindicator
test1003 0     r      STARTED    3 10.1kb 127.0.0.1 Jacqueline Falsworth
test1003 0     p      STARTED    3 10.1kb 127.0.0.1 Vindicator
```

3) Remove the disk that contains most/all of the data

Exceptions start to show in logs

```
2016-05-11 11:50:18,961][DEBUG][action.admin.indices.stats] [Vindicator] [indices:monitor/stats] failed to execute operation for shard [[[test1003/01ABN7pTQDCoTa80WMdAvg]][0], node[AMr_NWrVSFCuNV-YCOfsVg], [P], s[STARTED], a[id=IMwYwgWrTLCZYa08WJRNvg]]
ElasticsearchException[failed to refresh store stats]; nested: NoSuchFileException[/Volumes/KINGSTON/elasticsearch/nodes/0/indices/01ABN7pTQDCoTa80WMdAvg/0/index];
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1411)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1396)
    at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:54)
    at org.elasticsearch.index.store.Store.stats(Store.java:321)
    at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:632)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:137)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:166)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:414)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:393)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:380)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:65)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:468)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.NoSuchFileException: /Volumes/KINGSTON/elasticsearch/nodes/0/indices/01ABN7pTQDCoTa80WMdAvg/0/index
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:407)
    at java.nio.file.Files.newDirectoryStream(Files.java:457)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:215)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:234)
    at org.elasticsearch.index.store.FsDirectoryService$1.listAll(FsDirectoryService.java:135)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
    at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1417)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1409)
    ... 18 more
[2016-05-11 11:50:26,796][WARN ][monitor.fs               ] [Vindicator] Failed to fetch fs stats - returning empty instance
```

but `_cat/shards` shows everything is OK

```
index    shard prirep state   docs store ip        node
test1003 4     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 4     p      STARTED            127.0.0.1 Vindicator
test1003 3     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 3     p      STARTED            127.0.0.1 Vindicator
test1003 1     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 1     p      STARTED            127.0.0.1 Vindicator
test1003 2     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 2     p      STARTED            127.0.0.1 Vindicator
test1003 0     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 0     p      STARTED            127.0.0.1 Vindicator
```

3) Post a `_refresh`

No change

4) Index some data

```
{
   "error": {
      "root_cause": [
         {
            "type": "index_failed_engine_exception",
            "reason": "Index failed for [test1003#AVSghrSCuf6DFWq498vy]",
            "index_uuid": "01ABN7pTQDCoTa80WMdAvg",
            "shard": "1",
            "index": "test1003"
         }
      ],
      "type": "index_failed_engine_exception",
      "reason": "Index failed for [test1003#AVSghrSCuf6DFWq498vy]",
      "index_uuid": "01ABN7pTQDCoTa80WMdAvg",
      "shard": "1",
      "index": "test1003",
      "caused_by": {
         "type": "i_o_exception",
         "reason": "Input/output error: NIOFSIndexInput(path=\"/Volumes/KINGSTON/elasticsearch/nodes/0/indices/01ABN7pTQDCoTa80WMdAvg/1/index/_a.cfs\") [slice=_a_Lucene50_0.tim]",
         "caused_by": {
            "type": "i_o_exception",
            "reason": "Input/output error"
         }
      }
   },
   "status": 500
}
```

Logs show an exception

```
[2016-05-11 11:52:26,911][DEBUG][action.admin.indices.stats] [Vindicator] [indices:monitor/stats] failed to execute operation for shard [[[test1003/01ABN7pTQDCoTa80WMdAvg]][0], node[AMr_NWrVSFCuNV-YCOfsVg], [P], s[STARTED], a[id=IMwYwgWrTLCZYa08WJRNvg]]
ElasticsearchException[failed to refresh store stats]; nested: NoSuchFileException[/Volumes/KINGSTON/elasticsearch/nodes/0/indices/01ABN7pTQDCoTa80WMdAvg/0/index];
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1411)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1396)
    at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:54)
    at org.elasticsearch.index.store.Store.stats(Store.java:321)
    at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:632)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:137)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:166)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:414)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:393)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:380)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:65)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:468)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.NoSuchFileException: /Volumes/KINGSTON/elasticsearch/nodes/0/indices/01ABN7pTQDCoTa80WMdAvg/0/index
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:407)
    at java.nio.file.Files.newDirectoryStream(Files.java:457)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:215)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:234)
    at org.elasticsearch.index.store.FsDirectoryService$1.listAll(FsDirectoryService.java:135)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
    at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1417)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1409)
    ... 18 more
```

`_cat/shards` still show all shards STARTED

```
index    shard prirep state   docs store ip        node
test1003 4     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 4     p      STARTED            127.0.0.1 Vindicator
test1003 3     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 3     p      STARTED            127.0.0.1 Vindicator
test1003 1     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 1     p      STARTED            127.0.0.1 Vindicator
test1003 2     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 2     p      STARTED            127.0.0.1 Vindicator
test1003 0     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 0     p      STARTED            127.0.0.1 Vindicator
```

5) Wait 5 minutes,  Search some data:

No change

```
{
   "took": 15,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 3,
      "failed": 2,
      "failures": [
         {
            "shard": 0,
            "index": "test1003",
            "node": "AMr_NWrVSFCuNV-YCOfsVg",
            "reason": {
               "type": "i_o_exception",
               "reason": "Input/output error: NIOFSIndexInput(path=\"/Volumes/KINGSTON/elasticsearch/nodes/0/indices/01ABN7pTQDCoTa80WMdAvg/0/index/_0.cfs\") [slice=_0.fdt]",
               "caused_by": {
                  "type": "i_o_exception",
                  "reason": "Input/output error"
               }
            }
         },
         {
            "shard": 1,
            "index": "test1003",
            "node": "wK5mnEIaT82Wz3wdTAjv6Q",
            "reason": {
               "type": "i_o_exception",
               "reason": "Input/output error: NIOFSIndexInput(path=\"/Volumes/KINGSTON/elasticsearch/nodes/1/indices/01ABN7pTQDCoTa80WMdAvg/1/index/_2.cfs\") [slice=_2.fdt]",
               "caused_by": {
                  "type": "i_o_exception",
                  "reason": "Input/output error"
               }
            }
         }
      ]
   },
   "hits": {
      "total": 23,
      "max_score": 1,
      "hits": []
   }
}
```

```
index    shard prirep state   docs store ip        node
test1003 4     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 4     p      STARTED            127.0.0.1 Vindicator
test1003 3     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 3     p      STARTED            127.0.0.1 Vindicator
test1003 1     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 1     p      STARTED            127.0.0.1 Vindicator
test1003 2     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 2     p      STARTED            127.0.0.1 Vindicator
test1003 0     r      STARTED            127.0.0.1 Jacqueline Falsworth
test1003 0     p      STARTED            127.0.0.1 Vindicator
```
</description><key id="154281136">18279</key><summary>Hot swappable path.data disks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Store</label><label>enhancement</label><label>resiliency</label></labels><created>2016-05-11T16:04:28Z</created><updated>2016-10-24T21:43:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-12T10:06:16Z" id="218714389">Related to #18217
</comment><comment author="clintongormley" created="2016-05-12T10:06:58Z" id="218714548">While I think there may be improvements that can be made when a disk dies, if you want hot swapping etc I think you need a proper RAID system or LVS
</comment><comment author="s1monw" created="2016-05-13T09:25:45Z" id="218994716">I think we need to add some resiliency here:
- we should check if we can write on the datapath before we allocate
- we should fail the engine if we hit an IOException in any case it's really crazy that we don't do that. There should not be any IOException here 

I will take care of this
</comment><comment author="s1monw" created="2016-05-13T09:27:14Z" id="218995048">yeah I am torn on the hot-swapping. I think we can potentially take things out of the loop internally but if you are pluggin in a new disk and we should auto-detect that a datapath is good again I think you should restart the node instead?
</comment><comment author="PhaedrusTheGreek" created="2016-05-13T13:12:41Z" id="219038755">Definitely we don't want to _introduce_ any resiliency issues.    Some manual intervention makes sense, but restarting a node can sometimes take a long time.    Should there be something like delayed allocation on marking a path.data as failed? - there is the case of something like NFS, where a network problem might make the drive appear to come and go.
</comment><comment author="s1monw" created="2016-05-14T20:12:58Z" id="219249138">I think if you loose a disk you need to restart the node. I can totally improve along the lines of failing shards quicker but we shouldn't try to be fancy here. I think we should take the node out of the cluster somehow but that's something that needs more thought.
</comment><comment author="PhaedrusTheGreek" created="2016-05-17T15:40:54Z" id="219759176">Multiple disks on `path.data` offers some added benefit over RAID0, in that IO is spread out over all disks, theoretically matching RAID0 performance, but while not causing a total volume failure on a single disk loss.    

Restarting a node is much easier than re-building a logical volume, and much less data is lost, so either way we are ahead.
</comment><comment author="evanv" created="2016-08-10T13:42:29Z" id="238870117">&gt; I think if you loose a disk you need to restart the node. I can totally improve along the lines of failing shards quicker but we shouldn't try to be fancy here. I think we should take the node out of the cluster somehow but that's something that needs more thought.

In general this makes sense but it would be nice if you could apply something like a transient setting to tell that node that a disk has died and to temporarily stop trying to perform I/O on it. That would still require manual intervention, but it would allow to apply a temporary hotfix if a node restart is not immediately feasible.
</comment><comment author="evanv" created="2016-10-24T21:42:20Z" id="255874842">Had this issue come up against last night.

Our logging nodes have 4 SSDs. We've passed an array to the `path.data` in elasticsearch.yaml. 

Over the weekend, one of the file systems on one of the disks one one of the ES servers became corrupt. Over the next 12 hours, ES spewed 500GB of errors like the following into the logs, filling up the root partition and eventually alerting us (because we alert on disk usage but we didn't at the time have alerts on ES log file size / growth)

```
[2016-10-22 00:00:04,017][WARN ][cluster.action.shard     ] [deliverability_master02-es02] [logstash-delivery-2016.10.14.09][0] received shard failed for target shard [[logstash-delivery-2016.10.14.09][0], node[J_Wws-cKQPKPJjIE7lEacw], relocating [IIKJ3BHGRlG0IYmZ3GLeNA], [R], v[8192], s[INITIALIZING], a[id=HwzksPLITruZz94vsNTMvg, rId=6DS2pI5FS3uih0a1yvRJFw], expected_shard_size[25697352067]], indexUUID [RL1zWoD6SN6_ZmpjPGM0Yw], message [failed to create shard], failure [ElasticsearchException[failed to create shard]; nested: NotSerializableExceptionWrapper[file_system_exception: /storage/sdd1/deliverability/nodes/0/indices/logstash-delivery-2016.10.14.09/0/_state: Input/output error]; ]
[logstash-delivery-2016.10.14.09][[logstash-delivery-2016.10.14.09][0]] ElasticsearchException[failed to create shard]; nested: NotSerializableExceptionWrapper[file_system_exception: /storage/sdd1/deliverability/nodes/0/indices/logstash-delivery-2016.10.14.09/0/_state: Input/output error];
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:620)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:520)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:177)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: NotSerializableExceptionWrapper[file_system_exception: /storage/sdd1/deliverability/nodes/0/indices/logstash-delivery-2016.10.14.09/0/_state: Input/output error]
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
    at java.nio.file.Files.newDirectoryStream(Files.java:457)
    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:257)
    at org.elasticsearch.index.shard.ShardPath.loadShardPath(ShardPath.java:122)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 10 more
```

There are 12 data nodes in this cluster with 4 SSDs, 3 dedicated masters, and we run a replication factor of 2 using hourly indices with 2 primary shards.

During the time that this happened, Elasticsearch continued to place primary shards on the failed `storage/sdd1` drive. Because the writes to the primary failed, and because we were only alerted of the problem (interesting to note as well that the cluster remained green the entire time and none of our monitoring and alerting of /_cluster and _nodes stats caught it.. which is our fault, but still important to note) because the errors in the logs filled up the root disk. 

As a result of Elasticsearch continuing to place primary shards on the failed disk, we lost half of the log data for 9 out of the 12 hours that this disk was unreachable (because 9 out of 12 times it attempted to place at least one of each hour's primary shards on the unreachable disk; the writes to primary failed, the primary was never moved elsewhere).

I suspect, although I did not dig into it or write a test case to prove it, that the process whereby Elasticsearch determines which nodes are eligible to get a write and which disk to write to once it gets there might also bias further writes towards the drive that failed. In our case, we had 9 data nodes that were eligible to accept writes, each having 4 eligible disks that had not exceeded any water marks or otherwise were unwritable. Over 12 hours, 9 of the 24 primary shards created were allocated to the node with the disk failure and it routed them to the unreachable disk. As a result of being unwritable for several hours, that disk also was less full than the other disks on the cluster. Again, I don't know that a disk failure like the one we had biases shard placement in favor of writing to the unreachable disk. But we did see an abnormally high number of shards placed on one machine, and on one disk on one machine.... abnormal enough to make me wonder if that wasn't just a coincidence. 

All of which to say.... I think this issue is extremely important.  I also think @s1monw is right to suggest that ensuring a filepath is writable before placing a shard (especially a primary shard) will go a long way towards adding resiliency. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CORS should permit same origin requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18278</link><project id="" key="" /><description>When CORS is enabled, permit requests from the same origin as the request host, as the request is not a cross origin.

Closes #18256 
</description><key id="154272657">18278</key><summary>CORS should permit same origin requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v2.3.3</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T15:29:13Z</created><updated>2016-05-12T10:22:53Z</updated><resolved>2016-05-11T19:23:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-11T15:29:54Z" id="218495667">@clintongormley FYI.  Once its reviewed, I'll port to 2.x and master.
</comment><comment author="jasontedor" created="2016-05-11T18:39:41Z" id="218550902">LGTM.
</comment><comment author="abeyad" created="2016-05-11T19:23:19Z" id="218562793">Closed by https://github.com/elastic/elasticsearch/commit/d3f205b4a2c6e61810e818de3be83164cd2b69a0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_score as double, not float</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18277</link><project id="" key="" /><description>This is a floating point trap to users. Otherwise there are no single-precision floats, unless they explicitly create them. 

`float` document's fields are exposed as `double` via fielddata accessors, so they are effectively instantly promoted to double (regardless of whether the language in question would do that anyway).

`_score` is treated as `double` for one reason or another, by every other scripting language, including expressions. I think it should be here too. I also like the types being more predictable.
</description><key id="154271371">18277</key><summary>_score as double, not float</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T15:23:56Z</created><updated>2016-05-17T12:22:12Z</updated><resolved>2016-05-11T16:10:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-11T15:31:11Z" id="218496091">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lessen leniency of the query dsl.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18276</link><project id="" key="" /><description>This change does the following:
- Queries that are currently unsupported such as prefix queries on numeric
  fields or term queries on geo fields now throw an error rather than returning
  a query that does not match anything.
- Fuzzy queries on numeric, date and ip fields are now unsupported: they used
  to create range queries, we now expect users to use range queries directly.
  Fuzzy, regexp and prefix queries are now only supported on text/keyword
  fields (including `_all`).
- The `_uid` and `_id` fields do not support prefix or range queries anymore as
  it would prevent us to store them more efficiently in the future, eg. by
  using a binary encoding.

Note that it is still possible to ignore these errors by using the `lenient`
option of the `match` or `query_string` queries.
</description><key id="154251699">18276</key><summary>Lessen leniency of the query dsl.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T14:03:12Z</created><updated>2016-05-19T18:25:05Z</updated><resolved>2016-05-16T15:47:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-11T14:03:49Z" id="218469030">If this change is approved I'll work on adding deprecation warnings on 2.x.
</comment><comment author="rjernst" created="2016-05-15T03:17:59Z" id="219263858">LGTM, I left a couple minor comments.
</comment><comment author="jpountz" created="2016-05-17T10:31:17Z" id="219679753">@clintongormley just informed me that removing the support for fuzzy queries on numeric fields in query strings had been previously rejected in #4076. @kimchy Do you still have concerns about stopping to support fuzzy queries on numeric/date/ip fields?
</comment><comment author="danielmitterdorfer" created="2016-05-17T13:37:18Z" id="219719574">@jpountz we have an open issue to remove fuzzy query entirely in 6.0 (see #15760). So we should close this one too if we don't remove it...
</comment><comment author="jpountz" created="2016-05-17T15:00:04Z" id="219745456">@danielmitterdorfer If my understanding is correct, the concern is more about removing support for the fuzzy operator `~` in query strings than removing the `fuzzy` query.
</comment><comment author="danielmitterdorfer" created="2016-05-17T15:02:21Z" id="219746230">@jpountz Ok, I was just double-checking.
</comment><comment author="kimchy" created="2016-05-17T15:25:42Z" id="219753796">@jpountz I find the ability to use the fuzzy operator in a query string to be simpler to use in things like Kibana query string and so on. Is there a reason you want to remove it? I always viewed it as a "range query" operator, just apply to other types. To me, it does make sense to have `age:10~2` instead of doing the long form range query sample. But if I am in a minority, no problem with removing.
</comment><comment author="jpountz" created="2016-05-19T15:32:52Z" id="220362152">First I wanted to remove it for ip addresses since it does not make much sense now that we support ipv6 (the deltas would need to be huge for it to be useful). So then I questioned whether this is useful at all on numeric/date fields and I could not remember of any bug/discussion about this feature, even though it certainly has bugs (for instance, it did not check for overflows). So I am leaning towards removing it, especially given that the range is easy to type too, and this gives us a single way to search ranges of values, which I think prevents confusion.
</comment><comment author="kimchy" created="2016-05-19T18:25:05Z" id="220410944">Adrien, lead the way :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>throwing exception if use constant_score &amp; bool together at the same level of query object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18275</link><project id="" key="" /><description>only if use with `bool` together

``` json
{
  "query": {
    "bool": {
      "must": [{ "term": { "slug": "awe2ome" } }],
      "must_not": [{ "term": { "slug": "not_awe2ome" } }],
      "should": [],
      "minimum_should_match": 1
    },
    "constant_score": {
      "filter": {
        "bool": {
          "must": [{ "term": { "is_deleted": false } }]
        }
      }
    }
  }
}
```

```
Error: [parse_exception] failed to parse search source. expected field name but got [START_OBJECT]
```

official docker image elasticsearch:2.1.1
</description><key id="154243489">18275</key><summary>throwing exception if use constant_score &amp; bool together at the same level of query object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hbakhtiyor</reporter><labels /><created>2016-05-11T13:27:56Z</created><updated>2016-05-12T10:01:27Z</updated><resolved>2016-05-11T13:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-11T13:42:09Z" id="218462550">Your query is malformed.

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="hbakhtiyor" created="2016-05-11T13:56:45Z" id="218466914">hmm, what's wrong with the query?
</comment><comment author="clintongormley" created="2016-05-11T14:08:08Z" id="218470249">You can't do `{ "query": { "bool": {...}, "something_else: {....}}`
</comment><comment author="hbakhtiyor" created="2016-05-11T15:14:18Z" id="218490787">how to re-write with the right format?
</comment><comment author="clintongormley" created="2016-05-12T10:01:26Z" id="218713370">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add index name in IndexAlreadyExistsException default message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18274</link><project id="" key="" /><description>Adding the index name would be nicer to users especially when indices are automatically created
</description><key id="154228085">18274</key><summary>Add index name in IndexAlreadyExistsException default message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">obourgain</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>feedback_needed</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T12:10:59Z</created><updated>2016-05-13T11:46:34Z</updated><resolved>2016-05-12T18:32:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-11T12:22:45Z" id="218443363">Where is this a problem? The response already includes the index name (in the header and the content), and it's also included in the logs.

From the response:

``` bash
08:29:00 &#8962;62% [jason:~] $ curl -XPUT http://localhost:9200/i?pretty -d '
{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas": 0
    }
  }
}'
{
  "acknowledged" : true
}
08:29:02 [jason:~] $ curl -i -XPUT http://localhost:9200/i?pretty -d '
{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas": 0
    }
  }
}'
HTTP/1.1 400 Bad Request
es.index_uuid: jAsAGwRYTGO6t22OOkkt1A
es.index: i
Content-Type: application/json; charset=UTF-8
Content-Length: 366

{
  "error" : {
    "root_cause" : [ {
      "type" : "index_already_exists_exception",
      "reason" : "already exists",
      "index_uuid" : "jAsAGwRYTGO6t22OOkkt1A",
      "index" : "i"
    } ],
    "type" : "index_already_exists_exception",
    "reason" : "already exists",
    "index_uuid" : "jAsAGwRYTGO6t22OOkkt1A",
    "index" : "i"
  },
  "status" : 400
}
```

From the logs:

```
[2016-05-11 08:29:03,983][DEBUG][rest.suppressed          ] /i Params: {pretty=, index=i}
[i/jAsAGwRYTGO6t22OOkkt1A] IndexAlreadyExistsException[already exists]
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:140)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:442)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$100(MetaDataCreateIndexService.java:94)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$1.execute(MetaDataCreateIndexService.java:195)
        at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
        at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
        at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="obourgain" created="2016-05-11T12:43:58Z" id="218447954">The REST API is fine because the index name is included in the payload, the problem I have is more with the transport API.
The issue is that you need to go to the logs. This is annoying when the exception is reported by a system which only send the stacktrace + exception message, like a CI system reporting tests failures just by assertion or a tool tailing the logs of an app and sending emails.
Log level can also be changed, in which case you don't have any information in the logs.
</comment><comment author="jasontedor" created="2016-05-12T16:59:25Z" id="218819934">&gt; The REST API is fine because the index name is included in the payload, the problem I have is more with the transport API.

Okay, I'm convinced. I left a comment.
</comment><comment author="jasontedor" created="2016-05-12T18:32:20Z" id="218845777">LGTM. Thanks @obourgain!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>random_score doesn't work if passed filter at query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18273</link><project id="" key="" /><description>if remove the filter from the query, it works well

e.g. 

``` json
  "query": {
    "function_score": {
      "query": {
        "bool": {
          "must": [{ "term": { "slug": "awe2ome" } }],
          "must_not": [{ "term": { "slug": "not_awe2ome" } }],
          "should": [],
          "filter": [{ "term": { "is_deleted": false } }],
          "minimum_should_match": 1
        }
      },
      "random_score": {},
      "score_mode": "sum"
    }
  }
}
```

official docker image elasticsearch:2.1.1
</description><key id="154225900">18273</key><summary>random_score doesn't work if passed filter at query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hbakhtiyor</reporter><labels><label>feedback_needed</label></labels><created>2016-05-11T11:58:19Z</created><updated>2016-05-11T15:10:04Z</updated><resolved>2016-05-11T13:20:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-11T12:17:16Z" id="218442180">Can you be more specific about what does not work?
</comment><comment author="hbakhtiyor" created="2016-05-11T12:42:29Z" id="218447622">surely, randomization documents doesn't work
</comment><comment author="clintongormley" created="2016-05-11T13:20:33Z" id="218456686">Random scoring works just fine:

```
POST t/t/_bulk
{"index":{"_id": 1}}
{ "is_deleted": false, "slug": "awe2ome"}
{"index":{"_id": 2}}
{ "is_deleted": false, "slug": "awe2ome"}
{"index":{"_id": 3}}
{ "is_deleted": false, "slug": "awe2ome"}
{"index":{"_id": 4}}
{ "is_deleted": false, "slug": "awe2ome"}
{"index":{"_id": 5}}
{ "is_deleted": false, "slug": "awe2ome"}

GET t/_search?_source=false
{
  "query": {
    "function_score": {
      "query": {
        "bool": {
          "must": [
            {
              "term": {
                "slug": "awe2ome"
              }
            }
          ],
          "must_not": [
            {
              "term": {
                "slug": "not_awe2ome"
              }
            }
          ],
          "should": [],
          "filter": [
            {
              "term": {
                "is_deleted": false
              }
            }
          ],
          "minimum_should_match": 1
        }
      },
      "random_score": {},
      "score_mode": "sum"
    }
  }
}
```

Returns:  

```
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 5,
    "max_score": 0.30669054,
    "hits": [
      {
        "_index": "t",
        "_type": "t",
        "_id": "4",
        "_score": 0.30669054
      },
      {
        "_index": "t",
        "_type": "t",
        "_id": "3",
        "_score": 0.28026438
      },
      {
        "_index": "t",
        "_type": "t",
        "_id": "2",
        "_score": 0.2672884
      },
      {
        "_index": "t",
        "_type": "t",
        "_id": "5",
        "_score": 0.19387394
      },
      {
        "_index": "t",
        "_type": "t",
        "_id": "1",
        "_score": 0.10168079
      }
    ]
  }
}
```

The scores (and order) change on every run. 
</comment><comment author="hbakhtiyor" created="2016-05-11T13:56:05Z" id="218466693">result:

```
{
  "took": 62,
  "timed_out": false,
  "_shards": {
    "total": 15,
    "successful": 15,
    "failed": 0
  },
  "hits": {
    "total": 461426,
    "max_score": 0,
    "hits": [
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fde480",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fdea62",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fdee41",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bad66104e7300fdbb28",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bad66104e7300fdc968",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fdd7db",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fdf286",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fdf4b5",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fdf4c3",
        "_score": 0
      },
      {
        "_index": "my-index",
        "_type": "category",
        "_id": "56585bae66104e7300fdde9e",
        "_score": 0
      }
    ]
  }
}
```

the scores always are zeros and order not change on every run 
</comment><comment author="clintongormley" created="2016-05-11T14:07:17Z" id="218470022">@hbakhtiyor you're only providing half the information? how is your mapping/documents/query different from the one where i demonstrate that this is working correctly?
</comment><comment author="hbakhtiyor" created="2016-05-11T14:26:13Z" id="218475794">@clintongormley the query is the same, your examples works well too, but with my documents it doesn't work

``` json
{

    "state": "open",
    "settings": {
        "index": {
            "creation_date": "1462118131951",
            "number_of_shards": "5",
            "number_of_replicas": "1",
            "uuid": "erwjFwrPTcGbbRZZqcR9mw",
            "version": {
                "created": "2010199"
            }
        }
    },
    "mappings": {
        "organization": {
            "properties": {
                "is_deleted": {
                    "type": "boolean"
                },
                "city": {
                    "type": "string"
                },
                "country": {
                    "type": "string"
                },
                "zip_code": {
                    "type": "string"
                },
                "street": {
                    "type": "string"
                },
                "area": {
                    "type": "string"
                },
                "id": {
                    "type": "string"
                },
                "slug": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "legal_name": {
                    "type": "string"
                },
                "location": {
                    "type": "geo_point",
                    "lat_lon": true
                }
            }
        }
    },
    "aliases": [ ]

}
```
</comment><comment author="clintongormley" created="2016-05-11T14:40:12Z" id="218480160">@hbakhtiyor Please can you provide a full `curl` recreation demonstrating the problem (in the same way I did to demonstrate that it works)
</comment><comment author="hbakhtiyor" created="2016-05-11T14:42:10Z" id="218480768">@clintongormley 

now recognized that your example doesn't work too, if only leave `filter` at `bool`

``` json
GET t/_search?_source=false
{
  "query": {
    "function_score": {
      "query": {
        "bool": {
          "filter": [
            {
              "term": {
                "is_deleted": false
              }
            }
          ],
          "minimum_should_match": 1
        }
      },
      "random_score": {},
      "score_mode": "sum"
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-05-11T14:47:48Z" id="218482493">That's because the `filter` clause returns scores of `0`, which then get multiplied with any functions (because `boost_mode` defaults to `multiply`) which results in a final score of `0`.

Change `boost_mode` to `replace` or `sum`, or use a `constant_score` query instead of `bool.filter`
</comment><comment author="hbakhtiyor" created="2016-05-11T15:10:04Z" id="218489460">@clintongormley thanks for your explanation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.2 fails to start with jna tmp dir configured (core dump)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18272</link><project id="" key="" /><description>**Elasticsearch version** 2.2

**JVM version**: 1.7.0_75

**OS version**: CentOS release 6.7

**Description of the problem including expected versus actual behavior**:
Starting elasticsearch fails to start if the following is added to the /etc/sysconfig/elasticsearch file:

```
# Additional Java OPTS
ES_JAVA_OPTS="-Djna.tmpdir=/usr/share/elasticsearch/tmp -Djava.io.tmpdir=/usr/share/elasticsearch/tmp"
```

With this line commented out, ES starts but JNA is disabled as the default /tmp/ directory is mounted with noexec. If /tmp/ is mounted without noexec all works as expected.
/usr/share/elasticsearch/tmp exists, belongs to the right user/group and has permissions 755.

**Steps to reproduce**:

```
[root@storagenode01 ~]# service elasticsearch start
Starting elasticsearch: /usr/share/elasticsearch/bin/elasticsearch: line 155: 28878 Aborted                 (core dumped) exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS -Des.path.home="$ES_HOME" -cp "$ES_CLASSPATH" org.elasticsearch.bootstrap.Elasticsearch start "$@" 0&gt;&amp;-
                                                           [FAILED]
```

**Provide logs (if relevant)**:

```
[root@storagenode01 ~]# cat /tmp/hs_err_pid28216.log
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f424226f40a, pid=28216, tid=139922878629632
#
# JRE version: Java(TM) SE Runtime Environment (7.0_75-b13) (build 1.7.0_75-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.75-b04 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [jna4948368637624641726.tmp+0x1240a]  ffi_prep_closure_loc+0x1a
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.sun.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

---------------  T H R E A D  ---------------

Current thread (0x00007f424c00a000):  JavaThread "main" [_thread_in_native, id=28218, stack(0x00007f42556a0000,0x00007f42557a1000)]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x0000000000000000

Registers:
RAX=0x00007f424226f8c2, RBX=0x00007f425579ed48, RCX=0x00007f424c44a7b0, RDX=0x00007f4242264590
RSP=0x00007f425579eae0, RBP=0x00007f425579eae0, RSI=0x00007f424c44a7d0, RDI=0x0000000000000000
R8 =0x00007f425007ae43, R9 =0x0000000000000002, R10=0x00007f425579e870, R11=0x00007f424226f3f0
R12=0x0000000000000000, R13=0x0000000000000008, R14=0x00007f424c44a7b0, R15=0x0000000000000004
RIP=0x00007f424226f40a, EFLAGS=0x0000000000010246, CSGSFS=0x0000000000000033, ERR=0x0000000000000006
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f425579eae0)
0x00007f425579eae0:   00007f425579ebb0 00007f424226a4dd
0x00007f425579eaf0:   0000000000000005 00007f425579ed38
0x00007f425579eb00:   00007f425579ed50 00007f424c00a1e8
0x00007f425579eb10:   0000000100000000 00007f424c44a940
0x00007f425579eb20:   00007f424c450f70 00007f424c44a7d0
0x00007f425579eb30:   000000015579eca0 00007f424c44a980
0x00007f425579eb40:   00007f424c44a960 00007f424c44a790
0x00007f425579eb50:   0000000000000001 000000035579ed48
0x00007f425579eb60:   00007f425579ed40 00007f425579ed38
0x00007f425579eb70:   00007f4200000000 00007f425007ae43
0x00007f425579eb80:   00000000fb599510 00000000fb599510
0x00007f425579eb90:   0000000000000000 00000000fb599510
0x00007f425579eba0:   00007f425579ed50 00007f424c00a000
0x00007f425579ebb0:   00007f425579eca0 00007f425006fd98
0x00007f425579ebc0:   00007f425579ed30 00007f425579ed28
0x00007f425579ebd0:   0000000000000002 00007f4242271eb0
0x00007f425579ebe0:   00007f4242271eb0 00007f425579ecf8
0x00007f425579ebf0:   00007f4254c43f70 0000000000000000
0x00007f425579ec00:   0000000000000000 00007f425579ecd0
0x00007f425579ec10:   0000000000000000 00007f425579ecc0
0x00007f425579ec20:   00007f425579eda8 00007f425579ecc0
0x00007f425579ec30:   00007f424c00a998 00007f4243a48f90
0x00007f425579ec40:   0000000900000010 0000000c00000024
0x00007f425579ec50:   00007f424c00a000 00007f42000000b6
0x00007f425579ec60:   00007f425579ec60 00000000fb599510
0x00007f425579ec70:   00007f425579ed50 00000000fb59f350
0x00007f425579ec80:   0000000000000000 00000000fb599510
0x00007f425579ec90:   0000000000000000 00007f425579ecc0
0x00007f425579eca0:   00007f425579eda8 00007f4250063175
0x00007f425579ecb0:   00000000bbc87fb8 00007f425006bcd7
0x00007f425579ecc0:   00000000bae0d660 0000000000000000
0x00007f425579ecd0:   00000000bbd64530 0000000000000000

Instructions: (pc=0x00007f424226f40a)
0x00007f424226f3ea:   66 90 66 66 66 90 8b 06 55 41 b9 02 00 00 00 48
0x00007f424226f3fa:   89 e5 ff c8 83 f8 01 77 44 48 8b 05 c6 49 10 00
0x00007f424226f40a:   66 c7 07 49 bb 4c 89 47 0c 66 c7 47 0a 49 ba 48
0x00007f424226f41a:   89 47 02 8b 46 1c 48 89 77 18 48 89 57 20 48 89

Register to memory mapping:

RAX=0x00007f424226f8c2: ffi_closure_unix64+0 in /usr/share/elasticsearch/tmp/jna4948368637624641726.tmp at 0x00007f424225d000
RBX=0x00007f425579ed48 is pointing into the stack for thread: 0x00007f424c00a000
RCX=0x00007f424c44a7b0 is an unknown value
RDX=0x00007f4242264590: &lt;offset 0x7590&gt; in /usr/share/elasticsearch/tmp/jna4948368637624641726.tmp at 0x00007f424225d000
RSP=0x00007f425579eae0 is pointing into the stack for thread: 0x00007f424c00a000
RBP=0x00007f425579eae0 is pointing into the stack for thread: 0x00007f424c00a000
RSI=0x00007f424c44a7d0 is an unknown value
RDI=0x0000000000000000 is an unknown value
R8 =0x00007f425007ae43 is at code_begin+99 in an Interpreter codelet
invokestatic  184 invokestatic  [0x00007f425007ade0, 0x00007f425007af20]  320 bytes
R9 =0x0000000000000002 is an unknown value
R10=0x00007f425579e870 is pointing into the stack for thread: 0x00007f424c00a000
R11=0x00007f424226f3f0: ffi_prep_closure_loc+0 in /usr/share/elasticsearch/tmp/jna4948368637624641726.tmp at 0x00007f424225d000
R12=0x0000000000000000 is an unknown value
R13=0x0000000000000008 is an unknown value
R14=0x00007f424c44a7b0 is an unknown value
R15=0x0000000000000004 is an unknown value


Stack: [0x00007f42556a0000,0x00007f42557a1000],  sp=0x00007f425579eae0,  free space=1018k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [jna4948368637624641726.tmp+0x1240a]  ffi_prep_closure_loc+0x1a
C  [jna4948368637624641726.tmp+0xd4dd]  Java_com_sun_jna_Native_registerMethod+0x45d
j  com.sun.jna.Native.registerMethod(Ljava/lang/Class;Ljava/lang/String;Ljava/lang/String;[I[J[JIJJLjava/lang/Class;JIZ[Lcom/sun/jna/ToNativeConverter;Lcom/sun/jna/FromNativeConverter;Ljava/lang/String;)J+0
j  com.sun.jna.Native.register(Ljava/lang/Class;Lcom/sun/jna/NativeLibrary;)V+979
j  com.sun.jna.Native.register(Ljava/lang/Class;Ljava/lang/String;)V+28
j  com.sun.jna.Native.register(Ljava/lang/String;)V+7
j  org.elasticsearch.bootstrap.JNACLibrary.&lt;clinit&gt;()V+45
v  ~StubRoutines::call_stub
V  [libjvm.so+0x600045]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x365
V  [libjvm.so+0x5feaa8]  JavaCalls::call(JavaValue*, methodHandle, JavaCallArguments*, Thread*)+0x28
V  [libjvm.so+0x5c2f1a]  instanceKlass::call_class_initializer(Thread*)+0xca
V  [libjvm.so+0x5c3174]  instanceKlass::initialize_impl(instanceKlassHandle, Thread*)+0x234
V  [libjvm.so+0x5c35ca]  instanceKlass::initialize(Thread*)+0x6a
V  [libjvm.so+0x74454e]  LinkResolver::resolve_static_call(CallInfo&amp;, KlassHandle&amp;, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)+0x11e
V  [libjvm.so+0x7446b6]  LinkResolver::resolve_invokestatic(CallInfo&amp;, constantPoolHandle, int, Thread*)+0xe6
V  [libjvm.so+0x5f846f]  InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)+0x17f
j  org.elasticsearch.bootstrap.JNANatives.definitelyRunningAsRoot()Z+8
j  org.elasticsearch.bootstrap.Natives.definitelyRunningAsRoot()Z+22
j  org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Ljava/nio/file/Path;ZZZ)V+8
j  org.elasticsearch.bootstrap.Bootstrap.setup(ZLorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/env/Environment;)V+43
j  org.elasticsearch.bootstrap.Bootstrap.init([Ljava/lang/String;)V+238
j  org.elasticsearch.bootstrap.Elasticsearch.main([Ljava/lang/String;)V+1
v  ~StubRoutines::call_stub
V  [libjvm.so+0x600045]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x365
V  [libjvm.so+0x5feaa8]  JavaCalls::call(JavaValue*, methodHandle, JavaCallArguments*, Thread*)+0x28
V  [libjvm.so+0x638a59]  jni_invoke_static(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)+0x219
V  [libjvm.so+0x641672]  jni_CallStaticVoidMethod+0x162
C  [libjli.so+0x36d9]  JavaMain+0x7e9

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
j  com.sun.jna.Native.registerMethod(Ljava/lang/Class;Ljava/lang/String;Ljava/lang/String;[I[J[JIJJLjava/lang/Class;JIZ[Lcom/sun/jna/ToNativeConverter;Lcom/sun/jna/FromNativeConverter;Ljava/lang/String;)J+0
j  com.sun.jna.Native.register(Ljava/lang/Class;Lcom/sun/jna/NativeLibrary;)V+979
j  com.sun.jna.Native.register(Ljava/lang/Class;Ljava/lang/String;)V+28
j  com.sun.jna.Native.register(Ljava/lang/String;)V+7
j  org.elasticsearch.bootstrap.JNACLibrary.&lt;clinit&gt;()V+45
v  ~StubRoutines::call_stub
j  org.elasticsearch.bootstrap.JNANatives.definitelyRunningAsRoot()Z+8
j  org.elasticsearch.bootstrap.Natives.definitelyRunningAsRoot()Z+22
j  org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Ljava/nio/file/Path;ZZZ)V+8
j  org.elasticsearch.bootstrap.Bootstrap.setup(ZLorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/env/Environment;)V+43
j  org.elasticsearch.bootstrap.Bootstrap.init([Ljava/lang/String;)V+238
j  org.elasticsearch.bootstrap.Elasticsearch.main([Ljava/lang/String;)V+1
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
  0x00007f424c11e000 JavaThread "Service Thread" daemon [_thread_blocked, id=28231, stack(0x00007f4242f6c000,0x00007f424306d000)]
  0x00007f424c11b000 JavaThread "C2 CompilerThread1" daemon [_thread_blocked, id=28230, stack(0x00007f424306d000,0x00007f424316e000)]
  0x00007f424c119000 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=28229, stack(0x00007f424316e000,0x00007f424326f000)]
  0x00007f424c116800 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=28228, stack(0x00007f424326f000,0x00007f4243370000)]
  0x00007f424c114800 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=28227, stack(0x00007f4243370000,0x00007f4243471000)]
  0x00007f424c0eb800 JavaThread "Finalizer" daemon [_thread_blocked, id=28226, stack(0x00007f4243471000,0x00007f4243572000)]
  0x00007f424c0e9800 JavaThread "Reference Handler" daemon [_thread_blocked, id=28225, stack(0x00007f4243572000,0x00007f4243673000)]
=&gt;0x00007f424c00a000 JavaThread "main" [_thread_in_native, id=28218, stack(0x00007f42556a0000,0x00007f42557a1000)]

Other Threads:
  0x00007f424c0e5000 VMThread [stack: 0x00007f4243673000,0x00007f4243774000] [id=28224]
  0x00007f424c129000 WatcherThread [stack: 0x00007f4242e6b000,0x00007f4242f6c000] [id=28232]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap
 par new generation   total 78656K, used 16826K [0x00000000bae00000, 0x00000000c0350000, 0x00000000cfac0000)
  eden space 69952K,  24% used [0x00000000bae00000, 0x00000000bbe6e950, 0x00000000bf250000)
  from space 8704K,   0% used [0x00000000bf250000, 0x00000000bf250000, 0x00000000bfad0000)
  to   space 8704K,   0% used [0x00000000bfad0000, 0x00000000bfad0000, 0x00000000c0350000)
 concurrent mark-sweep generation total 174784K, used 0K [0x00000000cfac0000, 0x00000000da570000, 0x00000000fae00000)
 concurrent-mark-sweep perm gen total 21248K, used 8193K [0x00000000fae00000, 0x00000000fc2c0000, 0x0000000100000000)

Card table byte_map: [0x00007f424bad3000,0x00007f424bcfd000] byte_map_base: 0x00007f424b4fc000

Polling page: 0x00007f42557ac000

Code Cache  [0x00007f425005d000, 0x00007f42502cd000, 0x00007f425305d000)
 total_blobs=312 nmethods=36 adapters=230 free_code_cache=48617Kb largest_free_block=49754112

Compilation events (10 events):
Event: 0.281 Thread 0x00007f424c119000 nmethod 32 0x00007f42500cd2d0 code [0x00007f42500cd420, 0x00007f42500cd518]
Event: 0.282 Thread 0x00007f424c119000   33   !         sun.misc.URLClassPath$JarLoader::getResource (91 bytes)
Event: 0.304 Thread 0x00007f424c119000 nmethod 33 0x00007f42500de510 code [0x00007f42500de8c0, 0x00007f42500e0128]
Event: 0.304 Thread 0x00007f424c119000   34             java.util.Arrays::copyOf (19 bytes)
Event: 0.305 Thread 0x00007f424c119000 nmethod 34 0x00007f42500de0d0 code [0x00007f42500de220, 0x00007f42500de3f8]
Event: 0.306 Thread 0x00007f424c11b000 nmethod 31 0x00007f42500e5650 code [0x00007f42500e5a20, 0x00007f42500e7288]
Event: 0.313 Thread 0x00007f424c119000   35             java.util.Arrays::copyOfRange (63 bytes)
Event: 0.317 Thread 0x00007f424c119000 nmethod 35 0x00007f42500d6d90 code [0x00007f42500d6ee0, 0x00007f42500d70f8]
Event: 0.327 Thread 0x00007f424c11b000   36             java.lang.String::&lt;init&gt; (67 bytes)
Event: 0.329 Thread 0x00007f424c11b000 nmethod 36 0x00007f42500d6810 code [0x00007f42500d6960, 0x00007f42500d6bd8]

GC Heap History (0 events):
No events

Deoptimization events (1 events):
Event: 0.189 Thread 0x00007f424c00a000 Uncommon trap: reason=null_check action=make_not_entrant pc=0x00007f42500c0a60 method=java.lang.String.equals(Ljava/lang/Object;)Z @ 8

Internal exceptions (10 events):
Event: 0.330 Thread 0x00007f424c00a000 Threw 0x00000000bbd50128 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.332 Thread 0x00007f424c00a000 Threw 0x00000000bbd64960 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.332 Thread 0x00007f424c00a000 Threw 0x00000000bbd6bde8 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.332 Thread 0x00007f424c00a000 Threw 0x00000000bbd6f078 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.333 Thread 0x00007f424c00a000 Threw 0x00000000bbd71ef8 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.333 Thread 0x00007f424c00a000 Threw 0x00000000bbd75608 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.333 Thread 0x00007f424c00a000 Threw 0x00000000bbd87288 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.334 Thread 0x00007f424c00a000 Threw 0x00000000bbd8a530 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.334 Thread 0x00007f424c00a000 Threw 0x00000000bbd8d850 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281
Event: 0.334 Thread 0x00007f424c00a000 Threw 0x00000000bbd90b70 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u75/1940/hotspot/src/share/vm/prims/jvm.cpp:1281

Events (10 events):
Event: 0.333 loading class 0x00007f424c43de50
Event: 0.333 loading class 0x00007f424c43de50 done
Event: 0.333 loading class 0x00007f424c44a060
Event: 0.333 loading class 0x00007f424c44a060 done
Event: 0.334 loading class 0x00007f424c44a0a0
Event: 0.334 loading class 0x00007f424c44a0a0 done
Event: 0.334 loading class 0x00007f424c44a200
Event: 0.334 loading class 0x00007f424c44a200 done
Event: 0.334 loading class 0x00007f424c44a240
Event: 0.334 loading class 0x00007f424c44a240 done


Dynamic libraries:
00400000-00401000 r-xp 00000000 fd:00 19763                              /usr/java/jdk1.7.0_75/bin/java
00600000-00601000 rw-p 00000000 fd:00 19763                              /usr/java/jdk1.7.0_75/bin/java
00f22000-00f43000 rw-p 00000000 00:00 0                                  [heap]
bae00000-c0350000 rw-p 00000000 00:00 0
c0350000-cfac0000 rw-p 00000000 00:00 0
cfac0000-da570000 rw-p 00000000 00:00 0
da570000-fae00000 rw-p 00000000 00:00 0
fae00000-fc2c0000 rw-p 00000000 00:00 0
fc2c0000-100000000 rw-p 00000000 00:00 0
7f4200000000-7f4200021000 rw-p 00000000 00:00 0
7f4200021000-7f4204000000 ---p 00000000 00:00 0
7f4208000000-7f42082fd000 rw-p 00000000 00:00 0
7f42082fd000-7f420c000000 ---p 00000000 00:00 0
7f420c000000-7f420c021000 rw-p 00000000 00:00 0
7f420c021000-7f4210000000 ---p 00000000 00:00 0
7f4210000000-7f4210021000 rw-p 00000000 00:00 0
7f4210021000-7f4214000000 ---p 00000000 00:00 0
7f4214000000-7f421442f000 rw-p 00000000 00:00 0
7f421442f000-7f4218000000 ---p 00000000 00:00 0
7f4218000000-7f4218021000 rw-p 00000000 00:00 0
7f4218021000-7f421c000000 ---p 00000000 00:00 0
7f421e16f000-7f4224000000 r--p 00000000 fd:00 11888                      /usr/lib/locale/locale-archive
7f4224000000-7f4224021000 rw-p 00000000 00:00 0
7f4224021000-7f4228000000 ---p 00000000 00:00 0
7f4228000000-7f4228021000 rw-p 00000000 00:00 0
7f4228021000-7f422c000000 ---p 00000000 00:00 0
7f422c000000-7f422c021000 rw-p 00000000 00:00 0
7f422c021000-7f4230000000 ---p 00000000 00:00 0
7f4230000000-7f4230021000 rw-p 00000000 00:00 0
7f4230021000-7f4234000000 ---p 00000000 00:00 0
7f4234000000-7f4234021000 rw-p 00000000 00:00 0
7f4234021000-7f4238000000 ---p 00000000 00:00 0
7f4238000000-7f4238021000 rw-p 00000000 00:00 0
7f4238021000-7f423c000000 ---p 00000000 00:00 0
7f423c000000-7f423c021000 rw-p 00000000 00:00 0
7f423c021000-7f4240000000 ---p 00000000 00:00 0
7f4242015000-7f424225d000 rw-p 00000000 00:00 0
7f424225d000-7f4242274000 r-xp 00000000 fd:00 3301                       /usr/share/elasticsearch/tmp/jna4948368637624641726.tmp (deleted)
7f4242274000-7f4242373000 ---p 00017000 fd:00 3301                       /usr/share/elasticsearch/tmp/jna4948368637624641726.tmp (deleted)
7f4242373000-7f4242375000 rw-p 00016000 fd:00 3301                       /usr/share/elasticsearch/tmp/jna4948368637624641726.tmp (deleted)
7f4242375000-7f424237d000 r-xp 00000000 fd:00 19924                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libmanagement.so
7f424237d000-7f424257c000 ---p 00008000 fd:00 19924                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libmanagement.so
7f424257c000-7f424257d000 rw-p 00007000 fd:00 19924                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libmanagement.so
7f424257d000-7f424258d000 r-xp 00000000 fd:00 19927                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libnio.so
7f424258d000-7f424278d000 ---p 00010000 fd:00 19927                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libnio.so
7f424278d000-7f424278e000 rw-p 00010000 fd:00 19927                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libnio.so
7f424278e000-7f4242a8e000 rw-p 00000000 00:00 0
7f4242a8e000-7f4242aa4000 r-xp 00000000 fd:00 19926                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libnet.so
7f4242aa4000-7f4242ca4000 ---p 00016000 fd:00 19926                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libnet.so
7f4242ca4000-7f4242ca5000 rw-p 00016000 fd:00 19926                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libnet.so
7f4242ca5000-7f4242cad000 r--s 00062000 fd:00 22811                      /usr/share/elasticsearch/lib/lucene-backward-codecs-5.4.1.jar
7f4242cad000-7f4242cb1000 r--s 0002c000 fd:00 22821                      /usr/share/elasticsearch/lib/lucene-spatial-5.4.1.jar
7f4242cb1000-7f4242cb2000 r--s 00001000 fd:00 22825                      /usr/share/elasticsearch/lib/securesm-1.0.jar
7f4242cb2000-7f4242cdd000 r--s 001fc000 fd:00 22502                      /usr/share/elasticsearch/lib/guava-18.0.jar
7f4242cdd000-7f4242ce1000 r--s 0002d000 fd:00 22822                      /usr/share/elasticsearch/lib/lucene-spatial3d-5.4.1.jar
7f4242ce1000-7f4242ceb000 r--s 00059000 fd:00 22819                      /usr/share/elasticsearch/lib/lucene-queryparser-5.4.1.jar
7f4242ceb000-7f4242cf1000 r--s 00037000 fd:00 22818                      /usr/share/elasticsearch/lib/lucene-queries-5.4.1.jar
7f4242cf1000-7f4242cf2000 r--s 0000c000 fd:00 22483                      /usr/share/elasticsearch/lib/commons-cli-1.3.1.jar
7f4242cf2000-7f4242cf4000 r--s 00007000 fd:00 22816                      /usr/share/elasticsearch/lib/lucene-memory-5.4.1.jar
7f4242cf4000-7f4242cf5000 r--s 0000b000 fd:00 22511                      /usr/share/elasticsearch/lib/jackson-dataformat-cbor-2.6.2.jar
7f4242cf5000-7f4242cf9000 r--s 0001a000 fd:00 22815                      /usr/share/elasticsearch/lib/lucene-join-5.4.1.jar
7f4242cf9000-7f4242cfc000 r--s 00022000 fd:00 22827                      /usr/share/elasticsearch/lib/spatial4j-0.5.jar
7f4242cfc000-7f4242d13000 r--s 00100000 fd:00 22503                      /usr/share/elasticsearch/lib/hppc-0.7.1.jar
7f4242d13000-7f4242d17000 r--s 0003c000 fd:00 22506                      /usr/share/elasticsearch/lib/jackson-core-2.6.2.jar
7f4242d17000-7f4242d1c000 r--s 00038000 fd:00 22823                      /usr/share/elasticsearch/lib/lucene-suggest-5.4.1.jar
7f4242d1c000-7f4242d20000 r--s 00018000 fd:00 22486                      /usr/share/elasticsearch/lib/compiler-0.8.13.jar
7f4242d20000-7f4242d26000 r--s 0003c000 fd:00 22826                      /usr/share/elasticsearch/lib/snakeyaml-1.15.jar
7f4242d26000-7f4242d2a000 r--s 000dc000 fd:00 22804                      /usr/share/elasticsearch/lib/jna-4.1.0.jar
7f4242d2a000-7f4242d33000 r--s 0006f000 fd:00 22809                      /usr/share/elasticsearch/lib/log4j-1.2.17.jar
7f4242d33000-7f4242d43000 r--s 00088000 fd:00 22806                      /usr/share/elasticsearch/lib/joda-time-2.8.2.jar
7f4242d43000-7f4242d47000 r--s 00027000 fd:00 22817                      /usr/share/elasticsearch/lib/lucene-misc-5.4.1.jar
7f4242d47000-7f4242d49000 r--s 00008000 fd:00 22805                      /usr/share/elasticsearch/lib/joda-convert-1.2.jar
7f4242d49000-7f4242d5a000 r--s 000b2000 fd:00 22808                      /usr/share/elasticsearch/lib/jts-1.13.jar
7f4242d5a000-7f4242d73000 r--s 0012c000 fd:00 22824                      /usr/share/elasticsearch/lib/netty-3.10.5.Final.jar
7f4242d73000-7f4242d7c000 r--s 00046000 fd:00 22513                      /usr/share/elasticsearch/lib/jackson-dataformat-yaml-2.6.2.jar
7f4242d7c000-7f4242d7e000 r--s 00011000 fd:00 22512                      /usr/share/elasticsearch/lib/jackson-dataformat-smile-2.6.2.jar
7f4242d7e000-7f4242d81000 r--s 00018000 fd:00 22813                      /usr/share/elasticsearch/lib/lucene-grouping-5.4.1.jar
7f4242d81000-7f4242da9000 r--s 00218000 fd:00 22812                      /usr/share/elasticsearch/lib/lucene-core-5.4.1.jar
7f4242da9000-7f4242dbb000 r--s 00170000 fd:00 22810                      /usr/share/elasticsearch/lib/lucene-analyzers-common-5.4.1.jar
7f4242dbb000-7f4242e6b000 r--s 0083d000 fd:00 22500                      /usr/share/elasticsearch/lib/elasticsearch-2.2.0.jar
7f4242e6b000-7f4242e6c000 ---p 00000000 00:00 0
7f4242e6c000-7f4242f6c000 rw-p 00000000 00:00 0
7f4242f6c000-7f4242f6f000 ---p 00000000 00:00 0
7f4242f6f000-7f424306d000 rw-p 00000000 00:00 0
7f424306d000-7f4243070000 ---p 00000000 00:00 0
7f4243070000-7f424316e000 rw-p 00000000 00:00 0
7f424316e000-7f4243171000 ---p 00000000 00:00 0
7f4243171000-7f424326f000 rw-p 00000000 00:00 0
7f424326f000-7f4243272000 ---p 00000000 00:00 0
7f4243272000-7f4243370000 rw-p 00000000 00:00 0
7f4243370000-7f4243373000 ---p 00000000 00:00 0
7f4243373000-7f4243471000 rw-p 00000000 00:00 0
7f4243471000-7f4243474000 ---p 00000000 00:00 0
7f4243474000-7f4243572000 rw-p 00000000 00:00 0
7f4243572000-7f4243575000 ---p 00000000 00:00 0
7f4243575000-7f4243673000 rw-p 00000000 00:00 0
7f4243673000-7f4243674000 ---p 00000000 00:00 0
7f4243674000-7f42437bf000 rw-p 00000000 00:00 0
7f42437bf000-7f424397e000 r--s 039eb000 fd:00 22381                      /usr/java/jdk1.7.0_75/jre/lib/rt.jar
7f424397e000-7f4243afb000 rw-p 00000000 00:00 0
7f4243afb000-7f4243afc000 ---p 00000000 00:00 0
7f4243afc000-7f4243dfe000 rw-p 00000000 00:00 0
7f4243dfe000-7f4243dff000 ---p 00000000 00:00 0
7f4243dff000-7f4244000000 rw-p 00000000 00:00 0
7f4244000000-7f4244021000 rw-p 00000000 00:00 0
7f4244021000-7f4248000000 ---p 00000000 00:00 0
7f4248005000-7f4248009000 r--s 00020000 fd:00 22814                      /usr/share/elasticsearch/lib/lucene-highlighter-5.4.1.jar
7f4248009000-7f424800e000 r--s 0003e000 fd:00 22820                      /usr/share/elasticsearch/lib/lucene-sandbox-5.4.1.jar
7f424800e000-7f4248010000 r--s 0000e000 fd:00 22807                      /usr/share/elasticsearch/lib/jsr166e-1.1.0.jar
7f4248010000-7f4248012000 r--s 0000b000 fd:00 22828                      /usr/share/elasticsearch/lib/t-digest-3.0.jar
7f4248012000-7f4248014000 r--s 00012000 fd:00 22493                      /usr/share/elasticsearch/lib/compress-lzf-1.0.2.jar
7f4248014000-7f424801c000 r--s 00066000 fd:00 22482                      /usr/share/elasticsearch/lib/apache-log4j-extras-1.2.17.jar
7f424801c000-7f424b5cb000 rw-p 00000000 00:00 0
7f424b5cb000-7f424b6cf000 rw-p 00000000 00:00 0
7f424b6cf000-7f424bafe000 rw-p 00000000 00:00 0
7f424bafe000-7f424bb79000 rw-p 00000000 00:00 0
7f424bb79000-7f424bbcf000 rw-p 00000000 00:00 0
7f424bbcf000-7f424bcd3000 rw-p 00000000 00:00 0
7f424bcd3000-7f424bcde000 rw-p 00000000 00:00 0
7f424bcde000-7f424bcfc000 rw-p 00000000 00:00 0
7f424bcfc000-7f424bcfd000 rw-p 00000000 00:00 0
7f424bcfd000-7f424bcfe000 ---p 00000000 00:00 0
7f424bcfe000-7f424bdfe000 rw-p 00000000 00:00 0
7f424bdfe000-7f424bdff000 ---p 00000000 00:00 0
7f424bdff000-7f424beff000 rw-p 00000000 00:00 0
7f424beff000-7f424bf00000 ---p 00000000 00:00 0
7f424bf00000-7f424c000000 rw-p 00000000 00:00 0
7f424c000000-7f424c45c000 rw-p 00000000 00:00 0
7f424c45c000-7f4250000000 ---p 00000000 00:00 0
7f4250002000-7f425003e000 rw-p 00000000 00:00 0
7f425003e000-7f425005d000 rw-p 00000000 00:00 0
7f425005d000-7f42502cd000 rwxp 00000000 00:00 0
7f42502cd000-7f425305d000 rw-p 00000000 00:00 0
7f425305d000-7f4253077000 r-xp 00000000 fd:00 19938                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libzip.so
7f4253077000-7f4253277000 ---p 0001a000 fd:00 19938                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libzip.so
7f4253277000-7f4253278000 rw-p 0001a000 fd:00 19938                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libzip.so
7f4253278000-7f4253284000 r-xp 00000000 fd:00 14665                      /lib64/libnss_files-2.12.so
7f4253284000-7f4253484000 ---p 0000c000 fd:00 14665                      /lib64/libnss_files-2.12.so
7f4253484000-7f4253485000 r--p 0000c000 fd:00 14665                      /lib64/libnss_files-2.12.so
7f4253485000-7f4253486000 rw-p 0000d000 fd:00 14665                      /lib64/libnss_files-2.12.so
7f4253486000-7f42534af000 r-xp 00000000 fd:00 19908                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libjava.so
7f42534af000-7f42536af000 ---p 00029000 fd:00 19908                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libjava.so
7f42536af000-7f42536b1000 rw-p 00029000 fd:00 19908                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libjava.so
7f42536b1000-7f42536be000 r-xp 00000000 fd:00 19937                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libverify.so
7f42536be000-7f42538bd000 ---p 0000d000 fd:00 19937                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libverify.so
7f42538bd000-7f42538bf000 rw-p 0000c000 fd:00 19937                      /usr/java/jdk1.7.0_75/jre/lib/amd64/libverify.so
7f42538bf000-7f42538c6000 r-xp 00000000 fd:00 22663                      /lib64/librt-2.12.so
7f42538c6000-7f4253ac5000 ---p 00007000 fd:00 22663                      /lib64/librt-2.12.so
7f4253ac5000-7f4253ac6000 r--p 00006000 fd:00 22663                      /lib64/librt-2.12.so
7f4253ac6000-7f4253ac7000 rw-p 00007000 fd:00 22663                      /lib64/librt-2.12.so
7f4253ac7000-7f4253b4a000 r-xp 00000000 fd:00 14232                      /lib64/libm-2.12.so
7f4253b4a000-7f4253d49000 ---p 00083000 fd:00 14232                      /lib64/libm-2.12.so
7f4253d49000-7f4253d4a000 r--p 00082000 fd:00 14232                      /lib64/libm-2.12.so
7f4253d4a000-7f4253d4b000 rw-p 00083000 fd:00 14232                      /lib64/libm-2.12.so
7f4253d4b000-7f42548be000 r-xp 00000000 fd:00 19942                      /usr/java/jdk1.7.0_75/jre/lib/amd64/server/libjvm.so
7f42548be000-7f4254abe000 ---p 00b73000 fd:00 19942                      /usr/java/jdk1.7.0_75/jre/lib/amd64/server/libjvm.so
7f4254abe000-7f4254b82000 rw-p 00b73000 fd:00 19942                      /usr/java/jdk1.7.0_75/jre/lib/amd64/server/libjvm.so
7f4254b82000-7f4254bc3000 rw-p 00000000 00:00 0
7f4254bc3000-7f4254d4d000 r-xp 00000000 fd:00 3286                       /lib64/libc-2.12.so
7f4254d4d000-7f4254f4d000 ---p 0018a000 fd:00 3286                       /lib64/libc-2.12.so
7f4254f4d000-7f4254f51000 r--p 0018a000 fd:00 3286                       /lib64/libc-2.12.so
7f4254f51000-7f4254f52000 rw-p 0018e000 fd:00 3286                       /lib64/libc-2.12.so
7f4254f52000-7f4254f57000 rw-p 00000000 00:00 0
7f4254f57000-7f4254f59000 r-xp 00000000 fd:00 13342                      /lib64/libdl-2.12.so
7f4254f59000-7f4255159000 ---p 00002000 fd:00 13342                      /lib64/libdl-2.12.so
7f4255159000-7f425515a000 r--p 00002000 fd:00 13342                      /lib64/libdl-2.12.so
7f425515a000-7f425515b000 rw-p 00003000 fd:00 13342                      /lib64/libdl-2.12.so
7f425515b000-7f4255171000 r-xp 00000000 fd:00 21163                      /usr/java/jdk1.7.0_75/lib/amd64/jli/libjli.so
7f4255171000-7f4255371000 ---p 00016000 fd:00 21163                      /usr/java/jdk1.7.0_75/lib/amd64/jli/libjli.so
7f4255371000-7f4255372000 rw-p 00016000 fd:00 21163                      /usr/java/jdk1.7.0_75/lib/amd64/jli/libjli.so
7f4255372000-7f4255389000 r-xp 00000000 fd:00 3310                       /lib64/libpthread-2.12.so
7f4255389000-7f4255589000 ---p 00017000 fd:00 3310                       /lib64/libpthread-2.12.so
7f4255589000-7f425558a000 r--p 00017000 fd:00 3310                       /lib64/libpthread-2.12.so
7f425558a000-7f425558b000 rw-p 00018000 fd:00 3310                       /lib64/libpthread-2.12.so
7f425558b000-7f425558f000 rw-p 00000000 00:00 0
7f425558f000-7f42555af000 r-xp 00000000 fd:00 3277                       /lib64/ld-2.12.so
7f42555b0000-7f42555e2000 rw-p 00000000 00:00 0
7f42555e2000-7f4255698000 rw-p 00000000 00:00 0
7f4255698000-7f42556a0000 rw-s 00000000 fd:01 37                         /tmp/hsperfdata_elastic/28216
7f42556a0000-7f42556a3000 ---p 00000000 00:00 0
7f42556a3000-7f42557a5000 rw-p 00000000 00:00 0
7f42557a5000-7f42557a9000 r--s 0008b000 fd:00 20725                      /usr/java/jdk1.7.0_75/jre/lib/jsse.jar
7f42557a9000-7f42557ab000 r--s 00019000 fd:00 22406                      /usr/share/elasticsearch/lib/HdrHistogram-2.1.6.jar
7f42557ab000-7f42557ac000 rw-p 00000000 00:00 0
7f42557ac000-7f42557ad000 r--p 00000000 00:00 0
7f42557ad000-7f42557ae000 rw-p 00000000 00:00 0
7f42557ae000-7f42557af000 r--p 0001f000 fd:00 3277                       /lib64/ld-2.12.so
7f42557af000-7f42557b0000 rw-p 00020000 fd:00 3277                       /lib64/ld-2.12.so
7f42557b0000-7f42557b1000 rw-p 00000000 00:00 0
7fff3d014000-7fff3d029000 rw-p 00000000 00:00 0                          [stack]
7fff3d031000-7fff3d032000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Djna.tmpdir=/usr/share/elasticsearch/tmp -Djava.io.tmpdir=/usr/share/elasticsearch/tmp -Des.path.home=/usr/share/elasticsearch
java_command: org.elasticsearch.bootstrap.Elasticsearch start -p /var/run/elasticsearch/elasticsearch.pid -d -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.conf=/etc/elasticsearch
Launcher Type: SUN_STANDARD

Environment Variables:
PATH=/sbin:/usr/sbin:/bin:/usr/bin
SHELL=/bin/bash

Signal Handlers:
SIGSEGV: [libjvm.so+0x9a21f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGBUS: [libjvm.so+0x9a21f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGFPE: [libjvm.so+0x81ba40], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGPIPE: [libjvm.so+0x81ba40], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGXFSZ: [libjvm.so+0x81ba40], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGILL: [libjvm.so+0x81ba40], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGUSR1: SIG_DFL, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGUSR2: [libjvm.so+0x81d240], sa_mask[0]=0x00000000, sa_flags=0x10000004
SIGHUP: [libjvm.so+0x81e360], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGINT: SIG_IGN, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGTERM: [libjvm.so+0x81e360], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGQUIT: [libjvm.so+0x81e360], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004


---------------  S Y S T E M  ---------------

OS:CentOS release 6.7 (Final)

uname:Linux 2.6.32-573.7.1.el6.x86_64 #1 SMP Tue Sep 22 22:00:00 UTC 2015 x86_64
libc:glibc 2.12 NPTL 2.12
rlimit: STACK 10240k, CORE 0k, NPROC 1024, NOFILE 65535, AS infinity
load average:0.12 0.10 0.08

/proc/meminfo:
MemTotal:        3924284 kB
MemFree:         2475800 kB
Buffers:           95816 kB
Cached:          1058380 kB
SwapCached:         1656 kB
Active:           724236 kB
Inactive:         496232 kB
Active(anon):      52848 kB
Inactive(anon):    13496 kB
Active(file):     671388 kB
Inactive(file):   482736 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:       1048572 kB
SwapFree:        1015456 kB
Dirty:               272 kB
Writeback:             0 kB
AnonPages:         40536 kB
Mapped:            21068 kB
Shmem:                16 kB
Slab:             177536 kB
SReclaimable:     112616 kB
SUnreclaim:        64920 kB
KernelStack:        2880 kB
PageTables:         3948 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     3010712 kB
Committed_AS:     502800 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      150516 kB
VmallocChunk:   34359576444 kB
HardwareCorrupted:     0 kB
AnonHugePages:      6144 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:        8192 kB
DirectMap2M:     2088960 kB
DirectMap1G:     2097152 kB


CPU:total 4 (1 cores per cpu, 1 threads per core) family 6 model 63 stepping 2, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, aes, erms, tsc, tscinvbit

/proc/cpuinfo:
processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz
stepping    : 2
microcode   : 45
cpu MHz     : 3491.914
cache size  : 15360 KB
physical id : 0
siblings    : 1
core id     : 0
cpu cores   : 1
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts xtopology tsc_reliable nonstop_tsc aperfmperf unfair_spinlock pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm ida arat epb pln pts dts fsgsbase smep
bogomips    : 6983.82
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:

processor   : 1
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz
stepping    : 2
microcode   : 45
cpu MHz     : 3491.914
cache size  : 15360 KB
physical id : 2
siblings    : 1
core id     : 0
cpu cores   : 1
apicid      : 2
initial apicid  : 2
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts xtopology tsc_reliable nonstop_tsc aperfmperf unfair_spinlock pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm ida arat epb pln pts dts fsgsbase smep
bogomips    : 6983.82
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:

processor   : 2
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz
stepping    : 2
microcode   : 45
cpu MHz     : 3491.914
cache size  : 15360 KB
physical id : 4
siblings    : 1
core id     : 0
cpu cores   : 1
apicid      : 4
initial apicid  : 4
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts xtopology tsc_reliable nonstop_tsc aperfmperf unfair_spinlock pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm ida arat epb pln pts dts fsgsbase smep
bogomips    : 6983.82
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:

processor   : 3
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz
stepping    : 2
microcode   : 45
cpu MHz     : 3491.914
cache size  : 15360 KB
physical id : 6
siblings    : 1
core id     : 0
cpu cores   : 1
apicid      : 6
initial apicid  : 6
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts xtopology tsc_reliable nonstop_tsc aperfmperf unfair_spinlock pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm ida arat epb pln pts dts fsgsbase smep
bogomips    : 6983.82
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:



Memory: 4k page, physical 3924284k(2475800k free), swap 1048572k(1015456k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (24.75-b04) for linux-amd64 JRE (1.7.0_75-b13), built on Dec 18 2014 16:45:18 by "java_re" with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)

time: Wed May 11 10:18:58 2016
elapsed time: 0 seconds
```
</description><key id="154220256">18272</key><summary>Elasticsearch 2.2 fails to start with jna tmp dir configured (core dump)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fxh</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2016-05-11T11:25:31Z</created><updated>2017-04-15T22:20:50Z</updated><resolved>2016-07-23T00:37:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bubo77" created="2016-05-17T14:37:09Z" id="219738003">i have the same issue
https://github.com/elastic/elasticsearch/issues/18406
</comment><comment author="clintongormley" created="2016-05-20T09:40:20Z" id="220561677">This looks like a JNA problem.  See http://mail-archives.apache.org/mod_mbox/cassandra-user/201405.mbox/%3Cpan.2014.05.27.15.14.54@googlemail.com%3E

Try just setting the jna tmpdir to a tmp folder without noexec.
</comment><comment author="bubo77" created="2016-05-20T09:49:34Z" id="220563627">@clintongormley if you read this ticket and mine again you can see that we tried to point the ES config to a new directory with appropriate rights and it still doesn't work.

ls -alZ /var/lib/elasticsearch/
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       .
drwxr-xr-x. root          root          unconfined_u:object_r:usr_t:s0   ..
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       graylog2
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       tmp 
</comment><comment author="clintongormley" created="2016-05-20T11:00:04Z" id="220577299">Related to https://github.com/elastic/elasticsearch/issues/14372
</comment><comment author="jasontedor" created="2016-05-20T11:51:59Z" id="220586117">I've tried to reproduce this on Centos 6.7 and Fedora 23. It does not reproduce for me.

I notice that both error reports show death trying to register the C library. This the first step before we can even make any native calls. Thus, we are immediately dying here and it's not that we are interacting poorly with other native calls (e.g., `mlockall` or `seccomp`).

Is there additional configuration at play here that might be relevant? What else can you share about the system? What kernel do you have? I tested on `2.6.32-573.12.1.el6.x86_64` (Centos 6.7) and `4.2.3-300.fc23.x86_64` (Fedora 23). What's the output of `ldd --version`? What happens if you add `-Djna.nosys=false` to the `ES_JAVA_OPTS`?
</comment><comment author="fxh" created="2016-05-23T11:49:33Z" id="220958900">Kernel: 2.6.32-573.7.1.el6.x86_64 
CentOS release 6.7 (Final)

```
$ ldd --version
ldd (GNU libc) 2.12
Copyright (C) 2010 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
Written by Roland McGrath and Ulrich Drepper.
```

Adding `-Djna.nosys=false` did not help:

```
Starting elasticsearch: /usr/share/elasticsearch/bin/elasticsearch: line 155: 31386 Aborted                 (core dumped) exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS -Des.path.home="$ES_HOME" -cp "$ES_CLASSPATH" org.elasticsearch.bootstrap.Elasticsearch start "$@" 0&gt;&amp;-
```
</comment><comment author="bubo77" created="2016-05-23T13:18:33Z" id="220977496">CentOS Linux release 7.2.1511 (Core)
Kernel: 3.10.0-327.10.1.el7.x86_64

```
ldd (GNU libc) 2.17
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
Written by Roland McGrath and Ulrich Drepper
```

My tmp dir is on an LVM disk but it doesn't matter if the tmp dir is a symlink or not.

```
ls -alZ /export/apps/elasticsearch/
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       .
drwxr-xr-x. root          root          unconfined_u:object_r:usr_t:s0   ..
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       graylog2
drwxr-xr-x. elasticsearch elasticsearch system_u:object_r:usr_t:s0       tmp
```
</comment><comment author="jasontedor" created="2016-05-23T14:01:43Z" id="220988762">@fxh @bubo77 Thanks. Nothing alarming there. Can you add these system properties to `ES_JAVA_OPTS` (leave off the `-Djna.nosys=false`): 

```
-Djna.debug_load.jna=true -Djna.debug_load=true -Djna.tmpdir=/path/to/tmp -Djava.io.tmpdir=/path/to/tmp
```

and start Elasticsearch (only the first two should be new). This should produce output like:

```
Looking in classpath from sun.misc.Launcher$AppClassLoader@5c647e05 for /com/sun/jna/linux-x86-64/libjnidispatch.so
Found library resource at jar:file:/home/jason/elasticsearch/elasticsearch-5.0.0-SNAPSHOT/lib/jna-4.1.0.jar!/com/sun/jna/linux-x86-64/libjnidispatch.so
Trying /home/jason/elasticsearch/elasticsearch-5.0.0-SNAPSHOT/tmp/jna6458947537802950020.tmp
Found jnidispatch at /home/jason/elasticsearch/elasticsearch-5.0.0-SNAPSHOT/tmp/jna6458947537802950020.tmp
```

Can you share that and any additional JNA debugging output?
</comment><comment author="jasontedor" created="2016-05-23T15:17:36Z" id="221008050">Also, can you share your SELinux configuration via `sestatus` and `getsebool -a`? If you're willing to, can you _completely_ disable SELinux and try as well (preferably through `/etc/selinux/config` and a reboot)? If that starts up successfully, that will at least confirm my current hunch that it's SELinux and we just need to find the right configuration to let JNA do it's thing.
</comment><comment author="bubo77" created="2016-05-23T15:25:33Z" id="221009974">sestatus: 

```
tmp]# sestatus -v
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   enforcing
Mode from config file:          enforcing
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Max kernel policy version:      28

Process contexts:
Current context:                unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
Init context:                   system_u:system_r:init_t:s0
/usr/sbin/sshd                  system_u:system_r:sshd_t:s0-s0:c0.c1023

File contexts:
Controlling terminal:           unconfined_u:object_r:user_devpts_t:s0
/etc/passwd                     system_u:object_r:passwd_file_t:s0
/etc/shadow                     system_u:object_r:shadow_t:s0
/bin/bash                       system_u:object_r:shell_exec_t:s0
/bin/login                      system_u:object_r:login_exec_t:s0
/bin/sh                         system_u:object_r:bin_t:s0 -&gt; system_u:object_r:shell_exec_t:s0
/sbin/agetty                    system_u:object_r:getty_exec_t:s0
/sbin/init                      system_u:object_r:bin_t:s0 -&gt; system_u:object_r:init_exec_t:s0
/usr/sbin/sshd                  system_u:object_r:sshd_exec_t:s0
```

getsebool

```
abrt_anon_write --&gt; off
abrt_handle_event --&gt; off
abrt_upload_watch_anon_write --&gt; on
antivirus_can_scan_system --&gt; on
antivirus_use_jit --&gt; off
auditadm_exec_content --&gt; on
authlogin_nsswitch_use_ldap --&gt; off
authlogin_radius --&gt; off
authlogin_yubikey --&gt; off
awstats_purge_apache_log_files --&gt; off
boinc_execmem --&gt; on
cdrecord_read_content --&gt; off
cluster_can_network_connect --&gt; off
cluster_manage_all_files --&gt; off
cluster_use_execmem --&gt; off
cobbler_anon_write --&gt; off
cobbler_can_network_connect --&gt; off
cobbler_use_cifs --&gt; off
cobbler_use_nfs --&gt; off
collectd_tcp_network_connect --&gt; off
condor_tcp_network_connect --&gt; off
conman_can_network --&gt; off
cron_can_relabel --&gt; off
cron_system_cronjob_use_shares --&gt; off
cron_userdomain_transition --&gt; on
cups_execmem --&gt; off
cvs_read_shadow --&gt; off
daemons_dump_core --&gt; off
daemons_enable_cluster_mode --&gt; off
daemons_use_tcp_wrapper --&gt; off
daemons_use_tty --&gt; off
dbadm_exec_content --&gt; on
dbadm_manage_user_files --&gt; off
dbadm_read_user_files --&gt; off
deny_execmem --&gt; off
deny_ptrace --&gt; off
dhcpc_exec_iptables --&gt; off
dhcpd_use_ldap --&gt; off
domain_fd_use --&gt; on
domain_kernel_load_modules --&gt; off
entropyd_use_audio --&gt; on
exim_can_connect_db --&gt; off
exim_manage_user_files --&gt; off
exim_read_user_files --&gt; off
fcron_crond --&gt; off
fenced_can_network_connect --&gt; off
fenced_can_ssh --&gt; off
fips_mode --&gt; on
ftp_home_dir --&gt; off
ftpd_anon_write --&gt; off
ftpd_connect_all_unreserved --&gt; off
ftpd_connect_db --&gt; off
ftpd_full_access --&gt; off
ftpd_use_cifs --&gt; off
ftpd_use_fusefs --&gt; off
ftpd_use_nfs --&gt; off
ftpd_use_passive_mode --&gt; off
git_cgi_enable_homedirs --&gt; off
git_cgi_use_cifs --&gt; off
git_cgi_use_nfs --&gt; off
git_session_bind_all_unreserved_ports --&gt; off
git_session_users --&gt; off
git_system_enable_homedirs --&gt; off
git_system_use_cifs --&gt; off
git_system_use_nfs --&gt; off
gitosis_can_sendmail --&gt; off
glance_api_can_network --&gt; off
glance_use_execmem --&gt; off
glance_use_fusefs --&gt; off
global_ssp --&gt; off
gluster_anon_write --&gt; off
gluster_export_all_ro --&gt; off
gluster_export_all_rw --&gt; on
gpg_web_anon_write --&gt; off
gssd_read_tmp --&gt; on
guest_exec_content --&gt; on
haproxy_connect_any --&gt; off
httpd_anon_write --&gt; off
httpd_builtin_scripting --&gt; on
httpd_can_check_spam --&gt; off
httpd_can_connect_ftp --&gt; off
httpd_can_connect_ldap --&gt; off
httpd_can_connect_mythtv --&gt; off
httpd_can_connect_zabbix --&gt; off
httpd_can_network_connect --&gt; off
httpd_can_network_connect_cobbler --&gt; off
httpd_can_network_connect_db --&gt; off
httpd_can_network_memcache --&gt; off
httpd_can_network_relay --&gt; off
httpd_can_sendmail --&gt; off
httpd_dbus_avahi --&gt; off
httpd_dbus_sssd --&gt; off
httpd_dontaudit_search_dirs --&gt; off
httpd_enable_cgi --&gt; on
httpd_enable_ftp_server --&gt; off
httpd_enable_homedirs --&gt; off
httpd_execmem --&gt; off
httpd_graceful_shutdown --&gt; on
httpd_manage_ipa --&gt; off
httpd_mod_auth_ntlm_winbind --&gt; off
httpd_mod_auth_pam --&gt; off
httpd_read_user_content --&gt; off
httpd_run_ipa --&gt; off
httpd_run_preupgrade --&gt; off
httpd_run_stickshift --&gt; off
httpd_serve_cobbler_files --&gt; off
httpd_setrlimit --&gt; off
httpd_ssi_exec --&gt; off
httpd_sys_script_anon_write --&gt; off
httpd_tmp_exec --&gt; off
httpd_tty_comm --&gt; off
httpd_unified --&gt; off
httpd_use_cifs --&gt; off
httpd_use_fusefs --&gt; off
httpd_use_gpg --&gt; off
httpd_use_nfs --&gt; off
httpd_use_openstack --&gt; off
httpd_use_sasl --&gt; off
httpd_verify_dns --&gt; off
icecast_use_any_tcp_ports --&gt; off
irc_use_any_tcp_ports --&gt; off
irssi_use_full_network --&gt; off
kdumpgui_run_bootloader --&gt; off
kerberos_enabled --&gt; on
ksmtuned_use_cifs --&gt; off
ksmtuned_use_nfs --&gt; off
logadm_exec_content --&gt; on
logging_syslogd_can_sendmail --&gt; off
logging_syslogd_run_nagios_plugins --&gt; off
logging_syslogd_use_tty --&gt; on
login_console_enabled --&gt; on
logrotate_use_nfs --&gt; off
logwatch_can_network_connect_mail --&gt; off
lsmd_plugin_connect_any --&gt; off
mailman_use_fusefs --&gt; off
mcelog_client --&gt; off
mcelog_exec_scripts --&gt; on
mcelog_foreground --&gt; off
mcelog_server --&gt; off
minidlna_read_generic_user_content --&gt; off
mmap_low_allowed --&gt; off
mock_enable_homedirs --&gt; off
mount_anyfile --&gt; on
mozilla_plugin_bind_unreserved_ports --&gt; off
mozilla_plugin_can_network_connect --&gt; off
mozilla_plugin_use_bluejeans --&gt; off
mozilla_plugin_use_gps --&gt; off
mozilla_plugin_use_spice --&gt; off
mozilla_read_content --&gt; off
mpd_enable_homedirs --&gt; off
mpd_use_cifs --&gt; off
mpd_use_nfs --&gt; off
mplayer_execstack --&gt; off
mysql_connect_any --&gt; off
nagios_run_pnp4nagios --&gt; off
nagios_run_sudo --&gt; off
named_tcp_bind_http_port --&gt; off
named_write_master_zones --&gt; off
neutron_can_network --&gt; off
nfs_export_all_ro --&gt; on
nfs_export_all_rw --&gt; on
nfsd_anon_write --&gt; off
nis_enabled --&gt; off
nscd_use_shm --&gt; on
openshift_use_nfs --&gt; off
openvpn_can_network_connect --&gt; on
openvpn_enable_homedirs --&gt; on
openvpn_run_unconfined --&gt; off
pcp_bind_all_unreserved_ports --&gt; off
pcp_read_generic_logs --&gt; off
piranha_lvs_can_network_connect --&gt; off
polipo_connect_all_unreserved --&gt; off
polipo_session_bind_all_unreserved_ports --&gt; off
polipo_session_users --&gt; off
polipo_use_cifs --&gt; off
polipo_use_nfs --&gt; off
polyinstantiation_enabled --&gt; off
postfix_local_write_mail_spool --&gt; on
postgresql_can_rsync --&gt; off
postgresql_selinux_transmit_client_label --&gt; off
postgresql_selinux_unconfined_dbadm --&gt; on
postgresql_selinux_users_ddl --&gt; on
pppd_can_insmod --&gt; off
pppd_for_user --&gt; off
privoxy_connect_any --&gt; on
prosody_bind_http_port --&gt; off
puppetagent_manage_all_files --&gt; off
puppetmaster_use_db --&gt; off
racoon_read_shadow --&gt; off
rsync_anon_write --&gt; off
rsync_client --&gt; off
rsync_export_all_ro --&gt; off
rsync_full_access --&gt; off
samba_create_home_dirs --&gt; off
samba_domain_controller --&gt; off
samba_enable_home_dirs --&gt; off
samba_export_all_ro --&gt; off
samba_export_all_rw --&gt; off
samba_load_libgfapi --&gt; off
samba_portmapper --&gt; off
samba_run_unconfined --&gt; off
samba_share_fusefs --&gt; off
samba_share_nfs --&gt; off
sanlock_use_fusefs --&gt; off
sanlock_use_nfs --&gt; off
sanlock_use_samba --&gt; off
saslauthd_read_shadow --&gt; off
secadm_exec_content --&gt; on
secure_mode --&gt; off
secure_mode_insmod --&gt; off
secure_mode_policyload --&gt; off
selinuxuser_direct_dri_enabled --&gt; on
selinuxuser_execheap --&gt; off
selinuxuser_execmod --&gt; on
selinuxuser_execstack --&gt; on
selinuxuser_mysql_connect_enabled --&gt; off
selinuxuser_ping --&gt; on
selinuxuser_postgresql_connect_enabled --&gt; off
selinuxuser_rw_noexattrfile --&gt; on
selinuxuser_share_music --&gt; off
selinuxuser_tcp_server --&gt; off
selinuxuser_udp_server --&gt; off
selinuxuser_use_ssh_chroot --&gt; off
sftpd_anon_write --&gt; off
sftpd_enable_homedirs --&gt; off
sftpd_full_access --&gt; off
sftpd_write_ssh_home --&gt; off
sge_domain_can_network_connect --&gt; off
sge_use_nfs --&gt; off
smartmon_3ware --&gt; off
smbd_anon_write --&gt; off
spamassassin_can_network --&gt; off
spamd_enable_home_dirs --&gt; on
squid_connect_any --&gt; on
squid_use_tproxy --&gt; off
ssh_chroot_rw_homedirs --&gt; off
ssh_keysign --&gt; off
ssh_sysadm_login --&gt; off
staff_exec_content --&gt; on
staff_use_svirt --&gt; off
swift_can_network --&gt; off
sysadm_exec_content --&gt; on
telepathy_connect_all_ports --&gt; off
telepathy_tcp_connect_generic_network_ports --&gt; on
tftp_anon_write --&gt; off
tftp_home_dir --&gt; off
tmpreaper_use_nfs --&gt; off
tmpreaper_use_samba --&gt; off
tor_bind_all_unreserved_ports --&gt; off
tor_can_network_relay --&gt; off
unconfined_chrome_sandbox_transition --&gt; on
unconfined_login --&gt; on
unconfined_mozilla_plugin_transition --&gt; on
unprivuser_use_svirt --&gt; off
use_ecryptfs_home_dirs --&gt; off
use_fusefs_home_dirs --&gt; off
use_lpd_server --&gt; off
use_nfs_home_dirs --&gt; off
use_samba_home_dirs --&gt; off
user_exec_content --&gt; on
varnishd_connect_any --&gt; off
virt_read_qemu_ga_data --&gt; off
virt_rw_qemu_ga_data --&gt; off
virt_sandbox_use_all_caps --&gt; on
virt_sandbox_use_audit --&gt; on
virt_sandbox_use_mknod --&gt; off
virt_sandbox_use_netlink --&gt; off
virt_sandbox_use_nfs --&gt; off
virt_sandbox_use_samba --&gt; off
virt_sandbox_use_sys_admin --&gt; off
virt_transition_userdomain --&gt; off
virt_use_comm --&gt; off
virt_use_execmem --&gt; off
virt_use_fusefs --&gt; off
virt_use_nfs --&gt; off
virt_use_rawip --&gt; off
virt_use_samba --&gt; off
virt_use_sanlock --&gt; off
virt_use_usb --&gt; on
virt_use_xserver --&gt; off
webadm_manage_user_files --&gt; off
webadm_read_user_files --&gt; off
wine_mmap_zero_ignore --&gt; off
xdm_bind_vnc_tcp_port --&gt; off
xdm_exec_bootloader --&gt; off
xdm_sysadm_login --&gt; off
xdm_write_home --&gt; off
xen_use_nfs --&gt; off
xend_run_blktap --&gt; on
xend_run_qemu --&gt; on
xguest_connect_network --&gt; on
xguest_exec_content --&gt; on
xguest_mount_media --&gt; on
xguest_use_bluetooth --&gt; on
xserver_clients_write_xshm --&gt; off
xserver_execmem --&gt; off
xserver_object_manager --&gt; off
zabbix_can_network --&gt; off
zarafa_setrlimit --&gt; off
zebra_write_config --&gt; off
zoneminder_anon_write --&gt; off
zoneminder_run_sudo --&gt; off
```

systemctl dump:

```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f8c619d340a, pid=19921, tid=140242008348416
#
# JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [jna46830484485062808.tmp+0x1240a]  ffi_prep_closure_loc+0x1a
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

---------------  T H R E A D  ---------------

Current thread (0x00007f8c9c00a000):  JavaThread "main" [_thread_in_native, id=19939, stack(0x00007f8ca3071000,0x00007f8ca3172000)]

siginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x0000000000000000

Registers:
RAX=0x00007f8c619d38c2, RBX=0x00007f8ca316fb28, RCX=0x00007f8c9c5667f0, RDX=0x00007f8c619c8590
RSP=0x00007f8ca316f8c0, RBP=0x00007f8ca316f8c0, RSI=0x00007f8c9c566810, RDI=0x0000000000000000
R8 =0x00007f8c9c00ab60, R9 =0x0000000000000002, R10=0x00007f8ca316f650, R11=0x00007f8c619d33f0
R12=0x0000000000000000, R13=0x0000000000000008, R14=0x00007f8c9c5667f0, R15=0x0000000000000004
RIP=0x00007f8c619d340a, EFLAGS=0x0000000000010246, CSGSFS=0x0000000000000033, ERR=0x0000000000000006
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f8ca316f8c0)
0x00007f8ca316f8c0:   00007f8ca316f990 00007f8c619ce4dd
0x00007f8ca316f8d0:   00007f8ca316f950 00007f8ca316fb18
0x00007f8ca316f8e0:   00007f8ca316fb30 00007f8c9c00a1f8
0x00007f8ca316f8f0:   0000000100000000 00007f8c9c56b660
0x00007f8ca316f900:   00007f8c9c56b7a0 00007f8c9c566810
0x00007f8ca316f910:   00000001a316fa80 00007f8c9c566910
0x00007f8ca316f920:   00007f8c9c56a5b0 00007f8c9c56a590
0x00007f8ca316f930:   0000000000000001 00000003a316fb28
0x00007f8ca316f940:   00007f8ca316fb20 00007f8ca316fb18
0x00007f8ca316f950:   00007f8c00000000 00007f8c9c00ab60
0x00007f8ca316f960:   00007f8c7dcbcb78 00007f8c7dcbcb78
0x00007f8ca316f970:   0000000000000000 00007f8c7dcbcb78
0x00007f8ca316f980:   00007f8ca316fb30 00007f8c9c00a000
0x00007f8ca316f990:   00007f8ca316fa80 00007f8c8d0156d4
0x00007f8ca316f9a0:   00007f8ca316fb10 00007f8ca316fb08
0x00007f8ca316f9b0:   0000000000000002 00007f8c619d5eb0
0x00007f8ca316f9c0:   00007f8c619d5eb0 00007f8ca316fad8
0x00007f8ca316f9d0:   00007f8ca25ee0e0 0000000000000000
0x00007f8ca316f9e0:   0000000000000000 00007f8ca316fab0
0x00007f8ca316f9f0:   0000000000000000 00007f8ca316faa0
0x00007f8ca316fa00:   00007f8c9c00a000 00007f8c9c00a000
0x00007f8ca316fa10:   00007f8c7dcbcb78 0000000000000000
0x00007f8ca316fa20:   00007f8ca316fa80 00007f8c8d015459
0x00007f8ca316fa30:   00007f8c9c00a000 00007f8c8d015422
0x00007f8ca316fa40:   00007f8ca316fa40 00007f8c7dcbcb78
0x00007f8ca316fa50:   00007f8ca316fb30 00007f8c7dcc02d8
0x00007f8ca316fa60:   0000000000000000 00007f8c7dcbcb78
0x00007f8ca316fa70:   0000000000000000 00007f8ca316faa0
0x00007f8ca316fa80:   00007f8ca316fb88 00007f8c8d0074a0
0x00007f8ca316fa90:   00000006c14d96a8 00007f8c8d00f2b7
0x00007f8ca316faa0:   00000006c0010ad0 0000000000000000
0x00007f8ca316fab0:   00000006c15aa230 0000000000000000

Instructions: (pc=0x00007f8c619d340a)
0x00007f8c619d33ea:   66 90 66 66 66 90 8b 06 55 41 b9 02 00 00 00 48
0x00007f8c619d33fa:   89 e5 ff c8 83 f8 01 77 44 48 8b 05 c6 49 10 00
0x00007f8c619d340a:   66 c7 07 49 bb 4c 89 47 0c 66 c7 47 0a 49 ba 48
0x00007f8c619d341a:   89 47 02 8b 46 1c 48 89 77 18 48 89 57 20 48 89

Register to memory mapping:

RAX=0x00007f8c619d38c2: ffi_closure_unix64+0 in /export/apps/elasticsearch/tmp/jna46830484485062808.tmp at 0x00007f8c619c1000
RBX=0x00007f8ca316fb28 is pointing into the stack for thread: 0x00007f8c9c00a000
RCX=0x00007f8c9c5667f0 is an unknown value
RDX=0x00007f8c619c8590: &lt;offset 0x7590&gt; in /export/apps/elasticsearch/tmp/jna46830484485062808.tmp at 0x00007f8c619c1000
RSP=0x00007f8ca316f8c0 is pointing into the stack for thread: 0x00007f8c9c00a000
RBP=0x00007f8ca316f8c0 is pointing into the stack for thread: 0x00007f8c9c00a000
RSI=0x00007f8c9c566810 is an unknown value
RDI=0x0000000000000000 is an unknown value
R8 =0x00007f8c9c00ab60 is an unknown value
R9 =0x0000000000000002 is an unknown value
R10=0x00007f8ca316f650 is pointing into the stack for thread: 0x00007f8c9c00a000
R11=0x00007f8c619d33f0: ffi_prep_closure_loc+0 in /export/apps/elasticsearch/tmp/jna46830484485062808.tmp at 0x00007f8c619c1000
R12=0x0000000000000000 is an unknown value
R13=0x0000000000000008 is an unknown value
R14=0x00007f8c9c5667f0 is an unknown value
R15=0x0000000000000004 is an unknown value


Stack: [0x00007f8ca3071000,0x00007f8ca3172000],  sp=0x00007f8ca316f8c0,  free space=1018k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [jna46830484485062808.tmp+0x1240a]  ffi_prep_closure_loc+0x1a
C  [jna46830484485062808.tmp+0xd4dd]  Java_com_sun_jna_Native_registerMethod+0x45d
j  com.sun.jna.Native.registerMethod(Ljava/lang/Class;Ljava/lang/String;Ljava/lang/String;[I[J[JIJJLjava/lang/Class;JIZ[Lcom/sun/jna/ToNativeConverter;Lcom/sun/jna/FromNativeConverter;Ljava/lang/String;)J+0
j  com.sun.jna.Native.register(Ljava/lang/Class;Lcom/sun/jna/NativeLibrary;)V+979
j  com.sun.jna.Native.register(Ljava/lang/Class;Ljava/lang/String;)V+28
j  com.sun.jna.Native.register(Ljava/lang/String;)V+7
j  org.elasticsearch.bootstrap.JNACLibrary.&lt;clinit&gt;()V+44
v  ~StubRoutines::call_stub
V  [libjvm.so+0x681a26]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x1056
V  [libjvm.so+0x634fd7]  InstanceKlass::call_class_initializer_impl(instanceKlassHandle, Thread*)+0xd7
V  [libjvm.so+0x636c2c]  InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)+0x1ac
V  [libjvm.so+0x636fe1]  InstanceKlass::initialize(Thread*)+0x41
V  [libjvm.so+0x7e9756]  LinkResolver::resolve_static_call(CallInfo&amp;, KlassHandle&amp;, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)+0x246
V  [libjvm.so+0x7e99df]  LinkResolver::resolve_invokestatic(CallInfo&amp;, constantPoolHandle, int, Thread*)+0x23f
V  [libjvm.so+0x7e9f11]  LinkResolver::resolve_invoke(CallInfo&amp;, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)+0x4f1
V  [libjvm.so+0x67a972]  InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)+0x1b2
j  org.elasticsearch.bootstrap.JNANatives.definitelyRunningAsRoot()Z+8
j  org.elasticsearch.bootstrap.Natives.definitelyRunningAsRoot()Z+22
j  org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Ljava/nio/file/Path;ZZZ)V+7
j  org.elasticsearch.bootstrap.Bootstrap.setup(ZLorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/env/Environment;)V+43
j  org.elasticsearch.bootstrap.Bootstrap.init([Ljava/lang/String;)V+235
j  org.elasticsearch.bootstrap.Elasticsearch.main([Ljava/lang/String;)V+1
v  ~StubRoutines::call_stub
V  [libjvm.so+0x681a26]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x1056
V  [libjvm.so+0x6c3692]  jni_invoke_static(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)+0x362
V  [libjvm.so+0x6e009a]  jni_CallStaticVoidMethod+0x17a
C  [libjli.so+0x7bcc]  JavaMain+0x80c
C  [libpthread.so.0+0x7dc5]  start_thread+0xc5

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
j  com.sun.jna.Native.registerMethod(Ljava/lang/Class;Ljava/lang/String;Ljava/lang/String;[I[J[JIJJLjava/lang/Class;JIZ[Lcom/sun/jna/ToNativeConverter;Lcom/sun/jna/FromNativeConverter;Ljava/lang/String;)J+0
j  com.sun.jna.Native.register(Ljava/lang/Class;Lcom/sun/jna/NativeLibrary;)V+979
j  com.sun.jna.Native.register(Ljava/lang/Class;Ljava/lang/String;)V+28
j  com.sun.jna.Native.register(Ljava/lang/String;)V+7
j  org.elasticsearch.bootstrap.JNACLibrary.&lt;clinit&gt;()V+44
v  ~StubRoutines::call_stub
j  org.elasticsearch.bootstrap.JNANatives.definitelyRunningAsRoot()Z+8
j  org.elasticsearch.bootstrap.Natives.definitelyRunningAsRoot()Z+22
j  org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Ljava/nio/file/Path;ZZZ)V+7
j  org.elasticsearch.bootstrap.Bootstrap.setup(ZLorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/env/Environment;)V+43
j  org.elasticsearch.bootstrap.Bootstrap.init([Ljava/lang/String;)V+235
j  org.elasticsearch.bootstrap.Elasticsearch.main([Ljava/lang/String;)V+1
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
  0x00007f8c9c131000 JavaThread "Service Thread" daemon [_thread_blocked, id=19950, stack(0x00007f8c7cc1e000,0x00007f8c7cd1f000)]
  0x00007f8c9c109800 JavaThread "C1 CompilerThread1" daemon [_thread_blocked, id=19949, stack(0x00007f8c7cd1f000,0x00007f8c7ce20000)]
  0x00007f8c9c107800 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=19948, stack(0x00007f8c7ce20000,0x00007f8c7cf21000)]
  0x00007f8c9c105000 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=19947, stack(0x00007f8c7cf21000,0x00007f8c7d022000)]
  0x00007f8c9c103000 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=19946, stack(0x00007f8c7d022000,0x00007f8c7d123000)]
  0x00007f8c9c0d8800 JavaThread "Finalizer" daemon [_thread_blocked, id=19945, stack(0x00007f8c7d123000,0x00007f8c7d224000)]
  0x00007f8c9c0d6000 JavaThread "Reference Handler" daemon [_thread_blocked, id=19944, stack(0x00007f8c7d224000,0x00007f8c7d325000)]
=&gt;0x00007f8c9c00a000 JavaThread "main" [_thread_in_native, id=19939, stack(0x00007f8ca3071000,0x00007f8ca3172000)]

Other Threads:
  0x00007f8c9c0d1000 VMThread [stack: 0x00007f8c7d325000,0x00007f8c7d426000] [id=19943]
  0x00007f8c9c134000 WatcherThread [stack: 0x00007f8c7cb1d000,0x00007f8c7cc1e000] [id=19951]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap:
 par new generation   total 153344K, used 24554K [0x00000006c0000000, 0x00000006ca660000, 0x00000006ca660000)
  eden space 136320K,  18% used [0x00000006c0000000, 0x00000006c17fa9b8, 0x00000006c8520000)
  from space 17024K,   0% used [0x00000006c8520000, 0x00000006c8520000, 0x00000006c95c0000)
  to   space 17024K,   0% used [0x00000006c95c0000, 0x00000006c95c0000, 0x00000006ca660000)
 concurrent mark-sweep generation total 4023936K, used 0K [0x00000006ca660000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 8237K, capacity 8360K, committed 8576K, reserved 1056768K
  class space    used 998K, capacity 1040K, committed 1152K, reserved 1048576K

Card table byte_map: [0x00007f8c8c7ff000,0x00007f8c8d000000] byte_map_base: 0x00007f8c891ff000

Marking Bits: (CMSBitMap*) 0x00007f8c9c0499f8
 Bits: [0x00007f8c84299000, 0x00007f8c87fff800)

Mod Union Table: (CMSBitMap*) 0x00007f8c9c049ab8
 Bits: [0x00007f8c841a3000, 0x00007f8c842989a0)

Polling page: 0x00007f8ca317e000

CodeCache: size=245760Kb used=2282Kb max_used=2282Kb free=243477Kb
 bounds [0x00007f8c8d000000, 0x00007f8c8d270000, 0x00007f8c9c000000]
 total_blobs=900 nmethods=552 adapters=263
 compilation: enabled

Compilation events (10 events):
Event: 0.576 Thread 0x00007f8c9c109800  545   !   3       java.util.zip.ZipFile$ZipFileInflaterInputStream::close (67 bytes)
Event: 0.577 Thread 0x00007f8c9c109800 nmethod 545 0x00007f8c8d238450 code [0x00007f8c8d238620, 0x00007f8c8d238e58]
Event: 0.577 Thread 0x00007f8c9c109800  546       3       java.util.zip.InflaterInputStream::close (34 bytes)
Event: 0.577 Thread 0x00007f8c9c109800 nmethod 546 0x00007f8c8d239210 code [0x00007f8c8d2393a0, 0x00007f8c8d239698]
Event: 0.577 Thread 0x00007f8c9c109800  547       3       java.util.zip.ZipFile::access$100 (6 bytes)
Event: 0.577 Thread 0x00007f8c9c109800 nmethod 547 0x00007f8c8d239790 code [0x00007f8c8d239900, 0x00007f8c8d239a68]
Event: 0.577 Thread 0x00007f8c9c109800  548   !   3       java.util.zip.ZipFile::releaseInflater (41 bytes)
Event: 0.578 Thread 0x00007f8c9c109800 nmethod 548 0x00007f8c8d239b10 code [0x00007f8c8d239cc0, 0x00007f8c8d23a3f8]
Event: 0.578 Thread 0x00007f8c9c109800  549   !   3       java.util.zip.Inflater::reset (69 bytes)
Event: 0.578 Thread 0x00007f8c9c109800 nmethod 549 0x00007f8c8d23a710 code [0x00007f8c8d23a8a0, 0x00007f8c8d23acf8]

GC Heap History (0 events):
No events

Deoptimization events (2 events):
Event: 0.154 Thread 0x00007f8c9c00a000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f8c8d151b40 method=java.lang.String.indexOf(II)I @ 49
Event: 0.501 Thread 0x00007f8c9c00a000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f8c8d1f8274 method=sun.misc.MetaIndex.mayContain(Ljava/lang/String;)Z @ 38

Internal exceptions (10 events):
Event: 0.492 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/LayoutCustomizer&gt; (0x00000006c1385118) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.493 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/PatternLayoutCustomizer&gt; (0x00000006c1390fb8) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.506 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/DailyRollingFileAppenderBeanInfo&gt; (0x00000006c13cfdd0) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.506 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/FileAppenderBeanInfo&gt; (0x00000006c13d6ff8) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.506 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/WriterAppenderBeanInfo&gt; (0x00000006c13de3a0) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.506 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/AppenderSkeletonBeanInfo&gt; (0x00000006c13e6058) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.507 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/AppenderSkeletonCustomizer&gt; (0x00000006c13ee1e8) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.509 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/WriterAppenderCustomizer&gt; (0x00000006c1407818) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.511 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/FileAppenderCustomizer&gt; (0x00000006c141b250) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]
Event: 0.513 Thread 0x00007f8c9c00a000 Exception &lt;a 'java/lang/ClassNotFoundException': org/apache/log4j/DailyRollingFileAppenderCustomizer&gt; (0x00000006c14308c0) thrown at [/RE-WORK/workspace/8-2-build-linux-amd64/jdk8u45/3457/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 210]

Events (10 events):
Event: 0.574 loading class com/sun/jna/Function
Event: 0.574 loading class com/sun/jna/Function done
Event: 0.575 loading class com/sun/jna/MethodParameterContext
Event: 0.575 loading class com/sun/jna/MethodParameterContext done
Event: 0.575 loading class com/sun/jna/FunctionParameterContext
Event: 0.575 loading class com/sun/jna/FunctionParameterContext done
Event: 0.576 loading class com/sun/jna/MethodResultContext
Event: 0.576 loading class com/sun/jna/MethodResultContext done
Event: 0.576 loading class com/sun/jna/FunctionResultContext
Event: 0.576 loading class com/sun/jna/FunctionResultContext done


Dynamic libraries:
00400000-00401000 r-xp 00000000 fd:00 135193                             /usr/java/jre1.8.0_45/bin/java
00600000-00601000 rw-p 00000000 fd:00 135193                             /usr/java/jre1.8.0_45/bin/java
023b6000-023d7000 rw-p 00000000 00:00 0                                  [heap]
6c0000000-7c0120000 rw-p 00000000 00:00 0
7c0120000-800000000 ---p 00000000 00:00 0
7f8c4c000000-7f8c4c021000 rw-p 00000000 00:00 0
7f8c4c021000-7f8c50000000 ---p 00000000 00:00 0
7f8c50000000-7f8c50021000 rw-p 00000000 00:00 0
7f8c50021000-7f8c54000000 ---p 00000000 00:00 0
7f8c54000000-7f8c5423f000 rw-p 00000000 00:00 0
7f8c5423f000-7f8c58000000 ---p 00000000 00:00 0
7f8c58000000-7f8c581a4000 rw-p 00000000 00:00 0
7f8c581a4000-7f8c5c000000 ---p 00000000 00:00 0
7f8c5c000000-7f8c5c021000 rw-p 00000000 00:00 0
7f8c5c021000-7f8c60000000 ---p 00000000 00:00 0
7f8c619c1000-7f8c619d8000 r-xp 00000000 fd:05 131152                     /export/apps/elasticsearch/tmp/jna46830484485062808.tmp (deleted)
7f8c619d8000-7f8c61ad7000 ---p 00017000 fd:05 131152                     /export/apps/elasticsearch/tmp/jna46830484485062808.tmp (deleted)
7f8c61ad7000-7f8c61ad9000 rw-p 00016000 fd:05 131152                     /export/apps/elasticsearch/tmp/jna46830484485062808.tmp (deleted)
7f8c61ad9000-7f8c68000000 r--p 00000000 fd:00 4384                       /usr/lib/locale/locale-archive
7f8c68000000-7f8c68021000 rw-p 00000000 00:00 0
7f8c68021000-7f8c6c000000 ---p 00000000 00:00 0
7f8c6c000000-7f8c6c021000 rw-p 00000000 00:00 0
7f8c6c021000-7f8c70000000 ---p 00000000 00:00 0
7f8c70000000-7f8c70021000 rw-p 00000000 00:00 0
7f8c70021000-7f8c74000000 ---p 00000000 00:00 0
7f8c74000000-7f8c74021000 rw-p 00000000 00:00 0
7f8c74021000-7f8c78000000 ---p 00000000 00:00 0
7f8c78000000-7f8c78021000 rw-p 00000000 00:00 0
7f8c78021000-7f8c7c000000 ---p 00000000 00:00 0
7f8c7c0bb000-7f8c7c0bd000 rw-p 00000000 00:00 0
7f8c7c0bd000-7f8c7c0c5000 r-xp 00000000 fd:00 135250                     /usr/java/jre1.8.0_45/lib/amd64/libmanagement.so
7f8c7c0c5000-7f8c7c2c5000 ---p 00008000 fd:00 135250                     /usr/java/jre1.8.0_45/lib/amd64/libmanagement.so
7f8c7c2c5000-7f8c7c2c6000 rw-p 00008000 fd:00 135250                     /usr/java/jre1.8.0_45/lib/amd64/libmanagement.so
7f8c7c2c6000-7f8c7c2d1000 r--s 00210000 fd:00 135273                     /usr/java/jre1.8.0_45/lib/ext/localedata.jar
7f8c7c2d1000-7f8c7c2f3000 r--s 0038e000 fd:00 135392                     /usr/java/jre1.8.0_45/lib/ext/cldrdata.jar
7f8c7c2f3000-7f8c7c304000 r-xp 00000000 fd:00 135253                     /usr/java/jre1.8.0_45/lib/amd64/libnio.so
7f8c7c304000-7f8c7c503000 ---p 00011000 fd:00 135253                     /usr/java/jre1.8.0_45/lib/amd64/libnio.so
7f8c7c503000-7f8c7c504000 rw-p 00010000 fd:00 135253                     /usr/java/jre1.8.0_45/lib/amd64/libnio.so
7f8c7c504000-7f8c7c508000 r--s 00085000 fd:00 135516                     /usr/java/jre1.8.0_45/lib/jsse.jar
7f8c7c508000-7f8c7c808000 rw-p 00000000 00:00 0
7f8c7c808000-7f8c7c81e000 r-xp 00000000 fd:00 135252                     /usr/java/jre1.8.0_45/lib/amd64/libnet.so
7f8c7c81e000-7f8c7ca1e000 ---p 00016000 fd:00 135252                     /usr/java/jre1.8.0_45/lib/amd64/libnet.so
7f8c7ca1e000-7f8c7ca1f000 rw-p 00016000 fd:00 135252                     /usr/java/jre1.8.0_45/lib/amd64/libnet.so
7f8c7ca1f000-7f8c7ca23000 r--s 00018000 fd:00 138818                     /usr/share/elasticsearch/lib/compiler-0.8.13.jar
7f8c7ca23000-7f8c7ca4a000 r--s 0021c000 fd:00 138832                     /usr/share/elasticsearch/lib/lucene-core-5.5.0.jar
7f8c7ca4a000-7f8c7ca52000 r--s 00062000 fd:00 138831                     /usr/share/elasticsearch/lib/lucene-backward-codecs-5.5.0.jar
7f8c7ca52000-7f8c7ca5b000 r--s 00046000 fd:00 138826                     /usr/share/elasticsearch/lib/jackson-dataformat-yaml-2.6.2.jar
7f8c7ca5b000-7f8c7ca6c000 r--s 000b2000 fd:00 138830                     /usr/share/elasticsearch/lib/jts-1.13.jar
7f8c7ca6c000-7f8c7cb1d000 r--s 0084c000 fd:00 138820                     /usr/share/elasticsearch/lib/elasticsearch-2.3.2.jar
7f8c7cb1d000-7f8c7cb1e000 ---p 00000000 00:00 0
7f8c7cb1e000-7f8c7cc1e000 rw-p 00000000 00:00 0                          [stack:19951]
7f8c7cc1e000-7f8c7cc21000 ---p 00000000 00:00 0
7f8c7cc21000-7f8c7cd1f000 rw-p 00000000 00:00 0                          [stack:19950]
7f8c7cd1f000-7f8c7cd22000 ---p 00000000 00:00 0
7f8c7cd22000-7f8c7ce20000 rw-p 00000000 00:00 0                          [stack:19949]
7f8c7ce20000-7f8c7ce23000 ---p 00000000 00:00 0
7f8c7ce23000-7f8c7cf21000 rw-p 00000000 00:00 0                          [stack:19948]
7f8c7cf21000-7f8c7cf24000 ---p 00000000 00:00 0
7f8c7cf24000-7f8c7d022000 rw-p 00000000 00:00 0                          [stack:19947]
7f8c7d022000-7f8c7d025000 ---p 00000000 00:00 0
7f8c7d025000-7f8c7d123000 rw-p 00000000 00:00 0                          [stack:19946]
7f8c7d123000-7f8c7d126000 ---p 00000000 00:00 0
7f8c7d126000-7f8c7d224000 rw-p 00000000 00:00 0                          [stack:19945]
7f8c7d224000-7f8c7d227000 ---p 00000000 00:00 0
7f8c7d227000-7f8c7d325000 rw-p 00000000 00:00 0                          [stack:19944]
7f8c7d325000-7f8c7d326000 ---p 00000000 00:00 0
7f8c7d326000-7f8c7d426000 rw-p 00000000 00:00 0                          [stack:19943]
7f8c7d426000-7f8c7d5fe000 r--s 0341d000 fd:00 135563                     /usr/java/jre1.8.0_45/lib/rt.jar
7f8c7d5fe000-7f8c7dd3e000 rw-p 00000000 00:00 0
7f8c7dd3e000-7f8c7ddfe000 ---p 00000000 00:00 0
7f8c7ddfe000-7f8c7ddff000 ---p 00000000 00:00 0
7f8c7ddff000-7f8c80000000 rw-p 00000000 00:00 0                          [stack:19942]
7f8c80000000-7f8c80021000 rw-p 00000000 00:00 0
7f8c80021000-7f8c84000000 ---p 00000000 00:00 0
7f8c84000000-7f8c84002000 r--s 00019000 fd:00 135576                     /usr/share/elasticsearch/lib/HdrHistogram-2.1.6.jar
7f8c84002000-7f8c84019000 r--s 00100000 fd:00 138822                     /usr/share/elasticsearch/lib/hppc-0.7.1.jar
7f8c84019000-7f8c8402c000 r--s 0016f000 fd:00 135585                     /usr/share/elasticsearch/lib/lucene-analyzers-common-5.5.0.jar
7f8c8402c000-7f8c88000000 rw-p 00000000 00:00 0
7f8c88000000-7f8c88021000 rw-p 00000000 00:00 0
7f8c88021000-7f8c8c000000 ---p 00000000 00:00 0
7f8c8c000000-7f8c8c001000 rw-p 00000000 00:00 0
7f8c8c001000-7f8c8c005000 r--s 00020000 fd:00 138834                     /usr/share/elasticsearch/lib/lucene-highlighter-5.5.0.jar
7f8c8c005000-7f8c8c009000 r--s 00035000 fd:00 138840                     /usr/share/elasticsearch/lib/lucene-sandbox-5.5.0.jar
7f8c8c009000-7f8c8c00b000 r--s 0000e000 fd:00 138829                     /usr/share/elasticsearch/lib/jsr166e-1.1.0.jar
7f8c8c00b000-7f8c8c00e000 r--s 00018000 fd:00 138833                     /usr/share/elasticsearch/lib/lucene-grouping-5.5.0.jar
7f8c8c00e000-7f8c8c027000 r--s 0012c000 fd:00 138844                     /usr/share/elasticsearch/lib/netty-3.10.5.Final.jar
7f8c8c027000-7f8c8c052000 r--s 001fc000 fd:00 138821                     /usr/share/elasticsearch/lib/guava-18.0.jar
7f8c8c052000-7f8c8cfff000 rw-p 00000000 00:00 0
7f8c8cfff000-7f8c8d000000 rw-p 00000000 00:00 0
7f8c8d000000-7f8c8d270000 rwxp 00000000 00:00 0
7f8c8d270000-7f8c9c000000 ---p 00000000 00:00 0
7f8c9c000000-7f8c9c801000 rw-p 00000000 00:00 0
7f8c9c801000-7f8ca0000000 ---p 00000000 00:00 0
7f8ca0000000-7f8ca0002000 r--s 00008000 fd:00 135583                     /usr/share/elasticsearch/lib/joda-convert-1.2.jar
7f8ca0002000-7f8ca0008000 r--s 0003c000 fd:00 138846                     /usr/share/elasticsearch/lib/snakeyaml-1.15.jar
7f8ca0008000-7f8ca000c000 r--s 00027000 fd:00 138837                     /usr/share/elasticsearch/lib/lucene-misc-5.5.0.jar
7f8ca000c000-7f8ca0010000 r--s 0002d000 fd:00 138842                     /usr/share/elasticsearch/lib/lucene-spatial3d-5.5.0.jar
7f8ca0010000-7f8ca0015000 r--s 00038000 fd:00 138843                     /usr/share/elasticsearch/lib/lucene-suggest-5.5.0.jar
7f8ca0015000-7f8ca001a000 r--s 0003b000 fd:00 138841                     /usr/share/elasticsearch/lib/lucene-spatial-5.5.0.jar
7f8ca001a000-7f8ca026b000 rw-p 00000000 00:00 0
7f8ca026b000-7f8ca026c000 ---p 00000000 00:00 0
7f8ca026c000-7f8ca036c000 rw-p 00000000 00:00 0                          [stack:19941]
7f8ca036c000-7f8ca036d000 ---p 00000000 00:00 0
7f8ca036d000-7f8ca0477000 rw-p 00000000 00:00 0                          [stack:19940]
7f8ca0477000-7f8ca082d000 ---p 00000000 00:00 0
7f8ca082d000-7f8ca0847000 r-xp 00000000 fd:00 135266                     /usr/java/jre1.8.0_45/lib/amd64/libzip.so
7f8ca0847000-7f8ca0a47000 ---p 0001a000 fd:00 135266                     /usr/java/jre1.8.0_45/lib/amd64/libzip.so
7f8ca0a47000-7f8ca0a48000 rw-p 0001a000 fd:00 135266                     /usr/java/jre1.8.0_45/lib/amd64/libzip.so
7f8ca0a48000-7f8ca0a54000 r-xp 00000000 fd:00 4084                       /usr/lib64/libnss_files-2.17.so
7f8ca0a54000-7f8ca0c53000 ---p 0000c000 fd:00 4084                       /usr/lib64/libnss_files-2.17.so
7f8ca0c53000-7f8ca0c54000 r--p 0000b000 fd:00 4084                       /usr/lib64/libnss_files-2.17.so
7f8ca0c54000-7f8ca0c55000 rw-p 0000c000 fd:00 4084                       /usr/lib64/libnss_files-2.17.so
7f8ca0c55000-7f8ca0c5b000 rw-p 00000000 00:00 0
7f8ca0c5b000-7f8ca0c85000 r-xp 00000000 fd:00 135231                     /usr/java/jre1.8.0_45/lib/amd64/libjava.so
7f8ca0c85000-7f8ca0e85000 ---p 0002a000 fd:00 135231                     /usr/java/jre1.8.0_45/lib/amd64/libjava.so
7f8ca0e85000-7f8ca0e87000 rw-p 0002a000 fd:00 135231                     /usr/java/jre1.8.0_45/lib/amd64/libjava.so
7f8ca0e87000-7f8ca0e94000 r-xp 00000000 fd:00 135265                     /usr/java/jre1.8.0_45/lib/amd64/libverify.so
7f8ca0e94000-7f8ca1094000 ---p 0000d000 fd:00 135265                     /usr/java/jre1.8.0_45/lib/amd64/libverify.so
7f8ca1094000-7f8ca1096000 rw-p 0000d000 fd:00 135265                     /usr/java/jre1.8.0_45/lib/amd64/libverify.so
7f8ca1096000-7f8ca109d000 r-xp 00000000 fd:00 4096                       /usr/lib64/librt-2.17.so
7f8ca109d000-7f8ca129c000 ---p 00007000 fd:00 4096                       /usr/lib64/librt-2.17.so
7f8ca129c000-7f8ca129d000 r--p 00006000 fd:00 4096                       /usr/lib64/librt-2.17.so
7f8ca129d000-7f8ca129e000 rw-p 00007000 fd:00 4096                       /usr/lib64/librt-2.17.so
7f8ca129e000-7f8ca139f000 r-xp 00000000 fd:00 4074                       /usr/lib64/libm-2.17.so
7f8ca139f000-7f8ca159e000 ---p 00101000 fd:00 4074                       /usr/lib64/libm-2.17.so
7f8ca159e000-7f8ca159f000 r--p 00100000 fd:00 4074                       /usr/lib64/libm-2.17.so
7f8ca159f000-7f8ca15a0000 rw-p 00101000 fd:00 4074                       /usr/lib64/libm-2.17.so
7f8ca15a0000-7f8ca224e000 r-xp 00000000 fd:00 135270                     /usr/java/jre1.8.0_45/lib/amd64/server/libjvm.so
7f8ca224e000-7f8ca244d000 ---p 00cae000 fd:00 135270                     /usr/java/jre1.8.0_45/lib/amd64/server/libjvm.so
7f8ca244d000-7f8ca2524000 rw-p 00cad000 fd:00 135270                     /usr/java/jre1.8.0_45/lib/amd64/server/libjvm.so
7f8ca2524000-7f8ca2568000 rw-p 00000000 00:00 0
7f8ca2568000-7f8ca271e000 r-xp 00000000 fd:00 4066                       /usr/lib64/libc-2.17.so
7f8ca271e000-7f8ca291e000 ---p 001b6000 fd:00 4066                       /usr/lib64/libc-2.17.so
7f8ca291e000-7f8ca2922000 r--p 001b6000 fd:00 4066                       /usr/lib64/libc-2.17.so
7f8ca2922000-7f8ca2924000 rw-p 001ba000 fd:00 4066                       /usr/lib64/libc-2.17.so
7f8ca2924000-7f8ca2929000 rw-p 00000000 00:00 0
7f8ca2929000-7f8ca292c000 r-xp 00000000 fd:00 4072                       /usr/lib64/libdl-2.17.so
7f8ca292c000-7f8ca2b2b000 ---p 00003000 fd:00 4072                       /usr/lib64/libdl-2.17.so
7f8ca2b2b000-7f8ca2b2c000 r--p 00002000 fd:00 4072                       /usr/lib64/libdl-2.17.so
7f8ca2b2c000-7f8ca2b2d000 rw-p 00003000 fd:00 4072                       /usr/lib64/libdl-2.17.so
7f8ca2b2d000-7f8ca2b42000 r-xp 00000000 fd:00 135209                     /usr/java/jre1.8.0_45/lib/amd64/jli/libjli.so
7f8ca2b42000-7f8ca2d42000 ---p 00015000 fd:00 135209                     /usr/java/jre1.8.0_45/lib/amd64/jli/libjli.so
7f8ca2d42000-7f8ca2d43000 rw-p 00015000 fd:00 135209                     /usr/java/jre1.8.0_45/lib/amd64/jli/libjli.so
7f8ca2d43000-7f8ca2d59000 r-xp 00000000 fd:00 4092                       /usr/lib64/libpthread-2.17.so
7f8ca2d59000-7f8ca2f59000 ---p 00016000 fd:00 4092                       /usr/lib64/libpthread-2.17.so
7f8ca2f59000-7f8ca2f5a000 r--p 00016000 fd:00 4092                       /usr/lib64/libpthread-2.17.so
7f8ca2f5a000-7f8ca2f5b000 rw-p 00017000 fd:00 4092                       /usr/lib64/libpthread-2.17.so
7f8ca2f5b000-7f8ca2f5f000 rw-p 00000000 00:00 0
7f8ca2f5f000-7f8ca2f80000 r-xp 00000000 fd:00 4059                       /usr/lib64/ld-2.17.so
7f8ca2f80000-7f8ca2f82000 r--s 00011000 fd:00 138825                     /usr/share/elasticsearch/lib/jackson-dataformat-smile-2.6.2.jar
7f8ca2f82000-7f8ca2f84000 r--s 00007000 fd:00 138836                     /usr/share/elasticsearch/lib/lucene-memory-5.5.0.jar
7f8ca2f84000-7f8ca2f85000 r--s 00001000 fd:00 138845                     /usr/share/elasticsearch/lib/securesm-1.0.jar
7f8ca2f85000-7f8ca2f89000 r--s 000dc000 fd:00 138827                     /usr/share/elasticsearch/lib/jna-4.1.0.jar
7f8ca2f89000-7f8ca2f93000 r--s 00059000 fd:00 138839                     /usr/share/elasticsearch/lib/lucene-queryparser-5.5.0.jar
7f8ca2f93000-7f8ca2f95000 r--s 00012000 fd:00 138819                     /usr/share/elasticsearch/lib/compress-lzf-1.0.2.jar
7f8ca2f95000-7f8ca2f96000 r--s 0000c000 fd:00 135578                     /usr/share/elasticsearch/lib/commons-cli-1.3.1.jar
7f8ca2f96000-7f8ca2f9d000 r--s 00037000 fd:00 138838                     /usr/share/elasticsearch/lib/lucene-queries-5.5.0.jar
7f8ca2f9d000-7f8ca2fa5000 r--s 00066000 fd:00 138817                     /usr/share/elasticsearch/lib/apache-log4j-extras-1.2.17.jar
7f8ca2fa5000-7f8ca2fa8000 r--s 00022000 fd:00 138847                     /usr/share/elasticsearch/lib/spatial4j-0.5.jar
7f8ca2fa8000-7f8ca2fac000 r--s 0003c000 fd:00 138823                     /usr/share/elasticsearch/lib/jackson-core-2.6.2.jar
7f8ca2fac000-7f8ca2fbc000 r--s 00088000 fd:00 138828                     /usr/share/elasticsearch/lib/joda-time-2.8.2.jar
7f8ca2fbc000-7f8ca2fc5000 r--s 0006f000 fd:00 135584                     /usr/share/elasticsearch/lib/log4j-1.2.17.jar
7f8ca2fc5000-7f8ca3069000 rw-p 00000000 00:00 0
7f8ca3069000-7f8ca3071000 rw-s 00000000 fd:01 393218                     /tmp/hsperfdata_elasticsearch/19921
7f8ca3071000-7f8ca3074000 ---p 00000000 00:00 0
7f8ca3074000-7f8ca3176000 rw-p 00000000 00:00 0                          [stack:19939]
7f8ca3176000-7f8ca3178000 r--s 0000b000 fd:00 138848                     /usr/share/elasticsearch/lib/t-digest-3.0.jar
7f8ca3178000-7f8ca317c000 r--s 0001e000 fd:00 138835                     /usr/share/elasticsearch/lib/lucene-join-5.5.0.jar
7f8ca317c000-7f8ca317d000 r--s 0000b000 fd:00 138824                     /usr/share/elasticsearch/lib/jackson-dataformat-cbor-2.6.2.jar
7f8ca317d000-7f8ca317e000 rw-p 00000000 00:00 0
7f8ca317e000-7f8ca317f000 r--p 00000000 00:00 0
7f8ca317f000-7f8ca3180000 rw-p 00000000 00:00 0
7f8ca3180000-7f8ca3181000 r--p 00021000 fd:00 4059                       /usr/lib64/ld-2.17.so
7f8ca3181000-7f8ca3182000 rw-p 00022000 fd:00 4059                       /usr/lib64/ld-2.17.so
7f8ca3182000-7f8ca3183000 rw-p 00000000 00:00 0
7ffcb0189000-7ffcb01aa000 rw-p 00000000 00:00 0                          [stack]
7ffcb01fc000-7ffcb01fe000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Xms4g -Xmx4g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Djna.debug_load.jna=true -Djna.debug_load=true -Djava.io.tmpdir=/export/apps/elasticsearch/tmp -Djna.tmpdir=/export/apps/elasticsearch/tmp -Des.path.home=/usr/share/elasticsearch
java_command: org.elasticsearch.bootstrap.Elasticsearch start -Des.pidfile=/var/run/elasticsearch/elasticsearch.pid -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.conf=/etc/elasticsearch
java_class_path (initial): /usr/share/elasticsearch/lib/elasticsearch-2.3.2.jar:/usr/share/elasticsearch/lib/elasticsearch-2.3.2.jar:/usr/share/elasticsearch/lib/jackson-dataformat-cbor-2.6.2.jar:/usr/share/elasticsearch/lib/log4j-1.2.17.jar:/usr/share/elasticsearch/lib/lucene-join-5.5.0.jar:/usr/share/elasticsearch/lib/joda-time-2.8.2.jar:/usr/share/elasticsearch/lib/jackson-core-2.6.2.jar:/usr/share/elasticsearch/lib/t-digest-3.0.jar:/usr/share/elasticsearch/lib/spatial4j-0.5.jar:/usr/share/elasticsearch/lib/apache-log4j-extras-1.2.17.jar:/usr/share/elasticsearch/lib/lucene-queries-5.5.0.jar:/usr/share/elasticsearch/lib/commons-cli-1.3.1.jar:/usr/share/elasticsearch/lib/compress-lzf-1.0.2.jar:/usr/share/elasticsearch/lib/lucene-queryparser-5.5.0.jar:/usr/share/elasticsearch/lib/jna-4.1.0.jar:/usr/share/elasticsearch/lib/securesm-1.0.jar:/usr/share/elasticsearch/lib/lucene-spatial-5.5.0.jar:/usr/share/elasticsearch/lib/guava-18.0.jar:/usr/share/elasticsearch/lib/lucene-suggest-5.5.0.jar:/usr/share/elasticsearch/lib/lucene-memory-5.5.0.jar:/usr/share/elasticsearch/lib/jackson-dataformat-smile-2.6.2.jar:/usr/share/elasticsearch/lib/netty-3.10.5.Final.jar:/usr/share/elasticsearch/lib/lucene-spatial3d-5.5.0.jar:/usr/share/elasticsearch/lib/lucene-misc-5.5.0.jar:/usr/share/elasticsearch/lib/snakeyaml-1.15.jar:/usr/share/elasticsearch/lib/joda-convert-1.2.jar:/usr/share/elasticsearch/lib/lucene-grouping-5.5.0.jar:/usr/share/elasticsearch/lib/jsr166e-1.1.0.jar:/usr/share/elasticsearch/lib/lucene-analyzers-common-5.5.0.jar:/usr/share/elasticsearch/lib/hppc-0.7.1.jar:/usr/share/elasticsearch/lib/lucene-sandbox-5.5.0.jar:/usr/share/elasticsearch/lib/lucene-highlighter-5.5.0.jar:/usr/share/elasticsearch/lib/jts-1.13.jar:/usr/share/elasticsearch/lib/jackson-dataformat-yaml-2.6.2.jar:/usr/share/elasticsearch/lib/HdrHistogram-2.1.6.jar:/usr/share/elasticsearch/lib/lucene-backward-codecs-5.5.0.jar:/usr/share/elasticsearch/lib/lucene-core-5.5.0.jar:/usr/share/elasticsearch/lib/c
Launcher Type: SUN_STANDARD

Environment Variables:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin
SHELL=/sbin/nologin

Signal Handlers:
SIGSEGV: [libjvm.so+0xaad1e0], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGBUS: [libjvm.so+0xaad1e0], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGFPE: [libjvm.so+0x90b450], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGPIPE: [libjvm.so+0x90b450], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGXFSZ: [libjvm.so+0x90b450], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGILL: [libjvm.so+0x90b450], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGUSR1: SIG_DFL, sa_mask[0]=00000000000000000000000000000000, sa_flags=none
SIGUSR2: [libjvm.so+0x90ca90], sa_mask[0]=00000000000000000000000000000000, sa_flags=SA_RESTART|SA_SIGINFO
SIGHUP: [libjvm.so+0x90dde0], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGINT: [libjvm.so+0x90dde0], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGTERM: [libjvm.so+0x90dde0], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGQUIT: [libjvm.so+0x90dde0], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO


---------------  S Y S T E M  ---------------

OS:CentOS Linux release 7.2.1511 (Core)

uname:Linux 3.10.0-327.10.1.el7.x86_64 #1 SMP Tue Feb 16 17:03:50 UTC 2016 x86_64
libc:glibc 2.17 NPTL 2.17
rlimit: STACK 8192k, CORE 0k, NPROC 31854, NOFILE 65535, AS infinity
load average:0.25 0.43 0.29

/proc/meminfo:
MemTotal:        7912632 kB
MemFree:         1366836 kB
MemAvailable:    7015480 kB
Buffers:          116304 kB
Cached:          5265648 kB
SwapCached:         4928 kB
Active:          1525396 kB
Inactive:        4327224 kB
Active(anon):     270016 kB
Inactive(anon):   248668 kB
Active(file):    1255380 kB
Inactive(file):  4078556 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:        999420 kB
SwapFree:         935612 kB
Dirty:                72 kB
Writeback:             0 kB
AnonPages:        430132 kB
Mapped:            47044 kB
Shmem:             47952 kB
Slab:             607060 kB
SReclaimable:     568100 kB
SUnreclaim:        38960 kB
KernelStack:        3504 kB
PageTables:         8760 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     4955736 kB
Committed_AS:    5185924 kB
VmallocTotal:   34359738367 kB
VmallocUsed:       23160 kB
VmallocChunk:   34359710460 kB
HardwareCorrupted:     0 kB
AnonHugePages:    354304 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       43008 kB
DirectMap2M:     8345600 kB


CPU:total 2 (1 cores per cpu, 2 threads per core) family 6 model 63 stepping 2, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, lzcnt, ht, tsc, bmi1, bmi2

/proc/cpuinfo:
processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
stepping    : 2
microcode   : 0x2b
cpu MHz     : 2400.100
cache size  : 30720 KB
physical id : 0
siblings    : 2
core id     : 0
cpu cores   : 1
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt
bogomips    : 4800.20
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 1
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
stepping    : 2
microcode   : 0x2b
cpu MHz     : 2400.100
cache size  : 30720 KB
physical id : 0
siblings    : 2
core id     : 0
cpu cores   : 1
apicid      : 1
initial apicid  : 1
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt
bogomips    : 4800.20
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:



Memory: 4k page, physical 7912632k(1366836k free), swap 999420k(935612k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (25.45-b02) for linux-amd64 JRE (1.8.0_45-b14), built on Apr 10 2015 10:07:45 by "java_re" with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)

time: Mon May 23 15:16:48 2016
elapsed time: 0 seconds (0d 0h 0m 0s)
```
</comment><comment author="bubo77" created="2016-05-23T15:32:27Z" id="221011557">I've personally tried with setenforce 0 but nothing.
what is interesting that data dire and tmp points to the same directory and while data dir created and used without problem tmp cannot be for some reason
</comment><comment author="fxh" created="2016-05-23T16:18:04Z" id="221021459">```
[root@storagenode01 ~]# cat /etc/sysconfig/elasticsearch | grep ES_JAVA_OPTS
ES_JAVA_OPTS="-Djna.debug_load.jna=true -Djna.debug_load=true -Djna.tmpdir=/usr/share/elasticsearch/tmp -Djava.io.tmpdir=/usr/share/elasticsearch/tmp"
```

yields

```
[root@storagenode01 ~]# service elasticsearch restart
Stopping elasticsearch: [2016-05-23 16:07:35,615][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] stopping ...
[2016-05-23 16:07:35,776][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] stopped
[2016-05-23 16:07:35,776][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] closing ...
[2016-05-23 16:07:35,780][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] closed
                                                           [  OK  ]
Starting elasticsearch: /usr/share/elasticsearch/bin/elasticsearch: line 155:  2037 Aborted                 (core dumped) exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS -Des.path.home="$ES_HOME" -cp "$ES_CLASSPATH" org.elasticsearch.bootstrap.Elasticsearch start "$@" 0&gt;&amp;-
                                                           [FAILED]
[root@storagenode01 ~]#
```

furthermore as requested:

```
[root@storagenode01 ~]# sestatus
SELinux status:                 enabled
SELinuxfs mount:                /selinux
Current mode:                   enforcing
Mode from config file:          enforcing
Policy version:                 24
Policy from config file:        targeted
```

```
[root@storagenode01 ~]# getsebool -a
abrt_anon_write --&gt; off
abrt_handle_event --&gt; off
allow_console_login --&gt; on
allow_cvs_read_shadow --&gt; off
allow_daemons_dump_core --&gt; on
allow_daemons_use_tcp_wrapper --&gt; off
allow_daemons_use_tty --&gt; on
allow_domain_fd_use --&gt; on
allow_execheap --&gt; off
allow_execmem --&gt; on
allow_execmod --&gt; on
allow_execstack --&gt; on
allow_ftpd_anon_write --&gt; off
allow_ftpd_full_access --&gt; off
allow_ftpd_use_cifs --&gt; off
allow_ftpd_use_nfs --&gt; off
allow_gssd_read_tmp --&gt; on
allow_guest_exec_content --&gt; off
allow_httpd_anon_write --&gt; off
allow_httpd_mod_auth_ntlm_winbind --&gt; off
allow_httpd_mod_auth_pam --&gt; off
allow_httpd_sys_script_anon_write --&gt; off
allow_java_execstack --&gt; off
allow_kerberos --&gt; on
allow_mount_anyfile --&gt; on
allow_mplayer_execstack --&gt; off
allow_nsplugin_execmem --&gt; on
allow_polyinstantiation --&gt; off
allow_postfix_local_write_mail_spool --&gt; on
allow_ptrace --&gt; off
allow_rsync_anon_write --&gt; off
allow_saslauthd_read_shadow --&gt; off
allow_smbd_anon_write --&gt; off
allow_ssh_keysign --&gt; off
allow_staff_exec_content --&gt; on
allow_sysadm_exec_content --&gt; on
allow_unconfined_nsplugin_transition --&gt; off
allow_user_exec_content --&gt; on
allow_user_mysql_connect --&gt; off
allow_user_postgresql_connect --&gt; off
allow_write_xshm --&gt; off
allow_xguest_exec_content --&gt; off
allow_xserver_execmem --&gt; off
allow_ypbind --&gt; off
allow_zebra_write_config --&gt; on
antivirus_can_scan_system --&gt; off
antivirus_use_jit --&gt; off
authlogin_radius --&gt; off
authlogin_shadow --&gt; off
awstats_purge_apache_log_files --&gt; off
bacula_use_nfs --&gt; off
bacula_use_samba --&gt; off
cdrecord_read_content --&gt; off
cluster_can_network_connect --&gt; off
cluster_manage_all_files --&gt; on
cluster_use_execmem --&gt; off
cobbler_anon_write --&gt; off
cobbler_can_network_connect --&gt; off
cobbler_use_cifs --&gt; off
cobbler_use_nfs --&gt; off
collectd_tcp_network_connect --&gt; off
condor_domain_can_network_connect --&gt; off
cron_can_relabel --&gt; off
daemons_enable_cluster_mode --&gt; on
dhcpc_exec_iptables --&gt; off
domain_kernel_load_modules --&gt; off
exim_can_connect_db --&gt; off
exim_manage_user_files --&gt; off
exim_read_user_files --&gt; off
fcron_crond --&gt; off
fenced_can_network_connect --&gt; off
fenced_can_ssh --&gt; off
fips_mode --&gt; on
ftp_home_dir --&gt; off
ftpd_connect_db --&gt; off
ftpd_use_fusefs --&gt; off
ftpd_use_passive_mode --&gt; off
git_cgi_enable_homedirs --&gt; off
git_cgi_use_cifs --&gt; off
git_cgi_use_nfs --&gt; off
git_session_bind_all_unreserved_ports --&gt; off
git_session_users --&gt; off
git_system_enable_homedirs --&gt; off
git_system_use_cifs --&gt; off
git_system_use_nfs --&gt; off
global_ssp --&gt; off
gluster_anon_write --&gt; off
gluster_export_all_ro --&gt; off
gluster_export_all_rw --&gt; on
gpg_agent_env_file --&gt; off
gpg_web_anon_write --&gt; off
httpd_builtin_scripting --&gt; on
httpd_can_check_spam --&gt; off
httpd_can_network_connect --&gt; off
httpd_can_network_connect_cobbler --&gt; off
httpd_can_network_connect_db --&gt; off
httpd_can_network_memcache --&gt; off
httpd_can_network_relay --&gt; off
httpd_can_sendmail --&gt; off
httpd_dbus_avahi --&gt; on
httpd_dbus_sssd --&gt; off
httpd_enable_cgi --&gt; on
httpd_enable_ftp_server --&gt; off
httpd_enable_homedirs --&gt; off
httpd_execmem --&gt; off
httpd_manage_ipa --&gt; off
httpd_read_user_content --&gt; off
httpd_run_preupgrade --&gt; off
httpd_run_stickshift --&gt; off
httpd_serve_cobbler_files --&gt; off
httpd_setrlimit --&gt; off
httpd_ssi_exec --&gt; off
httpd_tmp_exec --&gt; off
httpd_tty_comm --&gt; on
httpd_unified --&gt; on
httpd_use_cifs --&gt; off
httpd_use_fusefs --&gt; off
httpd_use_gpg --&gt; off
httpd_use_nfs --&gt; off
httpd_use_openstack --&gt; off
httpd_verify_dns --&gt; off
icecast_connect_any --&gt; off
init_upstart --&gt; on
irssi_use_full_network --&gt; off
kdumpgui_run_bootloader --&gt; off
logging_syslog_can_read_tmp --&gt; off
logging_syslogd_can_sendmail --&gt; off
logging_syslogd_run_nagios_plugins --&gt; off
logging_syslogd_use_tty --&gt; on
logrotate_use_nfs --&gt; off
lsmd_plugin_connect_any --&gt; off
mcelog_foreground --&gt; off
mmap_low_allowed --&gt; off
mozilla_read_content --&gt; off
mysql_connect_any --&gt; off
nagios_run_sudo --&gt; off
named_bind_http_port --&gt; off
named_write_master_zones --&gt; off
ncftool_read_user_content --&gt; off
nscd_use_shm --&gt; on
nsplugin_can_network --&gt; on
openshift_use_nfs --&gt; off
openvpn_enable_homedirs --&gt; on
openvpn_run_unconfined --&gt; off
pcp_bind_all_unreserved_ports --&gt; off
piranha_lvs_can_network_connect --&gt; off
postgresql_can_rsync --&gt; off
pppd_can_insmod --&gt; off
pppd_for_user --&gt; off
privoxy_connect_any --&gt; on
puppet_manage_all_files --&gt; off
puppetmaster_use_db --&gt; off
qemu_full_network --&gt; on
qemu_use_cifs --&gt; on
qemu_use_comm --&gt; off
qemu_use_nfs --&gt; on
qemu_use_usb --&gt; on
racoon_read_shadow --&gt; off
rsync_client --&gt; off
rsync_export_all_ro --&gt; off
rsync_server --&gt; off
rsync_use_cifs --&gt; off
rsync_use_nfs --&gt; off
samba_create_home_dirs --&gt; off
samba_domain_controller --&gt; off
samba_enable_home_dirs --&gt; off
samba_export_all_ro --&gt; off
samba_export_all_rw --&gt; off
samba_load_libgfapi --&gt; off
samba_portmapper --&gt; off
samba_run_unconfined --&gt; off
samba_share_fusefs --&gt; off
samba_share_nfs --&gt; off
sanlock_use_fusefs --&gt; off
sanlock_use_nfs --&gt; off
sanlock_use_samba --&gt; off
secure_mode --&gt; off
secure_mode_insmod --&gt; off
secure_mode_policyload --&gt; off
sepgsql_enable_users_ddl --&gt; on
sepgsql_unconfined_dbadm --&gt; on
sge_domain_can_network_connect --&gt; off
sge_use_nfs --&gt; off
smartmon_3ware --&gt; off
spamassassin_can_network --&gt; off
spamd_enable_home_dirs --&gt; on
squid_connect_any --&gt; on
squid_use_tproxy --&gt; off
ssh_chroot_full_access --&gt; off
ssh_chroot_manage_apache_content --&gt; off
ssh_chroot_rw_homedirs --&gt; off
ssh_sysadm_login --&gt; off
swift_can_network --&gt; off
telepathy_tcp_connect_generic_network_ports --&gt; off
tftp_anon_write --&gt; off
tftp_use_cifs --&gt; off
tftp_use_nfs --&gt; off
tor_bind_all_unreserved_ports --&gt; off
unconfined_login --&gt; on
unconfined_mmap_zero_ignore --&gt; off
unconfined_mozilla_plugin_transition --&gt; off
use_fusefs_home_dirs --&gt; off
use_lpd_server --&gt; off
use_nfs_home_dirs --&gt; on
use_samba_home_dirs --&gt; off
user_direct_dri --&gt; on
user_direct_mouse --&gt; off
user_ping --&gt; on
user_rw_noexattrfile --&gt; on
user_setrlimit --&gt; on
user_tcp_server --&gt; off
user_ttyfile_stat --&gt; off
varnishd_connect_any --&gt; off
vbetool_mmap_zero_ignore --&gt; off
virt_use_comm --&gt; off
virt_use_execmem --&gt; off
virt_use_fusefs --&gt; off
virt_use_nfs --&gt; off
virt_use_samba --&gt; off
virt_use_sanlock --&gt; off
virt_use_sysfs --&gt; on
virt_use_usb --&gt; on
virt_use_xserver --&gt; off
webadm_manage_user_files --&gt; off
webadm_read_user_files --&gt; off
wine_mmap_zero_ignore --&gt; off
xdm_exec_bootloader --&gt; off
xdm_sysadm_login --&gt; off
xen_use_nfs --&gt; off
xguest_connect_network --&gt; on
xguest_mount_media --&gt; on
xguest_use_bluetooth --&gt; on
xserver_object_manager --&gt; off
zabbix_can_network --&gt; off
[root@storagenode01 ~]#
```

I'll disable SELinux and reboot later tonight and let you know the outcome. Thanks for your investigations.
</comment><comment author="jasontedor" created="2016-05-23T17:50:24Z" id="221044475">Poking around some more, the segmentation fault is happening in `libffi`. But JNA includes their own `libffi` instead of dynamically linking to the system `libffi`. There are some changes in the offending method (`ffi_prep_closure_loc`) and related methods in their integrated `libffi` from 4.1.0 (the version of JNA that we ship with) to 4.2.2 (the latest version of JNA). Can you also try with [jna-4.2.2](http://repo1.maven.org/maven2/net/java/dev/jna/jna/4.2.2/)? Just remove the jna-4.1.0.jar in the Elasticsearch lib folder and add the jna-4.2.2.jar.
</comment><comment author="fxh" created="2016-05-23T19:41:23Z" id="221073862">same result with jna-4.2.2.jar:

```
[root@storagenode01 lib]# service elasticsearch restart
Stopping elasticsearch: [2016-05-23 19:22:08,554][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] stopping ...
[2016-05-23 19:22:08,626][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] stopped
[2016-05-23 19:22:08,627][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] closing ...
[2016-05-23 19:22:08,630][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] closed
                                                           [  OK  ]
Starting elasticsearch: /usr/share/elasticsearch/bin/elasticsearch: line 155:  4395 Aborted                 (core dumped) exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS -Des.path.home="$ES_HOME" -cp "$ES_CLASSPATH" org.elasticsearch.bootstrap.Elasticsearch start "$@" 0&gt;&amp;-
                                                           [FAILED]
[root@storagenode01 lib]#
```

regarding SELinux, I disabled it (via /etc/selinux/config and reboot):

```
[root@storagenode01 ~]# sestatus -v
SELinux status:                 disabled
[root@storagenode01 ~]#
```

and this seems to work:

```
[root@storagenode01 ~]# service elasticsearch restart
Stopping elasticsearch: [2016-05-23 19:36:04,135][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] stopping ...
[2016-05-23 19:36:04,233][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] stopped
[2016-05-23 19:36:04,233][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] closing ...
[2016-05-23 19:36:04,238][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] closed
                                                           [  OK  ]
Starting elasticsearch: [2016-05-23 19:36:05,629][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed
[2016-05-23 19:36:06,817][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] version[2.2.0], pid[17611], build[3eeb1f1/2016-05-10T14:33:28Z]
[2016-05-23 19:36:06,817][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] initializing ...
[2016-05-23 19:36:07,282][INFO ][plugins                  ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] modules [lang-groovy, lang-expression], plugins [analysis-smartcn, analysis-kuromoji, delete-by-query, head, analysis-icu], sites [head]
[2016-05-23 19:36:07,297][INFO ][env                      ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] using [1] data paths, mounts [[/var (/dev/mapper/vg_root-var)]], net usable_space [456.9gb], net total_space [481.9gb], spins? [possibly], types [ext4]
[2016-05-23 19:36:07,297][INFO ][env                      ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] heap size [247.5mb], compressed ordinary object pointers [true]
[2016-05-23 19:36:08,305][INFO ][script                   ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] compiling script file [/etc/elasticsearch/scripts/has-multi-value.groovy]
[2016-05-23 19:36:08,923][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] initialized
[2016-05-23 19:36:08,923][INFO ][node                     ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] starting ...
[2016-05-23 19:36:08,963][INFO ][transport                ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {172.16.212.130:9300}, {[fe80::20c:29ff:fe7c:ec79]:9300}
[2016-05-23 19:36:08,970][INFO ][discovery                ] [squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7-node01] squirro-ce793b21-a7fd-400f-8b22-42ffbf5d6fd7/TzKoQtmaTKCneN6zKBlSEg
                                                           [  OK  ]
[root@storagenode01 ~]#
```
</comment><comment author="jasontedor" created="2016-05-23T20:39:20Z" id="221089248">Ah, so if I'm reading you correctly, it appears my hunch that it is SELinux is correct! Okay, I'll see if I can take your SELinux configuration somewhere then. Thanks for reporting back.
</comment><comment author="jasontedor" created="2016-05-24T01:38:57Z" id="221143917">@fxh Can you share the output of `cat /proc/mounts` and `ls -alZ /usr/share/elasticsearch`?
</comment><comment author="fxh" created="2016-05-24T06:36:29Z" id="221179206">```
[felix@storagenode01 ~]$ cat /proc/mounts
rootfs / rootfs rw 0 0
proc /proc proc rw,relatime 0 0
sysfs /sys sysfs rw,seclabel,relatime 0 0
devtmpfs /dev devtmpfs rw,seclabel,relatime,size=1946820k,nr_inodes=486705,mode=755 0 0
devpts /dev/pts devpts rw,seclabel,relatime,gid=5,mode=620,ptmxmode=000 0 0
tmpfs /dev/shm tmpfs rw,seclabel,nosuid,nodev,noexec,relatime 0 0
/dev/mapper/vg_root-root / ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0
none /selinux selinuxfs rw,relatime 0 0
devtmpfs /dev devtmpfs rw,seclabel,relatime,size=1946820k,nr_inodes=486705,mode=755 0 0
/proc/bus/usb /proc/bus/usb usbfs rw,relatime 0 0
/dev/sda1 /boot ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0
/dev/mapper/vg_root-home /home ext4 rw,seclabel,nodev,relatime,barrier=1,data=ordered 0 0
/dev/mapper/vg_root-tmp /tmp ext4 rw,seclabel,nosuid,nodev,noexec,relatime,barrier=1,data=ordered 0 0
/dev/mapper/vg_root-var /var ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0
/dev/mapper/vg_root-log /var/log ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0
/dev/mapper/vg_root-audit /var/log/audit ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0
/dev/mapper/vg_root-tmp /var/tmp ext4 rw,seclabel,nosuid,nodev,noexec,relatime,barrier=1,data=ordered 0 0
none /proc/sys/fs/binfmt_misc binfmt_misc rw,relatime 0 0
```

and 

```
[felix@storagenode01 ~]$ ls -alZ /usr/share/elasticsearch
drwxrwxr-x. root    root    system_u:object_r:usr_t:s0       .
drwxr-xr-x. root    root    system_u:object_r:usr_t:s0       ..
-rw-------. elastic elastic system_u:object_r:usr_t:s0       .bash_history
drwxrwxr-x. root    root    system_u:object_r:bin_t:s0       bin
drwxr-xr-x. root    root    system_u:object_r:usr_t:s0       config
drwxr-xr-x. root    root    system_u:object_r:usr_t:s0       data
drwxrwxr-x. root    root    system_u:object_r:lib_t:s0       lib
-rw-r--r--. root    root    system_u:object_r:usr_t:s0       LICENSE.txt
drwxr-xr-x. root    root    system_u:object_r:usr_t:s0       logs
drwxrwxr-x. root    root    system_u:object_r:usr_t:s0       modules
-rw-r--r--. root    root    system_u:object_r:usr_t:s0       NOTICE.txt
drwxrwxr-x. root    root    system_u:object_r:usr_t:s0       plugins
-rw-r--r--. root    root    system_u:object_r:usr_t:s0       README.textile
drwxr-xr-x. elastic elastic system_u:object_r:usr_t:s0       tmp
[felix@storagenode01 ~]$
```
</comment><comment author="olenm" created="2016-06-06T20:04:14Z" id="224071570">I have noticed I am able to install and successfully run ES 2.1.1 on a CoS 7.2 system with SELinux enabled and noexec on /tmp (without using java opts -Djna.tmpdir=  nor -Djava.io.tmpdir= 

2.1.2 and above (tested on the latest 2.3.3) crash with a similar dump mentioned by bubo77, both with and without the tmpdirs specified. 

Also using: jdk1.8.0_45-1.8.0_45-fcs.x86_64
</comment><comment author="jasontedor" created="2016-06-06T20:07:16Z" id="224072370">@olenm And if you disable SELinux, does Elasticsearch startup successfully?
</comment><comment author="olenm" created="2016-06-06T20:19:52Z" id="224075595">with SELinux disabled it does not startup successfully (just hit the reboot). forgot to mention in my first post (and more importantly) I have noexec on /tmp

EDIT: and I have verified removing the noexec on /tmp does NOT resolve the issue for 2.3.3 (if needed I can test this again on 2.1.2) - it takes a while longer to fail
</comment><comment author="jasontedor" created="2016-06-06T20:28:57Z" id="224078020">@olenm It's never going to work if `/tmp` is `noexec`, JNA loads native libraries from there. This is the reason that `jna.tmpdir` exists. Can you try with SELinux disabled and either remove `noexec` from the mount options for `/tmp`, or point `jna.tmpdir` somewhere else?
</comment><comment author="olenm" created="2016-06-06T20:48:25Z" id="224083433">ran a few tests:

for ES 2.1.1 with noexec on /tmp (selinux still disabled): using no `jna.tmpdir` I do not see any crashing with my current configs

with ES 2.3.3 with noexec on /tmp (selinux disabled):  
- set `jna.tmpdir` to a folder with `chmod 0777` 
- `-Djna.tmpdir=/test` within `/ect/sysconfig/easticsearch` 's ES_JAVA_OPTS

I am seeing an entry I have not before through journalctl: plugin cloud-aws is the culprit; yanking the cloud-aws and removing its section from the config for it did yield in a successful start of ES 2.3.3
</comment><comment author="jasontedor" created="2016-06-06T20:56:52Z" id="224085872">&gt; I am seeing an entry I have not before through journalctl: plugin cloud-aws is the culprit

@olenm Can you share the log message?
</comment><comment author="olenm" created="2016-06-06T21:08:19Z" id="224088963">`Exception in thread "main" java.lang.IllegalArgumentException: Plugin [cloud-aws] is incompatible with Elasticsearch [2.3.3]. Was designed for version [2.1.1]`

(and before it appears to be overlooked with the combination the tmp noexec)
</comment><comment author="jasontedor" created="2016-06-06T21:14:11Z" id="224090480">&gt; `Exception in thread "main" java.lang.IllegalArgumentException: Plugin [cloud-aws] is incompatible with Elasticsearch [2.3.3]. Was designed for version [2.1.1]`
&gt; 
&gt; (and before it appears to be overlooked with the combination the tmp noexec)

Thanks, that's unrelated to the issue here. The plugins are not loaded until node during startup, which happens after bootstrap where JNA is loaded.
</comment><comment author="jasontedor" created="2016-06-06T21:30:57Z" id="224094671">&gt; for ES 2.1.1 with noexec on /tmp (selinux still disabled): using no `jna.tmpdir` I do not see any crashing with my current configs

@olenm Right, crashing is not the expected behavior with `noexec` on `/tmp`, just that JNA will be useless. You should see log messages saying that `unable to load JNA native support library, native methods will be disabled` and a non-fatal `UnsatisfiedLinkError` exception in the logs.

Both of your experiments seem to continue to support the earlier hypothesis that there is a poor interaction between JNA and SELinux occurring here.
</comment><comment author="olenm" created="2016-06-06T22:47:28Z" id="224112223">Yeah sorry about the user error (with the proper version of that plugin installed and back to selinux + noexec on /tmp I did see the same issue related above and no additional plugin errors). 

EDIT: 2.1.1 did not log any issues in journalctl - but did log that the JNA would not be loaded in the flat log file
</comment><comment author="olenm" created="2016-06-07T01:05:59Z" id="224133987">@jasontedor I've played a bit more with this; only using ES 2.1.1 on CoS7.2 .. I'm editing this comment making it a bit cleaner (or may post again)
</comment><comment author="jasontedor" created="2016-06-07T01:49:38Z" id="224140253">@olenm I do not think that it's just `noexec`:

``` bash
[vagrant@localhost ~]$ cat /etc/fstab | grep tmp
tmpfs   /tmp    tmpfs   defaults,nosuid,noexec,nodev    0   0
[vagrant@localhost ~]$ findmnt --target /tmp
TARGET SOURCE FSTYPE OPTIONS
/tmp   tmpfs  tmpfs  rw,nosuid,nodev,noexec,relatime,seclabel
[vagrant@localhost ~]$ /usr/share/elasticsearch/bin/elasticsearch --path.conf config --path.logs logs --path.data data
[2016-04-27 09:45:45,070][WARN ][bootstrap                ] unable to load JNA native support library, native methods will be disabled.
java.lang.UnsatisfiedLinkError: /tmp/jna-226895185/jna7321475784156807333.tmp: /tmp/jna-226895185/jna7321475784156807333.tmp: failed to map segment from shared object
    at java.lang.ClassLoader$NativeLibrary.load(Native Method)
    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1938)
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1821)
    at java.lang.Runtime.load0(Runtime.java:809)
    at java.lang.System.load(System.java:1086)
    at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:761)
    at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:736)
    at com.sun.jna.Native.&lt;clinit&gt;(Native.java:131)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45)
    at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:89)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2016-04-27 09:45:45,079][WARN ][bootstrap                ] cannot check if running as root because JNA is not available
[2016-04-27 09:45:45,080][WARN ][bootstrap                ] cannot install syscall filters because JNA is not available
[2016-04-27 09:45:45,080][WARN ][bootstrap                ] cannot register console handler because JNA is not available
[2016-04-27 09:45:45,371][INFO ][node                     ] [Hardcore] version[2.1.1], pid[700], build[40e2c53/2015-12-15T13:05:55Z]
[2016-04-27 09:45:45,372][INFO ][node                     ] [Hardcore] initializing ...
[2016-04-27 09:45:45,454][INFO ][plugins                  ] [Hardcore] loaded [], sites []
[2016-04-27 09:45:45,502][INFO ][env                      ] [Hardcore] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [35.9gb], net total_space [39.2gb], spins? [possibly], types [ext4]
[2016-04-27 09:45:47,923][INFO ][node                     ] [Hardcore] initialized
[2016-04-27 09:45:47,923][INFO ][node                     ] [Hardcore] starting ...
[2016-04-27 09:45:48,006][INFO ][transport                ] [Hardcore] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
[2016-04-27 09:45:48,023][INFO ][discovery                ] [Hardcore] elasticsearch/D1wQ0Y29QoSoLibbluvM1Q
[2016-04-27 09:45:51,095][INFO ][cluster.service          ] [Hardcore] new_master {Hardcore}{D1wQ0Y29QoSoLibbluvM1Q}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-04-27 09:45:51,120][INFO ][http                     ] [Hardcore] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200}
[2016-04-27 09:45:51,121][INFO ][node                     ] [Hardcore] started
[2016-04-27 09:45:51,164][INFO ][gateway                  ] [Hardcore] recovered [0] indices into cluster_state
^C[2016-04-27 09:45:53,253][INFO ][node                     ] [Hardcore] stopping ...
[2016-04-27 09:45:53,272][INFO ][node                     ] [Hardcore] stopped
[2016-04-27 09:45:53,272][INFO ][node                     ] [Hardcore] closing ...
[2016-04-27 09:45:53,283][INFO ][node                     ] [Hardcore] closed
```

&gt; fstab tmp line: `none /dev/shm   tmpfs      defaults,nosuid,noexec,nodev     0 0`

Note that `/dev/shm` is not the same as `/tmp`.

&gt; the ES log warnings/errors and /tmp/hs_dumpfiles were not resultant of the selinux policy; only when enabling noexec on /tmp did the dumpfile occur.

I expect log messages as above when `/tmp` is mounted with `noexec`.

&gt; with selinux enabled (and I believe disabled/permissive as well),  noexec disable,  and excluding `jna.tmpdir`, the ES logfile would output `/tmp/jna--&lt;##&gt;/jna&lt;##&gt;.tmp: failed to map segment from shared object: Operation not permitted`

That looks like SELinux. Are you sure that you got a message stating "failed to map segment from shared object: Operation not permitted" with SELinux disabled or in permissive mode? Note that you you'll have to reboot if you switched between SELinux being disabled and enabled.
</comment><comment author="olenm" created="2016-06-07T04:15:14Z" id="224170386">That is exactly what I get for only grep'ing and sed'ing the fstab for noexec - when I should have nuked the systemd tmp.mount tmpfs service (I never seem to remember this until a bit of pain occurs).
Essentially massive user-error on that noexec part (and the plugin part from earlier) (sorry about that!)

I went overkill on cleaning up my scenarios (and manually testing them vs any automation tools) - surprisingly and happily I encountered the crash-dump once (and only with ES 2.1.1)) 
- (I use 2.1.1 in this example as previously I have RHEL6 system using it with selinux but no noexec on tmp).

The same conditions that caused the crash in ES 2.1.1 did not cause a crash in ES 2.3.3 - so I am guessing (since the thread started with 2.2) that all is well when using the tmpdir option and all plugins load properly.

As for the scenarios, here they are (they reiterate what you have said on the expected noexec messages and I have double checked on that `Operation not permitted` message):

Constants in all scenarios run:
- plugin included cloud-aws
- changed the systemd unit file enabling `LimitMEMLOCK=infinity`
- # otherwise causes a memlock warning (on initial tests)

relative fstab tmp lines:
1)  /tmp /var/tmp none bind 0 0
2)  none /dev/shm   tmpfs      defaults,nosuid,noexec,nodev     0 0

case 1:
- selinux enforce
- fstab includes 2 tmp lines, systemd tmpfs tmp.mount enabled
- tmp.mount unit-file enabled
- `jna.tmpdir` not set
- # ES 2.1.1 WARN `/tmp/jna--&lt;##&gt;/jna&lt;##&gt;.tmp: failed to map segment from shared object: Operation not permitted`

case 2:
- selinux enforce
- fstab includes 2 tmp lines, systemd tmpfs tmp.mount enabled
- tmp.mount unit-file enabled
- `jna.tmpdir` set to /test (where `chmod 0750 test` and owned by ES user/group)
- # ES 2.1.1 crash and dump, touch'ed logfile

case 3: 
- selinux enforce
- remove both fstab lines 1) and 2)
- tmp.mount unit-file disabled 
- `jna.tmpdir` not set
- # ES 2.1.1 starts with no warnings/errors/crashes

case 4: 
- selinux enforce
- remove both fstab lines 1) and (2)
- tmp.mount unit-file disabled 
- `jna.tmpdir` set to /test (where `chmod 0750 test` and owned by ES user/group)
- # ES 2.1.1 starts with no warnings/errors/crashes

case 5: 
- selinux disabed
- fstab includes 2 tmp lines, systemd tmpfs tmp.mount enabled
- tmp.mount unit-file enabled
- `jna.tmpdir` set to /test (where `chmod 0750 test` and owned by ES user/group)
- # ES 2.1.1 starts, no warnings/errors/crashes

case 6: 
- selinux disabed
- fstab includes 2 tmp lines, systemd tmpfs tmp.mount enabled
- tmp.mount unit-file enabled
- `jna.tmpdir` not set
- # ES 2.1.1 WARN `/tmp/jna--&lt;##&gt;/jna&lt;##&gt;.tmp: failed to map segment from shared object: Operation not permitted`

case 7: 
- selinux disabed
- remove both fstab lines 1) and 2)
- tmp.mount unit-file disabled 
- `jna.tmpdir` set to /test (where `chmod 0750 test` and owned by ES user/group)
- # ES 2.1.1 starts, no warnings/errors/crashes

case 8: 
- selinux disabed
- remove both fstab lines 1) and 2)
- tmp.mount unit-file disabled 
- `jna.tmpdir` not set
- # ES 2.1.1 starts, no warnings/errors/crashes

case 9: 
- selinux enforce
- fstab includes 2 tmp lines, systemd tmpfs tmp.mount enabled
- tmp.mount unit-file enabled
- `jna.tmpdir` not set
- # ES 2.3.3 WARN `/tmp/jna--&lt;##&gt;/jna&lt;##&gt;.tmp: failed to map segment from shared object: Operation not permitted`

case 10:
- selinux enforce
- fstab includes 2 tmp lines, systemd tmpfs tmp.mount enabled
- tmp.mount unit-file enabled
- `jna.tmpdir` set to /test (where `chmod 0750 test` and owned by ES user/group)
- # ES 2.3.3 starts, (warnings on other issues: max file descriptors, requested thread pool size too large)

case 11:
- selinux disabled
- fstab includes 2 tmp lines, systemd tmpfs tmp.mount enabled
- tmp.mount unit-file enabled
- `jna.tmpdir` set to /test (where `chmod 0750 test` and owned by ES user/group)
- # ES 2.3.3 starts, no warnings

Thanks for bearing with some user-error - and hope anybody else that searches this issue finds this info helpful
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.lang.IllegalArgumentException: the version needs to contain major, minor, and revision, and optionally the build: ${elasticsearch.version}</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18271</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0_alpha2
**JVM version**: openjdk version "1.8.0_51"
**OS version**: Red Hat Enterprise Linux Server release 6.7 (Santiago)

**Description of the problem including expected versus actual behavior**:

We installed the 5.0.0.alpha2 version with the .rpm and can't even start the Application:

**Provide logs (if relevant)**:

[2016-05-11 12:49:05,015][WARN ][bootstrap                ] unable to install syscall filter: 
java.lang.UnsupportedOperationException: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed
        at org.elasticsearch.bootstrap.Seccomp.linuxImpl(Seccomp.java:347)
        at org.elasticsearch.bootstrap.Seccomp.init(Seccomp.java:616)
        at org.elasticsearch.bootstrap.JNANatives.trySeccomp(JNANatives.java:215)
        at org.elasticsearch.bootstrap.Natives.trySeccomp(Natives.java:99)
        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:99)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:152)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
[2016-05-11 12:49:05,330][INFO ][node                     ] [Nathaniel Essex] version[5.0.0-alpha2], pid[28514], build[e3126df/2016-04-26T12:08:58.960Z]
[2016-05-11 12:49:05,330][INFO ][node                     ] [Nathaniel Essex] initializing ...
[2016-05-11 12:49:06,216][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: the version needs to contain major, minor, and revision, and optionally the build: ${elasticsearch.version}
        at org.elasticsearch.Version.fromString(Version.java:162)
        at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:94)
        at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:339)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:131)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:186)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:267)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
</description><key id="154216643">18271</key><summary>java.lang.IllegalArgumentException: the version needs to contain major, minor, and revision, and optionally the build: ${elasticsearch.version}</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ftieben</reporter><labels /><created>2016-05-11T11:05:04Z</created><updated>2016-12-02T19:55:30Z</updated><resolved>2016-05-11T11:56:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-11T11:18:05Z" id="218430736">You have a plugin causing issues, maybe a botched build? Which plugins do you have installed, where did you obtain them from, and how did you install them?
</comment><comment author="ftieben" created="2016-05-11T11:56:44Z" id="218438037">Thanks for that advice:

root@HOST:~$ /usr/share/elasticsearch/bin/elasticsearch-plugin remove hq
-&gt; Removing hq...
root@HOST:~$ /etc/init.d/elasticsearch start
Starting elasticsearch:                                    [  OK  ]
</comment><comment author="naoko" created="2016-12-02T19:55:30Z" id="264547327">Thank you! I see HQ is no longer supported. Attempt to uninstall plugin after upgrade is completed I got this error `ERROR: plugin hq not found; run 'elasticsearch-plugin list' to get list of installed plugins`. so I ran this `rm -rf /usr/share/elasticsearch/plugins/hq` to remove it and now things are running </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add primitive to shrink an index into a single shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18270</link><project id="" key="" /><description>This adds a low level primitive operations to shrink an existing
index into a new index with a single shard. This primitive expects
all shards of the source index to allocated on a single node. Once the target index is initializing on the shrink node it takes a snapshot of the source index shards and copies all files into the target indices data folder. An [optimization](https://issues.apache.org/jira/browse/LUCENE-7300) coming in Lucene 6.1 will also allow for optional constant time copy if hard-links are supported by the filesystem. All mappings are merged into the new indexes metadata once the snapshots have been taken on the merge node.

To shrink an existing index all shards must be moved to a single node (one instance of each shard) and the index must be read-only:

``` BASH
$ curl -XPUT 'http://localhost:9200/logs/_settings' -d '{
    "settings" : {
        "index.routing.allocation.require._name" : "shrink_node_name",
        "index.blocks.write" : true 
    }
}
```

once all shards are started on the shrink node. the new index can be created via:

``` BASH
$ curl -XPUT 'http://localhost:9200/logs/_shrink/logs_single_shard' -d '{
    "settings" : {
        "index.codec" : "best_compression",
        "index.number_of_replicas" : 1
    }
}'
```

This API will perform all needed check before the new index is created and selects the shrink node based on the allocation of the source index. This call returns immediately, to monitor shrink progress the recovery API should be used since all copy operations are reflected in the recovery API with byte copy progress etc.

The shrink operation does not modify the source index, if a shrink operation should
be canceled or if the shrink failed, the target index can simply be deleted and
all resources are released.
</description><key id="154204959">18270</key><summary>Add primitive to shrink an index into a single shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>das awesome</label><label>feature</label><label>release highlight</label><label>v5.0.0-alpha4</label></labels><created>2016-05-11T10:00:38Z</created><updated>2016-05-31T08:42:26Z</updated><resolved>2016-05-31T08:41:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-05-11T19:56:52Z" id="218571530">LGTM ... I'm surprised how small a change this was.
</comment><comment author="bleskes" created="2016-05-12T12:57:41Z" id="218748898">I left some minor comments. I'm also happy it's so small. IndexWriter.addIndexes is neat.  May biggest feedback is whether we need to make the shard initialization be lenient in waiting for it's source shards to  appear. Maybe we should require a different operation order for using this primitive:
-  move all shards to one node
- create and index and force the shard to be in the same node

This will allows us to remove the waiting make it a hard failure if the shards are missing.
</comment><comment author="s1monw" created="2016-05-12T13:12:33Z" id="218752419">&gt; Maybe we should require a different operation order for using this primitive:

I tell you it's all trappy, you create the index and somebody relocates a shard what do you do then. The semantics are pretty clear here as they are for that exact reason. I tried so many models with picking the right node, moving stuff first, then allocate, copy mapping on merge index creation etc. there are so many moving parts which makes stuff like this crazy trappy. There is literally no waiting here, we only even start recovering once all shards are are there and started, there is no waiting or anything, everything is implicit. I am afraid of doing something like your suggestion since it will cause users to make assumptions. We can't guarantee those in a dist system where basically everything is concurrent and nothing fails if you do it afterwards. 
</comment><comment author="s1monw" created="2016-05-20T21:25:43Z" id="220722331">@bleskes i pushed new changes with your idea of failing the shard - maybe you can take a quick look
</comment><comment author="bleskes" created="2016-05-23T09:39:33Z" id="220933673">@s1monw I like how the retry_failed simplified things. Thank you for doing that. I left another suggestion w.r.t mapping. Let me know what you think.
</comment><comment author="s1monw" created="2016-05-23T09:42:01Z" id="220934183">&gt; I like how the retry_failed simplified things.

it complicated things, sorry. you really need to know when all the relocaiton is done rather than make it pick things up automatically. this is much more complex IMO. you have to deal with retries etc. which sucks.
</comment><comment author="bleskes" created="2016-05-23T11:02:12Z" id="220950294">+1
</comment><comment author="s1monw" created="2016-05-26T13:10:34Z" id="221866002">@bleskes @mikemccand @clintongormley this is ready for review. I still need to write docs (user level documentation) but the REST API is done and tests are written. I also committed the hardlink directory to lucene. This should satisfy all valid security concerns raised by @rmuir (thanks for that)! Once we move to Lucene 6.1 we get the hardlink optimization as well with only lucene misc getting the hardlink permission granted. 

I added unittest on all levels, REST tests for the API and removed all nocommits.
</comment><comment author="s1monw" created="2016-05-27T08:20:55Z" id="222088638">@clintongormley I pushed some basic docs &#128131; 
</comment><comment author="bleskes" created="2016-05-27T11:58:28Z" id="222128598">went through it again - looking good - life some very minor comments
</comment><comment author="s1monw" created="2016-05-27T15:07:37Z" id="222171494">@bleskes addressed your comments
</comment><comment author="s1monw" created="2016-05-30T08:43:30Z" id="222443440">I just ran this think for kicks to get some numbers:

Here I shrunk a 36GB 5 shards index into 1 shard **WITHOUT** the lucene hardlink directory optimization

```
index             shard time  type         stage source_host source_node target_host target_node repository snapshot files files_recovered files_percent files_total bytes       bytes_recovered bytes_percent bytes_total translog_ops translog_ops_recovered translog_ops_percent
wiki              0     156ms store        done  127.0.0.1   Dusk        127.0.0.1   Dusk        n/a        n/a      0     0               100.0%        13          0           0               100.0%        7680841604  0            0                      100.0%
wiki              1     1.3s  store        done  127.0.0.1   Dusk        127.0.0.1   Dusk        n/a        n/a      0     0               100.0%        13          0           0               100.0%        7678841298  0            0                      100.0%
wiki              2     848ms store        done  127.0.0.1   Dusk        127.0.0.1   Dusk        n/a        n/a      0     0               100.0%        13          0           0               100.0%        7644680461  0            0                      100.0%
wiki              3     1.2s  store        done  127.0.0.1   Dusk        127.0.0.1   Dusk        n/a        n/a      0     0               100.0%        13          0           0               100.0%        7673073968  0            0                      100.0%
wiki              4     1.1s  store        done  127.0.0.1   Dusk        127.0.0.1   Dusk        n/a        n/a      0     0               100.0%        13          0           0               100.0%        7697787879  0            0                      100.0%
wiki_single_shard 0     9.3m  local_shards done  127.0.0.1   Dusk        127.0.0.1   Dusk        n/a        n/a      60    60              100.0%        60          38375224070 38375224070     100.0%        38375224070 -1           0                      -1.0%
```

Here I shrunk a 36GB 5 shards index into 1 shard **WITH** the lucene hardlink directory optimization

```
index             shard time  type         stage source_host source_node target_host target_node repository snapshot files files_recovered files_percent files_total bytes bytes_recovered bytes_percent bytes_total translog_ops translog_ops_recovered translog_ops_percent
wiki              0     378ms store        done  127.0.0.1   Carnivore   127.0.0.1   Carnivore   n/a        n/a      0     0               100.0%        13          0     0               100.0%        7680841604  0            0                      100.0%
wiki              1     223ms store        done  127.0.0.1   Carnivore   127.0.0.1   Carnivore   n/a        n/a      0     0               100.0%        13          0     0               100.0%        7678841298  0            0                      100.0%
wiki              2     860ms store        done  127.0.0.1   Carnivore   127.0.0.1   Carnivore   n/a        n/a      0     0               100.0%        13          0     0               100.0%        7644680461  0            0                      100.0%
wiki              3     693ms store        done  127.0.0.1   Carnivore   127.0.0.1   Carnivore   n/a        n/a      0     0               100.0%        13          0     0               100.0%        7673073968  0            0                      100.0%
wiki              4     548ms store        done  127.0.0.1   Carnivore   127.0.0.1   Carnivore   n/a        n/a      0     0               100.0%        13          0     0               100.0%        7697787879  0            0                      100.0%
wiki_single_shard 0     710ms local_shards done  127.0.0.1   Carnivore   127.0.0.1   Carnivore   n/a        n/a      0     0               100.0%        60          0     0               100.0%        38375224070 -1           0                      -1.0%
```

copying all the files took ~10 min on a slow disk, the hardlink optimization took ~700ms which is essentially instant! 
</comment><comment author="bleskes" created="2016-05-30T10:18:02Z" id="222461307">Looks great. Left a response on the little discussion around IndicesService,
</comment><comment author="s1monw" created="2016-05-30T12:48:04Z" id="222485483">since @bleskes asked I ran a `_forcemerge` with `codec:"best_compression"` on the new index and it's basically a `20%` disk space reduction:

```
curl -XGET localhost:9200/_cat/segments?v
index             shard prirep ip        segment generation docs.count docs.deleted   size size.memory committed searchable version compound
wiki              0     p      127.0.0.1 _8a8         10736    3249095            0  7.1gb     7230889 true      true       5.4.1   false
wiki              1     p      127.0.0.1 _8dh         10853    3248161            0  7.1gb     7302713 true      true       5.4.1   false
wiki              2     p      127.0.0.1 _8bc         10776    3246424            0  7.1gb     7181590 true      true       5.4.1   false
wiki              3     p      127.0.0.1 _88u         10686    3246699            0  7.1gb     7359849 true      true       5.4.1   false
wiki              4     p      127.0.0.1 _8dr         10863    3244730            0  7.1gb     7428584 true      true       5.4.1   false
wiki_single_shard 0     p      127.0.0.1 _5               5   16235109            0 29.2gb    21294296 true      true       6.0.0   false
```

The `.fdt` file (stored fields) take `16.1 GB` vs. `9.7GB` with `best_compression` so basically all gains are on stored fields. ie. having 5 shards here on this wikipedia index has small overhead in terms of term dicts etc.
</comment><comment author="bleskes" created="2016-05-30T13:18:29Z" id="222490980">LGTM. Thanks for the last commit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>First pass at improving analyzer docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18269</link><project id="" key="" /><description>I've rewritten the intro to analyzers plus the docs for all analyzers to provide working examples.

I've also removed:
- analyzer aliases (see #18244)
- analyzer versions (see #18267)
- snowball analyzer (see #8690)

Next steps will be tokenizers, token filters, char filters
</description><key id="154190485">18269</key><summary>First pass at improving analyzer docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T08:45:16Z</created><updated>2016-05-11T12:17:56Z</updated><resolved>2016-05-11T12:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-11T09:03:13Z" id="218402456">This looks great!
</comment><comment author="clintongormley" created="2016-05-11T12:17:49Z" id="218442300">thanks @jpountz - i've fixed those issues and will merge
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove fields from URI Search doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18268</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/master/search-uri-request.html#_parameters_3

In 5.0A2, this doesn't work anymore:

```
GET book/_search?fields=title
```
</description><key id="154190124">18268</key><summary>Remove fields from URI Search doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gquintana</reporter><labels /><created>2016-05-11T08:43:20Z</created><updated>2016-05-11T08:54:07Z</updated><resolved>2016-05-11T08:54:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-11T08:54:07Z" id="218400522">It works but only with stored fields:

```
PUT test
{
  "mappings": {
    "doc": {
      "properties": {
        "name": {
          "type": "text",
          "store": true
        }
      }
    }
  }
}
PUT test/doc/1
{
  "name": "david"
}
GET test/_search?fields=name
```

The documentation mentions that: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-uri-request.html

&gt; The selective **stored** fields of the document to return for each hit, comma delimited. Not specifying any value will cause no fields to return.

I'm closing. Feel free to reopen if you think I'm missing something.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove user-configurable analyzer versions </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18267</link><project id="" key="" /><description>All analyzers, tokenizers, etc accept a `version` parameter today (see https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html#backwards-compatibility).

The problem is, it doesn't work as expected.  For instance in master, you can specify that you want version `3.0` of an analyzer and your request will be accepted, but the code doesn't HAVE version `3.0` of the analyzer. Lucene 6 only includes version constants for 5.0 and above.

Internally, the version parameter is useful as it means we can recognise an index created before an analyzer was changed (eg see [ArabicAnalyzer.java](https://github.com/apache/lucene-solr/blob/branch_6x/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java#L137)) but we can't read indices created before Lucene 5.0 anyway, so exposing the `version` in the API doesn't make much sense to me.
</description><key id="154189595">18267</key><summary>Remove user-configurable analyzer versions </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label><label>v6.0.0</label></labels><created>2016-05-11T08:40:31Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-23T13:12:41Z" id="228045228">@MaineC Could you take this one please?
</comment><comment author="MaineC" created="2016-06-27T07:59:03Z" id="228677917">From the docs you linked it's not quite clear to me where the `version` parameter can be added today. Do you happen to have a brief json snippet including the parameter handy that I can send to a local ES instance to try it out?

Also, from reading at least some of our [version](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java#L114) parsing code it looks like users should at least see a warning in their log files. Is that the case here? (If so, then I'd guess the idea here would be to remove the leniency of defaulting to some version in case the version string cannot be parsed and fail instead.)

(Just trying to make sure I'm looking at the right thing here...)
</comment><comment author="clintongormley" created="2016-06-27T14:09:00Z" id="228756178">This is how you'd use it:

```
PUT foo
{
  "settings": {
    "analysis": {
      "analyzer": {
        "old_standard": {
          "type": "standard",
          "version": "2.0"
        }
      }
    }
  }
}
```

&gt;  it looks like users should at least see a warning in their log files. Is that the case here?

No, nothing in the logs

&gt; (If so, then I'd guess the idea here would be to remove the leniency of defaulting to some version in case the version string cannot be parsed and fail instead.)

i don't think this should be user-configurable at all.  It should be used purely to allow bwc for existing indices (and that use is internal only)
</comment><comment author="MaineC" created="2016-06-28T07:40:37Z" id="228975114">&gt; i don't think this should be user-configurable at all. It should be used purely to allow bwc for 
&gt; existing indices (and that use is internal only)

Makes sense to me. When looking for the code that actually parses the snippet you provided it looks to me like this is being checked only once the analyzer is actually used. Here's what I tried:

```
PUT foo
{
  "settings": {
    "analysis": {
      "analyzer": {
        "old_standard": {
          "type": "standard",
          "version": "2.0",
          "foobar": "23"
        }
      }
    }
  }
}
```

I can easily add this kind of setting without any complaints in the response nor in the logs. I can also use the analyzer with

```
GET /foo/_analyze
{
  "analyzer" : "old_standard",
  "text" : "this is a test"
}
```

without issue.

```
DELETE /foo

PUT foo
{
  "settings": {
    "analysis": {
      "analyzer": {
        "old_standard": {
          "type": "standard",
          "version": "bullshit"
        }
      }
    }
  }
}
```

This can be set just fine. When trying to use the analyzer through the _analyze endpoint I get good looking analysis results through HTTP, but a 

```
2016-06-28 09:28:36,102][WARN ][index.analysis           ] [Vidar] [foo] no version match bullshit,     default to 6.0.0
java.text.ParseException: Failed to parse major version from "bullshit" (got: bullshit)
    at org.apache.lucene.util.Version.parse(Version.java:161)
    at org.elasticsearch.common.lucene.Lucene.parseVersion(Lucene.java:112)
    at org.elasticsearch.index.analysis.Analysis.parseAnalysisVersion(Ana
```

in the logs.

Others may correct me if I'm wrong, but to me this looks a lot like analysis settings aren't validated?! I'm currently looking here:

https://github.com/elastic/elasticsearch/blob/93415d45061ca35042c21c45a1593cca39adb8d1/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java#L153 

Not sure how deep validation should go, explicitly disallowing the `version` entry but allowing other bullshit entries seems confusing to me. Deeply validating these settings doesn't look particularly "low-hanging-fruit"'y to me.
</comment><comment author="clintongormley" created="2016-06-29T10:20:39Z" id="229317146">&gt; Not sure how deep validation should go, explicitly disallowing the version entry but allowing other bullshit entries seems confusing to me. Deeply validating these settings doesn't look particularly "low-hanging-fruit"'y to me.

Agreed.  Analyzers need a big refactoring too.  For now I'd just focus on the `version` key.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fingerprint analyzer asciifolds AFTER deduplication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18266</link><project id="" key="" /><description>As currently implemented, the `fingerprint` analyzer performs asciifolding after terms have been deduplicated.  In other words, the following:

```
POST my_index/_analyze
{
  "analyzer": "fingerprint",
  "text": "Yes Godel yes, G&#246;del said this sentence is consistent and."
}
```

produces the token `and consistent godel godel is said sentence this yes`

I realise that this is as described in [OpenRefine](https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/clustering/binning/FingerprintKeyer.java#L68) but I'm wondering if it makes sense?

We have the `preserve_original` option to the parameter which results in two terms: one with extended characters and one without:

```
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_fingerprint_analyzer": {
          "type": "fingerprint",
          "preserve_original": true
        }
      }
    }
  }
}

POST my_index/_analyze
{
  "analyzer": "my_fingerprint_analyzer",
  "text": "Yes Godel yes, G&#246;del said this sentence is consistent and."
}
```

This currently produces: 
- `and consistent godel godel is said sentence this yes`
- `and consistent godel g&#246;del is said sentence this yes`

I'm thinking it would make more sense to:
- asciifold
- dedupe
- then concatenate

In which case the standard output would be: `and consistent godel is said sentence this yes` and with `preserve_original`:
- `and consistent godel is said sentence this yes`
- `and consistent godel g&#246;del is said sentence this yes`

@markharwood @polyfractal what do you think?
</description><key id="154185417">18266</key><summary>Fingerprint analyzer asciifolds AFTER deduplication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>discuss</label><label>enhancement</label></labels><created>2016-05-11T08:17:16Z</created><updated>2016-05-12T13:34:15Z</updated><resolved>2016-05-12T13:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-05-11T12:57:03Z" id="218450917">Yep, I raised this issue in the PR too.  It seemed odd to me as well, but @markharwood seemed to think it was acceptable since this will (usually?) be used for algorithmic purposes.
</comment><comment author="markharwood" created="2016-05-11T14:53:10Z" id="218484199">I misunderstood the problem - I thought it was about the choice of a single output token being folded or not. I now see it's about having both versions in the output. Agree with @clintongormley - it would make seem to make sense to change the order.
</comment><comment author="polyfractal" created="2016-05-11T16:15:59Z" id="218510117">Ah oops, I explained poorly then :)  PR sent!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[bug report]file descriptor leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18265</link><project id="" key="" /><description>**Elasticsearch version**:
1.7.0
**JVM version**:
jdk1.8.0_60
**OS version**:
CentOS Linux release 7.1.1503
**Description of the problem including expected versus actual behavior**:
1. use lsof |wc -l to check es process and found out that it use 785334 file descriptor
2. lsof to check more and lots of lucene files, but cat those files, those files doesn't exits.

lsof -p pid:
java    26400 admin  875r      REG                8,3      5285 2148641170 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/serialnumberindex/2/index/_1t64l.cfs
java    26400 admin  876r      REG                8,3  37509255 2148680299 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/serialnumberindex/2/index/_16d1_Lucene410_0.d
vd
java    26400 admin  877r      REG                8,3 117618112 2148651284 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/serialnumberindex/2/index/_26gh_Lucene41_0.do
c
java    26400 admin  878r      REG                8,3 195296325 2148651286 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/serialnumberindex/2/index/_26gh_Lucene41_0.ti
m
java    26400 admin  879r      REG                8,3 766023672 2148651289 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/serialnumberindex/2/index/_26gh.fdt
java    26400 admin  880r      REG                8,3   7468533 6443429245 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/.node_monitor-2016.05.06/0/index/_boa_Lucene4
1_0.doc
java    26400 admin  881r      REG                8,3      3406 6443478173 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/.node_monitor-2016.05.06/0/index/_ff2.nvd
java    26400 admin  882r      REG                8,3  33891307 2148651283 /export/Data/elasticsearch/jiesi-13-bak/jiesi-13-bak/nodes/0/indices/serialnumberindex/2/index/_26gh_Lucene410_0.d
vd

cat these files, they doesn't exit.
**Steps to reproduce**:
1. start es process
   2.running for a long time
</description><key id="154180747">18265</key><summary>[bug report]file descriptor leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-05-11T07:49:47Z</created><updated>2016-05-11T08:08:52Z</updated><resolved>2016-05-11T08:08:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-11T08:08:52Z" id="218390782">github is for bug reports only, can you open a discussion on the forum instead:
https://discuss.elastic.co/c/elasticsearch

My first 2c is that you have a lot of SearchContext open that keep those files alive. Please open a discussion on the forum so we can dig into your problem. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add 'ctx' keyword to painless.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18264</link><project id="" key="" /><description>This is used by ExecutableScripts (e.g. update scripts) and currently must be accessed as `input.ctx` which makes it different in painless from other scripting engines.

It has a known type `Map&lt;String,Object&gt;`, which is better than nothing: we get performance benefits from the additional typing.

If `ctx` is ever accessed in the script, we do a single hashmap get into a read-only local variable.
</description><key id="154169857">18264</key><summary>Add 'ctx' keyword to painless.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T06:34:19Z</created><updated>2016-05-11T06:50:52Z</updated><resolved>2016-05-11T06:50:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-11T06:36:53Z" id="218374478">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix bracket shortcuts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18263</link><project id="" key="" /><description>Fixed a bug in bracket shortcuts where they were failing to allow pieces of a variable chain to be loaded/stored after a shortcut.
</description><key id="154167494">18263</key><summary>Fix bracket shortcuts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T06:13:25Z</created><updated>2016-05-11T11:28:50Z</updated><resolved>2016-05-11T06:18:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-11T06:16:16Z" id="218371458">+1 Thanks for tracking this down and fixing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Painless doc access</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18262</link><project id="" key="" /><description>Currently access to a document works only as `input.doc['field'].value`. 

It would be much better to support `doc['field'].value` like the other scripting engines.

I plan to also followup with `ctx` and `_value` reserved words, too. After we've done these, I think we should rename `input` to `params` as that will be its general use, just accessing script parameters.

This change is the simplest step, it just treats `doc` as a `def` type. It is at least a small perf improvement on its own. 

Later, we can try to improve its type safety, ultimately treating it as a `Map&lt;String,ScriptDocValues&gt;` which will potentially allow us to avoid tons of dynamic shit for what really should not be dynamic...
</description><key id="154155480">18262</key><summary>Painless doc access</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T04:01:10Z</created><updated>2016-05-11T04:58:24Z</updated><resolved>2016-05-11T04:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-11T04:03:38Z" id="218356353">LGTM.  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[feature request]iteration term aggregration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18261</link><project id="" key="" /><description>**Describe the feature**:
currently term aggregation execute below: the node coordinating the search process will request each shard to provide its own top size term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client.
if try to get all terms agg, it will cause memory issue.
this request is below:
not based on top size term but top term on each shard's term dictionary and return the agg. maintain a context for iteration to traverse all terms and get exact results. 
</description><key id="154154953">18261</key><summary>[feature request]iteration term aggregration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-05-11T03:55:18Z</created><updated>2016-05-11T06:25:45Z</updated><resolved>2016-05-11T06:25:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-11T06:25:45Z" id="218372805">Closing as duplicate of #4915
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort plugins in list plugins command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18260</link><project id="" key="" /><description>This commit modifies the list plugins command to produce deterministic
output by sorting the plugins by comparing paths.

Relates #18051
</description><key id="154144629">18260</key><summary>Sort plugins in list plugins command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugins</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T02:01:32Z</created><updated>2016-05-11T11:24:33Z</updated><resolved>2016-05-11T02:06:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-11T02:03:19Z" id="218343156">LGTM
</comment><comment author="jasontedor" created="2016-05-11T02:05:43Z" id="218343415">Note that [`DirectoryStream`](https://docs.oracle.com/javase/8/docs/api/java/nio/file/DirectoryStream.html) does not guarantee the ordering of iterations, and this is leading to [test failures](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/510/testReport/). It probably makes sense to have the output sorted anyway.
</comment><comment author="jasontedor" created="2016-05-11T02:06:42Z" id="218343530">Thanks @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Require /bin/bash in packaging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18259</link><project id="" key="" /><description>This commit adds a hard requirement to the RPM and Debian packages for
/bin/bash to be present, and adds a note regarding this to the migration
docs.

Relates #18251
</description><key id="154139721">18259</key><summary>Require /bin/bash in packaging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>breaking</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T01:08:22Z</created><updated>2016-05-11T01:17:15Z</updated><resolved>2016-05-11T01:17:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-11T01:14:19Z" id="218337064">LGTM thanks Jason!
</comment><comment author="jasontedor" created="2016-05-11T01:17:15Z" id="218337432">Thanks @dakrone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Retrieve _score directly from Scorer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18258</link><project id="" key="" /><description>Currently, painless takes the lucene Scorer, wraps it in a ScoreAccessor, and stuffs it into a hashmap.

This means for each document, we have to do a hashmap get, which is unnecessary overhead. This hashing is part of what is responsible for the large difference performance gap between expressions and painless (https://benchmarks.elastic.co/index.html#search_qps_scripts).

We should just pass the Scorer as a parameter instead and avoid the hashing. I think we should also followup with passing the document's fields as ScriptDocValues[] and accessing those by array index, also just like expressions does. That is a trickier change as we can only optimize away the hash lookup when the field index is a string constant (syntax such as `doc['field']/doc.field/doc.get('field')`), but that should be a very common case.

These changes allow us to bypass the waste of the scripting API behind the scenes and get closer to the performance of expressions.
</description><key id="154139059">18258</key><summary>Retrieve _score directly from Scorer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T01:01:03Z</created><updated>2016-05-17T12:17:02Z</updated><resolved>2016-05-11T02:29:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-11T02:07:14Z" id="218343577">LGTM
</comment><comment author="rmuir" created="2016-05-11T02:28:39Z" id="218346097">Thanks @rjernst for reviewing. I will tackle `doc` next and try to make some progress on removing unnecessary hashing and invokeDynamic calls.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add TAG_SETTING to list of allowed tags for the ec2 discovery plugin.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18257</link><project id="" key="" /><description>I am unable to set ec2 discovery tags because this setting was
accidentally omitted from the register settings list in
Ec2DiscoveryPlugin.java. I get this:

java.lang.IllegalArgumentException: unknown setting [discovery.ec2.tag.project]
</description><key id="154133347">18257</key><summary>Add TAG_SETTING to list of allowed tags for the ec2 discovery plugin.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">j16r</reporter><labels><label>:Plugin Discovery EC2</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-11T00:05:24Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-05-11T06:19:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="j16r" created="2016-05-11T00:08:28Z" id="218327957">I have signed the CLA, shortly before submitting this PR - system might not have caught up yet?
</comment><comment author="dadoonet" created="2016-05-11T06:19:10Z" id="218371857">Thanks for reporting a 5.0 bug and submitting a PR!  I'm adding the [Pioneer Program](https://www.elastic.co/blog/elastic-pioneer-program) label to the issue to make you eligible for a gift pack.

The CLA is now fine. I'm going to merge your change really soonish.

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CORS filtering applied for same origin requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18256</link><project id="" key="" /><description>**Elasticsearch version**: `2.3.2`
**JVM version**: `1.8`
**OS version**: not relevant

**Description of the problem including expected versus actual behavior**:
- Actual behavior

Following [CORS implementation refactoring changes](https://github.com/elastic/elasticsearch/pull/16436) when a request contains an `Origin` header, it is considered as a cross-origin request and `CORS` filtering is applied based on configuration. 

This prevents the [head plugin](https://github.com/mobz/elasticsearch-head), if `CORS` is not enabled or not configured to enable a specific origin, to work with `Chrome` or `Safari` as [they add an origin header for every POST request](http://stackoverflow.com/a/15514049) 

More specifically `POST requests` from those browsers will be returned `403` (`Forbidden`)  
- Expected behavior 

According to [RFC 6454](https://tools.ietf.org/html/rfc6454#section-7.3):

&gt; The user agent MAY include an Origin header field in any HTTP request.

[CorsHandler](https://github.com/elastic/elasticsearch/blob/2.3/core/src/main/java/org/elasticsearch/http/netty/cors/CorsHandler.java) should check the `Host` header, and if it matches the domain in the `Origin` header, don't treat the request as a cross-origin request, and so allow it to be performed. 

**Steps to reproduce**:
-  1 - Disable `CORS` in elasticsearch configuration

``` yaml
http.cors.enabled: false
```
- 2 - Send a `same origin POST` request with an `Origin` header  

```
curl -H "Origin: http://localhost:9200" -X POST http://localhost:9200/_all/_search
```
</description><key id="154122832">18256</key><summary>CORS filtering applied for same origin requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">mchv</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.3</label></labels><created>2016-05-10T22:40:24Z</created><updated>2016-05-11T19:23:02Z</updated><resolved>2016-05-11T19:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-11T19:23:02Z" id="218562721">@mchv thank you for reporting!  Fixed by commit https://github.com/elastic/elasticsearch/commit/d3f205b4a2c6e61810e818de3be83164cd2b69a0

It will be part of 2.3.3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add // CONSOLE to validate and uri-request docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18255</link><project id="" key="" /><description>Two of the snippets in validate weren't working properly so they are
marked as skip and linked to this:
https://github.com/elastic/elasticsearch/issues/18254

We didn't properly handle empty parameter values. We were sending
them as the literal string "null". Now we do better and send them
as the empty string.

Related to #18160
</description><key id="154121717">18255</key><summary>Add // CONSOLE to validate and uri-request docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T22:32:48Z</created><updated>2016-05-18T01:35:01Z</updated><resolved>2016-05-18T01:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T22:32:57Z" id="218311801">@MaineC would you like to review?
</comment><comment author="MaineC" created="2016-05-17T19:49:23Z" id="219832696">Two minor typos, other than that LGTM
</comment><comment author="nik9000" created="2016-05-18T01:35:00Z" id="219902140">Thanks for the review @MaineC !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_validate/query's rewrite=true output seems wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18254</link><project id="" key="" /><description>**Steps to reproduce**:
Run this:

```
curl -XPUT localhost:9200/twitter/tweet/1?refresh -d'{
  "user" : "kimchy",
  "post_date" : "2009-11-15T14:12:12",
  "message" : "trying out Elasticsearch"
}'
curl -XPUT localhost:9200/twitter/tweet/2?refresh -d'{
  "user" : "kimchi",
  "post_date" : "2009-11-15T14:12:13",
  "message" : "My username is similar to @kimchy!"
}'
curl -XPUT localhost:9200/twitter/tweet/3?refresh -d'{
  "user" : "notkimchi",
  "post_date" : "2009-11-15T14:12:13",
  "message" : "My username is not similar to @kimchy!"
}'

curl -XGET 'localhost:9200/twitter/tweet/_validate/query?rewrite=true&amp;pretty' -d'{
  "query": {
    "match": {
      "user": {
        "query": "kimchy",
        "fuzziness": "auto"
      }
    }
  }
}'
```

The output of the last bit is:

```
{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "twitter",
    "valid" : true,
    "explanation" : "+() #(ConstantScore(_type:tweet))^0.0"
  } ]
}
```

`+()` isn't right at all. It should be more like `+(user:kimchy user:kimchi^0.75)` or something.

This came up when I was trying to add `// CONSOLE` to validate.asciidoc. I'm going to disable some of the tests that generates and mark them with this issue.
</description><key id="154119003">18254</key><summary>_validate/query's rewrite=true output seems wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>docs</label><label>v5.0.0</label></labels><created>2016-05-10T22:14:08Z</created><updated>2017-03-22T23:20:12Z</updated><resolved>2017-03-22T23:20:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="camilojd" created="2016-05-17T23:30:26Z" id="219884453">I gave it a shot. When querying `_validate`, the operation is performed against an unique random shard as specified by `TransportValidateQueryAction#shards`.

All possible results are:

```
"explanation" : "+() #(ConstantScore(_type:tweet))^0.0"
"explanation" : "+user:kimchy #(ConstantScore(_type:tweet))^0.0"
"explanation" : "+(user:kimchi)^0.8333333 #(ConstantScore(_type:tweet))^0.0"
```

The `+()` is a BooleanQuery with 0 clauses, a result of a query rewrite in an shard with no hits for the original fuzzy query. 

I'm now sure if would be worthy to hit all shards and somehow pick the most relevant explanation... or maybe remove irrelevant strings from the description.

Thoughts?
</comment><comment author="clintongormley" created="2016-05-18T11:38:15Z" id="220000864">Ah thanks for digging @camilojd - perhaps all that is needed here is some documentation about the rewrite parameter.
</comment><comment author="nik9000" created="2016-05-18T11:45:57Z" id="220002304">&gt; Ah thanks for digging @camilojd - perhaps all that is needed here is some documentation about the rewrite parameter.

Maybe, but right now the rewrite parameter just picks a shard for the rewrite so you end up with different results. The inconsistency makes it pretty hard to use. If this is just a docs problem then we should document it as "only useful for multi-term queries if all shards are exactly the same" or something.
</comment><comment author="clintongormley" created="2016-05-18T12:05:31Z" id="220006320">&gt; "only useful for multi-term queries if all shards are exactly the same" or something.

just explaining that it chooses a shard at random should be sufficient.  this reflects the actual query being run on one shard.  it's the same issue as indexing two docs in 5 shards then wondering why relevance is off.  
</comment><comment author="imotov" created="2017-03-22T23:20:12Z" id="288569727">I think we can consider this one done. We have mentioned it in documentation and #23697 makes it possible to get response from all shards if needed. Please feel free to reopen if you don't agree.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scan/Scroll performance degrading logarithmically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18253</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5
**JVM version**: java version "1.7.0_99"
OpenJDK Runtime Environment (rhel-2.6.5.0.el6_7-x86_64 u99-b00)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)
**OS version**: CentOS release 6.7 (Final) 2.6.32-573.22.1.el6.x86_64

Hi,
We are in the process of upgrading from 1.7 to 2.X, but first we must re-index into mappings compatible with 2.X.  We are using the `reindex()` helper method of `elasticsearch-py v1.9.0` to reindex a relatively small 7GB index, and noticed that performance starts out fast then logarithmically degrades over time:

![99445104-0bca-11e6-85f5-b0e3df98eed8](https://cloud.githubusercontent.com/assets/1916150/15163270/5daefa34-16bc-11e6-87bd-9e00cf94725c.png)
(Note this shows indexing requests, but this is a function of the slowdown in scan/scroll as I'll show below.)

As a test, I set up a simple call to `scan()` on the 7GB index and printed some benchmarks.  I aborted the scan after 13 hours, and it hadn't even gotten close to scanning all 27M docs over those 13 hours:

```
  Starting scan...
    Scanned 100,000 docs (batch time: 0:00:07 / total time: 0:00:07 )
    Scanned 200,000 docs (batch time: 0:00:11 / total time: 0:00:18 )
    Scanned 300,000 docs (batch time: 0:00:14 / total time: 0:00:33 )
    Scanned 400,000 docs (batch time: 0:00:18 / total time: 0:00:52 )
    Scanned 500,000 docs (batch time: 0:00:23 / total time: 0:01:15 )
    Scanned 600,000 docs (batch time: 0:00:26 / total time: 0:01:42 )
    Scanned 700,000 docs (batch time: 0:00:30 / total time: 0:02:13 )
    Scanned 800,000 docs (batch time: 0:00:34 / total time: 0:02:47 )
    Scanned 900,000 docs (batch time: 0:00:38 / total time: 0:03:25 )
    Scanned 1,000,000 docs (batch time: 0:00:41 / total time: 0:04:07 )
    Scanned 1,100,000 docs (batch time: 0:00:45 / total time: 0:04:53 )
    Scanned 1,200,000 docs (batch time: 0:00:49 / total time: 0:05:43 )
    Scanned 1,300,000 docs (batch time: 0:00:53 / total time: 0:06:36 )
    Scanned 1,400,000 docs (batch time: 0:00:58 / total time: 0:07:34 )
    Scanned 1,500,000 docs (batch time: 0:01:03 / total time: 0:08:37 )
    Scanned 1,600,000 docs (batch time: 0:01:06 / total time: 0:09:44 )
    Scanned 1,700,000 docs (batch time: 0:01:09 / total time: 0:10:54 )
    Scanned 1,800,000 docs (batch time: 0:01:13 / total time: 0:12:07 )
    Scanned 1,900,000 docs (batch time: 0:01:17 / total time: 0:13:24 )
    Scanned 2,000,000 docs (batch time: 0:01:20 / total time: 0:14:45 )
    Scanned 2,100,000 docs (batch time: 0:01:24 / total time: 0:16:10 )
    Scanned 2,200,000 docs (batch time: 0:01:34 / total time: 0:17:44 )
    Scanned 2,300,000 docs (batch time: 0:01:46 / total time: 0:19:30 )
    Scanned 2,400,000 docs (batch time: 0:01:37 / total time: 0:21:08 )
    Scanned 2,500,000 docs (batch time: 0:01:42 / total time: 0:22:50 )
    Scanned 2,600,000 docs (batch time: 0:01:50 / total time: 0:24:40 )
    Scanned 2,700,000 docs (batch time: 0:01:54 / total time: 0:26:34 )
    Scanned 2,800,000 docs (batch time: 0:01:54 / total time: 0:28:28 )
    Scanned 2,900,000 docs (batch time: 0:01:55 / total time: 0:30:23 )
    Scanned 3,000,000 docs (batch time: 0:02:05 / total time: 0:32:29 )
    Scanned 3,100,000 docs (batch time: 0:02:12 / total time: 0:34:42 )
    Scanned 3,200,000 docs (batch time: 0:02:13 / total time: 0:36:55 )
    Scanned 3,300,000 docs (batch time: 0:02:18 / total time: 0:39:14 )
    Scanned 3,400,000 docs (batch time: 0:02:14 / total time: 0:41:28 )
    Scanned 3,500,000 docs (batch time: 0:02:24 / total time: 0:43:53 )
    Scanned 3,600,000 docs (batch time: 0:02:40 / total time: 0:46:34 )
    Scanned 3,700,000 docs (batch time: 0:02:50 / total time: 0:49:25 )
    Scanned 3,800,000 docs (batch time: 0:02:37 / total time: 0:52:02 )
    Scanned 3,900,000 docs (batch time: 0:02:35 / total time: 0:54:38 )
    Scanned 4,000,000 docs (batch time: 0:02:37 / total time: 0:57:15 )
    Scanned 4,100,000 docs (batch time: 0:02:42 / total time: 0:59:58 )
    Scanned 4,200,000 docs (batch time: 0:02:45 / total time: 1:02:43 )
    Scanned 4,300,000 docs (batch time: 0:02:57 / total time: 1:05:40 )
    Scanned 4,400,000 docs (batch time: 0:03:02 / total time: 1:08:43 )
    Scanned 4,500,000 docs (batch time: 0:03:03 / total time: 1:11:46 )
    Scanned 4,600,000 docs (batch time: 0:03:12 / total time: 1:14:59 )
    Scanned 4,700,000 docs (batch time: 0:03:17 / total time: 1:18:16 )
    Scanned 4,800,000 docs (batch time: 0:03:15 / total time: 1:21:32 )
    Scanned 4,900,000 docs (batch time: 0:03:25 / total time: 1:24:57 )
    Scanned 5,000,000 docs (batch time: 0:03:23 / total time: 1:28:21 )
    Scanned 5,100,000 docs (batch time: 0:03:34 / total time: 1:31:55 )
    Scanned 5,200,000 docs (batch time: 0:03:37 / total time: 1:35:32 )
    Scanned 5,300,000 docs (batch time: 0:03:28 / total time: 1:39:01 )
    Scanned 5,400,000 docs (batch time: 0:03:37 / total time: 1:42:38 )
    Scanned 5,500,000 docs (batch time: 0:03:48 / total time: 1:46:27 )
    Scanned 5,600,000 docs (batch time: 0:03:54 / total time: 1:50:21 )
    Scanned 5,700,000 docs (batch time: 0:03:43 / total time: 1:54:04 )
    Scanned 5,800,000 docs (batch time: 0:03:46 / total time: 1:57:51 )
    Scanned 5,900,000 docs (batch time: 0:04:03 / total time: 2:01:55 )
    Scanned 6,000,000 docs (batch time: 0:04:09 / total time: 2:06:04 )
    Scanned 6,100,000 docs (batch time: 0:04:12 / total time: 2:10:17 )
    Scanned 6,200,000 docs (batch time: 0:04:07 / total time: 2:14:25 )
    Scanned 6,300,000 docs (batch time: 0:04:31 / total time: 2:18:56 )
    Scanned 6,400,000 docs (batch time: 0:04:20 / total time: 2:23:16 )
    Scanned 6,500,000 docs (batch time: 0:04:28 / total time: 2:27:45 )
    Scanned 6,600,000 docs (batch time: 0:04:22 / total time: 2:32:07 )
    Scanned 6,700,000 docs (batch time: 0:04:23 / total time: 2:36:31 )
    Scanned 6,800,000 docs (batch time: 0:04:38 / total time: 2:41:10 )
    Scanned 6,900,000 docs (batch time: 0:04:45 / total time: 2:45:56 )
    Scanned 7,000,000 docs (batch time: 0:04:56 / total time: 2:50:53 )
    Scanned 7,100,000 docs (batch time: 0:04:47 / total time: 2:55:40 )
    Scanned 7,200,000 docs (batch time: 0:04:56 / total time: 3:00:37 )
    Scanned 7,300,000 docs (batch time: 0:04:57 / total time: 3:05:34 )
    Scanned 7,400,000 docs (batch time: 0:05:04 / total time: 3:10:39 )
    Scanned 7,500,000 docs (batch time: 0:04:55 / total time: 3:15:34 )
    Scanned 7,600,000 docs (batch time: 0:05:14 / total time: 3:20:49 )
    Scanned 7,700,000 docs (batch time: 0:05:17 / total time: 3:26:06 )
    Scanned 7,800,000 docs (batch time: 0:05:12 / total time: 3:31:18 )
    Scanned 7,900,000 docs (batch time: 0:05:17 / total time: 3:36:36 )
    Scanned 8,000,000 docs (batch time: 0:05:32 / total time: 3:42:08 )
    Scanned 8,100,000 docs (batch time: 0:05:27 / total time: 3:47:36 )
    Scanned 8,200,000 docs (batch time: 0:05:44 / total time: 3:53:20 )
    Scanned 8,300,000 docs (batch time: 0:05:22 / total time: 3:58:43 )
    Scanned 8,400,000 docs (batch time: 0:05:32 / total time: 4:04:16 )
    Scanned 8,500,000 docs (batch time: 0:05:42 / total time: 4:09:59 )
    Scanned 8,600,000 docs (batch time: 0:05:48 / total time: 4:15:48 )
    Scanned 8,700,000 docs (batch time: 0:05:57 / total time: 4:21:45 )
    Scanned 8,800,000 docs (batch time: 0:06:00 / total time: 4:27:46 )
    Scanned 8,900,000 docs (batch time: 0:05:50 / total time: 4:33:37 )
    Scanned 9,000,000 docs (batch time: 0:06:00 / total time: 4:39:38 )
    Scanned 9,100,000 docs (batch time: 0:05:59 / total time: 4:45:38 )
    Scanned 9,200,000 docs (batch time: 0:06:16 / total time: 4:51:54 )
    Scanned 9,300,000 docs (batch time: 0:06:21 / total time: 4:58:15 )
    Scanned 9,400,000 docs (batch time: 0:06:22 / total time: 5:04:37 )
    Scanned 9,500,000 docs (batch time: 0:06:19 / total time: 5:10:57 )
    Scanned 9,600,000 docs (batch time: 0:06:11 / total time: 5:17:09 )
    Scanned 9,700,000 docs (batch time: 0:06:27 / total time: 5:23:36 )
    Scanned 9,800,000 docs (batch time: 0:06:19 / total time: 5:29:56 )
    Scanned 9,900,000 docs (batch time: 0:06:35 / total time: 5:36:32 )
    Scanned 10,000,000 docs (batch time: 0:06:42 / total time: 5:43:14 )
    Scanned 10,100,000 docs (batch time: 0:06:55 / total time: 5:50:10 )
    Scanned 10,200,000 docs (batch time: 0:06:48 / total time: 5:56:58 )
    Scanned 10,300,000 docs (batch time: 0:06:55 / total time: 6:03:54 )
    Scanned 10,400,000 docs (batch time: 0:06:45 / total time: 6:10:40 )
    Scanned 10,500,000 docs (batch time: 0:06:57 / total time: 6:17:37 )
    Scanned 10,600,000 docs (batch time: 0:07:04 / total time: 6:24:42 )
    Scanned 10,700,000 docs (batch time: 0:07:09 / total time: 6:31:52 )
    Scanned 10,800,000 docs (batch time: 0:07:11 / total time: 6:39:03 )
    Scanned 10,900,000 docs (batch time: 0:07:22 / total time: 6:46:25 )
    Scanned 11,000,000 docs (batch time: 0:07:38 / total time: 6:54:03 )
    Scanned 11,100,000 docs (batch time: 0:07:15 / total time: 7:01:19 )
    Scanned 11,200,000 docs (batch time: 0:07:27 / total time: 7:08:46 )
    Scanned 11,300,000 docs (batch time: 0:07:24 / total time: 7:16:11 )
    Scanned 11,400,000 docs (batch time: 0:07:32 / total time: 7:23:44 )
    Scanned 11,500,000 docs (batch time: 0:07:25 / total time: 7:31:09 )
    Scanned 11,600,000 docs (batch time: 0:07:31 / total time: 7:38:40 )
    Scanned 11,700,000 docs (batch time: 0:07:34 / total time: 7:46:15 )
    Scanned 11,800,000 docs (batch time: 0:07:48 / total time: 7:54:04 )
    Scanned 11,900,000 docs (batch time: 0:07:59 / total time: 8:02:04 )
    Scanned 12,000,000 docs (batch time: 0:07:44 / total time: 8:09:49 )
    Scanned 12,100,000 docs (batch time: 0:07:47 / total time: 8:17:36 )
    Scanned 12,200,000 docs (batch time: 0:08:05 / total time: 8:25:41 )
    Scanned 12,300,000 docs (batch time: 0:08:21 / total time: 8:34:02 )
    Scanned 12,400,000 docs (batch time: 0:08:06 / total time: 8:42:09 )
    Scanned 12,500,000 docs (batch time: 0:08:30 / total time: 8:50:39 )
    Scanned 12,600,000 docs (batch time: 0:08:07 / total time: 8:58:46 )
    Scanned 12,700,000 docs (batch time: 0:08:40 / total time: 9:07:26 )
    Scanned 12,800,000 docs (batch time: 0:09:11 / total time: 9:16:38 )
    Scanned 12,900,000 docs (batch time: 0:08:31 / total time: 9:25:09 )
    Scanned 13,000,000 docs (batch time: 0:08:28 / total time: 9:33:38 )
    Scanned 13,100,000 docs (batch time: 0:08:48 / total time: 9:42:26 )
    Scanned 13,200,000 docs (batch time: 0:09:04 / total time: 9:51:30 )
    Scanned 13,300,000 docs (batch time: 0:08:34 / total time: 10:00:05 )
    Scanned 13,400,000 docs (batch time: 0:09:04 / total time: 10:09:09 )
    Scanned 13,500,000 docs (batch time: 0:08:47 / total time: 10:17:57 )
    Scanned 13,600,000 docs (batch time: 0:08:51 / total time: 10:26:48 )
    Scanned 13,700,000 docs (batch time: 0:08:48 / total time: 10:35:37 )
    Scanned 13,800,000 docs (batch time: 0:09:02 / total time: 10:44:39 )
    Scanned 13,900,000 docs (batch time: 0:09:23 / total time: 10:54:03 )
    Scanned 14,000,000 docs (batch time: 0:09:22 / total time: 11:03:26 )
    Scanned 14,100,000 docs (batch time: 0:09:22 / total time: 11:12:49 )
    Scanned 14,200,000 docs (batch time: 0:09:31 / total time: 11:22:20 )
    Scanned 14,300,000 docs (batch time: 0:09:14 / total time: 11:31:35 )
    Scanned 14,400,000 docs (batch time: 0:09:33 / total time: 11:41:08 )
    Scanned 14,500,000 docs (batch time: 0:09:45 / total time: 11:50:54 )
    Scanned 14,600,000 docs (batch time: 0:09:46 / total time: 12:00:40 )
    Scanned 14,700,000 docs (batch time: 0:09:29 / total time: 12:10:10 )
    Scanned 14,800,000 docs (batch time: 0:09:43 / total time: 12:19:54 )
    Scanned 14,900,000 docs (batch time: 0:09:43 / total time: 12:29:37 )
    Scanned 15,000,000 docs (batch time: 0:09:40 / total time: 12:39:17 )
    Scanned 15,100,000 docs (batch time: 0:10:09 / total time: 12:49:27 )
    Scanned 15,200,000 docs (batch time: 0:10:10 / total time: 12:59:37 )
    Scanned 15,300,000 docs (batch time: 0:10:06 / total time: 13:09:44 )
```

As you can see, at the end it was taking 10+ minutes to iterate over a batch of 100k documents.  I don't even want to know how long it would take at the 27 millionth document batch.

Our cluster is at AWS and is comprised of the following:
- (5) `m4.xlarge` data nodes
- (3) `m3.medium` master nodes
- (1) `m4.large` client node

That's 20 CPUs and 80GB RAM amongst the data nodes, which you think would be well enough for a 7GB index.

We experimented with various `scan` values (`5s`, `30s`, `5m`, `10m`) and it didn't seem to make a difference.  I enabled slow logging during the scan, but nothing appeared in the logs.  I did grab a `hot_threads` output during the scan:

```
::: [estest][PiocenKcQCuuF-82oB7EZw][estest.domain.com][inet[/172.31.9.171:9300]]{master=true}
   Hot threads at 2016-05-02T20:23:39.940Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   31.4% (156.9ms out of 500ms) cpu usage by thread 'elasticsearch[estest][search][T#3]'
     2/10 snapshots sharing following 15 elements
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
       org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
       org.elasticsearch.search.SearchService.executeScan(SearchService.java:274)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     4/10 snapshots sharing following 19 elements
       org.apache.lucene.search.BitsFilteredDocIdSet.match(BitsFilteredDocIdSet.java:60)
       org.apache.lucene.search.FilteredDocIdSet$2.match(FilteredDocIdSet.java:103)
       org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:60)
       org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:257)
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
       org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
       org.elasticsearch.search.SearchService.executeScan(SearchService.java:274)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:735)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1137)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 17 elements
       org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:59)
       org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:257)
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
       org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
       org.elasticsearch.search.SearchService.executeScan(SearchService.java:274)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

   27.8% (139ms out of 500ms) cpu usage by thread 'elasticsearch[estest][search][T#4]'
     6/10 snapshots sharing following 19 elements
       org.apache.lucene.search.BitsFilteredDocIdSet.match(BitsFilteredDocIdSet.java:60)
       org.apache.lucene.search.FilteredDocIdSet$2.match(FilteredDocIdSet.java:103)
       org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:60)
       org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:257)
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
       org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
       org.elasticsearch.search.SearchService.executeScan(SearchService.java:274)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     4/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

   27.7% (138.2ms out of 500ms) cpu usage by thread 'elasticsearch[estest][search][T#1]'
     3/10 snapshots sharing following 19 elements
       org.apache.lucene.search.BitsFilteredDocIdSet.match(BitsFilteredDocIdSet.java:60)
       org.apache.lucene.search.FilteredDocIdSet$2.match(FilteredDocIdSet.java:103)
       org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:60)
       org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:257)
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
       org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
       org.elasticsearch.search.SearchService.executeScan(SearchService.java:274)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 16 elements
       org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:257)
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
       org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
       org.elasticsearch.search.SearchService.executeScan(SearchService.java:274)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 15 elements
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
       org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
       org.elasticsearch.search.SearchService.executeScan(SearchService.java:274)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
       org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:735)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1137)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
```

**Steps to reproduce**:
1. Scan over all documents in a large index.  Here's my scan code:

```
from elasticsearch import Elasticsearch, exceptions
from elasticsearch import helpers as es_helpers

docs = es_helpers.scan(
    sourceCluster,
    index=args.sourceIndex,
    scroll=args.scroll,
    fields=('_source', '_parent', '_routing', '_timestamp')
)

totalCount = 0
batchCount = 0
batchSize = 100000
batchTime = time()

for d in docs:
    batchCount += 1
    totalCount += 1
    if batchCount == batchSize:
        print "Scanned", format(totalCount, ",d"), "docs (batch time:", _timeDiff(batchTime), \
            "/ total time:", _timeDiff(start), ")"
        batchCount = 0
        batchTime = time()

def _timeDiff(start):
    m, s = divmod(time() - start, 60)
    h, m = divmod(m, 60)
    return "%d:%02d:%02d" % (h, m, s)
```

**Expected behavior**: Each batch of results from a `scan` call should return in the same amount of time.

**Actual behavior**: Each batch of results from a `scan` call gets slower and slower the deeper into the results it goes.

Is this a bug in scan/scroll?  From everything I've read, `scan` is supposed to make diving deep into large result sets fast.  It doesn't appear to be working here.  We are currently stuck on 1.7 until we can get past this, so any help would be appreciated.  We really want to move to 2.X ASAP.  Thanks!

(Note: I had originally filed a bug in `elasticsearch-py`, but came to the conclusion that the issue is in ES core rather than the Python client.  See original bug for more details and discussion:https://github.com/elastic/elasticsearch-py/issues/397)
</description><key id="154118482">18253</key><summary>Scan/Scroll performance degrading logarithmically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsnod</reporter><labels><label>:Scroll</label><label>feedback_needed</label></labels><created>2016-05-10T22:10:23Z</created><updated>2016-05-12T21:56:36Z</updated><resolved>2016-05-12T21:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-11T07:44:26Z" id="218386004">Did you try with a smaller batch size ? 100k seems big and could potentially use a lot of memory on the es side. What happens with a batch size of 1k ? You should also check the GC logs to see if your nodes have enough memory to handle a scan with big batches. 
</comment><comment author="clintongormley" created="2016-05-11T12:02:15Z" id="218439140">Do you realise that you're asking for 100,000 results X number of primary shards?  That's huge.  I'd reduce this to eg 5000 docs and remeasure.

Also, the `scroll` (you said `scan` but i think you meant `scroll`) needs to be long enough for you to process your docs and come back for the next tranche.  
</comment><comment author="jsnod" created="2016-05-11T15:18:50Z" id="218492233">To clarify, the batch size in my benchmark code is not the batch size for `scan` it's simply how often I print out the timer value (every 100k docs).  I am simply calling `scan` (https://elasticsearch-py.readthedocs.io/en/master/helpers.html#scan) with default values, and iterating over the results. (see **Steps to reproduce** in my original post).

Regarding the `scroll` value, it's plenty long, as I'm not doing any processing of the results, simply iterating over them.  In any case, I've tried multiple values there and it has no affect.
</comment><comment author="jimczi" created="2016-05-11T16:44:18Z" id="218518151">@jsnod sorry for the confusion. It seems that it's the opposite, the scan implementation in elasticsearch-py uses a ridiculously small batch size: 10. I might be wrong:
https://github.com/elastic/elasticsearch-py/blob/6e94bf76adbbbb2fe5b927a3933ed535cd738387/elasticsearch/helpers/__init__.py
@HonzaKral can you confirm ?
</comment><comment author="clintongormley" created="2016-05-12T10:47:56Z" id="218722978">@jsnod I've tried this out locally in 1.7.3.  I indexed 2 million empty docs then scrolled through them (with `search_type=scan`).

With size `10`, these are the times each batch took in seconds:

```
100000     9.558165
200000     9.448229
300000     9.986946
400000     10.793542
500000     11.296383
600000     12.203433
700000     12.712313
800000     13.569650
900000     14.012085
1000000    14.735239
1100000    15.399333
1200000    16.301194
1300000    16.838166
1400000    17.399276
1500000    18.171454
1600000    18.972609
1700000    19.450955
1800000    20.162497
1900000    21.228723
2000000    20.770031
```

With size `1000` (ie num_shards \* 1000), I got a much flatter graph:

```
100000     4.770898
200000     4.679908
300000     4.660660
400000     4.612432
500000     4.797696
600000     4.670094
700000     4.723260
800000     4.754570
900000     4.747548
1000000    4.748295
1100000    4.680469
1200000    4.788206
1300000    4.975887
1400000    4.822974
1500000    4.824638
1600000    4.745880
1700000    4.884137
1800000    4.884558
1900000    4.747686
2000000    4.783460
```

The best value for `size` depends on hardware, doc size etc.  I tried `5000` too and got similar results to size `1000`.  

So I'd recommend setting size to about `1000` (or at least experimenting with values around there).  Failing that, you can break your docs down into tranches (eg a filter on created_date) and run several reindexing jobs.  
</comment><comment author="HonzaKral" created="2016-05-12T11:23:49Z" id="218729900">@jimferenczi The `elasticsearch-py` client doesn't set any default value for the size so it uses the defaults. You can always raise it by passing in a larger number as argument.
</comment><comment author="jsnod" created="2016-05-12T21:56:35Z" id="218897798">EUREKA!  Simply passing in a `size=1000` kwarg into `scan()` has given me results similar to those posted by @clintongormley above:

```
    ...
    Scanned 6,700,000 docs (batch time: 0:00:08 / total time: 0:12:52 )
    Scanned 6,800,000 docs (batch time: 0:00:08 / total time: 0:13:00 )
    Scanned 6,900,000 docs (batch time: 0:00:08 / total time: 0:13:09 )
    Scanned 7,000,000 docs (batch time: 0:00:08 / total time: 0:13:17 )
    Scanned 7,100,000 docs (batch time: 0:00:08 / total time: 0:13:26 )
    Scanned 7,200,000 docs (batch time: 0:00:08 / total time: 0:13:34 )
    Scanned 7,300,000 docs (batch time: 0:00:08 / total time: 0:13:43 )
    Scanned 7,400,000 docs (batch time: 0:00:08 / total time: 0:13:52 )
    Scanned 7,500,000 docs (batch time: 0:00:08 / total time: 0:14:00 )
    Scanned 7,600,000 docs (batch time: 0:00:08 / total time: 0:14:09 )
    Scanned 7,700,000 docs (batch time: 0:00:08 / total time: 0:14:18 )
    Scanned 7,800,000 docs (batch time: 0:00:08 / total time: 0:14:27 )
    Scanned 7,900,000 docs (batch time: 0:00:08 / total time: 0:14:36 )
    Scanned 8,000,000 docs (batch time: 0:00:08 / total time: 0:14:44 )
    Scanned 8,100,000 docs (batch time: 0:00:09 / total time: 0:14:53 )
    Scanned 8,200,000 docs (batch time: 0:00:08 / total time: 0:15:02 )
    Scanned 8,300,000 docs (batch time: 0:00:09 / total time: 0:15:12 )
    Scanned 8,400,000 docs (batch time: 0:00:09 / total time: 0:15:21 )
    Scanned 8,500,000 docs (batch time: 0:00:09 / total time: 0:15:30 )
    Scanned 8,600,000 docs (batch time: 0:00:09 / total time: 0:15:39 )
    Scanned 8,700,000 docs (batch time: 0:00:09 / total time: 0:15:48 )
    Scanned 8,800,000 docs (batch time: 0:00:09 / total time: 0:15:58 )
    Scanned 8,900,000 docs (batch time: 0:00:09 / total time: 0:16:07 )
    Scanned 9,000,000 docs (batch time: 0:00:09 / total time: 0:16:17 )
    Scanned 9,100,000 docs (batch time: 0:00:09 / total time: 0:16:26 )
    Scanned 9,200,000 docs (batch time: 0:00:09 / total time: 0:16:36 )
    Scanned 9,300,000 docs (batch time: 0:00:09 / total time: 0:16:45 )
    Scanned 9,400,000 docs (batch time: 0:00:09 / total time: 0:16:55 )
    Scanned 9,500,000 docs (batch time: 0:00:09 / total time: 0:17:04 )
    Scanned 9,600,000 docs (batch time: 0:00:09 / total time: 0:17:14 )
    Scanned 9,700,000 docs (batch time: 0:00:09 / total time: 0:17:24 )
    Scanned 9,800,000 docs (batch time: 0:00:09 / total time: 0:17:33 )
    Scanned 9,900,000 docs (batch time: 0:00:09 / total time: 0:17:43 )
    ...
```

What previously took 6 hours now takes about 18 minutes!

After pulling my hair out for over a week, it seems insane that `elasticsearch-py` uses a default `size=10` when calling `scan()`.  We were effectively DDoS'ing ES with 100x the number of queries than if we had passed in `size=1000`.  A default of 10 makes sense for `search()` but presumably when calling `scan()` you are iterating over a relatively large set.  I will close this ticket and open a new one at `elasticsearch-py` to discuss upping the default size for `scan()` or at least updating the documentation to explain why using a much larger `size` would be beneficial, especially when calling `reindex()`.

Thanks ES team for getting us past this bump... onward to 2.X!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Python Search API Maximum of ~203 fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18252</link><project id="" key="" /><description>In the Python API, I can only list ~203 fields. 

`data = es.search(index='cms*', fields=fields.keys())`

I have to truncate the fields.keys() to 202 in length. I am assuming that this is due to some max limit on the HTTP request given that 202 is a rather odd number to have a bug around (ie not a multiple of 10 or 2).

The error is

```
  File "data_proc.py", line 37, in &lt;module&gt;
    data = es.search(index='cms*', fields=fields.keys())
  File "/home/users/amacdona/anaconda/lib/python2.7/site-packages/elasticsearch/client/utils.py", line 69, in _wrapped
    return func(*args, params=params, **kwargs)
  File "/home/users/amacdona/anaconda/lib/python2.7/site-packages/elasticsearch/client/__init__.py", line 548, in search
    doc_type, '_search'), params=params, body=body)
  File "/home/users/amacdona/anaconda/lib/python2.7/site-packages/elasticsearch/transport.py", line 329, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "/home/users/amacdona/anaconda/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py", line 105, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(('Connection aborted.', BadStatusLine("''",))) caused by: ProtocolError(('Connection aborted.', BadStatusLine("''",)))

```

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="154100169">18252</key><summary>Python Search API Maximum of ~203 fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aidan-plenert-macdonald</reporter><labels /><created>2016-05-10T20:33:38Z</created><updated>2016-05-10T20:39:29Z</updated><resolved>2016-05-10T20:39:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-10T20:39:29Z" id="218284040">There's a [dedicated repository](https://github.com/elastic/elasticsearch-py) for issues regarding the Python client.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch scripts to use bash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18251</link><project id="" key="" /><description>This commit switches the command-line scripts to use bash instead of sh
so that we can take advantage of features that bash provides like
arrays.

Closes #14002
</description><key id="154081297">18251</key><summary>Switch scripts to use bash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T19:00:01Z</created><updated>2016-05-11T01:09:12Z</updated><resolved>2016-05-10T19:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T19:02:32Z" id="218257140">I still like it. Quoting things in `sh` is pain. `bash`'s arrays make this so much nicer.
</comment><comment author="jasontedor" created="2016-05-10T19:03:42Z" id="218257456">&gt; Quoting things in `sh` is pain. `bash`'s arrays make this so much nicer.

That's exactly what is motivating this PR.
</comment><comment author="dakrone" created="2016-05-10T20:35:42Z" id="218282979">I know this is already merged, but can we add `bash` as a dependency for our RPM and DEB packages now?
</comment><comment author="dakrone" created="2016-05-10T20:36:05Z" id="218283077">I also think this should be marked as "breaking" and have a note in the migration guide
</comment><comment author="nik9000" created="2016-05-10T20:38:21Z" id="218283729">Both of those things are good. It is "breaking" for folks that are trying to support Elasticsearch outside of our packages. Some folks will have to react and that'll help.
</comment><comment author="jasontedor" created="2016-05-11T01:09:12Z" id="218336412">&gt; I know this is already merged, but can we add `bash` as a dependency for our RPM and DEB packages now?
&gt; 
&gt; I also think this should be marked as "breaking" and have a note in the migration guide

I opened #18259.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dangling indices are not imported if a tombstone for the index exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18250</link><project id="" key="" /><description>Dangling indices are not imported if a tombstone for the same index
(same name and UUID) exists in the cluster state.  This resolves a
situation where if an index data folder was copied into a node's data
directory while the node is running and that index had a tombstone in
the cluster state, the index would still get imported.

Closes #18249
</description><key id="154078327">18250</key><summary>Dangling indices are not imported if a tombstone for the index exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T18:46:13Z</created><updated>2016-05-11T16:56:59Z</updated><resolved>2016-05-11T16:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-10T18:47:11Z" id="218252768">@bleskes FYI
</comment><comment author="abeyad" created="2016-05-10T18:48:59Z" id="218253269">@bleskes One aspect up for debate is that if we run into this situation, I delete the contents on disk.  I do this because if the index remained on disk, we would keep trying to import it on every single cluster state change (with the message appearing in the logs).  Also, I feel that it is safe because if the index folder was copied from somewhere, presumably the index folder still exists in the location they copied it from.

I am interested in your feedback on this.
</comment><comment author="bleskes" created="2016-05-10T18:53:36Z" id="218254506">&gt;  I do this because if the index remained on disk, we would keep trying to import it on every single cluster state change (with the message appearing in the logs).

I think that would be the way to go to be honest - remember this is an odd ball case where someone copied a folder of a delete index in a node  - it will be weird / annoying to just delete it like a /dev/null . Since CS updates are in most deployment not super frequent I would opt for a warning as opposed to deleting the folder. 
</comment><comment author="abeyad" created="2016-05-10T18:56:54Z" id="218255488">@bleskes OK, I'll change the PR now to just contain the warning
</comment><comment author="abeyad" created="2016-05-10T19:05:15Z" id="218257862">@bleskes I've pushed up changes so its no longer deleting the folder
</comment><comment author="bleskes" created="2016-05-11T10:16:48Z" id="218418883">LGTM except for two minor comments. Thanks @abeyad 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dangling Indices should not be imported if a tombstone exists for the same Index UUID</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18249</link><project id="" key="" /><description>A somewhat rare but nevertheless problematic scenario can occur if a data folder containing a deleted index is copied over to a node.  Imagine the following steps:
1. We have a one node in the cluster, `A`
2. `A` creates an index called `idx` with UUID `xyz`.
3. The data directory for node `A` is copied to some external location.
4. `idx` is deleted from the cluster.
5. Start up a new node `B` in the cluster
6. After `B` has started up, copy the data directory previous put in an external location to the data directory for `B`
7. Start a new node `C`, just in order to trigger a cluster state update.

At this point, the previously deleted `idx` with UUID `xyz` is imported into the index as a dangling index, even though a tombstone exists for it in the cluster state.  This leads to a very confusing situation where the index exists in the cluster state and at the same time, there is a tombstone for it.  Also, when the node restarts, it will check the tombstones, see the index is deleted, so delete it - that means the index re-appeared in the cluster state for some time but will be deleted again upon node restart.

Also, note that this problem does not occur if the data directory is copied before `B` is started, because in that case on node startup, the tombstones are checked so the index is not permitted to be imported as dangling.
</description><key id="154060200">18249</key><summary>Dangling Indices should not be imported if a tombstone exists for the same Index UUID</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T17:17:15Z</created><updated>2016-05-11T16:56:59Z</updated><resolved>2016-05-11T16:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Feature request: search highlight pagination</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18248</link><project id="" key="" /><description>##### Use case

I am building a searchable document viewer with the primary goal of providing a significantly better search experience than CTRL+F. When the user searches, it jumps to the most relevant fragment rather than the first one (like CTRL+F). And there is a sidebar which lists all search results within the document ordered by relevance. When the user clicks a result, the viewer jumps to that fragment.
##### Problem

Some documents are 100s of pages long therefore a search can return 1000s of results. In order to get all results within a document currently, I can either set `number_of_fragments` to zero or a very large number. The problem with setting to zero is that I lose all scoring information. The problem with setting a very large number is that it is inefficient.
##### Solution

I think search highlight pagination would solve the problem.
</description><key id="154058088">18248</key><summary>Feature request: search highlight pagination</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rpedela</reporter><labels /><created>2016-05-10T17:06:35Z</created><updated>2016-05-11T12:14:17Z</updated><resolved>2016-05-11T09:55:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-11T09:55:02Z" id="218414206">@rpedela the right solution is to index your very long document as individual pages (perhaps as parent child)
</comment><comment author="rpedela" created="2016-05-11T12:14:17Z" id="218441577">It isn't one document that is 100s of pages. I have millions of them. Tika (or any tool I have tried) doesn't give page numbers when extracting raw text so I don't really see how that is a solution.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement needsScore() correctly.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18247</link><project id="" key="" /><description>Scripts have to report whether or not they use this. If they don't need `_score` then e.g. more things can be cached, for example queries.

Expressions is currently the only scripting language that doesn't always return `true` for this method. But because we have special handling for it, we should make us of this info and record if its needed.
</description><key id="154039614">18247</key><summary>Implement needsScore() correctly.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T15:43:02Z</created><updated>2016-05-17T12:19:12Z</updated><resolved>2016-05-10T23:21:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-10T15:45:21Z" id="218200493">I ported over the test expressions is using here. Basically we just apply a marker interface if scores are needed and then we can check with `instanceof`. I guess alternatively it could be an annotation or something like that, but this seems simple enough?

Separately we should followup with the special score handling. Maybe the scoreAccessor should not just be shoved into the `input` map like we do, but instead just passed in as a normal parameter (null if not needed or n/a)? This would avoid unnecessary per-document hashing.
</comment><comment author="uschindler" created="2016-05-10T15:53:17Z" id="218202947">Interface is fine, annotation is IMHO too complicated, but can be done without much changes.
</comment><comment author="jdconrad" created="2016-05-10T16:45:00Z" id="218218144">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect highlight when using in_order/slop on span_near query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18246</link><project id="" key="" /><description>Reported by a user on the forums:  https://discuss.elastic.co/t/highlighting-changes-from-1-7-to-2-3/49661

It looks like span_near queries in 2.3+ do not highlight properly when `in_order` and `slop` are specified.  Specifically, matching terms that _are not_ within the slop are also highlighted. I checked and this applies to 5.0-alpha as well.  This seems like a bug to me, rather than an intentional change.

Replication from the thread:

```
curl -XPUT 'http://localhost:9200/twitter/' -d '{
    "mappings": {
        "tweet": {
            "properties": {
                "message": {
                    "type": "string", 
                    "store": true
                }
            }
        }
    }
}'


curl -XPUT 'localhost:9200/twitter/tweet/1?refresh=true' -d '{
    "message" : ["short leg twice syndrome","short syndrome"]
}'
```

Then run this span_near query:

```
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty' -d '{
    "query" : {
        "span_near" : {
            "clauses" : [
                {"span_term": {"message": "short"}}, 
                {"span_term": {"message": "syndrome"}}
            ], 
            "slop": 0, 
            "in_order": true
        }
    }, 
    "highlight": {"fields": {"message": {"type": "plain"} } }
}'
```

in ES 1.7.1 you get:

``` json
{
  "took" : 56,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.11072598,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 0.11072598,
      "_source":{
    "message" : ["short leg twice syndrome","other words short syndrome"]
},
      "highlight" : {
        "message" : [ "other words &lt;em&gt;short&lt;/em&gt; &lt;em&gt;syndrome&lt;/em&gt;" ]
      }
    } ]
  }
}
```

in ES 2.3.2:

``` json
{
  "took" : 56,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.19178301,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 0.19178301,
      "_source" : {
        "message" : [ "short leg twice syndrome", "other words short syndrome" ]
      },
      "highlight" : {
        "message" : [ "&lt;em&gt;short&lt;/em&gt; leg twice &lt;em&gt;syndrome&lt;/em&gt;", "other words &lt;em&gt;short&lt;/em&gt; &lt;em&gt;syndrome&lt;/em&gt;" ]
      }
    } ]
  }
}
```

In 2.3.2 , 'short' and 'syndrome' in "short leg twice syndrome" get highlighted, even though they don't fulfill the terms of the slop/inorder parts of the span_near query.
</description><key id="154029832">18246</key><summary>Incorrect highlight when using in_order/slop on span_near query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2016-05-10T15:04:53Z</created><updated>2016-11-05T16:23:23Z</updated><resolved>2016-10-25T14:23:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-11T09:49:53Z" id="218413028">This works correctly in 5.0
</comment><comment author="mkrauklis" created="2016-10-25T14:08:37Z" id="256045131">Is there any active investigation for this bug?
</comment><comment author="jimczi" created="2016-10-25T14:23:46Z" id="256049390">This works correctly in 2.4 so I'll close this ticket. 
@mkrauklis which version are you using ? 
</comment><comment author="mkrauklis" created="2016-10-25T15:01:49Z" id="256060995">We're on 2.3.3. I'll give it a try on 2.4.x. Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up tests in Reindex module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18245</link><project id="" key="" /><description>Some clean ups in the tests of the Reindex modules now `ReindexResponse` has been removed in #18205 

@nik9000 Can you have a look please? Thanks
</description><key id="154024222">18245</key><summary>Clean up tests in Reindex module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T14:43:51Z</created><updated>2016-05-11T08:01:43Z</updated><resolved>2016-05-11T08:01:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T16:38:40Z" id="218216384">LGTM Thanks! This is very nice.
</comment><comment author="tlrx" created="2016-05-11T08:01:43Z" id="218389419">Thanks @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate analyser aliases?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18244</link><project id="" key="" /><description>Does anybody use aliases for analyzers?  (https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html#aliasing-analyzers)

I've never heard of anybody and can't think of a use case.  I think they should be deprecated.
</description><key id="154017235">18244</key><summary>Deprecate analyser aliases?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>deprecation</label><label>low hanging fruit</label><label>v5.0.0-beta1</label></labels><created>2016-05-10T14:14:57Z</created><updated>2016-09-14T14:44:25Z</updated><resolved>2016-08-15T20:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-08T08:35:49Z" id="231305498">+1 I think we should get rid of this in 6
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NullPointerException in SimpleQueryParser when analyzing text produces a null query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18243</link><project id="" key="" /><description>Resolves #18202

This is already fixed on Master/5.x, so this is just for the 2.x branches. I will forward-port the test though, always good to have more tests.
</description><key id="154014489">18243</key><summary>Fix NullPointerException in SimpleQueryParser when analyzing text produces a null query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-05-10T14:03:41Z</created><updated>2016-05-10T14:33:21Z</updated><resolved>2016-05-10T14:29:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-10T14:28:11Z" id="218174614">LGTM
</comment><comment author="dakrone" created="2016-05-10T14:33:21Z" id="218176300">Thanks Adrien!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add infrastructure support for microbenchmarking</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18242</link><project id="" key="" /><description>### Rationale

We have recently removed a few benchmarks from Elasticsearch core (see #15356). Although most of our benchmarking should be on the macro-level (i.e. running against a real Elasticsearch cluster) we're sometimes interested in benchmarking at a more narrow level. As we don't need a fully running cluster in that case, it makes sense to base them on [JMH](http://openjdk.java.net/projects/code-tools/jmh/) as it is the de-facto standard for microbenchmarking on the JVM.
### Scope

The scope of this ticket is just to add the necessary infrastructure to our build scripts (and maybe provide one or two example benchmarks). We will not recreate the former benchmark suite in this ticket (nor should we aim to recreate the whole suite again). 
### Next Steps

After this is set up, we should run the benchmarks also regularly on Jenkins (on a dedicated quiet machine for benchmarks) and add proper reporting, e.g. with the [JMH Benchmark Jenkins Plugin](https://github.com/blackboard/jmh-jenkins) to spot performance regressions.
</description><key id="154001976">18242</key><summary>Add infrastructure support for microbenchmarking</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>build</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-10T13:08:37Z</created><updated>2016-06-16T11:54:44Z</updated><resolved>2016-06-15T14:48:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add synthetic length property as alias to Lists, so they can be used like arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18241</link><project id="" key="" /><description>While writing tests for the array loads and stores I noticed some inconsistency in painless:

Syntax-wise you can exchange Lists and arrays, e.g. access a List like an array (this also works with maps, but that is out of scope for this issue). I failed to transform a simple `for` loop using `array.length` to using a List. The loads/stores worked, but Lists don't have a `length`property. To really allow lists to be used as arrays, we must add the "length" property manually.

The PR is simple: We need no change in the indy bootstrap part, a Definition is enough. The trick is to add an alias "getLength" to `List`, `List&lt;Object&gt;`, and `List&lt;String&gt;`. Then the property works automatically.
</description><key id="154001682">18241</key><summary>Add synthetic length property as alias to Lists, so they can be used like arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T13:07:11Z</created><updated>2016-05-17T12:23:43Z</updated><resolved>2016-05-10T16:37:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-10T13:11:30Z" id="218152431">+1 good idea and simple to do. It is nice since values in documents are also List and recommended with array syntax.

the javascript plugin even has a rest test for this: https://github.com/elastic/elasticsearch/blob/master/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml#L410
</comment><comment author="uschindler" created="2016-05-10T13:42:09Z" id="218160578">&gt; the javascript plugin even has a rest test for this

I won't write integration tests :-)
</comment><comment author="rmuir" created="2016-05-10T14:06:10Z" id="218167781">yes your test is the way to go here. i only looked at the other engines the other day to better round out our integration tests. Just trying to get an idea of what is missing, what is cumbersome, etc.
</comment><comment author="jdconrad" created="2016-05-10T16:37:08Z" id="218215964">@uschindler Thanks!  This is a good change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to disable GPG signing based on env variable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18240</link><project id="" key="" /><description>With this commit we allow to disable GPG signing based on the value
of the environment variable 'OSDISTRO'. This is needed as there are
compatibility issues with gpg on some platforms which are
complicated to fix (see e.g. mojohaus/rpm-maven-plugin#44 for more details).

We already have the possibility to disable GPG signing based on
the 'skipSign' property but this does not work well with our build 
infrastructure. As all builds are configured uniformly it is not easy 
to define different Maven settings for different build machines. 
Therefore, we will set the environment variable 'OSDISTRO'
on all machines in CI by issuing

`export OSDISTRO=$(facter os.distro.description)`

The Maven build script will then check the value of this variable and
disable GPG signing if it is known not to work on this platform together
with our build infrastructure (currently this is only Fedora 23).
</description><key id="153996593">18240</key><summary>Allow to disable GPG signing based on env variable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>build</label><label>v2.4.0</label></labels><created>2016-05-10T12:41:41Z</created><updated>2016-08-26T13:15:47Z</updated><resolved>2016-05-19T08:29:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-05-10T12:48:22Z" id="218146796">Hints for testing this change:
1. Set the environment variable `OSDISTRO`: `export OSDISTRO="Fedora release 23 (Twenty Three)"`
2. Check that the profile is active: `mvn help:active-profiles -pl distribution` (or do a full build and check the RPM is not signed)
</comment><comment author="danielmitterdorfer" created="2016-05-17T12:56:27Z" id="219708951">@nik9000 Could you please have look?
</comment><comment author="nik9000" created="2016-05-17T14:21:56Z" id="219732942">I'm ok with this change - it'll cause the vagrant tests to fail on distros that don't support signing but maybe that is ok? It is better than it is now. If you want to get the vagrant tests working on those things you might want to write whether or not signing was enabled to some file in the vagrant tests and skip the tests if the files contain `false`. But that can wait for another PR so we can get these distros un-broken.
</comment><comment author="nik9000" created="2016-05-17T14:22:26Z" id="219733102">Maybe a note in contributing.md? The weird thing is that this signing was never forward ported to 5.0 iirc. I think that might still be left to do?
</comment><comment author="danielmitterdorfer" created="2016-05-18T09:52:48Z" id="219979344">@nik9000 Thanks for looking at it. To your comments:

&gt; it'll cause the vagrant tests to fail on distros that don't support signing [...]

It seems to me that this is solved already in the Vagrant tests by using only boxes that support RPM signing (see https://github.com/elastic/elasticsearch/blob/2.x/qa/vagrant/pom.xml#L24). So I think they should not be affected by this change, no?

&gt; Maybe a note in contributing.md?

You mean that I should document there that `OSDISTRO` has to be set to disable RPM signing? If yes, then I'd still use `-DskipSign` on dev machines. This change is really intended for CI environments.

&gt; The weird thing is that this signing was never forward ported to 5.0 iirc. I think that might still be left to do?

I don't know about that but maybe @rjernst or @spinscale know about the status of package signing on master?
</comment><comment author="nik9000" created="2016-05-18T11:21:02Z" id="219997394">&gt; So I think they should not be affected by this change, no?

That is a list of guest boxes. The tests will fail if the host can't sign. That is probably ok though.

&gt; You mean that I should document there that OSDISTRO has to be set to disable RPM signing? If yes, then I'd still use -DskipSign on dev machines. This change is really intended for CI environments.

Or anyone that wants to run the vagrant tests....
</comment><comment author="danielmitterdorfer" created="2016-05-18T13:46:54Z" id="220030656">@nik9000 I've pushed another commit which adds documentation in `TESTING.asciidoc` how to disable RPM signing (I think this is a more suitable place than the contribution file). Can you please check?
</comment><comment author="nik9000" created="2016-05-18T18:03:28Z" id="220109606">LGTM
</comment><comment author="rjernst" created="2016-05-18T18:06:36Z" id="220110440">&gt; The weird thing is that this signing was never forward ported to 5.0 iirc. I think that might still be left to do?

Signing will be done completely outside of the ES gradle build in 5.0 (as part of the unified release).
</comment><comment author="danielmitterdorfer" created="2016-05-19T08:25:52Z" id="220259396">@nik9000 Thanks for the review. I'll update the docs once more and merge then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Further simplifications of plugin script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18239</link><project id="" key="" /><description>In 7d1fd17172f5f8cb7ed381de1ff23dfb9f1856a5 the parsing of command-line
properties in the plugin script was removed. That commit missed
additional parsing of the properties for es.default.path.conf. This
commit removes that parsing and also replaces the use of eval with exec.

Relates #18207
</description><key id="153984283">18239</key><summary>Further simplifications of plugin script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T11:30:56Z</created><updated>2016-05-10T14:10:06Z</updated><resolved>2016-05-10T14:08:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T13:26:47Z" id="218156394">LGTM
</comment><comment author="jasontedor" created="2016-05-10T14:10:06Z" id="218168978">Thanks @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for deprecated REST endpoints to the REST specs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18238</link><project id="" key="" /><description>today the REST Specs support `path` (the main endpoint) and `paths` (all variation of possible endpoints). Would be nice to also support `deprecated_paths`. This will enable testing them as long as they're still supported. Also, by isolating these, auto-generated client can skip these paths when generating their code.
</description><key id="153975230">18238</key><summary>Add support for deprecated REST endpoints to the REST specs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:REST</label><label>discuss</label><label>enhancement</label><label>test</label></labels><created>2016-05-10T10:37:45Z</created><updated>2017-05-05T14:47:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T13:39:47Z" id="218159915">Relates to https://github.com/elastic/elasticsearch/issues/17512. At least, it relates a little bit.
</comment><comment author="javanna" created="2017-04-14T14:38:18Z" id="294164895">We now always return deprecation warnings as headers, it is up to clients to act on them e.g. log them or even throw an error. We have been testing since then deprecations from both unit tests and yaml tests: we assert by default that no deprecated functionality is used, and we declare exceptions in case we want to test the deprecations themselves, to verify that they are properly raised. 

I am not sure about the auto-generated clients aspect. They could skip deprecated paths if they were declared separately in the spec. But then we would force users to migrate as soon as something gets deprecated, which is one major version before Elasticsearch actually removes the endpoint.</comment><comment author="javanna" created="2017-05-05T13:28:03Z" id="299464394">The users migration point that I made applies to cases where endpoints are removed after being deprecated, where I think clients should still expose them although deprecated, but then the whole endpoint is deprecated, not just one of its paths.

When it comes to renaming endpoints though, we usually remove from the spec the deprecated path, which becomes untested although it is still exposed in core, so there is no ambiguity and auto-generated clients know which path to pick. This would be improved by adding a `deprecated_paths` section.</comment><comment author="javanna" created="2017-05-05T13:29:59Z" id="299464821">@elastic/es-clients what do you think?</comment><comment author="Mpdreamz" created="2017-05-05T14:47:23Z" id="299484992">++ to a deprecated_paths section, I think clients should still generate the code for them but clients could then automatically mark them as obsolete. 

The .NET code generator can patch the spec where needed and with 5.4 there was one endpoint removed (exist alias) that we had to write a patch file for. 

Ideally it would be a map of endpoint/deprecation reason </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add the ability to partition a scroll in multiple slices.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18237</link><project id="" key="" /><description>API:

```
curl -XGET 'localhost:9200/twitter/tweet/_search?scroll=1m' -d '{
    "slice": {
        "field": "_uid", &lt;1&gt;
        "id": 0, &lt;2&gt;
        "max": 10 &lt;3&gt;
    },
    "query": {
        "match" : {
            "title" : "elasticsearch"
        }
    }
}
```

&lt;1&gt; (optional) The field name used to do the slicing (_uid by default)
&lt;2&gt; The id of the slice

By default the splitting is done on the shards first and then locally on each shard using the _uid field
with the following formula:
`slice(doc) = floorMod(hashCode(doc._uid), max)`
For instance if the number of shards is equal to 2 and the user requested 4 slices then the slices 0 and 2 are assigned
to the first shard and the slices 1 and 3 are assigned to the second shard.

Each scroll is independent and can be processed in parallel like any scroll request.

Closes #13494
</description><key id="153972238">18237</key><summary>Add the ability to partition a scroll in multiple slices.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jimczi</reporter><labels><label>:Scroll</label><label>feature</label><label>release highlight</label><label>v5.0.0-alpha4</label></labels><created>2016-05-10T10:21:04Z</created><updated>2016-06-15T08:08:53Z</updated><resolved>2016-06-07T14:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-10T10:37:46Z" id="218120840">Relates to https://github.com/elastic/elasticsearch/issues/13494#issuecomment-216839998 / @s1monw 
</comment><comment author="s1monw" created="2016-05-10T10:49:58Z" id="218123284">hey @jimferenczi this is awesome. Yet, I have concerns about the field data. Loading all the stuff into fielddata only has a real benefit if it's reused but I am not sure if that is the common case. I wonder if we should use something like a dedicated query that we can use that walks the `uid` term dictionary and calculates the hash lazily for each value and omits the ones that don't match the bucket? I think it's not making much of a difference but will be much nicer in terms of memory consumption and scalability? I even think we can just use uid by default and don't worry about making it configurable in the first place? I mean we can come back to it but it's really just for reproducibility, right?
</comment><comment author="jimczi" created="2016-05-10T20:46:45Z" id="218286055">@s1monw I've pushed another change that uses a BitSet per sliced scroll instead of building a specialized fieldcache for the _uid field: https://github.com/elastic/elasticsearch/pull/18237/commits/f391a5997c21d74c5628bb52512b29bdd52f4833
As you mentioned earlier this reduces the cost in memory to 1 bit per document per slice (was 4 bytes per document total with the fielddata).  

@jpountz I needed a way to tell the query cache that certain query should never be cached. For instance the SliceTermQuery builds a bitset for the toplevel reader associated with a sliced scroll. This bitset is valid during the lifetime of the scroll but should be freed when the scroll is cleared.
Could you please take a look at the NotCacheable interface and the way I use it ? 
</comment><comment author="s1monw" created="2016-05-11T12:42:48Z" id="218447686">@jimferenczi I left a bunch of comments
</comment><comment author="jpountz" created="2016-05-11T13:03:51Z" id="218452564">@jimferenczi For caching I think we have two options: we can either make the top-level index reader part of the query's equals/hashcode definitions so that the cache would only be used if the query is reused on the same top-level reader. This means that the query will only be cached in practice if the index is pretty static.

In case we really never want to cache (eg. if these queries are cheaper than a cached DocIdSet, like the MatchAllDocsQuery iterator for instance), then your QueryCachingPolicy wrapper looks good to me. I would just move it to its own class instead of an anonymous class in IndexShard?
</comment><comment author="jpountz" created="2016-05-11T15:07:44Z" id="218488741">`TermsSliceQuery` annoys me a bit given that the point of this pull request is to be able to process the output of a scroll in parallel. Yet the createWeight operation runs in O(maxDoc), so it defeats the point?
</comment><comment author="jimczi" created="2016-05-11T16:13:18Z" id="218509364">@jpountz  the TermsSliceQuery is just a way to avoid the usage of the fielddata on the _uid field. 

&gt; In case we really never want to cache (eg. if these queries are cheaper than a cached DocIdSet)

They are not cheaper. 

The DocValuesSliceQuery is not faster but avoids the bitset entirely.
The main problem with the bitset is that we heavily rely on it, if it's already built we're lightning fast and if not we're terribly slow. It's hard to control the I/O in the query cache and I wanted to avoid cache miss due to the activity on the node. 
I think it's better to never cache the DocValuesSliceQuery.

&gt; yet the createWeight operation runs in O(maxDoc)

The TermsSliceQuery is supposed to be a cached DocIdSet but I wanted to avoid the QueryCachingPolicy. I agree that it should use the query cache but only if we can ensure that this entry will reliably stay around during the lifespan of the scroll. Currently this is achieved by keeping the query (and the bitset built during the initial scroll query) in the SearchContext.
</comment><comment author="jpountz" created="2016-05-11T16:46:07Z" id="218518663">Forcing the query cache te hold on an entry for a certain amount of time would require to add more APIs, which I'd like to avoid. So I think the current approach to cache at the query/searchcontext level is better?
</comment><comment author="s1monw" created="2016-05-12T09:34:49Z" id="218707367">@jimferenczi I had a conversation with @jpountz and I think we should go without the sorting altogether in the first iteration of this feature. I think we should have another conversation how we can implement this feature really just as a parallel version of scroll that only guarantees that we partition all docs within a single scroll ID. ie. we somehow need to maintain a context per slice on the same scroll context. I am not sure about the implementation yet but lets have a chat about it first. I also talked to @costin and we are good with that.
</comment><comment author="jimczi" created="2016-05-17T09:47:22Z" id="219670237">@jpountz @s1monw  I've pushed another commit.
Firstly I removed the NotCacheable interface, the DocValuesSliceQuery will be able to use the regular query cache like any other query. 
Tough I did not remove the TermsSliceQuery yet, the cost is still O(maxDoc) but since the query is now cacheable in the query cache I think it gives a good OOB experience. If you're not convinced I'll remove it.
</comment><comment author="jimczi" created="2016-05-19T20:33:56Z" id="220443971">I did some testing of the `slice` feature. The following table shows the results for a full scroll of an index of 1M random documents on 1 shard (no replica) with different scroll configurations. The configuration contains the number of slices in the scroll, the number of documents per slice, the size of each request within the slice and the number of threads. The "elapsed time" is the total time for the scroll to complete in seconds. For instance the first line is for a scroll with 1 slice (current behavior), 1M documents in that slice, 10,000 documents per scroll request and 1 thread. The elapsed time is 23s. 
The slicing is done on the _uid (only when the number of slices is greater than 1) and the query cache is cleared after each run. I've kept the worst run out of 10 for each configuration:

| Number of slice | Number of documents per slice | Fetch size | Number of threads | Elapsed time |
| :-: | :-: | :-: | :-: | :-: |
| 1 | 1000000 | 10000 | 1 | **23s** |
| 1 | 1000000 | 1000 | 1 | 25s |
| 1 | 1000000 | 100 | 1 | 40s |
| 10 | 100000 | 10000 | 4 | **8s** |
| 10 | 100000 | 10000 | 3 | 10s |
| 10 | 100000 | 10000 | 2 | 12s |
| 10 | 100000 | 10000 | 1 | 25s |
| 10 | 100000 | 1000 | 4 | **8s** |
| 10 | 100000 | 1000 | 3 | 10s |
| 10 | 100000 | 1000 | 2 | 13s |
| 10 | 100000 | 1000 | 1 | 27s |
| 10 | 100000 | 100 | 4 | 11s |
| 10 | 100000 | 100 | 3 | 15s |
| 10 | 100000 | 100 | 2 | 19s |
| 10 | 100000 | 100 | 1 | 38s |
| 100 | 10000 | 10000 | 4 | 11s |
| 100 | 10000 | 10000 | 3 | 14s |
| 100 | 10000 | 10000 | 2 | 21s |
| 100 | 10000 | 10000 | 1 | 43s |
| 100 | 10000 | 1000 | 4 | 12s |
| 100 | 10000 | 1000 | 3 | 15s |
| 100 | 10000 | 1000 | 2 | 23s |
| 100 | 10000 | 1000 | 1 | 46s |
| 100 | 10000 | 100 | 4 | 13s |
| 100 | 10000 | 100 | 3 | 17s |
| 100 | 10000 | 100 | 2 | 26s |
| 100 | 10000 | 100 | 1 | 54s |
| 1000 | 1000 | 1000 | 4 | 44s |
| 1000 | 1000 | 1000 | 3 | 57s |
| 1000 | 1000 | 1000 | 2 | 85s |
| 1000 | 1000 | 1000 | 1 | 175s |
| 1000 | 1000 | 100 | 4 | 54s |
| 1000 | 1000 | 100 | 3 | 71s |
| 1000 | 1000 | 100 | 2 | 106s |
| 1000 | 1000 | 100 | 1 | 212s |
</comment><comment author="s1monw" created="2016-05-23T20:35:33Z" id="221088145">I really like the simplicity of the implementation I am still torn on the default uuid impl - @jpountz what do you think? What scares me is that the amount of bitsets is linear to the number of slices which can easily cause memory explosion.
</comment><comment author="jimczi" created="2016-05-24T19:15:23Z" id="221374025">@s1monw I pushed some changes to address your comments. 

&gt; What scares me is that the amount of bitsets is linear to the number of slices which can easily cause memory explosion.

I don't like it either but the same memory explosion could happen with range queries on point fields for instance ?
What bugs me here is the cost of this _uid field, it is indexed and stored which means that we write a dictionary of almost unique values twice but we cannot access the value for each document quickly (https://github.com/elastic/elasticsearch/issues/11887). Isn't it possible to remove it from the stored fields and rely on the doc_values instead ? I know that the cost would be bigger but it could be counter balanced if we use the SortedDocValues which uses a prefix compression for its dictionary ?
We could also reuse the terms dictionary of the postings lists, right ? A SortedDocValues that shares the dictionary of values with the postings ? It could be a cheap way to add doc_values to big cardinality fields but this would require to have ordinals (and a way to seek directly to an ordinal) in the terms dictionary.
</comment><comment author="jpountz" created="2016-05-24T20:01:04Z" id="221384991">&gt; Isn't it possible to remove it from the stored fields and rely on the doc_values instead ?

It is possible. For instance this is already what we are doing with the `_version`. The trade-off is not totally clear to me however, there is a disk usage/feature trade-off but then even if we decide to enable doc values on _uid (or probably rather on _type and _id), it is not clear to me whether we should use SORTED or BINARY doc values.

I think the easiest way to make progress here would be to remove the ability to slice on the _uid, merge this PR, and later see how annoyed users are with having to provide a field for slicing. If this looks like something we need to fix then we can prioritize #11887?

&gt; We could also reuse the terms dictionary of the postings lists,

I think I remember @mikemccand talking about it but I do not remember the conclusion. I suspect one challenge is that the terms dict of the inverted index needs to be able to seek by bytes while the terms dict of doc values needs to be able to seek by ord.
</comment><comment author="s1monw" created="2016-05-25T07:45:43Z" id="221497919">+1 to get it in without the uid support for now!
</comment><comment author="jimczi" created="2016-05-25T08:04:36Z" id="221501733">IMO we should not get it in unless we have a good solution with uid. In fact I think that only the solution with _uid should be used otherwise it may be too difficult to use this feature and even harder to integrate it into an existing one (reindex, hadoop, ...). Sorry if I am pushy on this but could we freeze this until we have a good solution to load the _uid (or just the _id) in the doc values ? I would be happy to work on this first and get this in afterward. I think it would be nice to have a Sorted(Set)DocValues that shares the dictionary with the postings, @jpountz @mikemccand what is the status of this:
org.apache.lucene.codecs.blocktreeords.OrdsBlockTreeTermsWriter ? It seems that we could use this dictionary (which gives an ordinal to every term and is able to seek by ord) for the postings and the DocValues. It could be a nice way to support doc values on indexed fields with minimal cost, WDYT ? Again I don't want to block anything but I have the feeling that this feature won't be used at all if the user needs to provide a field with obscure properties. 
</comment><comment author="jpountz" created="2016-05-25T13:32:32Z" id="221576923">So maybe it is time to start exploring the consequences of adding doc values to the _id field and stopping to store the _uid field (in terms of indexing throuhput, search performance and disk space). If this does not make things worse than today then I would be in favour of moving forward here as having doc values on the _id field does indeed make some things easier, but otherwise I would be cautious as indexing/search speed and disk usage are probably more common concerns among our users than the inability to consume scrolls from multiple clients.
</comment><comment author="mikemccand" created="2016-05-25T14:31:57Z" id="221594826">Can we slice using Lucene's docIDs?  Don't we already ensure follow-on scroll requests get exactly the same `IndexReader` instances, in which case docIDs are safe? (Hmm but what happens today if the replica a scroll request is using goes down...).

We have talked in the past about a combined postings + docvalues format that shared its terms dictionary, and I think maybe I had a rough starting point somewhere (though I can't find an issue in Jira now...).  It'd be somewhat complex, i.e. more than just `OrdsBlockTreeTermsWriter` because it would also have to do the "uninversion" and store the doc -&gt; ords mapping.
</comment><comment author="jimczi" created="2016-05-25T16:43:41Z" id="221635048">&gt; Can we slice using Lucene's docIDs? Don't we already ensure follow-on scroll requests get exactly the same IndexReader instances, in which case docIDs are safe? (Hmm but what happens today if the replica a scroll request is using goes down...).

@mikemccand that's exactly what I wanted to avoid. Relying on the same reader/docIDs complicates things and IMO we loose all the benefits of slicing (resume long running tasks and use multiple replicas for the same query).

&gt; We have talked in the past about a combined postings + docvalues format that shared its terms dictionary, and I think maybe I had a rough starting point somewhere (though I can't find an issue in Jira now...). It'd be somewhat complex, i.e. more than just OrdsBlockTreeTermsWriter because it would also have to do the "uninversion" and store the doc -&gt; ords mapping.

Yes so the idea would be to use the NumericDocValues to store the ordinals and to rely on the terms dictionary for the ord lookup. Though I have no idea how the ord lookup in the OrdsBlockTree would perform. Anyway it's just one idea if we say that the cost of the _id "uninversion" is too big.

&gt; If this does not make things worse than today then I would be in favour of moving forward here as having doc values on the _id field does indeed make some things easier, but otherwise I would be cautious as indexing/search speed and disk usage are probably more common concerns among our users than the inability to consume scrolls from multiple clients.

If you're ok I'll try the different alternatives to evaluate the impact ? I totally agree that we should not penalize the users for one small use case even though I see many other use cases that would benefit from that.
</comment><comment author="jpountz" created="2016-05-25T16:59:55Z" id="221639523">&gt; If you're ok I'll try the different alternatives to evaluate the impact ?

+1
</comment><comment author="mikemccand" created="2016-05-26T09:38:09Z" id="221824730">&gt; @mikemccand that's exactly what I wanted to avoid. Relying on the same reader/docIDs complicates things and IMO we loose all the benefits of slicing (resume long running tasks and use multiple replicas for the same query).

I see...  I agree it is brittle today, since that particular shard going down means your scroll is broken.

It's just scary to see 1 bit for every doc times the number of slices in heap here.  And you're right that queries like `PointRangeQuery` will do the same, but the number of those is naturally bounded by the search queue size whereas these bitsets are arbitrarily long lived right (how long user takes to consume the scrolls)?

If ES switched to Lucene's NRT (segment file) replication then searchers across primary and replicas would all search the same precise point-in-time view and we could safely use any replica's searcher, slicing by docID, since they would all match.  But that is a massive change with its own complex tradeoffs ;)

If we enabled doc values for `_uid` you would be able to use the ords to generate the slices right?  So the shared terms dict would be mostly a "reducing added disk usage" concern, though not storing the `_uid` values will already offset that.
</comment><comment author="jimczi" created="2016-05-26T13:03:48Z" id="221864457">&gt; It's just scary to see 1 bit for every doc times the number of slices in heap here. And you're right that queries like PointRangeQuery will do the same, but the number of those is naturally bounded by the search queue size whereas these bitsets are arbitrarily long lived right (how long user takes to consume the scrolls)?

Not exactly, we need one bitset per slice that's true but not all at the same time. Each slice is independent and it would be nuts to query them all in parallel. You could for instance split your query in 10,000 slices and use 4 threads/workers to process the scrolls (that would be 4 bitsets in the heap at a given time). Each scroll corresponds to one slice and each bitset can be trashed when the corresponding scroll is exhausted. The memory explosion could happen only if you send thousands of slice queries in parallel which I repeat would be ineffective.

&gt; If we enabled doc values for _uid you would be able to use the ords to generate the slices right? So the shared terms dict would be mostly a "reducing added disk usage" concern, though not storing the _uid values will already offset that.

Unfortunately not. We need the term itself simply because we cannot guarantee that the ord will remain consistent if the reader changes.
</comment><comment author="jimczi" created="2016-05-26T13:44:01Z" id="221874518">Just to clarify things regarding the bitsets, the slice scroll on _uid is implemented with the TermsSliceQuery.
The TermsSliceQuery enumerates the terms dictionary and builds a bitset with the matching documents.
This query is added as a filter for every scroll requests with a slice in it. This query treats the bitset exactly like the PointRangeQuery, we trash it at the end of the request. If for one slice we need 5 requests to get all the documents back each request would have to recompute the bitset entirely. Now it can be considerably faster when the query is cached which will eventually happen (like any query) depending on the query cache activity. 
This means that the number of bitset in the heap is still bounded by the size of the query cache plus the size of the search thread pool, nothing more. 
</comment><comment author="mikemccand" created="2016-05-26T13:48:35Z" id="221875781">&gt; This means that the number of bitset in the heap is still bounded by the size of the query cache plus the size of the search thread pool, nothing more.

Ahh, I didn't realize that ... that's great.

&gt; We need the term itself simply because we cannot guarantee that the ord will remain consistent if the reader changes.

Oh that's right.
</comment><comment author="jimczi" created="2016-06-01T10:11:30Z" id="222950526">@s1monw @jpountz I think we are protected against memory explosion (see @mikemccand comments).
WDYT ?
</comment><comment author="s1monw" created="2016-06-01T11:30:12Z" id="222965849">it's still trappy with a large number of slices? can we somehow split the number of slices across shards? ie if you have 8 shards and 16 slices we only slice one shard into 2 and read from all shards in parellel? that would make me happy I think 
</comment><comment author="clintongormley" created="2016-06-03T12:42:23Z" id="223568505">heya @jimferenczi - i've just thought of a potential problem with the current API: each scroll request no longer represents the same snapshot-in-time if indexing continues.  In practice I don't know if this really is an issue...
</comment><comment author="jimczi" created="2016-06-03T13:09:18Z" id="223574121">@clintongormley this is exactly what we wanted when this issue started. This means that the slices are independent and that they can be processed completely independently. The snapshot should be valid only during the lifespan of a single slice. The nice thing about slicing on _uid (or any pseudo random field that is never updated) is that we cannot miss any document. You can have new updates and deletes between two slices but it doesn't change the fact that each slice is consistent. 
</comment><comment author="clintongormley" created="2016-06-03T13:18:06Z" id="223576125">@jimferenczi yeah, I think that the fact that the point-in-time may be slightly different per slice doesn't make any real difference.  The same already applies to multiple shards.
</comment><comment author="jimczi" created="2016-06-03T15:47:06Z" id="223615879">@s1monw I pushed another commit to address the memory explosion. The slices are now splitted across shards like you suggested, so for instance if the number of shards is equal to 2 and the user requested 4 slices then the slices 0 and 2 are assigned to the first shard and the slices 1 and 3 are assigned to the second shard. This means that the total number of bitsets needed is bounded by the number of slices instead of (numShards*numSlices). 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Creates SearchParseContext class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18236</link><project id="" key="" /><description>This class can hold all object relating to parsing search requests. This will become useful when we move the parsing of the custom fetch sub phases from the shards to the coordinating node as it will serve as the object to retrieve the parser for the plugins sub fetch phase
</description><key id="153968995">18236</key><summary>Creates SearchParseContext class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label></labels><created>2016-05-10T10:03:23Z</created><updated>2016-05-11T12:12:53Z</updated><resolved>2016-05-11T08:39:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T13:36:37Z" id="218159086">Makes sense to me! Maybe we should have an example of using this in jvm-example or something?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.7, _optimize merges froze up..</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18235</link><project id="" key="" /><description>I'm trying to reduce the number of deleted documents in our indices, since we reindex most of the stuff ("real time" indexation directly to es and hourly reindexation from hadoop) the deleted documents takes about 40-50 percent of the total.

expunge_deletes_allowed to 1% isn't effective as I would expect, so I've tried _optimize?only_expunge_deletes=true on some index

_optimize?only_expunge_deletes=true initiates merges for this index, but the command eventually times out and the active merges in this index froze up (leading to the crashing cluster).
</description><key id="153966972">18235</key><summary>ES 1.7, _optimize merges froze up..</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Robitx</reporter><labels /><created>2016-05-10T09:53:15Z</created><updated>2016-05-11T09:23:31Z</updated><resolved>2016-05-11T09:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-11T09:23:31Z" id="218406905">Hi @Robitx 

Usually it is best to just leave Elasticsearch to manage its own merges, unless you have an index which will no longer receive any updates.  You're missing a lot of information here eg size of segments, number of segments, any merge throttling, free disk space, etc etc.

I suggest starting out by providing the above information in a question in the forum instead: https://discuss.elastic.co/

If you identify an actual bug (which hasn't been resolved in a newer release) then open a ticket here.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use better typing for dynamic method calls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18234</link><project id="" key="" /><description>Today painless boxes/casts all the parameters as Object. But I think instead we should pass the parameter types we have and let asType take care? It may be able to e.g. avoid boxing or other conversions in some situations.

FYI: I thought about this a lot, I like doing it here in the bytecode emitter, even though it looks strange, because its an implementation detail of how we make the call work.

@uschindler @jdconrad 
</description><key id="153965893">18234</key><summary>Use better typing for dynamic method calls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T09:47:37Z</created><updated>2016-05-17T12:15:32Z</updated><resolved>2016-05-10T23:17:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-05-10T15:58:42Z" id="218204579">+1

We should be able to do the same for array loads and stores and field stores, so index ints are passed as integers not as Integer. You just have to remove the static DESCRIPTOR from WriterConstants which is currently used and build one with MethodType.methodType(Object.class, arg, arg) or ASM's Type.getMethodDescriptor() for array setters and field setters.
</comment><comment author="rmuir" created="2016-05-10T16:07:59Z" id="218207508">@uschindler yeah, in that case it is just less clear how to get to the type information and disable casting. The same applies to regular Def loads and stores too, and also return values :)

Basically we should make use of all the typing we have and not issue our own casts/conversion to Object. 
</comment><comment author="jdconrad" created="2016-05-10T16:25:51Z" id="218212799">LGTM!
</comment><comment author="uschindler" created="2016-05-10T20:27:33Z" id="218280714">@jdconrad What we would need is a hit to do the same for field stores and array stores. The method to write the field store only writes the instruction, but the types are not available from context. For array loads/stores we also dont have the type of the "index".

@rmuir are you sure that the PR logic works? To me it is strange that you try to prevent the casting so late! At this point in time the arguments were already pushed to stack, so they are already objects. Or do I miss something?
</comment><comment author="rmuir" created="2016-05-10T21:13:35Z" id="218293464">&gt; At this point in time the arguments were already pushed to stack, so they are already objects. Or do I miss something?

Wait thats not true. They arent pushed until visit is called: line 724 in the loop.
</comment><comment author="jdconrad" created="2016-05-10T21:15:07Z" id="218293868">@uschindler For the question addressed to @rmuir  -- this does work as he changes the cast then visits the argument which is responsible for writing its own cast after it's written whatever expression is contains.  As for the first part, I'll see what I can do to get the other types available for consumption of the array/field indices. 
</comment><comment author="uschindler" created="2016-05-10T22:18:58Z" id="218309006">Hi,

sorry my fault. I missed the writer.visit(). Reason for this is because it was not a call on the execute objects, so I did not see it. The writer writes to the same execute object (the GeneratorAdaptor). Sorry for this.

@jdconrad: The main difference to field stores/loads and array stores/loads is that those arguments are indeed before actual call on stack, so you have no access to types anymore. If we could change this, it would be great! I am not only talking about the array index type, for field-/array-stores also about the type of the object to store.
</comment><comment author="jdconrad" created="2016-05-10T22:30:54Z" id="218311406">@uschindler On the array/field loads/stores, this is definitely a tricky issue.  It's a good point you make about not being able to do it the same way as we are for call here.  I need to give it a bit of thought about the best way to provide this.
</comment><comment author="rmuir" created="2016-05-10T23:17:53Z" id="218319641">I pushed this for the dynamic method calls. I agree, we should fix this for other cases too if we can, and try to reduce overhead in those cases!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes the now obsolete SearchParseElement implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18233</link><project id="" key="" /><description>All implementations of SearchParseElement have been removed since they are no longer used now that parsing is done on the coordinating node. The SearchParseElement and FetchSubPhaseParseElement classes are not removed as currently they are needed for plugins that add a custom fetch sub phase. These will be removed in a follow up PR that will allow fetch sub phase plugins to register a parser in a different way.
</description><key id="153957273">18233</key><summary>Removes the now obsolete SearchParseElement implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T09:03:27Z</created><updated>2016-05-10T09:32:37Z</updated><resolved>2016-05-10T09:32:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-10T09:05:27Z" id="218100436">LGTM. I just suspect that you wanted to use norelease rather than nocommit?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Array load/store and length with invokedynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18232</link><project id="" key="" /><description>After @rmuir's improvements for using invokedynamic (#18201) with dynamic method calls and field load/stores, this extends the whole thing to array loads and stores.

It also optimizes access to `array.length` by using a specialization for every array type. This works around a missing method in Java's MethodHandles class (e.g., `MethodHandles.arrayLengthGetter(Class&lt;?&gt; arrayType)`).

I did not do performance checks for now, because @rmuir knows better how to do that in ES - I just coded it :-)
</description><key id="153949327">18232</key><summary>Array load/store and length with invokedynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>:Scripting</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-10T08:22:49Z</created><updated>2016-05-17T12:23:48Z</updated><resolved>2016-05-10T10:55:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-10T09:41:28Z" id="218108556">Looks great. Thanks, the current Array get/set is really horrible, although in my tests the getLength is not so bad. 

So i think its best to do what you have, eliminate usage of this reflection class alltogether. Its performance is inconsistent and cannot be trusted!

Can you also update the docs to explain a bit more (https://github.com/uschindler/elasticsearch/blob/array_invoke_dynamic/modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java#L50) ? 

We may want to also test assigning different array types to `def` so we know nothing breaks.

Thanks again!
</comment><comment author="uschindler" created="2016-05-10T10:37:18Z" id="218120739">OK, I added a test class ArrayTests which also uses a for loop to calculate the sum of the aritmetic series.

I also updated docs to add array loads and stores to indy section in Def.
</comment><comment author="uschindler" created="2016-05-10T10:38:42Z" id="218121043">FYI: Reflective Array.get/Array.set are slow like hell - some pointers and issues at the OpenJDK bug tracker are here: http://stackoverflow.com/questions/30306160/performance-of-java-lang-reflect-array, https://bugs.openjdk.java.net/browse/JDK-8051447

People say like 20 times slower :-)
</comment><comment author="rmuir" created="2016-05-10T10:54:47Z" id="218124319">Thanks @uschindler for taking care of this.
</comment><comment author="uschindler" created="2016-05-13T10:20:31Z" id="219006259">FYI: There is now an issue for the `arraylength` bytecode `MethodHandle` that we emulated here by the inner utility class:

https://bugs.openjdk.java.net/browse/JDK-8156915
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make it clear that the cardinality benchmark results in docs are an example and user results might vary </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18231</link><project id="" key="" /><description>This issue was identified in this forum post: https://discuss.elastic.co/t/high-approximation-error-rate-of-cardinality-aggregation-for-low-cardinality-sets/49574

We should make it clearer in the documentation that the benchmark results in the ['Counts are Apporximate'](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html#_counts_are_approximate) section may be different from a users results if they perform the same test since the error depends on how the values in their dataset hash (specifically how many leading zeros are on the hash values)
</description><key id="153948203">18231</key><summary>Make it clear that the cardinality benchmark results in docs are an example and user results might vary </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>adoptme</label><label>docs</label><label>v5.0.0</label></labels><created>2016-05-10T08:16:27Z</created><updated>2016-10-18T08:31:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch nodes run into OOM during sustained ThreadPoolRejections</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18230</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.3
**JVM version**: Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
**OS version**: Linux ip-XXXXXXX.ec2.internal 4.1.13-19.30.amzn1.x86_64 #1 SMP Fri Dec 11 03:42:10 -- UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
**Description of the problem including expected versus actual behavior**:
3 data nodes (7GB heap), 3 dedicated master nodes.
1 index with 128 primary shards, 2 replicas. Index has 600k documents.
Default configuration, with reduced search thread pool size (from default of 13 to 5 threads). We also changed the search queue size from 1000 to 999 (to verify that we could issue cluster level settings updates during the test).

**Steps to reproduce**:
We were hitting this issue in production and were able to successfully reproduce it in a staging environment by doing the following:
1. spin off 1000 threads running the following query concurrently (same query across all threads) in a tight loop

`POST index_name/object/_search
{
  "size" : 5000,
  "query" : {
    "filtered" : {
      "query" : {
        "filtered" : {
          "query" : {
            "query_string" : {
              "query" : "(field1:value1 AND NOT field2:_PREFIX_*)",
              "lowercase_expanded_terms" : false
            }
          },
          "filter" : {
            "and" : {
              "filters" : [ {
                "term" : {
                  "field3" : "value2"
                }
              }, {
                "term" : {
                  "type" : "MTS"
                }
              }, {
                "or" : {
                  "filters" : [ {
                    "term" : {
                      "active" : true
                    }
                  }, {
                    "and" : {
                      "filters" : [ {
                        "term" : {
                          "active" : false
                        }
                      }, {
                        "range" : {
                          "lastActive" : {
                            "from" : 0,
                            "to" : null,
                            "include_lower" : true,
                            "include_upper" : true
                          },
                          "_cache" : false
                        }
                      } ]
                    }
                  } ]
                }
              } ]
            }
          }
        }
      },
      "filter" : {
        "missing" : {
          "field" : "_deletedOnMs",
          "null_value" : true,
          "existence" : true
        }
      }
    }
  },
  "version" : false,
  "_source" : {
    "includes" : [ ],
    "excludes" : [ "_props" ]
  },
  "sort" : [ {
    "createdOnMs" : {
      "order" : "asc"
    }
  } ]
}`
1. those queries will rapidly fill up the search queue and we get thread pool rejections.
2. after 5mn, all 3 data nodes get into a zombie state (ie, OOM'ed) and get kicked out of the cluster

**Provide logs (if relevant)**:
thread dumps
https://gist.github.com/mahdibh/025d7a909475c43f9154e661c3ef839f
https://gist.github.com/mahdibh/b89a1d437467339a5c30eb416a96cfb5

node log
https://gist.github.com/mahdibh/320b515788400fb1560f2da1b1f2897f

**Notes**
All nodes in the cluster run into the same issue. We took a heap dump of one of the nodes when it was in this state. 41% of the shallow size of the heap is consumed by byte arrays. long[] comes next with 16% and then char[] with 8%. We could privately share the heap dump if it helps figuring out what's going on.

When this happens, the clusters becomes useless. Only the master nodes respond to API calls. The head plugin shows a blank list of nodes.

We can easily reproduce this internally, if there is anything we can do to provide more details, please let us know.
</description><key id="153930425">18230</key><summary>Elasticsearch nodes run into OOM during sustained ThreadPoolRejections</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahdibh</reporter><labels><label>:Core</label><label>bug</label></labels><created>2016-05-10T06:07:20Z</created><updated>2016-07-05T19:09:44Z</updated><resolved>2016-07-05T08:45:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-10T09:32:05Z" id="218106464">Hi @mahdibh 

We have added eg the in-flight requests circuit breaker (https://github.com/elastic/elasticsearch/pull/17133) which may or may not help in this situation.  It depends exactly where memory is being consumed here.  eg the `requests` circuit breaker is applied during the query phase, but not during the fetch phase and not on the coordinating node.  So large `_source` fields might consume too much memory and result in an OOM.

Giving us access to the heap dump would be helpful.  Also, would you mind trying this same test out on 5.0.0-alpha2?

We may be able to find other safeguards (see https://github.com/elastic/elasticsearch/issues/11511) that will help us in these situations
</comment><comment author="mahdibh" created="2016-05-10T13:05:03Z" id="218150870">Hi @clintongormley 

We will try to repro this without requesting _source and see if that helps. Our document _source is usually 10kb. Not sure if that actually will matter here since we have only 5 search threads per node.

Would the number of shards (128) coupled with the fact that we are retrieving 5000 records (sorted) contribute to the memory bloat when the system isn't able to keep up ? My understanding is that ES will have to sort and return the top 5000 docs on each shard to the coordinating node which will have to sort across 128x5000=640k docs. Our sort field has doc values enabled and it's a long timestamp.

Also, would the ThreadPoolRejection exceptions cause extra allocations (since there is an error per shard). When the system is overloaded, it's likely all shards will reject and the response object will contain 128 shard failure messages.

I'll follow up regarding the heapdump and trying this out on 5.0.0-alpha2
</comment><comment author="mahdibh" created="2016-05-11T06:38:53Z" id="218374774">I tried removing the sort in the query, reducing the number of results fetched from 5k to 100, not fetching _source and using no regex in the query but still no luck. I can get the 3 data nodes to OOM in under 3 minutes of sustained load.

I then tried to use the same index but with 8 primary shards instead of 128 (+2 replica). In that case, I couldn't get the cluster to crash (nodes disappeared from the cluster temporarily, but then came back once I stopped the load).

I've also noticed that the cluster held-up more when I disabled the logging of ThreadPoolRejectionExceptions on the ES side PUT _cluster/settings" -d'{"transient" : {"logger.action.search.type" : "ERROR"}}'). Those were logged at a peak rate of 10k/second. No wonder that would consume CPU. I'm not sure why stack traces are logged for those exceptions by default, seems like shooting oneself in the foot when ES is trying to shed itself from further load.

@clintongormley any luck finding anything suspicious in the heap dump ?
</comment><comment author="mahdibh" created="2016-05-11T20:31:11Z" id="218580746">More data points:
We have upgraded to 1.7.5, but still run into the same issue.

We switched our query traffic from using the data nodes directly to just using client nodes only instead. This shifted the OOM to the client nodes (which is good, since those are way much cheaper to restart). I'll share the heap dump we got on the client nodes.
</comment><comment author="s1monw" created="2016-05-11T21:19:28Z" id="218593548">&gt; Would the number of shards (128) coupled with the fact that we are retrieving 5000 records (sorted) contribute to the memory bloat when the system isn't able to keep up ? My understanding is that ES will have to sort and return the top 5000 docs on each shard to the coordinating node which will have to sort across 128x5000=640k docs. Our sort field has doc values enabled and it's a long timestamp.

may I ask why you have 128 shards on 3 nodes with 600k doc? What about 1 shard? I also wonder why on earth you are fetching 5k documents instead of using pagination or sorted scroll or search_after?
</comment><comment author="mahdibh" created="2016-05-11T22:21:38Z" id="218607939">&gt; may I ask why you have 128 shards on 3 nodes with 600k doc? What about 1 shard?

This is just a load test environment. The number of shards is artificially high on so that we can determine whether that impact the memory issues we are observing. In my third update, I pointed out that we weren't able to repro this on an index with 8 shards.

&gt; I also wonder why on earth you are fetching 5k documents instead of using pagination or sorted scroll or search_after?
- Pagination would add latency, although we have never tried that. Would doing that be beneficial ? Latency aside, can you explain why that would be more efficient than retrieving 5k in one shot.
- Sorted scrolls would end up being costly due to the scroll context that has to be maintained.
- search_after isn't available in 1.7 afaik. 
</comment><comment author="s1monw" created="2016-05-12T07:26:10Z" id="218680317">&gt; This is just a load test environment. The number of shards is artificially high on so that we can determine whether that impact the memory issues we are observing. 

I see - is having 128 shards per node a realistic test? I mean I can put a ton of shards on a node and stuff will likely go bad? Nevertheless, I would be interested in the heapdump of a client node here.

&gt; In my third update, I pointed out that we weren't able to repro this on an index with 8 shards.

sorry missed that....

&gt; Pagination would add latency, although we have never tried that. Would doing that be beneficial ? Latency aside, can you explain why that would be more efficient than retrieving 5k in one shot.

to begin with, why do you need 5k results. Elasticsearch is a top N retrieval engine in the first place where N is small like 10, 20, 30? I doubt the efficiency here, I think you are overestimating latency given that you are fetching 5k \* 10kb of json latency becomes a constant factor given the amount of data you are fetching. Also keep in mind elasticsearch has to materialize all that data before sending back ie. it's not streaming. Working with chunks is the way to go.

&gt; Sorted scrolls would end up being costly due to the scroll context that has to be maintained.

what cost you are talking about here? Memory? it holds on to the index reader plus an integer per shard and scroll request. The cost is bascially an int per shard since you need the index reader anyway?

&gt; search_after isn't available in 1.7 afaik.

since you are in a testing phase why don't you go and use 2.x give that 5.x is around the corner?
</comment><comment author="mahdibh" created="2016-05-12T21:23:54Z" id="218890208">One other observation that we made during this is that when the client node runs out of memory, OOM exceptions are swallowed and the node stays up in zombie state holding to its connections to other data nodes. This in turn doesn't release the netty resources on the data nodes leading to a snowball effect that causes the data nodes to also run out of memory.

```
2016-05-12T18:48:01.533Z WARN  [ent_worker][T#7]{New I/O worker #7}] [annel.socket.nio.AbstractNioSelector]: Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
```

If you look at netty's AbstractNioSelector.java code, it's catching throwable, logging it and then moving on. 
If that exception gets bubbled all the way up to the JVM, the process would exit, or at least we could add an OOM handler in the JVM options so that we can kill the process and release the buffers it's holding into the data nodes.

Note that this pattern is also present in other places in the ES code, for example IndicesTTLService.run(), RestActionListener.onFailure() swallow throwable. I'm not sure if those are done on purpose (in the hope that the OOM was momentary?). It seems to me that the best course of action when handling an OOM is either to release memory or to let the exception go up the stack but not to swallow it.
</comment><comment author="mahdibh" created="2016-05-13T14:45:54Z" id="219064357">One more issue we also noted is the following code in MessageChannelHandler

```
            ThrowableObjectInputStream ois = new ThrowableObjectInputStream(buffer, transport.settings().getClassLoader());
            error = (Throwable) ois.readObject();
```

When the data nodes search queues are full, the rejection exception is serialized back to the client node. The deserialization of the EsRejectedExecutionException in ois.readObject() across the different netty worker threads is synchronized in java.lang.ClassLoader.loadClass(ClassLoader.java:357). This blocks all IO worker threads. See two thread dumps here:

https://gist.github.com/mahdibh/859ef5fcb509a95a9dd079baef369546

After this point, it seems that the system gets stuck (probably trying to allocate memory) but never OOMs (or maybe does, but since it's trying to allocate memory to log the OOM, it is never logged).

This is what I see in the logs, but never any OOM. I guess if I wait long enough, it'll likely happen on some other thread that is not trying to catch it.

2016-05-13T13:27:19.416Z INFO  [rch[metabase-es-26][scheduler][T#1]] [monitor.jvm                         ]: [metabase-es-26] [gc][old][1158][256] duration [6.1s], collections [1]/[6.6s], total [6.1s]/[11.5m], memory [3.8gb]-&gt;[3.8gb]/[3.9gb], all_pools {[young] [148.1mb]-&gt;[174.1mb]/[266.2mb]}{[survivor] [0b]-&gt;[0b]/[33.2mb]}{[old] [3.6gb]-&gt;[3.6gb]/[3.6gb]}
2016-05-13T13:27:31.022Z INFO  [rch[metabase-es-26][scheduler][T#1]] [monitor.jvm                         ]: [metabase-es-26] [gc][old][1160][258] duration [6.2s], collections [1]/[6.7s], total [6.2s]/[11.7m], memory [3.8gb]-&gt;[3.8gb]/[3.9gb], all_pools {[young] [205mb]-&gt;[178.8mb]/[266.2mb]}{[survivor] [0b]-&gt;[0b]/[33.2mb]}{[old] [3.6gb]-&gt;[3.6gb]/[3.6gb]}
2016-05-13T13:27:42.296Z INFO  [rch[metabase-es-26][scheduler][T#1]] [monitor.jvm                         ]: [metabase-es-26] [gc][old][1162][260] duration [6.3s], collections [1]/[6.5s], total [6.3s]/[11.8m], memory [3.9gb]-&gt;[3.9gb]/[3.9gb], all_pools {[young] [241.9mb]-&gt;[266.2mb]/[266.2mb]}{[survivor] [0b]-&gt;[7mb]/[33.2mb]}{[old] [3.6gb]-&gt;[3.6gb]/[3.6gb]}

See thread dumps below when it reaches this state
https://gist.github.com/mahdibh/f879362f2c4960e401d2c06385aec5d8

A bit after this, the node logs a ton of the following exceptions:

```

2016-05-13T14:33:08.631Z WARN  [rch[metabase-es-26][generic][T#456]] [search.action                       ]: [metabase-es-26] Failed to send release search context
org.elasticsearch.transport.SendRequestTransportException: [metabase-es-18][inet[/X.X.X.XX:9300]][indices:data/read/search[free_context]]
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:286)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:249)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendFreeContext(SearchServiceTransportAction.java:143)
...
Caused by: org.elasticsearch.transport.NodeNotConnectedException: [metabase-es-18][inet[/X.X.X.X:9300]] Node not connected
        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:964)
```

Most likely because the node has been kicked out of the cluster and doesn't see the other data nodes.

"Node not connected" is a bit misleading here since it indicates a networking problem reaching the node whereas it's actually just not finding the channel in the node mapping. This could better be characterized as "can't find {node} in the list of connected nodes". I'm mentioning this, because we wasted some time checking our networking when we saw in this issue in production.
</comment><comment author="jasontedor" created="2016-05-13T14:48:12Z" id="219065061">&gt; When the data nodes search queues are full, the rejection exception is serialized back to the client node. The deserialization of the EsRejectedExecutionException in ois.readObject() across the different netty worker threads is synchronized in java.lang.ClassLoader.loadClass(ClassLoader.java:357). This blocks all IO worker threads.

We don't use Java serialization for exceptions anymore. This is not the case in 2.x and master.
</comment><comment author="mahdibh" created="2016-05-13T23:04:52Z" id="219179970">&gt; We don't use Java serialization for exceptions anymore. This is not the case in 2.x and master.

That's great to hear. I would prefer that those do not even get serialized (especially the stack trace). For exceptions that are designed to shed the system from further load, generating them and sending them back to the client should be the cheapest possible. I think a status code indicating a thread pool rejection would be good enough. Not sure what the behavior is in 2.x/master.

2.x is not an option for us at this point because we can't do a rolling upgrade and it'll take us forever to do a full upgrade (and the plan-b if that fails is rather scary ;) ).
</comment><comment author="mahdibh" created="2016-05-16T18:38:53Z" id="219508460">Another thing that could help fix this is to check the netty channel's isWritable() status (see http://normanmaurer.me/presentations/2014-facebook-eng-netty/slides.html#10.0). If the channel isn't writable, the response/request will be queued, hence holding to memory that won't be released until the client/server responds. Under load, data nodes may become slow leading to more queuing on the coordinator side. Also, if the coordinator node is slow receiving, more memory will be held on the data node side. In both cases, OOMs are likely to happen.

The ES code (1.7.5 branch) doesn't have any reference to the isWritable method so there is definitely a risk of running out of memory under these conditions.
</comment><comment author="s1monw" created="2016-05-17T07:23:05Z" id="219639163">just to manage expectations, we won't do another 1.7.x release unless there is a really serious issue coming up. There won't be any features or larger modifications happening to 1.7.x. 
I appreciate your  comments but it's still unclear what exactly is causing the problems on your side? Why is the node going OOM? I also wonder if you are producing an artificial problem here since there haven't been issues reported related to this to my knowledge (I can be wrong). 

There are certainly improvements possible to the network code but I think we should contain them and have a clear purpose. We are more than happy to accept PRs / improvements like the `isWritable()` suggestion!

&gt; 2.x is not an option for us at this point because we can't do a rolling upgrade and it'll take us forever to do a full upgrade (and the plan-b if that fails is rather scary ;) ).

you should really reconsider your strategy - 2.x to 5.x will require a full cluster restart too. I recommend you build some infrastructure to make the move to newer version possible.
</comment><comment author="danielmitterdorfer" created="2016-05-17T12:53:25Z" id="219708234">Here's a summary of what I found out so far:

### Heap dumps

I looked at your heap dumps and the two major heap consumers are:
- `TransportService.clientHandlers` (consuming roughly 1.6GB or 40% of your heap)
- 144 instances of `NioAcceptedSocketChannel` (consuming roughly 2.2GB or 54% of your heap). These instances get created when we accept the network connection. However, if there are not enough workers available they get queued by Netty and the really bad thing is that the queue is an unbounded queue.

This indicates that there are too many open client connections (due to search requests). 

### Reproduction

I tried reproducing the problem with the information that I could find in the ticket. Although I was able to force `EsRejectedExecutionException`s easily, I could not get the system into a state where it OOMs (I am not saying at all that they don't happen; I could just not reproduce it locally).

### Mitigation

However, I think you can do a couple of things to mitigate this situation:
1. First of all, clients should back off when they get `EsRejectedExecutionException` in a response. Since Elasticsearch 2.2, we use an exponential backoff in the Java client (see #14829 and #15513) and you should do something similar if you use another language in your application.
2. You could look whether you can optimize your query. It is fairly complex so maybe you can do something there.
3. You should also look into pagination. Do you show 5.000 search results at once to your users?

An upgrade to 2.x could help you for multiple reasons:
1. We have included additional circuit breakers which could maybe help in your situation. The top memory consumers are due to request processing or rather queued requests but it maybe the case that the inflight request breaker can still help in this situation (will be available in release 2.4) although this is not the root cause of the problem here.
2. We have added a query profiler which can help you to find out why your query is slow and help you tune it.
</comment><comment author="mahdibh" created="2016-05-18T05:40:07Z" id="219930160">&gt; just to manage expectations, we won't do another 1.7.x release unless there is a really serious issue coming up

This is a serious issue IMHO. If a single node OOMs and gets into this zombie state this is what happens:
1. it completely stalls search requests in the whole cluster. ES becomes completely un-usable.
2. if the node is a coordinator node, it will hold to its connections on data nodes increasing the risk of data nodes also running out of memory. 

Swallowing an OOM is pretty serious too if you talk to any decent java engineer :)  It's a bad practice and leads to all sorts of subtle, hard to diagnose problems. 

Btw, there have been issues reported around this in the past, see #2528 and more specifically #9018

&gt; you should really reconsider your strategy - 2.x to 5.x will require a full cluster restart too. I recommend you build some infrastructure to make the move to newer version possible.

We are already doing that. Building that infrastructure is expensive ! The problem is that the 1.x and 2.x clients aren't compatible, so we have to maintain two client versions with two clusters of different versions while the data is changing underneath. If the clients were kept compatible, that would have saved us a lot of boilerplate work.
</comment><comment author="mahdibh" created="2016-05-18T05:47:53Z" id="219931091">thanks @danielmitterdorfer for taking a look at the dumps.

&gt; if there are not enough workers available they get queued by Netty and the really bad thing is that the queue is an unbounded queue.
&gt; Is there a setting to make this queue bounded ?

We have already implemented a backoff strategy to relieve the cluster when requests are being rejected. The result of the queries are actually not showed to users, but used by internal backend systems for other purposes.

What about search results? What happens when a client is too slow to consume a response. Do those also get queued on a queue with an unbounded size ?
</comment><comment author="danielmitterdorfer" created="2016-05-18T09:25:11Z" id="219972635">&gt; &gt; if there are not enough workers available they get queued by Netty and the really bad thing is that the queue is an unbounded queue.
&gt; 
&gt;   Is there a setting to make this queue bounded ?

No, there is no setting in Netty, see [AbstractNioSelector.java#L87](https://github.com/netty/netty/blob/3.10/src/main/java/org/jboss/netty/channel/socket/nio/AbstractNioSelector.java#L87).

&gt; We have already implemented a backoff strategy to relieve the cluster when requests are being rejected.

How do you backoff? Linearly, exponentially, some other strategy? It's also hard to tell without knowing your use case but it seems also that there are quite a lot of clients hitting the cluster (I don't know if you just used 1.000 as an example to produce the condition more quickly or if you really have that many clients hitting it in production). Nevertheless, it seems even with your backoff strategy too many requests reach the server as there are more than one hundred queued connections.
</comment><comment author="jasontedor" created="2016-05-18T11:56:37Z" id="220004490">@mahdibh We agree with you that Elasticsearch should be terminated on any `OutOfMemoryError`. However, the situation is not simple.

First, I think that you're missing that effectively all of the work in Elasticsearch is not done on the main thread. Even without catching `Throwable`, a death of one of these threads due to `OutOfMemoryError` or other `VirtualMachineError`s would (by default) not bring the process down anyway. An option would be to register an `UncaughtExceptionHandler` and terminate the process if this handler is invoked with an `OutOfMemoryError` with the cost of having to unwind all of the code blocks today that are catching `Throwable`.

So maybe a better option is that in later versions of Java (&gt;= 8u92) we will be able to take advantage of `ExitOnOutOfMemoryError` to immediately exit the process if an `OutOfMemoryError` is detected, but that is something that we need to wait until we require Java 9 for.

Even this has complications. For example, maybe it's best that when handling a remote request, we catch everything so that we can best-effort notify listeners and flush before proceeding to tear the process down? Or maybe not and we should just let the listeners be notified via `NodeDisconnectedException`?

Then there's the operations side of this which is that immediately tearing the process down on `OutOfMemoryError` means that end-users have to be prepared to automatically restart nodes or they could have nodes dropping out, not getting restarted in a timely fashion and so missing delayed allocation windows triggering a bunch of recoveries potentially exacerbating memory issues on other nodes leading to a cluster-wide outage. And maybe that's something that we just want to provide out of the box with our packaging?

Nothing is ever simple.
</comment><comment author="mahdibh" created="2016-05-19T05:40:23Z" id="220232374">&gt; How do you backoff?

We do exponential backoffs.

&gt; I don't know if you just used 1.000 as an example to produce the condition more quickly or if you really have that many clients hitting it in production. 

Since the client is doing async IO, it's actually the number of clients times the number of concurrent queries issued by client (till the IO threads are saturated) that determines the overall load on the system. If you multiply that by the number of shards in the index, the number quickly gets big.
</comment><comment author="mahdibh" created="2016-05-19T06:25:00Z" id="220238073">The core of the issue here is the un-bounded connection queues. That's what netty's `isWritable()` method is for. The handling of the OOM is secondary. It's important that the system protects itself against OOMs. The ES team have been putting a lot of these protections in place as part of the OOM resiliency work and that's great. I think this is another one of these issues that also needs to be added to the list.

Re. swallowing OOM, I think it's just the wrong thing to do regardless of the complications of bubbling up the exception all the way up to the main thread. The JDK documentation for `VirtualMachineError` clearly states: `Thrown to indicate that the Java Virtual Machine is broken or has run out of resources necessary for it to continue operating.`

I would completely understand catching this exception and then for example trying some last resort actions like freeing up memory or closing connections, etc.. However, it should be re-thrown because it's one of those exceptions that you can't just handle and continue to function properly.
The intent of the developer here is to save the system at any cost: catch the OOM and ignore it in the hope that memory will get freed-up somewhere else and the system will get back to working normally. However, that's the best case scenario. Keeping the system up and running in that state may do more harm than good. For example: data corruption (see #12041, #10066) or subtle deadlocks/race conditions that only happen during those low memory conditions.
</comment><comment author="jasontedor" created="2016-05-19T10:51:59Z" id="220291061">Did you see where I said we agree with you? I should add, we had discussions about this awhile ago and already came to the conclusion we should handle it differently than today.
</comment><comment author="danielmitterdorfer" created="2016-05-19T13:15:59Z" id="220320546">@mahdibh I think before we can fix something, we need be able to reproduce the issue (otherwise it's hard to tell we've really fixed the right thing). Therefore, I have started a gist where you can see the reproduction scenario I have created based on your information: https://gist.github.com/danielmitterdorfer/636d1fb1f6a2ec4353a4e0b5d552a391

It would be great if you could fork it and adapt it so we can get a scenario which we can reproduce on AWS. One thing I've already noticed: I have not used async IO as I didn't know it before.
</comment><comment author="danielmitterdorfer" created="2016-06-09T05:47:39Z" id="224805605">@mahdibh Did you have any chance to look at the reproduction scenario?
</comment><comment author="mahdibh" created="2016-06-09T06:04:31Z" id="224807607">Sorry @danielmitterdorfer I got pulled into other things. I think this will be hard to reproduce with the python client (I'm assuming it doesn't do any async IO). I'll try to spend some cycles on this next week and will update the ticket.
</comment><comment author="danielmitterdorfer" created="2016-06-09T06:07:50Z" id="224808029">@mahdibh No worries. You can use whatever is most convenient to you, I'm sure we can figure it out then. I just used the Python client when I originally created the scenario because I wasn't aware that you do it async back then (and btw the Python client does not do async IO but Honza has created an async io extension).
</comment><comment author="danielmitterdorfer" created="2016-07-05T08:45:11Z" id="230420787">Due to lack of a reproduction scenario we cannot make progress and I am closing this ticket now. But please feel free to reopen when you've got a reproduction scenario. Btw, @jasontedor started to eliminate the `catch Throwable` blocks in #19231.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adds dejavu, data browser plugin to the list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18229</link><project id="" key="" /><description>Adds a plugin to the list of community created plugins in the docs.
</description><key id="153918228">18229</key><summary>adds dejavu, data browser plugin to the list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">siddharthlatest</reporter><labels><label>docs</label></labels><created>2016-05-10T03:45:17Z</created><updated>2016-05-10T08:39:26Z</updated><resolved>2016-05-10T08:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="siddharthlatest" created="2016-05-10T03:49:26Z" id="218052459">I have signed the CLA after seeing the check fail.
</comment><comment author="clintongormley" created="2016-05-10T08:39:26Z" id="218093919">Hi @siddharthlatest 

I've merged this into 2.3 and 2.x, but not into master as _site plugins have been removed in master.  They should be implemented as Kibana plugins instead
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds UUIDs to snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18228</link><project id="" key="" /><description>This commit adds a UUID for each snapshot, in addition to the already
existing repository and snapshot name. The addition of UUIDs will enable
more robust handling of the deletion of previous snapshots and lingering
files from partially failed delete operations, on top of being able to
uniquely track each snapshot.

Relates #18156
</description><key id="153916322">18228</key><summary>Adds UUIDs to snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-10T03:24:13Z</created><updated>2016-06-02T21:02:36Z</updated><resolved>2016-06-02T21:02:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-10T03:28:01Z" id="218050227">@s1monw @ywelsch your feedback would be appreciated.
</comment><comment author="abeyad" created="2016-05-20T04:38:19Z" id="220515500">@ywelsch Commit https://github.com/elastic/elasticsearch/pull/18228/commits/92df39f33a3055a4af69731ae0d67d47501d32a2 addresses your code review comments and adds some tests.  Commit https://github.com/elastic/elasticsearch/pull/18228/commits/077fafa7731282d9b5e2e64ee834cbea1d2c3493 adds more tests.
</comment><comment author="ywelsch" created="2016-05-23T08:41:53Z" id="220920581">Left a few more comments. We're getting close...
</comment><comment author="abeyad" created="2016-05-23T18:02:50Z" id="221047991">@ywelsch I experimented with removing `resolveSnapshotNames` in https://github.com/elastic/elasticsearch/pull/18228/commits/455d1459b16efbd024031c399b962d7eb22f512d

WDYT?
</comment><comment author="abeyad" created="2016-05-23T21:12:47Z" id="221097846">@ywelsch you may also be interested in this not-so-pleasant commit https://github.com/elastic/elasticsearch/pull/18228/commits/d196050aff3f5000240dfe50de5df1ed2709021b :)
</comment><comment author="abeyad" created="2016-05-27T01:58:19Z" id="222042094">@ywelsch @imotov The latest commit addressing code review comments is here: https://github.com/elastic/elasticsearch/pull/18228/commits/a17039c86d7d9bfd10f77bbb15ca1b7a43496226

Also, @imotov has requested I create an infrastructure for testing repositories with old snapshots interoperating with new snapshots.  For now, the only thing to test is the blob names with respect to the changes, but it will make it easier to test changes down the road with generational index files.
</comment><comment author="abeyad" created="2016-05-28T07:25:04Z" id="222294920">@ywelsch @imotov 

This commit addresses the `TransportGetSnapshotsAction` issue with tests: https://github.com/elastic/elasticsearch/pull/18228/commits/ff262aecd28b8a7f92a2545901a40458dbb55713
This commit adds testing for repository interoperability with previous versions: https://github.com/elastic/elasticsearch/pull/18228/commits/1bbd2a52ebf48bfea32fdd99f9dcc6d35ca4f71e
</comment><comment author="abeyad" created="2016-05-31T23:18:14Z" id="222849658">@imotov https://github.com/elastic/elasticsearch/pull/18228/commits/9969ec3cf721ebf2f00b2fafbf76eebb9be1b071 fixes the issues with the upgradability test
</comment><comment author="imotov" created="2016-06-01T19:02:13Z" id="223092363">LGTM
</comment><comment author="abeyad" created="2016-06-01T22:50:22Z" id="223148260">Thank you for the review and valuable comments, @imotov!
</comment><comment author="ywelsch" created="2016-06-02T14:15:43Z" id="223304953">LGTM 2! Thanks for all the work here @abeyad. I think we've both learned quite a lot about some of the finer details of snap/restore.
</comment><comment author="abeyad" created="2016-06-02T14:19:04Z" id="223305975">@ywelsch thank you for your patience and great feedback!  Indeed we have learned a lot more on snapshot/resotre, which means more work ahead :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing comma to JSON data in curl example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18227</link><project id="" key="" /><description /><key id="153904004">18227</key><summary>Add missing comma to JSON data in curl example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zackdever</reporter><labels><label>docs</label></labels><created>2016-05-10T01:15:25Z</created><updated>2016-05-10T08:23:29Z</updated><resolved>2016-05-10T01:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-10T01:17:00Z" id="218034610">LGTM. Thanks, I'll forward-port to master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove 'sandbox' option for script settings, allow only registering a single language.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18226</link><project id="" key="" /><description>This removes all the mentions of the sandbox from the script engine
services and permissions model. This means that the following settings
are no longer supported:

``` yaml
script.inline: sandbox
script.stored: sandbox
```

Instead, only a `true` or `false` value can be specified.

Since this would otherwise break the default-allow parameter for
languages like expressions, painless, and mustache, all script engines
have been updated to have individual settings, for instance:

``` yaml
script.engine.groovy.inline: true
```

Would enable all inline scripts for groovy. (they can still be
overridden on a per-operation basis).

Expressions, Painless, and Mustache all default to `true` for inline,
file, and stored scripts to preserve the old scripting behavior.

Additionally, this changes script engine registration to allow
registering only a single language instead of multiple languages.

Resolves #17114 
Resolves #10598 
</description><key id="153877952">18226</key><summary>Remove 'sandbox' option for script settings, allow only registering a single language.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>blocker</label><label>breaking</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T21:49:20Z</created><updated>2016-05-13T15:50:55Z</updated><resolved>2016-05-13T15:50:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T18:20:22Z" id="218245184">Code LGTM. Left a comment about a thing I think is a typo. @clintongormley's comment about painless is still left to resolve but otherwise LGTM.
</comment><comment author="clintongormley" created="2016-05-11T10:27:09Z" id="218420864">thanks @dakrone - added some more comments
</comment><comment author="dakrone" created="2016-05-11T16:26:50Z" id="218513309">@clintongormley thanks for the comments, I pushed commits to restore parts of the dos

@nik9000 I have now extended this to remove the ability to register multiple languages for a single script engine service (as it makes it super confusing for settings and led to a lot of code complexity), can you take another look?
</comment><comment author="nik9000" created="2016-05-11T19:32:19Z" id="218565139">LGTM! One last thing though: changing `js` -&gt; `javascript` is breaking I think? Probably should be marked that way. Right?
</comment><comment author="dakrone" created="2016-05-11T19:37:31Z" id="218566440">&gt; One last thing though: changing js -&gt; javascript is breaking I think? Probably should be marked that way. Right?

I did mark this as breaking, and I put a note in the migration guide for this
</comment><comment author="jasontedor" created="2016-05-11T19:37:50Z" id="218566519">&gt; One last thing though: changing js -&gt; javascript is breaking I think?

Why are we picking `javascript` here? I think `js` is the more common extension and will "break" less people?
</comment><comment author="dakrone" created="2016-05-11T19:38:54Z" id="218566790">&gt; Why are picking javascript here? I think js is the more common extension and will "break" less people?

It only affects `"lang": "javascript"`, I'm +0 on either, I don't really care, I went with "javascript" since it was the most specific but I can change it
</comment><comment author="jasontedor" created="2016-05-11T19:42:28Z" id="218567666">&gt; It only affects "lang": "javascript", I'm +0 on either, I don't really care, I went with "javascript" since it was the most specific but I can change it

It's fine as the `lang`, I didn't realize this PR was not removing support for extensions and thought it was applying to both.
</comment><comment author="dakrone" created="2016-05-11T19:43:38Z" id="218567965">&gt; I didn't realize this PR was not removing support for extensions and thought it was applying to both.

Previously engines registered themselves as both "js" and "javascript", this makes it so they only register as a single language. I do think we should limit it to one extension also, and I'm planning on doing that in a subsequent PR (this one is noisy enough as it is)
</comment><comment author="jasontedor" created="2016-05-11T19:44:27Z" id="218568158">&gt; Previously engines registered themselves as both "js" and "javascript", this makes it so they only register as a single language.

Yeah, I just misinterpreted that both were happening here. Sorry.

&gt; I do think we should limit it to one extension also, and I'm planning on doing that in a subsequent PR (this one is noisy enough as it is)

+1
</comment><comment author="nik9000" created="2016-05-12T13:46:13Z" id="218761764">&gt; I did mark this as breaking, and I put a note in the migration guide for this

I see it. LGTM.
</comment><comment author="clintongormley" created="2016-05-13T14:59:49Z" id="219068786">LGTm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail build when docs contain // AUTOSENSE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18225</link><project id="" key="" /><description>It is deprecated and should be replaced with // CONSOLE.
</description><key id="153874496">18225</key><summary>Fail build when docs contain // AUTOSENSE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T21:29:31Z</created><updated>2016-05-09T21:33:03Z</updated><resolved>2016-05-09T21:33:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-09T21:31:00Z" id="217996232">LGTM.
</comment><comment author="nik9000" created="2016-05-09T21:31:54Z" id="217996448">Thanks for reviewing @jasontedor !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename autoSense to console in the docs tests generation code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18224</link><project id="" key="" /><description>Since `// AUTOSENSE` has been replaced by `// CONSOLE` we should use
the new name for the variable name.
</description><key id="153871686">18224</key><summary>Rename autoSense to console in the docs tests generation code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T21:14:38Z</created><updated>2016-05-09T21:30:30Z</updated><resolved>2016-05-09T21:30:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-09T21:23:15Z" id="217994229">LGTM
</comment><comment author="nik9000" created="2016-05-09T21:30:26Z" id="217996073">Thanks for reviewing @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow any number of leading spaces in docs tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18223</link><project id="" key="" /><description>0 too!
</description><key id="153862590">18223</key><summary>Allow any number of leading spaces in docs tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T20:30:11Z</created><updated>2016-05-09T21:19:13Z</updated><resolved>2016-05-09T21:19:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-09T21:09:14Z" id="217990603">LGTM
</comment><comment author="nik9000" created="2016-05-09T21:15:01Z" id="217992106">Thanks for reviewing @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18222</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="153860633">18222</key><summary>Fix minor typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasonquick</reporter><labels><label>docs</label></labels><created>2016-05-09T20:20:20Z</created><updated>2016-05-10T07:39:56Z</updated><resolved>2016-05-10T07:39:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-10T07:39:52Z" id="218081697">thanks @jasonquick - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18221</link><project id="" key="" /><description>Actual error message is here.

``` json
{"error":{"root_cause":[{"type":"circuit_breaking_exception","reason":"[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [1478832947/1.3gb]","bytes_wanted":1478843760,"bytes_limit":1478832947}],"type":"circuit_breaking_exception","reason":"[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [1478832947/1.3gb]","bytes_wanted":1478843760,"bytes_limit":1478832947},"status":503}
```

**Elasticsearch version**: 5.0Alpha2

**JVM version**:

```
openjdk version "1.8.0_91"
OpenJDK Runtime Environment (build 1.8.0_91-8u91-b14-0ubuntu4~14.04-b14)
OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)
```

**OS version**:
LXC Guest running Ubuntu 14.04
**Description of the problem including expected versus actual behavior**:
Elasticsearch complains about [&lt;http_request&gt;] would be larger than limit after awhile.  I can restart Elasticsearch with more memory to give it more time until it occurs again.

I use Logstash for Injestion; no X-Pack plugin installed.  

**Steps to reproduce**:
1. Tarball installation of Elasticsearch 5.0Alpha2
2. Tarball Installation of Logstash
3. Forward logs until it complains it has run out of memory and starts rejecting writes

Let me know if you want My Logstash configuration and a sample of my logs; I don't think they have anything to do with it as it's a pretty old setup that has worked for awhile and I haven't added any new log sources nor am I dealing with weird Java hacks.

**Provide logs (if relevant)**:

```
[2016-05-09 19:19:33,099][WARN ][rest.suppressed          ] /_bulk Params: {}
CircuitBreakingException[[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [1478832947/1.3gb
]]
        at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService
.java:211)
        at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreak
er.java:128)
        at org.elasticsearch.http.HttpServer.dispatchRequest(HttpServer.java:109)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:489)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:65)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.
java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:85)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.
java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.
java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.
java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.
java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:83)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="153850705">18221</key><summary>[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damm</reporter><labels /><created>2016-05-09T19:28:12Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-05-09T19:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-09T19:31:15Z" id="217965001">Hi @damm 

Thanks for trying out alpha2!  This issue should be fixed by https://github.com/elastic/elasticsearch/pull/18204 which will be out in the next release.  Would you mind testing it out once released and reporting back if you see similar problems?
</comment><comment author="damm" created="2016-05-09T19:41:45Z" id="217967544">Would love to :) I'll check back when Alpha3 is released.
</comment><comment author="ayadav77" created="2016-07-21T13:02:09Z" id="234246279">Hi @damm were you able to validate if Alpha3 fixed this issue for you? I ask because, I am on Alpha4 and I am seeing this exact issue. Thanks.
</comment><comment author="clintongormley" created="2016-07-21T14:12:12Z" id="234265730">@ayadav77 this issue was fixed in alpha3 - If you're seeing the same thing (as opposed to sending one too-large request) then please open a new issue
</comment><comment author="ayadav77" created="2016-07-21T14:20:26Z" id="234268326">I figured out what I was doing wrong. In addition to multiple bulk index requests, i was also making multiple queries to elasticsearch using the scroll api. My scroll had a timeout set for 1 minute. I was not clearing my scroll once I was done with it, and over time that caused the "data too large" error to eventually appear. Once I started clearing my scroll, that fixed my issue. Thanks.
</comment><comment author="damm" created="2016-07-22T00:24:55Z" id="234423010">I never really need test that Alpha3 fixed it because I really don't like running with a broken setup that eventually OOM's my servers..
</comment><comment author="clintongormley" created="2016-07-27T14:28:48Z" id="235602582">@damm It's an alpha.  We warn you not to use alpha releases in production
</comment><comment author="damm" created="2016-07-27T18:55:07Z" id="235684215">@clintongormley it's my test servers; still having them randomly go out isn't much fun either.  You expect your test environment to be a little uppity granted.

I just didn't want to install alpha3 and then waiting 1-2weeks for it to OOM

I could remove extra ram but :( 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make doc_values accessible for _type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18220</link><project id="" key="" /><description>`doc_values` for _type field are created but any attempt to load them throws an IAE.

This PR re-enables `doc_values` loading for _type, it also enables `fielddata` loading if doc_values are not enabled.

It also restores the old docs that gives example on how to sort or aggregate on _type field.
I am not sure if the removal was intentional or not. It it's the case please ignore this PR.
</description><key id="153838634">18220</key><summary>Make doc_values accessible for _type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T18:26:16Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-25T16:57:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-09T18:30:23Z" id="217948585">Thanks for fixing - I wouldn't add the fielddata support though.  The `_type` field has not been configurable since 2.0, so doc values should always be enabled.

&gt; It also restores the old docs that gives example on how to sort or aggregate on _type field.
&gt; I am not sure if the removal was intentional or not. It it's the case please ignore this PR.

I think the docs were deleted by @nik9000 because he found that aggs/sorting didn't work, and assumed that was intentional when really it's a bug.
</comment><comment author="rjernst" created="2016-05-09T18:31:22Z" id="217948841">&gt;  it also enables fielddata loading if doc_values are not enabled

We don't allow setting any settings on _type since 2.0, so I don't think we should do this?
</comment><comment author="nik9000" created="2016-05-09T18:56:04Z" id="217955870">&gt; I think the docs were deleted by @nik9000 because he found that aggs/sorting didn't work, and assumed that was intentional when really it's a bug.

Yup. It didn't work. Same for _parent iirc.
</comment><comment author="jimczi" created="2016-05-09T19:52:29Z" id="217970186">@rjernst I've enabled fielddata mainly because the doc_values were disabled between 2.0 and 2.1.
This means that index created in 2.0 will never be able to load the fielddata for _type. Though I can check the version and enable the fielddata only for this version or completely drop the support for fielddata ?
</comment><comment author="jimczi" created="2016-05-10T20:52:51Z" id="218287826">@clintongormley @rjernst seems like you agreed that the fielddata should not be supported at all ?
</comment><comment author="clintongormley" created="2016-05-11T10:46:27Z" id="218424662">@rjernst i wasn't aware that doc values weren't written for 2.0.x.  Perhaps we do need the fielddata fallback for 5.x
</comment><comment author="rjernst" created="2016-05-15T03:19:15Z" id="219263914">If we do that, then it should not be a fallback, it should only be used for versions which did not have docvalues.
</comment><comment author="jimczi" created="2016-05-17T10:12:44Z" id="219675851">@rjernst I activated the fielddata loading only for versions which did not have docvalues. Could you please take a look ?  
</comment><comment author="jpountz" created="2016-05-25T06:55:42Z" id="221488833">This looks good to me, I just left some minor comments.
</comment><comment author="jimczi" created="2016-05-25T07:49:54Z" id="221498711">Thanks @jpountz ! I pushed some changes to address your comments.
</comment><comment author="jpountz" created="2016-05-25T15:01:42Z" id="221604593">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove FieldMapper.Builder.indexName.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18219</link><project id="" key="" /><description>The ability to configure index names that are different from the full name was
removed in 2.0.
</description><key id="153820981">18219</key><summary>Remove FieldMapper.Builder.indexName.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T16:53:03Z</created><updated>2016-05-10T06:17:44Z</updated><resolved>2016-05-10T06:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-09T16:58:01Z" id="217923315">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove code duplication in FieldsVisitor.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18218</link><project id="" key="" /><description>It currently has twice the same method, once with a MapperService instance and
once with a DocumentMapper. This commits only keeps the former.
</description><key id="153819292">18218</key><summary>Remove code duplication in FieldsVisitor.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T16:44:08Z</created><updated>2016-05-10T06:19:06Z</updated><resolved>2016-05-10T06:19:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-09T16:57:23Z" id="217923137">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>recovery issues when using multiple entries in path.data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18217</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.8.0_74

**OS version**: OSX 10.11.4

**Description of the problem including expected versus actual behavior**:
When using multiple entries for `path.data` on a node and then simulating removing a drive from the node, the node does not mark the shards that were on that shard as unavailable so re-assignment can happen. I would expect to see some initial errors get thrown, but after a little bit of time, see the shards that were assigned to the missing or timing out filesystem to be allocated elsewhere in the cluster (or the same node but a different data path).

**Steps to reproduce**:
1. Create four empty directories
2. Create a new index (using default 5/1 shard allocation) and index some data
3. Start up two nodes of ES, each one pointing to two different data directories. For example:

```
bin/elasticsearch --node.name=node1 --path.data=/data/node1-drive1,/data/node1-drive2
bin/elasticsearch --node.name=node2 --path.data=/data/node2-drive1,/data/node2-drive2
```
1. Remove one of the directories completely

**Provide logs (if relevant)**:
- Observed https://github.com/elastic/elasticsearch/issues/17395#issuecomment-217903474
- Also observed the following logs to continue to happen over and over with no recovery; Below are some examples of the various errors seen:

```
{
  "_index": "index1",
  "_type": "docs",
  "_id": "AVSWUC2X9feFc7L66p0Q",
  "_version": 1,
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 1,
    "failures": [
      {
        "_index": "index1",
        "_shard": 4,
        "_node": "X4nxoaPZTMqX-RCWBM79Ew",
        "reason": {
          "type": "create_failed_engine_exception",
          "reason": "Create failed for [docs#AVSWUC2X9feFc7L66p0Q]",
          "shard": "4",
          "index": "index1",
          "caused_by": {
            "type": "no_such_file_exception",
            "reason": "/data/node1-drive1/elasticsearch/nodes/0/indices/index1/4/index/write.lock"
          }
        },
        "status": "INTERNAL_SERVER_ERROR",
        "primary": false
      }
    ]
  },
  "created": true
}
```

```
[2016-05-09 11:40:24,812][DEBUG][action.admin.indices.stats] [node1] [indices:monitor/stats] failed to execute operation for shard [[.kibana][0], node[X4nxoaPZTMqX-RCWBM79Ew], [P], v[4], s[STARTED], a[id=hTbB0MAbTXCZuOSwh_fArA]]
ElasticsearchException[failed to refresh store stats]; nested: NoSuchFileException[/data/node1-drive1/elasticsearch/nodes/0/indices/.kibana/0/index];
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1532)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
    at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
    at org.elasticsearch.index.store.Store.stats(Store.java:293)
    at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.NoSuchFileException: /data/node1-drive1/elasticsearch/nodes/0/indices/.kibana/0/index
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:407)
    at java.nio.file.Files.newDirectoryStream(Files.java:457)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:191)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:203)
    at org.elasticsearch.index.store.FsDirectoryService$1.listAll(FsDirectoryService.java:127)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
    at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1538)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
    ... 17 more

```

```
[2016-05-09 12:22:03,174][WARN ][cluster.action.shard     ] [node1] [index1][1] received shard failed for target shard [[index1][1], node[X4nxoaPZTMqX-RCWBM79Ew], [R], v[17860], s[INITIALIZING], a[id=Og_FzyceRPK2Dt-U0w8SJw], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-09T16:22:03.137Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: AccessControlException[access denied ("java.io.FilePermission" "/data" "read")]; ]]], indexUUID [OlAoAb1cRHydJ-P8CNZFHw], message [failed to create shard], failure [ElasticsearchException[failed to create shard]; nested: AccessControlException[access denied ("java.io.FilePermission" "/data" "read")]; ]
[index1][[index1][1]] ElasticsearchException[failed to create shard]; nested: AccessControlException[access denied ("java.io.FilePermission" "/data" "read")];
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:371)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.security.AccessControlException: access denied ("java.io.FilePermission" "/data" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
    at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)
    at java.nio.file.Files.createDirectories(Files.java:746)
    at org.elasticsearch.index.store.FsDirectoryService.newDirectory(FsDirectoryService.java:85)
    at org.elasticsearch.index.store.Store.&lt;init&gt;(Store.java:123)
    at org.elasticsearch.index.store.Store.&lt;init&gt;(Store.java:118)
    at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:162)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:369)
    ... 10 more
```
</description><key id="153815690">18217</key><summary>recovery issues when using multiple entries in path.data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2016-05-09T16:25:58Z</created><updated>2016-05-13T09:23:23Z</updated><resolved>2016-05-13T09:23:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-09T18:19:32Z" id="217945492">Until we get rid of guice, I'm not sure there is anything we can do to fix this.
</comment><comment author="clintongormley" created="2016-05-13T09:23:23Z" id="218994224">Closing in favour of #18279
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Account for rpm behavior difference on directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18216</link><project id="" key="" /><description>This commit modifies the packaging tests to account for the fact that
rpm behaves differently with respect to preserving directories marked as
"CONFIG | NOREPLACE" on older versions versus newer versions. Older
versions will leave the directory as-is while newer versions will append
the suffix ".rpmsave" to the directory name.
</description><key id="153814535">18216</key><summary>Account for rpm behavior difference on directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T16:20:39Z</created><updated>2016-05-09T16:26:29Z</updated><resolved>2016-05-09T16:25:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-09T16:24:28Z" id="217914265">LGTM
</comment><comment author="jasontedor" created="2016-05-09T16:26:29Z" id="217914792">Thanks @rjernst.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove old backward compatibility layer for version lookups.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18215</link><project id="" key="" /><description>The current code tries to handle the case that document versions are either
missing or stored in payloads rather than doc values. Since none of the 2.x
releases allowed this, we can remove this logic.
</description><key id="153812967">18215</key><summary>Remove old backward compatibility layer for version lookups.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T16:13:03Z</created><updated>2016-05-10T06:16:45Z</updated><resolved>2016-05-10T06:16:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-09T17:37:03Z" id="217933646">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest: Add script processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18214</link><project id="" key="" /><description>This adds another processor, that supports scripting for fields.
Right now it works by using the return value of a script to update
a specific field, as this blends better into the ingest processor infrastructure
than updating a full document.

Another possibility would be to have the same as the Update API, where you
can set arbitrary fields in the map and then store this map as the new source.

Current example call:

```
curl -X PUT localhost:9200/_ingest/pipeline/script-pipeline -d '
{
  "description" : "script pipeline",
  "processors" : [
    {
    "script" : {
      "field" : "bytes_total",
      "lang" : "painless",
      "inline" : "return input.ctx._source.doc.bytes_in + input.ctx._source.doc.bytes_out"
    }
  }
 ]
}
'

curl -X POST localhost:9200/_ingest/pipeline/script-pipeline/_simulate -d '
{
  "docs" : [
    { "_source" : { "bytes_in" : 1024, "bytes_out" : 2048 } }
  ]
}
'
```

TO DISCUSS: Alternatively behave like the Update API and allow to configure anything. However this is much
harder to implement, as the IngestDocument only allows for single field updates and a complete replacement
of the ingest document would be more tricky and resource intensive.

```
curl -X PUT localhost:9200/_ingest/pipeline/script-pipeline -d '
{
  "description" : "script pipeline",
  "processors" : [
    {
      "script" : {
        "lang" : "painless",
        "inline" : "input.ctx._source.bytes_total = input.ctx._source.doc.bytes_in + input.ctx._source.doc.bytes_out"
      }
    }
  ]
}
'
```
</description><key id="153803806">18214</key><summary>Ingest: Add script processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Ingest</label><label>discuss</label></labels><created>2016-05-09T15:30:32Z</created><updated>2016-05-25T12:36:48Z</updated><resolved>2016-05-25T12:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-05-10T23:47:03Z" id="218324722">We need to combine our efforts here: https://github.com/elastic/elasticsearch/pull/18193 :)

The first question I have is whether this should be a plugin, module, or in core?
</comment><comment author="rmuir" created="2016-05-12T05:08:26Z" id="218661486">btw if you merge up, the painless syntax has changed in master. you don't need `input.ctx` just `ctx` (and if you need to access a parameter, its eg. `params.param1`). `ctx` is a reserved word for the executable scripts (e.g. updates), IMO the right way to go here. adding something named `doc` ala #18193 is not going to work, because its reserved for the search api.
</comment><comment author="spinscale" created="2016-05-25T12:36:48Z" id="221561755">Closing this one in favor of #18193 after syncing with @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve logging of raw rest actions on failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18213</link><project id="" key="" /><description>Log the method and the path.

This should give me more to go on when debugging integration test failures like
http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/3039/console

They don't reproduce locally....
</description><key id="153795925">18213</key><summary>Improve logging of raw rest actions on failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T14:56:42Z</created><updated>2016-05-09T17:05:00Z</updated><resolved>2016-05-09T17:05:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-09T15:10:38Z" id="217892825">LGTM
</comment><comment author="nik9000" created="2016-05-09T16:15:00Z" id="217911653">Thanks @jpountz !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not return fieldstats information for fields that exist in the mapping but not in the index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18212</link><project id="" key="" /><description>In #17980 FieldStats was extended to report searchable/aggregatable fields on any mapped field.
Since then it was found that it's problematic to return information on fields that exist in the mapping but not in the index.
This PR reverts this behavior, stats on a mapped field with no values indexed will return null.
</description><key id="153794500">18212</key><summary>Do not return fieldstats information for fields that exist in the mapping but not in the index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Stats</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T14:50:33Z</created><updated>2016-05-09T17:25:39Z</updated><resolved>2016-05-09T17:25:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-09T15:01:37Z" id="217890213">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Autosense annotation for query dsl testing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18211</link><project id="" key="" /><description>This adds the autosense annotation to a couple of query dsl docs files and fixes the snippets to work in the tests along the way.

Still needed: Switch from autosense to console, convert remaining query dsl docs.

Relates to #18160 / @nik9000 
</description><key id="153786957">18211</key><summary>Add Autosense annotation for query dsl testing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha3</label><label>WIP</label></labels><created>2016-05-09T14:18:00Z</created><updated>2016-05-17T18:55:45Z</updated><resolved>2016-05-17T18:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-10T19:00:33Z" id="218256581">LGTM
</comment><comment author="MaineC" created="2016-05-12T11:25:44Z" id="218730237">Modified the remaining query-dsl docs.

Before going ahead and merging I'd appreciate some feedback in particular to the changes made to docs/reference/query-dsl/geo-polygon-query.asciidoc and docs/reference/query-dsl/parent-id-query.asciidoc - examples in both needed a bit of adjustment to get past parsing at all. A second pair of eyes stating that the result is equal to the intent of the example before would be welcome.
</comment><comment author="nik9000" created="2016-05-12T15:08:41Z" id="218787181">I left a comment on the geo-polygon query but overall it looks pretty similar? I think the parent query changes are good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Renamed all AUTOSENSE snippets to CONSOLE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18210</link><project id="" key="" /><description>Autosense snippets in master should be opened in the Console app in Kibana, rather than in Sense.

This PR changes all occurrences of `// AUTOSENSE` to  `// CONSOLE`
</description><key id="153778491">18210</key><summary>Renamed all AUTOSENSE snippets to CONSOLE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T13:39:58Z</created><updated>2016-05-09T13:42:43Z</updated><resolved>2016-05-09T13:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-09T13:42:09Z" id="217867249">LGTM. We should probably rename the `listAutoSenseCandidates` task but I can grab that later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Auto-generated ids should put the mac address before the timestamp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18209</link><project id="" key="" /><description>Given that the terms dictionary has the structure of a trie, it compresses better when there are shared prefixes. By putting the mac address before the timestamp in our auto-generated ids, the `_uid` field would compress better. I suspect it will also slightly help for stored fields since there will be longer sequences of bytes that are shared across values (the mac address followed by the first bytes of the timestamp).
</description><key id="153777293">18209</key><summary>Auto-generated ids should put the mac address before the timestamp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-05-09T13:34:14Z</created><updated>2016-05-13T15:24:51Z</updated><resolved>2016-05-13T15:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-05-09T13:37:01Z" id="217865909">This is a good idea!  I'll try this, and test indexing perf.  I think it should give smaller terms dict and shouldn't impact lookup performance, except maybe from more free RAM for the OS.
</comment><comment author="jpountz" created="2016-05-09T14:08:17Z" id="217874395">For the record I was running simulations of disk space savings related to #18154 and found out that this change would reduce disk usage for the uid field by about 30% for a single node cluster (so there is a single mac address) that indexes about 100 docs per second (the rate is important as higher rates mean that timestamps have longer common prefixes). Also the size reduction is similar to the disk overhead of having points and doc values enabled on a float field that stores random values, so these savings should be noticeable for applications that store small documents.
</comment><comment author="jpountz" created="2016-05-09T14:19:31Z" id="217877566">We could also go further and interleave the bytes of the timestamp and the sequence number.
</comment><comment author="mikemccand" created="2016-05-09T20:32:28Z" id="217980595">I simulated "100 docs / sec" by forcing the timestamp to advance by 10 msec on each ID on current master:

```
diff --git a/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java b/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java
index d1a22a1..2422399 100644
--- a/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java
@@ -57,7 +57,9 @@ class TimeBasedUUIDGenerator implements UUIDGenerator {
             // Don't let timestamp go backwards, at least "on our watch" (while this JVM is running).  We are still vulnerable if we are
             // shut down, clock goes backwards, and we restart... for this we randomize the sequenceNumber on init to decrease chance of
             // collision:
-            timestamp = Math.max(lastTimestamp, timestamp);
+            // nocommit
+            timestamp = Math.max(lastTimestamp+10, timestamp);
+            //timestamp = Math.max(lastTimestamp, timestamp);

             if (sequenceId == 0) {
                 // Always force the clock to increment whenever sequence number is 0, in case we have a long time-slip backwards:
```

I ran that 6 times and got these total docs/sec:

mike@beast2:/l/es.dev$ grep "Total docs/sec" before*.log
before1.log:Total docs/sec: 54424.1
before2.log:Total docs/sec: 54471.2
before3.log:Total docs/sec: 53751.9
before4.log:Total docs/sec: 54300.0
before5.log:Total docs/sec: 54890.4
before6.log:Total docs/sec: 54452.5

and index sizes:

mike@beast2:/l/es.dev$ grep "index size" before*.log
before1.log:index size 4310740 KB
before2.log:index size 4305668 KB
before3.log:index size 4307560 KB
before4.log:index size 4312356 KB
before5.log:index size 4311084 KB
before6.log:index size 4300544 KB

Then I tried this patch, swapping the MAC address and timestamp, and also simulating "100 docs/sec":

```
diff --git a/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java b/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java
index d1a22a1..7f30067 100644
--- a/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java
@@ -57,7 +57,9 @@ class TimeBasedUUIDGenerator implements UUIDGenerator {
             // Don't let timestamp go backwards, at least "on our watch" (while this JVM is running).  We are still vulnerable if we are
             // shut down, clock goes backwards, and we restart... for this we randomize the sequenceNumber on init to decrease chance of
             // collision:
-            timestamp = Math.max(lastTimestamp, timestamp);
+            // nocommit
+            //timestamp = Math.max(lastTimestamp, timestamp);
+            timestamp = Math.max(lastTimestamp+10, timestamp);

             if (sequenceId == 0) {
                 // Always force the clock to increment whenever sequence number is 0, in case we have a long time-slip backwards:
@@ -70,10 +72,16 @@ class TimeBasedUUIDGenerator implements UUIDGenerator {
         final byte[] uuidBytes = new byte[15];

         // Only use lower 6 bytes of the timestamp (this will suffice beyond the year 10000):
-        putLong(uuidBytes, timestamp, 0, 6);
+        //putLong(uuidBytes, timestamp, 0, 6);

         // MAC address adds 6 bytes:
-        System.arraycopy(SECURE_MUNGED_ADDRESS, 0, uuidBytes, 6, SECURE_MUNGED_ADDRESS.length);
+        //System.arraycopy(SECURE_MUNGED_ADDRESS, 0, uuidBytes, 6, SECURE_MUNGED_ADDRESS.length);
+
+        // Only use lower 6 bytes of the timestamp (this will suffice beyond the year 10000):
+        putLong(uuidBytes, timestamp, 6, 6);
+
+        // MAC address adds 6 bytes:
+        System.arraycopy(SECURE_MUNGED_ADDRESS, 0, uuidBytes, 0, SECURE_MUNGED_ADDRESS.length);

         // Sequence number adds 3 bytes:
         putLong(uuidBytes, sequenceId, 12, 3);
```

mike@beast2:/l/es.dev$ grep "Total docs/sec" after*.log
after1.log:Total docs/sec: 53288.5
after2.log:Total docs/sec: 53840.9
after3.log:Total docs/sec: 54685.0
after4.log:Total docs/sec: 52242.2
after5.log:Total docs/sec: 54789.3
after6.log:Total docs/sec: 54465.0
mike@beast2:/l/es.dev$

and index sizes:

mike@beast2:/l/es.dev$ grep "index size" after*.log
after1.log:index size 4232136 KB
after2.log:index size 4231428 KB
after3.log:index size 4242724 KB
after4.log:index size 4232464 KB
after5.log:index size 4221880 KB
after6.log:index size 4227432 KB

Results are noisy but it seems like the change may slow indexing some?  Thinking about it more, I think those added 9 byte (after the 6 MAC address bytes are base64 encoded) common prefix adds a non-trivial cost because on each ID lookup the FST must walk each of those 9 bytes before getting to the "interesting" bytes (that change across documents).  We could maybe optimize such cases, by having the FST record its common prefix, and doing a (faster) check on each incoming ID.

We could maybe do the opposite, and "rotate" the timestamp so that the leading bytes change more frequently (it's a long value, milliseconds).  This would make a larger index but maybe faster lookups.

It did make the overall index maybe ~2.8% smaller.
</comment><comment author="jpountz" created="2016-05-10T06:13:36Z" id="218067875">This is interesting. This probably also means that prepending the type in the uid is a mistake.
</comment><comment author="mikemccand" created="2016-05-10T08:07:54Z" id="218087238">&gt; This probably also means that prepending the type in the uid is a mistake.

Hmm, indeed.

Must we even prefix `type#` in the auto-id case?  I guess at read time there are places where we expect to parse a `_uid` value back into its type and id?
</comment><comment author="jpountz" created="2016-05-10T08:12:52Z" id="218088249">It looks tricky to me to remove the type from the uid without removing the concept of type entirely. However something we could explore would be to put the type in the end of the uid in the auto-generated case (or in all cases actually) and/or to make mappings generate unique ids for each type so that we can use these (short) ids in the uids rather than the actual type names.
</comment><comment author="polyfractal" created="2016-05-10T15:55:51Z" id="218203699">Obligatory stats nerd interjection: the difference in the two indexing speeds is not statistically significant.  Two-tailed, unpaired, unequal variance t-test gives a **p = 0.2889**, which is greater than the standard 0.05 threshold.

![screen shot 2016-05-10 at 11 38 16 am](https://cloud.githubusercontent.com/assets/1224228/15152709/43070ec8-16a4-11e6-8bbb-5a2c1c7352f3.png)

The change in index size is statistically significant though:  **p = 5.773 x 10&lt;sup&gt;-09&lt;/sup&gt;**

![screen shot 2016-05-10 at 11 44 54 am](https://cloud.githubusercontent.com/assets/1224228/15152875/fbfc3048-16a4-11e6-9c31-4f4262963940.png)

Obviously this is on a small sample size, so the indexing speed significance may change if more samples are added (a true mean shift, or an increase in variance, etc).  But from the current data I don't think we can draw any conclusions.
</comment><comment author="mikemccand" created="2016-05-10T17:56:07Z" id="218238302">Aha, thank you @polyfractal.  So we really can't conclude there is any meaningful difference yet ... too much noise in the measurement.

I'll run it some more times :)
</comment><comment author="mikemccand" created="2016-05-11T09:53:39Z" id="218413869">OK I ran some more trials ... maybe @polyfractal you could run the statistical analysis again?  Thanks!

Before docs/sec (now 20 trials):

```
54424.1
54803.8
56129.1
53756.7
53817.1
54910.2
53056.8
53524.4
52600.5
53839.0
52737.8
54471.2
54252.8
53751.9
54300.0
54890.4
54452.5
52735.3
53426.8
54348.3
```

After docs/sec:

```
53288.5
53578.1
52666.7
52669.9
51953.5
52132.6
54437.0
53650.7
52747.6
53737.2
51327.8
53840.9
53180.1
54685.0
52242.2
54789.3
54465.0
53210.0
54774.2
53202.8
```
</comment><comment author="polyfractal" created="2016-05-11T13:03:18Z" id="218452403">Ok, now it's statistically significant :)

p = 0.02701

![screen shot 2016-05-11 at 8 59 50 am](https://cloud.githubusercontent.com/assets/1224228/15181748/e3108f54-1756-11e6-8944-94fe3a7d6ec1.png)
</comment><comment author="mikemccand" created="2016-05-11T14:08:20Z" id="218470313">OK, too bad :)  Thanks @polyfractal.
</comment><comment author="jpountz" created="2016-05-13T15:24:51Z" id="219075887">I'm closing then. Maybe we should save space by better compressing suffixes instead (https://issues.apache.org/jira/browse/LUCENE-4702).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CONSOLE is the new AUTOSENSE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18208</link><project id="" key="" /><description>This makes the test generation support both while we move from
`// AUTOSENSE` to `// CONSOLE`.

Will bother #18160
</description><key id="153775636">18208</key><summary>CONSOLE is the new AUTOSENSE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T13:25:30Z</created><updated>2016-05-09T13:38:06Z</updated><resolved>2016-05-09T13:38:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-09T13:28:17Z" id="217863693">Other than the regex, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove plugin script parsing of system properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18207</link><project id="" key="" /><description>The plugin script parses command-line options looking for Java system
properties and extracts these arguments to pass to the java command when
starting the JVM. Since elasticsearch-plugin allows arbitrary user
arguments to the JVM via ES_JAVA_OPTS, this parsing is unnecessary. This
commit removes this unnecessary translation.

Relates #18140
</description><key id="153772915">18207</key><summary>Remove plugin script parsing of system properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>:Plugins</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T13:11:19Z</created><updated>2016-05-09T17:06:18Z</updated><resolved>2016-05-09T17:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-09T17:03:08Z" id="217924797">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support combined highlights with indirect reference to nested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18206</link><project id="" key="" /><description>I have a smart-case sensitive indexing setup in place as described in my [question on stackoverflow](http://stackoverflow.com/questions/36960683/smartcase-searches-highlights-with-elasticsearch/36972516) .

**tl;dr**
I created a smart-case indexing setup that allows the user to perform smart-case partial matching on dynamic text fields. I.e. I do not know up-front what the structure of documents to be ingested wil be. 

Everything works, except for the highlighting. I am capable of getting individual highlights back, but not combined as one. The **"matched_fields"** option enables the behaviour that I would need, but it requires to know which fields would be highlighted. In our case, we use the wildcard "*" to get back the matches. However, this has no meaning in the matched_fields field. 

**feature request**
I would like to be able to write something similar to:

```
  "highlight": {
    "number_of_fragments": 0,
    "require_field_match": false,
    "fields": {
      "*": {
        "matched_fields": [ "*.case_sensitive", "*.case_insensitive" ]
      }
    }
  }
```

or

```
  "highlight": {
    "number_of_fragments": 0,
    "require_field_match": false,
    "fields": {
      "*": {
        "nested_matched_fields": [ "case_sensitive", "case_insensitive" ]
      }
    }
  }
```

which for each field would return a combined highlight created from their nested case_sensitive and case_insensitive fields. In other words, if ingested documents only have two fields: **description** and **title**, then this would translate to:

```
  "highlight": {
    "number_of_fragments": 0,
    "require_field_match": false,
    "fields": {
      "title": {
        "matched_fields": [ "title", "title.case_sensitive", "title.case_insensitive" ]
      },
      "description": {
        "matched_fields": [ "description", "description.case_sensitive", "description.case_insensitive" ]
      }
    }
  }
```
</description><key id="153767158">18206</key><summary>Support combined highlights with indirect reference to nested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chvndb</reporter><labels><label>:Highlighting</label><label>discuss</label></labels><created>2016-05-09T12:40:31Z</created><updated>2016-05-27T09:51:15Z</updated><resolved>2016-05-27T09:51:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-09T15:20:21Z" id="217895685">Highlighting works on a single field.  What logic would you expect would be applied to combine the highlighting results from multiple fields, and why can't the same logic be applied application aside by simply highlighting on (eg):

```
"highlight": {
  "fields": {
    "*.case*": {}
  }
}
```
</comment><comment author="chvndb" created="2016-05-09T19:42:07Z" id="217967648">[My post on stackoverflow](http://stackoverflow.com/questions/36960683/smartcase-searches-highlights-with-elasticsearch/36972516) explains that I currently make use of:

```
"highlight": {
  "fields": {
    "*": {}
  }
}
```

which works great. However, it returns highlights on the same field as separate highlights. From the example I gave there I would get back as a response:

```
"highlight": {
  "text.case_sensitive": [
    "&lt;em&gt;tHis&lt;/em&gt; .is a! Test"
  ],
  "text.case_insensitive": [
    "tHis .is a!&lt;em&gt; Test&lt;/em&gt;"
  ]
}
```

This is indeed correct, but requires me to merge both highlights manually in my application as I don't want to expose this logic to the user. What I would prefer to get as a result is:

```
"highlight": {
  "text": [
    "&lt;em&gt;tHis&lt;/em&gt; .is a!&lt;em&gt; Test&lt;/em&gt;"
  ]
}
```

The feature added as a result of https://github.com/elastic/elasticsearch/issues/3750 provides the **matched_fields** field to combine highlights from multiple fields into one. This is ideal for multi-fields that index the same field in different ways, just like the setup I have. The only issue with this feature is that it requires to know which fields will contain the highlights and state them in the matched_field field. I do not know up front which text fields my documents will have. Therefore, I use dynamic templates. I only know that each text field will be mapped as a multi-field with two nested fields (**case_sensitive** and **case_insensitive**) that index the field in a different [(in-depth details here)](http://stackoverflow.com/questions/36960683/smartcase-searches-highlights-with-elasticsearch/36972516).

Therefore, I can not write:

```
"highlight": {
  "fields": {
    "whateverfield": {
      "matched_field": [ "whateverfield.case_sensitive", "whateverfield.case_insensitive" ]
    }
  }
}
```

as I do not know which fields the ingested documents consist of from within my application. Therefore, I want to be able to write something like:

```
"highlight": {
  "fields": {
    "*": {
      "nested_matched_field": [ "case_sensitive", "case_insensitive" ]
    }
  }
}
```
</comment><comment author="nik9000" created="2016-05-09T19:54:58Z" id="217970814">&gt; #3750 provides the matched_fields

I follow what you want. You want the matched fields to derive from the field name so it is properly compatible with `*`. I'm worried about features like this because

&gt; I do not know which fields the ingested documents consist of from within my application

Often works great for small numbers of documents but starts to come apart at the seams if you have enough documents to have made thousands of fields. I mean, you'll get the right results but it'll be super slow and consume tons of resources. In general Elasticsearch's creating fields on the fly is useful but if you don't keep the number of these fields in check in some way you are asking to exhaust resources.

If you know that you are only going to have a dozen fields then you should just list them with the matched_fields. If you might have lots more I'm worried it is a recipe for disaster!

So, yeah, that is my 2c. Sorry I don't have a happier 2c.
</comment><comment author="chvndb" created="2016-05-09T20:47:30Z" id="217984682">I agree about the performance issue, but this is an inherent consequence of using the `*` wildcard or trying to support searches/linking over heterogeneous and unstructured data, as I am trying to do. IMO, It is up to the developer to not mis-use this and/or to understand that response times are dramatically increased and resources are being exhausted because of this. This might also be perfectly fine depending on the application, infrastructure and amount of data. E.g. highlighting might be enabled/disabled from the UI allowing the user to understand how a hit was matched. This would imply only 1 document being highlighted, thus the overhead is limited.

This is crucial in my case, understanding is a big part of the application, while it is also mandatory to support heterogeneous (possibly unstructured, i.e. only one field) data and be able to search and find matches across indexes/types. So, yes this might be slow, but that is unavoidable to the nature of some applications. Luckily, we are not working with big data. However, having to list fields requires me to introduce configurations that can be updated at run-time which become very cumbersome and are not very user friendly. Especially when data becomes temporary, i.e. the user can ingest data, do some searches and then remove it again.

ps. I do not mind contributing this, we only need to figure out the specifications.
</comment><comment author="markharwood" created="2016-05-27T09:51:14Z" id="222106706">&gt; So, yes this might be slow, but that is unavoidable to the nature of some applications. Luckily, we are not working with big data

We try to avoid providing core features that, while useful for small datasets, can have catastrophic effects when used on larger production systems. For this reason we would be unlikely to adopt something that didn't have performance as a priority.
Highlighting is always a "best-efforts" solution as it cannot ever hope to fully represent the full complexity of all user queries while also doing things like summarizing docs, working with multiple analyzers etc.
Given the small dataset and specialized analysis I suggest adapting an existing highlighter to meet your needs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ReindexResponse in favor of BulkIndexByScrollResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18205</link><project id="" key="" /><description>`ReindexResponse` duplicates some code from `BulkIndexByScrollResponse` in order to write/not write some fields from the response. I think that Reindex/Update-By-Query (and later Delete-By-Query) could all share the same `BulkIndexByScrollResponse`. This commits removes `ReindexResponse` and replaced it by `BulkIndexByScrollResponse` with the usage of `ToXContent.Params`.

@nik9000 Could you have a look please? Thanks.
</description><key id="153764649">18205</key><summary>Remove ReindexResponse in favor of BulkIndexByScrollResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T12:25:19Z</created><updated>2016-05-09T17:44:57Z</updated><resolved>2016-05-09T15:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-09T12:50:36Z" id="217855255">I'd probably simplify this a bit more and drop the map in the response object and replace with passing those params during rendering at the rest level. Otherwise I'm happy with it!
</comment><comment author="tlrx" created="2016-05-09T13:39:15Z" id="217866505">@nik9000 Thanks - I agree it makes sense to move the params at Rest level. I updated the code according to your comment, is it better now?
</comment><comment author="nik9000" created="2016-05-09T14:29:13Z" id="217880347">LGTM. I left a comment about maybe being able to remove another type parameter. I'm not sure if you can but it'd be cool. Either way, LGTM.
</comment><comment author="tlrx" created="2016-05-09T15:15:14Z" id="217894139">@nik9000 Thanks! I removed the `Response` type parameter and merged.

Do you think it should be backported to 2.x?
</comment><comment author="nik9000" created="2016-05-09T17:44:57Z" id="217935756">&gt; Do you think it should be backported to 2.x?

I don't think it needs to be, no. Maybe if we find ourselves having to backport something that needs it then yes, but otherwise I don't think so.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Free bytes reserved on request breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18204</link><project id="" key="" /><description>With this commit we free all bytes reserved on the request circuit breaker.

Closes #18144
</description><key id="153734875">18204</key><summary>Free bytes reserved on request breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Circuit Breakers</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T09:19:04Z</created><updated>2016-05-09T13:09:29Z</updated><resolved>2016-05-09T13:09:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-05-09T09:22:11Z" id="217816415">I checked all call sites of the problematic constructor `BytesRestResponse(RestStatus status)` but only `RestMainAction` is affected.

I wonder whether we should remove this constructor entirely in order to avoid this problem in the future. I also thought how we might test this. The best thing that came to my mind was a common base class for all REST action (integration) tests that defines the necessary scaffolding for a circuit breaker test. To the reviewer: Wdyt? Depending on the decision I think it makes sense to create a separate issue then instead of solving this in the context of this ticket / PR.
</comment><comment author="danielmitterdorfer" created="2016-05-09T09:32:02Z" id="217818371">Test Scenario:
1. Issue `curl -XHEAD http://localhost:9200/`
2. Check that all breakers are at zero bytes: `curl -s http://localhost:9200/_nodes/stats | jq '.nodes | keys as $node | {breakers: map(.breakers)}'` (requires `jq`)
</comment><comment author="s1monw" created="2016-05-09T10:14:17Z" id="217826975">&gt; I wonder whether we should remove this constructor entirely in order to avoid this problem in the future

++
</comment><comment author="s1monw" created="2016-05-09T10:16:33Z" id="217827426">@danielmitterdorfer is there a place in our code where we can assert that the breaker size is 0 for instance once we shut down the node or anything like this? Maybe something in the test framework or so?
</comment><comment author="danielmitterdorfer" created="2016-05-09T11:40:29Z" id="217842181">@s1monw: We have the necessary asserts already. I was just thinking whether we should enforce that we always have a test and if yes how (btw, the assert is in `InternalTestCluster#ensureEstimatedStats()`). Nevertheless, I added a test for this specific method now and also removed the constructor.
</comment><comment author="jasontedor" created="2016-05-09T12:03:48Z" id="217846117">&gt; We have the necessary asserts already. 

I would think that if we had the necessary asserts then the build would be failing with the leak in place?
</comment><comment author="danielmitterdorfer" created="2016-05-09T12:05:45Z" id="217846476">@jasontedor It would have failed but we didn't have a test for it yet.
</comment><comment author="s1monw" created="2016-05-09T12:07:55Z" id="217846850">&gt; ... It would have failed but we didn't have a test for it yet.
&gt;  Show all checks

awesome - thanks for the test..
</comment><comment author="s1monw" created="2016-05-09T12:08:20Z" id="217846914">LGTM
</comment><comment author="danielmitterdorfer" created="2016-05-09T12:09:22Z" id="217847086">Thanks for the review. I'll merge soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Determine content length eagerly in HttpServer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18203</link><project id="" key="" /><description>With this commit we eagerly evaluate content length in HttpServer
and also pass the same value to ResourceHandlingHttpChannel. With
this change it easier to reason about the content length that is
freed leaving no doubt that it must be identical to the reserved
amount.
</description><key id="153734245">18203</key><summary>Determine content length eagerly in HttpServer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-09T09:15:40Z</created><updated>2016-05-09T13:10:43Z</updated><resolved>2016-05-09T13:10:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-09T12:31:42Z" id="217851459">LGTM
</comment><comment author="danielmitterdorfer" created="2016-05-09T13:09:48Z" id="217859553">@nik9000 thanks for your review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException with StopFilter and simple_query_string with analyze_wildcard if the query contains only stop word prefixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18202</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: OpenJDK Runtime Environment (IcedTea 2.6.4) (7u95-2.6.4-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)

**OS version**: Ubuntu 14.04.1

**Description of the problem including expected versus actual behavior**:
We had an existing API backed by Solr, which we later switched to an Elasticsearch backend. The search endpoint is called repeatedly by our clients with each character typed into a search box (i.e. queries look like `q=t`, `q=te`, `q=tes`, `q=test`, etc. We know about CompletionSuggester, we just haven't figured out how to work into our solution while maintaining backward compatibility with our existing clients).

Until recently we have been using `query_string` for simple keyword searches, appending a '*' to each term to give us a prefix search, and using `analyze_wildcard` (in spite of the performance implications) to still give us reasonable stemming and so on. Now we've added a new client to the portfolio with a larger and somewhat different user base, and the queries we've been seeing have been less forgiving on our simple query sanitisation, so we want to switch to `simple_query_string`. For the most part this is working well, but we've been seeing NullPointerExceptions when the query string consists purely of stop words.

**Steps to reproduce**:
    #!/bin/bash

```
export ELASTICSEARCH_ENDPOINT="http://localhost:9200"

# Create indexes

curl -XPUT "$ELASTICSEARCH_ENDPOINT/play" -d '{
    "mappings": {
        "thing": {
            "properties": {
                "prop": {
                    "type": "string",
                    "analyzer": "stop"
                }
            }
        }
    }
}'

# Index documents
curl -XPOST "$ELASTICSEARCH_ENDPOINT/_bulk?refresh=true" -d '
{"index":{"_index":"play","_type":"thing"}}
{"prop":"Some text"}
'

# Do searches

curl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '
{
    "query": {
        "simple_query_string": {
            "query": "the*",
            "fields": [
                "prop"
            ],
            "analyze_wildcard": true
        }
    }
}
'
```

**Provide logs (if relevant)**:

```
[2016-05-08 23:11:16,115][DEBUG][action.search.type       ] [Aireo] [play][1], node[-bdul1V7QgKIWXKi-HTkgQ], [P], v[2], s[STARTED], a[id=_0myvoG0Q2yfqC6jIFCsgg]: Failed to execute [org.elasticsearch.action.search.SearchRequest@458bdcb7] lastShard [true]
RemoteTransportException[[Aireo][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [
{
    "query": {
        "simple_query_string": {
            "query": "the*",
            "fields": [
                "prop"
            ],
            "analyze_wildcard": true
        }
    }
}
]]; nested: NullPointerException;
Caused by: SearchParseException[failed to parse search source [
{
    "query": {
        "simple_query_string": {
            "query": "the*",
            "fields": [
                "prop"
            ],
            "analyze_wildcard": true
        }
    }
}
]]; nested: NullPointerException;
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:853)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:652)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:618)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:369)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.query.SimpleQueryParser.newPrefixQuery(SimpleQueryParser.java:131)
        at org.apache.lucene.queryparser.simple.SimpleQueryParser.consumeToken(SimpleQueryParser.java:406)
        at org.apache.lucene.queryparser.simple.SimpleQueryParser.parseSubQuery(SimpleQueryParser.java:212)
        at org.apache.lucene.queryparser.simple.SimpleQueryParser.parse(SimpleQueryParser.java:152)
        at org.elasticsearch.index.query.SimpleQueryStringParser.parse(SimpleQueryStringParser.java:212)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:256)
        at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:303)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:206)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:201)
        at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:836)
        ... 10 more

```
</description><key id="153680404">18202</key><summary>NullPointerException with StopFilter and simple_query_string with analyze_wildcard if the query contains only stop word prefixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jamestait</reporter><labels><label>:Query DSL</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-05-08T22:12:40Z</created><updated>2016-05-10T14:34:04Z</updated><resolved>2016-05-10T14:34:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-09T07:45:38Z" id="217797128">@dakrone could you look at this one please?
</comment><comment author="dakrone" created="2016-05-09T22:48:19Z" id="218012622">Okay, this is because the query after being analyzed becomes `null`, which it then tries to set a boost on and causes an NPE. I'll work on a fix for this, thanks @jamestait!
</comment><comment author="dakrone" created="2016-05-10T14:34:04Z" id="218176494">@jamestait pushed a fix for this that will be released in 2.3.3 and 2.4.0 (master already had a check for this)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch painless dynamic calls to invokedynamic, remove perf hack/cheat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18201</link><project id="" key="" /><description>Currently using any dynamic language features of painless is slow. We even have in docs recommending people add a lot of casts to work around this. The hack in https://github.com/elastic/elasticsearch/pull/18169 speeds up access to a document's fields, but this is fragile and not a general solution.

This PR switches all dynamic calls, dynamic loads, and dynamic stores over to use invokedynamic with inline caching (cascaded, goes megamorphic at 5 types). I based it on this one (BSD license) https://github.com/qmx/jsr292-cookbook/blob/master/inlining-cache/src/jsr292/cookbook/icache/RT.java

We don't need to do anything with dynamic array load/store, its already a very simple 3-way decision tree and never involves a dynamic lookup. Same goes for the operators: i left that stuff as is.

I removed the instanceof-performance hack for accessing a document's fields, we don't need it anymore.

I cleaned up some getter/setter stuff too, we compute these at compile time. Also its now possible to get access to `isEmpty()`-style methods as `.empty`, we need that to support the scripting api the way its documented for groovy at least. 

For the nightly benchmark script I see:
SEARCH expression (median): 0.271813 sec
SEARCH painless_static (median): 0.517333 sec
SEARCH painless_dynamic (median): 0.582639 sec

So we should clean up and simplify the documentation, I don't think the dynamic access is trappy anymore, and we shouldn't recommend people add a bunch of casts to their scripts to try to make them faster. If someone wants to eek out extra speed, instead they should use lucene expressions which bypasses the slow scripting API.
</description><key id="153671840">18201</key><summary>Switch painless dynamic calls to invokedynamic, remove perf hack/cheat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>das awesome</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-08T18:59:26Z</created><updated>2016-05-10T10:31:33Z</updated><resolved>2016-05-10T01:46:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-09T06:02:28Z" id="217782256">I cleaned up and ran benchmarks: this is currently a script that runs on all of geonames and looks like this: `(Math.log(Math.abs(input.doc['population'].value)) + input.doc['elevation'].value * input.doc['latitude'].value)/_score`

I altered the way we access document fields and values to show perf for different dynamic language features:

| style | median time (master) | median time (branch) |
| --- | --- | --- |
| `doc['field'].value` | 0.631238 | 0.582639 |
| `doc['field'].getValue()` | 4.148362 | 0.609807 |
| `doc.field.0` | 5.567170 | 0.603587 |
| `doc.field.get(0)` | 6.588988 | 0.606248 |

`.value` shows the hack from #18169 working: we remove the hack here and have the same performance. 
`.getValue()`: no-arg dynamic method
`.field.0`: two field loads, one that was especially slow before (the .0)
`.field.get(0)`: field load + dynamic method with parameter

The static version (with casts) is ~ `0.517333 s`, so there isn't much overhead. I benchmark with the scripts `inline` so these times include compilation and JIT costs.
</comment><comment author="rmuir" created="2016-05-09T08:04:58Z" id="217800711">Also I want to defer trying to "improve" the signature we create here. Currently we often know the real types of parameters, e.g. `get(0)`, we know the one parameter is an int, we don't need to box it, but we explicitly do a lot of conversions anyway. That is because the current code is setup to work that way. It is silly to keep it at Object long-term but I want to fix the super slow calls first. 

We can later refactor to improve that (e.g. all tests pass if i do this: https://github.com/rmuir/elasticsearch/commit/e029b9ec546f872b7ee093a9ef8d6180b9709318 but thats really hacky).
</comment><comment author="clintongormley" created="2016-05-09T08:06:01Z" id="217800908">This is phenomenal - nice work!
</comment><comment author="s1monw" created="2016-05-09T08:10:52Z" id="217801817">&gt; This is phenomenal - nice work!

++
</comment><comment author="uschindler" created="2016-05-09T10:59:01Z" id="217835199">Ho Robert, I have seen you made the suggested changes already. Thanks! To me this looks fine - I only reviewed the CallSite part. Looks great.

I would also do the other optimizations in separate issues, like Array.getLength() (which may be slower than a real access to the length field in bytecode) to some static&amp;private getters (see your TODO). The problem is that getting the array length is only a property in Java syntax, but behind the scenes all is a separate bytecode for array load/store/length: https://en.wikipedia.org/wiki/Java_bytecode_instruction_listings
</comment><comment author="uschindler" created="2016-05-09T11:01:40Z" id="217835679">P.S.: Thanks for the fruitful discussions this weekend! The whole MethodHandle stuff is so great. I was happy to help and add suggestions. I will review the remaining code a bit later and provide some feedback if there are questions open.
</comment><comment author="mikemccand" created="2016-05-09T13:56:29Z" id="217871168">We are now running simple script search performance tests in the nightly classic benchmarks: https://benchmarks.elastic.co/index.html#search_qps_scripts

It shows already a big speedup (I will add annots!) for painless dynamic from the last few improvements.
</comment><comment author="uschindler" created="2016-05-09T15:59:11Z" id="217907171">Hi Robert,
I created the MethodHandle code for the array.length bytecode: https://gist.github.com/uschindler/916930741f245204e418c56025a8b58a

This code has a static method where you can pass an primitive or x-extends-Object array and you get a MethodHandle to retun its length. This should be faster than the reflective Array.getLength (because it is statically compiled and we have a separate method for every primitive type). Maybe we shold benchmark it.

Basically it holds a static map array-&gt;type to methodhandle (dynamically build) for primitives and Object[], and if the actual type is not found there (e.g, String[]), it adds a necessary cast to Object[].

The code has test as main() method inline.
</comment><comment author="rmuir" created="2016-05-09T17:49:04Z" id="217936948">Thanks Uwe, I will fold in your ArrayUtil class instead of calling the slow Array.getLength.
</comment><comment author="jdconrad" created="2016-05-09T18:06:14Z" id="217941857">LGTM.  Thanks for doing this!
</comment><comment author="rmuir" created="2016-05-09T20:45:23Z" id="217984141">@uschindler for the array length, i want to defer that to another issue. I don't want us having possibility of slowness, but Array.getLength is an intrinsic method, it may not really be slow. We can benchmark it to see.
</comment><comment author="danielmitterdorfer" created="2016-05-09T20:50:51Z" id="217985617">@rmuir that's a great improvement. :) I think that besides looking at the median, it's also worth looking at the percentile distribution (esp. higher percentiles like 99%, 99.9%) to get a better overall impression. Just out of curiosity: Did you have a chance to have a look at these too?
</comment><comment author="rmuir" created="2016-05-09T20:58:12Z" id="217987616">@danielmitterdorfer The nightly benchmark is just to explain what the differences mean at a high level, it is not really what I used to develop the changes. That benchmark is extremely noisy and only outputs the median.

This isn't a micro-optimization, I didn't tune anything with any benchmark. A branch in the code + exact methodhandle call is night and day vs looking up a methodhandle from our whitelist and adapting it, and the situation is well studied and understood. Its just important to use correct datastructures for the problem (e.g. polymorphic cache) so that we don't call lookups over and over.
</comment><comment author="danielmitterdorfer" created="2016-05-09T21:09:30Z" id="217990669">@rmuir Oh, I didn't expect that you'd use the benchmark to tune anything (and I also know that they're quite noisy. I think this is partly system-inherent but we should (a) also ensure that we eliminate as much noise as possible on the platform where we run nightlies and (b) adopt approaches that help us to deal with noise (both topics are on my todo list)). Thanks for your explanation.
</comment><comment author="rmuir" created="2016-05-09T21:31:58Z" id="217996464">The nightlies could use big improvement. I think traditionally effort has been on indexing performance and not query performance. All the queries are noisy and nothing is setup for e.g. a developer to iterate/explore with (means not reindexing every time you run the benchmark, and being reasonably fast). 

For scripts, we also only test a fictional search script to at least have something. But for example, we don't even have a fictional update script yet, forget about having any variety in the types of scripts we want to benchmark or anything like that.
</comment><comment author="uschindler" created="2016-05-09T23:13:41Z" id="218016878">&gt; @uschindler for the array length, i want to defer that to another issue. I don't want us having possibility of slowness, but Array.getLength is an intrinsic method, it may not really be slow. We can benchmark it to see.

Yes, there should be a separate issue abbout the arrays and array-like structures. The arrayLoad and arrayStore methods in Def can be removed completely and similarly handled by invokeDynamic. This is especially important because the reflective Array.get/Array.set are slow like hell (see http://stackoverflow.com/questions/30306160/performance-of-java-lang-reflect-array, https://bugs.openjdk.java.net/browse/JDK-8051447).

If we use invokeDyanmic here, too, the bootstrap method (maybe a different bootstrap method - not sure) can select the right MethodHandles correctly based on types. Partly it is already implemented for the map gets, this can be done in a similar way for the list get/sets and using MethodHandles.arrayElementGetter/Setter. This should bring a huge improvement for array accesses!
</comment><comment author="uschindler" created="2016-05-09T23:32:00Z" id="218019942">+1 as a first step. LGTM
</comment><comment author="rmuir" created="2016-05-10T01:47:13Z" id="218038559">Thanks @uschindler for reviewing.
</comment><comment author="danielmitterdorfer" created="2016-05-10T05:07:20Z" id="218060202">@rmuir thanks for your feedback. I have created elastic/rally#96 and elastic/rally#97. Feel free to create more tickets if you find more shortcomings or just approach me. I really appreciate your input.
</comment><comment author="rmuir" created="2016-05-10T10:28:02Z" id="218118831">@danielmitterdorfer thanks for opening those issues! FYI I hope it didn't come across as a criticism of rally, just our current state, lots of room for improvement. 
</comment><comment author="danielmitterdorfer" created="2016-05-10T10:31:33Z" id="218119564">@rmuir You're welcome. Your feedback did not come as criticism at all, quite the contrary. I know that we have a lot of room for improvement and I also have lots of ideas on how to improve the situation but I really value your feedback.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.3.1 ignores default_lang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18200</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0

**OS version**: Ubuntu

These are my configuration values:

```
script.inline: on
script.indexed: on
script.file: on
script.default_lang: javascript
script.engine.javascript.inline: on
script.engine.javascript.indexed: on
script.engine.javascript.file: on
```

I have loaded the lang-javascript plugin and I can confirm I can see it in the log when loading the cluster (on all nodes of the cluster)

However, when I try to define a custom field on Kibana that reads:

`new Date(doc['timestamp'].value).getHours()`

I get this exception:

```
unexpected token 'Date' on line (1) position (4) was expecting one of &lt;EOF&gt;
```

I can also see it tries to render this using 'expression' instead of 'javascript':

```
[Failed to compile inline script [new Date(doc['timestamp'].value_.getHours()] using lang [expression]
```
</description><key id="153656362">18200</key><summary>Elasticsearch 2.3.1 ignores default_lang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">orweinberger</reporter><labels /><created>2016-05-08T13:41:08Z</created><updated>2016-05-09T09:19:49Z</updated><resolved>2016-05-09T07:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-09T07:35:51Z" id="217795395">Hi @orweinberger 

This works just fine in Elasticsearch 2.3.1.  `expression` is not the default lang, that's `groovy`.  I think you'll find that Kibana sets the `lang` parameter to `expression` as it is the only language that is guaranteed to be enabled for `inline` scripts.
</comment><comment author="orweinberger" created="2016-05-09T09:07:40Z" id="217813370">I can't find any references to this in the Kibana repository or issues, do you have any ideas how I can change the language to Javascript instead of expression?
</comment><comment author="clintongormley" created="2016-05-09T09:19:49Z" id="217815947">You'd be better off asking in the Kibana list, but I don't think you can separately.  Kibana is waiting for this issue to be fixed so that they can detect which languages are available: https://github.com/elastic/elasticsearch/issues/17114
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation claims "index.translog.sync_interval" is dynamic, but it can not be set dynamically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18199</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.2

**JVM version**: 1.8

**OS version**: Windows 10 Enterprise

**Description of the problem including expected versus actual behavior**:

The documentation (https://www.elastic.co/guide/en/elasticsearch/reference/2.3/index-modules-translog.html) claims that "index.translog.sync_interval" is a dynamic setting, but it can not be set dynamically.

When I try to set it dynamically, I get the following error message:

"Can't update non dynamic settings[[index.translog.sync_interval]] for open indices [[twitter-20160417000000]]"

**Steps to reproduce**:
1. Send the following to /&lt;&lt;your index&gt;&gt;/_settings:

{
  "index": {
    "refresh_interval": "-1",
    "number_of_replicas": "0",
    "blocks": {
      "read": "true"
    },
    "translog": {
      "durability": "async",
      "sync_interval": "5m",
      "flush_threshold_size": "1gb",
      "flush_threshold_ops": "-1",
      "interval": "15s"
    }
  }
}
1. Get the error

**Provide logs (if relevant)**:
</description><key id="153621085">18199</key><summary>Documentation claims "index.translog.sync_interval" is dynamic, but it can not be set dynamically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paullovessearch</reporter><labels><label>docs</label></labels><created>2016-05-07T22:19:52Z</created><updated>2016-05-08T09:23:40Z</updated><resolved>2016-05-08T09:23:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-08T09:23:39Z" id="217705353">Closed by ca4cc273ba41dbb1058153e56e1223c7671e6c14
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove settings and system properties entanglement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18198</link><project id="" key="" /><description>Today when parsing settings during bootstrap, we add a system property
for every Elasticsearch setting. Additionally, settings can be set via
system properties. This commit simplifies this situation.
- settings are no longer propogated to system properties
- system properties can not be used to set settings
- the "es." prefix on settings is no longer required (nor permitted)
- test logging has a dedicated system property (tests.logger.level)

Closes #18197 
</description><key id="153603494">18198</key><summary>Remove settings and system properties entanglement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-05-07T16:34:48Z</created><updated>2016-05-20T08:31:49Z</updated><resolved>2016-05-19T18:08:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-10T15:31:37Z" id="218195115">LGTM, left a couple minor comments/questions; after that, its ready.
</comment><comment author="jaymode" created="2016-05-19T17:32:31Z" id="220396310">LGTM
</comment><comment author="jasontedor" created="2016-05-19T18:13:53Z" id="220407858">Thanks @pickypg, @abeyad, and @jaymode for reviewing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the `es.` prefix from command line settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18197</link><project id="" key="" /><description>Today, command line settings look like this:

```
./bin/elasticsearch -E es.node.attr=foo
```

The `es.` shouldn't be necessary
</description><key id="153600198">18197</key><summary>Remove the `es.` prefix from command line settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Settings</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-05-07T15:28:13Z</created><updated>2016-05-19T18:08:08Z</updated><resolved>2016-05-19T18:08:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Build: Add fake project to include buildSrc as normal project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18196</link><project id="" key="" /><description>This is a follow up to #18173 and includes adding pom generation to the
fake build-tools project, which is really just buildSrc, but builds
during normal builds.
</description><key id="153575138">18196</key><summary>Build: Add fake project to include buildSrc as normal project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-07T05:26:45Z</created><updated>2016-05-09T17:24:24Z</updated><resolved>2016-05-09T17:24:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-09T17:10:56Z" id="217926816">Nice! I left some questions, mostly around if we need the eclipse/idea plugin included in the bootstrap phase version of the project. Beyond that I like it! Let me pull it locally and play a bit!
</comment><comment author="nik9000" created="2016-05-09T17:23:08Z" id="217930003">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sub keyword field to string dynamic mappings - name and intent discussion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18195</link><project id="" key="" /><description>As discussed with @jpountz in https://github.com/elastic/elasticsearch/pull/17188#issuecomment-215742185 opening up a separate ticket for discussion here.

Some items for consideration:
- Defaulting this way will continue to pattern of users seeing increased disk utilization out of the box as they upgraded versions of elasticsearch
- By using `keyword` for the multi-field name we are tightly coupling it to what tokenizer is used. For example if we every rename the `keyword` tokenizer to `noop` (which I would love to see since it more accurately describes what it does and also is how we tend to explain it to folks) then the multi-field option. 
</description><key id="153567364">18195</key><summary>sub keyword field to string dynamic mappings - name and intent discussion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">djschny</reporter><labels><label>:Mapping</label><label>docs</label></labels><created>2016-05-07T02:10:36Z</created><updated>2016-08-11T19:17:28Z</updated><resolved>2016-08-11T19:17:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-07T13:13:58Z" id="217634240">In the original issue (https://github.com/elastic/elasticsearch/issues/12394) I went into great detail to explain the reasoning behind this change, but to address your questions here:

&gt; Defaulting this way will continue to pattern of users seeing increased disk utilization out of the box as they upgraded versions of elasticsearch

In the past, the `string` field could be used for full text search and for aggregations, by loading all the terms into the heap in fielddata.  The behaviour of these fields depended largely on the type of value that was specified, eg "The quick brown fox..." implied the use of full text search (but not aggregations or sorting), while "London" might be a single identifier used for single-term lookups, aggregations and sorting.  But "New York", which is probably intended for the second use case could actually only be used for the first.

We can't deduce which use case a user intends when we receive a string field - it could be either.  The solution for this is to provide a main `text` field for full text search (with fielddata disabled so that users don't unwittingly flood their heap by trying to run aggregations or sorting on that field), and a sub-field of type `keyword` for the single-term lookup, sorting, and aggregations use case.

The benefit of this is that, without any config, you get both access patterns for string fields out of the box.  The downside is that you index string values twice.This is exactly the same pattern that Logstash has used for string fields for a long time so users of Logstash are unlikely to see any change.

It is very easy to optimize disk space usage here: just map your fields as `text` or `keyword` or add a dynamic mapping for `text`which specifies whether a field should be only `text` or only `keyword`.  

&gt; By using keyword for the multi-field name we are tightly coupling it to what tokenizer is used. For example if we every rename the keyword tokenizer to noop (which I would love to see since it more accurately describes what it does and also is how we tend to explain it to folks) then the multi-field option.

No we aren't.  This field is not named after the `keyword` analyzer, it is named after the field type `keyword`.  The field type got its name in the same way as the `keyword` analyzer did: we don't want full `text`, we want to treat this value as a single keyword.  What other name would your recommend to describe the datatype for this field?

And `keyword` fields in the future will not be restricted to the `keyword` analyzer. We will add support for limited analysis which allows, eg lowercasing or performing unicode normalization, or unicode collations. 

For me, the only debate is whether this sub-field should be called `keyword` or `raw`, which is the name used today in Logstash.  For bwc, `raw` would probably be better, but I think that `keyword` is more descriptive. My current feeling is that we should continue to use `keyword`.  Logstash is free to keep their index template which uses `raw` instead.
</comment><comment author="jpountz" created="2016-05-08T15:29:21Z" id="217727071">+1 to what Clinton said. The fact that we did not map strings both for text search and keyword search/aggs in the past caused bad out-of-the-box experiences since you almost certainly had to reindex once you realized that you could not aggregate on whole string values.

Regarding disk usage, it will be higher with default mappings for sure, but the problem is mitigated by the use of `ignore_above: 256`. There is a trade-off for sure, but I think having to reindex to run aggregations is more disappointing than higher-than-expected disk usage.

However I'm also open to changing the name to either `raw` like logstash or `original` like @rjernst suggested. I have a slight preference for `keyword` though.
</comment><comment author="clintongormley" created="2016-05-13T09:26:31Z" id="218994889">Discussed it in Fix it Friday - we prefer the `keyword` field.  Logstash can continue to use `raw` with dynamic templates, should they so choose.

I will improve the docs to explain that we're optimising for the OOB experience, but disk usage can be improved with some simple mappings.
</comment><comment author="djschny" created="2016-05-19T20:26:03Z" id="220442022">&gt; What other name would your recommend to describe the datatype for this field?

`not_tokenized`
</comment><comment author="jordansissel" created="2016-08-04T21:52:02Z" id="237694871">&gt; Logstash can continue to use raw

Much of the road to 5.0 has been a theme of consistency. We've used `raw` for a long long time, and now are suddenly calling this thing `keyword` -- this is inconsistent.  Logstash should not keep inconsistency and is looking at [fixing that](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/462) very soon, which is why I'm here talking about our new friend `keyword`. I do not believe Logstash can continue using `raw` because after 5.0 this becomes a user experience problem that ES uses `keyword` for strings where Logstash uses `raw`.

That said, for me personally, `keyword` is the wrong name. "United States" is two _words_, "San Jose Sharks" is three _words_, and yet the `keyword` name implies a singular `word`. A user agent string is even further something I would consider a `keyword` and yet I use Logstash's `raw` feature to allow me to do aggregations on user agents. My chief concerns on naming things is about how much I expect it to confuse users.

With the hands-on-workshop, I teach people about analyzers/tokenizers by showing what happens to a `string` by default in Elasticsearch, then we talk about treating these entire strings as a single field value (or "term"). Because we're on the topic of analyzers, it is easy to say "We solve this by using this thing called `not_analyzed`, and logstash calls this field the 'raw' value". It is early for this keyword feature, but I have trouble coming up with such a story for teaching.
</comment><comment author="dadoonet" created="2016-08-05T03:59:17Z" id="237749646">And raw is a shorter name :)

I think consistency is a good point here.

But I'd like to be able to apply some token filters on this type of fields at some point so I don't think that having "raw" + an analyzer would make sense in term of meaning.
"Keyword" + an analyzer has more meaning IMO.

I think we should mark this discussion as a blocker for the next release because it will be hard to change after we released the beta.
</comment><comment author="jordansissel" created="2016-08-08T20:50:58Z" id="238372868">I've been thinking the past few days how to find a way to convince myself that `keyword` is the right name. Here is the story on how I can explain to myself why `keyword` might be the right name:

I thought `keyword` was poor because I view Elasticsearch field mappings as a way to say "The data is of this type". This worked well for me to understand and explain various obvious-to-me data types in Elasticsearch such as dates, longs, floats, strings, etc.

In this model, I was telling Elasticsearch _what the data is_, and trying to distinguish strings vs keyword vs text was not fitting my mental model.

The Elasticsearch [documentation on mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html) says this:

&gt; Mapping is the process of defining how a document, and the fields it contains, are stored and indexed.

In this description, it seems that the mapping is presented as _how_ Elasticsearch uses the data, not _what_ the data is. If I view things with the _how_ in mind, instead of the _what_, I think `keyword` makes sense -- I can tell Elasticsearch _how_ to treat something like "United States" (such as text or keyword).

The above explanation may be confusing, but I think I can use this model -- _how_ instead of _what_ -- to tell stories in trainings, etc, about reasons for using `text` vs `keyword`. "Treat it as a keyword", for example.

I am still nervous about the difficult schema change this will require on the Logstash side; in the battle for consistency, Logstash will want to change the multifield `.raw` to match what Elasticsearch uses: `.keyword`.
</comment><comment author="jpountz" created="2016-08-09T07:16:24Z" id="238473026">If this proves to be a challenge to logstash, I'd personally be ok with keeping the field called `keyword` but having it named `xxx.raw` in the default mappings. Am I right to assume this is something you'd be happy with?
</comment><comment author="cdahlqvist" created="2016-08-09T08:31:51Z" id="238488243">There are a lot of users with massive amounts of data ingested through Logstash where the current .raw field convention is used. Changing the default from .raw has the potential to unnecessarily break a lot of systems and cause problems for users using the default templates or custom index templates based on these. Please take this into consideration before deciding to change the existing .raw field naming convention.
</comment><comment author="jordansissel" created="2016-08-09T23:34:03Z" id="238724442">@cdahlqvist We're discussing the options and impacts of `.raw` vs `.keyword` over on https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/462

I have a rough draft of a proposal here: https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/462#issuecomment-238376557
</comment><comment author="jordansissel" created="2016-08-09T23:35:42Z" id="238724718">@jpountz I'd be OK having ES's default to `xxx.raw`, yes. The benefit there is to not divide users across the release boundary of 5.0 (new users and old users would both get `.raw` if we did this)
</comment><comment author="clintongormley" created="2016-08-10T11:38:15Z" id="238841499">@jordansissel I agree with the conclusion you reached in https://github.com/elastic/elasticsearch/issues/18195#issuecomment-238372868 and I think that `keyword` is fundamentally the right name for this field (including for the reasons cited in https://github.com/elastic/elasticsearch/issues/18195#issuecomment-237749646). Long term it makes the purpose of the field easier to explain.

While I'm not completely against keeping the field as `raw`, I think that (unfettered by history) we'd choose `keyword` today instead.

All that said, I obviously recognise that this makes for a painful transition in Logstash.  I don't have great suggestions for how to make this easier, but the options are probably as follows:
- New users - use `keyword` from the outset
- Existing users with custom templates - most of these won't be much impacted
- Existing users with short retention periods - could use `raw` and `keyword` for the duration of the transition
- Existing users with long retention periods - could change the template to just use `raw` going forwards
</comment><comment author="jordansissel" created="2016-08-10T14:28:55Z" id="238884211">+1 clint's comments and keeping 'keyword'.

I think we can help users through  this period of transition. It may be
hard, but I think it's the right direction.

On Wednesday, August 10, 2016, Clinton Gormley notifications@github.com
wrote:

&gt; @jordansissel https://github.com/jordansissel I agree with the
&gt; conclusion you reached in #18195 (comment)
&gt; https://github.com/elastic/elasticsearch/issues/18195#issuecomment-238372868
&gt; and I think that keyword is fundamentally the right name for this field
&gt; (including for the reasons cited in #18195 (comment)
&gt; https://github.com/elastic/elasticsearch/issues/18195#issuecomment-237749646).
&gt; Long term it makes the purpose of the field easier to explain.
&gt; 
&gt; While I'm not completely against keeping the field as raw, I think that
&gt; (unfettered by history) we'd choose keyword today instead.
&gt; 
&gt; All that said, I obviously recognise that this makes for a painful
&gt; transition in Logstash. I don't have great suggestions for how to make this
&gt; easier, but the options are probably as follows:
&gt; - New users - use keyword from the outset
&gt; - Existing users with custom templates - most of these won't be much
&gt;   impacted
&gt; - Existing users with short retention periods - could use raw and
&gt;   keyword for the duration of the transition
&gt; - Existing users with long retention periods - could change the
&gt;   template to just use raw going forwards
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18195#issuecomment-238841499,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AAIC6vUIeZey6EZJHL9KAaDqxjsgRugYks5qebhpgaJpZM4IZVF6
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings: Fix ip mapper to correctly serialize a null null_value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18194</link><project id="" key="" /><description>We recently added correct serialization for null values, but the helper
method used does not allow null. This fixes serialization to handle the
null.

Note that this was never released, so I marked this as a non-issue.
</description><key id="153560889">18194</key><summary>Mappings: Fix ip mapper to correctly serialize a null null_value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-07T00:09:46Z</created><updated>2016-05-07T00:26:34Z</updated><resolved>2016-05-07T00:26:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-07T00:18:04Z" id="217591808">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>new ScriptProcessor for Ingest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18193</link><project id="" key="" /><description>Enable running arbitrary scripts from within an ingest pipeline.
</description><key id="153560636">18193</key><summary>new ScriptProcessor for Ingest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-05-07T00:06:29Z</created><updated>2016-06-15T21:58:45Z</updated><resolved>2016-06-15T21:57:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-12T05:13:06Z" id="218661928">I know there is nothing to see, but please see my comment on https://github.com/elastic/elasticsearch/pull/18193

Using `doc` here would not work with painless: if we go with this pr can we use `ctx` instead? This is what other executable scripts are using.
</comment><comment author="talevy" created="2016-05-12T14:41:47Z" id="218778721">@rmuir thanks for the heads up! note taken.
</comment><comment author="rmuir" created="2016-06-08T18:34:26Z" id="224686211">I dont understand the comment about how "compiling once shoudl be nice". I'm -1 if this thing compiles for every document.
</comment><comment author="talevy" created="2016-06-08T19:10:21Z" id="224696467">@rmuir @martijnvg it is my fault, I wasn't clear in my partial refactors. I have been slowly getting back to updating this. I do not mean for it to compile for every document. will update
</comment><comment author="martijnvg" created="2016-06-08T19:14:14Z" id="224697500">Maybe we shouldn't support stored here? The scripts are part of the pipeline itself is stored in the cluster state, so referring to to another part of the cluster state that contains stored scripts feels wrong to me. This would allow us the compile the script in the factory upon pipeline creation.
</comment><comment author="talevy" created="2016-06-08T19:28:47Z" id="224701307">@martijnvg I felt the same way, but thought that supporting stored would be a big feature win? I don't know
</comment><comment author="talevy" created="2016-06-08T19:29:28Z" id="224701478">Actually, I do know. I will leave it out of this PR. if we wish to support it in the future, we can.
</comment><comment author="talevy" created="2016-06-14T21:02:23Z" id="226015253">It seems, the current issue with supporting file-based scripts is that the ingest processor does not pick up file-changes, so it would be invalid if the file changes. @martijnvg recommended that we push back support for files and only support inline scripts. does anyone have any strong opinions around this? I would like to move this forward
</comment><comment author="martijnvg" created="2016-06-14T22:52:32Z" id="226040065">Maybe my suggestion was a bit too far. The processor uses `ScriptService` so compiled scripts do get cached, but compiled scripts may get evicted. Whereas if we kept the reference to the compiled script in the script processor it is always 'cached', but that approach doesn't work out if the scripts stored or file based. So I think the current approach is good and I don't think we should not push back on file or stored scripts.
</comment><comment author="martijnvg" created="2016-06-15T09:56:31Z" id="226142002">Aside the two small documentation notes, this LGTM.
</comment><comment author="talevy" created="2016-06-15T21:58:45Z" id="226332763">thanks for the reviews everybody!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move documentation testing to a plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18192</link><project id="" key="" /><description>This makes it easier for other projects to use it. This also cleans up some issues I found while getting it working on "other projects".
</description><key id="153545648">18192</key><summary>Move documentation testing to a plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T21:51:49Z</created><updated>2016-05-24T03:02:58Z</updated><resolved>2016-05-09T18:07:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-09T14:31:59Z" id="217881147">@rjernst can you have a look at this one?
</comment><comment author="rjernst" created="2016-05-09T16:49:35Z" id="217921028">LGTM
</comment><comment author="shavo007" created="2016-05-23T02:15:55Z" id="220874053">What is the plugin!?
</comment><comment author="nik9000" created="2016-05-23T17:59:11Z" id="221047038">&gt; What is the plugin!?

Sorry, it is a gradle plugin. We access it like:

```
apply plugin: 'elasticsearch.docs-test'
```

If you feel the need to use Elasticsearch's build as a dependency for your own gradle builds then this is it's maven location: http://search.maven.org/#artifactdetails%7Corg.elasticsearch.gradle%7Cbuild-tools%7C5.0.0-alpha2%7Cjar

You might want to do that if you are developing an Elasticsearch plugin. We have relatively fancy tools for that in the gradle build.

I don't think this particular change is released to that though.
</comment><comment author="shavo007" created="2016-05-24T03:02:58Z" id="221154530">Cool!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log setting key not setting object in IMC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18191</link><project id="" key="" /><description>This commit modifies two logging statements in the
IndexingMemoryController to log the key for the setting
indices.memory.index_buffer_size instead of the object.
</description><key id="153532878">18191</key><summary>Log setting key not setting object in IMC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T20:35:08Z</created><updated>2016-05-11T14:37:35Z</updated><resolved>2016-05-11T14:37:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-06T20:35:36Z" id="217551643">This currently leads to logging statements that look like

```
[2016-05-06 16:29:00,769][TRACE][indices                  ] [Red Guardian] total indexing heap bytes used [0b] vs {
  "key" : "indices.memory.index_buffer_size",
  "properties" : [ "NodeScope" ],
  "is_group_setting" : false,
  "default" : "10%"
} [98.9mb], currently writing bytes [0b]
```
</comment><comment author="mikemccand" created="2016-05-11T14:13:30Z" id="218471871">Woops, thanks @jasontedor, LGTM!
</comment><comment author="jasontedor" created="2016-05-11T14:37:35Z" id="218479333">Thanks @mikemccand.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add check for non-existent pipelines provided to simulate requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18190</link><project id="" key="" /><description>Instead of receiving a

```
{
"type" : "null_pointer_exception",
"reason" : null
}
```

you now receive a more detailed error:

```
{
"type" : "illegal_argument_exception",
"reason" : "pipeline [&lt;PIPELINE_ID&gt;] does not exist"
}
```

fixes #18139
</description><key id="153530929">18190</key><summary>add check for non-existent pipelines provided to simulate requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T20:23:04Z</created><updated>2016-05-07T12:46:13Z</updated><resolved>2016-05-06T20:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-06T20:31:46Z" id="217550769">LGTM
</comment><comment author="talevy" created="2016-05-06T20:34:08Z" id="217551331">thanks for the quick review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix exception message in lifecycle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18189</link><project id="" key="" /><description>This commit fixes the exception messages for lifecycles when stopping in
illegal states.
</description><key id="153509994">18189</key><summary>Fix exception message in lifecycle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T18:23:46Z</created><updated>2016-05-06T20:14:44Z</updated><resolved>2016-05-06T20:14:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-06T18:26:15Z" id="217521756">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Preserve config files from RPM install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18188</link><project id="" key="" /><description>This commit modifies the packaging for the RPM package so that edits to
config files will not get lost during removal and upgrade.

Closes #18158
</description><key id="153478985">18188</key><summary>Preserve config files from RPM install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T15:38:57Z</created><updated>2016-05-06T17:24:54Z</updated><resolved>2016-05-06T17:24:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-06T15:42:10Z" id="217478778">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add node name to Cat Recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18187</link><project id="" key="" /><description>While purging some old issues I found that this is still not fixed despite of multiple attempts...

Since the regexp used for validation in the REST test can be very resource consuming, I changed the test to validate `source_node` and `target_node` separately.

closes #8041
</description><key id="153471335">18187</key><summary>Add node name to Cat Recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T15:04:08Z</created><updated>2016-05-07T12:43:40Z</updated><resolved>2016-05-06T15:20:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-06T15:15:25Z" id="217470535">LGTM.
</comment><comment author="tlrx" created="2016-05-06T15:20:58Z" id="217472097">@jasontedor thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add semicolon query string parameter delimiter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18186</link><project id="" key="" /><description>This commit adds support for the semicolon character as a valid query
string parameter delimiter.

Closes #18175
</description><key id="153453368">18186</key><summary>Add semicolon query string parameter delimiter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:REST</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T13:40:29Z</created><updated>2016-05-26T11:44:28Z</updated><resolved>2016-05-06T13:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-06T13:45:23Z" id="217444407">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong score under explain when using dfs search type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18185</link><project id="" key="" /><description>**Elasticsearch version**: `Elasticsearch 2.3`

**JVM version**: `Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)`

**OS version**: Any

**Description of the problem including expected versus actual behavior**:

This only happens when `search_type=dfs_query_then_fetch`! Looks like the score that the explain object is returning is different than the actual score that it is being returned in the hit.

First, start elasticsearch 1.7 to verify that the `explain` was working fine. Do the following bulk operation:

```
POST /_bulk
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
{ "index": { "_index": "trial", "_type": "type" } }
{ "city": "montevideo", "cityAliases": "mvd" }
```

Then, execute the following search operation:

```
POST /trial/_search?search_type=dfs_query_and_fetch&amp;explain&amp;size=1
{"query":{"multi_match":{"query":"mvd","fields":["city","cityAliases"]}}}
```

Verify that the score under explain is the same as the main score:

```
# Result - Main Score:  0.24285339, _explanation Score: 0.24285339
{
  "_shard": 0,
  "_node": "bPFH3L1nT9-BTOKWyd-i0Q",
  "_index": "trial",
  "_type": "type",
  "_id": "AVSGJuxrGCZTdIeL7K4h",
  "_score": 0.24285339,
  "_source": {
    "city": "montevideo",
    "cityAliases": "mvd"
  },
  "_explanation": {
    "value": 0.24285339,
    "description": "max of:",
    "details": [
      {
        "value": 0.24285339,
        "description": "weight(cityAliases:mvd in 0) [PerFieldSimilarity], result of:",
        "details": [
          {
            "value": 0.24285339,
            "description": "score(doc=0,freq=1.0), product of:",
            "details": [
              {
                "value": 0.26398334,
                "description": "queryWeight, product of:",
                "details": [
                  {
                    "value": 0.9199573,
                    "description": "idf(docFreq=12, maxDocs=12)"
                  },
                  {
                    "value": 0.28695172,
                    "description": "queryNorm"
                  }
                ]
              },
              {
                "value": 0.9199573,
                "description": "fieldWeight in 0, product of:",
                "details": [
                  {
                    "value": 1,
                    "description": "tf(freq=1.0), with freq of:",
                    "details": [
                      {
                        "value": 1,
                        "description": "termFreq=1.0"
                      }
                    ]
                  },
                  {
                    "value": 0.9199573,
                    "description": "idf(docFreq=12, maxDocs=12)"
                  },
                  {
                    "value": 1,
                    "description": "fieldNorm(doc=0)"
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  }
}
```

Now, do the same in Elasticsearch 2.3.  Use the same bulk operation and query!

Verify that the score under explain is **DIFFERENT** as the main score. The score is `0.24285339` but the explain is giving `0.2417773`. With larger datasets the difference between explain and the score can be bigger.

```
# Result - Main Score:  0.24285339, _explanation Score: 0.2417773
{
  "_shard": 1,
  "_node": "CY8y3EdDQyWmUkB9oB3MgQ",
  "_index": "trial",
  "_type": "type",
  "_id": "AVSGKy64GyfPh09elcKQ",
  "_score": 0.24285339,
  "_source": {
    "city": "montevideo",
    "cityAliases": "mvd"
  },
  "_explanation": {
    "value": 0.2417773,
    "description": "max of:",
    "details": [
      {
        "value": 0.2417773,
        "description": "weight(cityAliases:mvd in 0) [PerFieldSimilarity], result of:",
        "details": [
          {
            "value": 0.2417773,
            "description": "score(doc=0,freq=1.0), product of:",
            "details": [
              {
                "value": 0.3394233,
                "description": "queryWeight, product of:",
                "details": [
                  {
                    "value": 0.71231794,
                    "description": "idf(docFreq=3, maxDocs=3)",
                    "details": []
                  },
                  {
                    "value": 0.47650534,
                    "description": "queryNorm",
                    "details": []
                  }
                ]
              },
              {
                "value": 0.71231794,
                "description": "fieldWeight in 0, product of:",
                "details": [
                  {
                    "value": 1,
                    "description": "tf(freq=1.0), with freq of:",
                    "details": [
                      {
                        "value": 1,
                        "description": "termFreq=1.0",
                        "details": []
                      }
                    ]
                  },
                  {
                    "value": 0.71231794,
                    "description": "idf(docFreq=3, maxDocs=3)",
                    "details": []
                  },
                  {
                    "value": 1,
                    "description": "fieldNorm(doc=0)",
                    "details": []
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  }
}
```
</description><key id="153448284">18185</key><summary>Wrong score under explain when using dfs search type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels /><created>2016-05-06T13:15:23Z</created><updated>2016-05-06T13:19:17Z</updated><resolved>2016-05-06T13:19:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-06T13:19:17Z" id="217437190">Duplicate of #15369
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow leading `/` in AUTOSENSE path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18184</link><project id="" key="" /><description>Relates to #18160
</description><key id="153447541">18184</key><summary>Allow leading `/` in AUTOSENSE path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T13:10:58Z</created><updated>2016-05-06T13:26:45Z</updated><resolved>2016-05-06T13:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-06T13:25:38Z" id="217439096">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exclude all but string fields from highlighting if wildcards are used&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18183</link><project id="" key="" /><description>&#8230; in fieldname

We should prevent highlighting if a field is anything but a text or keyword field.
However, someone might implement a custom field type that has text and still want to
highlight on that. We cannot know in advance if the highlighter will be able to
highlight such a field and so we do the following:
If the field is only highlighted because the field matches a wildcard we assume
it was a mistake and do not process it.
If the field was explicitly given we assume that whoever issued the query knew
what they were doing and try to highlight anyway.

closes #17537

Note that with this pr if a user adds the full name of a `geo_point` field to the highlight part of the request they will still get the exception seen in  #17537.

Another option would be to list all the fields that we know we cannot highlight and exclude them in the HighlightPhase. But then someone with a custom mapper might run into issues when using wildcards in the highlight request.
</description><key id="153435308">18183</key><summary>Exclude all but string fields from highlighting if wildcards are used&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Highlighting</label><label>bug</label><label>v2.3.3</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T11:50:04Z</created><updated>2016-09-08T17:39:47Z</updated><resolved>2016-05-06T14:57:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-06T14:52:37Z" id="217463252">LGTM
</comment><comment author="clintongormley" created="2016-05-07T12:42:39Z" id="217633006">@brwe this is merged into 2.3, correct? please could you also merge to 2.x
</comment><comment author="brwe" created="2016-05-09T08:55:55Z" id="217810904">@clintongormley 2.x commit is here: https://github.com/elastic/elasticsearch/commit/968d3a0d561b0c77575de3feaf7b6ec207b548fc and 2.3 here:https://github.com/elastic/elasticsearch/commit/635b0e9db9bcbd5fce7722a85f4ef4decf448e23
</comment><comment author="clintongormley" created="2016-05-09T09:18:13Z" id="217815601">thanks @brwe - i've added the appropriate version labels
</comment><comment author="rodgermoore" created="2016-05-19T13:06:01Z" id="220318094">I'm still running into issue https://github.com/elastic/elasticsearch/issues/17537 after updating to 2.3.3. 
</comment><comment author="benzitohhh" created="2016-09-08T17:39:47Z" id="245677837">Highlighting no longer works for Lucene-style wilcard queries within a function-score.

This was working ok in v2.3.4, but is not working since v2.4.0

For example, previously the below was returning highlights ok. Now it still returns results, but without any highlights:

```
{
    "highlight": {
        "fields": {
          "name": {}
        }
    },

    "query": {
        "function_score": {
            "script_score": {
                "script": "(1 + 5 * log(1 + doc['size_active'].value) )"
            },

            "query": {
                "query_string": {
                    "fields": [
                        "name"
                    ],
                    "query": "microsof*"
                }
            }

        }
    }
}
```

The same query without the wildcard works returns highlights for all hits:

```
{
    "highlight": {
        "fields": {
          "name": {}
        }
    },

    "query": {
        "function_score": {
            "script_score": {
                "script": "(1 + 5 * log(1 + doc['size_active'].value) )"
            },

            "query": {
                "query_string": {
                    "fields": [
                        "name"
                    ],
                    "query": "microsoft"
                }
            }

        }
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception on shutting down the embedded node when any of the services is closed before the node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18182</link><project id="" key="" /><description>Elasticsearch version: 2.3.1

Background: I'm running an embedded elasticsearch node in a spring boot application and have a scheduled job using ClusterService. 

On application shutdown when ClusterService is closed independently, before node is closed, then I get a following exception on closing a node:

```
java.lang.IllegalStateException: Can't move to started state when closed
    at org.elasticsearch.common.component.Lifecycle.canMoveToStopped(Lifecycle.java:144)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:81)
    at org.elasticsearch.node.Node.stop(Node.java:326)
    at org.elasticsearch.node.Node.close(Node.java:351)
    at com.example.NodeManager.destroy(NodeManager.java:87)
    at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:262)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:578)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:554)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:972)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:523)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:979)
    at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1006)
    at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:982)
    at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:901)
```
1. Message seems to be wrong. It tries to move to STOPPED state.
2. Shouldn't closed services be ignored when node closes?
</description><key id="153429857">18182</key><summary>Exception on shutting down the embedded node when any of the services is closed before the node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bikeholik</reporter><labels /><created>2016-05-06T11:10:48Z</created><updated>2016-05-07T15:26:42Z</updated><resolved>2016-05-07T15:26:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-07T15:26:41Z" id="217644141">&gt; Message seems to be wrong. It tries to move to STOPPED state.

Already corrected in master in d0d2d2be8c2cbd94949273b4bc848aa0430c10a4.

&gt; Shouldn't closed services be ignored when node closes?

Why is the cluster service already closed? Closed services should not be ignored when stopping a node; it's a bug if we are stopping services that are already closed.

Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove StringBuilder reuse for uid creation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18181</link><project id="" key="" /><description>This would be better handled by escape analysis.
</description><key id="153427263">18181</key><summary>Remove StringBuilder reuse for uid creation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T10:53:18Z</created><updated>2016-05-06T11:25:20Z</updated><resolved>2016-05-06T11:25:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-06T10:58:01Z" id="217411145">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping cleanups.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18180</link><project id="" key="" /><description>This removes dead/duplicate code and makes the `_index` field not configurable.
(Configuration used to just be ignored, now we would throw an exception if anything
is provided.)
</description><key id="153426175">18180</key><summary>Mapping cleanups.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T10:45:37Z</created><updated>2016-05-10T06:14:55Z</updated><resolved>2016-05-10T06:14:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-09T16:55:31Z" id="217922598">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make _source transformations great again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18179</link><project id="" key="" /><description>There are ALOT of cases where I have a custom _id, that is not just a random/unique value but a combination of different values, that I also want to index, but don't want to store again in the _source.

Example _id: "project_id:user_id:thing_id:action:timestamp" and I would need to index all of these values separately, meaning storing them again as separate fields in the _source.

Currently this is not a problem if I don't store _source. Is there any alternative to transformations ?

Thank You
</description><key id="153419410">18179</key><summary>Make _source transformations great again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ddorian</reporter><labels /><created>2016-05-06T10:02:00Z</created><updated>2016-05-06T10:14:47Z</updated><resolved>2016-05-06T10:14:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-06T10:14:46Z" id="217404173">Those values need to be in the `_source` in order to be indexed, but you can disable storing the `_source`.  Of course, that prevents you from reindexing your document easily (although you could use a script to generate these fields from the `_id` field during reindexing).  but `_source`  transform isn't coming back.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch doesn't support backquote?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18178</link><project id="" key="" /><description>Hi, I met a problem these days that I cannot query backquote in elastic.
My setting included letter, digit, symbol and punctuation. But it seems that ` is not included. I tried version 1.6, 2.2 and 2.3 either. It seems the same.
Did anybody meet this problem?
</description><key id="153414499">18178</key><summary>elasticsearch doesn't support backquote?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qpily</reporter><labels /><created>2016-05-06T09:33:24Z</created><updated>2016-05-06T10:18:17Z</updated><resolved>2016-05-06T10:18:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-06T10:18:17Z" id="217404718">Hi @qpily 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pipeline Stats: Fix concurrent modification exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18177</link><project id="" key="" /><description>Due to trying to modify a map while iterating it, a concurrent modification
in the pipeline stats could be thrown. This uses an iterator to prevent this.

Closes #18126
</description><key id="153413154">18177</key><summary>Pipeline Stats: Fix concurrent modification exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T09:25:36Z</created><updated>2016-05-06T13:00:45Z</updated><resolved>2016-05-06T13:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-06T10:21:13Z" id="217405180">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back Version.V_5_0_0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18176</link><project id="" key="" /><description>This was lost whene releasing alpha2 since the version constant got renamed.
</description><key id="153395594">18176</key><summary>Add back Version.V_5_0_0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T07:36:25Z</created><updated>2016-05-06T10:31:20Z</updated><resolved>2016-05-06T10:31:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-06T10:11:54Z" id="217403695">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> rfc3986, html 4.01 spec: support semicolons as well as ampersand?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18175</link><project id="" key="" /><description>I noticed that this works:

`curl http://localhost:9200/_cluster/health?level=indices&amp;pretty=true`

but this doesn't work:

`curl http://localhost:9200/_cluster/health?level=indices;pretty=true`

Using the ampersand as separator seems to be pretty standard, see:
- https://www.w3.org/TR/html401/appendix/notes.html#h-B.2.2
- https://tools.ietf.org/html/rfc3986
- https://en.wikipedia.org/wiki/Query_string#Structure
</description><key id="153395119">18175</key><summary> rfc3986, html 4.01 spec: support semicolons as well as ampersand?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">LaszloHont</reporter><labels><label>:REST</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-05-06T07:32:26Z</created><updated>2016-05-06T13:54:26Z</updated><resolved>2016-05-06T13:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-06T09:11:40Z" id="217393102">Semi-colons are reserved sub-delimiters in query strings, meaning that semi-colons in keys/values should be percent encoded.  This means we could support them without risk
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove handshake from transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18174</link><project id="" key="" /><description>This commit removes handshaking from the transport client. This
handshaking is not needed because of the existence of the liveness
check.

Relates #15971 
</description><key id="153358753">18174</key><summary>Remove handshake from transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T01:09:40Z</created><updated>2016-05-06T13:17:18Z</updated><resolved>2016-05-06T13:17:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-06T12:54:07Z" id="217431016">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Add pom generation to assemble task</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18173</link><project id="" key="" /><description>In preparation for a unified release process, we need to be able to
generate the pom files independently of trying to actually publish. This
change adds back the maven-publish plugin just for that purpose. The
nexus plugin still exists for now, so that we do not break snapshots,
but that can be removed at a later time once snapshots are happenign
through the unified tools. Note I also changed the dir jars are written
into so that all our artifacts are under build/distributions.
</description><key id="153357744">18173</key><summary>Build: Add pom generation to assemble task</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-06T00:58:28Z</created><updated>2016-05-06T20:11:15Z</updated><resolved>2016-05-06T20:11:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-06T13:17:46Z" id="217436896">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Delay starting progress loggers for vagrant until test is running</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18172</link><project id="" key="" /><description>This was broken recently as part of making the vagrant tasks extend
LoggedExec. This change fixes the progress logger to not be started
until we start seeing output from vagrant.
</description><key id="153346713">18172</key><summary>Tests: Delay starting progress loggers for vagrant until test is running</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T23:13:38Z</created><updated>2016-05-09T17:16:15Z</updated><resolved>2016-05-09T17:16:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-09T17:14:23Z" id="217927720">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Random script fields can't overlap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18171</link><project id="" key="" /><description>This causes round tripping through xcontent to fail.

Closes #18166
</description><key id="153337326">18171</key><summary>Random script fields can't overlap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>jenkins</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T22:09:18Z</created><updated>2016-05-06T13:01:39Z</updated><resolved>2016-05-06T13:01:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-05T23:15:43Z" id="217306897">LGTM
</comment><comment author="jasontedor" created="2016-05-06T00:19:06Z" id="217316363">@nik9000 I marked this test as awaits fix in 1199cd8e2a1343ba74f208c96d6798ab0da4bdfd. Can you revert as part of integrating this pull request?
</comment><comment author="nik9000" created="2016-05-06T12:51:46Z" id="217430634">&gt; @nik9000 I marked this test as awaits fix in 1199cd8. Can you revert as part of integrating this pull request?

I didn't realize it was failing consistently enough for that. Sorry!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging: Make rpm not include parent dirs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18170</link><project id="" key="" /><description>With this change, the contents of the rpm are now this:

```
/etc/elasticsearch
/etc/elasticsearch/elasticsearch.yml
/etc/elasticsearch/jvm.options
/etc/elasticsearch/logging.yml
/etc/elasticsearch/scripts
/etc/init.d/elasticsearch
/etc/sysconfig/elasticsearch
/usr/lib/sysctl.d/elasticsearch.conf
/usr/lib/systemd/system/elasticsearch.service
/usr/lib/tmpfiles.d/elasticsearch.conf
/usr/share/elasticsearch/LICENSE.txt
/usr/share/elasticsearch/NOTICE.txt
/usr/share/elasticsearch/README.textile
/usr/share/elasticsearch/bin/elasticsearch
/usr/share/elasticsearch/bin/elasticsearch-plugin
/usr/share/elasticsearch/bin/elasticsearch-systemd-pre-exec
/usr/share/elasticsearch/bin/elasticsearch.in.sh
/usr/share/elasticsearch/lib/HdrHistogram-2.1.6.jar
/usr/share/elasticsearch/lib/apache-log4j-extras-1.2.17.jar
/usr/share/elasticsearch/lib/elasticsearch-5.0.0-alpha2-SNAPSHOT.jar
/usr/share/elasticsearch/lib/hppc-0.7.1.jar
/usr/share/elasticsearch/lib/jackson-core-2.7.1.jar
/usr/share/elasticsearch/lib/jackson-dataformat-cbor-2.7.1.jar
/usr/share/elasticsearch/lib/jackson-dataformat-smile-2.7.1.jar
/usr/share/elasticsearch/lib/jackson-dataformat-yaml-2.7.1.jar
/usr/share/elasticsearch/lib/jna-4.1.0.jar
/usr/share/elasticsearch/lib/joda-convert-1.2.jar
/usr/share/elasticsearch/lib/joda-time-2.8.2.jar
/usr/share/elasticsearch/lib/jopt-simple-4.9.jar
/usr/share/elasticsearch/lib/jts-1.13.jar
/usr/share/elasticsearch/lib/log4j-1.2.17.jar
/usr/share/elasticsearch/lib/lucene-analyzers-common-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-backward-codecs-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-core-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-grouping-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-highlighter-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-join-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-memory-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-misc-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-queries-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-queryparser-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-sandbox-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-spatial-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-spatial-extras-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-spatial3d-6.0.0.jar
/usr/share/elasticsearch/lib/lucene-suggest-6.0.0.jar
/usr/share/elasticsearch/lib/netty-3.10.5.Final.jar
/usr/share/elasticsearch/lib/securesm-1.0.jar
/usr/share/elasticsearch/lib/spatial4j-0.6.jar
/usr/share/elasticsearch/lib/t-digest-3.0.jar
/usr/share/elasticsearch/modules/ingest-grok/ingest-grok-5.0.0-alpha2-SNAPSHOT.jar
/usr/share/elasticsearch/modules/ingest-grok/jcodings-1.0.12.jar
/usr/share/elasticsearch/modules/ingest-grok/joni-2.1.6.jar
/usr/share/elasticsearch/modules/ingest-grok/plugin-descriptor.properties
/usr/share/elasticsearch/modules/lang-expression/antlr4-runtime-4.5.1-1.jar
/usr/share/elasticsearch/modules/lang-expression/asm-5.0.4.jar
/usr/share/elasticsearch/modules/lang-expression/asm-commons-5.0.4.jar
/usr/share/elasticsearch/modules/lang-expression/asm-tree-5.0.4.jar
/usr/share/elasticsearch/modules/lang-expression/lang-expression-5.0.0-alpha2-SNAPSHOT.jar
/usr/share/elasticsearch/modules/lang-expression/lucene-expressions-6.0.0.jar
/usr/share/elasticsearch/modules/lang-expression/plugin-descriptor.properties
/usr/share/elasticsearch/modules/lang-expression/plugin-security.policy
/usr/share/elasticsearch/modules/lang-groovy/groovy-2.4.6-indy.jar
/usr/share/elasticsearch/modules/lang-groovy/lang-groovy-5.0.0-alpha2-SNAPSHOT.jar
/usr/share/elasticsearch/modules/lang-groovy/plugin-descriptor.properties
/usr/share/elasticsearch/modules/lang-groovy/plugin-security.policy
/usr/share/elasticsearch/modules/lang-mustache/compiler-0.9.1.jar
/usr/share/elasticsearch/modules/lang-mustache/lang-mustache-5.0.0-alpha2-SNAPSHOT.jar
/usr/share/elasticsearch/modules/lang-mustache/plugin-descriptor.properties
/usr/share/elasticsearch/modules/lang-mustache/plugin-security.policy
/usr/share/elasticsearch/modules/lang-painless/antlr4-runtime-4.5.1-1.jar
/usr/share/elasticsearch/modules/lang-painless/asm-5.0.4.jar
/usr/share/elasticsearch/modules/lang-painless/asm-commons-5.0.4.jar
/usr/share/elasticsearch/modules/lang-painless/asm-tree-5.0.4.jar
/usr/share/elasticsearch/modules/lang-painless/lang-painless-5.0.0-alpha2-SNAPSHOT.jar
/usr/share/elasticsearch/modules/lang-painless/plugin-descriptor.properties
/usr/share/elasticsearch/modules/lang-painless/plugin-security.policy
/usr/share/elasticsearch/modules/reindex/plugin-descriptor.properties
/usr/share/elasticsearch/modules/reindex/reindex-5.0.0-alpha2-SNAPSHOT.jar
/usr/share/elasticsearch/plugins
/var/lib/elasticsearch
/var/log/elasticsearch
/var/run/elasticsearch
```

closes #18162
</description><key id="153329551">18170</key><summary>Packaging: Make rpm not include parent dirs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Packaging</label><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T21:24:00Z</created><updated>2016-05-05T23:13:52Z</updated><resolved>2016-05-05T23:13:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T23:07:17Z" id="217305519">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add fielddata accessors (.value/.values/.distance()/etc)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18169</link><project id="" key="" /><description>Currently fields in painless are recommended to be accessed as `input.doc['field'].0`.

Instead we should work closer to the current syntax `input.doc['field'].value`. To do this, we have to add ScriptDocValues to the whitelist so we can see its `getValue()` method.

This has other benefits, it means we expose more of the scripting api (see https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-groovy.html#_doc_value_properties_and_methods)

So `input.doc['field_name'].lat` and `doc['field_name'].arcDistance(34, 56)` work too and so on, and geo fields become usable.

I TODO'd whitelisting any joda-time stuff to be a separate change.

Field loads for this common `.value` case with some temporary cheating. This is a 467% performance improvement for scripts accessing document fields from painless, so I think we should do it for now. We can do fancy stuff later and remove this, but it means at least common use cases are much faster.

I cutover the integration tests and docs to use this syntax.
</description><key id="153327907">18169</key><summary>Add fielddata accessors (.value/.values/.distance()/etc)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T21:15:20Z</created><updated>2016-05-17T12:23:53Z</updated><resolved>2016-05-05T22:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-05T21:26:14Z" id="217284855">Looks great!  Just one comment.
</comment><comment author="rjernst" created="2016-05-05T21:35:35Z" id="217287005">LGMT, awesome speedup!
</comment><comment author="rmuir" created="2016-05-05T22:04:42Z" id="217293256">@jdconrad i cut these over to `&lt;Def&gt;`
</comment><comment author="jdconrad" created="2016-05-05T22:26:26Z" id="217297721">LGTM.  Thanks!
</comment><comment author="rmuir" created="2016-05-05T22:30:18Z" id="217298471">thanks for reviewing. there is a lot to followup with for this one, I will take care of it.
</comment><comment author="clintongormley" created="2016-05-06T08:48:45Z" id="217387362">@rmuir should i move these properties/methods (https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-groovy.html#_doc_value_properties_and_methods) back to the general scripting section instead of leaving them in groovy? or perhaps add them to the painless docs?
</comment><comment author="rmuir" created="2016-05-06T10:46:30Z" id="217409451">@clintongormley I don't know what to do here!

These properties/methods, lets call it "the scripting api", is really exposed to all script engines via Java. So today groovy, javascript, python, and painless support it.

Expressions basically exposes what looks like a subset of this API to users, but its a lie: it in fact bypasses the whole thing entirely, and the expressions API is inconsistent in random annoying ways: e.g. `doc['field'].count()` vs `doc['field'].size()`. So not really a proper subset :(

On one hand, I think painless needs to support this API, to get people off of groovy. On the other hand, I'm not sure if we want to declare it front-and-center "the scripting api", because its not very good. I don't mean this in an offensive way, the API is both slow and bad, its just a fact. Expressions is 2x faster than anything else because it bypasses it completely.

One reason it is cumbersome and slow is that it exposes a document's fields as always being multivalued lists, but its not clear to me user's even care about multi-valuedness whatsoever:  (https://github.com/elastic/elasticsearch/commit/4ddf916aabaf6e60e68f0cf59ef5dbc3c17cffed)

There are also tons and tons of geo functions, but many of these look very obscure. I think its ok to continue supporting them, to not break anyones scripts, but do we really have to document all the obscure stuff? 

I think it makes things overwhelming. An easy win might be to try to drop some of these obscure methods from the documentation. I think all the multi-valued methods (e.g. `.values`, `.lats`, `.lons`) should probably be dropped from the documentation too. It would be better to instead have a footnote at the bottom that says "each field is really a list so to get access to the other values you can use .get(1)/.size()/etc"
</comment><comment author="jdconrad" created="2016-05-06T16:34:44Z" id="217492366">I really like the proposal to drop the more obscure stuff other than as a footnote.  On the differences in API between Expressions and others -- maybe we can deprecate the ones that don't match and add ones that do match?
</comment><comment author="s1monw" created="2016-05-09T08:03:36Z" id="217800471">&gt; On one hand, I think painless needs to support this API, to get people off of groovy. On the other hand, I'm not sure if we want to declare it front-and-center "the scripting api", because its not very good. I don't mean this in an offensive way, the API is both slow and bad, its just a fact. Expressions is 2x faster than anything else because it bypasses it completely.

I do understand that we need to provide an API which is similar in it's functionality but putting ourself into the position of building painless on top of an already known problem sounds weird to me. I think we can build a new API that allows for good performance based on a subset of the current API and deprecate the current one. All other scripting languages can potentially have access to both such that folks can move gradually but we can build a new and fast API? Does this make sense and / or is this feasible at all?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in message for variable setup ES_MAX_MEM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18168</link><project id="" key="" /><description>Small typo fix in startup script.
</description><key id="153318005">18168</key><summary>Fix typo in message for variable setup ES_MAX_MEM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">radoondas</reporter><labels><label>:Packaging</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T20:27:54Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-05-05T23:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-05T20:42:12Z" id="217272330">LGTM
</comment><comment author="jasontedor" created="2016-05-05T23:17:12Z" id="217307122">Thanks. The Windows startup script had half of this copy-paste error, so I pushed f1fb6a37c0e348ef0898782d58acf989f2b5c9be.

I'm marking you as eligible for the [Pioneer Program](https://www.elastic.co/blog/elastic-pioneer-program).
</comment><comment author="radoondas" created="2016-05-06T07:20:43Z" id="217368985">@jasontedor Thanks! Thah made my day. :)
</comment><comment author="nik9000" created="2016-05-06T11:31:23Z" id="217417047">&gt; @jasontedor Thanks! Thah made my day. :)

Thanks for fixing it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the Snapshot class in favor of using SnapshotInfo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18167</link><project id="" key="" /><description>o/e/snapshots/Snapshot and o/e/snapshots/SnapshotInfo contain the same
fields and represent the same information.  Snapshot was used to
maintain snapshot information to the snapshot repository, while
SnapshotInfo was used to represent the snapshot information as presented
through the REST layer.  This removes the Snapshot class and combines
all uses into the SnapshotInfo class.

Relates to #18156 
</description><key id="153315886">18167</key><summary>Remove the Snapshot class in favor of using SnapshotInfo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T20:17:01Z</created><updated>2016-05-05T20:58:51Z</updated><resolved>2016-05-05T20:58:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-05T20:21:44Z" id="217266959">The same duality exists for tasks too. `Task` and `TaskInfo`. The difference there is that `Task` isn't `Writeable` or `ToXContent` but `TaskInfo` is. `TaskInfo` as a snapshot of a task. It works really well! I'm not sure if that was the intent here though. I'll review though.
</comment><comment author="abeyad" created="2016-05-05T20:28:17Z" id="217268600">@nik9000 The thought for merging `Snapshot` with `SnapshotInfo` came about because @ywelsch and I were talking about naming `SnapshotId` and it got to the point where we wanted to rename the current `SnapshotId` to `Snapshot` and make `SnapshotId` composed of `Snapshot` plus a uuid.  At that point, I noticed that `Snapshot` and `SnapshotInfo` represent the same exact info, the only difference really is that Snapshot's `toXContent` differed a bit (what was written to the repository) than `SnapshotInfo`, where the x-content is a bit more verbose.  I thought it was easier to solve this by merging the two and just having two different methods for x-content (the verbose and the non-verbose one).  We had a few places too where we would get a `Snapshot`, only to convert it to a `SnapshotInfo` for the APIs... this cleans up all that and the creation of unnecessary objects.

I can see the reverse point - that the two uses have different intents, but since I felt the intents only differed in how they are serialized for particular use cases, I went with this.

I'd appreciate your thoughts, if you think this makes sense
</comment><comment author="nik9000" created="2016-05-05T20:37:47Z" id="217271111">I left some stylistic comments. I had a quick look around and I agree with you - these two classes look to be the same information just used in different contexts. I like the idea of throwing one away.

I don't think I need to do another review - either make the changes I asked for or reply explaining why they aren't a good idea so I'll learn something and merge when you ready. LGTM.
</comment><comment author="nik9000" created="2016-05-05T20:41:29Z" id="217272142">&gt; I'd appreciate your thoughts,

I think that having the two classes makes sense for tasks because it is clear how they are different. They have subclasses that make them wildly different - compare `BulkByScrollTask` and `BulkByScrollTask.Status`. They are obviously different.

In this case these things are pretty much the same so I like merging them.

Also in my previous comment I wrote `TaskInfo` when I should have written `TaskStatus`. `TaskStatus` and `Task` are the dual classes.
</comment><comment author="abeyad" created="2016-05-05T20:42:52Z" id="217272498">@nik9000 great, thank you for the review!  I implemented all the changes you mentioned.  The only one that is outstanding is that `SnapshotInfo` still implements `FromXContentBuilder`, which isn't ideal, but the `ChecksumBlobStoreFormat` depends on it. 
</comment><comment author="nik9000" created="2016-05-05T20:44:25Z" id="217272899">&gt; but the ChecksumBlobStoreFormat depends on it.

I can live with that! I'll take incremental progress over being bogged down in changes forever any day!
</comment><comment author="abeyad" created="2016-05-05T20:46:46Z" id="217273508">@nik9000 great, thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] org.elasticsearch.index.query.InnerHitBuilderTests.testFromAndToXContent failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18166</link><project id="" key="" /><description>This asserts that the inner hits is equal to the parsed inner hits:

```
assertThat(innerHit, equalTo(secondInnerHits));
```

Failure here: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=debian/360/consoleText

It does reproduce with:

```
gradle :core:test -Dtests.seed=D38C14AF123CA974 -Dtests.class=org.elasticsearch.index.query.InnerHitBuilderTests -Dtests.method="testFromAndToXContent" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=lv-LV -Dtests.timezone=Australia/Currie
```
</description><key id="153314318">18166</key><summary>[CI] org.elasticsearch.index.query.InnerHitBuilderTests.testFromAndToXContent failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2016-05-05T20:08:41Z</created><updated>2016-05-06T13:01:39Z</updated><resolved>2016-05-06T13:01:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-06T00:18:48Z" id="217316324">I marked this test as awaits fix in 1199cd8e2a1343ba74f208c96d6798ab0da4bdfd.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] org.elasticsearch.smoketest.SmokeTestDocsIT failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18165</link><project id="" key="" /><description>```
==&gt; Test Info: seed=56269F69F703EF6B; jvm=1; suite=1
Suite: org.elasticsearch.smoketest.SmokeTestDocsIT
  1&gt; [2016-05-05 13:52:54,519][ERROR][org.elasticsearch.smoketest] This failing test was generated by documentation starting at analyzers/pattern-analyzer.asciidoc:91. It may include many snippets. See docs/README.asciidoc for an explanation of test generation.
  2&gt; REPRODUCE WITH: gradle :docs:integTest -Dtests.seed=56269F69F703EF6B -Dtests.class=org.elasticsearch.smoketest.SmokeTestDocsIT -Dtests.method="test {yaml=analyzers/pattern-analyzer/91}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.jvm.argline="-XX:-UseParNewGC -XX:-UseConcMarkSweepGC -XX:+UseG1GC" -Dtests.locale=ar-SD -Dtests.timezone=America/Halifax
FAILURE 0.85s | SmokeTestDocsIT.test {yaml=analyzers/pattern-analyzer/91} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [raw] returned [503 Service Unavailable] [{"error":{"root_cause":[{"type":"no_shard_available_action_exception","reason":"No shard available for [org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@3dc5f582]"}],"type":"no_shard_available_action_exception","reason":"No shard available for [org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@3dc5f582]"},"status":503}]
   &gt;    at __randomizedtesting.SeedInfo.seed([56269F69F703EF6B:DE72A0B359FF8293]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:107)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:395)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/hinmanm/es/elasticsearch/docs/build/testrun/integTest/J0/temp/org.elasticsearch.smoketest.SmokeTestDocsIT_56269F69F703EF6B-001
  2&gt; NOTE: test params are: codec=Lucene60, sim=ClassicSimilarity, locale=ar-SD, timezone=America/Halifax
  2&gt; NOTE: Linux 4.4.8-300.fc23.x86_64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=4,threads=1,free=443735048,total=536870912
  2&gt; NOTE: All tests run in this JVM: [SmokeTestDocsIT]
Completed [1/1] in 1.30s, 1 test, 1 failure &lt;&lt;&lt; FAILURES!
```

Looks like the documentation needs to be corrected. This reproduces with:

```
gradle :docs:integTest -Dtests.seed=56269F69F703EF6B -Dtests.class=org.elasticsearch.smoketest.SmokeTestDocsIT -Dtests.method="test {yaml=analyzers/pattern-analyzer/91}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.jvm.argline="-XX:-UseParNewGC -XX:-UseConcMarkSweepGC -XX:+UseG1GC" -Dtests.locale=ar-SD -Dtests.timezone=America/Halifax
```
</description><key id="153311477">18165</key><summary>[CI] org.elasticsearch.smoketest.SmokeTestDocsIT failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Analysis</label><label>build</label><label>docs</label><label>test</label></labels><created>2016-05-05T19:54:57Z</created><updated>2016-05-05T20:02:33Z</updated><resolved>2016-05-05T20:02:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-05T19:56:03Z" id="217260239">I'll have a look at it. It hadn't happened for me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>groovy scripting docs are buggy for 'multiValued'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18164</link><project id="" key="" /><description>The documentation (https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-groovy.html#_doc_value_properties_and_methods) states that you can do this:

```
doc['field_name'].multiValued
    A boolean indicating that the field has several values within the corpus. 
```

This does not work though, we don't have any actual code to support it. I don't think we need it either, since someone could just look at `doc['field_name'].size() &gt; 1` or a number of other ways, but we should fix the docs.
</description><key id="153307350">18164</key><summary>groovy scripting docs are buggy for 'multiValued'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>docs</label></labels><created>2016-05-05T19:35:29Z</created><updated>2016-05-06T08:40:52Z</updated><resolved>2016-05-06T08:40:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>_cat/fielddata reports node IP address instead of fielddata size of a field called "ip".</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18163</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 

&gt; [gsmith@crowley ~ ] &gt; java -version
&gt; java version "1.8.0_45"
&gt; Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
&gt; Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)

**OS version**: OS X 10.10.5

**Description of the problem including expected versus actual behavior**:

Response to `_cat/fielddata` when there is an actual "ip" field in the indexed data reports the IP address of the node in the column where the size of fielddata used by the "ip" field should appear.

**Steps to reproduce**:

```
#0. Delete the index
DELETE test_field_data
#1. Create the index with a document:
POST test_field_data/test_field_data
{"reason": "something is off", "ip": "192.168.1.0"}
#2. Flush
POST _flush
#3. Perform a search that loads fielddata
GET test_field_data/_search
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "ip": {
      "terms": {
        "field": "ip",
        "size": 10
      }
    },
    "reason": {
      "terms": {
        "field": "reason",
        "size": 10
      }
    }
  }
}
#4. Inspect the fielddata
GET _cat/fielddata/ip,reason?v
```

The return looks like:

```
id                     host      ip        node total reason ip        
FNsjPCVATN-xsg7O0vxIuQ 127.0.0.1 127.0.0.1 only 2.8kb   488b 127.0.0.1 
```

I have also seen a response from a 3 node cluster, also v 2.3.1, where two of the nodes are reported as shown above, and the third node actually has the fielddata usage reported, both in the correct place and in the column where the node ip address is supposed to appear.
</description><key id="153306047">18163</key><summary>_cat/fielddata reports node IP address instead of fielddata size of a field called "ip".</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GlenRSmith</reporter><labels><label>:CAT API</label><label>bug</label></labels><created>2016-05-05T19:28:22Z</created><updated>2016-05-06T08:38:20Z</updated><resolved>2016-05-06T08:38:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-06T08:38:20Z" id="217383906">Duplicate of #10249
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM package declares /etc/sysconfig and /var/run</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18162</link><project id="" key="" /><description>The RPM package for Elasticsearch is declaring `/etc/sysconfig` and `/var/run`. It does not need to because there are already going to be in place on any system where the RPM is relevant, and it can conflict with packages like the `filesystem` package for Amazon Linux that build the filesystem hierarchy there.
</description><key id="153294010">18162</key><summary>RPM package declares /etc/sysconfig and /var/run</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>adoptme</label><label>blocker</label><label>bug</label></labels><created>2016-05-05T18:32:21Z</created><updated>2016-05-06T08:50:10Z</updated><resolved>2016-05-05T23:13:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>possible memory leak index query cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18161</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.2 and 2.3.2

**JVM version**: 1.8_65 and 1.8_92

**OS version**: centos 7 (kernel 3.10.0-327.13.1.el7.x86_64)

**Description of the problem including expected versus actual behavior**:
this is a testing system, writing has been disabled, and only search is working, here is the environment:
- 10 indices, with the total of 156 shards (75 being primaries), and the total size of 6.4TB
- the cluster has 4 servers, with 64GB of ram each, and 30G allocated to elasticsearch (one instance per server)
- one index is 2.1tb (4.4tb total, with one replica), and created with 30 shards
- using the default gc tuning from elasticsearch
- number of segments: 4.7k (about 1.2k on each node)

configuration:

```
cluster.name: es_testing
#
# ------------------------------------ Node ------------------------------------
#
node.name: hostname1
node.max_local_storage_nodes: 1
#
# ----------------------------------- Paths ------------------------------------
#
path.conf: /etc/elasticsearch
path.data: /u1/elasticsearch,/u2/elasticsearch,/u3/elasticsearch,/u4/elasticsearch,/u5/elasticsearch
path.logs: /var/log/elasticsearch
#
# ----------------------------------- Memory -----------------------------------
#
bootstrap.mlockall: true
#
# ---------------------------------- Network -----------------------------------
#
network.host: 0.0.0.0
http.port: 9200
#
# ---------------------------------- Gateway -----------------------------------
#
#
# --------------------------------- Discovery ----------------------------------
#
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: hostname1,hostname2,hostname3,hostname4
discovery.zen.minimum_master_nodes: 2
#
# ---------------------------------- Various -----------------------------------
#
action.auto_create_index: true
action.destructive_requires_name: true
#
# -------------------------- Custom Chef Configuration --------------------------
#
action.disable_delete_all_indices: true
gateway.expected_nodes: 1
index.indexing.slowlog.threshold.index.debug: 2s
index.indexing.slowlog.threshold.index.info: 5s
index.indexing.slowlog.threshold.index.trace: 500ms
index.indexing.slowlog.threshold.index.warn: 10s
index.mapper.dynamic: false
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.trace: 200ms
index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.trace: 500ms
index.search.slowlog.threshold.query.warn: 10s
indices.breaker.fielddata.limit: 20%
indices.breaker.request.limit: 20%
indices.breaker.total.limit: 20%
indices.fielddata.cache.size: 10%
monitor.jvm.gc.old.debug: 2s
monitor.jvm.gc.old.info: 5s
monitor.jvm.gc.old.warn: 10s
monitor.jvm.gc.young.debug: 400ms
monitor.jvm.gc.young.info: 700ms
monitor.jvm.gc.young.warn: 1000ms
network.publish_host: _site_
script.engine.groovy.inline.aggs: true
script.engine.groovy.inline.mapping: false
script.engine.groovy.inline.plugin: false
script.engine.groovy.inline.search: true
script.engine.groovy.inline.update: false
script.groovy.sandbox.receiver_whitelist: "java.lang.String,java.lang.Object,java.lang.Math"
threadpool.search.size: 1000
```

**Steps to reproduce**:
the cluster stays fine without any queries (heap around 7gb on 2.3.2, and 4gb on 2.2.2). Once we start sending queries (200-300 reqs/s), the cluster eats up the heap, and after the oldgen gc starts to run, it never frees enough memory.

after a couple hours, the cluster becomes unresponsive and a restart is required.

**Provide logs (if relevant)**:
two memory dumps were taken, and both reported the same suspects, here is one taken from one of the dump:
- more than 5000k instances of org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader, using a total of 15gb
- 990 instances of org.apache.lucene.index.SegmentCoreReaders, using a total of 6gb
- 1mil instaces of HashMap, using 6gb as well

attached memory reports
</description><key id="153291644">18161</key><summary>possible memory leak index query cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wfelipe</reporter><labels><label>:Cache</label><label>:Core</label><label>feedback_needed</label></labels><created>2016-05-05T18:20:56Z</created><updated>2016-05-07T15:20:39Z</updated><resolved>2016-05-07T15:20:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wfelipe" created="2016-05-05T18:22:44Z" id="217233107">forgot to attach the images, here it goes:

&lt;img width="798" alt="screen shot 2016-05-05 at 10 41 14 am" src="https://cloud.githubusercontent.com/assets/77432/15052662/9641534a-12b3-11e6-91d1-0ae4fb4bcf59.png"&gt;
&lt;img width="630" alt="screen shot 2016-05-05 at 9 53 04 am" src="https://cloud.githubusercontent.com/assets/77432/15052665/9a9cb95c-12b3-11e6-9ae0-1a55b0e66e77.png"&gt;
&lt;img width="631" alt="screen shot 2016-05-05 at 9 53 09 am" src="https://cloud.githubusercontent.com/assets/77432/15052675/a7dbc6d0-12b3-11e6-8112-3f88983ee5cb.png"&gt;
&lt;img width="628" alt="screen shot 2016-05-05 at 9 54 15 am" src="https://cloud.githubusercontent.com/assets/77432/15052680/aaca6dba-12b3-11e6-9bcf-e64f949a102c.png"&gt;
</comment><comment author="clintongormley" created="2016-05-06T08:36:14Z" id="217383582">What happens when you remove the ridiculously high search thread pool? 

```
threadpool.search.size: 1000
```
</comment><comment author="wfelipe" created="2016-05-06T17:55:50Z" id="217514101">we keep track of the thread pool usage, and raising it to 1000 was an attempt to see the behavior. The number of threads being used goes around 5-15. It only reaches to 100 when the heap is taken, so that's a side effect.
</comment><comment author="clintongormley" created="2016-05-07T10:50:07Z" id="217628391">@wfelipe yes, but what happens when you use the default setting for the search threadpool size, which is (number of processors \* 3)/2+1.  You don't mention how many processors you have, but just unsetting this setting will give you the default. With a high size, if search is struggling for whatever reason, then it'll just use one of the many threads that you have allowed it to use which will bring a system to its knees.  Instead, with a reasonable thread pool size, search requests will be queued or rejected, keeping the system healthy.

That's why I want to see what happens to memory usage when the threads setting is the default.
</comment><comment author="jpountz" created="2016-05-07T15:20:39Z" id="217643788">Everything you are describing is a side effect of having too many threads \* segments per node. Lucene keeps state in a thread local per segment, which is why you are seeing so many instances of SegmentCodeReaders and CompressingStoredFieldsReader. You should try to reduce the size of the search/get pools and have fewer (larger) segments per node.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert all the docs to // CONSOLE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18160</link><project id="" key="" /><description>**Describe the feature**:
#18075 will merge soon, testing all the `// CONSOLE` snippets in our docs. But lots of our docs _aren't_ annotated with it! We should annotate more snippets so we test more of the docs.

Edit: `// CONSOLE` has replaced `// AUTOSENSE`. The sense application is named console in 5.0.0.
</description><key id="153291075">18160</key><summary>Convert all the docs to // CONSOLE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v6.0.0</label></labels><created>2016-05-05T18:18:03Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-05T18:25:58Z" id="217233937">Running `gradle listConsoleCandidates` will spit out a list of snippets that maybe should be converted to `// AUTOSENSE`. I imagine it has a very very high false positive rate. If someone is so included, they can run it and convert what they can.

If we can get the false positive rate down to 0 then we can fail the build if this finds any snippets. _if_.
</comment><comment author="geekpete" created="2016-05-06T06:01:43Z" id="217354355">Hi nik9000, do you have any easy guide on how to assist with converting?
</comment><comment author="nik9000" created="2016-05-06T11:29:31Z" id="217416815">The simplest thing to convert to `// AUTOSENSE` is `curl` commands because they are already complete commands.
1. Remove the `curl -X` part. Sense doesn't need it.
2. Remove the `localhost:9200/` part. Sense has the port and host configured.
3. Remove the `-d'{` and put the opening `{` on the next line. Sense likes it there and the `-d'` is implied.
4. Remove the closing `'`.
5. If the source tag at the top of the snippet is `[source,sh]` or something switch it to `[source,js]`.

Now that should just work with `// AUTOSENSE`. You can read `docs/README.md` for any of the extra test markup. Stuff like `// TESTRESPONSE` for matching the response and `// TEST[continued]` for continuing on from the last snippet.

You can test your changes by running `gradle :docs:check`. The first time you do it will take a while, just like any java build. The list of things it has to do before it can start and skips if they are already done is immense. From downloading dependencies to compiling Elasticsearch to transforming the `.asciidoc` files into tests.

There are places where we didn't write the whole API call but I think those are easier once you know the sense syntax.
</comment><comment author="clintongormley" created="2016-05-06T11:33:25Z" id="217417334">```
* What went wrong:
Execution failed for task ':docs:buildRestTests'.
&gt; Path shouldn't start with a '/': reference/search.asciidoc[20:27](js)// AUTOSENSE
  POST /twitter/tweet?routing=kimchy
  {
    "user": "kimchy",
    "postDate": "2009-11-15T14:12:12",
    "message": "trying out Elasticsearch"
  }
```

It's perfectly fine for the path to start with a `/`.  In fact, I prefer it because it looks like a real path
</comment><comment author="clintongormley" created="2016-05-06T11:37:03Z" id="217417842">&gt; Remove the curl -X part. Sense doesn't need it.
&gt; Remove the localhost:9200/ part. Sense has the port and host configured.
&gt; Remove the -d'{ and put the opening { on the next line. Sense likes it there and the -d' is implied.
&gt; Remove the closing '.
&gt; If the source tag at the top of the snippet is [source,sh] or something switch it to [source,js].

Or paste it into Sense, which should convert it to Sense syntax.  Then press Ctrl-I or Cmd-I to reformat.
</comment><comment author="nik9000" created="2016-05-06T11:42:20Z" id="217418571">&gt; It's perfectly fine for the path to start with a /. In fact, I prefer it because it looks like a real path

I didn't like their being two ways to do it and it looked like more than half of the docs did it sans-`/` so I picked that way. No one seemed to complain but maybe no one noticed?

If you really want to have the inconsistency back I can support that too.

&gt; Or paste it into Sense, which should convert it to Sense syntax. Then press Ctrl-I or Cmd-I to reformat.

The great irony of this is that I never use sense. I tried this morning and, of course, it didn't like that I was running on Elasticsearch's master branch and I hadn't built it from source. I don't have the patience nor the skill with javascript to run that from master too, I think.
</comment><comment author="clintongormley" created="2016-05-06T11:46:44Z" id="217419221">&gt; If you really want to have the inconsistency back I can support that too.

If anything, I'd prefer the other way.  But I'm OK with the inconsistency
</comment><comment author="nik9000" created="2016-05-06T13:11:23Z" id="217435659">@clintongormley I created #18184 for it.
</comment><comment author="MaineC" created="2016-05-09T09:40:40Z" id="217820189">@nik9000 How deep do we want those tests to be? TESTRESPONSE seems to be present only in a couple of documents.

Concrete example: For the query dsl docs - should there be more than just the AUTOSENSE annotation to include them in the test run so parse failures are caught?

Another questions: Should we/ do we barf in case there's deprecated stuff in those snippets?
</comment><comment author="nik9000" created="2016-05-09T12:38:03Z" id="217852709">&gt; @nik9000 How deep do we want those tests to be? TESTRESPONSE seems to be present only in a couple of documents.

I don't know. Just trying to run the query at all is a huge improvement over what we have. As a first pass I'd just `// AUTOSENSE`ify all of them.

If we want to go deeper we can use the `// TEST[setup:name_of_data_set]` markup to reference a dataset defined in `docs/build.gradle` and have that run before the snippet. Then you can make a `// TESTRESPONSE`.

In my first pass I was just trying to convert what we had, inventing syntax along the way. I just tried to pick up all the `// AUTOSENSE` stuff and do sensible things with the snippets that looked like responses. I don't know what the final state should be.

&gt; Another questions: Should we/ do we barf in case there's deprecated stuff in those snippets?

Yes we should. No we don't.

I don't know that Elasticsearch has a parameter to reject deprecated stuff. If not we should invent a parameter and make a big deal out of it because it seems really useful. And all the snippets should send that parameter by default. I think some snippets will talk about deprecated syntax and we can make `// TEST[deprecated]` or something to allow the snippet to send deprecated syntax.
</comment><comment author="MaineC" created="2016-05-09T12:52:46Z" id="217855749">&gt; I don't know. Just trying to run the query at all is a huge improvement over what we have. As a 
&gt; first pass I'd just // AUTOSENSEify all of them.

That's what I'm currently doing for the query-dsl docs (went through them about half a year ago already when switching the parsing code to ParseField, the more complicated json snippets ended up as *fromJSON tests in the *QueryBuilder Java tests back then, so I should recognize most of the issues I run into while going along)

&gt; I don't know that Elasticsearch has a parameter to reject deprecated stuff.

For parsing queries through ParseField there is an option called "index.query.parse.strict" that will fail to parse deprecated json parameters with an exception.
</comment><comment author="nik9000" created="2016-05-09T12:56:27Z" id="217856546">&gt; For parsing queries through ParseField there is an option called "index.query.parse.strict" that will fail to parse deprecated json parameters with an exception.

Cool! Is that at the index level? I'd love something at the request level for this though I haven't looked at how hard it'd be to implement. And it'd be great if it were global including stuff like detecting deprecated mappings and stuff. It is the kind of thing we could have a lively discussion with @clintongormley about, I think.
</comment><comment author="MaineC" created="2016-05-09T13:04:55Z" id="217858425">#17512 is related to this setting.
</comment><comment author="nik9000" created="2016-05-09T13:07:33Z" id="217859014">Lol, I should remember my own issues.
</comment><comment author="nik9000" created="2016-05-16T13:28:13Z" id="219424500">@clintongormley, you mentioned to me that you planned to rewrite some areas of the docs soon and would get to this as part of the rewrite. Like #18356. Anyway, do you have a list of places you plan to rewrite or a list of places you know you won't rewrite?
</comment><comment author="clintongormley" created="2016-05-17T08:29:17Z" id="219652819">&gt; Anyway, do you have a list of places you plan to rewrite or a list of places you know you won't rewrite?

Mappings I won't rewrite (or only a little).  Nor cluster and index settings.  Analysis is in progress.  That's about it - other docs I'll redo as I have the time
</comment><comment author="MaineC" created="2016-05-17T10:50:45Z" id="219683716">Am I the only one having trouble to run `gradle :docs:check` even on master at the moment?

Running into a 

```
==&gt; Test Info: seed=FA5BECA4868DAF99; jvm=1; suite=1
Suite: org.elasticsearch.smoketest.SmokeTestDocsIT
ERROR   0.01s | SmokeTestDocsIT.initializationError &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.IllegalArgumentException: Negative position
   &gt;    at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:717)
   &gt;    at org.elasticsearch.test.rest.parser.RestTestSuiteParser.parse(RestTestSuiteParser.java:54)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.collectTestCandidates(ESRestTestCase.java:171)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.createParameters(ESRestTestCase.java:148)
   &gt;    at org.elasticsearch.smoketest.SmokeTestDocsIT.parameters(SmokeTestDocsIT.java:40)
   &gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
Completed [1/1] in 0.01s, 1 test, 1 error &lt;&lt;&lt; FAILURES!
```
</comment><comment author="nik9000" created="2016-05-17T11:56:43Z" id="219696315">Clint had that a while back and needed to `gradle docs:clean` though I
don't know what caused it.
On May 17, 2016 6:50 AM, "Isabel Drost-Fromm" notifications@github.com
wrote:

&gt; Am I the only one having trouble to run gradle :docs:check even on master
&gt; at the moment?
&gt; 
&gt; Running into a
&gt; 
&gt; ==&gt; Test Info: seed=FA5BECA4868DAF99; jvm=1; suite=1
&gt; Suite: org.elasticsearch.smoketest.SmokeTestDocsIT
&gt; ERROR   0.01s | SmokeTestDocsIT.initializationError &lt;&lt;&lt; FAILURES!
&gt; 
&gt; &gt; Throwable #1: java.lang.IllegalArgumentException: Negative position
&gt; &gt;    at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:717)
&gt; &gt;    at org.elasticsearch.test.rest.parser.RestTestSuiteParser.parse(RestTestSuiteParser.java:54)
&gt; &gt;    at org.elasticsearch.test.rest.ESRestTestCase.collectTestCandidates(ESRestTestCase.java:171)
&gt; &gt;    at org.elasticsearch.test.rest.ESRestTestCase.createParameters(ESRestTestCase.java:148)
&gt; &gt;    at org.elasticsearch.smoketest.SmokeTestDocsIT.parameters(SmokeTestDocsIT.java:40)
&gt; &gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
&gt; &gt; Completed [1/1] in 0.01s, 1 test, 1 error &lt;&lt;&lt; FAILURES!
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18160#issuecomment-219683716
</comment><comment author="MaineC" created="2016-05-17T16:58:52Z" id="219783240">That did the trick for me. Thanks.

Another question: Is it possible to limit test execution to just one doc file? That might speed up finding typos a bit when converting doc files with several snippets which look easy enough to not require trying each one of them separately.
</comment><comment author="nik9000" created="2016-05-18T02:12:42Z" id="219906932">&gt; Is it possible to limit test execution to just one doc file

`-Dtests.method="*file_name*"` is what I've been doing.
</comment><comment author="nik9000" created="2016-08-17T13:38:35Z" id="240413378">I just checked. We currently run 553 docs tests and we have about 970 snippets in the docs that `gradle docs:listConsoleCandidates` thinks should be converted. I'm fairly sure it is over counting though I don't know by how much. We have more than a third but less than half of the snippets tested at this point.
</comment><comment author="nik9000" created="2016-09-21T15:13:53Z" id="248643394">Removing the `v5.0.0` label because this isn't going to make 5.0.0. There are ~730 unconverted snippets.
</comment><comment author="geekpete" created="2016-09-21T22:05:16Z" id="248756813">If we went totally crazy on it, do you rekon we could smash through those snippets?
</comment><comment author="nik9000" created="2016-09-21T22:17:35Z" id="248759343">I'm not sure really. I know a pile of the snippets come from aggregations and for those it really helps to have the right set of data. I did a bunch of the pipeline aggregations ones and made a little data set in `docs/build.gradle` that matched well and everything worked out well.

I reckon we could split them up and say like "you do the `_cat` snippets, you do the aggregation snippets, you do the query snippets, etc" and after a couple of days work for a few people in parallel we could knock them out.

I try to knock out a few every few days, but at that rate it is going to take a while.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest tests need to clean up snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18159</link><project id="" key="" /><description>**Elasticsearch version**:
5.0.0

**Description of the problem including expected versus actual behavior**:
Right now the rest tests wipe snapshots by deleting all of the repositories. This doesn't really work because if you recreate a repository with the same name it'll still contain the snapshots. The tests should actually wipe all of the snapshots.
</description><key id="153279759">18159</key><summary>Rest tests need to clean up snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v5.0.0-beta1</label></labels><created>2016-05-05T17:22:50Z</created><updated>2016-09-17T09:02:50Z</updated><resolved>2016-09-15T19:46:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-05T17:23:48Z" id="217216676">I've started poking at this but not really got all the way to fixing it. I'll come back to it in a few days if no one else wants it first.
</comment><comment author="javanna" created="2016-09-14T15:47:57Z" id="247058551">@nik9000 this is still valid right?
</comment><comment author="nik9000" created="2016-09-14T15:52:16Z" id="247059985">I _think_ so. I just forgot about it. I can pick it up again soon though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm -U deletes elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18158</link><project id="" key="" /><description>**Elasticsearch version**:
5.0 Alpha 2
**JVM version**:
openjdk version "1.8.0_91"
**OS version**:
Centos 7
**Description of the problem including expected versus actual behavior**:
inplace upgrade using 64bit rpm (e.g. rpm -U elasticsearch-5.0.0-alpha2.rpm) overwrites the elasticsearch.yml file . Expect it to create an rpmnew file. 
**Steps to reproduce**:
 1.install alpha 1
 2.edit elasticsearch.yml
 3.upgrade with rpm -U elasticsearch-5.0.0-alpha2.rpm

**Provide logs (if relevant)**:
</description><key id="153259598">18158</key><summary>rpm -U deletes elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Martin-Logan</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-05-05T15:40:23Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-05-06T17:24:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T15:52:06Z" id="217192405">This is the expected behavior in your circumstance. When `rpm -U` is invoked, there are different scenarios. The scenarios come from there being three possible config files: the original config file distributed with alpha1, your config file with edits, and the config file that is distributed with alpha2. In this situation, you have alpha1 = x, your config = y and it turns out that alpha2 = z (there was a change to the shipped config file between alpha1 and alpha2). In this case, rpm assumes that z must be used with the new package (it can not safely assume that either x or y are safe to use with the new package). That is why it will not produce an `rpmnew` here.
</comment><comment author="Martin-Logan" created="2016-05-05T15:56:19Z" id="217193569">Would it not be better then to produce an rpmold rather than entirely lose the configuration?
</comment><comment author="jasontedor" created="2016-05-05T15:59:05Z" id="217194327">&gt; Would it not be better then to produce an rpmold rather than entirely lose the configuration?

It should have produced an `rpmsave`. Are you saying that it did not? If not, that is a bug that we should fix.
</comment><comment author="Martin-Logan" created="2016-05-05T16:01:57Z" id="217195135">It indeed did not
</comment><comment author="jasontedor" created="2016-05-05T16:04:18Z" id="217195763">&gt; It indeed did not

Yes, I just reproduced this as well. Thanks for reporting.
</comment><comment author="jasontedor" created="2016-05-06T15:40:40Z" id="217478380">@Martin-Logan I've marked you as eligible for the [Pioneer Program](https://www.elastic.co/blog/elastic-pioneer-program) and opened #18188.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify the semantics of the BlobContainer interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18157</link><project id="" key="" /><description>This commit clarifies the behavior that must be adhered to by any implementors of the BlobContainer interface.  This is done through expanded Javadocs.

Closes #15580
</description><key id="153246884">18157</key><summary>Clarify the semantics of the BlobContainer interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-05-05T14:43:33Z</created><updated>2016-05-31T23:25:45Z</updated><resolved>2016-05-31T23:25:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-05T19:16:10Z" id="217249671">@jpountz thank you for the feedback!  I made the changes, and am catching `IOException`s now
</comment><comment author="jpountz" created="2016-05-06T07:26:32Z" id="217369761">Thanks for improving it. I think @rmuir should have a look to confirm it fully addresses #15580.
</comment><comment author="abeyad" created="2016-05-23T18:04:54Z" id="221048516">@rmuir would you like to take a look at this PR with regards to #15580?
</comment><comment author="rmuir" created="2016-05-23T18:49:28Z" id="221060490">Thanks for documenting this! It is much better. I added some questions for potential followup improvements of this stuff. But the contract is now way more clear: +1
</comment><comment author="abeyad" created="2016-05-23T22:16:20Z" id="221112445">@rmuir I've pushed a new commit https://github.com/elastic/elasticsearch/pull/18157/commits/c3c593148b024ee5c3c8d5cac08932df603a6a03

Thank you for reviewing this!
</comment><comment author="abeyad" created="2016-05-24T15:01:47Z" id="221299792">@rmuir I changed `deleteBlob`'s javadocs to reflect that the contract is to throw an IOException if the blob does not exist, and this issue captures the various BlobContainer implementations that need to be fixed in order to conform: https://github.com/elastic/elasticsearch/issues/18530
</comment><comment author="abeyad" created="2016-05-27T20:18:52Z" id="222243777">@rmuir do you think any other changes are required or is the PR in good shape now? (combined with the github issues created to address the outstanding issues)
</comment><comment author="rmuir" created="2016-05-31T22:34:17Z" id="222841773">sorry i missed your ping, I was +1 8 days ago for these changes! Thanks for opening followups
</comment><comment author="abeyad" created="2016-05-31T22:36:33Z" id="222842206">Thanks @rmuir and @jpountz for your reviews!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use UUIDs in working with snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18156</link><project id="" key="" /><description>The fact that snapshots are only identified by a name has led to some issues, especially with how snapshots are represented in their underlying storage repositories.  For example, if a snapshot is deleted, but the deletion fails to delete some of the files in the repository, then a snapshot by the same name is created again, this could lead to some conflict and/or overwriting with the left over snapshot files.  This is captured in #15579 and #13159.  

In addition, snapshot names don't necessarily make good blob names for the snapshot repository.  For example, having a `:` in the snapshot name is legal, but presents an issue with accessing that snapshot in URI based repositories.  Therefore, we would have to strip those problematic characters when naming blobs and as a result, the name itself is no longer a valid way to uniquely identify each snapshot.  See issue #7540.

A solution is to introduce the notion of a UUID for each snapshot.  In this way, we can store the UUID along with the name for each snapshot, and the repository should identify, store, and retrieve snapshots using the UUID.  UUIDs will also help define the repository semantics more clearly, as discussed in #15580.  

This effort can be broken up with the following tasks, each of which will build on top of the previous tasks:
- [x] Define the contract of the `BlobContainer` API, using Javadocs.  #18157
- [x] `SnapshotInfo` and `Snapshot` represent the same data, so just merge all usages to `SnapshotInfo` and get rid of the `Snapshot` class.  This will help us in naming as well, as you will see below. #18167
- [x] Modify the notion of a `SnapshotId` so it includes a UUID.  Store the UUID along with the name in the repository's snapshot index file. #18228
- [x] When writing a new snapshot index file to the snapshot repository, ensure that it is an atomic move (similar to how the `MetaDataStateFormat` class atomically writes the metadata state to disk), and make the snapshot index file generational. #19002 
- [x] Use the snapshot index file as the source of truth for which snapshots are in the repository and valid, instead of listing snapshot blobs and the snapshot index file as back-up.  Listing the blobs in the repository as the source of truth for which snapshots are part of the ES cluster could lead to confusion if a snapshot deletion leaves behind undeleted files. #19002 
- [x] Use snapshot UUIDs to name blobs in a snapshot repository. #19421 
- [x] Blobs related to indices in the snapshot repository should also be named with a strip down version of the name plus the index UUID, to prevent the same blob naming problems as described above. #19421 
- [x] `FsBlobContainer` should pass in `StandardOpenOptions.CREATE_NEW` when opening a file output stream, so we never silently truncate/overwrite a file.  Since snapshot files are stored by name + uuid, this should not be an problem to implement any longer. #19749 
</description><key id="153245133">18156</key><summary>Use UUIDs in working with snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>Meta</label></labels><created>2016-05-05T14:35:06Z</created><updated>2016-09-17T09:27:58Z</updated><resolved>2016-08-06T05:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-08-06T05:30:21Z" id="238007029">Closing this issue as all sub-tasks have been completed and their respective PRs noted in the description.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default to server VM and add client VM check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18155</link><project id="" key="" /><description>Today we softly warn about running with the client VM. However, we
should really refuse to start in production mode if running with the
client VM as the performance of the client VM is too devastating for a
server application. This commit adds an option to jvm.options to ensure
that we are starting with the server VM (on all 32-bit non-Windows
platforms on server-class machines (2+ CPUs, 2+ GB physical RAM) this is
the default and on all 64-bit platforms this is the only option) and
adds a bootstrap check for the client VM.
</description><key id="153238052">18155</key><summary>Default to server VM and add client VM check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T13:58:55Z</created><updated>2016-05-05T14:38:46Z</updated><resolved>2016-05-05T14:36:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-05T14:29:38Z" id="217169308">LGTM
</comment><comment author="jasontedor" created="2016-05-05T14:38:41Z" id="217171535">Thanks @dakrone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_uid` should be indexed in Lucene in binary form, not base64</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18154</link><project id="" key="" /><description>@rmuir had this idea:

Today, when ES auto-generates an ID (`TimeBasedUUIDGenerator.getBase64UUID`), it uses 15 bytes, but then we immediately Base64 encode that to 20 bytes, a 33% "waste".

This is really a holdover from the past when Lucene could not index fully binary terms.

I think we should explore passing the raw binary form to Lucene instead?  We could implement back-compat based on the version as of when the index was created.
</description><key id="153209464">18154</key><summary>`_uid` should be indexed in Lucene in binary form, not base64</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>:Mapping</label><label>stalled</label><label>v6.0.0</label></labels><created>2016-05-05T10:54:42Z</created><updated>2017-07-07T12:22:47Z</updated><resolved>2017-07-07T12:22:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-05T11:40:42Z" id="217132461">Would this also make the choice for which doc values implementation(https://github.com/elastic/elasticsearch/issues/11887)  to use more obvious? 
</comment><comment author="mikemccand" created="2016-05-05T12:42:31Z" id="217142990">&gt; Would this also make the choice for which doc values implementation(#11887) to use more obvious?

Well, both sorted and binary doc values in Lucene accept full binary terms, so this change shouldn't favor one over another.
</comment><comment author="jpountz" created="2016-05-05T17:45:41Z" id="217223344">@mikemccand I'm curious why you added the discuss label. Can you foresee any potential problem if we do that?
</comment><comment author="mikemccand" created="2016-05-05T18:55:09Z" id="217242455">&gt; @mikemccand I'm curious why you added the discuss label

Well, just because this seems like a biggish change (for me!)... there are so many places in ES where we pass around `String id` now.

I would change them all to `byte[]` (I think `BytesRef` is overkill?).  But this is mutable (vs `String` today) ... is that dangerous?

Uid values would also need to be represented as `byte[]` everywhere.

Should we allow users to also pass binary id values in indexing/bulk/get requests?  I think yes, but then we need to accept either incoming `String` or `byte[]` (encoded via base64?) via rest and Java client APIs?  Actually, it seems like we must allow this, since with auto-ids, the binary ids will come back in search results, and the user can then e.g. do a get or update from there.
</comment><comment author="rmuir" created="2016-05-05T19:06:39Z" id="217245764">as a first step couldn't it just be an internal encoding thing? e.g. just make sure we base64-encode before putting it in the terms dict and decode before doing anything with it. this seems like it would require less api changes but would give us the smaller index.
</comment><comment author="mikemccand" created="2016-05-05T19:54:02Z" id="217259422">Ooh that's a great idea!

It would require that down in `InternalEngine` we can know that an incoming `String id` was in fact base64 encoded (because it was auto-gen'd) vs not base64 encoded (because user passed in their own id).  We used to record this as a `autoGeneratedId` boolean in `Engine.Create` but looks like we removed it.

Hmm but then what would happen if a user does auto-generated IDs at first, then adds some documents with their own IDs?  This is allowed today right?  At search time we wouldn't know whether we're supposed to base64 encode it or not?  Maybe we could disallow this?
</comment><comment author="jpountz" created="2016-05-07T15:49:06Z" id="217646286">Currently the uids have the following format: `${utf8-encoded type}#${utf8-encoded id}`. Maybe we could do the following: if the id looks like an url-safe base64 string, then we would use a different separator (maybe `\0`?) so that:
- either the id looks like a base 64 string (like our auto-generated ids) and the uid would be `${utf8-encoded type}\0${base64-decoded id}`
- otherwise, the uid would remain `${utf8-encoded type}#${utf8-encoded id}`

I think we could keep everything working while saving space for auto-generated ids and not modifying the API. Actually users could even use binary ids themselves by providing base64 strings as ids. A side-effect would be that numeric ids (eg. auto-increment ids generated by a database) would also require less space since elasticsearch would base64-decode them.
</comment><comment author="mikemccand" created="2016-05-07T22:07:52Z" id="217671467">That's a clever solution @jpountz!  I think the mapping is safe, because \0 is not a character that is allowed as a `type` field value, right (no utf-8 encoded string has the 0 byte)?  So then to users it just appears that we are particularly compact at storing ID values that are valid `base64` encoded strings.

This would also mean we cannot represent uid values in the code as `String` anymore.

Hmm, you can also query `_uid` right?  So we would have to munge the incoming query to match the binary form?  For e.g. `TermsQuery` that's easy, but what about e.g. `PrefixQuery` or others ... hmm? 
</comment><comment author="jpountz" created="2016-05-08T15:44:56Z" id="217727745">Right: range, prefix and fuzzy queries would not work anymore. But maybe it is not something that should be supported on a uid field as it reduces our freedom to improve its efficiency for indexing and get operations (there was a similar discussion on #17994 about ip fields)? I also suspect that performance of ranges and prefixes (maybe fuzzy too) is terrible due to the cardinality of the field and the length of the terms (in the auto-generated case), so it is not something you can really rely on anyway?
</comment><comment author="mikemccand" created="2016-05-08T21:39:39Z" id="217747827">OK I think it makes sense to restrict the queries you can run on `_uid`, so this will be a breaking change, but it's unlikely users really do this heavily.
</comment><comment author="clintongormley" created="2016-05-09T07:39:18Z" id="217796032">I think the main reason one would use a range query on `_uid` is to partition an index for reindexing purposes.  This requirement is obviated by the changes proposed in https://github.com/elastic/elasticsearch/issues/13494#issuecomment-217054402 so +1 to restricting the queries that you can run on `_uid`
</comment><comment author="jpountz" created="2016-05-09T15:08:51Z" id="217892322">I ran some simulations to check whether this would actually save space. For an app that would index 1M docs at a rate of about 100 docs per second per shard (the rate is important, since faster rates mean that ids have longer common prefixes and vice-versa), disk space used by the `_uid` field goes from 25429976 bytes to 21608441 (-15%).

This is substantial, but the reduction drops to 7% if we put the mac address before the timestamp as discussed in #18209 (17794115 bytes to 16540472), which gets quickly drowned if you have other fields. I am unsure whether the gain is worth the complexity so I will probably wait to see what we do on #18209 and whether this change would have a positive impact on indexing speed in addition to disk space.
</comment><comment author="jpountz" created="2016-05-10T09:28:59Z" id="218105732">I did the same changes as Mike did in #18209 to simulate 100 docs per second. The classic benchmark reported the same indexing speed and the index was about 0.5% smaller. I am not sure this is worth the complexity of this change.
</comment><comment author="jpountz" created="2016-05-27T09:38:39Z" id="222104229">I think we should revisit this once we somehow remove types and enforce ids to be unique per index (rather than per type).
</comment><comment author="bleskes" created="2016-05-29T21:32:10Z" id="222384015">@jpountz I'm not sure exactly why types are in the way but if it's because their stringy nature, we can also assign each type a numerical id? 
</comment><comment author="jpountz" created="2016-05-30T06:15:43Z" id="222418865">@bleskes They are not exactly in the way, but prepending types to the ids makes lookups slower since the FST still needs to walk the bytes of the type before reaching bytes that make a difference. It is tempting to put them in the end but then the compression of the terms dictionary degrades considerably given that the terms dict only performs prefix-based compression (https://issues.apache.org/jira/browse/LUCENE-4702 might help a bit from that perspective but the compression ratio would still be worse than if prepending _type to the _uid). We could make things a bit better by assigning ids to types as you suggest, but I have the feeling that we reached a point where gettig rid of types, or at least decoupling them from mappings and treating them as index partitions, has become an acceptable path. Then we could directly use the _id as a primary key and I think we should reevaluate this since the space/speed benefit of not base64-ing the ids might become more interesting and worth the complexity of the change.
</comment><comment author="jpountz" created="2016-08-05T13:32:49Z" id="237851085">I have been working on a super hacky patch (tests don't even compile) to try to estimate how much we could win by doing this: https://github.com/jpountz/elasticsearch/tree/enhancement/optional_type. This branch has an index setting called `index.mapping.single_type`, which when set does several things:
- it forbids you from configuring another type than the value of this setting
- it does not prepend `&lt;typename&gt;#` to the `_uid`
- it expects ids to be base64 strings and encodes them in binary form (so that eg. auto-generated ids would use 15 bytes rather than 20)

@mikemccand  helped me benchmark the change and this resulted in a 13% improvement in indexing throughput with the NYC taxi rides dataset.
</comment><comment author="nik9000" created="2016-08-05T13:47:05Z" id="237854570">&gt; @mikemccand helped me benchmark the change and this resulted in a 13% improvement in indexing throughput with the NYC taxi rides dataset.

Very cool!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Loading plugins directly from lib dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18153</link><project id="" key="" /><description>This feature is permanently deleted or I made smth wrong?

It works on 1.x but doesn't on 2.x.

It makes building of embedded elasticsearch distribution easier (plugins as dependencies).
</description><key id="153206199">18153</key><summary>Loading plugins directly from lib dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nikoncode</reporter><labels /><created>2016-05-05T10:30:24Z</created><updated>2016-05-05T11:00:03Z</updated><resolved>2016-05-05T11:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T11:00:03Z" id="217125216">This feature no longer exists in 2.x and 5.x; only tests and the transport client can load plugins from the classpath.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range filter not working when writting query with ejs aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18152</link><project id="" key="" /><description>My requirement is to add a filter in to the query by using ejs but i couldn't get success on that.
I have created a Json query for the same that is working for me but i need to convert that Json in form of ejs aggregation.
Below is my JSON query:-

{
    "aggs": {
        "parentsku": {
            "terms": {
                "field": "parentSku",
                "order": {
                    "_term": "asc"
                },
                "size": 2
            },
            "aggs": {
                "cost": {
                    "stats": {
                        "field": "directCost"
                    }
                },
                "sales": {
                    "stats": {
                        "field": "revenu"
                    }
                },
                "quantity": {
                    "stats": {
                        "field": "quantity"
                    }
                }
            }

```
    }
},
 "query": {
    "filtered": {
        "filter": {
            "range": {
                "quantity": {
                    "gte": "500"
                }
            }
        }
    }
}
```

}

This query works for me when i check it with Postman client of google crome but i couldn't convert this is the form ejs aggregation and I have to use all the queries in form of ejs aggregation only.
Below is my ejs aggregation query:-

var myquery= ejs.Request()
                                 .agg(ejs.TermsAggregation('parentsku').field(skufield.value                    || 'parentSku').order("_term", "asc").size(50)
                                  .agg(ejs.StatsAggregation('cost').field('directCost'))
                    .agg(ejs.StatsAggregation('sales').field('revenu'))
                                  .agg(ejs.StatsAggregation('quantity').field('quantity'))
                                );

I only need to add below part of query with above aggregation query and it will work for me.

 "query": {
        "filtered": {
            "filter": {
                "range": {
                    "quantity": {
                        "gte": "500"
                    }
                }
            }
        }
    }

In short i want to convert above Json query in form of aggregration.I tried many alternates for that below is one from them 

var query= ejs.Request().query(ejs.FilterAggregation('filtered').filter(ejs.RangeFilter('quantity').gt(200)))
but it is not working exception is like "Argument must be query" it means "ejs.FilterAggregation('filtered').filter(ejs.RangeFilter('quantity').gt(200))" is not a query. 
</description><key id="153197543">18152</key><summary>Range filter not working when writting query with ejs aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pawansmartdata</reporter><labels /><created>2016-05-05T09:27:00Z</created><updated>2016-05-05T09:59:04Z</updated><resolved>2016-05-05T09:33:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T09:33:39Z" id="217112531">Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) for general questions.
</comment><comment author="pawansmartdata" created="2016-05-05T09:59:04Z" id="217116091">I have asked same question there too but no one replied there..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>painless: optimize/simplify dynamic field and method access</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18151</link><project id="" key="" /><description>The current code checks each classes hash twice. This results in more hash lookups than we really need. Also I think using 'return' instead of 'break' here just makes the code easier to read.

for dynamic access speedup is as high as 20%. this codepath will always be the worst case, even if we add fancy stuff to avoid it later.

@jdconrad can you look?
</description><key id="153185318">18151</key><summary>painless: optimize/simplify dynamic field and method access</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>:Scripting</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T07:51:42Z</created><updated>2016-05-05T14:19:34Z</updated><resolved>2016-05-05T14:19:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-05-05T07:56:23Z" id="217096619">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Painless: Single-Quoted Strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18150</link><project id="" key="" /><description>Added single-quoted strings to Painless + tests.
Added String -&gt; char and char -&gt; String casting.
Fixed a minor bug related to statically-typed shortcuts in lists.
</description><key id="153169419">18150</key><summary>Painless: Single-Quoted Strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>:Scripting</label><label>das awesome</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T05:18:47Z</created><updated>2016-05-05T16:30:02Z</updated><resolved>2016-05-05T16:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-05T05:24:37Z" id="217078643">I'll review it. This should be a huge win.

Today we don't support `doc['field']`, leaving only `doc["field"]` and `doc.field`. We document/recommend `doc.field` only because double quote escaping such as `doc[\"field\"]` is hellacious. But `doc.field` is much slower.

Being able to support `doc['field']` gives us consistency with the script engines today, and it gives a 50% performance increase (we gave it more type information, removes dynamic lookup).
</comment><comment author="rmuir" created="2016-05-05T06:31:30Z" id="217085384">Looks good. I pushed a commit with some docs and tests changes and ran benchmarks.
</comment><comment author="jdconrad" created="2016-05-05T16:30:02Z" id="217202293">Used merge --squash.  This made it into master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>auto indexing off</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18149</link><project id="" key="" /><description>**Elasticsearch version**: 2.1

**JVM version**: latest

**OS version**: windows 8

**Description of the problem including expected versus actual behavior**:

i am creating index using php-client with 6lac of data by reading CSV file with bulk indexing.
I have several issues in it:

1: when i run script from browser and then stop the script even close the browser, request is running on background. how its working? and how can i stop request?
2: After completion of 6 lac, index updating documents automatically and it goes to 12 lac and so on... how can i create documents with unique ID.
3: can i pass multiple parameters to $client-&gt;bultk() methods: like $client-&gt;bultk($params,$params2....) ;??

my script:
it takes 6lac record and process it.

**$presona_arr = array(
                    'index' =&gt; 'amplify4',
                    'type' =&gt; 'persona',
                    'body' =&gt; array(
                        'persona_email' =&gt; $pemail,
                        'company_id' =&gt; 18,
                        'date_added' =&gt; date("Y-m-d")
                    )
                );
                $response_per = $client-&gt;index($presona_arr);
                $persona_insert_id = $response_per['_id'];

```
            $bulkParamsDemo['body'][] = [
                'index' =&gt; [
                    '_index' =&gt; indexName,
                    '_type' =&gt; 'demo',
                    '_id' =&gt; $i,
                    'routing' =&gt; $persona_insert_id
                ]
            ];
            $bulkParamsDemo['body'][] = array(data);

            $bulkParamsTrans['body'][] = [
                'index' =&gt; [
                    '_index' =&gt; indexName,
                    '_type' =&gt; 'trans',
                    '_id' =&gt; $i,
                    'routing' =&gt; $persona_insert_id
                ]
            ];
            $bulkParamsTrans['body'][] = array(data);

            $bulkParamsGeo['body'][] = [
                'index' =&gt; [
                    '_index' =&gt; indexName,
                    '_type' =&gt; 'geo',
                    '_id' =&gt; $i,
                    'routing' =&gt; $persona_insert_id
                ]
            ];
            $bulkParamsGeo['body'][] = $geoData;

            $bulkParamsPhy['body'][] = [
                'index' =&gt; [
                    '_index' =&gt; indexName,
                    '_type' =&gt; 'phy',
                    '_id' =&gt; $i,
                    'routing' =&gt; $persona_insert_id
                ]
            ];
            $bulkParamsPhy['body'][] = array(data);

            unset($demoData);
            unset($transData);
            unset($geoData);
            unset($phyData);

            $count = $i;
           if ($i % 2800 == 0) {
                $responses = $client-&gt;bulk($bulkParamsDemo);
                 $responses = $client-&gt;bulk($bulkParamsTrans);
              $responses = $client-&gt;bulk($bulkParamsGeo);
             $responses = $client-&gt;bulk($bulkParamsPhy);
            // erase the old bulk request
                $bulkParamsDemo = ['body' =&gt; []];
                $bulkParamsTrans = ['body' =&gt; []];
                $bulkParamsGeo = ['body' =&gt; []];
                $bulkParamsPhy = ['body' =&gt; []];

                // unset the bulk response when you are done to save memory
                unset($responses);
            }**
```
</description><key id="153168612">18149</key><summary>auto indexing off</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arafay696</reporter><labels /><created>2016-05-05T05:06:46Z</created><updated>2016-05-05T08:26:59Z</updated><resolved>2016-05-05T08:26:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T08:26:59Z" id="217102293">Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) for general questions. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18148</link><project id="" key="" /><description>Bug Fixes Added for several issues https://github.com/elastic/elasticsearch/issues/11498 
 and has_child and inner_hits for grandchild hit doesn't work #11118. Test suites changed as well to reflect new code.
</description><key id="153165336">18148</key><summary>2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lwiskowski</reporter><labels /><created>2016-05-05T04:19:48Z</created><updated>2016-05-05T04:40:12Z</updated><resolved>2016-05-05T04:40:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T04:40:12Z" id="217075553">This appears to have been opened in error. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: improve logging for vagrant to emit entire output on failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18147</link><project id="" key="" /><description>This change makes the vagrant tasks extend LoggedExec, so that the
entire vagrant output can be dumped on failure (and completely logged
when using --info). It should help for debugging issues like #18122.

Note I reformatted these files to conform to the 4 space indent used by the rest of our files, so there is a little noise here.
</description><key id="153146188">18147</key><summary>Tests: improve logging for vagrant to emit entire output on failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-05T00:34:39Z</created><updated>2016-05-05T00:46:53Z</updated><resolved>2016-05-05T00:46:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T00:46:49Z" id="217048984">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect behavior with long index names on some systems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18146</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7.1

**JVM version**: OpenJDK 1.7.0_91

**OS version**:
- Linux Mint 17 Qiana (based on Ubuntu 14.04)
- kernel 3.13.0-24
- **ext4 filesystem with eCryptfs** (important!)

**Description of the problem including expected versus actual behavior**:
As per #8158, maximal length for index name was changed to 255. When using eCryptfs (and probably some other filesystem-level encryptions), file names that long are not allowed.

When creating an index with name too long for my filesystem, Elasticsearch fails on any try to search this index (because, in fact, it was never created). I would expect it to directly reject the request with index creation.

**Steps to reproduce**:
1. Configure Elasticsearch so it stores its data in folder encrypted with eCryptfs. It would probably behave the same on any other system where maximal allowed file name is less than 255.
2. Create index with name of length 255:
   
   ```
   POST /aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
   {
       "number_of_replicas": 0
   }
   ```
   
   This results with:
   
   ```
   {
      "acknowledged": true
   }
   ```
3. Index is now red, even though `number_of_replicas` is set to zero:
   
   ```
   GET /_cat/indices
   ```
   
   Results with:
   
   ```
   red open aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa 5 0     
   ```
   
   It's possible to retrieve information about index:
   
   ```
   GET /aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
   ```
   
   Results with:
   
   ```
   {
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa": {
         "aliases": {},
         "mappings": {},
         "settings": {
            "index": {
               "creation_date": "1462403349650",
               "uuid": "LrwgZOXaSPqFlbeBqnsArQ",
               "number_of_replicas": "0",
               "number_of_shards": "5",
               "version": {
                  "created": "1070199"
               }
            }
         },
         "warmers": {}
      }
   }
   ```
4. Try to search the index:
   
   ```
   GET /aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa/_search
   ```
   
   Results with:
   
   ```
   {
      "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed]",
      "status": 503
   }
   ```
5. See that data directory with indices, in fact, doesn't contain the index:
   
   ```
       $ ls indices # no output
   ```
   
   And yes, it is the correct directory:
   
   ```
   POST /foo
   POST /bar
   
   $ ls indices/
   bar  foo
   ```

What I would consider to be the correct behavior is a response like "Index name is too long." directly to the POST request that tries to create such index. In fact, that's how it responds when index name  exceeds the length of 255:

```
POST /aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
```

Results with:

```
{
   "error": "InvalidIndexNameException[[aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa] Invalid index name [aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa], index name is too long, (256 &gt; 255)]",
       "status": 400
}
```

**Conclusion**:
If it's not possible to create the index directory at hand and verify it succeeded prior to answering to the client, I think a sufficient solution to this would be to make a maximal allowed index name configurable.

BTW: This is not a real-world case. I figured this out by accident when running some tests on my workstation.
</description><key id="153141201">18146</key><summary>Incorrect behavior with long index names on some systems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tzima</reporter><labels /><created>2016-05-04T23:49:36Z</created><updated>2016-05-05T00:27:31Z</updated><resolved>2016-05-05T00:19:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-05T00:19:57Z" id="217045523">&gt; I think a sufficient solution to this would be to make a maximal allowed index name configurable.

In master, index names are no longer tied to the name of the directory (we've separated name from identity). The directory name is now just a uuid.

I note that you're on 1.7.1. I don't think we will do anything about this in 1.x nor 2.x.
</comment><comment author="tzima" created="2016-05-05T00:27:31Z" id="217046584">&gt; In master, index names are no longer tied to the name of the directory (we've separated name from identity). The directory name is now just a uuid.

That's much better solution!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Maybe we should be able to opt arbitrary requests out of tripping the http request breaker?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18145</link><project id="" key="" /><description>**Describe the feature**:
I was talking to someone about a problem they were having with a test cluster (#18144) bumping into the http breaker and I asked them to run the tasks list API to see if any tasks were stuck. And it tripped the breaker. Maybe we should opt the tasks list API out of the breaker? Maybe we should allow any request to opt out of the breaker via url parameter and only use it for administrative stuff? I dunno.
</description><key id="153133892">18145</key><summary>Maybe we should be able to opt arbitrary requests out of tripping the http request breaker?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Circuit Breakers</label><label>discuss</label></labels><created>2016-05-04T22:56:23Z</created><updated>2016-05-06T09:16:44Z</updated><resolved>2016-05-06T09:16:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-04T23:16:32Z" id="217035765">Relates #17951
</comment><comment author="danielmitterdorfer" created="2016-05-05T12:16:33Z" id="217138556">I think this is necessary although I want #17951 in before, so we use similar approaches. At least in the first step, I wouldn't go so far as to allow users to opt-out via an API call but just statically opt-out actions that are necessary for inspecting a cluster.
</comment><comment author="nik9000" created="2016-05-05T13:01:38Z" id="217146143">&gt; just statically opt-out actions that are necessary for inspecting a cluster

My reasoning behind just putting it on the API is that we don't have to know that list. I'm not sure we'll know it consistently. We could make it a dynamic setting but then settings update would have to be in the list or else we couldn't change the list when we're in a bad state.
</comment><comment author="jasontedor" created="2016-05-05T15:27:49Z" id="217185700">I don't think we need to do this. The circuit breaker setting is dynamic. There are actions (e.g., `TransportReplicationAction`) that should _always_ be exempt from the circuit breaker setting but otherwise if trouble arises the end-user can resize the limit.
</comment><comment author="clintongormley" created="2016-05-05T16:23:48Z" id="217200819">I'm against adding this as well.  Sysadmins would have to scrub this option from every request they receive from users intent on bypassing the circuit breaker.
</comment><comment author="clintongormley" created="2016-05-06T09:16:43Z" id="217393991">Discussed in FixItFriday - we prefer the whitelist of admin requests, and the ability to dynamically update this settings, to just allowing any request to ignore the circuit breaker limit.  Better for sysadmins
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CircuitBreakingException on extremely small dataset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18144</link><project id="" key="" /><description>**Elasticsearch version**: alpha2

**JVM version**: build 1.8.0_74-b02

**OS version**: OS X El Capitan 10.11.3 

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. Install and run elasticsearch-alpha2, topbeat-alpha2, and kibana-alpha2 (Topbeat is only monitoring the node process on a 20 second interval.)
2. I am using Kibana to monitor a node process. Here is the query Kibana is using to generate the visualization:
   
   ``` javascript
   {
      "size":0,
      "aggs":{
         "2":{
            "date_histogram":{
               "field":"@timestamp",
               "interval":"30s",
               "time_zone":"America/Los_Angeles",
               "min_doc_count":1
            },
            "aggs":{
               "1":{
                  "max":{
                     "field":"proc.mem.rss"
                  }
               }
            }
         }
      },
      "highlight":{
         "pre_tags":[
            "@kibana-highlighted-field@"
         ],
         "post_tags":[
            "@/kibana-highlighted-field@"
         ],
         "fields":{
            "*":{
   
            }
         },
         "require_field_match":false,
         "fragment_size":2147483647
      },
      "query":{
         "bool":{
            "must":[
               {
                  "query_string":{
                     "query":"*",
                     "analyze_wildcard":true
                  }
               },
               {
                  "match":{
                     "proc.cmdline":{
                        "query":"/Users/tyler/.nvm/versions/node/v4.4.3/bin/node ./bin/../src/cli",
                        "type":"phrase"
                     }
                  }
               },
               {
                  "range":{
                     "@timestamp":{
                        "gte":1462389453519,
                        "lte":1462390353519,
                        "format":"epoch_millis"
                     }
                  }
               }
            ],
            "must_not":[
   
            ]
         }
      }
   }
   ```
3. Run for about 30 minutes.

I have 1027 documents, and the total size is 1.6MB.

**Provide logs (if relevant)**:

``` bash
CircuitBreakingException[[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [726571417/692.9mb]]
    at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:211)
    at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128)
    at org.elasticsearch.http.HttpServer.dispatchRequest(HttpServer.java:109)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:489)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:65)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:85)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:83)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Eventually, all requests to ES will fail with this exception including _stats.
</description><key id="153098704">18144</key><summary>CircuitBreakingException on extremely small dataset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tylersmalley</reporter><labels><label>:Circuit Breakers</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T19:45:22Z</created><updated>2016-05-11T07:19:13Z</updated><resolved>2016-05-09T13:09:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-04T19:54:45Z" id="216982652">"fun"

&gt; data for [&lt;http_request&gt;] would be larger than limit of [726571417/692.9mb]

@danielmitterdorfer this might be you though it is hard to tell.

@tylersmalley is there any chance you can take a thread dump when this happens? Maybe just the hot_threads API (though it might not work because of the breaker)?
</comment><comment author="tylersmalley" created="2016-05-04T20:00:59Z" id="216984318">@nik9000, I will get that once it returns to a failed state again.
</comment><comment author="nik9000" created="2016-05-04T20:03:16Z" id="216984893">&gt; @nik9000, I will get that once it returns to a failed state again.

Thanks!
</comment><comment author="tylersmalley" created="2016-05-04T20:56:15Z" id="217000187">The _msearch requests will begin failing before the entire ES cluster. Nothing ever appeared in `/_nodes/hot_threads` and eventually it would fail with the same exception:

``` json
{
  "error" : {
    "root_cause" : [ {
      "type" : "circuit_breaking_exception",
      "reason" : "[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [726571417/692.9mb]",
      "bytes_wanted" : 726582240,
      "bytes_limit" : 726571417
    } ],
    "type" : "circuit_breaking_exception",
    "reason" : "[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [726571417/692.9mb]",
    "bytes_wanted" : 726582240,
    "bytes_limit" : 726571417
  },
  "status" : 503
}
```

Here is the thread dump: https://gist.githubusercontent.com/tylersmalley/00105a27a0dd7b86016d78dc65e1bfb1/raw/jstack_7647_2.log

I will keep the cluster in a failed state should you need any additional information from it.
</comment><comment author="nik9000" created="2016-05-04T21:19:20Z" id="217006049">&gt; Here is the thread dump

It says "I'm not doing anything". Any chance you can get a [task list](https://www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html)? `curl localhost:9200/_tasks` should do it. That is another (new and shiny!) way for me to figure out what is going on.

The breaker you are hitting is trying to prevent requests from overwhelming memory. If you had in flight requests I should have seen them doing something in the thread dump. Lots of stuff in Elasticsearch is async so I wouldn't see everything but I expected something. The task list goes the other way - it registers something whenever a request starts and removes it when it stops. If we see something in the task list, especially if it is a lot of somethings, then we have our smoking gun. If we see nothing, well, we go look other places.

The next place might be getting a heap dump. But I'm not going to put you through that. I should be able to reproduce this on my side.

I believe @danielmitterdorfer, who I pinged, will not be around tomorrow so I might just keep this issue.
</comment><comment author="tylersmalley" created="2016-05-04T21:31:53Z" id="217010041">&gt;  Any chance you can get a task list?

``` bash
curl 'http://localhost:9200/_tasks?pretty=true'
{
  "error" : {
    "root_cause" : [ {
      "type" : "circuit_breaking_exception",
      "reason" : "[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [726571417/692.9mb]",
      "bytes_wanted" : 726582240,
      "bytes_limit" : 726571417
    } ],
    "type" : "circuit_breaking_exception",
    "reason" : "[parent] Data too large, data for [&lt;http_request&gt;] would be larger than limit of [726571417/692.9mb]",
    "bytes_wanted" : 726582240,
    "bytes_limit" : 726571417
  },
  "status" : 503
}
```

I will restart the cluster and monitor the `_tasks` endpoint until it begins throwing exceptions and report back.

Here is a heap dump in its current failed state: https://gist.github.com/tylersmalley/00105a27a0dd7b86016d78dc65e1bfb1/raw/jmap_7647.bin
</comment><comment author="danielmitterdorfer" created="2016-05-05T12:09:59Z" id="217137647">@nik9000 I tried to reproduce the scenario locally by running topbeat and running the query above periodically but so far the circuit breaker did not trip. I am not surprised that the thread dump does not reveal much because the circuit breaker essentially prevents further work from coming into the system. Based on an analysis of the heap dump I guess that the system is not really busy but the bytes are not freed properly and add up over time. I had a closer look at how the bytes are freed in `HttpServer.ResourceHandlingHttpChannel`:

``` java
inFlightRequestsBreaker(circuitBreakerService).addWithoutBreaking(-request().content().length());
```

Considering that the content is represented by a `ChannelBufferBytesReference` which returns the readable bytes in the channel buffer we could end up in a situation where we reserve more bytes than we free (as the readable bytes could change over time). But this is only a theory and I was not able to observe this behavior. If this were the case, the fix is to simply provide the number of reserved bytes to `HttpServer.ResourceHandlingHttpChannel` but I am somewhat reluctant to do a fix without being able to reproduce it.
</comment><comment author="danielmitterdorfer" created="2016-05-05T13:29:15Z" id="217154669">I have also installed Kibana 5.0.0-alpha2, imported the dashboard from topbeat, opened it and set it to auto-refresh every 5 seconds. I could just see that the request breaker (which is used by `BigArrays`) is slowly increasing (a few MB after a few minutes). The inflight requests breaker always resets to zero. I think it just happens to be the victim as it's trying very early during request processing to reserve an amount of bytes (we can also see from the stack trace that actually the parent circuit breaker is tripping, not the inflight requests breaker).

So I followed the respective `close()` calls that are supposed to free the reserved bytes but there are a lot of places to follow. @dakrone: Is it intended that the number of reserved bytes in the request circuit breaker grow over time as `BigArrays` seems to be intended as some kind of pool or should the reserved number of bytes in `BigArrays` be zero after a request has been processed?
</comment><comment author="dakrone" created="2016-05-05T14:28:30Z" id="217169018">@danielmitterdorfer in my testing the request circuit breaker (backing BigArrays) has always reset to 0 if there are no requests

You should be able to turn on TRACE logging for the `org.elasticsearch.indices.breaker` package and see _all_ increments and decrements to the breakers (note this is very verbose)
</comment><comment author="danielmitterdorfer" created="2016-05-05T14:38:44Z" id="217171552">@dakrone Thanks for the hint. I'll check that.
</comment><comment author="tylersmalley" created="2016-05-05T16:08:42Z" id="217196915">@danielmitterdorfer I believe to have found what was causing this on my end, but unsure if it should have triggered the the CircuitBreaker. While doing other testing I still had a script running which hit the `cluster/_health` endpoint, paused for 100ms, then repeated. I am fairly certain this was not an issue in 2.3, but I can double check.
</comment><comment author="danielmitterdorfer" created="2016-05-05T16:12:27Z" id="217197961">@tylersmalley Even that should not trip any circuit breaker so we definitely need to investigate. If you can shed more light on how we can reproduce it, that's great.
</comment><comment author="dakrone" created="2016-05-05T17:59:42Z" id="217227100">I was able to reproduce this also on 5.0.0-alpha2 with x-pack installed and kibana hitting the node. Just like @danielmitterdorfer said, the request breaker is increasing very slowly, it looks like there is a leak.

I also tried setting `network.breaker.inflight_requests.overhead: 0.0` and it looks like it is not being taken into account in at least one place (still having bytes added over time instead of all in_flight_request additions being 0)
</comment><comment author="tylersmalley" created="2016-05-05T20:40:41Z" id="217271924">@danielmitterdorfer here is the node script I have to preform the health requests on ES. In it I added a second check to run in parallel for speed up the fault.

https://gist.githubusercontent.com/tylersmalley/00105a27a0dd7b86016d78dc65e1bfb1/raw/test.js
</comment><comment author="dakrone" created="2016-05-05T20:55:56Z" id="217277457">This reproduces pretty easily now, building from master (or 5.0.0-alpha2), simple turn on logging and then run Kibana, the periodic health check that kibana does causing it to increase over time.
</comment><comment author="danielmitterdorfer" created="2016-05-09T04:54:03Z" id="217775744">@dakrone I can reproduce the increase now too but the problem is _not_ the `in_flights_request` breaker but the `request` breaker that keeps increasing. Nevertheless, I'll investigate what's going on.
</comment><comment author="danielmitterdorfer" created="2016-05-09T08:24:55Z" id="217804629">I have investigated and now have a minimal reproduction scenario: `curl -XHEAD http://localhost:9200/`

The problem is that a `XContentBuilder` is created which is backed by a `BigArrays` instance but then we use a constructor of `BytesRestResponse` without a builder. After that we lose track of the `BigArrays` instance and don't free it. This affects at least `RestMainAction` and probably other actions too. I am now working on a fix.
</comment><comment author="danielmitterdorfer" created="2016-05-09T09:28:29Z" id="217817676">I have also checked 2.x. It is not affected as the code is structured differently there.
</comment><comment author="danielmitterdorfer" created="2016-05-09T13:13:37Z" id="217860373">@tylersmalley The problem is fixed now and the fix will be included in the next release of the 5.0 series. Thanks for reporting and helping on the reproduction. Much appreciated!
</comment><comment author="tylersmalley" created="2016-05-09T18:03:13Z" id="217940999">Great, thanks @danielmitterdorfer! 
</comment><comment author="danielmitterdorfer" created="2016-05-10T12:26:23Z" id="218142016">@dakrone I also checked why this happens:

&gt; I also tried setting network.breaker.inflight_requests.overhead: 0.0 and it looks like it is not being taken into account in at least one place (still having bytes added over time instead of all in_flight_request additions being 0)

It's caused by the implementation of [`ChildMemoryCircuitBreaker#limit()](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/breaker/ChildMemoryCircuitBreaker.java#L149-L176). As far as I can see the overhead is only taken into account for logging statements but never for actual limiting. To me this does not sound that it's intended that way.
</comment><comment author="dakrone" created="2016-05-10T15:19:34Z" id="218191297">@danielmitterdorfer the overhead is taken into account also when comparing against the limit:

``` java
if (memoryBytesLimit &gt; 0 &amp;&amp; newUsedWithOverhead &gt; memoryBytesLimit) {
    ....
}
```

I remember it correctly now (I was misinterpreting what a feature I added did, doh!), the overhead is only for tweaking the estimation of an addition, not to factor into the total at all. This is because the fielddata circuit breaker estimates the amount of memory used but ultimately adjusts with the exact value used, so it should not add the overhead-modified usage, but the actual usage. Only the overhead is used for the _per-addition_ check.

Hopefully that clarifies, I was slightly confusing myself there too assuming it was taken into account with the added total amount for the breaker, but the current behavior is correct.
</comment><comment author="danielmitterdorfer" created="2016-05-11T07:19:13Z" id="218381274">@dakrone Ah, right. I missed this line... . Thanks for the explanation. Maybe we should add a comment in the code so the next time it comes up we don't have to dig to find this in the ticket again. :) With that explanation I am not sure whether any circuit breaker except the field data circuit breaker should have a user-defined overhead at all. Wdyt?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>custom discovery ping error handler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18143</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

Export `MasterFaultDetection.retryCount`/`NodeFD.retryCount` as a part of stats or allow injecting a custom `MasterPinger`/`NodeFD` into `MasterFaultDetection`/`NodesFaultDetection`.

I think it is important to monitor ping timeouts proactively instead of relying on ping_timeout and ping_retries configuration settings to kick out a node that stops responding. From my experience if the ping timeout is not a completely isolated hardware/networking issue, it is often an indication of a more fundamental problem which might get only worse by resharding process.

Hence, I think it would be very beneficial to include ping failures/retries as a part of collecting stats. I am aware that it is possible to build your own discovery plugin that would be a copy of the existing zen discovery mechanism + needed changes. However, I think it would be an overkill if one merely wants to collect additional information and do not change the actual behavior of the discovery mechanism.
</description><key id="153086068">18143</key><summary>custom discovery ping error handler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amreshakim</reporter><labels><label>:Stats</label></labels><created>2016-05-04T18:45:01Z</created><updated>2016-05-07T16:18:31Z</updated><resolved>2016-05-07T16:18:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-04T19:07:33Z" id="216969257">The retry counts get reset to zero on every successful reply from a node that is answering a fault detection request but I guess that you mean monitoring when it increases from zero? But then aggregated stats are not what you would want because it doesn't tell you which nodes the master failed to reply to (for `MasterFaultDetection`), and it doesn't tell you which nodes failed to reply to the master (for `NodeFaultDetection`). It also doesn't tell you which side of the partition a node is on (i.e., if the master fails to reply to a node, is it because the master is partitioned away, or is it because the node is partitioned away?). So I question the usefulness of this. I'm especially cautious about adding an endpoint that dives down into the discovery layer (especially since different discovery implementations might not even have a notion of pings). I'm -1 but I await feedback that could convince me otherwise?
</comment><comment author="amreshakim" created="2016-05-05T15:37:39Z" id="217188712">Jason,

Thanks for the reply and I agree it is not easy to make these stats useful. But I hope I don't have to argue about usefulness of having this information proactively.

&gt; I'm especially cautious about adding an endpoint that dives down into the discovery layer (especially since different discovery implementations might not even have a notion of pings)

Agreed w.r.t. exposing specifics of the implementations. How about just a mechanism to register a listener able to subscribe to generic events (similar to ShardBitsetFilterCache)?
</comment><comment author="jasontedor" created="2016-05-05T15:43:50Z" id="217190329">&gt; Thanks for the reply and I agree it is not easy to make these stats useful. But I hope I don't have to argue about usefulness of having this information proactively.

I wouldn't call it an argument as much as a discussion. &#128516;

But we do need to discuss it because of my thoughts above (and especially since you seem to agree the stats aren't immediately useful), I don't see the point?

&gt; How about just a mechanism to register a listener able to subscribe to generic events (similar to ShardBitsetFilterCache)?

Yeah, that's roughly how we'd do it _if_ there is a compelling reason for doing it. 
</comment><comment author="amreshakim" created="2016-05-05T17:29:51Z" id="217218325">Here is why I would love to have an ability to expose ping related stats/events from the discovery module. This is based solely on my experience with using ES 1.7 on AWS.

Kicking a machine out of cluster (and moving shards around as a consequence) is useful only if the issue is completely isolated and has nothing to do with ES itself, e.g., hardware, network. However, in many cases ping failures are just an indication of a bigger problem (CPU spikes, Java GC stalls, OOM), and there is no one-size-fits-it-all policy to handle it. Sometimes, it is enough to shutdown the misbehaving machine but often it requires a more comprehensive involvement. More importantly, moving shards as a reaction is (1) already too late and (2) will most likely aggravate the problem. I believe, human would be in a much better position to make an appropriate action to such failures and minimize potential outages (red cluster state).

In our production setup we have increased ping_retries to a much higher value to prevent a default reaction to ping failures. And it would have been very useful to get notified on these events as soon as possible.

We are quite happy with zen discovery in general and would consider implementing our own discovery plugin (which would be a pretty much copy&amp;paste of the existing one + ping failures listener) as our last resort.
</comment><comment author="jasontedor" created="2016-05-07T16:16:55Z" id="217648218">&gt; Kicking a machine out of cluster (and moving shards around as a consequence) is useful only if the issue is completely isolated and has nothing to do with ES itself, e.g., hardware, network.

[Delayed allocation](https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html) can help you here.

&gt; However, in many cases ping failures are just an indication of a bigger problem (CPU spikes, Java GC stalls, OOM), and there is no one-size-fits-it-all policy to handle it.

A broad alert (a node missed a ping) where the first step is checking these metrics likely to be less useful than just having fine-grained alerting on these specific metrics.

&gt; In our production setup we have increased ping_retries to a much higher value to prevent a default reaction to ping failures.

This is generally not a good idea. This will allow hung or dead nodes to stick around far longer than is ideal which will impact querying and indexing.

I'm sorry that I'm continuing to miss the usefulness of this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove client option for JVM for plugins script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18142</link><project id="" key="" /><description>Today we specify the client option for the JVM when executing plugin
commands. Yet, this option does nothing on a 64-bit capable JDK as such
JDKs always select the Hotspot server VM. And for 32-bit JDKs, running
plugin commands with the server VM is okay. Thus, we should just remove
this unnecessary flag and just let the default VM be selected.
</description><key id="153073454">18142</key><summary>Remove client option for JVM for plugins script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>:Plugins</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T17:43:19Z</created><updated>2016-05-05T08:38:23Z</updated><resolved>2016-05-04T18:11:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-04T18:09:04Z" id="216951972">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin install using absolute file path on Windows requires three protocol slashses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18141</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.8.0_u45

**OS version**: Windows 7 and Windows 10

**Description of the problem including expected versus actual behavior**:
The plugin install using a URL with absolute file path on Windows requires use of three slashes in the protocol `file:///C:\` as opposed to `file://C:\`.

**Provide logs (if relevant)**:

Here where proper file URL format is used, plugin is not installed.

```
C:\Users\IEUser\Downloads\elasticsearch-2.3.2&gt;bin\plugin.bat install file://C:\Users\IEuser\Downloads\license-2.3.2.zip
-&gt; Installing from file://C:/Users/IEuser/Downloads/license-2.3.2.zip...
Plugins directory [C:\Users\IEUser\Downloads\elasticsearch-2.3.2\plugins] does not exist. Creating...
Trying file://C:/Users/IEuser/Downloads/license-2.3.2.zip ...
Failed: UnknownHostException[C]
ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
```

Here where three slashes in the URL is required:

```
C:\Users\IEUser\Downloads\elasticsearch-2.3.2&gt;bin\plugin.bat install file:///C:\Users\IEuser\Downloads\license-2.3.2.zip

-&gt; Installing from file:/C:/Users/IEuser/Downloads/license-2.3.2.zip...
Trying file:/C:/Users/IEuser/Downloads/license-2.3.2.zip ...
Downloading .DONE
Verifying file:/C:/Users/IEuser/Downloads/license-2.3.2.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Installed license into C:\Users\IEUser\Downloads\elasticsearch-2.3.2\plugins\license
```
</description><key id="153056961">18141</key><summary>plugin install using absolute file path on Windows requires three protocol slashses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels /><created>2016-05-04T16:28:09Z</created><updated>2016-05-04T16:44:13Z</updated><resolved>2016-05-04T16:37:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2016-05-04T16:36:01Z" id="216922519">Three slashes is actually the proper syntax for Windows.  See https://blogs.msdn.microsoft.com/ie/2006/12/06/file-uris-in-windows/
</comment><comment author="jasontedor" created="2016-05-04T16:37:42Z" id="216922984">This is how [file URIs are specified](https://tools.ietf.org/html/rfc1738#section-3.10). A file URI is of the form `file://host/path` so you can say `file://localhost/path` or `file:///path` to also indicate `localhost` if the `host` part is omitted (as per [RFC 1738 3.10](https://tools.ietf.org/html/rfc1738#section-3.10)).

This is why you see `Failed: UnknownHostException[C]` in the logs; with two slashes the `C:` is being interpreted as hostname.
</comment><comment author="jasontedor" created="2016-05-04T16:40:33Z" id="216923765">&gt; Three slashes is actually the proper syntax for Windows

It's not just Windows. It's all file URIs. Some parsers will handle `file://path` but they are not supposed to as that is invalid per the RFC.
</comment><comment author="djschny" created="2016-05-04T16:43:07Z" id="216924763">It looks like I need to file in Kibana instead then, as it fails if three slashes are used but succeeds if two are used. 
</comment><comment author="jasontedor" created="2016-05-04T16:44:13Z" id="216925340">&gt; It looks like I need to file in Kibana instead then, as it fails if three slashes are used but succeeds if two are used.

That's a paddlin'.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pass ES_JAVA_OPTS to JVM for plugins script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18140</link><project id="" key="" /><description>This commit adds support for ES_JAVA_OPTS to the elasticsearch-plugin
script.

Closes #16790
</description><key id="153056836">18140</key><summary>Pass ES_JAVA_OPTS to JVM for plugins script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>:Plugins</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T16:27:30Z</created><updated>2016-05-04T16:48:40Z</updated><resolved>2016-05-04T16:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-04T16:43:29Z" id="216924957">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when calling non existing ingest pipeline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18139</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha2
**Description of the problem including expected versus actual behavior**: NPE is thrown when calling non existing ingest pipeline. I would expect a better message like "Pipeline [{}] does not exist".

**Steps to reproduce**:

``` sh
curl -XDELETE "localhost:9200/_ingest/pipeline/doesnotexist?pretty"
curl -XPOST "localhost:9200/_ingest/pipeline/doesnotexist/_simulate?pretty" -d '{
    "docs": [ {
        "_index": "index",
        "_type": "type",
        "_id": "id",
        "_source": {
            "foo" : "baz"
        }
    }   ]
}'
```

Gives back:

``` json
  "error" : {
    "root_cause" : [ {
      "type" : "null_pointer_exception",
      "reason" : null
    } ],
    "type" : "null_pointer_exception",
    "reason" : null
  },
  "status" : 500
}
```

**Provide logs (if relevant)**:

```
[2016-05-04 16:55:54,814][WARN ][rest.suppressed          ] /_ingest/pipeline/doesnotexist/_simulate Params: {pretty=, id=doesnotexist}
java.lang.NullPointerException
    at org.elasticsearch.action.ingest.SimulateExecutionService$1.doRun(SimulateExecutionService.java:72)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="153033593">18139</key><summary>NPE when calling non existing ingest pipeline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Ingest</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-05-04T14:58:46Z</created><updated>2016-05-06T20:34:22Z</updated><resolved>2016-05-06T20:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-04T15:17:28Z" id="216898252">@talevy could you take a look please?
</comment><comment author="talevy" created="2016-05-04T16:01:35Z" id="216912420">yup!
</comment><comment author="talevy" created="2016-05-06T20:30:46Z" id="217550545">this should make for a nicer exception ^^
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Modernize README.textile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18138</link><project id="" key="" /><description>- camel_case all the things
- `?pretty` all the things (we tell people to do this in bug reports)

Closes #18136
</description><key id="153025112">18138</key><summary>Modernize README.textile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label></labels><created>2016-05-04T14:25:36Z</created><updated>2016-05-04T15:02:02Z</updated><resolved>2016-05-04T15:02:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-04T14:32:28Z" id="216883615">LGTM
</comment><comment author="jasontedor" created="2016-05-04T14:37:39Z" id="216885121">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update misc.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18137</link><project id="" key="" /><description>Added documentation for the cluster.indices.tombstones.size property for maximum tombstones in the cluster state.
</description><key id="153024076">18137</key><summary>Update misc.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>docs</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T14:21:15Z</created><updated>2016-05-04T22:42:45Z</updated><resolved>2016-05-04T22:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-05-04T14:21:30Z" id="216880239">@clintongormley would you like to review?
</comment><comment author="clintongormley" created="2016-05-04T15:21:51Z" id="216899958">I'd also mention whether the setting can be updated dynamically
</comment><comment author="abeyad" created="2016-05-04T15:50:50Z" id="216909145">I incorporated all the suggested changes.  
</comment><comment author="dakrone" created="2016-05-04T19:17:56Z" id="216971871">LGTM
</comment><comment author="abeyad" created="2016-05-04T22:42:45Z" id="217025712">Closed by commit https://github.com/elastic/elasticsearch/commit/67c0734bf3d1f6748be56e83c292ad4910339c38
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>README examples in github repo do not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18136</link><project id="" key="" /><description>The examples in https://github.com/elastic/elasticsearch seem out of date. For example it has:

```
"query" : {
  "matchAll" : {}
}
```

but this just returns an error in master, e.g. should be `match_all`?
</description><key id="153022638">18136</key><summary>README examples in github repo do not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-05-04T14:15:19Z</created><updated>2016-05-04T15:02:01Z</updated><resolved>2016-05-04T15:02:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>trying to merge the field stats of field but the field type is incompatible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18135</link><project id="" key="" /><description>linked to https://github.com/elastic/kibana/issues/7127

**Elasticsearch version**:
5.0.0-alpha2

**JVM version**:
java version "1.8.0_45"
**OS version**:
Linux
**Description of the problem including expected versus actual behavior**:
Error when calling _field_stats API on Logstash 5.0.0-alpha2 generated index

```
abonuccelli@w530 /opt/elk/PROD/scripts $ curl -XGET "https://192.168.1.105:9200/logstash-syslog-2016.05.04/_field_stats?fields=@timestamp&amp;level=indices&amp;pretty" -k --cacert /opt/elk/PROD/FS/secure/cacert.pem  -u elastic:xxxxxx
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_state_exception",
      "reason" : "trying to merge the field stats of field [@timestamp] from index [logstash-syslog-2016.05.04] but the field type is incompatible, try to set the 'level' option to 'indices'"
    } ],
    "type" : "illegal_state_exception",
    "reason" : "trying to merge the field stats of field [@timestamp] from index [logstash-syslog-2016.05.04] but the field type is incompatible, try to set the 'level' option to 'indices'"
  },
  "status" : 500
}
```

calling the same on .monitoring-es-\* index timestamp field works ok

```
abonuccelli@w530 /opt/elk/PROD/scripts $  curl -XGET "https://192.168.1.105:9200/.monitoring-es-2-2016.05.04/_field_stats?fields=timestamp&amp;level=indices&amp;pretty" -k --cacert /opt/elk/PROD/FS/secure/cacert.pem  -u elastic:xxxxxx
{
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "indices" : {
    ".monitoring-es-2-2016.05.04" : {
      "fields" : {
        "timestamp" : {
          "max_doc" : 21360,
          "doc_count" : 21360,
          "density" : 100,
          "sum_doc_freq" : -1,
          "sum_total_term_freq" : 21360,
          "min_value" : 1462353531772,
          "min_value_as_string" : "2016-05-04T09:18:51.772Z",
          "max_value" : 1462370153801,
          "max_value_as_string" : "2016-05-04T13:55:53.801Z"
        }
      }
    }
  }

}
```

**Provide logs (if relevant)**:

```
[2016-05-04 15:50:22,052][WARN ][rest.suppressed          ] /logstash-syslog-2016.05.04/_field_stats Params: {pretty=, level=indices, index=logstash-syslog-2016.05.04, fields=@timestamp}
java.lang.IllegalStateException: trying to merge the field stats of field [@timestamp] from index [logstash-syslog-2016.05.04] but the field type is incompatible, try to set the 'level' option to 'indices'
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:108)
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:58)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.finishHim(TransportBroadcastAction.java:248)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.onOperation(TransportBroadcastAction.java:213)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:193)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:180)
    at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:789)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:178)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:143)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

```

**other info

@timestamp mapping for problematic logstash-syslog-\* index

```
logstash-syslog-* 

 "timestamp" : {
            "type" : "date",
           }
```

timestamp mapping for .monitoring-es-\* index 

```
 "timestamp" : {
            "type" : "date",
            "format" : "date_time"
          }
```
</description><key id="153019392">18135</key><summary>trying to merge the field stats of field but the field type is incompatible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>:Stats</label><label>bug</label></labels><created>2016-05-04T14:01:13Z</created><updated>2016-06-01T16:28:37Z</updated><resolved>2016-05-04T21:28:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-04T16:15:36Z" id="216916659">Trying to work up a recreation
</comment><comment author="clintongormley" created="2016-05-04T17:05:41Z" id="216933517">OK - simple recreation.  Start a cluster with two nodes, then run the following:

```
DELETE t

POST t/t/_bulk
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}
{"index":{}}
{"@timestamp":"2000-01-01T00:00:00.0+00"}

GET t/_field_stats?level=indices&amp;fields=@timestamp
```

The above returns:

```
{
   "error": {
      "root_cause": [
         {
            "type": "illegal_state_exception",
            "reason": "trying to merge the field stats of field [@timestamp] from index [t] but the field type is incompatible, try to set the 'level' option to 'indices'"
         }
      ],
      "type": "illegal_state_exception",
      "reason": "trying to merge the field stats of field [@timestamp] from index [t] but the field type is incompatible, try to set the 'level' option to 'indices'"
   },
   "status": 500
}
```

With the following stack trace:

```
[2016-05-04 19:04:43,705][WARN ][rest.suppressed          ] /t/_field_stats Params: {level=indices, index=t, fields=@timestamp}
java.lang.IllegalStateException: trying to merge the field stats of field [@timestamp] from index [t] but the field type is incompatible, try to set the 'level' option to 'indices'
  at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:108)
  at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:58)
  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.finishHim(TransportBroadcastAction.java:248)
  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.onOperation(TransportBroadcastAction.java:213)
  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:193)
  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:180)
  at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:789)
  at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:178)
  at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:143)
  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
  at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
  at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
  at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
  at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
  at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="jimczi" created="2016-05-04T21:28:35Z" id="217009242">This is a serialization bug in the field stats transport that was introduced when the new point api was added. It occurs only with 5.0.0-alpha2 and has been fixed in master with this commit https://github.com/elastic/elasticsearch/commit/f600c4ab9c4a575a269287bb5aaa22d828c56496
</comment><comment author="nellicus" created="2016-05-11T15:36:32Z" id="218497804">@jimferenczi this effectively prevents, at least in my environment, any use in kibana of the data coming from logstash. just raising a concern that this might impede proper testing of alpha2 through our user base
</comment><comment author="clintongormley" created="2016-05-12T10:01:59Z" id="218713498">@nellicus yeah - not much we can do about it until the next release
</comment><comment author="anhlqn" created="2016-05-13T14:33:55Z" id="219060736">This also occurred when I tried to use the winlogbeat indexes on a fresh installation without Logstash
</comment><comment author="gluckspilz" created="2016-05-27T00:33:37Z" id="222031867">Will adding "format" : "date_time" to the mapping for the index created by logstash stop this error?
</comment><comment author="michalterbert" created="2016-05-30T19:30:57Z" id="222544721">@gluckspilz: for me doesn't work &#128078; 
</comment><comment author="clintongormley" created="2016-06-01T16:28:37Z" id="223048714">This is fixed in 5.0.0-alpha3, which is out already
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>grade build-tools jar missing for alpha2 on sonatype</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18134</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha2

The build-tools jar, which is the gradle plugin providing all the awesome test infrastructure for plugins has not been deployed with, see https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/gradle/build-tools/

so this `build.gradle` part does not work for alpha2:

```
buildscript {
  repositories {
    mavenLocal()
    mavenCentral()
    jcenter()
  }

  dependencies {
    classpath "org.elasticsearch.gradle:build-tools:5.0.0-alpha2"
  }
}
```

When I build master locally the jar seems to be generated though
</description><key id="153016302">18134</key><summary>grade build-tools jar missing for alpha2 on sonatype</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>build</label></labels><created>2016-05-04T13:48:33Z</created><updated>2016-05-05T08:41:35Z</updated><resolved>2016-05-04T19:50:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-04T14:40:32Z" id="216885975">/cc @rjernst this wasn't produced during the alpha2 build
</comment><comment author="rjernst" created="2016-05-04T15:04:41Z" id="216894130">The build tools jar currently must be built separately. Notice in jenkins when building snapshots we cd into buildSrc and also do an uploadArchives there. I had done some magic to have it deploy with the rest of the project when we briefly switched to the maven publish plugin, but that didn't work for signing. I'm working on a change right now that will add it back for the long term. 
</comment><comment author="rjernst" created="2016-05-04T15:10:28Z" id="216895917">And I just remembered for alpha1-i published this manually to sonatype...I'll do that again...
</comment><comment author="rjernst" created="2016-05-04T19:50:56Z" id="216981563">Ok I published build-tools for alpha2. As I noted before, I have a change that will be coming up to make this easier, but this isn't really a bug, it's just something we forgot to do.
</comment><comment author="spinscale" created="2016-05-04T20:03:33Z" id="216984971">works now in my plugin, thx for fixing!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QueryBuilder does not need generics.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18133</link><project id="" key="" /><description>QueryBuilder has generics, but those are never used: all call sites use
`QueryBuilder&lt;?&gt;`. Only `AbstractQueryBuilder` needs generics so that the base
class can contain a default implementation for setters that returns `this`.
</description><key id="153012904">18133</key><summary>QueryBuilder does not need generics.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Java API</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T13:32:17Z</created><updated>2016-05-06T07:13:24Z</updated><resolved>2016-05-06T07:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-05-04T13:38:56Z" id="216867593">I felt a great disturbance in the Force, as if hundreds of spurious warnings suddenly cried out in terror, and were suddenly silenced. I fear something awesome has happened.
</comment><comment author="nik9000" created="2016-05-04T15:02:29Z" id="216893436">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reorganise scripting docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18132</link><project id="" key="" /><description>This PR is based on the reorganisation started by @rmuir in #18116.  It tidies up some of the asciidoc layout, adds more examples using Lucene expressions and rewrites some of the more confusing sections.

I've also moved the doc-values properties/methods under the Groovy section as these aren't supported by Painless (yet?) of Expressions. I added the experimental label to text-scoring-in-scripts as its value and implementation is questionable.

Closes #18116
</description><key id="153003803">18132</key><summary>Reorganise scripting docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Scripting</label><label>docs</label><label>review</label></labels><created>2016-05-04T12:51:19Z</created><updated>2016-05-04T16:17:10Z</updated><resolved>2016-05-04T16:17:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-04T12:51:55Z" id="216853178">@jdconrad @rmuir could you take a look at this when you have a moment?
</comment><comment author="rmuir" created="2016-05-04T15:58:04Z" id="216911338">Thanks for improving this!

FYI I tried to build this locally to look but hit a snag:

```
a2x: executing: "xmllint" --nonet --noout --valid "/home/rmuir/workspace/elasticsearch-docs/index.xml"

/home/rmuir/workspace/elasticsearch-docs/index.xml:53014: element link: validity error : IDREF attribute linkend references an unknown ID "aggregations"
/home/rmuir/workspace/elasticsearch-docs/index.xml:52863: element link: validity error : IDREF attribute linkend referencesa2x: ERROR: "xmllint" --nonet --noout --valid "/home/rmuir/workspace/elasticsearch-docs/index.xml" returned non-zero exit status 4
 an unknown ID "script-reloading"
```
</comment><comment author="clintongormley" created="2016-05-04T16:17:05Z" id="216917054">thanks @rmuir and @jpountz - i've addressed the issues and will merge
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins jars are not published to maven central</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18131</link><project id="" key="" /><description>http://repo1.maven.org/maven2/org/elasticsearch/plugin/analysis-phonetic/5.0.0-alpha2/

```
analysis-phonetic-5.0.0-alpha2-javadoc.jar         26-Apr-2016 12:52               49029
analysis-phonetic-5.0.0-alpha2-javadoc.jar.asc     26-Apr-2016 12:52                 475
analysis-phonetic-5.0.0-alpha2-javadoc.jar.asc.md5 26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2-javadoc.jar.asc...&gt; 26-Apr-2016 12:52                  40
analysis-phonetic-5.0.0-alpha2-javadoc.jar.md5     26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2-javadoc.jar.sha1    26-Apr-2016 12:52                  40
analysis-phonetic-5.0.0-alpha2-sources.jar         26-Apr-2016 12:52               16483
analysis-phonetic-5.0.0-alpha2-sources.jar.asc     26-Apr-2016 12:52                 475
analysis-phonetic-5.0.0-alpha2-sources.jar.asc.md5 26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2-sources.jar.asc...&gt; 26-Apr-2016 12:52                  40
analysis-phonetic-5.0.0-alpha2-sources.jar.md5     26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2-sources.jar.sha1    26-Apr-2016 12:52                  40
analysis-phonetic-5.0.0-alpha2.pom                 26-Apr-2016 12:52                3282
analysis-phonetic-5.0.0-alpha2.pom.asc             26-Apr-2016 12:52                 475
analysis-phonetic-5.0.0-alpha2.pom.asc.md5         26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2.pom.asc.sha1        26-Apr-2016 12:52                  40
analysis-phonetic-5.0.0-alpha2.pom.md5             26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2.pom.sha1            26-Apr-2016 12:52                  40
analysis-phonetic-5.0.0-alpha2.zip                 26-Apr-2016 12:52              275937
analysis-phonetic-5.0.0-alpha2.zip.asc             26-Apr-2016 12:52                 475
analysis-phonetic-5.0.0-alpha2.zip.asc.md5         26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2.zip.asc.sha1        26-Apr-2016 12:52                  40
analysis-phonetic-5.0.0-alpha2.zip.md5             26-Apr-2016 12:52                  32
analysis-phonetic-5.0.0-alpha2.zip.sha1            26-Apr-2016 12:52                  40
```

We used to publish them before 5.0. See http://repo1.maven.org/maven2/org/elasticsearch/plugin/analysis-phonetic/2.3.2/

```
analysis-phonetic-2.3.2-sources.jar                21-Apr-2016 16:14               12815
analysis-phonetic-2.3.2-sources.jar.asc            21-Apr-2016 16:14                 473
analysis-phonetic-2.3.2-sources.jar.md5            21-Apr-2016 16:14                  32
analysis-phonetic-2.3.2-sources.jar.sha1           21-Apr-2016 16:14                  40
analysis-phonetic-2.3.2.jar                        21-Apr-2016 16:14               15582
analysis-phonetic-2.3.2.jar.asc                    21-Apr-2016 16:14                 473
analysis-phonetic-2.3.2.jar.md5                    21-Apr-2016 16:14                  32
analysis-phonetic-2.3.2.jar.sha1                   21-Apr-2016 16:14                  40
analysis-phonetic-2.3.2.pom                        21-Apr-2016 16:14                1398
analysis-phonetic-2.3.2.pom.asc                    21-Apr-2016 16:14                 473
analysis-phonetic-2.3.2.pom.md5                    21-Apr-2016 16:14                  32
analysis-phonetic-2.3.2.pom.sha1                   21-Apr-2016 16:14                  40
analysis-phonetic-2.3.2.zip                        21-Apr-2016 16:14              273183
analysis-phonetic-2.3.2.zip.asc                    21-Apr-2016 16:14                 473
analysis-phonetic-2.3.2.zip.md5                    21-Apr-2016 16:14                  32
analysis-phonetic-2.3.2.zip.sha1                   21-Apr-2016 16:14                  40
```

Can we add them back?
</description><key id="153002809">18131</key><summary>Plugins jars are not published to maven central</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label></labels><created>2016-05-04T12:45:42Z</created><updated>2016-06-28T09:34:23Z</updated><resolved>2016-05-27T09:43:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-04T14:22:59Z" id="216880695">@dadoonet why do you need them?  don't they just need to be installed in elasticsearch?

/cc @rjernst 
</comment><comment author="spinscale" created="2016-05-04T14:25:05Z" id="216881335">the phonetic analyzer is just a bit of glue code (exposing a lucene analyzer via ES core), not sure if we need to expose the jar here? 

I tried to write a plugin and wanted to use painless as a scripting engine for testing, but that one is not exposed either due to being a module. I could have of course mocked my own, but painless looked eas to use, except the dependency missing.
</comment><comment author="nik9000" created="2016-05-04T14:27:13Z" id="216882013">&gt; I tried to write a plugin and wanted to use painless as a scripting engine for testing

This seems like a fair reason, though, to be honest, ES core requires that all of its plugins be tested in isolation so maybe we have the same requirements for plugins? I dunno.
</comment><comment author="dadoonet" created="2016-05-04T14:29:06Z" id="216882561">First reason is that because we have this JAR generated so we should upload it.
There are some code in there anyone can use.

Now for my use case, I'm testing an ingest plugin I wrote. I want to test it using a Mock Node which starts:
- my plugin
- the geo ip plugin

I can register my plugin in my Mock node but not the geoip one.
</comment><comment author="rjernst" created="2016-05-04T20:24:32Z" id="216991801">&gt; First reason is that because we have this JAR generated so we should upload it.

The purpose of deploying to maven is so users can use the software, but plugins do not require a jar in maven.

&gt; There are some code in there anyone can use.

We are not a library, and especially our plugins are not libraries. Their purpose is to add additional capabilities to elasticsearch, and should remain with that narrow purpose.

&gt; Now for my use case, I'm testing an ingest plugin I wrote. I want to test it using a Mock Node

Don't do that. We have real integration test capabilities, and if you want to test "with other plugins", then it should be done through those integration tests, which would install the plugin via the zip, not a fantasy land test that drags in a plugin into the classpath.

The integ test setup in gradle for adding plugins currently expects a gradle project as input. However, I think we can extend this to take a dependency string, so it would pull the plugin zip from maven.
</comment><comment author="nik9000" created="2016-05-04T20:28:46Z" id="216992878">&gt; The purpose of deploying to maven is so users can use the software, but plugins do not require a jar in maven.

Actually! If they expose anything over the java API I think they do?

I would love us to get our test tooling working so it can be used from outside the project to test plugins with the zip though.
</comment><comment author="rjernst" created="2016-05-04T20:30:34Z" id="216993344">&gt;  If they expose anything over the java API I think they do?

That is true, but is only necessary until we have an http client I think?
</comment><comment author="nik9000" created="2016-05-04T20:35:02Z" id="216994608">&gt; That is true, but is only necessary until we have an http client I think?

I don't know if we can tell people who use a plugin or module that they have to move to the java client earlier than users of just the core so I think we'll need to publish the jars at least until the rest based java client is **awesome**. I haven't a clue how long that'll take though.
</comment><comment author="jasontedor" created="2016-05-04T20:46:41Z" id="216997623">&gt; Actually! If they expose anything over the java API I think they do?

Do we have plugins that expose additional Java APIs?
</comment><comment author="nik9000" created="2016-05-04T21:06:31Z" id="217002824">&gt; Do we have plugins that expose additional Java APIs?

The reindex module and delete-by-query plugin both do. I'm not sure it the ingest-grok module or the other ingest plugins do but they might. I'm fine with only publishing jars for things that add to the java api.
</comment><comment author="jasontedor" created="2016-05-04T21:31:44Z" id="217010006">&gt; I'm fine with only publishing jars for things that add to the java api.

That's exactly where I was going. I'd argue though that we are making the same mistake we are making in core right now where there is an API that we want to expose, but we produce only a monolithic jar and so end-users end up depending on the Internet that Maven downloads.
</comment><comment author="dadoonet" created="2016-05-04T21:32:15Z" id="217010133">IMHO the situation is simple.

We are building jar. We assemble then this JAR within a ZIP.
Let's publish the JAR and the ZIP. Simple as that.

If we don't want to publish a JAR, then there is no need to build one. Just copy the classes and the ressources within the ZIP.

I don't think we should think instead of devs or users by reducing what they might want to do with what we are building.

If a user can start a Node within an integration test he is writing without the need of depending on our test infra, that's perfectly fine.
If a user wants to use ant, ivy or maven to build his project (plugin or not), that's also fine to me.

For the case I reported, I'm building a plugin with Maven. I don't want to use the REST tests which are launching an external node. As the plugin developper, it's my decision to run the tests as I want. It can be manual tests with a shell script. It can be tests run from JUnit. Whatever. I'll choose the most efficient tool for me.
The only contract I should respect regarding elasticsearch is the interfaces I need to implement.
And I'm perfectly fine saying that the only **official and supported** way to create a plugin is by using gradle as the build system and run REST tests using the yaml suite we defined.

We used to publish the jars at the same time we were publishing the zip. We are still building the JAR. Is that super complicated to upload the JAR to maven central which is used within the ZIP file?
</comment><comment author="rjernst" created="2016-05-04T21:59:55Z" id="217016735">&gt; Is that super complicated to upload the JAR to maven central which is used within the ZIP file?

Yes and no. It was sort of broken how we were doing it before. We publish the zip and the jar with the same artifact id, and in fact the pom is for the jar. It means when getting the zip, it appears that it has dependencies, when it doesn't really.

&gt; If we don't want to publish a JAR, then there is no need to build one. Just copy the classes and the ressources within the ZIP.

This is simply not how plugins work. The way we create child classloaders is by adding jar files. We should not add everything in the zip to the child classloader.
</comment><comment author="nik9000" created="2016-05-04T22:00:57Z" id="217016958">&gt; That's exactly where I was going. I'd argue though that we are making the same mistake we are making in core right now where there is an API that we want to expose, but we produce only a monolithic jar and so end-users end up depending on the Internet that Maven downloads.

We could certainly try to separate the client stuff, the common stuff, and the implementation but I don't think it is worth the effort if we expect there will be no more client at some point in the future. I'm ok with doing it how we've been doing it so long as the transport client is a known dead end that we only have to support while we are replacing it. I don't like having to make new things for the transport client but I can live with it, placing great hope in @javanna's work on the rest based java client.

I'm not really sure how to respond to @dadoonet's point. Personally I think we should publish the jars so users can do whatever they want with them. I also agree that ESIntegTestCase is a crutch and it'd be great not to have to use it and instead have the build run a separate process. I think that is reasonably easy to do by depending on Elasticsearch's build. But if you for some reason can't or won't use gradle then ESIntegTestCase is really the most convenient thing. And for some things, convenience is more important than doing it right. Actually a lot of the time.... But then again we, Elasticsearch committers, have to keep ESIntegTestCase alive for all that to work.

My opinion is still that we should publish the jars and encourage people to use the gradle builds. When we remove the last user of ESIntegTestCase from Elasticsearch we delete it and have a public wake, celebrating its long, glorious life and the great service it did Elasticsearch in 0.90 and 1.x. By then we'll have something better for people to use, or everyone will be using gradle, or it won't matter for some other reason. Or we'll help people.
</comment><comment author="clintongormley" created="2016-05-06T09:20:19Z" id="217394608">Discussed in FixItFriday - we should only publish jars that expose an API that the client needs to program against.  Once the HTTP client becomes the standard, we won't even need to do that.
</comment><comment author="dadoonet" created="2016-05-27T09:43:23Z" id="222105133">Closing as we are publishing the ZIP file on maven central and the outcome of the previous discussion is that we are not going to publish jars but only zip files.

BTW I wonder then if it makes sense to publish our plugins on maven central after all. We are publishing them on our download service. May be it's not needed to bring all plugins ZIP to maven central.
</comment><comment author="nomoa" created="2016-05-31T20:54:50Z" id="222817373">As a plugin developer it was extremely convenient to have the jars deployed:
- add a test dependency, implements nodePlugins.

Concerning classloader issue, I totally agree and to be honest I spotted one issue only when I deployed on a test node.

But what's the suggested solution here? Do I have to refactor my build system and switch to gradle?

Thanks.
</comment><comment author="jprante" created="2016-06-01T17:59:32Z" id="223074754">As a plugin developer, I am instructing my users to rely on published jars. This is essential for reusability. Not only exposed APIs, but all the classes and resources. Plugin ZIPs are just an additional service for non-developers, but not essential.

So I agree with @dadoonet why Elastic is even publishing plugins on Maven Central at all, and not just pointing users to a download zip URL somewhere.

I can only see the solution is in copy/paste all the ES plugin code and publish them to Maven Central again under my "org.xbib" group but with all regular Java artifacts.
</comment><comment author="jprante" created="2016-06-01T18:05:52Z" id="223076603">I started with a "plugin bundle" of mine on Maven Central, see http://search.maven.org/#search%7Cga%7C1%7Cxbib

This could be the way to publish ES core plugins too.

Sad thing is that current Elasticsearch plugin tool does not find plugin ZIPs from Maven Central generated by a classifier "plugin". But that is not painful to me since I do not use Elasticsearch plugin tool, instead I unzip plugin archives manually and build Docker images.
</comment><comment author="dadoonet" created="2016-06-08T13:11:33Z" id="224583874">For the record, we have the same concern with modules. See #16466.
https://discuss.elastic.co/t/percolator-java-api-5-0-0-alpha3/52161/2
</comment><comment author="rjernst" created="2016-06-14T20:41:40Z" id="226009551">&gt; So I agree with @dadoonet why Elastic is even publishing plugins on Maven Central at all, and not just pointing users to a download zip URL somewhere

Plugin zips are published to maven so that they can be tested with. In gradle, the integ test setup allows adding arbitrary extra plugins.

&gt; we should only publish jars that expose an API that the client needs to program against

I can add this to the build setup to happen automatically (via a flag in build.gradle) for those modules/plugins that need it. But we will need to use a different artifact id (eg right now in 2.x, we put up a pom for the jar with dependencies, and then the zip just sits alongside it with the same pom, but in 5.x we put up a pom that has no deps and basically is there to mark the zip as being a binary artifact). So I would propose taking the module/plugin name and appending `-client` for the artifact id of the jar that will be published (again, only for those plugins/modules which have things to add to the transport client).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count by query should be "POST" type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18130</link><project id="" key="" /><description /><key id="152988814">18130</key><summary>Count by query should be "POST" type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AshishAAB</reporter><labels /><created>2016-05-04T11:25:49Z</created><updated>2016-05-04T11:30:35Z</updated><resolved>2016-05-04T11:30:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AshishAAB" created="2016-05-04T11:30:01Z" id="216833734">Signing the CLA and re-opening
</comment><comment author="dadoonet" created="2016-05-04T11:30:35Z" id="216833832">No. GET is fine here.

But thanks for the contribution!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Consolidate query generation in QueryShardContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18129</link><project id="" key="" /><description>Currently we have a lot of methods left in QueryShardContext that take parsers or BytesReference arguments to do some xContent
parsing on the shard. While this still seems necessary in some cases (e.g. percolation, phrase suggester), the shard context should only
be concerned with generating lucene queries from QueryBuilders.

This change removes all of the parseX() methods in favour of two public methods toQuery(QueryBuilder) and toFilter(QueryBuilder) that
either call the query builders toFilter() or toQuery() method and move all code required for parsing out to the respective callers.
</description><key id="152980385">18129</key><summary>Consolidate query generation in QueryShardContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T10:32:09Z</created><updated>2016-05-04T14:53:38Z</updated><resolved>2016-05-04T14:53:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-04T12:49:43Z" id="216852740">LGTM
</comment><comment author="cbuescher" created="2016-05-04T13:55:59Z" id="216872290">@jpountz thanks for the review, and bc. of #18133 I will also revert any unbounded type parameters I added here.
</comment><comment author="jpountz" created="2016-05-04T14:12:11Z" id="216877495">Feel free to merge before I do if you want, I can fix the merge conflicts myself.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18128</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="152976871">18128</key><summary>2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hanzhuzu</reporter><labels /><created>2016-05-04T10:10:27Z</created><updated>2016-05-04T10:11:27Z</updated><resolved>2016-05-04T10:11:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-04T10:11:27Z" id="216820221">Opened by mistake I guess.

Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add the ability to use the breadth_first mode with nested aggregations (such as `top_hits`) which require access to score information.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18127</link><project id="" key="" /><description>The score is recomputed lazily for each document belonging to a top bucket.
Relates to #9825
</description><key id="152963261">18127</key><summary>Add the ability to use the breadth_first mode with nested aggregations (such as `top_hits`) which require access to score information.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T09:00:18Z</created><updated>2016-05-04T14:41:07Z</updated><resolved>2016-05-04T13:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-04T09:00:26Z" id="216798876">@jpountz I misunderstood how the DeferringBucketCollector works especially this part:
The `order` parameter can still be used to refer to data from a child aggregation when using the `breadth_first` setting - the parent aggregation understands that this child aggregation will need to be called first before any of the other child aggregations.

This means that we don't need to make it smarter ;).
</comment><comment author="jpountz" created="2016-05-04T12:33:36Z" id="216846984">Thanks @jimferenczi. I left some comments.
</comment><comment author="jimczi" created="2016-05-04T12:52:46Z" id="216853366">Thanks @jpountz.
I pushed https://github.com/elastic/elasticsearch/pull/18127/commits/d01ab76a43029daf7d6ac029f539325cbab49ef5 to address your comments.
</comment><comment author="jpountz" created="2016-05-04T13:28:48Z" id="216865028">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.util.ConcurrentModificationException could be sent by ingest service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18126</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha2
**Description of the problem including expected versus actual behavior**: java.util.ConcurrentModificationException could be sent by ingest service
**Provide logs (if relevant)**:

```
[2016-05-04 09:53:17,737][WARN ][cluster.service          ] [Airstrike] failed to notify ClusterStateListener
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextNode(HashMap.java:1429)
    at java.util.HashMap$KeyIterator.next(HashMap.java:1453)
    at org.elasticsearch.ingest.PipelineExecutionService.updatePipelineStats(PipelineExecutionService.java:127)
    at org.elasticsearch.ingest.PipelineExecutionService.clusterChanged(PipelineExecutionService.java:120)
    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:652)
    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

According to @martijnvg:

&gt; removing an element from a map while iterating its keys causes this expection
&gt; instead an iterator should be used
</description><key id="152956700">18126</key><summary>java.util.ConcurrentModificationException could be sent by ingest service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-04T08:30:57Z</created><updated>2016-05-06T13:00:41Z</updated><resolved>2016-05-06T13:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update java client.asciidoc - Settings.settingsBuilder() removal.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18125</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

as of https://github.com/elastic/elasticsearch/commit/42526ac28e07da0055faafca1de6f8c5ec96cd85
5.0.0 alpha2 have no settingsBuilder() method.
</description><key id="152930721">18125</key><summary>Update java client.asciidoc - Settings.settingsBuilder() removal.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">jeesim2</reporter><labels><label>docs</label></labels><created>2016-05-04T05:08:27Z</created><updated>2016-05-04T05:21:12Z</updated><resolved>2016-05-04T05:20:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-04T05:21:12Z" id="216748169">Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query_phase_execution_exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18124</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.1

**JVM version**: 1.8

**OS version**: CentOS 6.7

**Description of the problem including expected versus actual behavior**:[query_phase_execution_exception] Result window is too large, from + size must be less than or equal to: [10000] but was [45120]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.
</description><key id="152869676">18124</key><summary>query_phase_execution_exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kalwar</reporter><labels /><created>2016-05-03T20:43:03Z</created><updated>2016-05-03T20:54:08Z</updated><resolved>2016-05-03T20:50:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-03T20:50:46Z" id="216660517">You specified to large of a result window, the soft limit is 10000 and you specified 45120. You can increase this with the setting `index.max_result_window`. But large result windows are deadly for your system, that's why this soft limit is in place. Instead you should use the [scroll API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When searching or creating a document with a fields content with a forward slash followed by a capital F causes a json parse exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18123</link><project id="" key="" /><description>**Elasticsearch version**: 2.3

**JVM version**:

**OS version**: UBUNTU

**Description of the problem including expected versus actual behavior**:
When running an exact phrase match  or creating a document with a forward slash followed by a capital F causes elastic to throw an exception, however if it is lower cased the query works fine.

**Steps to reproduce**:
 1.
PUT /movies/movie/5
{
    "title": "The God\Father",
    "director": "Francis Ford Coppola",
    "year": 1972,
    "genres": ["Crime", "Drama"]
}

 2.

GET /movies/_search
{
"query": {
  "query_string": {
    "fields" : ["title"],
    "query": "\"The God\Father\""
   }
  }
}
</description><key id="152865633">18123</key><summary>When searching or creating a document with a fields content with a forward slash followed by a capital F causes a json parse exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels /><created>2016-05-03T20:22:35Z</created><updated>2016-05-03T21:18:57Z</updated><resolved>2016-05-03T20:30:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-03T20:30:51Z" id="216655395">A character preceded by a single backslash is an [escape sequence](https://en.wikipedia.org/wiki/Escape_sequence). For example, `\n` represents a newline. However, `\F` is not a valid escape sequence and that's why you will get a JSON parse exception. The reason that you do not get such an exception with `\f` is due to this being a valid escape sequence representing the form-feed character.
</comment><comment author="imranazad" created="2016-05-03T21:03:52Z" id="216663989">Thank you I'm aware that it's an escape sequence however how would I do an exact phrase match for the \"The God\Father\"" I have tried escaping the back slash with a forward slash like so \"The God\/Father\"" but I'm still not getting any results. Thanks.
</comment><comment author="jasontedor" created="2016-05-03T21:13:14Z" id="216666353">You escape the backslash by adding another backslash: `\\F`. And you probably want a match phrase query:

```
GET /movies/_search
{
  "query": {
    "match_phrase": { "title": "The God\\Father" }
  }
}
```

Please take any additional questions to the [Discourse forums](https://discuss.elastic.co) or appropriate [IRC channel](https://www.elastic.co/community). Elastic reserves GitHub for bug reports and feature requests.
</comment><comment author="imranazad" created="2016-05-03T21:16:29Z" id="216667127">@jasontedor Thank you. :+1: 
</comment><comment author="jasontedor" created="2016-05-03T21:18:57Z" id="216667766">&gt; Thank you. :+1: 

You're welcome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Vagrant Test Failure: qa:vagrant:vagrantFedora22#up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18122</link><project id="" key="" /><description>Build:
(https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+packaging-tests/344/console)

Failure:
:qa:vagrant:vagrantFedora22#up (Thread[main,5,main]) started.
:qa:vagrant:vagrantFedora22#up
Executing task ':qa:vagrant:vagrantFedora22#up' (up-to-date check took 0.0 secs) due to:
  Task has not declared any outputs.
Starting process 'command 'vagrant''. Working directory: /var/lib/jenkins/workspace/elastic+elasticsearch+master+packaging-tests/qa/vagrant Command: vagrant up fedora-22 --provision --provider virtualbox
Successfully started process 'command 'vagrant''
The SSH command responded with a non-zero exit status. Vagrant
assumes that this means the command failed. The output for this command
should be in the log above. Please read the output to determine what
went wrong.
:qa:vagrant:vagrantFedora22#up FAILED
:qa:vagrant:vagrantFedora22#up (Thread[main,5,main]) completed. Took 50.772 secs.

FAILURE: Build failed with an exception.
- What went wrong:
  Execution failed for task ':qa:vagrant:vagrantFedora22#up'.
  &gt; Process 'command 'vagrant'' finished with non-zero exit value 1

BUILD FAILED

Total time: 21 mins 5.763 secs
Stopped 0 compiler daemon(s).
- Try:
  Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.
  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; SCRIPT EXECUTION END &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
  DURATION: 1274811ms
  STDOUT: 180704 bytes
  STDERR: 1069 bytes
  WRAPPED PROCESS: FAILURE (1)
  BUILD: https://5086a1f436ee16623a447bdf25881bbc.us-east-1.aws.found.io:9243/build-1453259317148/t/20160503184620-1C840FDD
  NOTIFYING SLACK
  MAILING: dev+build-elasticsearch@e***.co
  Build step 'Execute shell' marked build as failure
  Sending e-mails to: infra-root+build@elastic.co
  Finished: FAILURE
</description><key id="152859331">18122</key><summary>Vagrant Test Failure: qa:vagrant:vagrantFedora22#up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>test</label></labels><created>2016-05-03T19:49:37Z</created><updated>2016-10-18T07:48:38Z</updated><resolved>2016-10-18T07:48:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-10-18T07:48:38Z" id="254432553">Closing as old
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure: org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18121</link><project id="" key="" /><description>Reproduce:
gradle :core:integTest -Dtests.seed=A46FAA84C4686F93 -Dtests.class=org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT -Dtests.method="testReadonlyRepository" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=en-MT -Dtests.timezone=Kwajalein

Build Failure:
(https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=sles/349/console)

Stack Trace:
Suite: org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT
  1&gt; [2016-05-03 18:01:40,772][WARN ][org.elasticsearch.snapshots] [node_s0] failed to create snapshot [test-repo:test-snap]
  1&gt; SnapshotCreationException[[test-repo:test-snap] failed to create snapshot]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.initializeSnapshot(BlobStoreRepository.java:284)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository.initializeSnapshot(MockRepository.java:131)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService.beginSnapshot(SnapshotsService.java:310)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService.access$500(SnapshotsService.java:96)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService$1$1.run(SnapshotsService.java:232)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:275)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:339)
  1&gt;    at org.elasticsearch.repositories.blobstore.ChecksumBlobStoreFormat.writeBlob(ChecksumBlobStoreFormat.java:182)
  1&gt;    at org.elasticsearch.repositories.blobstore.ChecksumBlobStoreFormat.write(ChecksumBlobStoreFormat.java:154)
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.initializeSnapshot(BlobStoreRepository.java:281)
  1&gt;    ... 8 more
  1&gt; [2016-05-03 18:01:42,679][WARN ][org.elasticsearch.snapshots] [node_s1] [[test-idx][7]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][7]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:42,688][WARN ][org.elasticsearch.snapshots] [node_s0] [[test-idx][0]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][0]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:42,692][WARN ][org.elasticsearch.snapshots] [node_s1] [[test-idx][1]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][1]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:42,699][WARN ][org.elasticsearch.snapshots] [node_s1] [[test-idx][5]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][5]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:42,704][WARN ][org.elasticsearch.snapshots] [node_s1] [[test-idx][3]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][3]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:42,729][WARN ][org.elasticsearch.snapshots] [node_s0] [[test-idx][6]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][6]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  2&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=A46FAA84C4686F93 -Dtests.class=org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT -Dtests.method="testReadonlyRepository" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=en-MT -Dtests.timezone=Kwajalein
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:42,735][WARN ][org.elasticsearch.snapshots] [node_s0] [[test-idx][4]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][4]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:42,739][WARN ][org.elasticsearch.snapshots] [node_s0] [[test-idx][2]] [test-repo:test-snap] failed to create snapshot
  1&gt; [test-idx/zAAHYkFOT92e-uEZhSVlZw][[test-idx][2]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException[Random IOException];
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:600)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:183)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:343)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:79)
  1&gt;    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:299)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.IOException: Random IOException
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
  1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.writeBlob(MockRepository.java:345)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:652)
  1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:598)
  1&gt;    ... 9 more
  1&gt; [2016-05-03 18:01:46,474][WARN ][org.elasticsearch.snapshots] [node_s0] failed to get snapshot [test-repo:test-snap-2]
  1&gt; java.lang.IllegalStateException: class org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards (pos=-7 getFilePointer()=0)
  1&gt;    at org.apache.lucene.store.ChecksumIndexInput.seek(ChecksumIndexInput.java:50)
  1&gt;    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:468)
  1&gt;    at org.elasticsearch.repositories.blobstore.ChecksumBlobStoreFormat.readBlob(ChecksumBlobStoreFormat.java:106)
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreFormat.read(BlobStoreFormat.java:86)
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.readSnapshot(BlobStoreRepository.java:438)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService.snapshots(SnapshotsService.java:154)
  1&gt;    at org.elasticsearch.action.admin.cluster.snapshots.get.TransportGetSnapshotsAction.masterOperation(TransportGetSnapshotsAction.java:80)
  1&gt;    at org.elasticsearch.action.admin.cluster.snapshots.get.TransportGetSnapshotsAction.masterOperation(TransportGetSnapshotsAction.java:49)
  1&gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction.masterOperation(TransportMasterNodeAction.java:77)
  1&gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:161)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-05-03 18:01:47,339][WARN ][org.elasticsearch.repositories] [node_s0] failed to create repository [test-repo]
  1&gt; java.lang.IllegalStateException: trying to modify or unregister repository that is currently used 
  1&gt;    at org.elasticsearch.repositories.RepositoriesService.ensureRepositoryNotInUse(RepositoriesService.java:421)
  1&gt;    at org.elasticsearch.repositories.RepositoriesService.access$000(RepositoriesService.java:60)
  1&gt;    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:113)
  1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-05-03 18:01:51,728][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo:test-snap] failed to restore snapshot
  1&gt; SnapshotRestoreException[[test-repo:test-snap] cannot modify setting [index.number_of_shards] on restore]
  1&gt;    at org.elasticsearch.snapshots.RestoreService$1.updateIndexSettings(RestoreService.java:420)
  1&gt;    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:242)
  1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-05-03 18:01:51,732][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo:test-snap] failed to restore snapshot
  1&gt; java.lang.IllegalArgumentException: must specify non-negative number of shards for index [test-idx]
  1&gt;    at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.build(IndexMetaData.java:854)
  1&gt;    at org.elasticsearch.snapshots.RestoreService$1.updateIndexSettings(RestoreService.java:426)
  1&gt;    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:242)
  1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-05-03 18:02:37,197][WARN ][org.elasticsearch.repositories.fs] [node_s0] cannot read snapshot file [test-repo:test-snap-1]
  1&gt; java.lang.IllegalStateException: class org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards (pos=-14 getFilePointer()=0)
  1&gt;    at org.apache.lucene.store.ChecksumIndexInput.seek(ChecksumIndexInput.java:50)
  1&gt;    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:468)
  1&gt;    at org.elasticsearch.repositories.blobstore.ChecksumBlobStoreFormat.readBlob(ChecksumBlobStoreFormat.java:106)
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreFormat.read(BlobStoreFormat.java:86)
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.readSnapshot(BlobStoreRepository.java:438)
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.deleteSnapshot(BlobStoreRepository.java:299)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService$8.run(SnapshotsService.java:1010)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-05-03 18:02:40,923][WARN ][org.elasticsearch.snapshots] [node_s0] failed to create snapshot [readonly-repo:test-snap-2]
  1&gt; RepositoryException[[readonly-repo] cannot create snapshot in a readonly repository]
  1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.initializeSnapshot(BlobStoreRepository.java:268)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService.beginSnapshot(SnapshotsService.java:310)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService.access$500(SnapshotsService.java:96)
  1&gt;    at org.elasticsearch.snapshots.SnapshotsService$1$1.run(SnapshotsService.java:232)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
ERROR   0.90s J2 | SharedClusterSnapshotRestoreIT.testReadonlyRepository &lt;&lt;&lt; FAILURES!

&gt; Throwable #1: java.lang.IllegalArgumentException: Cannot delete indices that are being snapshotted: [[test-idx/XCtt1HdASBWhR200DDsvWg]]. Try again after snapshot finishes or cancel the currently running snapshot.
&gt;    at **randomizedtesting.SeedInfo.seed([A46FAA84C4686F93:F99ECF613C58E47B]:0)
&gt;    at org.elasticsearch.snapshots.SnapshotsService.checkIndexDeletion(SnapshotsService.java:1072)
&gt;    at org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$1.execute(MetaDataDeleteIndexService.java:90)
&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; [2016-05-03 18:02:41,531][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test-idx][0]] marking and sending shard failed due to [failed recovery]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,533][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=_g39LpVlTgWx__Z-6AjY0w], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=_g39LpVlTgWx__Z-6AjY0w], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,777][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test-idx][4]] marking and sending shard failed due to [failed recovery]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,778][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][4] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=Nz99tjKpQs6ewMbg-bfIiw], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=Nz99tjKpQs6ewMbg-bfIiw], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,780][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][4] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=Nz99tjKpQs6ewMbg-bfIiw], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=Nz99tjKpQs6ewMbg-bfIiw], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,781][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=_g39LpVlTgWx__Z-6AjY0w], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=_g39LpVlTgWx__Z-6AjY0w], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,787][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][4] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=Nz99tjKpQs6ewMbg-bfIiw], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=Nz99tjKpQs6ewMbg-bfIiw], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,787][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=_g39LpVlTgWx__Z-6AjY0w], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=_g39LpVlTgWx__Z-6AjY0w], unassigned_info[[reason=NEW_INDEX_RESTORED], at[2016-05-03T18:02:41.462Z], details[restore_source[test-repo/test-snap]]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,822][WARN ][org.elasticsearch.indices.cluster] [node_s1] [[test-idx][0]] marking and sending shard failed due to [failed recovery]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,857][WARN ][org.elasticsearch.indices.cluster] [node_s1] [[test-idx][4]] marking and sending shard failed due to [failed recovery]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,862][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=y9U_fNTiR-6V3fUkbxH78Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=y9U_fNTiR-6V3fUkbxH78Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,875][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][4] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=h5bJOlm7QmKILC4NEmp1gQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=h5bJOlm7QmKILC4NEmp1gQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,878][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][4] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=h5bJOlm7QmKILC4NEmp1gQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][4], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=h5bJOlm7QmKILC4NEmp1gQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][4]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,882][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=y9U_fNTiR-6V3fUkbxH78Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=y9U_fNTiR-6V3fUkbxH78Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.789Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,903][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test-idx][0]] marking and sending shard failed due to [failed recovery]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,907][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=f0CRd2TDRgyOyYPiAMgyUQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.880Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[0actTwozTcOFyQcVEeZUHg], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=f0CRd2TDRgyOyYPiAMgyUQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.880Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,925][WARN ][org.elasticsearch.indices.cluster] [node_s1] [[test-idx][0]] marking and sending shard failed due to [failed recovery]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,935][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]
&gt;   1&gt; [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:165)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:98)
&gt;   1&gt;    at org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1115)
&gt;   1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$3(IndicesClusterStateService.java:673)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:260)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$1(StoreRecovery.java:100)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
&gt;   1&gt;    ... 7 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:207)
&gt;   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:255)
&gt;   1&gt;    ... 9 more
&gt;   1&gt; Caused by: [test-idx/xndMMqOGRoScqfN18YskFQ][[test-idx][0]] IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException];
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:866)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:205)
&gt;   1&gt;    ... 10 more
&gt;   1&gt; Caused by: java.io.IOException: Random IOException
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.maybeIOExceptionOrBlock(MockRepository.java:259)
&gt;   1&gt;    at org.elasticsearch.snapshots.mockstore.MockRepository$MockBlobStore$MockBlobContainer.readBlob(MockRepository.java:303)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$PartSliceStream.openSlice(BlobStoreIndexShardRepository.java:768)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.nextStream(SlicedInputStream.java:53)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.currentStream(SlicedInputStream.java:67)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.SlicedInputStream.read(SlicedInputStream.java:88)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:133)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.RateLimitingInputStream.read(RateLimitingInputStream.java:69)
&gt;   1&gt;    at java.io.FilterInputStream.read(FilterInputStream.java:107)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:923)
&gt;   1&gt;    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:863)
&gt;   1&gt;    ... 11 more
&gt;   1&gt; [2016-05-03 18:02:41,947][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,969][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:41,979][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test-idx][0] received shard failed for target shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], source shard [[[test-idx/xndMMqOGRoScqfN18YskFQ]][0], node[SwMin9aMQZi6_RkwE9MWZw], [P], restoring[test-repo:test-snap], s[INITIALIZING], a[id=KXbgjCJhSHepoIl9XAcz4Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-05-03T18:02:41.912Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IndexShardRestoreFailedException[restore failed]; nested: IndexShardRestoreFailedException[failed to restore snapshot [test-snap]]; nested: IndexShardRestoreFailedException[Failed to recover index]; nested: IOException[Random IOException]; ]]], message [master {node_s0}{0actTwozTcOFyQcVEeZUHg}{local}{local[54]} marked shard as initializing, but shard is marked as failed, resend shard failure]
&gt;   1&gt; [2016-05-03 18:02:46,885][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo][test-snap] failed to restore snapshot
&gt;   1&gt; SnapshotRestoreException[[test-repo:test-snap] indices [test-idx-2] and [test-idx-1] are renamed into the same index [same-name]]
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService.renamedIndices(RestoreService.java:698)
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:210)
&gt;   1&gt;    at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
&gt;   1&gt;    at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
&gt;   1&gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction.masterOperation(TransportMasterNodeAction.java:77)
&gt;   1&gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:161)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; [2016-05-03 18:02:46,898][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo][test-snap] failed to restore snapshot
&gt;   1&gt; SnapshotRestoreException[[test-repo:test-snap] indices [test-idx-2] and [test-idx-1] are renamed into the same index [test-idx-1]]
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService.renamedIndices(RestoreService.java:698)
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:210)
&gt;   1&gt;    at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
&gt;   1&gt;    at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
&gt;   1&gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction.masterOperation(TransportMasterNodeAction.java:77)
&gt;   1&gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:161)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; [2016-05-03 18:02:46,901][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo:test-snap] failed to restore snapshot
&gt;   1&gt; [__WRONG**] InvalidIndexNameException[Invalid index name [**WRONG**], must not start with '_']
&gt;   1&gt;    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:152)
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:255)
&gt;   1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; [2016-05-03 18:02:46,920][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo:test-snap] failed to restore snapshot
&gt;   1&gt; [alias-3] InvalidIndexNameException[Invalid index name [alias-3], already exists as alias]
&gt;   1&gt;    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:170)
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:255)
&gt;   1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   2&gt; NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/sles/core/build/testrun/integTest/J2/temp/org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT_A46FAA84C4686F93-001
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; [2016-05-03 18:02:46,923][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo:test-snap] failed to restore snapshot
&gt;   1&gt; SnapshotRestoreException[[test-repo:test-snap] cannot rename index [test-idx-1] into [alias-1] because of conflict with an alias with the same name]
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService$1.checkAliasNameConflicts(RestoreService.java:340)
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:318)
&gt;   2&gt; NOTE: test params are: codec=Asserting(Lucene60): {field1.keyword=PostingsFormat(name=Asserting), field1=PostingsFormat(name=Asserting), _field_names=PostingsFormat(name=Asserting), foo=PostingsFormat(name=Asserting), value.keyword=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), _timestamp=PostingsFormat(name=Asserting), value=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting), foo.keyword=PostingsFormat(name=Asserting)}, docValues:{field1.keyword=DocValuesFormat(name=Lucene54), value.keyword=DocValuesFormat(name=Lucene54), _type=DocValuesFormat(name=Lucene54), _version=DocValuesFormat(name=Asserting), _timestamp=DocValuesFormat(name=Asserting), foo.keyword=DocValuesFormat(name=Lucene54)}, maxPointsInLeafNode=101, maxMBSortInHeap=4.917083511936533, sim=ClassicSimilarity, locale=en-MT, timezone=Kwajalein
&gt;   2&gt; NOTE: Linux 3.12.51-52.31-default amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=4,threads=1,free=361959312,total=512753664
&gt;   2&gt; NOTE: All tests run in this JVM: [NoMasterNodeIT, UpdateNumberOfReplicasIT, PendingTasksBlocksIT, GetTermVectorsCheckDocFreqIT, ExtendedStatsBucketIT, NestedIT, NettyTransportIT, NettyPipeliningDisabledIT, CircuitBreakerNoopIT, CumulativeSumIT, SharedClusterSnapshotRestoreIT]
&gt;   1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; [2016-05-03 18:02:46,933][WARN ][org.elasticsearch.snapshots] [node_s0] [test-repo:test-snap] failed to restore snapshot
&gt;   1&gt; SnapshotRestoreException[[test-repo:test-snap] cannot rename index [test-idx-1] into [alias-2] because of conflict with an alias with the same name]
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService$1.checkAliasNameConflicts(RestoreService.java:340)
&gt;   1&gt;    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:318)
&gt;   1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:513)
&gt;   1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; [2016-05-03 18:02:47,842][WARN ][org.elasticsearch.repositories.fs] [node_s0] cannot read metadata for snapshot [test-repo:test-snap-1]
&gt;   1&gt; SnapshotMissingException[[test-repo:test-snap-1] is missing]; nested: NoSuchFileException[/var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/sles/core/build/testrun/integTest/J2/temp/org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT_A46FAA84C4686F93-001/tempDir-001/repos/AwWIiwkhuq/meta-test-snap-1.dat];
&gt;   1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.readSnapshotMetaData(BlobStoreRepository.java:470)
&gt;   1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.deleteSnapshot(BlobStoreRepository.java:309)
&gt;   1&gt;    at org.elasticsearch.snapshots.SnapshotsService$8.run(SnapshotsService.java:1010)
&gt;   1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   1&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   1&gt; Caused by: java.nio.file.NoSuchFileException: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/sles/core/build/testrun/integTest/J2/temp/org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT_A46FAA84C4686F93-001/tempDir-001/repos/AwWIiwkhuq/meta-test-snap-1.dat
&gt;   1&gt;    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
&gt;   1&gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
&gt;   1&gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
&gt;   1&gt;    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
&gt;   1&gt;    at java.nio.file.Files.newByteChannel(Files.java:361)
&gt;   1&gt;    at java.nio.file.Files.newByteChannel(Files.java:407)
&gt;   1&gt;    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
&gt;   1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newInputStream(FilterFileSystemProvider.java:192)
&gt;   1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newInputStream(FilterFileSystemProvider.java:192)
&gt;   1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newInputStream(FilterFileSystemProvider.java:192)
&gt;   1&gt;    at org.apache.lucene.mockfile.HandleTrackingFS.newInputStream(HandleTrackingFS.java:92)
&gt;   1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newInputStream(FilterFileSystemProvider.java:192)
&gt;   1&gt;    at org.apache.lucene.mockfile.HandleTrackingFS.newInputStream(HandleTrackingFS.java:92)
&gt;   1&gt;    at java.nio.file.Files.newInputStream(Files.java:152)
&gt;   1&gt;    at org.elasticsearch.common.blobstore.fs.FsBlobContainer.readBlob(FsBlobContainer.java:93)
&gt;   1&gt;    at org.elasticsearch.repositories.blobstore.ChecksumBlobStoreFormat.readBlob(ChecksumBlobStoreFormat.java:100)
&gt;   1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreFormat.read(BlobStoreFormat.java:86)
&gt;   1&gt;    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.readSnapshotMetaData(BlobStoreRepository.java:468)
&gt;   1&gt;    ... 6 more
&gt; Completed [66/286] on J2 in 73.05s, 36 tests, 1 error &lt;&lt;&lt; FAILURES!
</description><key id="152858217">18121</key><summary>Test Failure: org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Snapshot/Restore</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T19:43:57Z</created><updated>2016-05-20T14:54:22Z</updated><resolved>2016-05-20T14:54:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-05-13T15:41:45Z" id="219080801">Increased logger level to DEBUG and hope it fails again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build Failure: org.elasticsearch.search.aggregations.bucket.SignificantTermsSignificanceScoreIT.testScriptScore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18120</link><project id="" key="" /><description>Unable to replicate, but I do not have a Windows machine for testing.

REPRODUCE WITH: gradle :core:integTest -Dtests.seed=4DA46F7D8DF8CF05 -Dtests.class=org.elasticsearch.search.aggregations.bucket.SignificantTermsSignificanceScoreIT -Dtests.method="testScriptScore" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=sr-Latn-RS -Dtests.timezone=Asia/Chongqing

Build Failure: (http://build-us-00.elastic.co/job/es_core_master_window-2008/3699/testReport/junit/org.elasticsearch.search.aggregations.bucket/SignificantTermsSignificanceScoreIT/testScriptScore/)
</description><key id="152851232">18120</key><summary>Build Failure: org.elasticsearch.search.aggregations.bucket.SignificantTermsSignificanceScoreIT.testScriptScore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>bug</label><label>test</label><label>v5.1.1</label></labels><created>2016-05-03T19:08:26Z</created><updated>2016-10-27T13:30:45Z</updated><resolved>2016-10-27T13:30:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2016-05-04T18:18:51Z" id="216954565">Ditto - wouldn't replicate on OSX here. Any chance of a quick Windows test @costin ?
</comment><comment author="javanna" created="2016-07-22T07:26:32Z" id="234473540">This fails around 3/4 times a month in our CI. @jpountz pushed https://github.com/elastic/elasticsearch/commit/cad959b to ease debugging which fields are null.

It does not only fail on Windows. 

As far as I can see the null field is not always the same: I've seen `_superset_size` being null, and also  `_subset_freq`. Here is the latest failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/1601/console .

@markharwood would you mind taking another look? maybe @colings86 would like to check as well?
</comment><comment author="clintongormley" created="2016-10-18T07:49:15Z" id="254432674">@markharwood is this still a problem?
</comment><comment author="markharwood" created="2016-10-19T08:08:32Z" id="254743574">I've not seen any examples of failures in the available history:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/311/testReport/junit/org.elasticsearch.search.aggregations.bucket/SignificantTermsSignificanceScoreIT/history/?start=25 
</comment><comment author="javanna" created="2016-10-19T08:46:55Z" id="254752063">This is still a problem, last failure was two days ago: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/328/console . Still happens a few times a month.
</comment><comment author="markharwood" created="2016-10-19T08:58:31Z" id="254754799">OK, thanks. Will take a look
</comment><comment author="markharwood" created="2016-10-19T11:20:16Z" id="254784631">Could not reproduce with that seed over many iterations.

The debug code that Adrien added shows that the very first parameter it tries to retrieve (subset_freq) is null.
This is also the first of many non-null parameters set in ScriptHeuristic.initialize():

```
public void initialize(ExecutableScript executableScript) {
    this.executableScript = executableScript;
    this.executableScript.setNextVar("_subset_freq", subsetDfHolder);
    this.executableScript.setNextVar("_subset_size", subsetSizeHolder);
    this.executableScript.setNextVar("_superset_freq", supersetDfHolder);
    this.executableScript.setNextVar("_superset_size", supersetSizeHolder);
}
```

I notice elsewhere that there is a reference to the fact that the test framework can sometimes invoke script.run() before initialization is complete:

```
@Override
public double getScore(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize) {
    if (executableScript == null) {
        //In tests, wehn calling assertSearchResponse(..) the response is streamed one additional time with an arbitrary version, see assertVersionSerializable(..).
        // Now, for version before 1.5.0 the score is computed after streaming the response but for scripts the script does not exists yet.
        // assertSearchResponse() might therefore fail although there is no problem.
        // This should be replaced by an exception in 2.0.
        ESLoggerFactory.getLogger("script heuristic").warn("cannot compute score - script has not been initialized yet.");
        return 0;
    }
    subsetSizeHolder.value = subsetSize;
    supersetSizeHolder.value = supersetSize;
    subsetDfHolder.value = subsetFreq;
    supersetDfHolder.value = supersetFreq;
    return ((Number) executableScript.run()).doubleValue();
}
```

If this is possible/acceptable then maybe we should reverse the order of the operations in the initialize method - set the script variables up _and only then_ set this.executableScript to the fully initialized object. Any failure to set the variables properly will result in a null script which when getScore() is called will log the appropriate error message "script has not been initialized yet". 
</comment><comment author="colings86" created="2016-10-19T12:11:35Z" id="254794184">I remember talking to @brwe about that warning when she created the script score but I don't remember exactly why we needed it. @brwe do you remember?
</comment><comment author="cbuescher" created="2016-10-20T12:09:31Z" id="255087945">Here's another one: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.0+java9-periodic/367/console
</comment><comment author="markharwood" created="2016-10-20T13:33:08Z" id="255106743">Added what could be a fix in master: https://github.com/elastic/elasticsearch/commit/4a815bf6654b5e271942a62d3f2b0dc04c408de9

Will push to other branches if we conclude that this makes the difference
</comment><comment author="brwe" created="2016-10-20T16:47:41Z" id="255162201">@colings86 I do not remember all of it but believe this was an &lt; 2.0 problem and we should really replace the log message for script = null with an assertion. 
@markharwood I do not think that changing the order is a fix here but also do not fully understand what is going on yet. I will dig.
</comment><comment author="brwe" created="2016-10-21T15:42:41Z" id="255412092">I think the problem is that the ScriptHeuristic object is reused and hence accessed from different threads concurrently. How exactly this happens and if it is a problem of our tests only I cannot say yet because I am struggling to navigate aggregations. Will continue Monday. 
</comment><comment author="brwe" created="2016-10-24T12:40:16Z" id="255728956">If a coordinating node routes search requests to local shards they will not be streamed but the same aggregations builder object will be used for each individual shards that the request is executed on. When the aggregation is build significant terms heuristic is reused and hence several shards can end up with the same ScriptHeuristic object and also modify it concurrently. Instead the builders should make a copy and pass these to the aggregators. Hope I got the nomenclature right. I can work on a fix. 
</comment><comment author="colings86" created="2016-10-24T13:08:29Z" id="255735349">@brwe that makes sense, we have to do something similar for the Scripted Metric Aggregation
</comment><comment author="brwe" created="2016-10-24T13:35:08Z" id="255741809">One correction to what I wrote above: The script must be be reused either because we set variables in it. We need one instance per shard.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add whether the shard state fetch is pending to the allocation explain API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18119</link><project id="" key="" /><description>If the shard state fetch is still pending, this will now return a
message like:

``` json
{
  "shard" : {
    "index" : "i",
    "index_uuid" : "de1W1374T4qgvUP4a9Ieaw",
    "id" : 0,
    "primary" : false
  },
  "assigned" : false,
  "shard_state_fetch_pending": true,
  "unassigned_info" : {
    "reason" : "INDEX_CREATED",
    "at" : "2016-04-26T16:34:53.227Z"
  },
  "allocation_delay_ms" : 0,
  "remaining_delay_ms" : 0,
  "nodes" : {
    "z-CbkiELT-SoWT91HIszLA" : {
      "node_name" : "Brain Cell",
      "node_attributes" : {
        "testattr" : "test"
      },
      "store" : {
        "shard_copy" : "NONE"
      },
      "final_decision" : "NO",
      "final_explanation" : "the shard state fetch is pending",
      "weight" : 5.0,
      "decisions" : [ ]
    }
  }
}
```

Adds the `shard_state_fetch_pending` field and uses the state to
influence the final decision and final explanation.

Relates to #17372
</description><key id="152831848">18119</key><summary>Add whether the shard state fetch is pending to the allocation explain API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Stats</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T17:35:52Z</created><updated>2016-05-25T11:33:02Z</updated><resolved>2016-05-23T16:47:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-04T22:36:51Z" id="217024591">@ywelsch can you take a look at this?
</comment><comment author="dakrone" created="2016-05-17T15:38:18Z" id="219758273">@ywelsch I changed the way this works, instead of checking that flag I added a
method to the `GatewayAllocator` that takes a shard and checks whether checks
are in-flight for it, let me know if you think this is a better way to solve
this.
</comment><comment author="ywelsch" created="2016-05-18T07:51:20Z" id="219952080">@dakrone I like the change of adding a small method to `GatewayAllocator` to determine shard fetching status. I've added some suggestions to make `calculateNodeExplanation` more in line with actual decision making by Primary/ReplicaShardAlloctor and improvements to the error messages displayed to the user. One more thing I'm wondering about is whether/how we can test this feature using integration tests (ClusterAllocationExplainIT).
</comment><comment author="dakrone" created="2016-05-20T17:44:38Z" id="220672320">@ywelsch I pushed some more commits for this

&gt; One more thing I'm wondering about is whether/how we can test this feature using integration tests (ClusterAllocationExplainIT)

I'm not sure how to do this, I tried looking for tests for the async fetching, but they're all unit tests, I'm open to ideas if you have any
</comment><comment author="ywelsch" created="2016-05-23T10:16:21Z" id="220941578">I like `calculateNodeExplanation` better now. LGTM.
One way to test this is if we add a `NetworkPartition` rule that delays requests for a specific action (e.g. `internal:gateway/local/started_shards`). This can be done by generalizing `MockService.addUnresponsiveRule` with an additional `Predicate` parameter that checks if the request is to be delayed.
</comment><comment author="dakrone" created="2016-05-23T15:42:24Z" id="221013924">&gt; One way to test this is if we add a NetworkPartition rule that delays requests for a specific action (e.g. internal:gateway/local/started_shards). This can be done by generalizing MockService.addUnresponsiveRule with an additional Predicate parameter that checks if the request is to be delayed.

That's a good idea, thanks Yannick, I'll work on this as a subsequent PR
</comment><comment author="clintongormley" created="2016-05-24T09:08:30Z" id="221210204">@dakrone was this merged? if not, please remove the version label
</comment><comment author="dakrone" created="2016-05-24T15:50:52Z" id="221315914">@clintongormley this was merged (with a merge commit no less!) but the Github hiccups that were going on yesterday caused this to be marked as closed instead of merged
</comment><comment author="clintongormley" created="2016-05-25T11:33:02Z" id="221546652">Closed by bfce901edf3a07c211b3892b93e24e95f1ae38ca
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Grammar change in error response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18118</link><project id="" key="" /><description>The error message on line 61 should say "an HTTP port" not "a HTTP port".

PR https://github.com/elastic/elasticsearch/pull/18117 has been opened.

Thanks,
Fl0yd
</description><key id="152821733">18118</key><summary>Grammar change in error response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fl0yd</reporter><labels><label>:Exceptions</label><label>non-issue</label></labels><created>2016-05-03T16:47:51Z</created><updated>2016-05-13T02:47:46Z</updated><resolved>2016-05-13T02:47:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-13T02:47:46Z" id="218939477">No feedback on #18117, closing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Grammar fix in transport exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18117</link><project id="" key="" /><description>Updating the output to be more grammatically correct.

Closes #18118
</description><key id="152821639">18117</key><summary>Grammar fix in transport exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fl0yd</reporter><labels><label>feedback_needed</label></labels><created>2016-05-03T16:47:29Z</created><updated>2016-05-13T02:46:55Z</updated><resolved>2016-05-13T02:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-03T16:56:05Z" id="216594436">Did you [run the tests](https://github.com/elastic/elasticsearch/blob/83973288b0ab55075c18f48d93a78697664ec900/.github/PULL_REQUEST_TEMPLATE.md)? I think that this will break the `NettySizeHeaderFrameDecoderTests#testThatTextMessageIsReturnedOnHTTPLikeRequest` test. Can you check, update the test accordingly, and run the full test suite as our pull request guidelines request?
</comment><comment author="jasontedor" created="2016-05-13T02:46:55Z" id="218939366">No feedback, closing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reorganize scripting documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18116</link><project id="" key="" /><description>The current scripting documentation (https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html) is a bit chaotic. 

I think I made it even worse recently when elaborating on the expressions documentation, simply by adding more to the huge page.

Instead I think we should factor out language-specific stuff to its own pages. This helps to organize things better, for example all the crazy security complexities of `groovy` are not relevant to languages like `painless` and `expressions`. We still need to keep it "front and center" so that groovy users are aware, but we gotta move forward too!

I also split the table into "general purpose languages" (painless, groovy, javascript, python) and "special purpose languages" (expressions, mustache, native), along with what makes them special. 

I removed the experimental tag from `expressions`, I feel like this should be ok? I think reality is quite the opposite, its the simplest, most stable, fastest one we have. We shouldn't discourage people from using it. Its been around since lucene 4.6 and we are looking at lucene 6.0 right now :)

It still feels like we are just begging people to use groovy, since all the examples use it, but I'd rather just leave that be for now, and try to improve the "path" to the "good stuff" (expressions/painless) and improve their docs at the moment. This is just a step...

And security stuff is factored out to a dedicated page, but I made lots of links to it, including very early in the page. 

Suggestions are welcome, I barely even know how to use asciidoc. I also am still not happy with both the main page and many of the subpages, I think we can improve this a lot more. 
</description><key id="152818254">18116</key><summary>Reorganize scripting documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>docs</label></labels><created>2016-05-03T16:32:05Z</created><updated>2016-05-04T16:17:10Z</updated><resolved>2016-05-04T16:17:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-03T18:43:32Z" id="216625964">@jdconrad @rjernst @clintongormley if you guys have time, can you look at this change? 
</comment><comment author="jdconrad" created="2016-05-03T19:01:14Z" id="216631579">This looks like a big improvement.  Thanks for doing this!  I'd be more comfortable if @rjernst @clintongormley also took a look.
</comment><comment author="clintongormley" created="2016-05-03T19:23:26Z" id="216637751">@rmuir this is an awesome change. thanks for doing it.  I've pulled it locally to use as a starting point to do a bigger reorganisation (and tidying up some asciidoc stuff).  
</comment><comment author="rmuir" created="2016-05-03T19:26:15Z" id="216638554">ok, thanks in advance for any tidying up!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update function-score-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18115</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="152816281">18115</key><summary>Update function-score-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kels17</reporter><labels><label>docs</label></labels><created>2016-05-03T16:23:11Z</created><updated>2016-05-03T17:29:30Z</updated><resolved>2016-05-03T17:29:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T17:29:30Z" id="216604620">thanks @kels17 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add documentation for shard store output in allocation explain API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18114</link><project id="" key="" /><description>Relates to #17689
</description><key id="152809034">18114</key><summary>Add documentation for shard store output in allocation explain API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>docs</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T15:53:16Z</created><updated>2016-05-03T16:46:53Z</updated><resolved>2016-05-03T16:46:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-03T16:43:25Z" id="216590215">A small comment regarding one line, otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make reset in QueryShardContext private</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18113</link><project id="" key="" /><description>The query shard reset() method resets some internal state in the query shard context, like clearing query names, the filter flag
or named queries. The problem with this method being public is that it currently (miss?) used for modifying an existing context
for recursive invocatiob, but the contexts that have been reseted that way cannot be properly set back to their previous state.

This PR is a step towards removing reset() entirely by first making it only be used internally in QueryShardContext. In places where
reset() was used we can either create new QueryShardContexts or modify the existing context because it is discarded afterwards anyway.
</description><key id="152803394">18113</key><summary>Make reset in QueryShardContext private</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T15:29:45Z</created><updated>2016-05-03T16:57:21Z</updated><resolved>2016-05-03T16:57:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-05-03T16:35:41Z" id="216587405">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add _score order in the terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18112</link><project id="" key="" /><description>- Sort terms by max score in the bucket.

This is already achievable with a max aggregation and the aggregation order but this would be needed if we decide to implement the breadth_first strategy when the score is needed.
Currently the breadth_first strategy is not possible when the score is needed or if the sort is extracted from an inner aggregation.
With this approach we could break the second limitation and focus on the first one.
Relates to #9825
</description><key id="152801821">18112</key><summary>Add _score order in the terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Aggregations</label><label>feature</label><label>review</label></labels><created>2016-05-03T15:24:05Z</created><updated>2016-05-04T14:10:51Z</updated><resolved>2016-05-04T08:03:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-04T07:04:01Z" id="216761977">I don't think we should do it. This does not look like a common use-case to me, and this has a significant impact on the API/impls since buckets need to expose the max score, terms aggregation object need to transport the max score, and terms aggregator impls need to be able to track the max score per bucket.
</comment><comment author="jimczi" created="2016-05-04T07:19:09Z" id="216764186">The main use case I see for this is the field collapsing. Currently we advise users to do field collapsing like this:

```
...
"aggs": {
    "users": {
      "terms": {
        "field":   "user.name.raw",      
        "order": { "top_score": "desc" } 
      },
      "aggs": {
        "top_score": { "max":      { "script":  "_score"           }}, 
        "blogposts": { "top_hits": { "_source": "title", "size": 5 }}  
      }
    }
  }
```

This works great but the memory consumption is huge, the top_hits needs to keep track of all buckets. One way to reduce the memory consumption is to use the breadth_first strategy but it won't work on this example because the max aggregations is used to sort the terms agg. With this feature we could remove the max aggregations, of course this is not enough because the top_hits would still need to access the score of each document. Though this limitation could be addressed if we implement a way to recompute the score of each top bucket documents in the breadth_first strategy.
IMO this is a common use case (field collapsing) and this PR is the first step to make the field collapsing efficient. 
</comment><comment author="jpountz" created="2016-05-04T07:39:12Z" id="216767588">Thanks for the complete picture. Then maybe what we need is to make `breadth_first` a bit smarter? For instance it could figure out that it needs to run the max aggregation on the first pass since it is required for sorting, and then we could also add the ability to recompute scores for the second pass like you suggested?
</comment><comment author="jimczi" created="2016-05-04T08:03:12Z" id="216773220">Thanks @jpountz this sounds good to me, I'll try that.
</comment><comment author="jpountz" created="2016-05-04T08:07:07Z" id="216775687">@jimferenczi To make reviews easier, I think these changes should be in different PRs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort by numeric distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18111</link><project id="" key="" /><description>Today, I'm able to define a point (lat, lon) and sort my documents based on the distance from this point. This is super handy.

[Someone asked for a similar feature](http://stackoverflow.com/questions/37005785/how-to-find-the-nearest-closest-number-using-query-dsl-in-elasticsearch) but for numerical values.

For example, let say I want to find a product and sort it by the closest price I set.

Obviously as explained [here](http://stackoverflow.com/a/37006104/1432281) you could use a script for that but I think that it would be nice if can support a similar feature out of the box.

Something like:

``` js
{
    "sort" : [
        {
            "_distance" : {
                "price" : 10.0,
                "order" : "asc"
            }
        }
    ]
}
```

WDYT? 
</description><key id="152783260">18111</key><summary>Sort by numeric distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>discuss</label><label>feature</label></labels><created>2016-05-03T14:11:46Z</created><updated>2016-05-03T17:34:50Z</updated><resolved>2016-05-03T17:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="litong01" created="2016-05-03T16:14:44Z" id="216581276">@dadoonet I think that will be a very nice feature.
</comment><comment author="clintongormley" created="2016-05-03T17:28:37Z" id="216604390">Honestly, this is so easy to do with a script (including a Lucene expression script, which is really fast) that I don't see the point, especially given that this is not a common need.

```
POST t/t/_bulk
{"index": {}}
{ "num": 1 }
{"index": {}}
{ "num": 2 }
{"index": {}}
{ "num": 3 }
{"index": {}}
{ "num": 4 }
{"index": {}}
{ "num": 5 }
{"index": {}}
{ "num": 6 }

GET t/_search
{
  "sort": {
    "_script": {
      "type": "number",
      "order": "asc",
      "script": {
        "inline": "abs(target - doc['num'])",
        "lang": "expression",
        "params": {
          "target": 3
        }
      }
    }
  }
}
```
</comment><comment author="litong01" created="2016-05-03T17:34:50Z" id="216606051">will it become more complex when queries are nested? like in aggregation?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Strings#splitStringToArray</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18110</link><project id="" key="" /><description>This commit removes the method `Strings#splitStringToArray` and replaces
the call sites with invocations to `String#split`. There are only two
explanations for the existence of this method. The first is that
String#split is slightly tricky in that it accepts a regular expression
rather than a character to split on. This means that if `s` is a string,
`s.split(".")`  does not split on the character '.', but rather splits on
the regular expression '.' which splits on every character (of course,
this is easily fixed by invoking `s.split("\\.")` instead). The second
possible explanation is that (again) `String#split` accepts a regular
expression. This means that there could be a performance concern
compared to just splitting on a single character. However, it turns out
that `String#split` has a fast path for the case of splitting on a single
character and microbenchmarks show that `String#split` has 1.5x--2x the
throughput of `Strings#splitStringToArray`. There is a slight behavior
difference between `Strings#splitStringToArray` and `String#split`: namely,
the former would return an empty array in cases when the input string
was null or empty but `String#split` will just NPE at the call site on
null and return a one-element array containing the empty string when the
input string is empty. There was only one place relying on this behavior
and the call site has been modified accordingly.
</description><key id="152782210">18110</key><summary>Remove Strings#splitStringToArray</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T14:08:17Z</created><updated>2016-05-04T13:38:47Z</updated><resolved>2016-05-04T13:38:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-05-04T07:05:21Z" id="216762156">This is great! LGTM.
</comment><comment author="nik9000" created="2016-05-04T12:30:27Z" id="216845403">This might also have had something to do with removing empty strings caused by repetitions of the split character. It looks like `splitStringToArray` didn't add them to the array and I'm pretty sure `String#split` does. This isn't an objection to doing this. It is just worth looking into.
</comment><comment author="jasontedor" created="2016-05-04T12:33:44Z" id="216847069">&gt; This might also have had something to do with removing empty strings caused by repetitions of the split character. It looks like `splitStringToArray` didn't add them to the array and I'm pretty sure `String#split` does.

We were either not testing for it or not relying on it, but I will investigate carefully before merging.
</comment><comment author="jasontedor" created="2016-05-04T12:52:21Z" id="216853267">@jpountz @nik9000 Because of the behavior that @nik9000 mentioned, today this is legal:

``` bash
$ curl -XGET localhost:9200/i//t/1 -d '{"f":"v"}'
{
  "_index" : "i",
  "_type" : "t",
  "_id" : "1",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}
```

Note the double '/' between the index and type names.

With this change, this is no longer legal:

``` bash
$ curl -XGET localhost:9200/i//t/1 -d '{"f":"v"}'
No handler found for uri [/i//t/1?pretty=1] and method [POST]
```

This behavior appears to be undocumented and is definitely untested. I think it's okay to proceed with the PR as is, but what do you think?
</comment><comment author="nik9000" created="2016-05-04T13:01:56Z" id="216855333">&gt; Note the double '/' between the index and type names.

Eep. Those double path separators is just the kind of thing someone is going to accidentally rely on. I think we should continue with your change, but maybe add something to the breaking changes doc?
</comment><comment author="jpountz" created="2016-05-04T13:22:37Z" id="216862884">I know we have been doing it in the past, but maybe we have been too far with documenting this kind of things as breaking changes. Then it looks like everything broke backward compatibility when it is only really about catching user errors. So I would be in favor of not documenting it as a breaking change.
</comment><comment author="jasontedor" created="2016-05-04T13:28:00Z" id="216864815">While investigating @nik9000's observation, I did notice one thing about `PathTrie` that bothered me. Namely, we have a constructor that is only used internal to `PathTrie` that allows for arbitrary separator/wildcard pairs but it is otherwise unused and not tested. I removed it. Can you take a look at 9fe5ce934279a274abdf67b417aae8d1566cde63 @jpountz and @nik9000?
</comment><comment author="nik9000" created="2016-05-04T13:29:38Z" id="216865207">LGTM
</comment><comment author="jpountz" created="2016-05-04T13:30:56Z" id="216865560">This commit looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch package uninstall does not delete plugin bin directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18109</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha2 deb/rpm

**Steps to reproduce**:
1. Install elasticsearch package on any distro
2. Install a plugin with a `bin/` directory, ie `x-pack`
3. Run `rpm -e elasticsearch` or `dpkg -P elasticsearch`
4. Install the package again
5. Install `x-pack` again and watch an exception because the `bin/` directory is still there

```
/usr/share/elasticsearch/bin/elasticsearch-plugin install x-pack
-&gt; Downloading x-pack from elastic
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.RuntimePermission setFactory
* java.util.PropertyPermission * read,write
* javax.net.ssl.SSLPermission setHostnameVerifier
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
Exception in thread "main" java.nio.file.FileAlreadyExistsException: /usr/share/elasticsearch/bin/x-pack
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
    at java.nio.file.Files.createDirectory(Files.java:674)
    at org.elasticsearch.plugins.InstallPluginCommand.installBin(InstallPluginCommand.java:456)
    at org.elasticsearch.plugins.InstallPluginCommand.install(InstallPluginCommand.java:419)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:202)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:188)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
    at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:57)
```

The `postrm` scripts currently deletes the `plugins` directory on purge, but leaves the `bin` directory as is, thus cleans everything only half up.
</description><key id="152778589">18109</key><summary>Elasticsearch package uninstall does not delete plugin bin directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2016-05-03T13:52:51Z</created><updated>2017-03-14T00:32:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nikoncode" created="2016-05-12T15:10:36Z" id="218787801">If plugins with binaries always create dir in elasticsearch bin (like x-pack), we need to delete all folders from elastic bin (rm -rf */).
</comment><comment author="jordansissel" created="2016-06-09T04:33:28Z" id="224797762">Solving this will require a packaging script that runs after (or just before) the package is removed. I'll put some rough notes here about what I recall about these package formats --

This is made annoying because both Debian and Red Hat have different mechanisms for this. In RPM, you'll want a "postun" (for after removing a package) or a "preun" (before removing a package). In Debian, you'll want a "postrm" or "prerm" script. Further, I think each have different special ways of handling upgrades (which, in a way, is a package removal). Finally, Debian has two concepts for package removal -- "remove" and "purge". Fun!

Hope this helps. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid string concatentation in IngestDocument.FieldPath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18108</link><project id="" key="" /><description>Today, the constructor for IngestDocument#FieldPath does a string
concatentation and two object allocations on every field path. This
commit removes these unnecessary operations.
</description><key id="152774805">18108</key><summary>Avoid string concatentation in IngestDocument.FieldPath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T13:35:29Z</created><updated>2016-05-03T14:05:44Z</updated><resolved>2016-05-03T14:05:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-05-03T13:38:29Z" id="216529982">LGTM
</comment><comment author="jasontedor" created="2016-05-03T14:05:20Z" id="216537992">Thanks @tlrx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add note on configuring assertions in IDEs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18107</link><project id="" key="" /><description>This commit adds a note to the contributing docs on how to configure
assertions inside Eclipse and IntelliJ.

Closes #18087
</description><key id="152762153">18107</key><summary>Add note on configuring assertions in IDEs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T12:31:30Z</created><updated>2016-05-03T13:29:55Z</updated><resolved>2016-05-03T13:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-03T13:17:37Z" id="216524047">LGTM. Thanks !
</comment><comment author="mfussenegger" created="2016-05-03T13:29:55Z" id="216527807">Btw for IntelliJ this could also be done with the gradle idea task:

```
def jvmTestFlags = ['-ea' ]

idea {
    workspace {
        iws.withXml { xmlFile -&gt;
            def runManager = xmlFile.asNode().component.find { it.@name == 'RunManager' }

            // enable assertions for junit tests
            def junitDefaults = runManager.configuration.find { it.@default == 'true' &amp;&amp; it.@type == 'JUnit' }
            junitDefaults.option.find { it.@name == 'VM_PARAMETERS' }.replaceNode {
                option(name: 'VM_PARAMETERS', value: jvmTestFlags.join(' '))
            }

   ...
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds a methods to find (and dynamically create) the mappers for the parents of a field with dots in the field name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18106</link><project id="" key="" /><description>Closes https://github.com/elastic/elasticsearch/issues/15951
</description><key id="152761751">18106</key><summary>Adds a methods to find (and dynamically create) the mappers for the parents of a field with dots in the field name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T12:29:03Z</created><updated>2016-05-16T08:36:08Z</updated><resolved>2016-05-16T08:36:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-04T02:30:37Z" id="216726029">@colings86 I think this is pretty good, just about what I was thinking. I left some comments as I think what you have can be simplified a little. I would also consider renaming the "get" method with a "addDynamicParentMappers". If you do the single context element like I suggested as a possibility, then I think the code could be as simple as:

```
ObjectMapper parent parent = addDynamicParentMappers(...);
// .. stuff
if (paths.length != 1) {
    context.path().remove()
}
```
</comment><comment author="rjernst" created="2016-05-04T02:31:19Z" id="216726084">I think it would be good to have @jpountz look at this as well. I glanced through the tests, but he may have opinions for things that are still lacking tests.
</comment><comment author="colings86" created="2016-05-10T07:33:54Z" id="218080609">@rjernst @jpountz I pushed some updated commits to resolve all the comments except one (the first outdated diff on this page) which I need some clarification on.
</comment><comment author="rjernst" created="2016-05-15T03:36:43Z" id="219264364">LGTM, thanks for all the extra tests!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Circuit Breaker Settings are not working </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18105</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.1

**JVM version**: 1.8

**OS version**: Linux localhost.localdomain 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**: 

we have applied the following settings to prevent OOM error.

none the settings are working, it was working for ES 1.7 but not 2.3.1

even, we tried indices.breaker.request.limit : 10% but no luck.

indices.fielddata.cache.size:  30%

index.cache.field.type: soft

indices.memory.index_buffer_size: 30%

indices.breaker.request.limit : 60%

**Steps to reproduce**:
1. open elasticsearch.yml
2. add the following block
   indices.fielddata.cache.size:  30%

index.cache.field.type: soft

indices.memory.index_buffer_size: 30%

indices.breaker.request.limit : 60%
1. try to execute a big query to test circuit breaker settings 

**Provide logs (if relevant)**:
</description><key id="152748490">18105</key><summary>Circuit Breaker Settings are not working </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adhamelia</reporter><labels /><created>2016-05-03T11:03:56Z</created><updated>2016-06-08T20:20:15Z</updated><resolved>2016-05-03T11:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T11:37:45Z" id="216502446">For a start, 30% + 30% + 60% = 120% of heap, never mind all the other data that needs to fit into the same heap.  Second, why are you using so much fielddata on 2.3?  You should be using almost entirely doc values.  Third, soft caches are no longer supported.

I'm not sure why you're changing these settings from the defaults, but you should really understand what you're doing before making changes like this.  I suggest asking questions like these in the forum instead: https://discuss.elastic.co/
</comment><comment author="adhamelia" created="2016-05-03T15:16:27Z" id="216561163">thanks we figured out 
</comment><comment author="bkandanoor" created="2016-06-08T20:20:14Z" id="224715090">what was the issue why circuit breaker was not working which you figured it out. i am having hte same issue where i am trying to test the circuit breaker and it does not do anything. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inner hits parsing broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18104</link><project id="" key="" /><description>This was working in 5.0.0-alpha1, broken in alpha2:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "nested"
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": {
    "bar": "x"
  }
}

GET t/_search
{
  "query": {
    "nested": {
      "inner_hits": {},
      "path": "foo",
      "query": {
        "match": {
          "foo.bar": "x"
        }
      }
    }
  }
}
```

throws exception: 

```
     "caused_by": {
        "type": "illegal_state_exception",
        "reason": "Neither a nested or parent/child inner hit"
     }
```
</description><key id="152732337">18104</key><summary>Inner hits parsing broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Inner Hits</label><label>regression</label></labels><created>2016-05-03T09:23:31Z</created><updated>2016-05-07T15:00:51Z</updated><resolved>2016-05-07T15:00:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T10:06:15Z" id="216485925">Also, the real exception here should be bubbled up as the `root_cause`.  Currently it is hidden as follows:

```
{
   "error": {
      "root_cause": [
         {
            "type": "query_shard_exception",
            "reason": "failed to create query: {\n  \"nested\" : {\n    \"query\" : {\n      \"match\" : {\n        \"foo.bar\" : {\n          \"query\" : \"x\",\n          \"operator\" : \"OR\",\n          \"prefix_length\" : 0,\n          \"max_expansions\" : 50,\n          \"fuzzy_transpositions\" : true,\n          \"lenient\" : false,\n          \"zero_terms_query\" : \"NONE\",\n          \"boost\" : 1.0\n        }\n      }\n    },\n    \"path\" : \"foo\",\n    \"ignore_unmapped\" : false,\n    \"score_mode\" : \"avg\",\n    \"boost\" : 1.0,\n    \"inner_hits\" : {\n      \"from\" : 0,\n      \"size\" : 3,\n      \"version\" : false,\n      \"explain\" : false,\n      \"track_scores\" : false,\n      \"query\" : {\n        \"match_all\" : {\n          \"boost\" : 1.0\n        }\n      }\n    }\n  }\n}",
            "index_uuid": "OCBvzEhsQwCXJSVZ-LB_JA",
            "index": "t"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "t",
            "node": "aLUS681NQtCqQ0YiLnjcMw",
            "reason": {
               "type": "query_shard_exception",
               "reason": "failed to create query: {\n  \"nested\" : {\n    \"query\" : {\n      \"match\" : {\n        \"foo.bar\" : {\n          \"query\" : \"x\",\n          \"operator\" : \"OR\",\n          \"prefix_length\" : 0,\n          \"max_expansions\" : 50,\n          \"fuzzy_transpositions\" : true,\n          \"lenient\" : false,\n          \"zero_terms_query\" : \"NONE\",\n          \"boost\" : 1.0\n        }\n      }\n    },\n    \"path\" : \"foo\",\n    \"ignore_unmapped\" : false,\n    \"score_mode\" : \"avg\",\n    \"boost\" : 1.0,\n    \"inner_hits\" : {\n      \"from\" : 0,\n      \"size\" : 3,\n      \"version\" : false,\n      \"explain\" : false,\n      \"track_scores\" : false,\n      \"query\" : {\n        \"match_all\" : {\n          \"boost\" : 1.0\n        }\n      }\n    }\n  }\n}",
               "index_uuid": "OCBvzEhsQwCXJSVZ-LB_JA",
               "index": "t",
               "caused_by": {
                  "type": "illegal_state_exception",
                  "reason": "Neither a nested or parent/child inner hit"
               }
            }
         }
      ],
      "caused_by": {
         "type": "query_shard_exception",
         "reason": "failed to create query: {\n  \"nested\" : {\n    \"query\" : {\n      \"match\" : {\n        \"foo.bar\" : {\n          \"query\" : \"x\",\n          \"operator\" : \"OR\",\n          \"prefix_length\" : 0,\n          \"max_expansions\" : 50,\n          \"fuzzy_transpositions\" : true,\n          \"lenient\" : false,\n          \"zero_terms_query\" : \"NONE\",\n          \"boost\" : 1.0\n        }\n      }\n    },\n    \"path\" : \"foo\",\n    \"ignore_unmapped\" : false,\n    \"score_mode\" : \"avg\",\n    \"boost\" : 1.0,\n    \"inner_hits\" : {\n      \"from\" : 0,\n      \"size\" : 3,\n      \"version\" : false,\n      \"explain\" : false,\n      \"track_scores\" : false,\n      \"query\" : {\n        \"match_all\" : {\n          \"boost\" : 1.0\n        }\n      }\n    }\n  }\n}",
         "index_uuid": "OCBvzEhsQwCXJSVZ-LB_JA",
         "index": "t",
         "caused_by": {
            "type": "illegal_state_exception",
            "reason": "Neither a nested or parent/child inner hit"
         }
      }
   },
   "status": 400
}
```
</comment><comment author="clintongormley" created="2016-05-07T15:00:48Z" id="217642846">Fixed in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use object equality to compare versions in IndexSettings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18103</link><project id="" key="" /><description>Fixes an issue where updating index metadata on a index from a version that it does not have in its static list of known versions fails (e.g. when v5.0.0 loads an index from v2.4.0 that was released after v5.0.0).
</description><key id="152724841">18103</key><summary>Use object equality to compare versions in IndexSettings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T08:36:41Z</created><updated>2016-05-03T09:32:01Z</updated><resolved>2016-05-03T09:31:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-03T09:23:42Z" id="216476940">LGTM
</comment><comment author="ywelsch" created="2016-05-03T09:32:01Z" id="216478992">Thanks @dadoonet!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add toString() to GetResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18102</link><project id="" key="" /><description>This PR simply adds `toString()` to `GetResponse` similar to `SearchResponse`.
</description><key id="152720520">18102</key><summary>Add toString() to GetResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">izeye</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-03T08:06:57Z</created><updated>2016-05-09T22:53:24Z</updated><resolved>2016-05-09T20:56:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="izeye" created="2016-05-09T05:03:02Z" id="217776481">@nik9000 Thanks for the quick feedback. I changed to what you suggested and to make `SearchResponse` align with `GetResponse`, made it use the same utility.
</comment><comment author="nik9000" created="2016-05-09T12:52:35Z" id="217855710">Cool! I'll see about merging then!
</comment><comment author="izeye" created="2016-05-09T15:10:05Z" id="217892682">@nik9000 Sorry. I didn't check. I replaced tabs with spaces and checked by `gradle core:precommit`.
</comment><comment author="nik9000" created="2016-05-09T20:56:39Z" id="217987178">OK! I finally got the chance to fetch it locally and have a look and it looks great. Thanks @izeye ! I've merged.
</comment><comment author="izeye" created="2016-05-09T22:53:24Z" id="218013531">@nik9000 Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow CORS requests to work with HTTP compression enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18101</link><project id="" key="" /><description>With this commit we fix an issue that prevented to turn on
HTTP compression when using CORS. The problem boiled down
to a problematic order of the respective handlers in
Netty's pipeline.

Note that the same problem is already fixed in ES 5.0 by #18066.

Relates #18066
Fixes #18089
</description><key id="152706500">18101</key><summary>Allow CORS requests to work with HTTP compression enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-05-03T06:07:46Z</created><updated>2016-05-03T06:41:17Z</updated><resolved>2016-05-03T06:41:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-03T06:37:52Z" id="216450780">LGTM
</comment><comment author="danielmitterdorfer" created="2016-05-03T06:40:17Z" id="216451355">@dadoonet thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Client nodes don't report each other [2.1.x]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18100</link><project id="" key="" /><description>I've found that when I turn on the client setting (master: false, data: false) in testing a cluster on 2.1, when querying the `_nodes` endpoint from a client node, it won't show other client nodes, but if I query the `_nodes` endpoint from a data node, it will show all client nodes. I've tried this same scenario on elasticsearch version 1.7.4, and I can see all client nodes from any node. 

This issue is also mentioned [here](https://github.com/mobz/elasticsearch-head/issues/246)
</description><key id="152686268">18100</key><summary>Client nodes don't report each other [2.1.x]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bdharrington7</reporter><labels /><created>2016-05-03T02:18:38Z</created><updated>2016-05-03T20:27:04Z</updated><resolved>2016-05-03T08:00:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-05-03T08:00:35Z" id="216462788">This should be fixed with #16898 in master and released with 5.0-alpha1. Note also that setting master and data to false was not always the same as setting client to true. That was solved with #16963.
</comment><comment author="bdharrington7" created="2016-05-03T16:12:35Z" id="216580218">Thanks @javanna! Will this be back ported to 2.x?
</comment><comment author="javanna" created="2016-05-03T19:53:27Z" id="216645646">Hi @bdharrington7 no it wasn't backported to 2.x and I am not sure it is safe to do so.
</comment><comment author="bdharrington7" created="2016-05-03T20:17:58Z" id="216652151">Ah cool, this looks like we can still get expected behavior by using `node.master: false` and `node.data: false` for a client node
</comment><comment author="javanna" created="2016-05-03T20:27:04Z" id="216654450">right, not sure if that is good or bad though :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add tests/doc for boolean fields with expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18099</link><project id="" key="" /><description>boolean fields happen to already work with expressions (e.g. ternary operator) but we don't document they work or test them.

This just adds doc and a test (and fixes 2-space indent of previous test caused by our gradle build destroying my ide on every `gradle clean`)
</description><key id="152658351">18099</key><summary>Add tests/doc for boolean fields with expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Expressions</label><label>docs</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T22:15:08Z</created><updated>2016-05-03T09:52:13Z</updated><resolved>2016-05-02T23:38:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-02T22:16:10Z" id="216381807">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch Integration Test Never Finishes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18098</link><project id="" key="" /><description>**Elasticsearch version**: Works on 2.1.2 fails on 2.2.0 and later

**JVM version**: 1.8

**OS version**: Ubuntu 15.10

**Description of the problem including expected versus actual behavior**: Test never finishes. The actual body of the test run normally, but appears to infinitely wait for something to shutdown.

**Steps to reproduce**:
1. git clone git@github.com:winstonewert/broken-es-test.git
2. cd broken-es-test
3. ./gradlew test
</description><key id="152649482">18098</key><summary>Elasticsearch Integration Test Never Finishes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">winstonewert</reporter><labels /><created>2016-05-02T21:26:57Z</created><updated>2016-05-06T20:46:48Z</updated><resolved>2016-05-03T09:50:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T09:50:37Z" id="216482984">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="winstonewert" created="2016-05-03T15:34:48Z" id="216568527">This is a bug report. An empty subclass of ESIntegTestCase doesn't terminate after a certain release of elasticsearch. That seems like a bug to me. 

If you insist I can take it to the form, but I'm pretty sure this is a bug.
</comment><comment author="jasontedor" created="2016-05-03T15:37:25Z" id="216569332">&gt; This is a bug report. An empty subclass of ESIntegTestCase doesn't terminate after a certain release of elasticsearch. That seems like a bug to me. 
&gt; 
&gt; If you insist I can take it to the form, but I'm pretty sure this is a bug.

Since our tests run, I don't think there's a bug here but rather an issue with how you've set up your build.
</comment><comment author="winstonewert" created="2016-05-06T20:46:48Z" id="217554477">Updating this issue in case someone ends up here from Google trying to track down the same issue.

Elasticsearch 2.2.0 started enabling a security manager by default which breaks gradle's test runner. 

See: https://issues.gradle.org/browse/GRADLE-2170, which has a workaround.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException[failed to parse]; nested: IllegalStateException[Mixing up field types: class org.elasticsearch.index.er.internal.IdFieldMapper$IdFieldType on field _id];</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18097</link><project id="" key="" /><description>i use nodejs(v0.12.7) mongoosastic@4.0.0 and elasticsearch,,

what error is that ?
when works this code,,i see that result and error...

![image](https://cloud.githubusercontent.com/assets/15065065/14967279/944f5a3e-10c7-11e6-9e88-0013c5234d48.png)
![image](https://cloud.githubusercontent.com/assets/15065065/14967292/a29c2b62-10c7-11e6-92a3-11aab102f5bc.png)

![image](https://cloud.githubusercontent.com/assets/15065065/14967195/348a4686-10c7-11e6-9774-23001e679517.png)
</description><key id="152639959">18097</key><summary>MapperParsingException[failed to parse]; nested: IllegalStateException[Mixing up field types: class org.elasticsearch.index.er.internal.IdFieldMapper$IdFieldType on field _id];</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nairicode</reporter><labels /><created>2016-05-02T20:40:48Z</created><updated>2016-05-02T22:19:44Z</updated><resolved>2016-05-02T22:19:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support geo_point fields in lucene expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18096</link><project id="" key="" /><description>This adds minimal support for geo_point fields to expressions, with a barebones api (`.empty`, `.lat`,  `.lon`).

For example `_score + haversin(38.9072, 77.0369, doc['location'].lat, doc['location'].lon)`

I reorganized the code here significantly. This means expressions now has 3 "types" (numeric, date, and geo) rather than just 2, so we have to organize things better to support that. 

It means more code but IMO simpler code, otherwise we need a ton of conditionals to check that some method call or variable access is really allowed by the specific type.

I tried to also reorganize the expressions docs a bit in the same fashion.
</description><key id="152620991">18096</key><summary>Support geo_point fields in lucene expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Expressions</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T19:05:49Z</created><updated>2016-05-03T09:51:28Z</updated><resolved>2016-05-02T21:50:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-02T19:17:19Z" id="216333922">LGTM. One suggestion is to make all the ValueSource classes pkg private, I think it was an oversight as to why the existing ones are public.
</comment><comment author="rmuir" created="2016-05-02T19:36:01Z" id="216338953">Thanks ryan. I pushed a new commit.
</comment><comment author="rjernst" created="2016-05-02T19:37:39Z" id="216339374">Great, thanks for the cleanups. Still looks good.
</comment><comment author="jdconrad" created="2016-05-02T19:43:20Z" id="216340813">LGTM too.  This is a nice improvement.  Thanks for doing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide an option to skip score explanation computation for explain api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18095</link><project id="" key="" /><description>**Describe the feature**:  

The explain api is not only useful for score explanations, but it also provides debugging information on what node(s)/shards(s) were used when returning documents for a request.  It will be nice to provide an option to skip the score explanation computation for those who are only interested in the node/shard information.
</description><key id="152619191">18095</key><summary>Provide an option to skip score explanation computation for explain api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label></labels><created>2016-05-02T18:58:16Z</created><updated>2016-05-03T07:58:04Z</updated><resolved>2016-05-03T07:58:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-02T19:18:58Z" id="216334324">The main point of the explain API is to provide scoring. However, scoring is calculated locally to a shard (because of how IDF is computed unless using `dfs_query_then_fetch`) so showing the shard is important. But I'm generally not sure how valuable it is to know what shard handled a request outside the context of scoring? I'm -1 on adding this.
</comment><comment author="ppf2" created="2016-05-02T22:58:55Z" id="216389805">Sometimes, we use the explain api as one of the tools to determine if a query causing hotspots is due to an unbalanced number of documents being routed to specific shards when not using the default routing.  And it is also useful for confirming if search preference options and things like prefer-local-shards for shard allocation awareness are working.
</comment><comment author="jasontedor" created="2016-05-02T23:48:33Z" id="216397805">&gt; Sometimes, we use the explain api as one of the tools to determine if a query causing hotspots is due to an unbalanced number of documents being routed to specific shards when not using the default routing. 

This is done more appropriately with the count API:

``` bash
POST /_count?routing=custom_routing
{
  "query": {
    .
    .
    .
  }
}
```

&gt; And it is also useful for confirming if search preference options and things like prefer-local-shards for shard allocation awareness are working.

We have tests to validate this. I don't think we should add features to Elasticsearch to validate other features of Elasticsearch. Another point is that this output would not actually prove that the operation is routed to the specified node, validating that really requires a network trace.
</comment><comment author="ppf2" created="2016-05-03T07:58:04Z" id="216462359">ok fair enough :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't try to compute completion stats on a reader after we already closed it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18094</link><project id="" key="" /><description>This can cause `AlreadyClosedException` and can also crash your JVM if `mmapfs` is in use and Lucene's best-effort check to catch this illegal usage fails, in 2.3.2.

I'll also fix this in 5.0.0, but there the bug is more harmless (computes stats twice, but without causing ACE/possible JVM crash).
</description><key id="152612191">18094</key><summary>Don't try to compute completion stats on a reader after we already closed it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label><label>v2.3.3</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T18:24:03Z</created><updated>2016-05-02T19:04:41Z</updated><resolved>2016-05-02T18:40:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-05-02T18:25:18Z" id="216318872">+1
</comment><comment author="jasontedor" created="2016-05-02T18:39:22Z" id="216322770">LGTM.
</comment><comment author="mikemccand" created="2016-05-02T18:39:37Z" id="216322834">Looks like the bug first appeared in 2.3.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add xContent shuffling to some more tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18093</link><project id="" key="" /><description>This adds some random shuffling of xContent to some more test cases. 

Relates to #5831 
</description><key id="152601879">18093</key><summary>Add xContent shuffling to some more tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T17:32:44Z</created><updated>2016-05-26T11:42:07Z</updated><resolved>2016-05-06T08:55:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-03T12:06:10Z" id="216508226">@nik9000 thanks for the review, I pushed the changes you suggested and left one follow up question.
</comment><comment author="nik9000" created="2016-05-04T18:30:07Z" id="216957927">Left small comments. I'll give it a proactive LGTM. It is fine as if but I'd be happy if you'd resolve the comments and then merge it when you are done. I don't need to review again I think, unless you think my requests are crazy. If they are then I'm misunderstanding something.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change scriptFields member in InnerHitBuilder to set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18092</link><project id="" key="" /><description>Adding random shuffling of xContent to InnterHitBuilderTests shows that the scriptFields are stored in order as a list internally although
they are an unordered json objects in the query dsl. This changes the internal representation to a set and updates serialization accordingly.

Relates to #5831 
</description><key id="152595540">18092</key><summary>Change scriptFields member in InnerHitBuilder to set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Inner Hits</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T17:01:24Z</created><updated>2016-05-04T16:16:46Z</updated><resolved>2016-05-04T16:16:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-02T17:01:56Z" id="216293075">@martijnvg could you take a look if this looks okay to you?
</comment><comment author="jpountz" created="2016-05-04T14:40:53Z" id="216886096">LGTM
</comment><comment author="cbuescher" created="2016-05-04T14:45:22Z" id="216887558">@jpountz thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inner Hits sometimes missing "_index" key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18091</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: Oracle Corporation 1.8.0_45 (64-bit)

**OS version**:  Mac OS X 10.11.4 x86_64

**Description of the problem including expected versus actual behavior**:

I am running tests using your integration test framework (`ESIntegTestCase`), however, I am only using it to setup a cluster and index some documents.  I have it configured so the http is enabled on a random port, and then I use the rest api for my tests.  

I am currently running a test that uses the inner hits functionality and I have found that one of my asserts on the index name of an inner hit fails.  After inspecting the json response, I can see that the _index key on the inner hit is missing.  Every time it fails it looks like the test framework has sets a test locale that result in non-english text in the logs.  I don't know if that has anything to do with it or not, however, this is the only consistent thing I have noticed.

The inner hits are only against nested documents and the request that fails has multiple levels of nested objects.  All levels of the nested response are missing the _index key.  My search request is only using the top-level inner hits request structure since I need multiple levels of inner hits.

I know there has been changes to inner hits lately so I am not sure this is even relevant in versions greater than 2.2. 

**Provide logs (if relevant)**:  

https://gist.github.com/mattweber/614e45ce0ab9a0487400fa69400a4131
</description><key id="152588903">18091</key><summary>Inner Hits sometimes missing "_index" key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-05-02T16:25:10Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-06-21T12:14:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T09:29:54Z" id="216478088">@martijnvg could you take a look at this please?

@mattweber any chance you could add the test you're running?
</comment><comment author="mattweber" created="2016-05-03T13:09:28Z" id="216521679">@clintongormley It has some code that belongs to a client I am working with so I can't post it directly.  I will work on writing an isolated test that essentially does the same thing though.
</comment><comment author="mattweber" created="2016-05-03T15:54:23Z" id="216574412">@clintongormley @martijnvg 

https://gist.github.com/mattweber/19c37d9b8a01fd305afa782bc38f9d2f

Here is a quick and dirty test.  Add this file as `core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIssueIT.java` to a checkout of the 2.2 branch.  The test failure reproduces for me consistently with:

`mvn clean verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=C293056A17C37DCE -Dtests.class=org.elasticsearch.search.innerhits.InnerHitsIssueIT -Dtests.method="testIssue18091" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=sv_SE -Dtests.timezone=Portugal`
</comment><comment author="clintongormley" created="2016-05-03T17:18:54Z" id="216601388">thanks @mattweber 
</comment><comment author="mattweber" created="2016-05-04T00:44:17Z" id="216711053">Reproducible against `2.x` with following:

`mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=7AA0546C9A7151D7 -Dtests.class=org.elasticsearch.search.innerhits.InnerHitsIssueIT -Dtests.method="testIssue18091" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=mk -Dtests.timezone=Etc/GMT+9`

Same test 
</comment><comment author="mattweber" created="2016-05-04T14:41:00Z" id="216886145">Also reproducible against current master (5.x) using the new multi-level inner hit functionality inside of the nested queries vs. a top-level request. 

Here is the test for master:
https://gist.github.com/mattweber/578c77e1c5f148e13a3a9c238d169fea

Here is my reproduce command:
`gradle :core:integTest -Dtests.seed=98F2AB350B0F5BF2 -Dtests.class=org.elasticsearch.search.innerhits.InnerHitsIssueIT -Dtests.method="testIssue18091" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=zh-CN -Dtests.timezone=Etc/GMT-8`

Note, I run this test in a loop until it fails.  It usually takes less than 5 runs before I hit the failure.
</comment><comment author="martijnvg" created="2016-05-17T10:05:39Z" id="219674357">@mattweber @clintongormley If I recall correctly during node to node serialization the `_index` key was not included for nested inner hits, since it isn't really needed, because the `_index` key is always the same as the parent search hit. The inconsistency here is that when search hits aren't serialized then the `_index` key is included. (which happens in test with a single node)

I think the inconsistency should be fixed or the `_index` key should always be included in inner hits. 
</comment><comment author="mattweber" created="2016-06-17T13:51:18Z" id="226773954">Thanks @martijnvg!  Is index/type/id even needed here?  It will always be the same as the parent document right?  Plus we have the `_nested` object that gives us the offsets inside source nested array.
</comment><comment author="martijnvg" created="2016-06-17T16:48:20Z" id="226821137">@mattweber I agree. index/type/id are not needed for nested inner hits.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix and test handling of `null_value`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18090</link><project id="" key="" /><description>This was mostly untested and had some bugs.

Closes #18085
</description><key id="152577015">18090</key><summary>Fix and test handling of `null_value`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T15:26:57Z</created><updated>2016-05-06T07:29:08Z</updated><resolved>2016-05-06T07:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-04T19:23:47Z" id="216973309">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CORS won't work currectly when HTTP compression is enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18089</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: jdk1.8.0_60

**OS version**: Linux RedHat Enterprise 7.2

**Description of the problem including expected versus actual behavior**:
When enable HTTP compression, CORS stops to work, and the elasticsearch present this error in the log file.

**Steps to reproduce**:
1. enable HTTP compression
2. enable CORS
3. try to use elasticsearch and error below heppens.

**Provide logs (if relevant)**:
[2016-04-27 15:57:16,312][WARN ][http.netty ] [node_1_data] Caught exception while handling client http traffic, closing connection [id: 0x246f9cb0, /172.30.XXX.XX:40855 =&gt; /172.30.XXX.XX:9200]
java.lang.IllegalStateException: cannot send more responses than requests
at org.jboss.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:101)
at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:105)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
at org.jboss.netty.channel.Channels.write(Channels.java:704)
at org.jboss.netty.channel.Channels.write(Channels.java:671)
at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:348)
at org.elasticsearch.http.netty.cors.CorsHandler.handlePreflight(CorsHandler.java:123)
at org.elasticsearch.http.netty.cors.CorsHandler.messageReceived(CorsHandler.java:80)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
</description><key id="152569234">18089</key><summary>CORS won't work currectly when HTTP compression is enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">klinux</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-05-02T14:50:09Z</created><updated>2016-05-03T06:43:25Z</updated><resolved>2016-05-03T06:42:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-05-02T15:00:03Z" id="216259781">The issue already came up while I worked on #18066 and is fixed on 5.0. But we should really fix this for 2.x too (although without applying the mentioned PR thus avoiding to change also the HTTP compression defaults). The root cause is that the CORS handler is coming "too early" in the pipeline of `NettyHttpServerTransport`. The request must be processed by `ESHttpResponseEncoder` first.
</comment><comment author="danielmitterdorfer" created="2016-05-03T06:42:52Z" id="216451726">Closed by #18101 (included in Elasticsearch 2.3.3 and Elasticsearch 2.4.0). Will be fixed in Elasticsearch 5.0 by #18066.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add system bootstrap checks escape hatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18088</link><project id="" key="" /><description>Today when running in production mode the bootstrap checks are
completely unforgiving. But there are cases where an end-user might not
have the ability to modify some of the system-level settings that cause
the bootstrap checks to trip (e.g., guest settings that are inherited
from a host and can not be modified). This commit adds a setting that
allows system-level bootstrap checks to be ignored for these
end-users. We classify certain bootstrap checks into system-level checks
and only those bootstrap checks will be ignored if this flag is
enabled. All other bootstrap checks are still subject to being enforced
if the user is in production mode. We will still log warnings for these
bootstrap checks because the end-user does still need to be made aware
that they are running in a configuration that is less-than-ideal from a
resiliency perspective.
</description><key id="152566133">18088</key><summary>Add system bootstrap checks escape hatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T14:35:10Z</created><updated>2016-05-02T14:59:54Z</updated><resolved>2016-05-02T14:59:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-02T14:36:24Z" id="216252406">Documentation for this setting will come for a later pull request that will address documenting all of the bootstrap checks.
</comment><comment author="s1monw" created="2016-05-02T14:44:27Z" id="216254525">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`gradle eclipse` should set `-ea` if possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18087</link><project id="" key="" /><description>Eclipse has a flag that enabled `-ea` when it sets up any tests. Elasticsearch's tests fail immediately if they aren't run with `-ea`. So `gradle eclipse` should set that flag if at all possible. It is kind of a mystery to me what `gradle eclipse` is allowed to set but it is worth looking into. At a minimum we need something in CONTRIBUTING.md about it.
</description><key id="152564786">18087</key><summary>`gradle eclipse` should set `-ea` if possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>adoptme</label><label>build</label></labels><created>2016-05-02T14:28:10Z</created><updated>2016-05-03T13:32:33Z</updated><resolved>2016-05-03T13:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-02T14:42:50Z" id="216254091">Similar to #15076
</comment><comment author="jasontedor" created="2016-05-03T12:31:39Z" id="216513421">&gt; At a minimum we need something in CONTRIBUTING.md about it.

The most straightforward approach here is a simple note in the CONTRIBUTING.md about how this is a required setting if developers want to run tests inside their IDE. I opened #18107.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhance even more the ignore_above settings documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18086</link><project id="" key="" /><description>Just a suggestion, we can add more information on how the `ignore_above` settings works. It is much better now how it explains the way to work with it, but i would enhance this in order to add even more information about searching:

The analyzer will ignore strings (tokens from analyzed strings or entire single-token non-analyzed strings) larger than this size, storing them in the document but making them non-searchable, if they are longer than this parameter. For example, if ignore_above is set to 5, then the non-analyzed string or token output from an analyzed string &#8220;12345&#8221; would be searchable for exact match and for partial matches using the regex operator or the glob match, but &#8220;123456&#8221; would not be searchable.
</description><key id="152533287">18086</key><summary>Enhance even more the ignore_above settings documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>docs</label></labels><created>2016-05-02T11:08:29Z</created><updated>2016-05-02T11:49:33Z</updated><resolved>2016-05-02T11:49:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-02T11:49:33Z" id="216217063">@gmoskovicz the `ignore_above` setting is not accepted by the new `text` field type, so it has nothing to do with analysis any more.

This page (https://www.elastic.co/guide/en/elasticsearch/reference/master/ignore-above.html) gives a full example of how `ignore_above` works, which I think covers the rest of the info you mention
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't set dates to null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18085</link><project id="" key="" /><description>```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "date": {
          "type": "date"
        }
      }
    }
  }
}

PUT t/t/1
{
  "date": null
}
```

Returns:

```
{
  "error": {
    "root_cause": [
      {
        "type": "mapper_parsing_exception",
        "reason": "failed to parse [date]"
      }
    ],
    "type": "mapper_parsing_exception",
    "reason": "failed to parse [date]",
    "caused_by": {
      "type": "illegal_state_exception",
      "reason": "Can't get text on a VALUE_NULL at 2:11"
    }
  },
  "status": 400
}
```
</description><key id="152531442">18085</key><summary>Can't set dates to null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T10:54:29Z</created><updated>2016-05-06T07:29:08Z</updated><resolved>2016-05-06T07:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nikoncode" created="2016-05-02T13:48:51Z" id="216240165">Not reproduces in 5.0.0-alpha1 release.
</comment><comment author="jpountz" created="2016-05-04T08:47:11Z" id="216792305">@nikoncode indeed, this is related to a change that only made it to alpha2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refering to neighbor function score for further calculation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18084</link><project id="" key="" /><description>**Describe the feature**:

When using Function score. Should we be able to name the function then use the function result in other functions.

For example, 

I have one function which perform a calculation using document fields. With that calculation, I want to use the result to determine two different values. Without ability to refer to calculated function, I have to do the same calculation twice to get the result.

Plus, naming thing seems to be a standard way for Elasticsearch structured it's query. Why having it differently for function score?
</description><key id="152527742">18084</key><summary>Refering to neighbor function score for further calculation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RobGThai</reporter><labels /><created>2016-05-02T10:26:58Z</created><updated>2016-05-02T11:43:23Z</updated><resolved>2016-05-02T11:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-02T11:43:23Z" id="216215212">Hi @RobGThai 

Your description is somewhat fuzzy - there are some things we can support and some we can't.  eg we have this issue https://github.com/elastic/elasticsearch/issues/17116 open to support naming scripts which can then be used together to calculate a score.

however, storing the value from functions score scripts to be returned later on with hits is a no-go.  We could potentially have to cache billions of calculations just to return 10 of them.

Closing in favour of https://github.com/elastic/elasticsearch/issues/17116
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient "Could not initialize class" Exception in Google Cloud Yarn cluster.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18083</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:2.2.0

**JVM version**:penjdk version "1.8.0_72-internal"

**OS version**:Debian 3.16.7

**Description of the problem including expected versus actual behavior**:
I am using Spark on Google Cloud, and I need to get data from an ElasticSearch database. I am  using the following code to connect to ElasticSearch

```
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.transport.TransportClient;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.SearchHit;


 public  TransportClient openConnection(String ipAddress, int ipPort) throws UnknownHostException {

    Settings settings = Settings.settingsBuilder().put("cluster.name", "elasticsearch").build();
    TransportClient client = TransportClient.builder().settings(settings).build().
            addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(ipAddress), ipPort));

    return client;

}
```

When I run it locally, i.e. `spark-submit --master local[*]` everything runs OK. When I run it in a google cloud spark cluster I get the following Exception: 

```
 java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.threadpool.ThreadPool at 
org.elasticsearch.client.transport.TransportClient$Builder.build(TransportClient.java:133) at 
javaTools.ElasticSearchConnection.openConnection(ElasticSearchConnection.java:24)
```

The last referred method (`openConnection`) is the connection described above. 

The code is uploaded to the google cloud using a fat jar created using sbt asssembly, so all libraries used are common. 
</description><key id="152523996">18083</key><summary>TransportClient "Could not initialize class" Exception in Google Cloud Yarn cluster.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">orestisgorgas</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-05-02T10:00:04Z</created><updated>2016-10-08T11:19:16Z</updated><resolved>2016-10-08T11:19:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bhdrk" created="2016-08-15T22:16:39Z" id="239946232">+1
</comment><comment author="ryandonglin" created="2016-09-26T00:53:10Z" id="249458240">also met the same problem in my project, Some people said because of lacking guava dependency, but after import the guava repository, still the same problem
</comment><comment author="ryandonglin" created="2016-09-26T01:16:35Z" id="249459968">already solve the problem, just import guava-18.0 dependency to your project, hete is the pom:

&lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;18.0&lt;/version&gt;
        &lt;/dependency&gt;
</comment><comment author="clintongormley" created="2016-10-08T11:19:16Z" id="252419201">thanks for letting us know @ryandonglin 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail automatic string upgrade if the value of `index` is not recognized.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18082</link><project id="" key="" /><description>Closes #18062
</description><key id="152510903">18082</key><summary>Fail automatic string upgrade if the value of `index` is not recognized.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-05-02T09:00:39Z</created><updated>2016-05-02T15:30:38Z</updated><resolved>2016-05-02T15:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-05-02T14:40:30Z" id="216253487">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>jQuery library does not work when minified. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18081</link><project id="" key="" /><description>When using **elasticsearch.jquery.js**, everything works fine.

When using **elasticsearch.jquery.min.js** I get the following errors:

![screen shot 2016-05-02 at 10 11 31 am](https://cloud.githubusercontent.com/assets/329834/14948938/5035c078-104e-11e6-9d5a-41f7bb8b5a7d.png)
</description><key id="152493546">18081</key><summary>jQuery library does not work when minified. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nasht00</reporter><labels /><created>2016-05-02T07:12:06Z</created><updated>2016-05-02T14:02:08Z</updated><resolved>2016-05-02T08:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-02T08:17:44Z" id="216133258">I think that you should report that here: https://github.com/elastic/elasticsearch-js
</comment><comment author="nasht00" created="2016-05-02T14:02:08Z" id="216243860">I think they already opened it: https://github.com/elastic/elasticsearch-js/issues/369
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>String Column Aggregation Will Stops Working Elasticsearch-2.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18080</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.8

**OS version**: Cent OS

**Description of the problem including expected versus actual behavior** AND **Steps to reproduce**: 
actually we have one large application and in that the indexing always going on around min 500-1000 logs per second, and we have our defined syntax for users to execute searching in elasticsearch. 
    But now we have one biggest issue after elastic version 2.3.2 that while indexing is going on and our user try to get aggregation on data like sum, min, max etc.. on that time customer doesn't know that the column is string or numeric so. Elasticsearch throws Error (SERVER_ERROR) that need numeric field but found string field. 
   In this case Elasticsearch is right and scenario is perfect but after this happens more then two times Elasticsearch stop working It's not even responding in any plugin. It's also not even give status by http://localhost:9200  so Please resolve my problem and reply me as soon as possible.

We have very big work with Elasticsearch and then we stuck over here.
</description><key id="152430877">18080</key><summary>String Column Aggregation Will Stops Working Elasticsearch-2.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ervivekmehta</reporter><labels /><created>2016-05-01T21:02:21Z</created><updated>2016-05-02T14:04:07Z</updated><resolved>2016-05-01T23:03:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-01T23:03:00Z" id="216080061">This looks like a general question more than it does a bug report in Elasticsearch. Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.
</comment><comment author="ervivekmehta" created="2016-05-02T06:01:03Z" id="216111784">Brother I know Issues are only going to post over here. And this is seriously biggest issue for us. Elasticsearch stop working after this aggregation Exception occurs.
[: Expected numeric type on field [severity], but got [string]]

You must dig in to this brother. It is a bug. I think Elasticsearch throw and error it's a perfect way but it's stop working after throwing this exception, I think this bug must be resolved bro. because from client side we cant even able to predict that which column client will select for aggregation. and if any other way then please post here or we can communicate through mail.
</comment><comment author="nik9000" created="2016-05-02T14:04:07Z" id="216244290">It is kind of the client responsibility not to send bogus aggregations. Elasticsearch should certainly reply with 400-level responses when it does which does sound like a bug but ultimately you are responsible for understanding the schema of your data. You can have the client look at the mappings before giving the user the list of things they can aggregate on. https://github.com/elastic/elasticsearch/pull/17980 also should help with that in 5.0 too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy scripting - possible memory leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18079</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.1

**JVM version**: openjdk version "1.8.0_71-debug"
OpenJDK Runtime Environment (build 1.8.0_71-debug-b15)
OpenJDK 64-Bit Server VM (build 25.71-b15-debug, mixed mode)

**OS version**: CentOS 6.7, kernel: 2.6.32-573.18.1.el6.x86_64

**Description of the problem including expected versus actual behavior**:  I'm using the Java client and a couple of dynamic Groovy scripts to update certain fields and nested objects in my documents. As can be seen by my [node stats](https://gist.github.com/anonymous/5e5a47a3f5476f61e2b30464bd96ccf7), almost all of my largish heap is being used up, even though I'm not using any field_data (the index itself is only around 7g, but 12g of heap is being used). I did a heap dump and a leak suspects analysis that seems to point to a memory leak involving Groovy classes: 
![screen shot 2016-05-01 at 1 58 25 pm](https://cloud.githubusercontent.com/assets/9867183/14943466/389e444a-0fa8-11e6-8de5-f7bb2dacc7e4.png)

**Steps to reproduce**: Unclear if can be easily reproduced in different context

**Provide logs (if relevant)**: Node stats and leak suspects screenshot above
</description><key id="152330907">18079</key><summary>Groovy scripting - possible memory leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PeteyPabPro</reporter><labels><label>:Scripting</label></labels><created>2016-05-01T18:28:32Z</created><updated>2016-05-06T09:23:44Z</updated><resolved>2016-05-06T09:23:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="PeteyPabPro" created="2016-05-01T23:09:58Z" id="216080415">Some more details about how I'm using Groovy scripting can be found on the Stack Overflow [question](http://stackoverflow.com/questions/36918850/elasticsearch-using-a-lot-of-memory-gc-thrashing) I asked about this.
</comment><comment author="ywelsch" created="2016-05-02T09:24:12Z" id="216169797">This is a tricky one.

In Groovy 2.4.0, a memory leak was fixed (https://issues.apache.org/jira/browse/GROOVY-6704) by using `java.lang.ClassValue`s. Later, it was discovered that this fix introduced another leak caused by a JVM bug (https://bugs.openjdk.java.net/browse/JDK-8136353). From Groovy 2.4.5 on (we use 2.4.6 in Elasticsearch), the decision to use classvalues was reversed (https://issues.apache.org/jira/browse/GROOVY-7591). This reintroduced the old memory leak (which I guess is the one you're seeing here). You can rerevert back to the 2.4.0 - 2.4.4 groovy behavior by setting the system property `-Dgroovy.use.classvalue=true`. Note that this reintroduces the other leak. This essentially means: pick your poison. A test that I used to verify this bug report did not run into OOM issues when using `-Dgroovy.use.classvalue=true`, but that might not hold for your scripts.

I would suggest that you use native scripts written in Java (https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html#native-java-scripts).
</comment><comment author="PeteyPabPro" created="2016-05-02T14:51:16Z" id="216256349">1) Thanks, I'll try the system property first and see how that goes. I can add that to ES_JAVA_OPTS, correct?

 2) Dynamic scripting is a bit more suitable for our workflow - do you think using something like Javascript instead of Groovy would be a reasonable course of action if there is still a leak after setting the system property?
</comment><comment author="nik9000" created="2016-05-02T14:56:13Z" id="216257855">&gt; 2) Dynamic scripting is a bit more suitable for our workflow - do you think using something like Javascript instead of Groovy would be a reasonable course of action if there is still a leak after setting the system property?

This is why lang-painless is a thing. It is still very much "incubating" but it is going to the answer in the future I think. If you can live with groovy being, well, groovy until 5.0 the best thing to do is to wait for 5.0 try and port stuff to lang-painless as part of your "migrate to 5.0" process and file issues when you hit things it can't do. Well, that is the best thing for _us_ and I think it'd be a good choice for you too, but I'm biased.
</comment><comment author="PeteyPabPro" created="2016-05-03T17:19:45Z" id="216601612">@nik9000 Definitely looking forward to 5.0, and will certainly move over to lang-painless! But unfortunately the leak is problematic in the meantime...

@ywelsch I ran it with `-Dgroovy.use.classvalue=true`, and another leak did pop up. Does this one seem to be the ClassValue one you mentioned?
![leak suspects 2](https://cloud.githubusercontent.com/assets/9867183/14991772/103b75c4-1131-11e6-81bb-dc64f18cb887.png)
</comment><comment author="clintongormley" created="2016-05-06T09:23:44Z" id="217395282">Not much more that we can do here other than make Painless better, or wait for Groovy to fix this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change qoutes to quotes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18078</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

Updated misspelling in documentation
</description><key id="152046070">18078</key><summary>Change qoutes to quotes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tdicken73</reporter><labels><label>docs</label></labels><created>2016-04-30T18:48:11Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-05-02T11:06:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-02T11:06:04Z" id="216203362">thanks @tdicken73 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for .empty to expressions, and some docs improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18077</link><project id="" key="" /><description>Its possible to treat missing values as something else other than zero today, but its not intuitive: e.g. you can do it with something like `doc['field'].count() == 0 ? 500 : doc['field']`.

There are two problems with this:
- aggregate methods on fields don't seem to be documented anywhere.
- its not as intuitive as .empty, making it inconsistent with other scripting languages.

I think we should fix both of these: we should document count()/max()/avg()/sum()/min()/etc so that people can work better with multi-valued fields, and we should syntactic sugar (.empty) to make handling empty cases easier.
</description><key id="152029515">18077</key><summary>Add support for .empty to expressions, and some docs improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Expressions</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-30T13:09:14Z</created><updated>2016-05-02T13:08:25Z</updated><resolved>2016-05-02T13:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-04-30T13:10:26Z" id="215962342">Here is a quick hack. It required some refactoring, as we didnt really have things logically organized to support "variables" other than `.value`. @jdconrad can you look at this?
</comment><comment author="rmuir" created="2016-04-30T14:23:27Z" id="215969020">as far as the re-organization, i had in mind we may want to support some other "variables" that the other script engines support besides just `.value` and `.empty`. For example, maybe simple additions like `.lat`/`.lon` so that geo fields can start to work with expressions too? 
</comment><comment author="rmuir" created="2016-04-30T15:23:24Z" id="215973033">Another TODO: in the docs. They emphasize `Only numeric fields may be accessed` but i don't think its necessarily clear that date fields will work too. I doubt most people immediately jump to the conclusion that date fields are numbers too, except us.
</comment><comment author="jdconrad" created="2016-04-30T21:56:20Z" id="215997688">LGTM.  Thanks for adding this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid JSON Being Returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18076</link><project id="" key="" /><description>**Elasticsearch version**:
2.3.2

**JVM version**:
1.8.0_60 OpenJDK 64-Bit Server VM 25.60-b23 Oracle Corporation

**OS version**:
OpenSUSE Leap 42.1

**Description of the problem including expected versus actual behavior**:
Invalid JSON returned in Elasticsearch results.  Expect results from Elasticsearch to be valid JSON.

**Steps to reproduce**:
We have an aggregation that includes longitude, latitude and location name separated by a pipe.  ES isn't escaping the quotes within one of them.  ES returns this as results:

{"took":59,"aggregations":{"time_group":{"buckets":[{"key":78469,"d":{"buckets":[{"d":{"buckets":[{"key":"34.774663|-112.467699|Chino Valley Fire Department"}]}}]},"w":{"value":1.0}},{"key":78388,"d":{"buckets":[{"d":{"buckets":[{"key":"34.775424|-112.452459|Windmill 7"}]}}]},"w":{"value":1.0}},{"key":78333,"d":{"buckets":[{"d":{"buckets":[{"key":"34.773574|-112.465953|"19 Remembered" Hotshot Honors"}]}}]},"w":{"value":1.0}},{"key":78309,"d":{"buckets":[{"d":{"buckets":[{"key":"34.773311|-112.46529|Chino Valley Saluted America's Heros"}]}}]},"w":{"value":1.0}},{"key":78305,"d":{"buckets":[{"d":{"buckets":[{"key":"34.77383|-112.465432|Chino Valley Public Library"}]}}]},"w":{"value":1.0}},{"key":78112,"d":{"buckets":[{"d":{"buckets":[{"key":"34.769398|-112.447186|Chino Valley Community Center Park"}]}}]},"w":{"value":1.0}},{"key":77976,"d":{"buckets":[{"d":{"buckets":[{"key":"34.777816|-112.447498|Stoned Security Donkey"}]}}]},"w":{"value":1.0}},{"key":77830,"d":{"buckets":[{"d":{"buckets":[{"key":"34.772045|-112.427787|Peavine Trails"}]}}]},"w":{"value":1.0}},{"key":77585,"d":{"buckets":[{"d":{"buckets":[{"key":"34.644372|-112.432358|Prescott Municipal Airport"}]}}]},"w":{"value":1.0}},{"key":77371,"d":{"buckets":[{"d":{"buckets":[{"key":"34.760367|-112.447588|Hope Lutheran Church"}]}}]},"w":{"value":1.0}},{"key":56892,"d":{"buckets":[{"d":{"buckets":[{"key":"35.483665|-111.556402|Coconino National Forest"}]}}]},"w":{"value":0.89}},{"key":51987,"d":{"buckets":[{"d":{"buckets":[{"key":"35.313604|-112.852838|Seligman, Arizona"}]}}]},"w":{"value":0.84}}]}}}

Specifically this bucket key isn't escaped:

{"key":"34.773574|-112.465953|"19 Remembered" Hotshot Honors"}
</description><key id="152019984">18076</key><summary>Invalid JSON Being Returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">shawn-digitalpoint</reporter><labels><label>:REST</label><label>bug</label></labels><created>2016-04-30T09:28:38Z</created><updated>2016-08-05T10:27:02Z</updated><resolved>2016-08-05T10:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shawn-digitalpoint" created="2016-04-30T23:49:36Z" id="216003407">Found a workaround... if you don't use the filter_path, the results are properly escaped.
</comment><comment author="jpountz" created="2016-05-01T19:52:11Z" id="216067758">@shawn-digitalpoint then this looks like a bug in how `filter_path` works. Can you share your request (including the `filter_path`)?
</comment><comment author="shawn-digitalpoint" created="2016-05-02T22:47:56Z" id="216387792">Using PHP to build the JSON, but this is the PHP array (not sure why github's code block isn't working, sorry...):

```
$params = array(
  'size' =&gt; 0,
  'sort' =&gt; array(
    'date' =&gt; array(
      'order' =&gt; 'desc'
    )
  ),
  'query' =&gt; array(
    'bool' =&gt; array(
      'must' =&gt; array(
        array(
          'match' =&gt; array(
            'attacking_agent_id' =&gt; 100,
          ),
        ),
        array(
          'range' =&gt; array(
            'date' =&gt; array(
              'gte' =&gt; 1430606758
            )
          ),
        ),
      )
    )
  ),
  'aggs' =&gt; array(
    'time_group' =&gt; array(
      'terms' =&gt; array(
        'field' =&gt; 'location_id',
        'size' =&gt; 100000,
        'order'=&gt; array(
          '_term' =&gt; 'desc'
        ),
      ),

      'aggs' =&gt; array(
        'd' =&gt; array(
          'terms' =&gt; array(
            'field' =&gt; 'timegroup_agent',
            'size' =&gt; 1
          ),
          'aggs' =&gt; array(
            'd' =&gt; array(
              'terms' =&gt; array(
                'field' =&gt; 'raw_geo_name',
                'size' =&gt; 1
              )
            ),
          )
        ),
        'total_date' =&gt; array(
          'sum' =&gt; array(
            'field' =&gt; 'date'
          )
        ),
        'unique_attacks' =&gt; array(
          'cardinality' =&gt; array(
            "field" =&gt; "timegroup_agent"
          )
        ),

        "w" =&gt; array (
          'bucket_script' =&gt; array(
            'buckets_path' =&gt; array(
              'totalDate' =&gt; 'total_date',
              'unique' =&gt; 'unique_attacks',
              'docCount' =&gt; '_count',
            ),
            "script" =&gt; 'min(' . $maxWeight . ', max(' . $minWeight . ', round(((' . ($daysBack * 86400) . ' * docCount) - ((' . (XenForo_Application::$time) . ' * docCount) - totalDate)) / docCount / ' . ($daysBack * 86400) . ' * unique * 100) / 100))'  // round(((' . ($daysBack * 86400)  . ' * docCount) - ((' . XenForo_Application::$time . ' * docCount) - totalDate)) / ' . ($daysBack * 86400) . ' * 100) / 100
          )
        ),
      )
    ),
  )
);
```
</comment><comment author="shawn-digitalpoint" created="2016-05-02T22:52:08Z" id="216388583">Oh, and the filter_path used when getting invalid JSON is:

took,aggregations.time_group.buckets.key,aggregations.time_group.buckets.d.buckets.d.buckets.key,aggregations.time_group.buckets.w
</comment><comment author="tlrx" created="2016-05-06T11:56:06Z" id="217421099">Thanks for reporting! It's probably a Jackson bug so I created FasterXML/jackson-core/pull/280 to see and discuss this issue with the Jackson team. I'll update this issue once I have some feedback.
</comment><comment author="tlrx" created="2016-05-09T08:42:52Z" id="217808184">The Jackson issue has been confirmed and merged. This issue will be resolved once we move on a new release of jackson-core that integrates the fix.
</comment><comment author="s1monw" created="2016-05-09T10:19:10Z" id="217827948">awesome @tlrx great to fix this kind of stuff upstream directly!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Maybe we should execute the snippets in our docs?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18075</link><project id="" key="" /><description>I think we should execute the snippets we have in our docs as tests. We have this handy dandy rest test framework. Why not cram those snippets into it? Well, the snippets are in JSON and the rest framework is in YAML. But we can still do it with the power of really big strings!

This PR is both terrible and wonderful. It is full of horrible, horrible hacks. They provide a way for us to actually test our documentation to make sure it doesn't go out of date. Please, review this and suggest ways to have less hacks.

If we end up going this way (I hope we do!) then this is more of a beginning step. This starts to give us tools to make decisions about the snippets in our docs at build time.

Edit: mostly we've resolved the hacks now. So the PR is probably not terrible!
</description><key id="152001521">18075</key><summary>Maybe we should execute the snippets in our docs?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-04-30T02:27:42Z</created><updated>2016-05-05T18:50:06Z</updated><resolved>2016-05-05T18:20:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-30T02:28:45Z" id="215926382">cc @polyfractal 
</comment><comment author="nik9000" created="2016-04-30T02:51:49Z" id="215927463">In my exuberance I didn't describe the net result of the patch: if you stick `// TEST` under a snippet in the docs it'll generate a rest test that runs the command. This isn't a hugely useful test, but it is nice. It only really works with `// AUTOSENSE` style snippets.

It also adds `// TESTRESPONSE` which you can stick on a snippet. If you do it'll add an assertion to the test generated by the last `// TEST` snippet that checks if the response matches the response.
</comment><comment author="dadoonet" created="2016-04-30T05:56:13Z" id="215940648">IMHO we should do the opposite.

Build doc from existing tests.

I described this there https://github.com/elastic/docs/issues/4

That said, that's nice.
</comment><comment author="jpountz" created="2016-05-02T08:05:12Z" id="216128321">I am not familiar enough with gradle to fully understand the build part, but in general I like the PR. Maybe you can list the hacks you are the least happy with so that we can think about how to make things better?

Related to #12583.
</comment><comment author="nik9000" created="2016-05-02T12:35:14Z" id="216224026">&gt; Related to #12583.

Thanks for that!

&gt; list the hacks you are the least happy with

Sure! I'll go leave comments at them.
</comment><comment author="nik9000" created="2016-05-03T19:30:54Z" id="216639897">OK! I've just finished getting (almost) all the `// AUTOSENSE` snippets passing! They generate tests. Most pass all on their own. Sometimes we do tricks like sneak a `PUT my_index` into the front of the tests they make to get them working and not change the output.

It is still worth going through the docs and making sure that lots of places use `// AUTOSENSE` just so we can actually catch stuff.
</comment><comment author="jpountz" created="2016-05-04T08:04:27Z" id="216774046">This looks wonderful. There are some hacks indeed, but they look manageable to me. It would be great if someone who is better educated about gradle than I am could take a look.

@clintongormley this would be a big change to our docs, do you have any opinions?
</comment><comment author="nik9000" created="2016-05-04T12:36:22Z" id="216848657">I'm removing the `discuss` label in that case.

@clintongormley will have to decide what the right way to label this is though - I just put `docs` and `build` and `test` because it touches those things in major ways but I think `docs` is reserved for PRs that _only_ update docs so the script that builds that release notes can skip them?

Anyway, I have two things left on my list to do, but I'd love a review in the mean time:
- Get that one less file running. Most of it has to be skipped because it modifies the cluster in bad ways. We should really (separately) have a way to reset the cluster settings between these tests. I think that'd unblock a few tests.
- Add a notification to the failures giving you some hint what file the tests were generated from.
</comment><comment author="rjernst" created="2016-05-04T23:19:54Z" id="217036456">This looks good to me to get in so we can iterate and improve on it. The one comment which I have is about simplifying the configuration. I think we should use `ConfigurableFileTree`, and this should allow removing this extra root dir input which confused me.
</comment><comment author="nik9000" created="2016-05-05T14:33:34Z" id="217170282">OK! I've cleaned up quite a lot this morning. I've handled all of @rjernst's points. His help was super useful and let me resolve the issue around the really annoying/confusing extra root.

I'm going ton keep using the exploded way of printing error messages for mismatches in the body for now. I don't really like it too much but it produces nice error messages.

I'm going to get that one last file running and then squash and rebase. There will be conflicts but I expect them just to be with docs files. Then I think it is time to merge!
</comment><comment author="nik9000" created="2016-05-05T17:36:01Z" id="217220117">&gt; get that one last file running

Yeah, that'll have to wait: https://github.com/elastic/elasticsearch/issues/18159
</comment><comment author="rmuir" created="2016-05-05T18:50:06Z" id="217241081">thanks for doing this, great idea
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add ability to specify multiple grok patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18074</link><project id="" key="" /><description>now you can specify a list of grok patterns to match your field with
and the first one to successfully match wins.

Fixes #17903.
</description><key id="151970113">18074</key><summary>add ability to specify multiple grok patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-29T21:13:50Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-25T19:20:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-05-01T11:25:17Z" id="216035781">Is there a possibility to find out which match actually matched for debugging/simulating?

Aside from that LGTM
</comment><comment author="talevy" created="2016-05-02T15:10:42Z" id="216262860">&gt; Is there a possibility to find out which match actually matched for debugging/simulating?

Like a special debug section that would be added to the response of `simulate?verbose` responses?
</comment><comment author="spinscale" created="2016-05-02T17:14:04Z" id="216297601">I havent spent too much thoughts into it, how to do it, I just figured that this might be important for debugging. Simulate API w/ verbose sounds about right to me. That does not need to be part of this PR however!
</comment><comment author="rjernst" created="2016-05-02T18:08:06Z" id="216313606">This feature (multiple groks) seems trappy. It operates linearly on the number of patterns. Instead, I think it would be better to fix grok to be able to have multiple patterns in a single grok (so it can be compiled to a single regex)?
</comment><comment author="talevy" created="2016-05-02T18:26:32Z" id="216319218">@rjernst, true. It would be more performant to scan through the text once. I guess I was looking for a "quick" fix here and was not too worried about performance for these less frequent multi-regex cases.

I will see what it will take to do this so that it can merge N patterns together, and tell you which matched (if any do). Might be difficult to extract the subcaptures of potentially overlapping expressions
</comment><comment author="talevy" created="2016-05-02T18:50:50Z" id="216325902">@rjernst, after thinking about it for a little bit. what would your expected behavior be for when multiple expressions match? how would you deterministically decide on a winner without this trappiness of order?

edit: I misunderstood trappiness to mean the linearity of matching rather than the concern over performance. please disregard the above comment &#128516; 
</comment><comment author="clintongormley" created="2016-05-03T09:46:14Z" id="216482194">&gt; Instead, I think it would be better to fix grok to be able to have multiple patterns in a single grok (so it can be compiled to a single regex)?

This could work as long as users only use named captures.  If they use positional captures then stringing regexes together will break.
</comment><comment author="talevy" created="2016-05-05T05:41:13Z" id="217080234">@clintongormley you're right. We are only interested in non-overlapping named-capture matches. so a simple union will do the trick.

@rjernst I have updated this to combine the patterns into one grok expression using `|`

@spinscale As part of this, I have introduced a `trace_match` flag for debugging which pattern matched. When set, the GrokProcessor will inject the index into the `patterns` list of the pattern that matched. The field is `_ingest._grok_match_index`. The combined regex expression gets rather complicated with this flag turned on because I introduce a fake `(?&lt;_ingest._grok_match_index.IDX&gt;^).*` pattern in front of each pattern to match the beginning of that pattern.

for example, this would be injected for a patterns list whose second pattern was matched.

```
_ingest._grok_match_index.0 =&gt; ""
```

Then I update this value to be

```
_ingest._grok_match_index =&gt; "0"
```

upon exiting from the processor, so this is what people actually see.

does that make sense?

A side-effect of multiple patterns that can match is that all the non-matched pattern's fields were inserted 
into the document as `null` values. This polluted the document with unrelated fields. In the latest revision, I filter these out. So, if a user expected a null match in their regex, I will simply not insert that field.

^ this kind of bothers me, but I am not sure how to remedy this at the moment
</comment><comment author="talevy" created="2016-05-05T05:59:36Z" id="217081977">```
org.elasticsearch.smoketest.IngestWithMustacheIT.test {yaml=ingest/20_combine_processors/Test logging}
```

is failing. I believe this is due to he `null` value filtering. will investigate further
</comment><comment author="clintongormley" created="2016-05-05T09:09:25Z" id="217107978">&gt; The combined regex expression gets rather complicated with this flag turned on because I introduce a fake (?&lt;_ingest._grok_match_index.IDX&gt;^).\* pattern in front of each pattern to match the beginning of that pattern.

Nice solution!

&gt; In the latest revision, I filter these out. So, if a user expected a null match in their regex, I will simply not insert that field.  this kind of bothers me, but I am not sure how to remedy this at the moment

Isn't it just a matter of only filtering out null matches for names beginning with `_grok_match_index`?
</comment><comment author="talevy" created="2016-05-05T19:20:15Z" id="217250763">@clintongormley 
when `trace_match == false` then I do introduce meta name-captures, I only wrap them in an 
unnamed group. So, I cannot differentiate between the two. I thought it would be more performant that way, since I do not care about tracking. I will do some benchmarking on this to find out if this is _actually_ more performant.
</comment><comment author="talevy" created="2016-05-06T19:28:02Z" id="217537128">I don't think there is a way to distinguish between the different fields without prefixing each recursive named capture with the tracking `_ingest._grok_match_index.IDX`. should I do this?
</comment><comment author="talevy" created="2016-05-10T18:19:01Z" id="218244776">ping for followup
</comment><comment author="martijnvg" created="2016-05-19T09:15:20Z" id="220270434">Left one test related comment. Aside from that LGTM. I like the pattern combining logic! 
</comment><comment author="talevy" created="2016-05-19T18:48:23Z" id="220417525">@martijnvg, thanks!

what are your thoughts on this:

&gt; In the latest revision, I filter these out. So, if a user expected a null match in their regex, I will simply not insert that field. this kind of bothers me, but I am not sure how to remedy this at the moment

beforehand we would store all named captures as fields... even if they were null. Now that _all_ named captures (from different regexes) are in one expression, I cannot really distinguish between them, so I went with not inserting any of them if they were null.
</comment><comment author="martijnvg" created="2016-05-20T07:22:02Z" id="220535755">&gt; beforehand we would store all named captures as fields... even if they were null. Now that all named captures (from different regexes) are in one expression, I cannot really distinguish between them, so I went with not inserting any of them if they were null.

Discussed this with @talevy and we think it is okay to ignore null keys in the ingest document if the grok parser can't match the named capture. (before we would add keys with null values)
</comment><comment author="talevy" created="2016-05-24T20:40:12Z" id="221394524">@martijnvg added unit tests for `combinePatterns`
</comment><comment author="martijnvg" created="2016-05-25T07:58:07Z" id="221500350">Left one minor comment. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 5 wont start with config setting index.number_of_shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18073</link><project id="" key="" /><description>ES 5.0.0 apha1 fails to start with  setting "index.number_of_shards: 1" in elasticsearch.yml. However, same config file for version elasticsearch-2.2.0 and elasticsearch-2.3.2 works fine.

As per documentation in Configuration section it says it should work
https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html
"Index level settings can be set on the node level as well, for example, within the elasticsearch.yml file...."

Exception in log file:
curl -XPUT 'http://localhost:9200/_all/_settings?preserve_existing=true' -d '{
  "index.number_of_shards" : "1"
}'

---

[2016-04-29 16:13:59,193][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: node settings must not contain any index level settings
    at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:113)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:103)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:148)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:229)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:189)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)

Issue found on:
Elasticsearch version: elasticsearch-5.0.0-alpha1
OS version: Windows 10
Java version 1.8.0_77
</description><key id="151964012">18073</key><summary>ES 5 wont start with config setting index.number_of_shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mubinn</reporter><labels /><created>2016-04-29T20:39:45Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-04-29T22:32:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-29T22:32:50Z" id="215898684">The exception message tells you the problem. Index settings can no longer be set through `elasticsearch.yml`. You need to actually set these in your index (or indirectly by creating indexes with templates).
</comment><comment author="clintongormley" created="2016-05-02T10:50:08Z" id="216196930">The error message in 5.0.0-alpha2 is much nicer and tells you exactly what needs doing:

```
*************************************************************************************
Found index level settings on node level configuration.

Since elasticsearch 5.x index level settings can NOT be set on the nodes
configuration like the elasticsearch.yaml, in system properties or command line
arguments.In order to upgrade all indices the settings must be updated via the
/${index}/_settings API. Unless all settings are dynamic all indices must be closed
in order to apply the upgradeIndices created in the future should use index templates
to set default values.

Please ensure all required values are updated on all indices by executing:

curl -XPUT 'http://localhost:9200/_all/_settings?preserve_existing=true' -d '{
  "index.number_of_shards" : "3"
}'
*************************************************************************************
```
</comment><comment author="t3chn0m4g3" created="2016-06-29T16:22:03Z" id="229409604">Could you please clarify why that is the case? What are the pros / cons?
</comment><comment author="mitchswaldman" created="2016-06-29T21:07:05Z" id="229488999">Is there a new file to declare index level settings in? Or must you update each index's settings through an API call?
</comment><comment author="jasontedor" created="2016-06-29T21:10:30Z" id="229489918">&gt; Is there a new file to declare index level settings in? Or must you update each index's settings through an API call?

You can use [index templates](https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html).
</comment><comment author="mitchswaldman" created="2016-06-29T21:15:02Z" id="229491058">@jasontedor  I'm actually having a little bit of an issue with my index templates in 5.0.0-alpha3. #19163 Maybe you can help me out with that too ;) 
</comment><comment author="jasontedor" created="2016-06-30T02:56:27Z" id="229547006">&gt; Maybe you can help me out with that too ;)

It's likely that someone else will get to it before I have a chance to.
</comment><comment author="s1monw" created="2016-06-30T07:17:41Z" id="229579221">&gt; Could you please clarify why that is the case? What are the pros / cons?

@t3chn0m4g3 sure - lemme iterate some of the points:
- the yaml file is for node level configurations. The settings can be different from node to node ie. if a node has more CPU than another you might want different settings on the threadpool or you have repository configured or you have some nodes with more than one disk etc.
- all settings that start with `index.` are index level setting and must be configured in a way that whatever node is the master when the index is created the result is the same. We can't guarantee this with node level settings. (yet there is one exception `index.codec` since this is really a node level setting in the way it's used)
- our construct to set defaults for an index are index templates that can hold default configurations, supporting this on the yaml file is yet another source of defaults which causes a bigger level of complexity since all these settings must be consequently merged together. 
- not allowing these settings in a yaml file reduces the chance of inconsistent configuration - this is now also guaranteed for users not useing a package manager (you would be suprised!!)
- using templates streamlines the way default settings are passed to ES
- also there are way less surprises, lets say you create an index with a template and the template sets `index.number_of_replicas: 2` but in your yaml you have `index.number_of_replicas: 0` the latter will never have any effect since templates are superseding node configurations. 

&gt; Is there a new file to declare index level settings in? Or must you update each index's settings through an API call?

@mitchswaldman you can user templates for this so it's a single API call and it's persistent across restarts. You don't need to update existing indices since the settings in the yaml file are just defaults.
</comment><comment author="t3chn0m4g3" created="2016-07-04T09:14:26Z" id="230243994">@s1monw Thank you for sharing a much appreciated and detailed answer!
I think it makes a lot of sense, however for me it complicates things. I am using an ELK stack within a docker container, storing all information outside the container on the host. Is there any best practice on how to tell elasticsearch to use `index.number_of_shards: 1` and `index.number_of_replicas: 0`? I could use the `curl` method of course, but I can only do so after the image is built and the container is running on the host; resulting in Xputting the settings at every container start.
Maybe I am just blindsided and you see a different approach?
</comment><comment author="s1monw" created="2016-07-04T10:11:05Z" id="230255724">no you have to set this through an index template. I am sorry that this complicates things for you but there is no way to preset index values on a node level anymore. 
</comment><comment author="t3chn0m4g3" created="2016-07-04T10:42:16Z" id="230261541">Thanks for the update. I have not worked with templates yet. Can you link a good starting point / how to?
</comment><comment author="s1monw" created="2016-07-04T12:02:18Z" id="230274908">@t3chn0m4g3 here you go https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html

the template is stored in the cluster state and that is persistent. If you do this:
1. start node
2. put template with `index.number_of_replicas=0`
3. stop node
4. start node
5. create index 

the index will have `0` replicas, maybe that simplifies your scenario? The good thing is if that node joins a cluster it gets the right templates form the cluster instead.
</comment><comment author="t3chn0m4g3" created="2016-07-05T11:58:15Z" id="230458682">Thank you!
</comment><comment author="forewarned" created="2016-07-15T13:42:35Z" id="232953810">If we have to apply settings on an index level, is there a default template somewhere for documents that do not already match a template in Elastic? If so, how do we access and modify it?
</comment><comment author="clintongormley" created="2016-07-15T15:26:51Z" id="232983872">You can create a template which matches `*`
</comment><comment author="zhangtanzhang" created="2017-03-21T09:19:05Z" id="288019839">index.analysis.analyzer.default: "analyzer-ik" &#65292;how to configer it in elasticsearch.yml</comment><comment author="zhangtanzhang" created="2017-03-21T09:19:26Z" id="288019917">thank  you </comment><comment author="bleskes" created="2017-03-21T09:20:47Z" id="288020252">@zhangtanzhang see message above:

&gt; Index settings can no longer be set through elasticsearch.yml. You need to actually set these in your index (or indirectly by creating indexes with templates).

If you have any more questions please ask at discuss.elastic.co . We keep github for issues and feature requests. thx!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ScalingThreadPoolTests.testScalingThreadPoolThreadsAreTerminatedAfterKeepAlive CI Failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18072</link><project id="" key="" /><description>Seems to be semi-consistently failing:
- https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=opensuse/345/console
- https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=oraclelinux/343/console
- https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=opensuse/341/console
- https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/615/console

However, I was unable to reproduce any of these failures on either of my two machines. :(

```
gradle :core:test -Dtests.seed=A1BF5ED4DA80E472 -Dtests.class=org.elasticsearch.threadpool.ScalingThreadPoolTests -Dtests.method="testScalingThreadPoolThreadsAreTerminatedAfterKeepAlive" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=ar-LB -Dtests.timezone=Asia/Anadyr

FAILURE 0.14s J1 | ScalingThreadPoolTests.testScalingThreadPoolThreadsAreTerminatedAfterKeepAlive &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;128L&gt;
   &gt;      but: was &lt;127L&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([A1BF5ED4DA80E472:BC4ACB642517FFFA]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.threadpool.ScalingThreadPoolTests.lambda$testScalingThreadPoolThreadsAreTerminatedAfterKeepAlive$7(ScalingThreadPoolTests.java:196)
   &gt;    at org.elasticsearch.threadpool.ScalingThreadPoolTests.runScalingThreadPoolTest(ScalingThreadPoolTests.java:244)
   &gt;    at org.elasticsearch.threadpool.ScalingThreadPoolTests.testScalingThreadPoolThreadsAreTerminatedAfterKeepAlive(ScalingThreadPoolTests.java:167)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-04-29 19:31:31,374][WARN ][org.elasticsearch.common.settings] [testResizingScalingThreadPoolQueue] failed to apply settings
  1&gt; java.lang.IllegalArgumentException: thread pool [warmer] of type scaling can not have its queue re-sized but was [518323132]
  1&gt;    at org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:528)
  1&gt;    at org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:628)
  1&gt;    at org.elasticsearch.common.settings.Setting$3$1.apply(Setting.java:727)
  1&gt;    at org.elasticsearch.common.settings.Setting$3$1.apply(Setting.java:702)
  1&gt;    at org.elasticsearch.common.settings.AbstractScopedSettings$SettingUpdater.lambda$updater$0(AbstractScopedSettings.java:319)
  1&gt;    at org.elasticsearch.common.settings.AbstractScopedSettings.applySettings(AbstractScopedSettings.java:165)
  1&gt;    at org.elasticsearch.threadpool.ScalingThreadPoolTests.lambda$null$9(ScalingThreadPoolTests.java:226)
  1&gt;    at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2677)
  1&gt;    at org.elasticsearch.threadpool.ScalingThreadPoolTests.lambda$testResizingScalingThreadPoolQueue$10(ScalingThreadPoolTests.java:224)
  1&gt;    at org.elasticsearch.threadpool.ScalingThreadPoolTests.runScalingThreadPoolTest(ScalingThreadPoolTests.java:244)
  1&gt;    at org.elasticsearch.threadpool.ScalingThreadPoolTests.testResizingScalingThreadPoolQueue(ScalingThreadPoolTests.java:222)
  1&gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  1&gt;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  1&gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  1&gt;    at java.lang.reflect.Method.invoke(Method.java:498)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  1&gt;    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  1&gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  1&gt;    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  1&gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  1&gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  1&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
  1&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
  1&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
  1&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
  1&gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  1&gt;    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  1&gt;    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  1&gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  1&gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  1&gt;    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  1&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  1&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/opensuse/core/build/testrun/test/J1/temp/org.elasticsearch.threadpool.ScalingThreadPoolTests_A1BF5ED4DA80E472-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {}, docValues:{}, maxPointsInLeafNode=1221, maxMBSortInHeap=6.699272557998749, sim=RandomSimilarity(queryNorm=false,coord=yes): {}, locale=ar-LB, timezone=Asia/Anadyr
  2&gt; NOTE: Linux 3.16.7-29-default amd64/Oracle Corporation 1.8.0_72 (64-bit)/cpus=4,threads=1,free=431645528,total=517996544
  2&gt; NOTE: All tests run in this JVM: [JoinProcessorTests, URIPatternTests, PrimaryElectionRoutingTests, BootstrapSettingsTests, RecoveryStatusTests, ExceptionSerializationTests, DateFormatTests, AwarenessAllocationTests, LegacyLongFieldTypeTests, PipelineExecutionServiceTests, NodeVersionAllocationDeciderTests, FsBlobStoreContainerTests, ParentFieldTypeTests, TransportAnalyzeActionTests, SimpleMapperTests, EnvironmentTests, RebalanceAfterActiveTests, FailProcessorTests, TermVectorsUnitTests, SimulateProcessorResultTests, PercentilesTests, LockedRecyclerTests, MaxMapCountCheckTests, QueryPhaseTests, ByteUtilsTests, DiskThresholdDeciderTests, LongNestedSortingTests, ShardRoutingTests, ClusterChangedEventTests, YamlFilteringGeneratorTests, GeoQueryContextTests, BlockingClusterStatePublishResponseHandlerTests, GeoDistanceQueryBuilderTests, IndicesStatsTests, CompoundProcessorTests, HistogramTests, CamelCaseFieldNameTests, FieldDataCacheTests, LaplaceModelTests, FileInfoTests, SpanNearQueryBuilderTests, FuzzinessTests, CardinalityTests, MinTests, ScoreSortBuilderTests, MatchPhrasePrefixQueryBuilderTests, QueryShardContextTests, IndicesRequestCacheTests, Murmur3HashFunctionTests, EnvelopeBuilderTests, RenameProcessorTests, NodeClientHeadersTests, SearchSlowLogTests, WildcardExpressionResolverTests, ReverseNestedTests, KeepFilterFactoryTests, IndexSearcherWrapperTests, GeoUtilsTests, ScriptContextRegistryTests, ScalingThreadPoolTests]
```

```
BUILD INFO

Build   20160429192716-1B1E4023
Log https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=opensuse/345/console
Duration    5m 41s (341210ms)
Started 2016-04-29T19:27:16.547Z
Ended   2016-04-29T19:32:57.757Z
Exit Code   1
Host    slave-f1f8d528 (up 116 days)
OS  OpenSuSE 13.2, Linux 3.16.7-29-default
Specs   4 CPUs, 15.45GB RAM
java.version    1.8.0_72
java.vm.name    OpenJDK 64-Bit Server VM
java.vm.version 25.72-b15
java.runtime.version    1.8.0_72-b15
java.home   /usr/lib64/jvm/java-1.8.0-openjdk-1.8.0
```
</description><key id="151955388">18072</key><summary>ScalingThreadPoolTests.testScalingThreadPoolThreadsAreTerminatedAfterKeepAlive CI Failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>test</label></labels><created>2016-04-29T19:55:47Z</created><updated>2016-04-29T20:21:43Z</updated><resolved>2016-04-29T20:21:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-29T20:15:32Z" id="215868574">It's due to a race between the main test thread and last few thread pool threads that are executing tasks during the test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TermsQueryBuilderTests.testToQuery CI Failure due to rewritten TermsLookup Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18071</link><project id="" key="" /><description>Test failure replicates for me with:

```
gradle :core:test -Dtests.seed=84A9BC00947C2198 -Dtests.class=org.elasticsearch.index.query.TermsQueryBuilderTests -Dtests.method="testToQuery" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=bg-BG -Dtests.timezone=Europe/Kiev
```

Several related failures:
- https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=amazon/342/console
- https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=fedora/342/console

It looks like the test is generating a TermsLookup query:

``` json
{
  "terms" : {
    "TofcQn" : {
      "type" : "UJdzjobcpF",
      "id" : "bHARfpgRnh",
      "path" : "PFQXDKsBkU",
      "routing" : "bELufARfhy"
    },
    "boost" : 0.13333334,
    "_name" : "dkTVvamG0"
  }
}
```

But it get's rewritten into a MatchNoDocs query:

```
(MatchNoDocsQuery["No terms supplied for "terms" query."])^0.13333334
```

This causes the test to fail, because the test sees that `termslookup() != null`, so it skips past the check for MatchNoDocs and fails because the query is not actually a BooleanQuery

``` java
if (queryBuilder.termsLookup() == null &amp;&amp; (queryBuilder.values() == null || queryBuilder.values().isEmpty())) {
    assertThat(query, instanceOf(MatchNoDocsQuery.class));
} else {
    assertThat(query, instanceOf(BooleanQuery.class));
     ....
```

It feels like a test error, but I wasn't sure if the correct fix is to just change the conditional here, or something else.  I dont' quite understand how the lookup process happens in relation to rewrites. 

/cc @jimferenczi @MaineC @cbuescher  since you all worked on MatchNoDocs recently.
</description><key id="151911571">18071</key><summary>TermsQueryBuilderTests.testToQuery CI Failure due to rewritten TermsLookup Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>test</label></labels><created>2016-04-29T16:24:16Z</created><updated>2016-04-29T17:38:00Z</updated><resolved>2016-04-29T17:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-29T17:25:32Z" id="215822354">I found the problem and will push the corrected test shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin-descriptor.properties error for offline installation of Marvel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18070</link><project id="" key="" /><description>Installed new ES version 2.3.2, and installed the license and marvel-agent plugins according to the offline installation instructions located [here](https://www.elastic.co/guide/en/marvel/current/installing-marvel.html#offline-installation)

Received the following error:

```
/opt/elasticsearch/bin$ ./plugin install file:///home/user/marvel-2.3.2.tar.gz 
-&gt; Installing from file:/home/user/marvel-2.3.2.tar.gz...
Trying file:/home/user/marvel-2.3.2.tar.gz ...
Downloading ................DONE
Verifying file:/home/user/marvel-2.3.2.tar.gz checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip

```

And FYI, I am only doing this offline because the Marvel build is missing the zip file location.  When you run 
`./bin/plugin install marvel`
An error shows up, because the zip file does not exist on the ES server.  Something is wrong with the build.

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.2

**JVM version**: 1.7.0_91

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**: Marvel plugin will not install online or offline

**Steps to reproduce**:
1. Try plugin install marvel
2. Try to install offline with the tar.gz file using plugin install file:///path/to/marvel-2.3.2.tar.gz
   3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="151896318">18070</key><summary>plugin-descriptor.properties error for offline installation of Marvel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasonmclose</reporter><labels /><created>2016-04-29T15:31:54Z</created><updated>2016-04-29T15:49:21Z</updated><resolved>2016-04-29T15:41:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-29T15:41:44Z" id="215765695">I think you have the steps mixed up, marvel is a kibana plugin, not elasticsearch.  You should be installing marvel-agent.

&gt; When you run `./bin/plugin install marvel` An error shows up

Again, marvel is not a plugin for elasticsearch, it is called marvel-agent. And the instructions you link use file installation, not name based installation, so I'm not sure where the confusion came from?
</comment><comment author="jasonmclose" created="2016-04-29T15:49:21Z" id="215770769">You are absolutely correct.  Sorry for the trouble.  Thanks for catching that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Append-only indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18069</link><project id="" key="" /><description>Hi,
Elasticsearch currently doesn't have a parameter which switches indices in append only mode. This could be interesting from performance point of view.

We are dealing with a problem of limited ingestion rate in ES. We tried to focus on a couple of hot spots to resolve potential bottlenecks. One of them was DocValues merges for sparse fields, then after changing approach synchronous translog became a bottleneck. It even seems that contention problem is not negligible: https://github.com/elastic/elasticsearch/issues/18053. These are only a few places where we tried to find optimizations. While analyzing profiles in Java Mission Control it was hard to ignore the another major problem: versions lookup. See the image below.

![selection_383](https://cloud.githubusercontent.com/assets/6692291/14913686/309efd74-0e05-11e6-9437-f0497a9bcaca.png)

Profiler shows that versions lookups are taking up to 25% of CPU time. This is a lot. Of course one could say I am showing a very specific case because I am using tiny documents in tests. I think this is not exactly true. Elasticsearch stack is much focused on logs aggregation and recently also metrics (Logstash, Kibana, Marvel, Beats, ...). Most systems designed for metric or log aggregation indices are append-only. Of course ES is general purpose distributed search and analytics engine so document versions are really important ...  but in many cases append-only mode could be sufficient.
One of the major strengths of Lucene internally is that segments are append only. Why not to use this simple but powerful assumption on higher level? Not all ES use cases require concurrent update functionality in which versions control really matters. What if Elasticsearch indices are marked as append-only (on they are created. Indices have properties which cannot be changed after index is created (eg. number of shards). This could be another parameter which can be applied only on index creation.

The contention problem mentioned in https://github.com/elastic/elasticsearch/issues/18053 would also become less painful in append-only indices. Version lookup is inside synchronized block in InternalEngine#innerIndex. This means threads are in synchronized section for a longer time than needed (in case where versions are not needed).

During tests we disabled versions control in Elasticsearch which brought us significant improvement in indexing speed. Look at the chart below. Indexing speed on the same indices (without removing old data - just changed ES version) increased by 50% only by removing a version control.

My test environment is not very powerful. Only 2 machines with 4 CPU cores each. Indices have only one replica.

https://apps.sematext.com/spm-reports/s/ucSD8cuTRr
![selection_397](https://cloud.githubusercontent.com/assets/6692291/14913721/59e14ad4-0e05-11e6-8a7b-ab88f398e625.png)

I would really appreciate any feedback. Am I missing something obvious? Thanks
</description><key id="151864163">18069</key><summary>Append-only indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2016-04-29T13:22:14Z</created><updated>2016-12-22T08:02:51Z</updated><resolved>2016-05-06T09:27:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-29T13:29:38Z" id="215711175">Can you try rerunning the profile with the changes from #18060/#18067 to address the contention in #18053; you should see almost no contention now in an append-only workload. Please see #9125 and #13857 for why the version checks will not be removed.
</comment><comment author="prog8" created="2016-04-29T14:04:33Z" id="215726347">Hi Jason. Thank you for pointing me to the fix. It seems this is very fresh fix :) Contention is only small problem comparing to versions where you could really see CPU cycles wasted.

Thanks for sharing #9125 and #13857. They both confirm that ES is general purpose solution which prevents users from unexpected cases. I think in some cases one can ignore the fact that data is duplicated because of failed bulk which was sent to ES again. Of course this would be important problem for credit card transactions stored in index but it is not important at all if one keeps for example application monitoring datapoints.
</comment><comment author="mikemccand" created="2016-04-29T18:27:21Z" id="215839420">@prog8 which ES version is this?  Can you try with 5.0.0 alpha1?  E.g. we've made `mmapfs` the default, which is important for the version lookups since with `niofs` a seek + read a few bytes requires filling an 8 KB buffer each time.

Do your shards have a contained number of segments?  Version lookup cost can be linear cost in the number of segments.

Are you using ES's IDs?  ES assigns IDs in a predictable way (a derivative of Flake IDs), which enables Lucene to sometimes skip whole segments that cannot possible contain a given ID.

Do you leave plenty of free RAM for the OS to cache hot pages?  Specifically, the terms dict files need to remain hot so the version lookups don't hit disk.

In the past when I've tested this, the hit was much lower than 50% (more like 5%), on the logging use case.

I think if ES used segment files NRT replication (recently added to Lucene:  https://issues.apache.org/jira/browse/LUCENE-5438) instead of document replication, supporting append only indexing should be much easier since there is a "single source of truth" (the primary shard) ... and it would mean less CPU on replicas since they just copy files instead of indexing documents again ... but that would be a very large change ;)
</comment><comment author="otisg" created="2016-04-30T04:01:37Z" id="215932835">@mikemccand - this was ES 5.0 alpha1 (and 2).

&gt; Do your shards have a contained number of segments? Version lookup cost can be linear cost in the number of segments.

In order to reduce expensive merges we've allowed a larger number of segments.  Sounds like that's more expensive for version lookups, but if we change settings to reduce the number of segments then we'll pay the price in segment merges.

&gt; Are you using ES's IDs? 

Yes.
</comment><comment author="mikemccand" created="2016-04-30T10:56:44Z" id="215954951">&gt; In order to reduce expensive merges we've allowed a larger number of segments. Sounds like that's more expensive for version lookups, but if we change settings to reduce the number of segments then we'll pay the price in segment merges.

Hmm that's usually the wrong tradeoff, but, yes, if you relax merging (allow more segments in the index) then version lookups get slower.
</comment><comment author="prog8" created="2016-05-01T12:21:01Z" id="216038394">Yeah we used ES 5.0 alpha2 directly from master branch. It means we are really up to date with all bug fixes and improvements.

&gt; Hmm that's usually the wrong tradeoff, but, yes, if you relax merging (allow more segments in the index) then version lookups get slower.

It means we are in a position where we either lose indexing throughput by slower versions lookup (more segments) or by more merges (more aggressive merge policy).
We actually didn't measure this precisely, but even with more aggressive merge policy versions lookup is relatively expensive operation, when the number of documents in a single shard grows to tens of millions.
</comment><comment author="ebuildy" created="2016-05-03T10:01:08Z" id="216485005">I agree this is a very common case, we always use immutable index (for log purpose, and because we are using HDFS =&gt; Pig =&gt; ES) and looking for any fix that could improve indexing rate.

Please can you share what did you do for that? Many thanks.
</comment><comment author="prog8" created="2016-05-05T08:54:33Z" id="217106025">Hi @ebuildy. I used naive and ugly approach just to see if disabling versions helps and to be able to quickly show a few numbers. I think you don't have to follow this path :) What I actually did was simply commenting out all lines related to versions in `core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java` in method `innerIndex`.
</comment><comment author="ebuildy" created="2016-05-05T09:14:56Z" id="217109490">Good to know, thanks you.
Le 5 mai 2016 10:55, "Pawe&#322; R&#243;g" notifications@github.com a &#233;crit :

&gt; Hi @ebuildy https://github.com/ebuildy. I used naive and ugly approach
&gt; just to see if disabling versions helps and to be able to quickly show a
&gt; few numbers. I think you don't have to follow this path :) What I actually
&gt; did was simply commenting out all lines related to versions in
&gt; core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java in
&gt; method innerIndex.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18069#issuecomment-217106025
</comment><comment author="mikemccand" created="2016-05-05T10:48:18Z" id="217123051">@prog8 @otisg Thanks for all the answers.  How small are these documents?  The version lookup cost will be a proportionally higher percentage the smaller the documents.  In the extreme (you index just the `_uid`) it's very costly...

Does the OS have enough free RAM to keep the terms dict fully hot?  `top` will tell you how many bytes the OS is caching, and you should be able to see the total size of terms dictionary files on disk using the segments stats API, and `_uid` typically dominates the terms dict size.

ES used to have an optimization to skip the version lookup when it had auto-generated a new ID, however this proved to be dangerous, with error cases where a node to node retry within ES could result in cross shard corruption (replica and primary out of sync) because the replica indexed duplicate documents.  https://github.com/elastic/elasticsearch/pull/9125 has more details.  Maybe we need to revisit if there is a safe way to re-enable this ...

Before that, @s1monw had long ago pursued a branch to optimize for the append-only use case in ES, but that proved to be too complex a "fork" of ES's sources I think.

I still believe the "right" solution to this problem would be for ES to switch to Lucene's new NRT replication (https://issues.apache.org/jira/browse/LUCENE-5438).  Because it replicates at the segment file level, it is not possible for a replica to become out of sync versus the primary.  But that is an enormous change and has its own complex tradeoffs.

One thing ES should do (this was @rmuir's idea) is to use the full binary term space when indexing `_uid` ... today it sends the base64 encoded version (25% waste I think) to Lucene, a holdover from the past when Lucene could not index binary terms.  I'll open a separate issue for this ...
</comment><comment author="mikemccand" created="2016-05-05T10:55:18Z" id="217124478">I opened https://github.com/elastic/elasticsearch/issues/18154 to index `_uid` in binary form.
</comment><comment author="prog8" created="2016-05-05T12:12:21Z" id="217137968">@mikemccand thank you for this very extensive response. Our documents are really small. Single document contains a couple of text fields (3-5) which contain short keyword. Moreover there are also 4 pure doc values fields. _all and _source are disabled. This means "versions problem" is more visible in such a case.

I looked at the size of .tim files. Sumaric size of all .tim files is &gt;100GB while machine has only 8GB RAM (half of it used by JVM). This leaves not much space for OS caches. I am only thinking if the problem is in not fully hot term dictionaries shouldn't we see high CPU wait time and many IOPS? This is not the case. CPU wait time is close to 2% and CPU user time is close to 90%.

Thank you for information about previous tries of having append-only ES.
I agree that LUCENE-5438 can be really great. If I understand it correctly, it will reduce CPU usage by a factor of 2 in 1-replica environment (without losing any functionality). Can you reveal a secret about plans of using LUCENE-5438 in ES? I am impatiently looking forward for this change. It can be _huge_ improvement for most of ES setups :)

Once again, thanks for all information. This is really helpful.
</comment><comment author="clintongormley" created="2016-05-06T09:27:23Z" id="217395987">I think there's nothing more to do on this issue, so will close.
</comment><comment author="mikemccand" created="2016-05-06T13:16:35Z" id="217436664">&gt; Sumaric size of all .tim files is &gt;100GB while machine has only 8GB RAM (half of it used by JVM). This leaves not much space for OS caches

Wow, that's a very large terms dict.  But, yes, high CPU utilization means the OS is somehow keeping things hot (not sure how).

&gt; Can you reveal a secret about plans of using LUCENE-5438 in ES? I am impatiently looking forward for this change. It can be huge improvement for most of ES setups :)

Well would be downsides to it as well, e.g. higher NRT refresh latency, merged segments need to be moved on the wire too (we could maybe fix that, but that's also hairy) so it's more network traffic within the cluster.  But it would also give precise searcher versions across primary and all replicas, so you are searching the exact point-in-time view regardless of which replica you use.  I don't know of anyone exploring doing this for ES now ... it would be a massive change.
</comment><comment author="jasontedor" created="2016-05-06T13:20:31Z" id="217437419">&gt; I don't know of anyone exploring doing this for ES now ... it would be a massive change.

This is correct, it is not being explored at this time.
</comment><comment author="prog8" created="2016-05-06T13:25:33Z" id="217439071">@mikemccand, @jasontedor Thanks for information. I believe that when LUCENE-5438 is taken into consideration for ES I will find the github issue about this :)
</comment><comment author="bleskes" created="2016-12-22T08:02:51Z" id="268741581">BTW - this is also addressed in 5.0 with https://github.com/elastic/elasticsearch/pull/20211 and there are [more ideas on the table](https://github.com/elastic/elasticsearch/issues/19813) .</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Row-centric output for _cat/fielddata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18068</link><project id="" key="" /><description>I deleted field `total`, because I think it's bad idea to mix aggregate size with individual field sizes. You can get total field data usage per node by `_cat/nodes` API.

Closes #10249
</description><key id="151859640">18068</key><summary>Row-centric output for _cat/fielddata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>:CAT API</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-04-29T12:57:35Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-05-10T14:28:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-07T14:02:06Z" id="217638423">@dakrone could you review this please?
</comment><comment author="dakrone" created="2016-05-09T21:20:59Z" id="217993699">@alexshadow007 this looks good to me, can you drop a note into `docs/reference/migration/migrate_5_0.asciidoc` mentioning this breaking change? After that I will happily merge, thanks!
</comment><comment author="alexshadow007" created="2016-05-10T13:42:59Z" id="218160832">@dakrone Done
</comment><comment author="dakrone" created="2016-05-10T14:28:40Z" id="218174755">Merged, thanks @alexshadow007!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sliced lock contention 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18067</link><project id="" key="" /><description>This pull request is the backport of #18060 to 2.x. This backport also includes a refactoring of `KeyedLock` that was done in master in #16872.

Relates #18053
</description><key id="151856180">18067</key><summary>Sliced lock contention 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.4.0</label></labels><created>2016-04-29T12:36:41Z</created><updated>2016-06-27T11:06:26Z</updated><resolved>2016-04-29T15:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-29T12:37:06Z" id="215699764">Do you mind taking a look @jpountz and @nik9000?
</comment><comment author="nik9000" created="2016-04-29T12:41:06Z" id="215700542">Looks much better to me. I've demonstrated lately that I'm not an expert on threading stuff so I'll wait for @jpountz to sign off.
</comment><comment author="jpountz" created="2016-04-29T15:03:08Z" id="215746095">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable HTTP compression by default with compression level 3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18066</link><project id="" key="" /><description>With this commit we compress HTTP responses provided the client
supports it (as indicated by the HTTP header 'Accept-Encoding').

We're also able to process compressed HTTP requests if needed.

The default compression level is lowered from 6 to 3 as benchmarks
have indicated that this reduces query latency with a negligible
increase in network traffic.

Closes #7309
</description><key id="151853993">18066</key><summary>Enable HTTP compression by default with compression level 3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:REST</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-29T12:23:34Z</created><updated>2016-05-03T06:57:43Z</updated><resolved>2016-05-03T06:57:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-02T07:05:30Z" id="216119830">@danielmitterdorfer maybe we should add a small note on the breaking changes about the fact that compressed request will not check the http.compression setting anymore. I think it's important because this means that any client could send a compressed request without checking if the compression is disabled or not on the cluster.
</comment><comment author="jimczi" created="2016-05-02T07:07:11Z" id="216120082">I've left a tiny comment but the change looks great.
LGTM
</comment><comment author="danielmitterdorfer" created="2016-05-03T05:39:53Z" id="216443147">@jimferenczi I have documented the change now in the breaking changes docs. As I already got a LGTM, I'll merge the change soon. Thanks a lot for your review.
</comment><comment author="danielmitterdorfer" created="2016-05-03T06:57:43Z" id="216453841">Closed by 0a6f40c7f5f3af4e04a8875216e059025e1bbdf3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backwards compatibility tests for 5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18065</link><project id="" key="" /><description>**Describe the feature**:
The 2.x branch has a bunch of backwards compatibility tests and we need to examine each one and decide if we need to port it to the master branch (and then 5.x branch, if we've cut such a branch before doing this issue.)

Many of them just want to run features against a mixed version cluster. We should get good enough coverage for those by running all the REST tests. Right now we run the core REST tests. I think all we have to do is run all the plugin REST tests for those tests.

Others assert that things work **during** a rolling restart. Those are going to be harder to port.
</description><key id="151853297">18065</key><summary>Backwards compatibility tests for 5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>blocker</label><label>test</label></labels><created>2016-04-29T12:19:44Z</created><updated>2016-11-18T09:22:56Z</updated><resolved>2016-11-18T09:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T16:16:36Z" id="215787773">Closing in favour of #14406
</comment><comment author="s1monw" created="2016-11-18T08:40:19Z" id="261477629">do we wanna close this @clintongormley @nik9000 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support analyzer for keyword type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18064</link><project id="" key="" /><description>Sometimes you want to analyze text to make it consistent when running aggregations on top of it.

For example, let's say I have a `city` field mapped as a `keyword`.

This field can contain `San Francisco`, `SAN FRANCISCO`, `San francisco`...

If I build a terms aggregation on top of it, I will end up with

```
San Francisco: 1
SAN FRANCISCO: 1
San francisco: 1
```

I'd like to be able to analyze this text before it gets indexed. Of course I could use a `text` field instead and set `fielddata: true` but that would not create doc values for this field.

I can imagine that we allow an analyzer at index time for this field.

We can restrict its usage if we wish and only allows analyzers which are using tokenizers like `lowercase`, `keyword`, `path` but I would let the user decide.

If we allow setting `analyzer: simple` for example, my aggregation will become:

```
san francisco: 3
```

Same applies for path tokenizer.

Let say I'm building a dir tree like:

```
/tmp/dir1/file1.txt
/tmp/dir1/file2.txt
/tmp/dir2/file3.txt
/tmp/dir2/file4.txt
```

Applying a path tokenizer would help me to generate an aggregation like:

```
/tmp/dir1: 2
/tmp/dir2: 2
/tmp: 4
```
</description><key id="151848594">18064</key><summary>Support analyzer for keyword type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Mapping</label><label>discuss</label><label>enhancement</label></labels><created>2016-04-29T11:48:41Z</created><updated>2017-03-23T15:53:20Z</updated><resolved>2016-12-30T08:36:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-13T08:56:09Z" id="232297258">Most of the work needed to implement this feature has been merged into Lucene and will be available in 6.2. Analyzer got a new method called `normalize` that only applies the subset of the analysis chain that is about normalization (and not eg. stemming) https://issues.apache.org/jira/browse/LUCENE-7355.

Note that it would NOT work for the path tokenization use-case mentioned above since it has a restriction that it can generate a single token, so such use-cases would have to be handled differently, eg. using an ingest processor.

I am wondering if we should use a different property name than `analyzer` since the analyzer will not be used for tokenizing. I am currently thinking about:

``` json
"my_field": {
  "type": "keyword",
  "normalizer": "standard"
}
```

This would avoid potential confusion about what happens with analyzers that would generate multiple tokens and make clearer that only normalization would be applied?
</comment><comment author="dadoonet" created="2016-07-13T08:59:25Z" id="232297988">&gt; Note that it would NOT work for the path tokenization use-case mentioned above since it has a restriction that it can generate a single token, so such use-cases would have to be handled differently, eg. using an ingest processor.

That would complicate the process but I guess we have to live with that. At least, we have a workaround.

&gt; I am wondering if we should use a different property name than `analyzer` since the analyzer will not be used for tokenizing.

Totally agree. 
</comment><comment author="synhershko" created="2016-11-02T03:41:27Z" id="257764151">Instead of calling it a "normalizer", I'd call it by it's name `token_filters` and accept an array of token filters. I don't think analyzers should be used it here as they propose a use of a tokenizer.
</comment><comment author="jpountz" created="2016-11-02T08:59:27Z" id="257807518">I think I agree with that. I initially thought that maybe integration with https://issues.apache.org/jira/browse/LUCENE-7355 would make sense, but maybe we should just apply a list of token filters manually, this would probably be simpler.
</comment><comment author="synhershko" created="2016-11-02T14:48:35Z" id="257886965">Yeah, I think it's a much simpler approach than involving a queryparser here. No need for one IMO. Also please note that order matters in the `token_filters` array.
</comment><comment author="clintongormley" created="2016-11-04T14:12:45Z" id="258441773">What about character filters?  They can also be useful here.  My initial thought was to keep it as `analyzers` and to only allow analyzers which use the `keyword` tokenizer.  But `normalizers` would work too...
</comment><comment author="ugolas" created="2016-11-09T15:58:11Z" id="259449484">hi guys, great to see you have an enhancement for this requirement!

Any idea how can I support case insensitive search on a "keyword" type field (which I also use for aggregations) for v5.0?

In ES 2.3 I used:
"analyzer_keyword": {
      "tokenizer": "keyword",
      "filter": "lowercase"
}

But that does not seem to work without enabling fielddata in ES 5.

Any workaround I can use for now?
</comment><comment author="dadoonet" created="2016-11-09T19:18:15Z" id="259500547">You can use ingest to lower case your field.
</comment><comment author="sumithub" created="2016-12-01T00:53:29Z" id="264045833">Hi Guys, 
Since lucene added custom analyzer normalization with 6.2 release. https://issues.apache.org/jira/browse/LUCENE-7355
Just wondering whether this feature would be available soon in elasticsearch?
Our application makes heavy use of aggregations with lowercase filters to be used as doc-values.</comment><comment author="wgerlach" created="2017-03-23T15:50:15Z" id="288763726">I understand from this thread that the ability has been added to sort case insensitive. But how? Is there documentation or an example available ?</comment><comment author="fabiocatalao" created="2017-03-23T15:53:20Z" id="288764933">@wgerlach : I've added an example for lowercase/asciifolding normalizer on elastic forum: https://discuss.elastic.co/t/wildcard-case-insensitive-query-string/75050/5</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Securemock which is included in Elasticsearch 5.0.0-alpha1 test-framework contains classes from org.objenesis and thus causes jar-hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18063</link><project id="" key="" /><description>**Elasticsearch version**:5.0.0-alpha1

**JVM version**:1.8.0_66

**OS version**:Windows 8.1

**Description of the problem including expected versus actual behavior**: 

I use the Elasticsearch Test-Framework for testing some of my code which uses the Elasticsearch Java Client. It works fine with 2.3.2, when I try this with 5.0.0-alpha1, I get jar-hell errors as follows:

```
java.lang.RuntimeException: found jar hell in test classpath
    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:89)
    at org.elasticsearch.test.ESTestCase.&lt;clinit&gt;(ESTestCase.java:117)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:585)
Caused by: java.lang.IllegalStateException: jar hell!
class: org.objenesis.instantiator.android.Android10Instantiator
jar1: C:\Users\.gradle\caches\modules-2\files-2.1\org.objenesis\objenesis\2.1\87c0ea803b69252868d09308b4618f766f135a96\objenesis-2.1.jar
jar2: C:\Users\.gradle\caches\modules-2\files-2.1\org.elasticsearch\securemock\1.2\98201d4ad5ac93f6b415ae9172d52b5e7cda490e\securemock-1.2.jar
    at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:282)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:186)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:87)
    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:87)
    ... 4 more
```

It seems the new securemock package of Elasticsearch contains classes from org.objenesis and thus leads to jarhell failures, e.g. when using easymock, which pulls in the official org.objenesis package.

**Steps to reproduce**:
1. Create a project using ESIntegrationTest
2. Use both easymock and the following Gradle dependencies
   
   testCompile 'org.elasticsearch:elasticsearch:5.0.0-alpha1'
   testCompile 'org.elasticsearch.test:framework:5.0.0-alpha1'
   testCompile 'org.easymock:easymock:3.3.1'
3. Try to run the unit-test
</description><key id="151833912">18063</key><summary>Securemock which is included in Elasticsearch 5.0.0-alpha1 test-framework contains classes from org.objenesis and thus causes jar-hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">centic9</reporter><labels /><created>2016-04-29T10:09:57Z</created><updated>2016-04-29T15:21:08Z</updated><resolved>2016-04-29T15:21:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-29T15:21:08Z" id="215752928">&gt; Use both easymock and the following Gradle dependencies

This combination is not supported, since mockito is itself of easymock, I believe, and using the ES test framework requires using securemock instead of mockito. Securemock is a simple copy of mockito with important calls to the jdk wrapped with doPrivileged blocks that work correctly with security manager. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>String with index:false results in keyword with index:true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18062</link><project id="" key="" /><description>Putting this mapping:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "string",
          "index": false
        }
      }
    }
  }
}
```

Results in:

```
{
  "t": {
    "mappings": {
      "t": {
        "properties": {
          "foo": {
            "type": "keyword"
          }
        }
      }
    }
  }
}
```
</description><key id="151821165">18062</key><summary>String with index:false results in keyword with index:true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-04-29T08:53:43Z</created><updated>2016-05-02T15:29:51Z</updated><resolved>2016-05-02T15:29:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-29T08:56:14Z" id="215662599">I suspect the parser gets confused by the fact that it mixes new/old syntax (`index:false` instead of `index:no` on a legacy `string` field).
</comment><comment author="jpountz" created="2016-04-29T08:59:23Z" id="215663139">What is the expected behaviour? I think it should throw an error?
</comment><comment author="clintongormley" created="2016-04-29T09:02:32Z" id="215663700">I'm ok with throwing the error, as at least it doesn't fail silently.  The alternative would probably be more complex
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BWC tests for Logstash/Beats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18061</link><project id="" key="" /><description>Users need to be able to upgrade Elasticsearch to the next major version before they upgrade Logstash/Beats.  In other words, we need to provide bwc for:
- the `bulk` API
- index templates
- the index APIs used by these projects

We should add tests to confirm that this upgrade process continues to work.
</description><key id="151812900">18061</key><summary>BWC tests for Logstash/Beats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Bulk</label><label>:Index APIs</label><label>:Index Templates</label><label>adoptme</label><label>test</label><label>v5.0.0</label></labels><created>2016-04-29T07:56:50Z</created><updated>2016-10-18T07:49:29Z</updated><resolved>2016-10-18T07:49:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T14:24:12Z" id="216544757">Related to https://github.com/elastic/elasticsearch/issues/17275
</comment><comment author="s1monw" created="2016-10-07T14:15:12Z" id="252263610">with https://github.com/elastic/elasticsearch/pull/20491 committed I think we can remove the blocker label here or maybe even close?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid sliced locked contention in internal engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18060</link><project id="" key="" /><description>Today we use a sliced lock strategy for acquiring locks to prevent
concurrent updates to the same document. The number of sliced locks is
computed as a linear function of the number of logical
processors. Unfortunately, the probability of a collision against a
sliced lock is prone to the birthday problem and grows faster than
expected. In fact, the mathematics works out such that for a fixed
target probability of collision, the number of lock slices should grow
like the square of the number of logical processors. This is
less-than-ideal, and we can do better anyway. This commit introduces a
strategy for avoiding lock contention within the internal
engine. Ideally, we would only have lock contention if there were
concurrent updates to the same document. We can get close to this ideal
world by associating a lock with the ID of each document. This
association can be held in a concurrent hash map. Now, the JDK
ConcurrentHashMap also uses a sliced lock internally, but it has several
strategies for avoiding taking the locks and these locks are only held
for a very short period of time. This implementation associates a
reference count with the lock that is associated with a document ID and
automatically removes the document ID from the concurrent hash map when
the reference count reaches zero.

Closes #18053
</description><key id="151807909">18060</key><summary>Avoid sliced locked contention in internal engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-29T07:22:28Z</created><updated>2016-04-30T21:38:00Z</updated><resolved>2016-04-29T12:05:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-29T07:41:41Z" id="215650338">Running the same workload as @polyfractal in #18053, the strategy employed in this pull request showed a consistent 15--20% gain in performance (on a single machine) and almost completely eliminates lock contention in the internal engine (the extremely tiny bit that remains is from the concurrent hash map). Interestingly, it shifts some contention to the translog writer, but nowhere near enough to offset the gains from reducing the contention here. It's important to note in the comparison images from Java Flight Recorder below how long threads were blocked on the sliced lock in internal engine (compared to how long they are blocked on the translog writer).

master @ 0cf1d16187fcf10e9d0e64402bb6d0ebba5d7b80:

![screen shot 2016-04-29 at 3 26 48 am](https://cloud.githubusercontent.com/assets/4744941/14910505/fc2b2994-0dbb-11e6-880e-4cdb3ef2f76d.png)

sliced-lock-contention @ a78adcb08dab3e3ef1ddcb7209fe46323920a9dd:

![screen shot 2016-04-29 at 3 39 20 am](https://cloud.githubusercontent.com/assets/4744941/14910508/014f0ad0-0dbc-11e6-9e1d-7d71aef9efe0.png)
</comment><comment author="nik9000" created="2016-04-29T11:07:30Z" id="215685367">Nice!
</comment><comment author="jpountz" created="2016-04-29T11:17:31Z" id="215686872">LGTM
</comment><comment author="nik9000" created="2016-04-29T12:06:02Z" id="215694353">Awesome!
</comment><comment author="jpountz" created="2016-04-29T12:17:34Z" id="215696288">I am curious whether this change will be noticeable on https://benchmarks.elastic.co.
</comment><comment author="nik9000" created="2016-04-29T12:22:57Z" id="215697289">&gt; I am curious whether this change will be noticeable on https://benchmarks.elastic.co.

Me too! It might not be as big as 20% because @polyfractal's workload is pretty unique iirc. Really small documents and _source disabled I think? Looks like sensor data?
</comment><comment author="polyfractal" created="2016-04-29T13:39:27Z" id="215714781">Awesome, thanks @jasontedor, this is great! :)

I'm curious about the nightly benchmarks too.  @nik9000 is correct, this is small doc "metric/sensor" style data.  Four small cardinality integers, timestamp and one float metric (pulled from a gaussian unique to each "set" of data), `_source` and `_all` disabled.
</comment><comment author="jasontedor" created="2016-04-30T16:12:37Z" id="215975894">Some of the nightly [benchmarks](https://benchmarks.elastic.co/index.html) showed a healthy bump: 

&lt;img width="525" alt="screen shot 2016-04-30 at 12 12 06 pm" src="https://cloud.githubusercontent.com/assets/4744941/14937161/c9bea7da-0ecc-11e6-8fd3-02c0c22645eb.png"&gt;
</comment><comment author="jasontedor" created="2016-04-30T16:20:32Z" id="215976535">&gt; Awesome, thanks @jasontedor, this is great! :)

@polyfractal Thank you for discovering and raising the issue in #18053 and verifying the results on your setup!
</comment><comment author="nik9000" created="2016-04-30T17:38:22Z" id="215982843">Awesome!
On Apr 30, 2016 12:20 PM, "Jason Tedor" notifications@github.com wrote:

&gt; Awesome, thanks @jasontedor https://github.com/jasontedor, this is
&gt; great! :)
&gt; 
&gt; @polyfractal https://github.com/polyfractal Thank you for discovering
&gt; and raising the issue in #18053
&gt; https://github.com/elastic/elasticsearch/issues/18053 and verifying the
&gt; results on your setup!
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/18060#issuecomment-215976535
</comment><comment author="drewr" created="2016-04-30T19:40:59Z" id="215989484">It will be interesting to see if this addresses the bulk thread contention wave I've seen with hundreds of clients over dozens of nodes.  Sounds birthday-ish in itself.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex API dedicated pool?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18059</link><project id="" key="" /><description>I'm using the reindex api to create a daily new index, that's optimised to only contain certain docs etc.
There are crons running every hour that could increase the amount of searches and if the reindex is running in the meanwhile it would abort whenever the search thread pool capacity is reached : 

```
    curl -XPOST "http://elastic26:9200/_reindex?wait_for_completion=true" -d '
             {
                "conflicts": "proceed",
                "source" : {
                    "index": "prod_users_v5",
                    "type": "person",
                    "query" : { "constant_score" : { "query" : { "bool" : { "filter" : [ { "term" : { "accountstatus" : "active" } }, { "range" : { "lastlogindate" : { "gte" : "2015-04-29 02:00", "lte" : "2016-04-29 02:00" } } }, { "range" : { "birthdate" : { "gte" : "1989-04-29", "lte" : "1993-04-29" } } } ] } } } }
                },
                "dest": {
                    "index": "prod_users_v1611902_last_year",
                    "op_type" : "create"
                }
            }' 2&gt;/dev/null
```

```
{"took":984897,"timed_out":false,"total":2451467,"updated":0,"created":730574,"batches":7315,"version_conflicts":926,"noops":0,"retries":0,"failures":[{"shard":-1,"index":null,"reason":{"type":"es_rejected_execution_exception","reason":"rejected execution of org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler@4f66670b on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@5eab79c7[Running, pool size = 49, active threads = 49, queued tasks = 1000, completed tasks = 1351110446]]
```

resulting in a "semi completed" state at the end, 
I know it's not good to have a full search queue.. but I rather have a few searches failing then a reindex process that aborts, 
can't there be dedicated queue for reindex? 
</description><key id="151804761">18059</key><summary>Reindex API dedicated pool?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jrots</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-29T06:58:05Z</created><updated>2016-05-18T10:09:37Z</updated><resolved>2016-05-17T17:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-29T10:50:34Z" id="215682964">&gt; can't there be dedicated queue for reindex?

It is possible but I think that'd be difficult. Reindex already has logic to backoff and retry when the bulk queue is full. Applying that logic to the search and scroll portions would be much simpler by comparison and ought to work for this. Would you mind if I repurposed this issue to track that?
</comment><comment author="jrots" created="2016-04-29T13:22:31Z" id="215709451">Sure, retry logic for searches sounds good too 
Think also having the ability to bump the search/ scroll limit to f.e. 15000 instead of default 100 would help to avoid having these issues.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix issue with tombstones matching active indices in cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18058</link><project id="" key="" /><description>When checking if an index tombstone can be applied, use both the index name and uuid because the cluster state may contain an active index of the same name (but different uuid).

Closes #18054
</description><key id="151788693">18058</key><summary>Fix issue with tombstones matching active indices in cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-29T03:58:17Z</created><updated>2016-04-29T15:56:25Z</updated><resolved>2016-04-29T15:56:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-29T04:01:17Z" id="215621275">@jasontedor Perhaps you'd like to review?  The test I wrote passes even before making the change in `IndicesService`.  The reason being, the `IllegalStateException` that is thrown is not propagated anywhere that is visible in the IT test.  Is there a general mechanism to retrieve such exceptions in the IT tests?  If not, I'll probably have to create a different kind of test. 
</comment><comment author="jasontedor" created="2016-04-29T05:53:57Z" id="215632471">LGTM. Thanks @abeyad.
</comment><comment author="abeyad" created="2016-04-29T15:37:59Z" id="215763276">@jasontedor Changed the test to go in `IndicesServiceTests` at the unit tests level.  
</comment><comment author="jasontedor" created="2016-04-29T15:48:18Z" id="215770058">&gt; Changed the test to go in IndicesServiceTests at the unit tests level.

Much better. &#128516; LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Who can tell me  java api of delete-by-query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18057</link><project id="" key="" /><description>delete-by-query is a plugin is ES 2.3.1 . But I can not find java api of delete-by-query plugin. 
</description><key id="151788526">18057</key><summary>Who can tell me  java api of delete-by-query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andykai</reporter><labels /><created>2016-04-29T03:55:49Z</created><updated>2016-04-29T03:57:41Z</updated><resolved>2016-04-29T03:57:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-29T03:57:40Z" id="215620967">Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to list available (supported) plugins from the plugin command line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18056</link><project id="" key="" /><description>Provide the ability to list (at least supported) plugins from the elasticsearch-plugin command line.

This would be an easy way to show a list or filtered list of available plugins then install the ones you want without ever having to go to the documentation page to see what's available.

Might get hard to maintain possibly, but start with the officially supported plugins should be fine.

A further feature could be also listing all available versions of a plugin
</description><key id="151788029">18056</key><summary>Ability to list available (supported) plugins from the plugin command line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels /><created>2016-04-29T03:48:21Z</created><updated>2016-04-29T04:52:52Z</updated><resolved>2016-04-29T04:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-29T04:44:45Z" id="215625312">`elasticsearch-plugin` shows the official plugins:

```
$ bin/elasticsearch-plugin install --help
Install a plugin

The following official plugins may be installed by name:
  analysis-icu
  analysis-kuromoji
  analysis-phonetic
  analysis-smartcn
  analysis-stempel
  delete-by-query
  discovery-azure
  discovery-ec2
  discovery-gce
  ingest-attachment
  ingest-geoip
  lang-javascript
  lang-python
  mapper-attachments
  mapper-murmur3
  mapper-size
  repository-azure
  repository-hdfs
  repository-s3
  store-smb
  x-pack

Non-option arguments:
plugin id

Option         Description
------         -----------
-b, --batch    Enable batch mode explicitly,
                 automatic confirmation of security
                 permission
-h, --help     show help
-s, --silent   show minimal output
-v, --verbose  show verbose output
```
</comment><comment author="jasontedor" created="2016-04-29T04:49:22Z" id="215625967">&gt; Might get hard to maintain possibly, but start with the officially supported plugins should be fine.

For exactly this reason I do no think that we will ever add non-official plugins here.

&gt; A further feature could be also listing all available versions of a plugin

The official plugins are always version-locked to the version of Elasticsearch.
</comment><comment author="geekpete" created="2016-04-29T04:52:51Z" id="215626635">Perfect!

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in include-in-all.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18055</link><project id="" key="" /><description>Fix small typo in the documentation
</description><key id="151787552">18055</key><summary>Fix typo in include-in-all.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robinjoseph08</reporter><labels><label>docs</label></labels><created>2016-04-29T03:42:10Z</created><updated>2016-04-29T16:03:06Z</updated><resolved>2016-04-29T16:02:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T16:03:05Z" id="215779641">thanks @robinjoseph08 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot delete index, it is still part of the cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18054</link><project id="" key="" /><description>I've seen this a few times in the last few days. This is a fresh build off of master (72fb93e61220550d36eb5b63227cbb86df8e4a72) and a fresh data directory. The node was started, had some data indexed into it, deleted those indices, and shutdown the node, and then started the node again and saw this stacktrace on startup:

```
[2016-04-28 19:13:59,265][WARN ][cluster.service          ] [Ape-Man] failed to notify ClusterStateListener                                                   
java.lang.IllegalStateException: Cannot delete index [[data/czEFA9CDRWSpUdU0GPzSIw]], it is still part of the cluster state.
        at org.elasticsearch.indices.IndicesService.verifyIndexIsDeleted(IndicesService.java:686)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedIndices(IndicesClusterStateService.java:250)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:183)
        at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:652)
        at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

Another observation is that there is _not_ a folder on disk with that UUID.
</description><key id="151765125">18054</key><summary>Cannot delete index, it is still part of the cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>bug</label></labels><created>2016-04-28T23:24:48Z</created><updated>2016-04-29T15:56:25Z</updated><resolved>2016-04-29T15:56:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-28T23:28:18Z" id="215591953">There is a tombstone for this index in the cluster state:

```
  {
        "index" : {
          "index_name" : "data",
          "index_uuid" : "czEFA9CDRWSpUdU0GPzSIw"
        },
        "delete_date_in_millis": 1461885168863
  }
```
</comment><comment author="jasontedor" created="2016-04-28T23:35:55Z" id="215593138">Here's one more observation: creating an index with the same name as the deleted index appears to be related to the message here. In particular, if I delete the second index (with the same name) then that message does not appear. But if I create the index again (so a third time), the message does.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>High contention on InternalIndex.innerIndex()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18053</link><project id="" key="" /><description>I was running some informal benchmarking on my home cluster for fun and found my max throughput.  Interestingly, the cluster didn't appear saturated on any particular resource and I was unable to coax the indexing rate higher no matter what I did.

I reran the test while recording telemetry via Flight Recorder (using `XX:+DebugNonSafepoints` as recommended by @danielmitterdorfer and @rmuir).  I didn't notice anything too outrageous, but there was very high contention on `InternalEngine.innerIndex()`:

![default_lock_count](https://cloud.githubusercontent.com/assets/1224228/14901215/4e2fa1a2-0d62-11e6-8698-eef84e99da27.png)

On a 6min test, it racked up **3242 contention events** and **~24min** of CPU time.  The [`innerIndex()` method synchronizes](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java#L359) on a pool of locks, which are [set to be numProcessors \* 10](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java#L131)

My machine has 32 cores (16 physical + hyperthreading), so the pool has 320 locks.  @nik9000 and @jpountz suggested it may be an issue with collisions in that pool.  I wasn't really sure of the math to verify, but I found [an article which gives a formula](http://preshing.com/20110504/hash-collision-probabilities/) to calculate the probability of generating `k` unique draws out of `N` possible values.

Since our case is a bit more complex -- pool count (`N`) scales in relation to core count (`k`) -- so I put together a spreadsheet to calculate everything&lt;sup&gt;1&lt;/sup&gt;.  Assuming I didn't do anything silly with the math, the collision rate is grim.  For my machine, 32 simultaneous requests have a ~79% chance of at least one collision in a pool of 320 locks.

![image](https://cloud.githubusercontent.com/assets/1224228/14901467/8bc05ca4-0d63-11e6-8d79-119b12817315.png)
_y-axis: probability of collision_
_x-axis: number of simultaneous requests on an equal number of cores_
### Bumping the pool count

As a quick test, I bumped the lock multiplier from `* 10` to `* 1000`, meaning my machines had 32000 available locks, which gives a theoretical ~1.5% collision rate.  Rerunning the test consistently gives a **20k docs/s speedup**, and drops contention down to **297 events** and **~3min** CPU time.

![high_lock_count](https://cloud.githubusercontent.com/assets/1224228/14901563/fd30f11e-0d63-11e6-8625-5b52bbeda89d.png)
### Next?

All of this is touching delicate code that I don't understand, and my informal test may be biased by something I'm not accounting for.  So I'm not really sure what or if there is a next step.  But I thought these charts were interesting enough that more knowledgeable folks may want to have a look.
#### Footnotes
1. [Spreadsheet calculating collision probability](https://github.com/elastic/elasticsearch/files/241442/collision.xlsx)
2. To verify my math, I pinned the pool count to 320 and calculated the same metric against 1-64 requests. [It shows a sigmoid like the original article](https://cloud.githubusercontent.com/assets/1224228/14901459/751cd9c8-0d63-11e6-8e7c-9e0f2a52b37d.png), so I think my math is correct.
</description><key id="151747455">18053</key><summary>High contention on InternalIndex.innerIndex()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Core</label></labels><created>2016-04-28T21:27:09Z</created><updated>2016-04-29T12:05:23Z</updated><resolved>2016-04-29T12:05:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-28T21:40:28Z" id="215571328">&gt; so I think my math is correct

The math is correct. It's just the [birthday problem](https://en.wikipedia.org/wiki/Birthday_problem) which is exactly the formula that I see in your [spreadsheet](https://github.com/elastic/elasticsearch/files/241442/collision.xlsx).
</comment><comment author="jasontedor" created="2016-04-28T22:19:49Z" id="215579897">An observation here is that the number of locks should scale like the square of the number of cores for a targeted probability of collision but we are only scaling linearly. That is, if we want to target a probability `p` for `2 * c` threads running on `c` cores (indexing and bulk thread pools are limited to the number of cores), we would want approximately `-2 * c^2 / log (1 - p)` locks in the lock pool.

I'm _not_ saying that that is the approach that we should take, just saying that this is why this is breaks down so hard as core counts increase.
</comment><comment author="mikemccand" created="2016-04-28T22:30:17Z" id="215582384">An increase of 20 K docs/sec sounds substantial!  What was the indexing rate before you increased the lock count?

It's frustrating that ES even needs these locks ... it's because if ES sends the same `uid` to Lucene concurrently, i.e. with different documents, there's no way for ES to know which document "won".

We have talked about having Lucene return a sequence number so that callers could figure it out ... and @s1monw may have a patch somewhere that started on this.  I wonder if that would be enough for ES to remove the locks?  E.g. we could use this sequence number to correctly update the version map ...
</comment><comment author="polyfractal" created="2016-04-28T22:39:04Z" id="215583911">Total throughput went from ~270,000 to ~290,000 docs/s.  

Note that was in a 4-node cluster, unsure if similar/less/better gains would be had from a single-node.  Although I did check and the contention issues seem to exist on a single node too (which makes sense, based on what @jasontedor said about linear lock pool scaling)  

There was some variance in between runs, but the difference seemed to hold up.  I have half a mind to rerun the tests a few dozen times and do a t-test just to make sure :)
</comment><comment author="otisg" created="2016-04-29T03:18:49Z" id="215617711">Nice find!  For what it's worth, I was running a profiler on ES today and saw this same "innerIndex" stuff as one of the top hotspots (ES 5.0 alpha2, built from source).  This was on a small 2-datanode ES cluster indexing ~20K docs/sec.
</comment><comment author="jpountz" created="2016-04-29T07:16:25Z" id="215644536">Maybe a short term fix for this would be to use something like [KeyedLock](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java). This would still prevent two index/delete operations on the same id from occurring concurrently, but two different ids would never share the same lock.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update Jackson 2.6.2 -&gt; 2.6.6 (latest and final 2.6 patch)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18052</link><project id="" key="" /><description>There are a few important fixes to Jackson between 2.6.2 and 2.6.6; latter is also likely to be the last update for 2.6 branch. Submitting this PR against 2.3 since I was not 100% what is the right way (instructions suggest master but...). In any case, whether via this PR or not, change is one-liner, easy to make wherever.

I also ran `mvn clean test` and did not observe any test failures, so I do not see obvious regressions.
</description><key id="151738110">18052</key><summary>Update Jackson 2.6.2 -&gt; 2.6.6 (latest and final 2.6 patch)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cowtowncoder</reporter><labels><label>:Internal</label><label>upgrade</label><label>v2.3.3</label><label>v2.4.0</label></labels><created>2016-04-28T20:40:36Z</created><updated>2016-08-10T22:27:00Z</updated><resolved>2016-04-28T21:49:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cowtowncoder" created="2016-04-28T20:41:01Z" id="215555844">Odd. I did sign the CLA online just few minutes ago.
</comment><comment author="dakrone" created="2016-04-28T20:53:42Z" id="215559237">@cowtowncoder I can see the CLA signed (we're having a little trouble with the auto-checking of it), I'll check this out, thanks!
</comment><comment author="cowtowncoder" created="2016-04-28T21:05:19Z" id="215562208">@dakrone Cool. I will probably introduce suggested update for master later on, once 2.7.4 is released, but thought 2.6.6 upgrade should be simple and safe at first.

Also, should there be any issues just let me know; while unlikely, there's always possibility of some minor detail differing.
</comment><comment author="karmi" created="2016-04-28T21:20:04Z" id="215565880">Hi @cowtowncoder, we have indeed your signature in our records, but you have signed with a different e-mail than the one used in yout Github commit. The easiest solution is probably to add both of these to your Github account (can be hidden), and comment here, in this way we can match your e-mails to your Github profile.
</comment><comment author="cowtowncoder" created="2016-04-28T21:32:20Z" id="215569317">@karmi Ok. I didn't realize email was different, so that's a good find in itself. I have 2 main email addresses so I'll add the other one.
</comment><comment author="cowtowncoder" created="2016-04-28T21:36:28Z" id="215570376">Actually, I can't see the difference. My email address for @cowtowncoder is the same as what CLA document has, looking at both: `tsaloranta@gmail.com`. What email address do you see?

But could it perhaps be that my local git email settings differ? I'll check that next.
</comment><comment author="cowtowncoder" created="2016-04-28T21:39:16Z" id="215571042">Ah, yes. It's not github or CLA, it's `git config` for `user.email`. Hmmh.

I am not sure that email address should actually be used to verify anything as it may depend on host used, settings, so CLAs most likely should verify with github account. It does not do that now so maybe it is a flaw in business logic of the verification system?

I'll see if I can add `tatu.saloranta@iki.fi` as well, to straighten this out. Should be fine as it gets routed to one of the 2 real accounts.
</comment><comment author="cowtowncoder" created="2016-04-28T21:43:13Z" id="215571902">Should be settled now, with `tatu.saloranta@iki.fi` also included.
</comment><comment author="karmi" created="2016-04-28T21:44:44Z" id="215572268">@cowtowncoder, thanks for looking into that! Yes, indeed it's about the Git config, you can check it at Github by appending `.patch` to the PR URL. Our systems need to check Github API that the address used is _actually_ used as a correct e-mail at Github. And as it should, as soon as you add that other e-mail, the status is green :) Thanks again!
</comment><comment author="dakrone" created="2016-04-28T21:50:15Z" id="215573459">Merged, this, I'll forward port to 2.x, thanks @cowtowncoder!
</comment><comment author="cowtowncoder" created="2016-04-28T21:54:27Z" id="215574345">Actually it does make sense in the end. So commit has email, that's checked against account etc.
Glad it all clears out now!
</comment><comment author="cowtowncoder" created="2016-08-10T22:27:00Z" id="239024091">Fwtw, there was one more full release of 2.6, 2.6.7. It had only 5 fixes as per:

https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.6.7

so upgrade is optional, but wanted to mention this for sake of completeness.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add plugin information for Verbose mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18051</link><project id="" key="" /><description>Fix for #16375 

With this Pull Request, we will show extra information about plugins running the `bin/elasticsearch-plugin` command under `verbose` mode.
</description><key id="151729169">18051</key><summary>Add plugin information for Verbose mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-28T19:56:52Z</created><updated>2016-05-11T09:53:03Z</updated><resolved>2016-05-10T15:23:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmoskovicz" created="2016-04-28T19:57:44Z" id="215544049">@jasontedor @nik9000 Can you please verify this PR? 
</comment><comment author="jasontedor" created="2016-04-28T20:01:02Z" id="215544895">Sure, I would be happy to give it a review. 
</comment><comment author="gmoskovicz" created="2016-04-29T17:28:08Z" id="215822981">@rjernst @jasontedor 

Fixed code review comments. Please note that now, if the `.description` file is missing OR it is not supported (wrong JVM/ES version), we will raise an exception when running the `bin/elasticsearch-plugin list` command, no matter if using `-v` or not. 
</comment><comment author="gmoskovicz" created="2016-04-29T18:16:10Z" id="215836112">The new format looks like

```
&#10140;  elasticsearch-5.0.0-alpha2-SNAPSHOT bin/elasticsearch-plugin list -v
Plugins directory: /Users/Gabriel/Documents/ElasticSearch/elasticsearch-5.0.0-alpha2-SNAPSHOT/plugins
license
- Plugin information:
Name: license
Description: Internal Elasticsearch Licensing Plugin
Version: 2.2.0
 * Classname: org.elasticsearch.license.plugin.LicensePlugin
marvel-agent
- Plugin information:
Name: marvel-agent
Description: Elasticsearch Marvel Agent
Version: 2.2.0
 * Classname: org.elasticsearch.marvel.MarvelPlugin
```
</comment><comment author="gmoskovicz" created="2016-05-02T16:26:29Z" id="216284102">@rjernst @jasontedor does it looks better now? Ready to merge?
</comment><comment author="jasontedor" created="2016-05-02T16:37:52Z" id="216286948">&gt; does it looks better now? Ready to merge?

I left more feedback.
</comment><comment author="gmoskovicz" created="2016-05-02T18:16:56Z" id="216316408">Added more fixes @jasontedor 
</comment><comment author="jasontedor" created="2016-05-06T14:26:06Z" id="217454214">&gt; Added more fixes @jasontedor

Left some comments.
</comment><comment author="gmoskovicz" created="2016-05-06T14:42:01Z" id="217458790">Thanks @jasontedor . Fixed those as well. 

Ready to :shipit:  ? &#128516; 
</comment><comment author="jasontedor" created="2016-05-06T15:14:18Z" id="217470283">&gt; Ready to :shipit:  ? &#128516; 

I left more comments. Can you run `gradle check`?
</comment><comment author="gmoskovicz" created="2016-05-09T19:07:36Z" id="217959054">@jasontedor Gradle check was successful . 
</comment><comment author="jasontedor" created="2016-05-10T15:23:20Z" id="218192433">LGTM. Thanks @gmoskovicz!
</comment><comment author="gmoskovicz" created="2016-05-10T15:32:50Z" id="218195518">Sweet! Thanks for all the help @jasontedor !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Fix plugin properties generation when version changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18050</link><project id="" key="" /><description>This change fixes the generation of plugin properties files when the
version changes. Before, it would not regenerate, and running integTest
would fail with an incompatibile version error.
</description><key id="151715256">18050</key><summary>Build: Fix plugin properties generation when version changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-04-28T18:46:23Z</created><updated>2016-04-28T21:44:00Z</updated><resolved>2016-04-28T18:48:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-28T18:47:56Z" id="215525332">LGTM
</comment><comment author="dadoonet" created="2016-04-28T21:44:00Z" id="215572092">Thanks a lot Ryan!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FVH not working with proximity phrases and the word "de"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18049</link><project id="" key="" /><description>**Elasticsearch version**:  2.3.1 **JVM version**:1.8.0_31 **OS version**: Win7

**Description**:
Using fast-vector-highlighter, one phrase query produces the search hit, another one makes highlights disappear. The first phrase has proximity configured, the second one is composed by the word "de".

 _"query": "text:\"reformas reformas\"**~1000**  OR text:\"pensoes **de** reforma\""_

**Setup**:

``` shell
# an index
curl -XPUT 'localhost:9200/my-index' -d '{
    "mappings": {
        "my-type": {
            "properties": {
                "title": {
                    "type": "string",
                    "term_vector" : "with_positions_offsets", 
                    "store": "yes",
                    "index": "analyzed",
                    "copy_to": ["text"]
                    },
                "message": {
                    "type": "string",
                    "term_vector" : "with_positions_offsets",
                    "store": "yes",
                    "index": "analyzed",
                    "analyzer": "my_word_delimiter",
                    "copy_to": ["text"]
                },
                "text": {
                    "type": "string",
                    "term_vector" : "with_positions_offsets",
                    "store": "yes",
                    "index": "analyzed",
                    "analyzer": "my_word_delimiter"
                }
            }
        }
    },
    "settings": {
        "analysis": {
            "analyzer": {
                "my_word_delimiter": {
                    "type": "custom",
                    "tokenizer": "whitespace",
                    "filter": ["word_delimiter"]
                }
            }
        }
    }
}'

# a document
curl -XPOST 'localhost:9200/my-index/my-type' -d '{
  "title": "Programa do Governo: declara&#231;&#245;es de M&#225;rio Centeno",
  "message": "reformas ponto de chegada que n&#227;o sera reformas"
}'
```

**Testing highlight**:

``` shell
curl -XPOST 'localhost:9200/my-index/my-type/_search' -d '{
  "query": {
    "query_string": {
      "query": "text:\"reformas reformas\"~1000  OR text:\"pensoes de reforma\"",
      "default_operator": "OR",
      "allow_leading_wildcard": false,
      "default_field": "text",
      "auto_generate_phrase_queries": true
    }
  },
  "highlight": {
    "fields": {
      "text": {
        "type": "fvh"
      }
    }
  }
}'

# results
"hits": [
      {
        "_index": "my-index",
        "_type": "my-type",
        "_id": "AVRdxgd2_pwuzofKZKF5",
        "_score": 0.007454391,
        "_source": {
          "title": "Programa do Governo: declara&#231;&#245;es de M&#225;rio Centeno",
          "message": "reformas ponto de chegada que n&#227;o sera reformas"
        }
      }
```

Result: highlight missing
**Expected result**: The word "reformas" should be highlighted by the first proximity query

**Testing another search** (without the "de" word in the second phrase):

``` shell
curl -XPOST 'localhost:9200/my-index/my-type/_search' -d '{
  "query": {
    "query_string": {
      "query": "text:\"reformas reformas\"~1000  OR text:\"pensoes reforma\"",
      "default_operator": "OR",
      "allow_leading_wildcard": false,
      "default_field": "text",
      "auto_generate_phrase_queries": true
    }
  },
  "highlight": {
    "fields": {
      "text": {
        "type": "fvh"
      }
    }
  }
}'

# results
"hits": [
      {
        "_index": "my-index",
        "_type": "my-type",
        "_id": "AVRdxgd2_pwuzofKZKF5",
        "_score": 0.00850572,
        "_source": {
          "title": "Programa do Governo: declara&#231;&#245;es de M&#225;rio Centeno",
          "message": "reformas ponto de chegada que n&#227;o sera reformas"
        },
        "highlight": {
          "text": [
            "&lt;em&gt;reformas&lt;/em&gt; ponto de chegada que n&#227;o sera &lt;em&gt;reformas&lt;/em&gt;"
          ]
        }
      }
    ]
```

Result OK: One hit with one highlight

**Testing another search** (without proximity but maintaining the word "de" ):

``` shell
curl -XPOST 'localhost:9200/my-index/my-type/_search' -d '{
  "query": {
    "query_string": {
      "query": "text:\"reformas ponto\"  OR text:\"pensoes de reforma\"",
      "default_operator": "OR",
      "allow_leading_wildcard": false,
      "default_field": "text",
      "auto_generate_phrase_queries": true
    }
  },
  "highlight": {
    "fields": {
      "text": {
        "type": "fvh"
      }
    }
  }
}'

# results
"hits": {
    "total": 1,
    "max_score": 0.019722465,
    "hits": [
      {
        "_index": "my-index",
        "_type": "my-type",
        "_id": "AVRdxgd2_pwuzofKZKF5",
        "_score": 0.019722465,
        "_source": {
          "title": "Programa do Governo: declara&#231;&#245;es de M&#225;rio Centeno",
          "message": "reformas ponto de chegada que n&#227;o sera reformas"
        },
        "highlight": {
          "text": [
            "&lt;em&gt;reformas ponto&lt;/em&gt; de chegada que n&#227;o sera reformas"
          ]
        }
      }
    ]
  }
```

Result OK: One hit with one highlight
</description><key id="151690399">18049</key><summary>FVH not working with proximity phrases and the word "de"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fmont</reporter><labels><label>:Highlighting</label></labels><created>2016-04-28T16:44:44Z</created><updated>2016-11-25T14:53:57Z</updated><resolved>2016-11-25T14:53:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-25T14:53:57Z" id="262971845">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update aws sdk to 1.10.69 and add throttle_retries repository setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18048</link><project id="" key="" /><description>This PR brings 2 changes:
- Upgrade to AWS SDK 1.10.69
- Add repositories.s3.use_throttle_retries setting
# Upgrade to AWS SDK 1.10.69
## Release notes highlights from 1.10.33 for the SDK
- Moving from JSON.org to Jackson for request marshallers.
- The Java SDK now supports retry throttling to limit the rate of retries during periods of reduced availability. This throttling behavior can be enabled via ClientConfiguration or via the system property "-Dcom.amazonaws.sdk.enableThrottledRetry".
- Fixed String case conversion issues when running with non English locales.
- AWS SDK for Java introduces a new dynamic endpoint system that can compute endpoints for services in new regions.
- Introducing a new AWS region, ap-northeast-2.
- Added a new metric, HttpSocketReadTime, that records socket read latency. You can enable this metric by adding enableHttpSocketReadMetric to the system property com.amazonaws.sdk.enableDefaultMetrics. For more information, see [Enabling Metrics with the AWS SDK for Java](https://java.awsblog.com/post/Tx3C0RV4NRRBKTG/Enabling-Metrics-with-the-AWS-SDK-for-Java).
- New Client Execution timeout feature to set a limit spent across retries, backoffs, ummarshalling, etc. This new timeout can be specified at the client level or per request.
  Also included in this release is the ability to specify the existing HTTP Request timeout per request rather than just per client.
## Release notes highlights from 1.10.33 for S3
- Added support for RequesterPays for all operations.
- Ignore the 'Connection' header when generating S3 responses.
- Allow users to generate an AmazonS3URI from a string without using URL encoding.
- Fixed issue that prevented creating buckets when using a client configured for the s3-external-1 endpoint.
- Amazon S3 bucket lifecycle configuration supports two new features: the removal of expired object delete markers and an action to abort incomplete multipart uploads.
- Allow TransferManagerConfiguration to accept integer values for multipart upload threshold.
- Copy the list of ETags before sorting aws/aws-sdk-java#589.
- Option to disable chunked encoding aws/aws-sdk-java#586.
- Adding retry on InternalErrors in CompleteMultipartUpload operation. aws/aws-sdk-java#538
- Deprecated two APIs : AmazonS3#changeObjectStorageClass and AmazonS3#setObjectRedirectLocation.
- Added support for the aws-exec-read canned ACL. Owner gets FULL_CONTROL. Amazon EC2 gets READ access to GET an Amazon Machine Image (AMI) bundle from Amazon S3.
## Release notes highlights from 1.10.33 for EC2
- Added support for referencing security groups in peered Virtual Private Clouds (VPCs). For more information see the service announcement at https://aws.amazon.com/about-aws/whats-new/2016/03/announcing-support-for-security-group-references-in-a-peered-vpc/ .
- Fixed a bug in AWS SDK for Java - Amazon EC2 module that returns NPE for dry run requests.
- Regenerated client with new implementation of code generator.
- This feature enables support for DNS resolution of public hostnames to private IP addresses when queried over ClassicLink. Additionally, you can now access private hosted zones associated with your VPC from a linked EC2-Classic instance. ClassicLink DNS support makes it easier for EC2-Classic instances to communicate with VPC resources using public DNS hostnames.
- You can now use Network Address Translation (NAT) Gateway, a highly available AWS managed service that makes it easy to connect to the Internet from instances within a private subnet in an AWS Virtual Private Cloud (VPC). Previously, you needed to launch a NAT instance to enable NAT for instances in a private subnet. Amazon VPC NAT Gateway is available in the US East (N. Virginia), US West (Oregon), US West (N. California), EU (Ireland), Asia Pacific (Tokyo), Asia Pacific (Singapore), and Asia Pacific (Sydney) regions. To learn more about Amazon VPC NAT, see [New - Managed NAT (Network Address Translation) Gateway for AWS](https://aws.amazon.com/blogs/aws/new-managed-nat-network-address-translation-gateway-for-aws/)
- A default read timeout is now applied when querying data from EC2 metadata service.
# Add `repositories.s3.use_throttle_retries` setting

Defaults to `true`.
If anyone is having trouble with this option, you could disable it with `repositories.s3.use_throttle_retries: false` in `elasticsearch.yml` file.

Backport of #17784 in 2.x branch
</description><key id="151675542">18048</key><summary>Update aws sdk to 1.10.69 and add throttle_retries repository setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>upgrade</label><label>v2.4.0</label></labels><created>2016-04-28T15:39:34Z</created><updated>2016-06-16T05:04:52Z</updated><resolved>2016-06-16T05:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-27T10:42:13Z" id="222116182">@tlrx also updated the code here
</comment><comment author="dadoonet" created="2016-06-13T06:24:52Z" id="225498208">ping @tlrx 
</comment><comment author="dadoonet" created="2016-06-15T12:59:29Z" id="226179388">@tlrx I guess you also agreed with this one as it's similar to the one in master and the one for ES 1.7?
</comment><comment author="tlrx" created="2016-06-15T13:09:02Z" id="226181686">Yep
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Explicitly set target and source compatibility for javac</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18047</link><project id="" key="" /><description>Gradle has a "shortcut" which omits the target and source compatibility
we set when it thinks it is not necessary (eg gradle is running on the
same version as the target compat). However, the way we compile against
java 9 is to set javac to use java 9, while gradle still runs on java 8.
This change makes -source and -target explicit for now, until these
"optimizations" can be removed from gradle.

closes #18039
</description><key id="151669448">18047</key><summary>Build: Explicitly set target and source compatibility for javac</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label></labels><created>2016-04-28T15:16:53Z</created><updated>2016-04-28T15:30:43Z</updated><resolved>2016-04-28T15:30:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-28T15:26:55Z" id="215466120">LGTM, (gradle &#3232;_&#3232;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow import of all projects in Eclipse on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18046</link><project id="" key="" /><description>Currently, the projects are using `:` at the beginning of the names. This doesn't allow you to import the entire solution to Eclipse. This fixes the issue.
</description><key id="151649801">18046</key><summary>Allow import of all projects in Eclipse on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-28T14:04:58Z</created><updated>2016-04-28T14:29:06Z</updated><resolved>2016-04-28T14:29:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-28T14:08:08Z" id="215434826">@rjernst do you want to have a look at this? Eclipse on windows doesn't like our (perfectly sensible) `:` in project names. So this turns them into `_` on Windows so they can import the project. This is very sad.
</comment><comment author="rjernst" created="2016-04-28T14:15:55Z" id="215437718">LGTM. Note that the colons are not "ours", they ae simply the separator gradle uses between project names. 
</comment><comment author="nik9000" created="2016-04-28T14:20:36Z" id="215440011">&gt; ours

I'm the one that decided they should be part of the Eclipse project name. Because they are part of the Gradle project name. It just makes sense! I didn't realize I was breaking windows users.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog buffer is not configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18045</link><project id="" key="" /><description>Even though translog config has buffer size parameter in constructor and also IndexConfig it always uses default size of buffer. The default size is pretty small: 8KB. Why not to make it bigger or just configurable? It may turn out that translog generates a lot IO operations, even when it is configured to be "async".
</description><key id="151646411">18045</key><summary>Translog buffer is not configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels /><created>2016-04-28T13:50:28Z</created><updated>2016-05-02T08:44:18Z</updated><resolved>2016-04-28T20:51:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-28T15:33:09Z" id="215468636">&gt; The default size is pretty small: 8KB. Why not to make it bigger or just configurable?

An 8k buffer avoids the JVM having to malloc/free on every write, this is the same reason that Lucene writes in 8kb chunks
</comment><comment author="prog8" created="2016-04-28T16:39:13Z" id="215488776">JVM having malloc/free? I think I don't get it. Why bigger buffer cannot be used?
</comment><comment author="dakrone" created="2016-04-28T20:47:49Z" id="215557529">&gt; JVM having malloc/free?

The JVM is written in C/C++, writes to a filesystem larger than 8kb require a `malloc()` and `free()` instead of allocating the buffer on the stack. Additionally, I don't believe we have any plans to make this buffer configurable.
</comment><comment author="jasontedor" created="2016-04-28T20:51:18Z" id="215558586">I agree with everything @dakrone has said here and I also do not think this should be configurable.
</comment><comment author="prog8" created="2016-04-28T21:17:18Z" id="215565202">&gt; The JVM is written in C/C++, writes to a filesystem larger than 8kb require a malloc() and free() instead of allocating the buffer on the stack.

Sure. I don't mean it is not written in C/C++. I just didn't see connection between malloc and why buffer size for translog cannot be configurable. I was using default/hardcoded buffer size (8KB) and async translog. Java Mission Control showed me a lot of write operations to disk, which means a lot of IOPS. After I changed the hardcoded buffer size to 512KB, JMC shows much less write operations but they are 512KB instead of 8KB. This is why I don't see a reason why the buffer size cannot be configurable.
</comment><comment author="otisg" created="2016-05-01T19:08:39Z" id="216064862">@prog8 are you able to quantify the change in write ops when you changed the buffer from 8KB to 512KB?  Maybe if that change is large and there are no negative side-effects this would be worth reconsidering @dakrone @jasontedor ?
</comment><comment author="prog8" created="2016-05-02T08:29:03Z" id="216139240">I was just looking at Java Mission Control. Before the change I saw plenty of write operations in JMC. After changing it to 512KB the number was much smaller. I cannot quantify the change in number of write operations per second. I was just surprised that many parameters are configurable in ES - including translog sync_interval but it is not possible to change translog buffer size so it is not easy to find out how changing the buffer size impacts the number of write operations.
</comment><comment author="s1monw" created="2016-05-02T08:44:18Z" id="216148269">&gt; I agree with everything @dakrone has said here and I also do not think this should be configurable.

+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_cat/pending_tasks presents many tasks with _add_listener source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18044</link><project id="" key="" /><description>Kibana connected to cluster cannot connnect to elasticsearch frequently.

Nothing interesting in logs, except this request: /_cat/pending_tasks

What are those entries?

/_cat/pending_tasks
108043        1.8h HIGH     _add_listener_ 
     108058        1.8h HIGH     _add_listener_ 
     108035        1.8h HIGH     _add_listener_ 
     108048        1.8h HIGH     _add_listener_ 
     108064        1.8h HIGH     _add_listener_ 
     108024        1.8h HIGH     _add_listener_ 
     108051        1.8h HIGH     _add_listener_ 
     108044        1.8h HIGH     _add_listener_ 
     108066        1.8h HIGH     _add_listener_ 
     108060        1.8h HIGH     cluster_reroute(post_node_add) 
     108103        1.8h HIGH     _add_listener_ 
     108065        1.8h HIGH     _add_listener_ 
     108069        1.8h HIGH     _add_listener_ 
     108073        1.8h HIGH     _add_listener_ 
     108074        1.8h HIGH     _add_listener_ 
     108075        1.8h HIGH     _add_listener_ 
     108079        1.8h HIGH     _add_listener_ 
     108026        1.8h HIGH     _add_listener_ 
     108081        1.8h HIGH     _add_listener_ 
     108082        1.8h HIGH     _add_listener_ 
     108091        1.8h HIGH     _add_listener_ 
     108092        1.8h HIGH     _add_listener_ 
     108089        1.8h HIGH     _add_listener_ 
     108090        1.8h HIGH     _add_listener_ 
     108099        1.8h HIGH     _add_listener_ 
     108098        1.8h HIGH     _add_listener_ 
     108104        1.8h HIGH     _add_listener_ 
     108186        1.7h HIGH     _add_listener_ 
     108108        1.8h HIGH     _add_listener_ 
     108114        1.8h HIGH     _add_listener_ 
     108110        1.8h HIGH     _add_listener_ 
     108113        1.8h HIGH     _add_listener_ 
     108117        1.7h HIGH     _add_listener_ 
     108215        1.7h HIGH     _add_listener_ 
     108119        1.7h HIGH     _add_listener_ 
     108125        1.7h HIGH     _add_listener_ 
     108124        1.7h HIGH     _add_listener_ 
     108129        1.7h HIGH     _add_listener_ 
     108134        1.7h HIGH     _add_listener_ 
     108133        1.7h HIGH     _add_listener_ 
     108137        1.7h HIGH     _add_listener_ 
     108141        1.7h HIGH     _add_listener_ 
     108034        1.8h HIGH     _add_listener_ 
     108143        1.7h HIGH     _add_listener_ 
     108147        1.7h HIGH     _add_listener_ 
     108149        1.7h HIGH     _add_listener_ 
     108152        1.7h HIGH     _add_listener_ 
     108151        1.7h HIGH     _add_listener_ 
     108172        1.7h HIGH     _add_listener_ 
     108155        1.7h HIGH     _add_listener_ 
     108165        1.7h HIGH     _add_listener_ 
     108307        1.6h HIGH     _add_listener_ 
     108166        1.7h HIGH     _add_listener_ 
     108167        1.7h HIGH     _add_listener_ 
     108176        1.7h HIGH     _add_listener_ 
     108178        1.7h HIGH     _add_listener_ 
     108337        1.5h HIGH     _add_listener_ 
     108177        1.7h HIGH     _add_listener_ 
     108187        1.7h HIGH     _add_listener_ 
     108352        1.5h HIGH     _add_listener_ 
     108189        1.7h HIGH     _add_listener_ 
     108191        1.7h HIGH     _add_listener_ 
     108222        1.7h HIGH     _add_listener_ 
     108201        1.7h HIGH     _add_listener_ 
     108197        1.7h HIGH     _add_listener_ 
     108196        1.7h HIGH     _add_listener_ 
     108204        1.7h HIGH     _add_listener_ 
     108217        1.7h HIGH     _add_listener_ 
     108209        1.7h HIGH     _add_listener_ 
     108208        1.7h HIGH     _add_listener_ 
     108212        1.7h HIGH     _add_listener_ 
     108218        1.7h HIGH     _add_listener_ 
     108128        1.7h HIGH     _add_listener_ 
     108227        1.6h HIGH     _add_listener_ 
     108228        1.6h HIGH     _add_listener_ 
     108226        1.7h HIGH     _add_listener_ 
     108231        1.6h HIGH     _add_listener_ 
     108246        1.6h HIGH     _add_listener_ 
     108238        1.6h HIGH     _add_listener_ 
     108236        1.6h HIGH     _add_listener_ 
     108239        1.6h HIGH     _add_listener_ 
     108243        1.6h HIGH     _add_listener_ 
     108247        1.6h HIGH     _add_listener_ 
     108245        1.6h HIGH     _add_listener_ 
     108255        1.6h HIGH     _add_listener_ 
     108256        1.6h HIGH     _add_listener_ 
     108257        1.6h HIGH     _add_listener_ 
     108254        1.6h HIGH     _add_listener_ 
     108263        1.6h HIGH     _add_listener_ 
     108499        1.4h HIGH     _add_listener_ 
     108265        1.6h HIGH     _add_listener_ 
     108142        1.7h HIGH     _add_listener_ 
     108052        1.8h HIGH     _add_listener_ 
     108302        1.6h HIGH     _add_listener_ 
     108279        1.6h HIGH     _add_listener_ 
     108277        1.6h HIGH     _add_listener_ 
     108278        1.6h HIGH     _add_listener_ 
     108295        1.6h HIGH     _add_listener_ 
     108284        1.6h HIGH     _add_listener_ 
     108288        1.6h HIGH     _add_listener_ 
     108286        1.6h HIGH     _add_listener_ 
     108287        1.6h HIGH     _add_listener_ 
     108296        1.6h HIGH     _add_listener_ 
     108293        1.6h HIGH     _add_listener_ 
     108294        1.6h HIGH     _add_listener_ 
     108306        1.6h HIGH     _add_listener_ 
     108583        1.3h HIGH     _add_listener_ 
     108305        1.6h HIGH     _add_listener_ 
     108310        1.6h HIGH     _add_listener_ 
     108314        1.6h HIGH     _add_listener_ 
     108312        1.6h HIGH     _add_listener_ 
     108313        1.6h HIGH     _add_listener_ 
     108331        1.5h HIGH     _add_listener_ 
     108324        1.6h HIGH     _add_listener_ 
     108325        1.6h HIGH     _add_listener_ 
     108326        1.6h HIGH     _add_listener_ 
     108323        1.6h HIGH     _add_listener_ 
     108335        1.5h HIGH     _add_listener_ 
     108336        1.5h HIGH     _add_listener_ 
     108651        1.3h HIGH     _add_listener_ 
     108342        1.5h HIGH     _add_listener_ 
     108182        1.7h HIGH     _add_listener_ 
     108343        1.5h HIGH     _add_listener_ 
     108344        1.5h HIGH     _add_listener_ 
     108680        1.2h HIGH     _add_listener_ 
     108356        1.5h HIGH     _add_listener_ 
     108357        1.5h HIGH     _add_listener_ 
     108358        1.5h HIGH     _add_listener_ 
     108363        1.5h HIGH     _add_listener_ 
     108367        1.5h HIGH     _add_listener_ 
     108368        1.5h HIGH     _add_listener_ 
     108369        1.5h HIGH     _add_listener_ 
     108375        1.5h HIGH     _add_listener_ 
     108373        1.5h HIGH     _add_listener_ 
     108374        1.5h HIGH     _add_listener_ 
     108391        1.5h HIGH     _add_listener_ 
     108377        1.5h HIGH     _add_listener_ 
     108745        1.2h HIGH     _add_listener_ 
     108387        1.5h HIGH     _add_listener_ 
     108392        1.5h HIGH     _add_listener_ 
     108393        1.5h HIGH     _add_listener_ 
     108400        1.5h HIGH     _add_listener_ 
     108401        1.5h HIGH     _add_listener_ 
     108402        1.5h HIGH     _add_listener_ 
     108399        1.5h HIGH     _add_listener_ 
     108409        1.5h HIGH     _add_listener_ 
     108410        1.5h HIGH     _add_listener_ 
     108413        1.5h HIGH     _add_listener_ 
     108412        1.5h HIGH     _add_listener_ 
     108417        1.5h HIGH     _add_listener_ 
     108422        1.5h HIGH     _add_listener_ 
     108420        1.5h HIGH     _add_listener_ 
     108421        1.5h HIGH     _add_listener_ 
     108431        1.5h HIGH     _add_listener_ 
     108432        1.5h HIGH     _add_listener_ 
     108429        1.5h HIGH     _add_listener_ 
     108430        1.5h HIGH     _add_listener_ 
     108441        1.4h HIGH     _add_listener_ 
     108442        1.4h HIGH     _add_listener_ 
     108439        1.5h HIGH     _add_listener_ 
     108440        1.4h HIGH     _add_listener_ 
     108451        1.4h HIGH     _add_listener_ 
     108452        1.4h HIGH     _add_listener_ 
     108449        1.4h HIGH     _add_listener_ 
     108450        1.4h HIGH     _add_listener_ 
     108469        1.4h HIGH     _add_listener_ 
     108461        1.4h HIGH     _add_listener_ 
     108464        1.4h HIGH     _add_listener_ 
     108462        1.4h HIGH     _add_listener_ 
     108460        1.4h HIGH     _add_listener_ 
     108470        1.4h HIGH     _add_listener_ 
     108472        1.4h HIGH     _add_listener_ 
     108471        1.4h HIGH     _add_listener_ 
     108480        1.4h HIGH     _add_listener_ 
     108481        1.4h HIGH     _add_listener_ 
     108479        1.4h HIGH     _add_listener_ 
     108485        1.4h HIGH     _add_listener_ 
     108490        1.4h HIGH     _add_listener_ 
     108488        1.4h HIGH     _add_listener_ 
     108489        1.4h HIGH     _add_listener_ 
     108492        1.4h HIGH     _add_listener_ 
     108520        1.4h HIGH     _add_listener_ 
     108267        1.6h HIGH     _add_listener_ 
     108497        1.4h HIGH     _add_listener_ 
     108498        1.4h HIGH     _add_listener_ 
     108516        1.4h HIGH     _add_listener_ 
     108502        1.4h HIGH     _add_listener_ 
     108515        1.4h HIGH     _add_listener_ 
     108511        1.4h HIGH     _add_listener_ 
     108510        1.4h HIGH     _add_listener_ 
     108266        1.6h HIGH     _add_listener_ 
     108083        1.8h HIGH     _add_listener_ 
     108519        1.4h HIGH     _add_listener_ 
     108530        1.4h HIGH     _add_listener_ 
     108528        1.4h HIGH     _add_listener_ 
     108527        1.4h HIGH     _add_listener_ 
     108529        1.4h HIGH     _add_listener_ 
     108538        1.4h HIGH     _add_listener_ 
     108536        1.4h HIGH     _add_listener_ 
     108537        1.4h HIGH     _add_listener_ 
     108540        1.4h HIGH     _add_listener_ 
     108557        1.3h HIGH     _add_listener_ 
     108548        1.3h HIGH     _add_listener_ 
     108550        1.3h HIGH     _add_listener_ 
     108549        1.3h HIGH     _add_listener_ 
     108547        1.3h HIGH     _add_listener_ 
     108558        1.3h HIGH     _add_listener_ 
     108559        1.3h HIGH     _add_listener_ 
     108563        1.3h HIGH     _add_listener_ 
     108568        1.3h HIGH     _add_listener_ 
     108569        1.3h HIGH     _add_listener_ 
     108567        1.3h HIGH     _add_listener_ 
     108571        1.3h HIGH     _add_listener_ 
     108575        1.3h HIGH     _add_listener_ 
     108587        1.3h HIGH     _add_listener_ 
     108577        1.3h HIGH     _add_listener_ 
     108578        1.3h HIGH     _add_listener_ 
     109140       50.1m HIGH     _add_listener_ 
     108588        1.3h HIGH     _add_listener_ 
     108589        1.3h HIGH     _add_listener_ 
     108593        1.3h HIGH     _add_listener_ 
     108597        1.3h HIGH     _add_listener_ 
     108601        1.3h HIGH     _add_listener_ 
     108603        1.3h HIGH     _add_listener_ 
     108602        1.3h HIGH     _add_listener_ 
     108609        1.3h HIGH     _add_listener_ 
     108607        1.3h HIGH     _add_listener_ 
     108608        1.3h HIGH     _add_listener_ 
     108612        1.3h HIGH     _add_listener_ 
     108624        1.3h HIGH     _add_listener_ 
     108620        1.3h HIGH     _add_listener_ 
     108619        1.3h HIGH     _add_listener_ 
     108617        1.3h HIGH     _add_listener_ 
     108634        1.3h HIGH     _add_listener_ 
     108627        1.3h HIGH     _add_listener_ 
     108630        1.3h HIGH     _add_listener_ 
     108629        1.3h HIGH     _add_listener_ 
     108638        1.3h HIGH     _add_listener_ 
     108639        1.3h HIGH     _add_listener_ 
     108641        1.3h HIGH     _add_listener_ 
     108644        1.3h HIGH     _add_listener_ 
     108659        1.2h HIGH     _add_listener_ 
     108652        1.3h HIGH     _add_listener_ 
     108345        1.5h HIGH     _add_listener_ 
     108656        1.2h HIGH     _add_listener_ 
     108653        1.3h HIGH     _add_listener_ 
     108660        1.2h HIGH     _add_listener_ 
     108661        1.2h HIGH     _add_listener_ 
     108665        1.2h HIGH     _add_listener_ 
     108672        1.2h HIGH     _add_listener_ 
     108673        1.2h HIGH     _add_listener_ 
     108674        1.2h HIGH     _add_listener_ 
     108676        1.2h HIGH     _add_listener_ 
     108681        1.2h HIGH     _add_listener_ 
     108679        1.2h HIGH     _add_listener_ 
     109338       38.9m HIGH     _add_listener_ 
     108685        1.2h HIGH     _add_listener_ 
     108693        1.2h HIGH     _add_listener_ 
     108694        1.2h HIGH     _add_listener_ 
     108696        1.2h HIGH     _add_listener_ 
     108695        1.2h HIGH     _add_listener_ 
     108704        1.2h HIGH     _add_listener_ 
     108705        1.2h HIGH     _add_listener_ 
     108707        1.2h HIGH     _add_listener_ 
     108703        1.2h HIGH     _add_listener_ 
     108715        1.2h HIGH     _add_listener_ 
     108717        1.2h HIGH     _add_listener_ 
     108716        1.2h HIGH     _add_listener_ 
     108714        1.2h HIGH     _add_listener_ 
     108725        1.2h HIGH     _add_listener_ 
     108727        1.2h HIGH     _add_listener_ 
     108726        1.2h HIGH     _add_listener_ 
     108724        1.2h HIGH     _add_listener_ 
     108737        1.2h HIGH     _add_listener_ 
     108736        1.2h HIGH     _add_listener_ 
     108734        1.2h HIGH     _add_listener_ 
     108735        1.2h HIGH     _add_listener_ 
     109446       32.6m HIGH     _add_listener_ 
     108747        1.2h HIGH     _add_listener_ 
     108746        1.2h HIGH     _add_listener_ 
     108744        1.2h HIGH     _add_listener_ 
     108753        1.2h HIGH     _add_listener_ 
     108752        1.2h HIGH     _add_listener_ 
     108757        1.2h HIGH     _add_listener_ 
     108756        1.2h HIGH     _add_listener_ 
     108762        1.1h HIGH     _add_listener_ 
     108761        1.1h HIGH     _add_listener_ 
     108766        1.1h HIGH     _add_listener_ 
     108767        1.1h HIGH     _add_listener_ 
     108771        1.1h HIGH     _add_listener_ 
     108770        1.1h HIGH     _add_listener_ 
     108775        1.1h HIGH     _add_listener_ 
     108776        1.1h HIGH     _add_listener_ 
     108782        1.1h HIGH     _add_listener_ 
     108784        1.1h HIGH     _add_listener_ 
     108783        1.1h HIGH     _add_listener_ 
     108786        1.1h HIGH     _add_listener_ 
     108790        1.1h HIGH     _add_listener_ 
     108789        1.1h HIGH     _add_listener_ 
     108806        1.1h HIGH     _add_listener_ 
     108807        1.1h HIGH     _add_listener_ 
     108794        1.1h HIGH     _add_listener_ 
     108796        1.1h HIGH     _add_listener_ 
     108803        1.1h HIGH     _add_listener_ 
     108802        1.1h HIGH     _add_listener_ 
     108811        1.1h HIGH     _add_listener_ 
     108812        1.1h HIGH     _add_listener_ 
     108822        1.1h HIGH     _add_listener_ 
     108816        1.1h HIGH     _add_listener_ 
     108817        1.1h HIGH     _add_listener_ 
     108821        1.1h HIGH     _add_listener_ 
     108850        1.1h HIGH     _add_listener_ 
     108827        1.1h HIGH     _add_listener_ 
     108826        1.1h HIGH     _add_listener_ 
     108839        1.1h HIGH     _add_listener_ 
     108832        1.1h HIGH     _add_listener_ 
     108831        1.1h HIGH     _add_listener_ 
     108838        1.1h HIGH     _add_listener_ 
     108844        1.1h HIGH     _add_listener_ 
     108846        1.1h HIGH     _add_listener_ 
     108843        1.1h HIGH     _add_listener_ 
     108855        1.1h HIGH     _add_listener_ 
     108854        1.1h HIGH     _add_listener_ 
     108859        1.1h HIGH     _add_listener_ 
     108860        1.1h HIGH     _add_listener_ 
     108865        1.1h HIGH     _add_listener_ 
     108864        1.1h HIGH     _add_listener_ 
     108867          1h HIGH     _add_listener_ 
     108870          1h HIGH     _add_listener_ 
     108871          1h HIGH     _add_listener_ 
     108872          1h HIGH     _add_listener_ 
     108893          1h HIGH     _add_listener_ 
     108894          1h HIGH     _add_listener_ 
     108883          1h HIGH     _add_listener_ 
     108884          1h HIGH     _add_listener_ 
     108879          1h HIGH     _add_listener_ 
     108880          1h HIGH     _add_listener_ 
     108888          1h HIGH     _add_listener_ 
     108889          1h HIGH     _add_listener_ 
     108898          1h HIGH     _add_listener_ 
     108899          1h HIGH     _add_listener_ 
     108902          1h HIGH     _add_listener_ 
     108904          1h HIGH     _add_listener_ 
     108908          1h HIGH     _add_listener_ 
     108907          1h HIGH     _add_listener_ 
     108910          1h HIGH     _add_listener_ 
     108912          1h HIGH     _add_listener_ 
     108939          1h HIGH     _add_listener_ 
     108918          1h HIGH     _add_listener_ 
     108919          1h HIGH     _add_listener_ 
     108931          1h HIGH     _add_listener_ 
     108923          1h HIGH     _add_listener_ 
     108922          1h HIGH     _add_listener_ 
     108930          1h HIGH     _add_listener_ 
     108933          1h HIGH     _add_listener_ 
     108934          1h HIGH     _add_listener_ 
     108938          1h HIGH     _add_listener_ 
     108943          1h HIGH     _add_listener_ 
     108944          1h HIGH     _add_listener_ 
     108948          1h HIGH     _add_listener_ 
     108949          1h HIGH     _add_listener_ 
     108954          1h HIGH     _add_listener_ 
     108953          1h HIGH     _add_listener_ 
     108967          1h HIGH     _add_listener_ 
     108968          1h HIGH     _add_listener_ 
     108964          1h HIGH     _add_listener_ 
     108961          1h HIGH     _add_listener_ 
     108960          1h HIGH     _add_listener_ 
     108969          1h HIGH     _add_listener_ 
     108977       59.9m HIGH     _add_listener_ 
     108976       59.9m HIGH     _add_listener_ 
     108982       59.6m HIGH     _add_listener_ 
     108981       59.7m HIGH     _add_listener_ 
     108984       59.4m HIGH     _add_listener_ 
     108989       59.1m HIGH     _add_listener_ 
     108987       59.2m HIGH     _add_listener_ 
     108988       59.1m HIGH     _add_listener_ 
     108993       58.8m HIGH     _add_listener_ 
     108995       58.7m HIGH     _add_listener_ 
     108998       58.6m HIGH     _add_listener_ 
     108999       58.6m HIGH     _add_listener_ 
     109011       57.7m HIGH     _add_listener_ 
     109008         58m HIGH     _add_listener_ 
     109003       58.2m HIGH     _add_listener_ 
     109006       58.1m HIGH     _add_listener_ 
     109007         58m HIGH     _add_listener_ 
     109016       57.4m HIGH     _add_listener_ 
     109014       57.6m HIGH     _add_listener_ 
     108508        1.4h HIGH     _add_listener_ 
     108274        1.6h HIGH     _add_listener_ 
     109024       56.9m HIGH     _add_listener_ 
     109022         57m HIGH     _add_listener_ 
     109023       56.9m HIGH     _add_listener_ 
     109038       56.3m HIGH     _add_listener_ 
     109031       56.6m HIGH     _add_listener_ 
     109033       56.6m HIGH     _add_listener_ 
     109034       56.4m HIGH     _add_listener_ 
     109037       56.3m HIGH     _add_listener_ 
     109040         56m HIGH     _add_listener_ 
     109044       55.8m HIGH     _add_listener_ 
     109052       55.4m HIGH     _add_listener_ 
     109048       55.7m HIGH     _add_listener_ 
     109047       55.7m HIGH     _add_listener_ 
     109056       55.1m HIGH     _add_listener_ 
     109057       55.1m HIGH     _add_listener_ 
     109055       55.2m HIGH     _add_listener_ 
     109064       54.8m HIGH     _add_listener_ 
     109068       54.5m HIGH     _add_listener_ 
     109066       54.6m HIGH     _add_listener_ 
     109067       54.6m HIGH     _add_listener_ 
     109076         54m HIGH     _add_listener_ 
     109077         54m HIGH     _add_listener_ 
     109075       54.2m HIGH     _add_listener_ 
     109080       53.9m HIGH     _add_listener_ 
     109084       53.6m HIGH     _add_listener_ 
     109087       53.4m HIGH     _add_listener_ 
     109086       53.4m HIGH     _add_listener_ 
     109089       53.3m HIGH     _add_listener_ 
     109092       53.1m HIGH     _add_listener_ 
     109097       52.8m HIGH     _add_listener_ 
     109095       52.8m HIGH     _add_listener_ 
     109096       52.8m HIGH     _add_listener_ 
     109100       52.5m HIGH     _add_listener_ 
     109104       52.3m HIGH     _add_listener_ 
     109107       52.2m HIGH     _add_listener_ 
     109108       52.2m HIGH     _add_listener_ 
     109110       51.9m HIGH     _add_listener_ 
     109118       51.5m HIGH     _add_listener_ 
     109114       51.7m HIGH     _add_listener_ 
     109117       51.5m HIGH     _add_listener_ 
     109122       51.1m HIGH     _add_listener_ 
     109121       51.3m HIGH     _add_listener_ 
     109125         51m HIGH     _add_listener_ 
     109126         51m HIGH     _add_listener_ 
     109128       50.7m HIGH     _add_listener_ 
     109132       50.5m HIGH     _add_listener_ 
     109135       50.4m HIGH     _add_listener_ 
     109136       50.4m HIGH     _add_listener_ 
     108168        1.7h NORMAL   master ping (from: [mq1eVWTaQQW6vyLqiJyxgA]) 
     109141         50m HIGH     _add_listener_ 
     109144       49.9m HIGH     _add_listener_ 
     109145       49.9m HIGH     _add_listener_ 
     109150       49.5m HIGH     _add_listener_ 
     109151       49.4m HIGH     _add_listener_ 
     109154       49.3m HIGH     _add_listener_ 
     109155       49.2m HIGH     _add_listener_ 
     109168       48.3m HIGH     _add_listener_ 
     109163       48.7m HIGH     _add_listener_ 
     109161       48.8m HIGH     _add_listener_ 
     109160       48.9m HIGH     _add_listener_ 
     109164       48.7m HIGH     _add_listener_ 
     109172       48.2m HIGH     _add_listener_ 
     109174         48m HIGH     _add_listener_ 
     109173       48.2m HIGH     _add_listener_ 
     109181       47.7m HIGH     _add_listener_ 
     109183       47.6m HIGH     _add_listener_ 
     109182       47.7m HIGH     _add_listener_ 
     109190       47.1m HIGH     _add_listener_ 
     109186       47.4m HIGH     _add_listener_ 
     109191       47.1m HIGH     _add_listener_ 
     109193       46.9m HIGH     _add_listener_ 
     109197       46.8m HIGH     _add_listener_ 
     109202       46.5m HIGH     _add_listener_ 
     109201       46.5m HIGH     _add_listener_ 
     109206       46.3m HIGH     _add_listener_ 
     109207       46.2m HIGH     _add_listener_ 
     109211       45.9m HIGH     _add_listener_ 
     109212       45.9m HIGH     _add_listener_ 
     109216       45.8m HIGH     _add_listener_ 
     109217       45.6m HIGH     _add_listener_ 
     109221       45.4m HIGH     _add_listener_ 
     109222       45.3m HIGH     _add_listener_ 
     109226       45.2m HIGH     _add_listener_ 
     109227       45.1m HIGH     _add_listener_ 
     109239       44.5m HIGH     _add_listener_ 
     109234       44.7m HIGH     _add_listener_ 
     109236       44.6m HIGH     _add_listener_ 
     109233       44.8m HIGH     _add_listener_ 
     109243       44.2m HIGH     _add_listener_ 
     109244       44.1m HIGH     _add_listener_ 
     109248         44m HIGH     _add_listener_ 
     109249       43.9m HIGH     _add_listener_ 
     109255       43.6m HIGH     _add_listener_ 
     109257       43.5m HIGH     _add_listener_ 
     109256       43.6m HIGH     _add_listener_ 
     109261       43.4m HIGH     _add_listener_ 
     109263         43m HIGH     _add_listener_ 
     109265         43m HIGH     _add_listener_ 
     109275       42.4m HIGH     _add_listener_ 
     109267       42.9m HIGH     _add_listener_ 
     109273       42.5m HIGH     _add_listener_ 
     109271       42.8m HIGH     _add_listener_ 
     109276       42.4m HIGH     _add_listener_ 
     109288       41.8m HIGH     _add_listener_ 
     109280       42.1m HIGH     _add_listener_ 
     109292       41.6m HIGH     _add_listener_ 
     109287       41.8m HIGH     _add_listener_ 
     109285       41.9m HIGH     _add_listener_ 
     109294       41.3m HIGH     _add_listener_ 
     109297       41.2m HIGH     _add_listener_ 
     109296       41.2m HIGH     _add_listener_ 
     109299         41m HIGH     _add_listener_ 
     109303       40.7m HIGH     _add_listener_ 
     109308       40.4m HIGH     _add_listener_ 
     109307       40.6m HIGH     _add_listener_ 
     109306       40.6m HIGH     _add_listener_ 
     109312       40.2m HIGH     _add_listener_ 
     109317       39.9m HIGH     _add_listener_ 
     109316         40m HIGH     _add_listener_ 
     109315         40m HIGH     _add_listener_ 
     109321       39.6m HIGH     _add_listener_ 
     109326       39.4m HIGH     _add_listener_ 
     109328       39.3m HIGH     _add_listener_ 
     109327       39.4m HIGH     _add_listener_ 
     109331         39m HIGH     _add_listener_ 
     109344       38.5m HIGH     _add_listener_ 
     109337       38.9m HIGH     _add_listener_ 
     108059        1.8h NORMAL   master ping (from: [jKaTTXmARGGZ4wawz5Qq4A]) 
     109340       38.7m HIGH     _add_listener_ 
     109345       38.3m HIGH     _add_listener_ 
     109346       38.3m HIGH     _add_listener_ 
     109350       38.2m HIGH     _add_listener_ 
     109353       37.9m HIGH     _add_listener_ 
     109357       37.7m HIGH     _add_listener_ 
     109359       37.6m HIGH     _add_listener_ 
     109358       37.7m HIGH     _add_listener_ 
     109363       37.3m HIGH     _add_listener_ 
     109379       36.5m HIGH     _add_listener_ 
     109365       37.1m HIGH     _add_listener_ 
     109366       37.1m HIGH     _add_listener_ 
     109378       36.5m HIGH     _add_listener_ 
     109369         37m HIGH     _add_listener_ 
     109375       36.7m HIGH     _add_listener_ 
     109380       36.4m HIGH     _add_listener_ 
     109384         36m HIGH     _add_listener_ 
     109388       35.9m HIGH     _add_listener_ 
     109389       35.9m HIGH     _add_listener_ 
     109390       35.9m HIGH     _add_listener_ 
     109394       35.5m HIGH     _add_listener_ 
     109397       35.4m HIGH     _add_listener_ 
     109396       35.4m HIGH     _add_listener_ 
     109400       35.3m HIGH     _add_listener_ 
     109407       34.8m HIGH     _add_listener_ 
     109408       34.8m HIGH     _add_listener_ 
     109410       34.7m HIGH     _add_listener_ 
     109406       34.9m HIGH     _add_listener_ 
     109418       34.3m HIGH     _add_listener_ 
     109416       34.4m HIGH     _add_listener_ 
     109417       34.3m HIGH     _add_listener_ 
     109422       34.1m HIGH     _add_listener_ 
     109426       33.8m HIGH     _add_listener_ 
     109430       33.7m HIGH     _add_listener_ 
     109432       33.5m HIGH     _add_listener_ 
     109431       33.7m HIGH     _add_listener_ 
     109438       33.1m HIGH     _add_listener_ 
     109436       33.1m HIGH     _add_listener_ 
     109437       33.1m HIGH     _add_listener_ 
     109451       32.5m HIGH     _add_listener_ 
     109441         33m HIGH     _add_listener_ 
     108388        1.5h NORMAL   master ping (from: [j5uAM6fBRqissdk5wbF_Ag]) 
     109450       32.5m HIGH     _add_listener_ 
     109452       32.4m HIGH     _add_listener_ 
     109463       31.7m HIGH     _add_listener_ 
     109460       31.9m HIGH     _add_listener_ 
     109457         32m HIGH     _add_listener_ 
     109459       31.9m HIGH     _add_listener_ 
     109465       31.5m HIGH     _add_listener_ 
     109468       31.4m HIGH     _add_listener_ 
     109469       31.4m HIGH     _add_listener_ 
     109478       30.8m HIGH     _add_listener_ 
     109471       31.1m HIGH     _add_listener_ 
     109476       30.9m HIGH     _add_listener_ 
     109479       30.8m HIGH     _add_listener_ 
     109482       30.5m HIGH     _add_listener_ 
     109486       30.3m HIGH     _add_listener_ 
     109489       30.2m HIGH     _add_listener_ 
     109488       30.2m HIGH     _add_listener_ 
     109493       29.9m HIGH     _add_listener_ 
     109499       29.6m HIGH     _add_listener_ 
     109497       29.7m HIGH     _add_listener_ 
     109498       29.6m HIGH     _add_listener_ 
     109512       28.8m HIGH     _add_listener_ 
     109502       29.4m HIGH     _add_listener_ 
     109508         29m HIGH     _add_listener_ 
     109506       29.1m HIGH     _add_listener_ 
     109507         29m HIGH     _add_listener_ 
     109516       28.6m HIGH     _add_listener_ 
     109521       28.4m HIGH     _add_listener_ 
     109520       28.4m HIGH     _add_listener_ 
     109525       28.1m HIGH     _add_listener_ 
     109524       28.2m HIGH     _add_listener_ 
     109530       27.9m HIGH     _add_listener_ 
     109529       27.9m HIGH     _add_listener_ 
     109533       27.6m HIGH     _add_listener_ 
     109540       27.3m HIGH     _add_listener_ 
     109536       27.4m HIGH     _add_listener_ 
     109541       27.3m HIGH     _add_listener_ 
     109544       26.9m HIGH     _add_listener_ 
     109561       26.2m HIGH     _add_listener_ 
     109549       26.8m HIGH     _add_listener_ 
     109547       26.8m HIGH     _add_listener_ 
     109548       26.8m HIGH     _add_listener_ 
     109557       26.3m HIGH     _add_listener_ 
     109556       26.4m HIGH     _add_listener_ 
     109562       26.2m HIGH     _add_listener_ 
     109566       25.8m HIGH     _add_listener_ 
     109568       25.7m HIGH     _add_listener_ 
     109571       25.6m HIGH     _add_listener_ 
     109572       25.6m HIGH     _add_listener_ 
     109577       25.1m HIGH     _add_listener_ 
     109576       25.3m HIGH     _add_listener_ 
     109582         25m HIGH     _add_listener_ 
     109581         25m HIGH     _add_listener_ 
     109587       24.5m HIGH     _add_listener_ 
     109586       24.6m HIGH     _add_listener_ 
     109592       24.4m HIGH     _add_listener_ 
     109591       24.4m HIGH     _add_listener_ 
     109596         24m HIGH     _add_listener_ 
     109597         24m HIGH     _add_listener_ 
     109601       23.8m HIGH     _add_listener_ 
     109600       23.8m HIGH     _add_listener_ 
     109608       23.4m HIGH     _add_listener_ 
     109607       23.5m HIGH     _add_listener_ 
     109613       23.3m HIGH     _add_listener_ 
     109612       23.3m HIGH     _add_listener_ 
     109617       22.9m HIGH     _add_listener_ 
     109618       22.9m HIGH     _add_listener_ 
     109622       22.6m HIGH     _add_listener_ 
     109621       22.6m HIGH     _add_listener_ 
     109631       22.1m HIGH     _add_listener_ 
     109632       22.1m HIGH     _add_listener_ 
     109630       22.2m HIGH     _add_listener_ 
     109629       22.3m HIGH     _add_listener_ 
     109651       21.1m HIGH     _add_listener_ 
     109637       21.7m HIGH     _add_listener_ 
     109639       21.7m HIGH     _add_listener_ 
     109650       21.1m HIGH     _add_listener_ 
     109643       21.6m HIGH     _add_listener_ 
     109642       21.6m HIGH     _add_listener_ 
     109661       20.5m HIGH     _add_listener_ 
     109653       20.9m HIGH     _add_listener_ 
     109654       20.9m HIGH     _add_listener_ 
     109660       20.5m HIGH     _add_listener_ 
     109667       20.3m HIGH     _add_listener_ 
     109666       20.3m HIGH     _add_listener_ 
     109671       19.9m HIGH     _add_listener_ 
     109670       19.9m HIGH     _add_listener_ 
     109677       19.7m HIGH     _add_listener_ 
     109678       19.7m HIGH     _add_listener_ 
     109682       19.4m HIGH     _add_listener_ 
     109683       19.4m HIGH     _add_listener_ 
     109691       18.9m HIGH     _add_listener_ 
     109688       19.1m HIGH     _add_listener_ 
     109687       19.1m HIGH     _add_listener_ 
     109692       18.8m HIGH     _add_listener_ 
     109696       18.6m HIGH     _add_listener_ 
     109697       18.6m HIGH     _add_listener_ 
     109701       18.3m HIGH     _add_listener_ 
     109703       18.2m HIGH     _add_listener_ 
     109707       17.9m HIGH     _add_listener_ 
     109706       17.9m HIGH     _add_listener_ 
     109711       17.7m HIGH     _add_listener_ 
     109710       17.7m HIGH     _add_listener_ 
     109716       17.4m HIGH     _add_listener_ 
     109717       17.4m HIGH     _add_listener_ 
     109720       17.1m HIGH     _add_listener_ 
     109721       17.1m HIGH     _add_listener_ 
     109725       16.7m HIGH     _add_listener_ 
     109726       16.7m HIGH     _add_listener_ 
     109731       16.6m HIGH     _add_listener_ 
     109730       16.6m HIGH     _add_listener_ 
     109736       16.1m HIGH     _add_listener_ 
     109735       16.1m HIGH     _add_listener_ 
     109737         16m HIGH     _add_listener_ 
     109739         16m HIGH     _add_listener_ 
     109748       15.6m HIGH     _add_listener_ 
     109749       15.6m HIGH     _add_listener_ 
     109751       15.4m HIGH     _add_listener_ 
     109750       15.5m HIGH     _add_listener_ 
     109759         15m HIGH     _add_listener_ 
     109760         15m HIGH     _add_listener_ 
     109758         15m HIGH     _add_listener_ 
     109764       14.9m HIGH     _add_listener_ 
     109771       14.4m HIGH     _add_listener_ 
     109769       14.4m HIGH     _add_listener_ 
     109770       14.4m HIGH     _add_listener_ 
     109774       14.3m HIGH     _add_listener_ 
     109781       13.8m HIGH     _add_listener_ 
     109783       13.7m HIGH     _add_listener_ 
     109782       13.8m HIGH     _add_listener_ 
     109780       13.8m HIGH     _add_listener_ 
     109791       13.2m HIGH     _add_listener_ 
     109792       13.2m HIGH     _add_listener_ 
     109790       13.2m HIGH     _add_listener_ 
     109796       13.2m HIGH     _add_listener_ 
     109805       12.6m HIGH     _add_listener_ 
     109806       12.6m HIGH     _add_listener_ 
     109803       12.6m HIGH     _add_listener_ 
     109804       12.6m HIGH     _add_listener_ 
     109815       12.1m HIGH     _add_listener_ 
     109816       12.1m HIGH     _add_listener_ 
     109817       12.1m HIGH     _add_listener_ 
     109818       12.1m HIGH     _add_listener_ 
     109826       11.5m HIGH     _add_listener_ 
     109827       11.5m HIGH     _add_listener_ 
     109824       11.5m HIGH     _add_listener_ 
     109825       11.5m HIGH     _add_listener_ 
     109837       10.9m HIGH     _add_listener_ 
     109838       10.9m HIGH     _add_listener_ 
     109839       10.9m HIGH     _add_listener_ 
     109836       10.9m HIGH     _add_listener_ 
     109847       10.3m HIGH     _add_listener_ 
     109844       10.4m HIGH     _add_listener_ 
     109845       10.4m HIGH     _add_listener_ 
     109846       10.4m HIGH     _add_listener_ 
     109861        9.8m HIGH     _add_listener_ 
     109859        9.8m HIGH     _add_listener_ 
     109860        9.8m HIGH     _add_listener_ 
     109862        9.7m HIGH     _add_listener_ 
     109870        9.2m HIGH     _add_listener_ 
     109871        9.2m HIGH     _add_listener_ 
     109869        9.2m HIGH     _add_listener_ 
     109875          9m HIGH     _add_listener_ 
     109889        8.1m HIGH     _add_listener_ 
     109881        8.6m HIGH     _add_listener_ 
     109879        8.6m HIGH     _add_listener_ 
     109880        8.6m HIGH     _add_listener_ 
     109885        8.4m HIGH     _add_listener_ 
     109890        8.1m HIGH     _add_listener_ 
     109888        8.1m HIGH     _add_listener_ 
     109894        7.8m HIGH     _add_listener_ 
     109905        7.3m HIGH     _add_listener_ 
     109902        7.5m HIGH     _add_listener_ 
     109903        7.5m HIGH     _add_listener_ 
     109901        7.5m HIGH     _add_listener_ 
     109916        6.7m HIGH     _add_listener_ 
     109909        6.9m HIGH     _add_listener_ 
     109910        6.9m HIGH     _add_listener_ 
     109911        6.9m HIGH     _add_listener_ 
     109922        6.3m HIGH     _add_listener_ 
     109923        6.3m HIGH     _add_listener_ 
     109924        6.3m HIGH     _add_listener_ 
     109927        6.2m HIGH     _add_listener_ 
     109937        5.5m HIGH     _add_listener_ 
     109933        5.8m HIGH     _add_listener_ 
     109931        5.8m HIGH     _add_listener_ 
     109932        5.8m HIGH     _add_listener_ 
     109942        5.3m HIGH     _add_listener_ 
     109943        5.3m HIGH     _add_listener_ 
     109941        5.3m HIGH     _add_listener_ 
     109946          5m HIGH     _add_listener_ 
     109959        4.3m HIGH     _add_listener_ 
     109952        4.7m HIGH     _add_listener_ 
     109953        4.7m HIGH     _add_listener_ 
     109954        4.7m HIGH     _add_listener_ 
     109963        4.2m HIGH     _add_listener_ 
     109964        4.2m HIGH     _add_listener_ 
     109965        4.2m HIGH     _add_listener_ 
     109970        3.8m HIGH     _add_listener_ 
     109975        3.6m HIGH     _add_listener_ 
     109973        3.6m HIGH     _add_listener_ 
     109974        3.6m HIGH     _add_listener_ 
     109981        3.3m HIGH     _add_listener_ 
     109985          3m HIGH     _add_listener_ 
     109986          3m HIGH     _add_listener_ 
     109987          3m HIGH     _add_listener_ 
     109991        2.7m HIGH     _add_listener_ 
     109999        2.4m HIGH     _add_listener_ 
     109995        2.5m HIGH     _add_listener_ 
     110000        2.4m HIGH     _add_listener_ 
     110015        1.5m HIGH     _add_listener_ 
     110003        2.1m HIGH     _add_listener_ 
     110008          2m HIGH     _add_listener_ 
     110011        1.8m HIGH     _add_listener_ 
     110012        1.8m HIGH     _add_listener_ 
     110016        1.4m HIGH     _add_listener_ 
     110020        1.2m HIGH     _add_listener_ 
     110019        1.2m HIGH     _add_listener_ 
     110025       55.6s HIGH     _add_listener_ 
     110023          1m HIGH     _add_listener_ 
     110029       42.4s HIGH     _add_listener_ 
     110030       42.4s HIGH     _add_listener_ 
     110034       26.7s HIGH     _add_listener_ 
     110036       20.4s HIGH     _add_listener_ 
     109015       57.4m HIGH     _add_listener_ 
     109019       57.1m HIGH     _add_listener_ 
     108509        1.4h HIGH     _add_listener_ 
     110043        7.4s HIGH     _add_listener_ 
     110042        7.4s HIGH     _add_listener_ 
</description><key id="151646323">18044</key><summary>/_cat/pending_tasks presents many tasks with _add_listener source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awislowski</reporter><labels /><created>2016-04-28T13:50:06Z</created><updated>2016-04-28T13:55:58Z</updated><resolved>2016-04-28T13:55:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="awislowski" created="2016-04-28T13:55:57Z" id="215431152">I've moved question to:

https://discuss.elastic.co/t/-cat-pending-tasks-presents-many-tasks-with--add-listener-source/48683

It will be a better place I think :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex API allow op_type delete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18043</link><project id="" key="" /><description>Would be nice if we the reindex api also should support delete as op_type, 
think it uses the bulk api internally so would only require a few adjustments.. 
{"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"opType [delete] not allowed, either [index] or [create] are allowed"}],"type":"illegal_argument_exception","reason":"opType [delete] not allowed, either [index] or [create] are allowed"},"status":400}

Now I still need to use the npm package elasticdump for these kind of operations, 
if delete was also present I could remove this dependency
</description><key id="151645928">18043</key><summary>Reindex API allow op_type delete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">jrots</reporter><labels><label>:Reindex API</label><label>enhancement</label></labels><created>2016-04-28T13:48:19Z</created><updated>2016-06-06T09:22:09Z</updated><resolved>2016-06-06T09:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-28T14:14:20Z" id="215436768">&gt; bulk api internally so would only require a few adjustments

The whole `op_type` thing is an attempt to mimic the `_update` API. This is still certainly possible. Some folks brought it up when I started work on the `_reindex` API and I said "I'll do it after index and create are done" and I never did.

I'm busy doing other stuff and will probably try and work on "reindex from a remote cluster" before I pick this up, but I'll get it eventually. On the other hand, if anyone is excited about it and wants to work on it I'd be happy to review a PR and/or help anyone get started on reindex's code.
</comment><comment author="nik9000" created="2016-04-28T17:40:15Z" id="215505924">Depending on how it is done, this might be the same thing as #16883. Or, rather, it might make sense to close both issues with the same PR because they are so similar.
</comment><comment author="tlrx" created="2016-05-02T13:30:16Z" id="216234595">I gave this a look and I think we should remove the current delete-by-query plugin and reimplement the delete-by-query feature using the reindex infrastructure. We could benefit of throttling, maybe scripting too, and the response format will be similar to reindex and update-by-query. I think it's a matter of creating few classes and mutualize/move around some code. 

I don't really know why we chose to implement reindex/update-by-query as a module (because of script usage?) but I think it make sense to have everything at the same place... update-by-query and delete-by-query share the same trade-offs in term of conflict resolution. I'll be happy to do it anyway :)
</comment><comment author="nik9000" created="2016-05-02T13:38:25Z" id="216237129">&gt; why we chose to implement reindex/update-by-query as a module

I implemented it as a plugin at first because I wasn't sure how long it was going to take. I grew to love it being a plugin because:
1. I can be sure that nothing depends on it. It is a "dead end" as far as code goes. That makes it easier to reason about.
2. When you change only it you can usually just run a small-ish number of tests. This is kind of a result of point 1 but it still lovely.
3. It serves as an example of how you can add fairly extensive functionality in a plugin. You can point people at it and say "plugins can make whole new actions and stuff!" This is a bit of a cheat because I changed core to make writing the plugin easier, but it is still nice!

So when it came time to "merge" it to core I just made it into a module so it was shipped with core by default and still super self contained.

I think if you wanted to move delete-by-query to reindex's infrastructure you'd just implement it in the reindex module. It really isn't conceptually any different from reindex.

BTW, I'd be really excited if `_delete_by_query` just ended up being the same thing as `_update_by_query` with `op_type` set to delete. Reindex could reuse that! It'd be cool! I think.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get (mostly) rid of random test data generator for sort tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18042</link><project id="" key="" /><description>Inspired by an earlier review comment by @cbuescher this gets rid of most of the data generation code specific to sort testing in favour of re-using code available in ESTestCase by now.
</description><key id="151642548">18042</key><summary>Get (mostly) rid of random test data generator for sort tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-04-28T13:35:07Z</created><updated>2016-05-04T11:15:49Z</updated><resolved>2016-05-04T11:15:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-28T14:28:04Z" id="215443911">I like this cleanup, I just think we should only use the random-except-some-value cases mostly in the mutate() methods, for the general creation of the randomized builder I think it makes things harder to read without adding much.
</comment><comment author="MaineC" created="2016-05-02T13:43:35Z" id="216238228">@cbuescher @nik9000 Thanks for the feedback. Greatly appreciated - to me things look more readable now than before.

Tried to address all your comments, hopefully I did catch everything.
</comment><comment author="MaineC" created="2016-05-04T09:27:39Z" id="216809862">@cbuescher @nik9000 Thanks for your round of comments, as a result I realized I could get rid of the helper class for generating nested query builders altogether. Hopefully you like that change as well.
</comment><comment author="cbuescher" created="2016-05-04T09:48:56Z" id="216814735">@MaineC LGTM, I left a bunch of nitpicks where I think we can leave out the ESTestCase prefix when calling the helper methods, I don't mind too much if they are in there but they should be easy to remove.
</comment><comment author="MaineC" created="2016-05-04T10:15:33Z" id="216820983">Thanks for catching those - removed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase default max open files to 65536</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18041</link><project id="" key="" /><description>This commit increases the default number of max open files from 65535 to
65536 to be consistent with the bootstrap file descriptor warning.

Closes #18040 
</description><key id="151641021">18041</key><summary>Increase default max open files to 65536</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.4.0</label></labels><created>2016-04-28T13:28:15Z</created><updated>2016-04-29T16:17:38Z</updated><resolved>2016-04-29T12:32:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-28T14:22:33Z" id="215441043">LGTM. Is this a thing you need to do in master and can it be tested with the vagrant tests?
</comment><comment author="jasontedor" created="2016-04-28T14:25:19Z" id="215442723">&gt; Is this a thing you need to do in master and can it be tested with the vagrant tests?

It was already fixed in master but was never backported. I tried to cherry-pick the commit but it didn't apply cleanly so thought that it should go through review.
</comment><comment author="nik9000" created="2016-04-28T14:27:23Z" id="215443668">&gt; I tried to cherry-pick the commit but it didn't apply cleanly so thought that it should go through review.

I'm not surprised! Anyway, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian package/config file descriptor count inconsistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18040</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: openjdk-7-jre-headless:amd64   7u101-2.6.6-1~deb8u1

**OS version**: Debian 8 64bit

**Description of the problem including expected versus actual behaviour**:
The number of file descriptors set by the Debian systemd service file mismatches Elasticsearches' startup check triggering a WARNing.

**Steps to reproduce**:
1. Install elasticsearch package on Debian 8
2. Start elasticsearch
3. Watch the elasticsearch logfile

**Provide logs (if relevant)**:
[2016-04-28 13:03:53,452][WARN ][env                      ] [node-1] max file descriptors [65535] for elasticsearch process likely too low, consider increasing to at least [65536]
</description><key id="151637684">18040</key><summary>Debian package/config file descriptor count inconsistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abraxxa</reporter><labels /><created>2016-04-28T13:12:15Z</created><updated>2016-04-29T12:38:16Z</updated><resolved>2016-04-29T12:38:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-28T13:18:52Z" id="215420134">Duplicates #17430 and #17468, but the fix in #17431 was only applied to master and never backported. I will backport.
</comment><comment author="jasontedor" created="2016-04-28T13:28:31Z" id="215422577">The fix did not backport cleanly so the fix should go through proper review. I opened #18041.
</comment><comment author="abraxxa" created="2016-04-28T14:04:44Z" id="215433752">Sorry for opening another issue, I only searched the open ones.
Thanks for the fix!
</comment><comment author="jasontedor" created="2016-04-29T12:38:11Z" id="215699956">Closed by #18041
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build is buggy for java 9.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18039</link><project id="" key="" /><description>Try compiling with java 9:

First you will hit this. This can be fixed by using `assertEquals`, which also simplifies the test.

```
/home/rmuir/workspace/elasticsearch/core/src/test/java/org/elasticsearch/threadpool/ScalingThreadPoolTests.java:180: error: method invoked with incorrect number of arguments; expected 2, found 0
            assertThat(stats(threadPool, threadPoolName).getThreads(), equalTo(128));
                      ^
/home/rmuir/workspace/elasticsearch/core/src/test/java/org/elasticsearch/threadpool/ScalingThreadPoolTests.java:196: error: method invoked with incorrect number of arguments; expected 2, found 0
            assertThat(stats(threadPool, threadPoolName).getCompleted(), equalTo(128L));
                      ^
```

Next problem is that I am unsure `-source/-target` settings are being passed at all. Because I see class files referencing java 9 StringConcatFactory.

Next problem is that forbidden-apis screams, presumably because its being run in the wrong JVM (not the JAVA_HOME that is set, but the java 8 one running gradle itself). It sees StringConcatFactory and fails the build:

```
Caused by: org.gradle.internal.UncheckedException: de.thetaphi.forbiddenapis.ForbiddenApiException: Check for forbidden API calls failed: java.lang.ClassNotFoundException: java.lang.invoke.StringConcatFactory
    at org.gradle.internal.UncheckedException.throwAsUncheckedException(UncheckedException.java:45)
    at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:78)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:227)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:220)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:209)
    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:585)
    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:568)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:80)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:61)
    ... 68 more
Caused by: de.thetaphi.forbiddenapis.ForbiddenApiException: Check for forbidden API calls failed: java.lang.ClassNotFoundException: java.lang.invoke.StringConcatFactory
    at de.thetaphi.forbiddenapis.Checker.run(Checker.java:550)
    at de.thetaphi.forbiddenapis.gradle.CheckForbiddenApis.checkForbidden(CheckForbiddenApis.java:551)
    at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75)
    ... 75 more
Caused by: java.lang.ClassNotFoundException: java.lang.invoke.StringConcatFactory
    at de.thetaphi.forbiddenapis.Checker.getClassFromClassLoader(Checker.java:264)
    at de.thetaphi.forbiddenapis.Checker.lookupRelatedClass(Checker.java:277)
    at de.thetaphi.forbiddenapis.ClassScanner$2.checkMethodAccess(ClassScanner.java:349)
    at de.thetaphi.forbiddenapis.ClassScanner$2.checkHandle(ClassScanner.java:411)
    at de.thetaphi.forbiddenapis.ClassScanner$2.visitInvokeDynamicInsn(ClassScanner.java:497)
    at de.thetaphi.forbiddenapis.asm.ClassReader.a(Unknown Source)
    at de.thetaphi.forbiddenapis.asm.ClassReader.b(Unknown Source)
    at de.thetaphi.forbiddenapis.asm.ClassReader.accept(Unknown Source)
    at de.thetaphi.forbiddenapis.asm.ClassReader.accept(Unknown Source)
    at de.thetaphi.forbiddenapis.Checker.checkClass(Checker.java:528)
    at de.thetaphi.forbiddenapis.Checker.run(Checker.java:545)
    ... 77 more


BUILD FAILED
```
</description><key id="151636081">18039</key><summary>Build is buggy for java 9.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>build</label></labels><created>2016-04-28T13:04:42Z</created><updated>2017-03-21T14:07:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-04-28T13:10:36Z" id="215418034">I think the bug is plain simple:
- You do not pass -source/-target. This causes the source code compiled with Java 9 's javac emitting Java 9 bytecode that refers to Java 9's StringConcatFactory. In fact the code won't run with Java 8
- Forbidden fails because it runs in Gradle's JVM. In fact it is right: The code won't run on Java 8 at all.

So I think the compiler is wrong: It should also pass source/target 8 to make javac only use Java 8 source format and produce Java 8 class files.
</comment><comment author="rmuir" created="2016-04-28T13:13:12Z" id="215418669">Yeah, if i just `grep` for StringConcatFactory, it matches tons of class files. So either its not set correctly in the build, or its being ignored.
</comment><comment author="uschindler" created="2016-04-28T14:56:32Z" id="215452913">It is quite clear: I was not aware that the Groovy build runs under JDK 8 and just invokes the compiler of Java 9. If the java 9 compiler is not configured to compile class files for Java 8 (target is 1.8), it will use StringConcatFactory.

## As the Groovy build runs under jdk 8 and loads forbiddenApis in its own JVM, it cannot find this class. In fact, it found the bug in the class output by javac, which produces wrong class files.

Uwe Schindler
H.-H.-Meier-Allee 63, 28213 Bremen
http://www.thetaphi.de
</comment><comment author="rjernst" created="2016-04-28T15:01:38Z" id="215454556">We do actually tell gradle what the target and source compatibility are. However, gradle has a check that breaks since when compiling with java 9, gradle is running under java 8, and javac is running with java 9.  This check needs to be simple, no condition on the trying to be "smart" and omit the compatibility arg:

```
if (targetCompatibility != null &amp;&amp; !JavaVersion.current().equals(JavaVersion.toVersion(targetCompatibility))) {
            args.add("-target");
            args.add(targetCompatibility);
        }
```
</comment><comment author="rmuir" created="2016-04-28T16:38:40Z" id="215488628">I'm reopening because things still don't work. I see this now:

```
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.12
  OS Info               : Linux 4.2.0-30-generic (amd64)
  JDK Version (gradle)  : Oracle Corporation 1.8.0_45 [Java HotSpot(TM) 64-Bit Server VM 25.45-b02]
  JAVA_HOME (gradle)    : /usr/local/jdk1.8.0_45
  JDK Version (compile) : Oracle Corporation 9-ea [Java HotSpot(TM) 64-Bit Server VM 9-ea+115]
  JAVA_HOME (compile)   : /home/rmuir/Downloads/jdk-9
:core:compileJava
warning: [options] bootstrap class path not set in conjunction with -source 1.8
error: warnings found and -Werror specified
1 error
1 warning
:core:compileJava FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':core:compileJava'.
&gt; Compilation failed with exit code 1; see the compiler error output for details.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 16.944 secs
```

Additionally, we still need to deal with the compiler error in the test class at least too.
</comment><comment author="uschindler" created="2016-04-28T17:43:23Z" id="215506793">Can we please open a bug @ Gradle. This is total BULLSHIT! Why should it be smart at all? There is no reason! If one configures version "1.8" it should just pass it down to javac no matter which version it is. The good thing is that javac also complains if it is too old for the version you configured.

Total sonsense. The more I look into Gradle, the more it annoys me.

Just be safe and always pass -source and -target as configured! PERIOD.
</comment><comment author="rjernst" created="2016-04-28T17:53:27Z" id="215509594">Yes, I plan to open a bug (and fix) with gradle.
</comment><comment author="rmuir" created="2016-04-28T17:56:39Z" id="215510495">to be fair, i only tested with Gradle 2.12, but 2.13 is now released. the last time i had issues with java 9 stuff the newer releases seemed to have some fixes, so we may want to see if it happens on 2.13.
</comment><comment author="rjernst" created="2016-04-28T18:01:14Z" id="215511797">&gt;  so we may want to see if it happens on 2.13

I have gradle checked out, the code i pasted is from tip of master. So the problem still exists. :/
</comment><comment author="rjernst" created="2016-04-28T18:15:02Z" id="215515774">I created a gradle discuss thread here (they don't want users creating jiras anymore I guess):
https://discuss.gradle.org/t/source-and-target-are-only-passed-to-javac-when-java-version-is-different-than-gradle-java-version/17235
</comment><comment author="rjernst" created="2016-04-28T18:29:44Z" id="215520121">And I opened a PR: gradle/gradle#627
</comment><comment author="uschindler" created="2016-04-28T20:00:46Z" id="215544820">Other thing: Does anybody know why the test code fails to compile with Java 9? Robert may say: "this is too complicated code" but actually this seems to be a javac bug? So report this to http://bugs.java.com/ !
</comment><comment author="uschindler" created="2016-04-28T20:48:08Z" id="215557629">@rmuir Isn't the new one just a duplicate of #13448?
</comment><comment author="rjernst" created="2016-04-28T23:21:17Z" id="215590826">I was not able to create a minimal reproduction for the test compile failure. I will switch it to assertEquals as @rmuir suggested.
</comment><comment author="abhi8642" created="2016-04-29T17:25:03Z" id="215822225">try clean and build option instead of build.
</comment><comment author="rmuir" created="2016-05-01T14:16:52Z" id="216044353">&gt; Other thing: Does anybody know why the test code fails to compile with Java 9? Robert may say: "this is too complicated code" but actually this seems to be a javac bug? So report this to http://bugs.java.com/ !

I was hoping a couple javac bugs would put a dent in hamcrest matcher usage :)

In all seriousness, what causes things like `assertThat(x, equalTo(128))` and `assertTrue("x should be equal to 128 but isn't", x.equals(128))`. I know in the latter case, some features of the intellij IDE were responsible. almost as bad as the eclipse auto-generated "try/catch". 

Maybe we should ban this equalTo() method? 
</comment><comment author="uschindler" created="2016-05-01T16:02:47Z" id="216051949">I agree, the hamcrest  stuff is only useful sometimes for more complicated matchers, but just comparing for equals should be done with assetEquals and not hamcrest or assetTrue.

I just said that we should report this bug to Oracle, because it could affect other code, too. It is clearly a bug if it fails to compile. Similar to the 8u20 problems with final vars in lambdas.

Unfortunately Ryan did not find a reproducer on isolation :(
</comment><comment author="jasontedor" created="2016-05-04T18:35:28Z" id="216959465">&gt; Maybe we should ban this equalTo() method?

I think that `equalTo` is more readable than `assertEquals`. I also find it to be more flexible (e.g., `assertThat(s, anyOf(equalTo("x"), equalTo("y"))` instead of `assertTrue(s != null &amp;&amp; (s.equals("x") || s.equals("y")))` because the latter gives a useless assertion message).

Since this is a subjective assessment, I think that banning `equalTo` is a bridge too far.
</comment><comment author="rmuir" created="2016-05-06T22:25:12Z" id="217578419">&gt; I also find it to be more flexible

This very flexibility is what i hate. Why make complex assertions like that? Keep it simple.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo: Default it `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18038</link><project id="" key="" /><description>Sentence:
**Default it `true`.**
was replaced with
**Defaults to `true`.**
</description><key id="151621855">18038</key><summary>Typo: Default it `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rafalbig</reporter><labels><label>docs</label></labels><created>2016-04-28T11:50:11Z</created><updated>2016-04-29T10:59:27Z</updated><resolved>2016-04-29T10:59:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T10:59:27Z" id="215684271">thanks @rafalbig 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Corrected invalid json sample</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18037</link><project id="" key="" /><description>v Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
v Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
</description><key id="151614087">18037</key><summary>Corrected invalid json sample</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nl5887</reporter><labels><label>docs</label></labels><created>2016-04-28T11:13:52Z</created><updated>2016-04-29T10:56:07Z</updated><resolved>2016-04-29T10:55:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T10:56:07Z" id="215683792">thanks @nl5887 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduces GeoValidationMethod to GeoDistanceSortBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18036</link><project id="" key="" /><description>Previously like in other geo related query parsers we were using
a combination of two booleans for coerce and ignore_malformed
which was error prone and not very clear.

Switched to using GeoValidationMethod instead as we already do
e.g. in GeoBoundingBoxQueryBuilder.

Left support for both, coerce and ignore_malformed in the parser
but deprecated the two in favour of validation method.

Introduced the same deprecation in geo bounding box query builder.

@cbuescher care to take a brief look?
</description><key id="151600500">18036</key><summary>Introduces GeoValidationMethod to GeoDistanceSortBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-28T10:14:47Z</created><updated>2016-05-04T11:46:07Z</updated><resolved>2016-05-04T11:46:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-28T10:42:32Z" id="215383659">@MaineC thanks, looks good. Could you also check the documentation for geo distance sort and geo bounding box and add deprecation notices there? And maybe add some info to the 5.0 migration notes about the removed setters in the java api. 
</comment><comment author="MaineC" created="2016-04-28T12:30:24Z" id="215408820">@cbuescher Updated according to your comments. I found a couple more classes where we support both, coerce/ignore_malformed and validation_method. Adjusted those as well, but kept things in separate commits, maybe that helps the review.
</comment><comment author="cbuescher" created="2016-04-29T12:20:25Z" id="215696779">@MaineC I took another look, the one thing I that needs some final consideration is whether we should include deprecation information to the docs (not only the migration doc) for the fields that are replaces by `validation_method`  for all the affected queries.
</comment><comment author="MaineC" created="2016-05-04T10:06:04Z" id="216819203">Added deprecation warnings to docs as needed.
</comment><comment author="cbuescher" created="2016-05-04T10:29:57Z" id="216823467">@MaineC LGTM, I left one comment but the method in question will most likely go away when #18042 is merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighter tags extraneous terms from an array when used with match_phrase query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18035</link><project id="" key="" /><description>**Elasticsearch version**:
2.3.1

**Description of the problem including expected versus actual behavior**:
This is similar to https://github.com/elastic/elasticsearch/issues/15291 but still happens with version 2.3.1 on slightly different circumstances. In the previous issue the field was a string, now it's an array.

```
POST /test_index
{
   "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 0
   },
       "mappings" : {
        "file" : { }
        }
    }
}

POST /test_index/file
{
   "texts": [
      "quick brown fox here.",
      "just a fox"
   ]
}

POST /test_index/file/_search
{
   "query": {
      "match_phrase": {
         "text": "quick brown fox"
      }
   },
   "highlight": {
      "fields": {
         "text": {
            "number_of_fragments": 0
         }
      }
   }
}
```

I expect to get a single highlighted fragment from the first string in the array, but instead I get

```
   "hits" : [ {                                                                                 
     "_index" : "test_index",                                                                   
     "_type" : "file",                                                                          
     "_id" : "AVRcVPVIk4eDYtUZXTb8",                                                            
     "_score" : 0.34520942,                                                                     
     "_source" : {                                                                              
       "texts" : [ "quick brown fox here.", "just a fox" ]                                      
     },                                                                                         
     "highlight" : {                                                                            
       "texts" : [ "&lt;em&gt;quick&lt;/em&gt; &lt;em&gt;brown&lt;/em&gt; &lt;em&gt;fox&lt;/em&gt; here.", "just a &lt;em&gt;fox&lt;/em&gt;" ]  
     }                                                                                          
   } ]                                                                                          
```

The separate fox is also highlighted even though it doesn't actually match.
</description><key id="151600032">18035</key><summary>Highlighter tags extraneous terms from an array when used with match_phrase query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daniellandau</reporter><labels /><created>2016-04-28T10:12:04Z</created><updated>2017-03-17T16:40:39Z</updated><resolved>2016-04-29T09:00:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T09:00:42Z" id="215663366">Duplicate of https://github.com/elastic/elasticsearch/issues/17848
</comment><comment author="ashitpupu" created="2017-03-17T16:40:39Z" id="287406610">@clintongormley I am seeing the same issue in ES 2.3.3. How to fix this issue ???
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtered Query Match on Document Contents Not Working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18034</link><project id="" key="" /><description>Hello guys,

Following below a link with the steps / setup / etc with what I am trying to achieve with no success:
https://discuss.elastic.co/t/unable-to-find-document-searching-contents-mapper-attachments-plugin/48500

Thank you very much in advance.

Cheers,
Gui
</description><key id="151595397">18034</key><summary>Filtered Query Match on Document Contents Not Working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">guikeller</reporter><labels /><created>2016-04-28T09:49:21Z</created><updated>2016-04-28T09:55:10Z</updated><resolved>2016-04-28T09:55:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-28T09:55:10Z" id="215373129">Please keep the discussion on discuss unless we found out it's an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>List custom thread pools in _cat/thread_pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18033</link><project id="" key="" /><description>**Describe the feature**:

It is easy to create a custom thread pool in elasticsearch.yml, e.g.:

```
threadpool.asdf.type: fixed
threadpool.asdf.size: 10
```

But this thread pool is not listed, when you use `_cat/thread_pool?v=asdf` command, because only `SUPPORTED_NAMES` can be listed in `org.elasticsearch.rest.action.cat.RestThreadPoolAction#getTableWithHeader`
</description><key id="151584108">18033</key><summary>List custom thread pools in _cat/thread_pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avlukanin</reporter><labels><label>:Internal</label><label>stalled</label></labels><created>2016-04-28T08:55:17Z</created><updated>2016-04-29T09:40:39Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T09:17:08Z" id="215666162">We've been talking about adding a class which is responsible for creating threads.  Any other class that needs a thread pool should register its requirements with this class, and they should be available in the stats.

Related to #17915 and #12666
</comment><comment author="avlukanin" created="2016-04-29T09:40:39Z" id="215670378">The stats is already collected for custom thread pools, but not shown. I had to patch `RestThreadPoolAction` to add my custom thread pool name to `SUPPORTED_NAMES` and `SUPPORTED_ALIASES` to show it.

Can you confirm, that the stats for custom thread pools will not only be collected, but shown as well, when the new paradigm is implemented?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Java] Mutil-field terms aggregation fetch the result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18032</link><project id="" key="" /><description>Elasticsearch 2.3.0
I want to get the result aggregations use Java. My expected Json result is [{"by_from:{"nearme":xxx,"xiaomi":xxx,"huaweisf":xxx,"qq":xxx}"},{"by_sdkver":{"3.1.4":xxx,"3.0.6":xxx,"3.1.1":xxx}}]. When I use the signal field(by_from) terms aggregation , that is success. But when I use the mutil-field ,like by_from and by_sdkver, the result is so amazing, that is ,{"by_from":{"3.1.4":xxx,"3.0.6":xxx,"3.1.1":xxx}},{"by_sdkver":{"3.1.4":xxx,"3.0.6":xxx,"3.1.1":xxx}} , "by_sdk" and "by_from" have the same content ! 
The trouble is that I wanna to get the detail "by_from" keys,like "nearme","xiaomi".. I don't know why the result is that. If I want get the expected result ,how should I do? 
Mutil-field terms aggregation result:
{
  "aggregations" : {
    "by_from" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 175,
      "buckets" : [ {
        "key" : "nearme",
        "doc_count" : 514,
        "by_sdkver" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "3.1.4",
            "doc_count" : 492
          }, {
            "key" : "3.0.6",
            "doc_count" : 20
          }, {
            "key" : "3.1.1",
            "doc_count" : 2
          } ]
        }
      }, {
        "key" : "xiaomi",
        "doc_count" : 426,
        "by_sdkver" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "3.1.4",
            "doc_count" : 410
          }, {
            "key" : "3.1.1",
            "doc_count" : 11
          }, {
            "key" : "3.0.6",
            "doc_count" : 5
          } ]
        }
      }, {
        "key" : "huaweisf",
        "doc_count" : 202,
        "by_sdkver" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "3.1.4",
            "doc_count" : 202
          } ]
        }
      }, {
        "key" : "qq",
        "doc_count" : 167,
        "by_sdkver" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "3.1.4",
            "doc_count" : 130
          }, {
            "key" : "3.0.6",
            "doc_count" : 35
          }, {
            "key" : "3.1.1",
            "doc_count" : 2
          } ]
        }
      }]
    }
  }
}
</description><key id="151583215">18032</key><summary>[Java] Mutil-field terms aggregation fetch the result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">singledoublemonkey</reporter><labels /><created>2016-04-28T08:50:29Z</created><updated>2016-04-29T09:08:06Z</updated><resolved>2016-04-29T08:56:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T08:56:48Z" id="215662709">Hi @singledoublemonkey 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="singledoublemonkey" created="2016-04-29T09:08:06Z" id="215664641">@clintongormley thanks for your reminder
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make significant terms work on fields that are indexed with points.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18031</link><project id="" key="" /><description>It will keep using the caching terms enum for keyword/text fields and falls back
to IndexSearcher.count for fields that do not use the inverted index for
searching (such as numbers and ip addresses). Note that this probably means that
significant terms aggregations on these fields will be less efficient than they
used to be. It should be ok under a sampler aggregation though.

This moves tests back to the state they were in before numbers started using
points, and also adds a new test that significant terms aggs fail if a field is
not indexed.

In the long term, we might want to follow the approach that Robert initially
proposed that consists in collecting all documents from the background filter in
order to compute frequencies using doc values. This would also mean that
significant terms aggregations do not require fields to be indexed anymore.
</description><key id="151579769">18031</key><summary>Make significant terms work on fields that are indexed with points.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-28T08:31:46Z</created><updated>2016-05-11T14:53:45Z</updated><resolved>2016-05-11T14:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2016-04-28T09:39:28Z" id="215368335">Many thanks for this, @jpountz .
I see this uses term queries to derive counts on the fly when they aren't stored. 

However if we go down this route of deriving counts on the fly what do you think of an approach where we try to re-use the aggregation framework  - so a broadening of "significant terms" into "significant buckets". That would give us the flexibility to do the following analysis:
- Individual terms (the limit of what we do today)
- Numeric ranges e.g. age bands
- Geo areas 
- Term groups (e.g. Java=[jsp,ejb,jts....] vs Javascript=[react,angular...] vs ..)

In each of these scenarios the significance algorithm is tuning out the uneven-ness in the data in order to identify buckets of interest e.g. those with a propensity to commit a crime or purchase a product.  We know so much of data is Zipfy and skewed towards the popular so background-diffing is a useful lens through which to view most data (e.g. in the same way "per-capita" stats help as a saner basis of comparisons).

There are several advantages to re-using aggs for the bulk of this work:
1) Bucket aggs provide a clean syntax for grouping low-level index entries (e.g. date agg's "1m", "1w" etc)
2) Metric aggs allow us to sum things like paymentAmount - this can be a more useful number to consider as a basis for computing significance than what we use today (just volumes of documents)

There's a lot to think about in adopting an agg-based approach - the JSON syntax, the changes to existing aggs to support this background-stats use case. I appreciate this is likely too much to debate here on this PR but I wanted to run it by you because it is related to the changes being made here.
</comment><comment author="jpountz" created="2016-04-28T14:06:17Z" id="215434273">I think that is worth exploring but this looks quite ambitious at the same time. We should try to do baby steps as much as possible. The first one would probably to be to compute doc counts of the background set using doc values (just like an aggregation would do).Then we could try to see how we can plug the terms agg in to do the job. And afterwards maybe supporting arbitrary numeric metric aggs.
</comment><comment author="markharwood" created="2016-05-03T11:33:38Z" id="216501771">Was trying with my weblog data to test performance but hit this array index out of bounds exception:

```
DELETE test

PUT test
{
   "settings": {
      "number_of_replicas": 0,
      "number_of_shards":1
   },
   "mappings": {
      "test": {                
         "properties": {
            "ipField": {
               "type": "ip"
            }
         }
      }
   }
}


POST test/test
{"ipField":"95.108.241.251"}
POST test/test
{"ipField":"95.108.241.251"}
POST test/test
{"ipField":"95.108.241.251"}
POST test/test
{"ipField":"95.108.241.251"}


GET test/_search?q=95.108.241.251
{
   "size":0,
   "aggs": {
      "boom": {
         "significant_terms": {
            "field": "ipField"
         }
      }
   }
}
```

Exception:

```
Failed to execute phase [merge], [reduce] 
    at org.elasticsearch.action.search.SearchQueryAndFetchAsyncAction$1.onFailure(SearchQueryAndFetchAsyncAction.java:77)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:438)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 16
    at org.apache.lucene.util.UnicodeUtil.UTF8toUTF16(UnicodeUtil.java:602)
    at org.apache.lucene.util.BytesRef.utf8ToString(BytesRef.java:152)
    at org.elasticsearch.search.aggregations.bucket.significant.SignificantStringTerms$Bucket.getKeyAsString(SignificantStringTerms.java:115)
    at org.elasticsearch.search.aggregations.bucket.significant.InternalSignificantTerms.doReduce(InternalSignificantTerms.java:176)
    at org.elasticsearch.search.aggregations.InternalAggregation.reduce(InternalAggregation.java:158)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:159)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:403)
    at org.elasticsearch.action.search.SearchQueryAndFetchAsyncAction$1.doRun(SearchQueryAndFetchAsyncAction.java:65)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:452)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)    
```
</comment><comment author="jpountz" created="2016-05-04T08:30:21Z" id="216783850">@markharwood I pushed a new commit to address your comment about the documentation.

Regarding the test failure you got, we need #18003 to make terms aggs work with ip addresses. The issue here is that elasticsearch tried to convert some ip bytes to an utf8 string, but this failed since the array boundary was crossed while reading what looked like a 4-bytes UTF8 code point.
</comment><comment author="markharwood" created="2016-05-04T14:57:22Z" id="216891462">For now I hacked SignificantStringTerms.getKeyAsString() so I can run some benchmarks without causing errors. There was a noticeable slow-down on IP address fields compared to the `keyword` equivalent on my test index (~4 times slower) but that is to be expected.
</comment><comment author="jpountz" created="2016-05-10T14:35:36Z" id="218176949">@markharwood Do you think this is good to merge or would you like me to change the way it works?
</comment><comment author="markharwood" created="2016-05-10T16:18:55Z" id="218210842">My only concern was around the likelihood of duplicated frequency lookups when a sig_term agg is embedded under a parent terms agg or similar. I experimented with adding a term-&gt;count cache on some weblog data but failed to get a noticeable improvement. There may already be some caching effects occurring at the Lucene or OS file system level that make this agg-level caching redundant?

Otherwise LGTM
</comment><comment author="jpountz" created="2016-05-10T16:37:26Z" id="218216046">There is caching happening indeed through the filesystem and the query cache. It is not as efficient as the terms enum though, which even if it did not cache, would still have the benefit of reusing `IndexInput`s.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add org.apache.lucene.search.MatchNoDocsQuery to forbiddenApi</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18030</link><project id="" key="" /><description>In #17780 we introduced our own MatchNoDocsQuery that comes with an explanation of why no documents were matched. We switched to using that one in #17981 

Turns out there is another MatchNoDocsQuery in Lucene itself. To avoid confusion over which one to use, should we add the Lucene one as forbiddenApi? Another thing I thought about - @jimferenczi would it make sense to work with the Lucene project to make the support for explanations available in their original MatchNoDocsQuery?
</description><key id="151578831">18030</key><summary>Add org.apache.lucene.search.MatchNoDocsQuery to forbiddenApi</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>low hanging fruit</label></labels><created>2016-04-28T08:26:43Z</created><updated>2016-10-17T17:15:10Z</updated><resolved>2016-10-10T19:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-05-06T09:29:50Z" id="217396451">Let's try to merge the MatchNoDocsQuery in Lucene directly. I'll open a ticket.
</comment><comment author="MaineC" created="2016-05-07T12:54:43Z" id="217633498">@jimferenczi Would be nice if you could post a link to said ticket here once it's open (didn't find it after searching for it in the Lucene issue tracker for 5min, possible I overlooked it).
</comment><comment author="jimczi" created="2016-05-09T07:47:40Z" id="217797572">That was my intention @MaineC:
https://issues.apache.org/jira/browse/LUCENE-7276
Sorry for the confusion.
</comment><comment author="pickypg" created="2016-08-17T22:18:28Z" id="240566720">FYI, the Lucene side got merged into Lucene 6.2.
</comment><comment author="jimczi" created="2016-10-10T19:42:51Z" id="252727010">Closing since org.elasticsearch.common.lucene.search.MatchNoDocsQuery has been removed and we now use the new Lucene version with the explanation inside (https://github.com/elastic/elasticsearch/commit/c80a563a71706a135fa478c4899b58697471f382)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>This one weird trick will make your query crash.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18029</link><project id="" key="" /><description>This fails on ES v2.3.2:

```
curl -XPUT http://127.0.0.1:9200/twitter/

curl http://127.0.0.1:9200/twitter/tweat/_search -d '
{
  "query": {
    "filtered": {
      "filter": {
        "terms": {
          "and": [
            {
              "terms": {
                "favorite_color": [
                  "green"
                ]
              }
            },
            {
              "not": {
                "and": [
                  {
                    "query": {
                      "terms": {
                        "favorite_color": ["blue"]
                      }
                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  }
}'
```

With the error:

`{"reason":"[_na] query malformed, must start with start_object"}`

The query works when any feature of the query is removed. For example, it works when the "not" is removed.
</description><key id="151533945">18029</key><summary>This one weird trick will make your query crash.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">matthuhiggins</reporter><labels /><created>2016-04-28T02:16:56Z</created><updated>2016-06-13T20:29:30Z</updated><resolved>2016-04-29T08:40:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T08:40:33Z" id="215659748">Love the clickbait.  Your query is malformed :)  Here:

```
  "filter": {
    "terms": {
      "and": [
```

The `terms` filter requires a field name
</comment><comment author="matthuhiggins" created="2016-04-29T16:07:44Z" id="215782364">Arg, I pasted the wrong query. Here is the distilled problem:

```
curl -XPUT http://127.0.0.1:9200/twitter/

# This query crashes with "query malformed, must start with start_object":
curl http://127.0.0.1:9200/twitter/tweat/_search -d '
{
  "query": {
    "filtered": {
      "filter": {
        "not": {
          "and": [
            {
              "terms": {
                "favorite_color": ["blue"]
              }
            }
          ]
        }
      }
    }
  }
}'

# Removing the "not" filter works
curl http://127.0.0.1:9200/twitter/tweat/_search -d '
{
  "query": {
    "filtered": {
      "filter": {
        "and": [
          {
            "terms": {
              "favorite_color": ["blue"]
            }
          }
        ]
      }
    }
  }
}'

# Keeping the "not" filter and removing the "and" filter works
curl http://127.0.0.1:9200/twitter/tweat/_search -d '
{
  "query": {
    "filtered": {
      "filter": {
        "not": {
          "terms": {
            "favorite_color": ["blue"]
          }
        }
      }
    }
  }
}'
```
</comment><comment author="clintongormley" created="2016-04-29T16:47:32Z" id="215803346">Not sure why, but in this case the `not` filter requires a `filter` parameter:

```
GET /_search
{
  "query": {
    "filtered": {
      "filter": {
        "not": {
          "filter": {
            "and": [
              {
                "terms": {
                  "favorite_color": [
                    "blue"
                  ]
                }
              }
            ]
          }
        }
      }
    }
  }
}
```

Either way, the `not` clause is deprecated and has been removed in 5.0.  Use a `bool.must_not` instead
</comment><comment author="matthuhiggins" created="2016-06-09T20:19:19Z" id="225013908">Is this the shortest way to write `NOT(city:Portland AND state:Oregon)`?:

```
{
  "bool": {
    "must_not": {
      "bool": {
        "must": [
          {
            "term": {
              "city": "Portland"
            }
          },
          {
            "term": {
              "state":"Oregon"
            }
          }
        ]
      }
    }
  }
}
```
</comment><comment author="s1monw" created="2016-06-10T07:28:27Z" id="225112376">```
{
    "query_string" : {
        "query" : "NOT(city:Portland AND state:Oregon)"
    }
}
```

works too :)
</comment><comment author="clintongormley" created="2016-06-13T18:07:24Z" id="225661558">~~You can use a single `bool`:~~~

No you can't :)
</comment><comment author="matthuhiggins" created="2016-06-13T20:27:38Z" id="225698067">Right. A single `bool.must_not` is equivalent to `NOT(city:Portland OR state:Oregon)`, which is the same as `NOT(city:Portland) AND NOT(state:Oregon)`. We have sorted out what to change in our application - only commenting here for the record.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stricter validation of Reindex's requests_per_second</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18028</link><project id="" key="" /><description>And fail any rethrottle requests that don't set requests_per_second.
</description><key id="151504156">18028</key><summary>Stricter validation of Reindex's requests_per_second</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T22:13:06Z</created><updated>2016-05-04T20:30:37Z</updated><resolved>2016-05-04T20:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-27T22:23:39Z" id="215249724">LGTM, thanks!
</comment><comment author="jasontedor" created="2016-04-27T22:29:21Z" id="215250774">Why are we introducing leniency like this in a relatively new feature?
</comment><comment author="dakrone" created="2016-04-27T22:30:55Z" id="215251086">&gt; Why are we introducing leniency like this in a relatively new feature?

That's a good point, I do agree with @jasontedor that throwing on any value other than -1, 0, or positive integer sounds best to me.

(Personally, I don't like using `0` as "unlimited", I prefer to use `-1` as "disabled throttling"), what do you think @nik9000?
</comment><comment author="jasontedor" created="2016-04-27T22:39:49Z" id="215253011">&gt; (Personally, I don't like using `0` as "unlimited", I prefer to use `-1` as "disabled throttling")

I'm not seeing why `requests_per_second` is required and we just treat missing as meaning do not throttle? I think that non-positive `requests_per_second` should be rejected.
</comment><comment author="dakrone" created="2016-04-27T22:58:41Z" id="215257851">&gt; I think that non-positive `requests_per_second` should be rejected.

I'd rather have a special signifier for throttling being disabled (ie, -1) than have people specifying `?requests_per_second=109283987192` to get around it, just like with `refresh_interval`, -1 is much cleaner than setting it to `283m`
</comment><comment author="nik9000" created="2016-04-27T23:04:24Z" id="215258855">&gt; I'm not seeing why requests_per_second is required and we just treat missing as meaning do not throttle? I think that non-positive requests_per_second should be rejected.

That isn't what happens. `requests_per_second` is required by the rethrottle action and it fails if you don't specify it. It used to just fail at the REST layer if you didn't send the url param. Now it does it at the java layer too.

`requests_per_second` isn't required for `_reindex` or `_update_by_query`. They default to 0 which means "unthrottled". I used 0 for that because 0 can't mean "stop". I picked 0 rather than -1 for that number because I wanted to make it clear that 0 didn't mean stop. @dakrone convinced me that everyone and their mother is going to use -1 to means "unthrottled" so I added an exception for that. This is an extension to that.

I can switch it so anything `&lt; 0` just gives back a 400 level error.
</comment><comment author="nik9000" created="2016-04-27T23:08:27Z" id="215259594">Also the word `unlimited` works for unlimited as well. I like that.
</comment><comment author="jasontedor" created="2016-04-27T23:12:03Z" id="215260181">&gt; `requests_per_second` is required by the rethrottle action and it fails if you don't specify it.

Just curious, why is it required?

&gt; Also the word `unlimited` works for unlimited as well. I like that.

Yes, this. But I also think that we should reject 0.
</comment><comment author="nik9000" created="2016-04-27T23:18:34Z" id="215261352">&gt; Yes, this. But I also think that we should reject 0.

Do you want me to reject 0 from the REST api and make it the only way to do it from the java API? Or should the java API do something totally different I haven't yet thought of?

&gt; Just curious, why is it required?

Because the only point of the rethrottle request is to change that value. If you are using that request you have some value in mind, maybe `unlimited`, but you have some value in mind. And you should pick it. And I shouldn't let you fat finger the parameter and just use some crazy default.
</comment><comment author="jasontedor" created="2016-04-27T23:27:54Z" id="215262961">&gt; Do you want me to reject 0 from the REST api and make it the only way to do it from the java API? Or should the java API do something totally different I haven't yet thought of?

`Float.POSTIVE_INFINITY`?

&gt; Because the only point of the rethrottle request is to change that value. If you are using that request you have some value in mind, maybe unlimited, but you have some value in mind. And you should pick it. And I shouldn't let you fat finger the parameter and just use some crazy default.

I'm convinced.
</comment><comment author="nik9000" created="2016-04-27T23:51:23Z" id="215266776">&gt; Float.POSTIVE_INFINITY?

Sold.
</comment><comment author="nik9000" created="2016-04-28T00:02:53Z" id="215268607">I pushed a commit that does most of the cutting over. Since this features hasn't actually be released (only in alpha) this isn't a breaking change. I need to add some more tests, specifically a round trip test for the rethrottle request - I botched that the first time around and I have no idea how tests didn't fail. But, yeah. And I need something that uses the throttle over the java api because, right now, the throttle stuff is only tested at the rest layer.
</comment><comment author="nik9000" created="2016-04-28T16:04:09Z" id="215478225">OK! I've pushed some more tests including an integration test and a unit test that would have noticed the broken serialization. I decided to do a long overdo refactoring of the base classes for the integration tests into one class. That made it easier to add the integration test.
</comment><comment author="nik9000" created="2016-04-28T16:26:48Z" id="215485360">Bleh - one last thing - I have to change the docs I think.
</comment><comment author="nik9000" created="2016-04-28T18:10:06Z" id="215514230">OK! I pushed a bunch of extra docs! I added docs for `_tasks/{task_id}/_cancel` and `_reindex/{task_id}/_rethottle`.
</comment><comment author="nik9000" created="2016-04-28T20:24:12Z" id="215551385">Note for later: change the xcontent representation of POSITIVE_INFINITY in the status.
</comment><comment author="nik9000" created="2016-04-29T12:13:28Z" id="215695636">@dakrone and @jasontedor I think this is ready for review again.
</comment><comment author="dakrone" created="2016-05-03T15:11:28Z" id="216559662">@nik9000 left two minor comments, are you going to backport this to 2.x also? (just curious)
</comment><comment author="dakrone" created="2016-05-03T15:11:44Z" id="216559748">Oh, LGTM also
</comment><comment author="nik9000" created="2016-05-03T15:13:06Z" id="216560181">&gt; @nik9000 left two minor comments, are you going to backport this to 2.x also? (just curious)

Yes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrap settings at 140 columns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18027</link><project id="" key="" /><description>and remove the checkstyle suppressions.
</description><key id="151499406">18027</key><summary>Wrap settings at 140 columns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T21:49:59Z</created><updated>2016-04-27T22:19:17Z</updated><resolved>2016-04-27T22:19:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-27T22:15:22Z" id="215248022">Left one comment, could probably bikeshed this to oblivion since everyone has their favorite multi-line indentation style, but LGTM
</comment><comment author="nik9000" created="2016-04-27T22:18:06Z" id="215248589">&gt; favorite multi-line indentation style

All of them are better then never wrapping the line at all. I'd love to hear about your favorites sometime because I don't really have a favorite.
</comment><comment author="nik9000" created="2016-04-27T22:19:17Z" id="215248841">Thanks for catching my mistake @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle case when readThrowable returns null.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18026</link><project id="" key="" /><description>If readThrowable returns null, assign the error variable a TransportSerializationException. It is important to assign a value, because null causes the handleException method to throw a NullPointerException and never call the handler.
</description><key id="151496422">18026</key><summary>Handle case when readThrowable returns null.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rculbertson</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-04-27T21:33:31Z</created><updated>2016-06-06T23:29:45Z</updated><resolved>2016-06-06T23:29:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-27T21:37:45Z" id="215237583">Is this fixing a particular failure you've seen or is it speculative? Can you add a test that fails without this behavior? That'd be useful.
</comment><comment author="rculbertson" created="2016-04-28T20:02:03Z" id="215545157">We hit this case when we used v2.1.0 of the client to connect to a cluster which was v1.4.4 (we didn't realize the cluster was running a old version). Since the handler callback was never called, it caused our client process to hang. With this patch it would fail right away with TransportSerializationException, which at least gives some indication as to what went wrong.

Sure I'd be happy to add a test. Thanks!
</comment><comment author="jasontedor" created="2016-04-28T20:16:46Z" id="215549061">In master we fail far earlier on incompatible versions now. I am not sure if this is needed but a test will help clarify the situation. 
</comment><comment author="jasontedor" created="2016-06-06T23:29:45Z" id="224119563">No feedback, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle indices=["_all"] when restoring a snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18025</link><project id="" key="" /><description>It is documented that setting indices on the RestoreSnapshotRestore to
["_all"] restores all indices, but that was not case. Instead a
IndexNotFoundException was raised.

This fixes/replaces https://github.com/elastic/elasticsearch/pull/16927
</description><key id="151493298">18025</key><summary>Handle indices=["_all"] when restoring a snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfussenegger</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T21:16:58Z</created><updated>2016-04-28T12:10:01Z</updated><resolved>2016-04-28T12:10:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-04-28T09:10:07Z" id="215360580">Hi Matthias, thanks for your contribution. Can you make sure that the commit has the same e-mail address that you provided on the CLA? I will then proceed with merging this.
</comment><comment author="mfussenegger" created="2016-04-28T09:30:34Z" id="215366327">Did something change in the CLA check? I'm using the same e-mail as with previous contributions
</comment><comment author="ywelsch" created="2016-04-28T12:05:09Z" id="215402907">&gt; Did something change in the CLA check?

Not that I'm aware of. The weird thing is that the check is now back to green. Anyhow, I'll go ahead and merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrap tasks code at 140 columns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18024</link><project id="" key="" /><description>Switch something from an explicit toString to Strings.toString which
is the same thing but with more code reuse.

Also renamed a constant to be CONSTANT_CASE.
</description><key id="151491590">18024</key><summary>Wrap tasks code at 140 columns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T21:08:20Z</created><updated>2016-04-27T21:25:35Z</updated><resolved>2016-04-27T21:25:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-27T21:16:15Z" id="215231872">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrap Version at 140 columns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18023</link><project id="" key="" /><description>and remove its checkstyle_suppression line
</description><key id="151488803">18023</key><summary>Wrap Version at 140 columns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T20:55:01Z</created><updated>2016-04-27T21:26:02Z</updated><resolved>2016-04-27T21:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-27T21:19:11Z" id="215232633">LGTM
</comment><comment author="nik9000" created="2016-04-27T21:26:01Z" id="215234448">Thanks for reviewing @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Too many open files" exception when running from Docker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18022</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2 (Dockerfile elasticsearch:latest)

**JVM version**: openjdk version "1.8.0_72-internal"

**OS version**: Docker image: https://hub.docker.com/_/elasticsearch/

**Description of the problem including expected versus actual behavior**: 
Expected behavior: An elasticsearch docker container successfully started.
Actual behavior: An elasticsearch docker container fails to start.

**Steps to reproduce**:
1. Run `docker pull elasticsearch` 
2. Run `docker run -i -t --rm -v /srv/elasticseach/data:/usr/share/elasticsearch/data -v /srv/elasticseach/config:/usr/share/elasticsearch/config elasticsearch`
3. A java.nio.file.FileSystemException is raised with message "Too many open files". 

**Provide logs (if relevant)**:
http://pastebin.com/raw/kf0LDGxD
</description><key id="151470416">18022</key><summary>"Too many open files" exception when running from Docker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fauria</reporter><labels /><created>2016-04-27T19:25:54Z</created><updated>2016-04-29T07:55:54Z</updated><resolved>2016-04-28T04:37:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-28T04:37:42Z" id="215308156">The problem is your max file descriptors are too low. There's even a warning in your logs:

&gt; `[2016-04-27 19:08:27,576][WARN ][env                      ] [Box IV] max file descriptors [4096] for elasticsearch process likely too low, consider increasing to at least [65536]`
</comment><comment author="fauria" created="2016-04-28T08:50:16Z" id="215355899">Im running elasticsearch from the official docker image: https://hub.docker.com/_/elasticsearch/

On my host environment, both hard and soft limits are greater than 65536:

`ulimit -Hn &amp;&amp; ulimit -Sn`

```
98304
98304
```

However inside the container:

`docker run -i -t --rm elasticsearch bash`
`ulimit -Hn &amp;&amp; ulimit -Sn`

```
4096
1024
```
</comment><comment author="jasontedor" created="2016-04-28T10:11:27Z" id="215376876">There is not an official Docker image that is affiliated with Elastic. To be clear, the "official" Docker image on Docker Hub is not affiliated with Elastic.
</comment><comment author="fauria" created="2016-04-28T12:51:51Z" id="215413560">I would never have thought that... Already submitted to https://github.com/docker-library/elasticsearch/issues/102 thank you!
</comment><comment author="fauria" created="2016-04-29T07:55:54Z" id="215652455">Fixed!

If anyone has the same problem, try running the container with `--ulimit nofile=98304:98304`command line option, i.e.:

`docker run -d --ulimit nofile=98304:98304 --name elasticsearch ...`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18021</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="151454709">18021</key><summary>.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amrishs</reporter><labels /><created>2016-04-27T18:16:41Z</created><updated>2016-04-27T18:51:26Z</updated><resolved>2016-04-27T18:51:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-27T18:51:15Z" id="215191263">This appears to have been opened in error as the content and title are currently empty.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport: Reindex's throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18020</link><project id="" key="" /><description>Adds `requests_per_second` to `_update_by_query` and `_reindex`. Adds an action to dynamically rethrottle a running reindex request at `/_reindex/{taskId}/_rethrottle` and `/_update_by_query/{taskId}/_rethrottle`.
</description><key id="151438294">18020</key><summary>Backport: Reindex's throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>feature</label><label>review</label><label>v2.4.0</label></labels><created>2016-04-27T17:07:27Z</created><updated>2016-05-11T10:00:46Z</updated><resolved>2016-05-10T19:33:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-27T17:08:00Z" id="215150628">@dakrone can you look at these? It is a relatively strait forward backport of code I believe you already reviewed.
</comment><comment author="dakrone" created="2016-04-27T21:55:11Z" id="215241817">Left minor comments and questions, mostly needs the version guards for the backport.

Also, not sure how hard it would be, but it would be great to add a test for this with a mixed-version cluster and make sure nothing breaks
</comment><comment author="nik9000" created="2016-04-27T22:15:03Z" id="215247943">I'll have a look at doing a mixed version test in the morning. After fixing the serialization I expect it to work  in a mixed version cluster if the coordinating node is 2.4.
</comment><comment author="nik9000" created="2016-04-28T14:52:48Z" id="215451720">Stalling until #18028 is merged.
</comment><comment author="nik9000" created="2016-05-04T22:54:12Z" id="217027967">@dakrone I've added the POSITIVE_INFINITY stuff to this backport!
</comment><comment author="nik9000" created="2016-05-10T15:05:22Z" id="218186722">@dakrone, I hate to pull your attention back to 2.x stuff, but do you think this is ok? I've got to remove the change to `.gitignore` from this but otherwise? I'd like to put it to bed.
</comment><comment author="dakrone" created="2016-05-10T15:32:47Z" id="218195500">LGTM (with the `.gitignore` thing too)

Thanks for working through this and adding lots of tests!
</comment><comment author="nik9000" created="2016-05-10T18:22:32Z" id="218245800">@dakrone Thanks for reviewing and prompting lots of great changes to reindex! I'm excited to get useful for 2.x folks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add pull request template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18019</link><project id="" key="" /><description>This commits adds a pull request template for contributors that submit
documentation updates or code patches.
</description><key id="151437704">18019</key><summary>Add pull request template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label></labels><created>2016-04-27T17:04:24Z</created><updated>2016-04-27T17:24:40Z</updated><resolved>2016-04-27T17:24:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-27T17:06:33Z" id="215150190">I'm not sure about the Markdown syntax, since someone submitting a PR is not going to have rendered syntax until _after_ they submit (or if they preview it), but it's totally up to you. LGTM regardless :)
</comment><comment author="jasontedor" created="2016-04-27T17:17:27Z" id="215155427">@dakrone I pushed another commit to bring the content out of the comment block.
</comment><comment author="dakrone" created="2016-04-27T17:20:38Z" id="215157123">Still LGTM (though I pine for 80-column character limits on all documents)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Deduplicating ingest processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18018</link><project id="" key="" /><description>Adds a processor that deduplicates an array of elements.

Deduplication is aided if the input is sorted, however that means the processor needs to know if the last processor to run was a `sort` processor.  

I've tweaked the Processor interface to include a `setLastType()` method which is invoked by CompoundProcessor.  This allows processors to know what the previous processor was, to help with potential optimization.

We can remove this optimization if we don't want processors to know state about the pipeline.

This is part 2 of the fingerprinting ingest components (the other being #17999).  Part three would be a "meta-processor" that chains split + lowercase + sort + dedupe + join together.  Same concerns in #17999 about a single unified processor vs multiple individual components apply here too.
</description><key id="151430754">18018</key><summary>Add Deduplicating ingest processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Ingest</label><label>feature</label></labels><created>2016-04-27T16:33:02Z</created><updated>2017-04-08T00:19:23Z</updated><resolved>2017-04-08T00:19:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-28T08:46:07Z" id="215354976">What if we always require the `sort` processor to be defined before this processor? We can then: 
- remove the de-duplication for unsorted lists.
- replace the `setLastType()` optimisation with a validation that we check at pipeline construction time by adding a validation method to `Processor.Factory`.

The factory validation can work like this:
- We can add the following method to `Processor.Factory`: 

``` java
default void validate(Pipeline pipeline) {}
```

By default this does nothing, unless factories implement it. This gives processor implementation like this one to fail if a de-duplication processor isn't preceded by a `sort` processor. (it will need to iterate over all processors and if it finds de-duplication processor then check if the previous processor is a sort processor)
- Add the following logic to `Pipeline.Factory#create(...)` to make sure that validation is being invoked:

``` java
Pipeline pipeline = new Pipeline(id, description, compoundProcessor);
for (Processor.Factory factory : processorRegistry) {
   factory.validate(pipeline);
}
return pipeline;
```
</comment><comment author="polyfractal" created="2016-04-28T12:05:46Z" id="215403019">Hmm, a potential concern is if a user wants deduplication but not to have their token stream re-arranged by a sort?  I can't think of any off-hand, but it may be a problem?

Also unfortunate that a user might define a pipeline and only know it's "wrong" once they execute and see they need a `sort` too.  Less user-friendly.

How about using that `validate()` method to create a specialized class for each scenario?  If a `sort` has been previously defined, the factory returns a `DeduplicateFromSortedProcessor`, otherwise it returns a `DeduplicateFromUnsortedProcessor` ?  Then we can get rid of the `setLastTag()` runtime call, and each specialized dedupe processor only has one method of operation.
</comment><comment author="martijnvg" created="2016-04-28T12:27:42Z" id="215408235">&gt; Hmm, a potential concern is if a user wants deduplication but not to have their token stream re-arranged by a sort? 

I assumed that users were always ok with sorting the array :) but yes that may not always be desired.

&gt; Also unfortunate that a user might define a pipeline and only know it's "wrong" once they execute and see they need a sort too. Less user-friendly.

But they would find it out when calling the put pipeline api, because the validation is in the factory instead of the processor. I think this ok? The pipeline never gets created and UI's can nicely show this.

&gt; How about using that validate() method to create a specialized class for each scenario?

+1 We can do it easier, I think we don't need an extra method in that case. But if we add `List&lt;Processor` to `Factory#create` then we check what the previously created processors are and pick the right implementation?
</comment><comment author="talevy" created="2016-04-28T16:28:47Z" id="215485962">@polyfractal, what if a user knows beforehand that their array is sorted (pre-pipeline), would they not be hindered by this since the pipeline itself does not have this knowledge?
</comment><comment author="polyfractal" created="2016-04-28T16:49:09Z" id="215491578">@talevy Yep you're right...it'd use the sub-optimal HashSet based method.  OTOH, the current setup guarantees it'll do the right thing.  E.g. if they say it's sorted but not actually, the de-duping would run without error but start removing wrong tokens.

I'm ambivalent on the issue though, we could certainly expose a power-user flag that allows the optimized de-duping if you promise your data is sorted :)
</comment><comment author="dakrone" created="2016-09-12T21:33:50Z" id="246501422">@polyfractal are you still working on this?
</comment><comment author="dakrone" created="2017-04-07T23:13:41Z" id="292673216">@polyfractal ping, are you still working on this?</comment><comment author="polyfractal" created="2017-04-08T00:19:23Z" id="292680495">Nope, I'm the worst and forgot to close this after chatting with @talevy in Prague about it.  Closing!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue with inline script using Joda DateTimeZone.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18017</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

Using the offical Elasticsearch Docker image for 2.3.1

**Elasticsearch version**: 2.3.1

**JVM version**: java:8-jre

**OS version**: debian jessie
## **Description of the problem including expected versus actual behavior**:
### Actual

Using DateTimeZone with an inline script results in an error. 

```
java.io.IOException: Resource not found: "org/joda/time/tz/data/America/New_York"
```
### Expected

The Correct DateTimeZone instance to be returned.
## **Steps to reproduce**:
1. Start elasticsearch 2.3.1.
2. Pass the startup param `-Des.script.engine.groovy.inline.aggs=true`
3. Create index and mapping with a `date_time_no_millis` field.
4. Run a search query like the one in `Appendix: A`
5. Notice an error like `The datetime zone id 'America/New_York' is not recognised`
6. Run a Date Histogram query like the one in `Appendix: B`
7. The same error will occur
8. Restart the elasticsearch instance
9. Re-run Appendix A and B, they now both work.
## Appendix
### A: Aggregate Query

``` json
{
  "size": 0,
  "aggs": {
    "group_by_hour": {
      "terms": {
        "script": "def opDate = new DateTime(doc['operation_date'].date); opDate.withZone(DateTimeZone.forID('America/New_York')).getHourOfDay()",
        "order": {
          "_term": "asc"
        }
      }
    }
  }
}
```
### B: Date Histogram Query

``` json
{
  "size": 0,
  "aggs": {
    "group_by_hour": {
      "date_histogram": {
        "field": "date_field",
        "interval": "hour",
        "format": "H",
        "time_zone": "America/New_York"
      }
    }
  }
}
```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="151416532">18017</key><summary>Issue with inline script using Joda DateTimeZone.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">deanobarnett</reporter><labels><label>:Dates</label><label>:Scripting</label><label>bug</label></labels><created>2016-04-27T15:32:34Z</created><updated>2016-05-27T12:51:19Z</updated><resolved>2016-05-27T12:51:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-29T08:25:01Z" id="215657049">Whether this fails or not depends on the order of requests, eg given this document:

```
PUT t/t/1
{
  "operation_date": "2001/10/10"
}
```

If you run a non-scripting agg which refers to the time zone first:

```
GET _search
{
  "aggs": {
    "NAME": {
      "date_histogram": {
        "field": "operation_date",
        "interval": "hour",
        "time_zone": "America/New_York"
      }
    }
  }
}
```

then this request succeeds, and so do the scripting request:

```
GET _search
{
  "size": 0,
  "aggs": {
    "group_by_hour": {
      "terms": {
        "script": "def opDate = new DateTime(doc['operation_date'].date); opDate.withZone(DateTimeZone.forID('America/New_York')).getHourOfDay()",
        "order": {
          "_term": "asc"
        }
      }
    }
  }
}
```

If you reverse the order of the searches, then both fail.
</comment><comment author="gmoskovicz" created="2016-05-13T13:12:58Z" id="219038824">If you don't have an aggregation, there is currently no way to get timezones and work with scripts. Any ETA on fixing this? Any workaround that can be defined inside the script?

I tried modifying the `java.policy` file but doesn't look like it works. Isn't this related to #14524? Is there any workaround other than performing a fake search operation before running the script?
</comment><comment author="jasontedor" created="2016-05-13T13:15:41Z" id="219039470">Modifying the `java.policy` file will not help, the issue is in Joda Time.
</comment><comment author="gmoskovicz" created="2016-05-13T13:17:56Z" id="219039988">Thanks for the quick answer @jasontedor . So there isn't a real workaround? Is it related to JodaOrg/joda-time#327? Until this is not fixed, one cannot do this operation? 

Is there something that we can do for this? Looks like the Joda Time issue has been opened for a while already.
</comment><comment author="jasontedor" created="2016-05-19T02:13:10Z" id="220210268">I opened JodaOrg/joda-time#375. When we can incorporate a new release of Joda Time that contains this into Elasticsearch we will be able to close this bug out. 
</comment><comment author="gmoskovicz" created="2016-05-19T11:45:00Z" id="220300889">@jasontedor is this going to be added in a `2.3.x` release for ES?
</comment><comment author="jasontedor" created="2016-05-19T11:47:52Z" id="220301475">&gt; is this going to be added in a 2.3.x release for ES?

I don't know; we have to see if JodaOrg/joda-time#375 is accepted into Joda Time first, and if it is, the timeline under which a bug fix release of Joda Time is made that includes it that we can incorporate into Elasticsearch. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix backward compatibility of fuzzy queries on ip addresses.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18016</link><project id="" key="" /><description>This makes the new ip field use the same definition for fuzzy queries as the
old (ipv4-only) ip field.
</description><key id="151406873">18016</key><summary>Fix backward compatibility of fuzzy queries on ip addresses.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2016-04-27T14:55:57Z</created><updated>2016-05-11T14:09:40Z</updated><resolved>2016-05-11T14:09:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-07T14:03:51Z" id="217638929">&gt; Can we do this instead (throw an exception)? And just tell the user to use a cidr mask? Supporting "fuzzy" queries on ip addresses seems crazy, and I don't think we should continue this.

I agree.  Would be nice to have deprecation logging in 2.3 for this, but not a requirement.
</comment><comment author="jpountz" created="2016-05-11T14:05:40Z" id="218469536">Closed in favor of #18276.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog configurable path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18015</link><project id="" key="" /><description>I am dealing with problem with high ingestion rate of very small documents. Currently I store indices in EBS (EC2). I was considering changing location of translog to faster (local) storage with low access latency and more IOPS. I realized it is not possible now. Though i found TranslogConfig class in source code. Moreover there is path already, just waiting to get new functionality :)

The only limitation is that TranslogConfig's path depends only on _shardPath().resolveTranslog()_&#8203;

`new TranslogConfig(shardId, shardPath().resolveTranslog(), indexSettings, bigArrays);`

What if translog path is configurable with prefix? This would make it possible to store translog in local directories.

PS. I am sure that translog is reducing the ingestion throughput. CPU wait time varies from 15% to 20%. When I change translog strategy to async it dropped to 6%.
</description><key id="151406536">18015</key><summary>Translog configurable path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels /><created>2016-04-27T14:54:33Z</created><updated>2016-04-27T15:14:05Z</updated><resolved>2016-04-27T15:14:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-27T15:14:05Z" id="215115808">I do not think this should be configurable. A translog is intimately tied to its shard. If it were configurable, a simple misconfiguration could cause an abundance of chaos (let alone the chaos from users that might try to park their translog on a RAM disk).

I also doubt that this is even the right solution to your problem, but I think that you should open a post on the [Elastic discourse forums](https://discuss.elastic.co) for a broader discussion since Elastic reserves GitHub for bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail queries on not indexed fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18014</link><project id="" key="" /><description>While returning no hits on fields that are not mapped may be fine, it is not
for fields that are mapped but not indexed (`index:false`). We should fail the
query in that case rather than returning no hits.
</description><key id="151396885">18014</key><summary>Fail queries on not indexed fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T14:15:00Z</created><updated>2016-04-28T07:54:37Z</updated><resolved>2016-04-28T07:54:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-27T18:10:25Z" id="215176776">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_msearch errors don't contain status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18013</link><project id="" key="" /><description>Error responses to `_msearch` are not consistent with responses to `_search` - they are missing the `status` field identifying the http status code corresponding to the error. This is used in many clients to find the correct exception to raise which makes it not an option with `_msearch`.

Ideally the `responses` key in the `msearch` response would be exact match to the bodies of the search requests as if they were fired of independently.
</description><key id="151391294">18013</key><summary>_msearch errors don't contain status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-04-27T13:52:06Z</created><updated>2016-06-16T12:31:39Z</updated><resolved>2016-06-16T12:31:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Javadoc: In Stream* scannability is priority 1!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18012</link><project id="" key="" /><description /><key id="151387333">18012</key><summary>Javadoc: In Stream* scannability is priority 1!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T13:37:49Z</created><updated>2016-04-27T13:43:04Z</updated><resolved>2016-04-27T13:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-27T13:39:50Z" id="215085832">LGTM. Thanks for writing this up!
</comment><comment author="nik9000" created="2016-04-27T13:43:04Z" id="215087111">Writing down the decisions we've made and, in some cases, internalized is something I'm very keen to do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add XPointValues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18011</link><project id="" key="" /><description>Forked utility methods from Lucene's PointValues until LUCENE-7257 is released.
Replace PointValues with XPointValues where needed.
Fixes #18010
</description><key id="151377808">18011</key><summary>Add XPointValues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Internal</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T12:54:20Z</created><updated>2016-04-27T19:27:06Z</updated><resolved>2016-04-27T19:26:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-27T12:59:34Z" id="215074012">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Uncaught exception thrown from PointValues static functions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18010</link><project id="" key="" /><description>The static helpers on PointValues throws an exception if a leaf has points indexed but not for the given field. 
When using this function in es the exception is never caught which could lead to error in search request or field stats request.
The problem has been fixed in Lucene: https://issues.apache.org/jira/browse/LUCENE-7257 but we agreed with @jpountz that we should have a temporary fix in es that we could remove afterward.
The proposal is to have a XPointValues which acts exactly like the PointValues with the LUCENE-7257 patch and to use this class to call all the static helpers in es.
</description><key id="151372009">18010</key><summary>Uncaught exception thrown from PointValues static functions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Internal</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T12:23:21Z</created><updated>2016-04-27T19:26:55Z</updated><resolved>2016-04-27T19:26:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>S3 repositories credentials should be filtered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18009</link><project id="" key="" /><description>When working on #18008 I found while reading the code that we don't filter anymore `repositories.s3.access_key` and `repositories.s3.secret_key`.

Also fixed a typo in REST test
</description><key id="151370140">18009</key><summary>S3 repositories credentials should be filtered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T12:12:28Z</created><updated>2016-05-10T18:34:03Z</updated><resolved>2016-05-10T18:34:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-03T14:24:46Z" id="216544936">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Repository instance settings should not appear in cluster settings when using include_defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18008</link><project id="" key="" /><description>In elasticsearch 5.0, we now register settings which could be set within a cluster.

For example, when you want to define `access_key` in S3 repositories, you have multiple choices:
- In `elasticsearch.yml`, set `cloud.aws.access_key` or `cloud.aws.s3.access_key`
- In `elasticsearch.yml`, set `repositories.s3.access_key`
- When you create a Repository using `PUT /snapshot/repo`, you can pass a `access_key` setting:

```
PUT _snapshot/my_s3_repository
{
  "type": "s3",
  "settings": {
    "access_key": "mykey"
  }
}
```

Until now we registered all settings when [the plugin starts](https://github.com/elastic/elasticsearch/blob/master/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java#L132-L146).

The problem with that is when you call `GET _cluster/settings?include_defaults`, all those settings are returned, including `access_key` without any namespace.

If we don't register the repository level settings anymore, it will work **but** when we do a `GET _snapshot/repo` the credentials which are supposed to be hidden will be returned as they are not filtered anymore.
</description><key id="151368890">18008</key><summary>Repository instance settings should not appear in cluster settings when using include_defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Settings</label><label>feedback_needed</label></labels><created>2016-04-27T12:05:18Z</created><updated>2016-06-01T14:00:41Z</updated><resolved>2016-06-01T14:00:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-03T14:25:55Z" id="216545464">why is `access_key` in the settings at all? shouldn't this be private to the repository? This setting should not be registered since the repository settings still do their own thing and therefor can't be filtered since they use a different infrastructure?
</comment><comment author="dadoonet" created="2016-05-03T15:47:00Z" id="216572285">@s1monw I believe it's because we want to define credentials only once an not per repository?

Let say I'm an admin and I defined S3 credentials for repositories.
Then anyone could create a repository without having to know the credentials.
</comment><comment author="s1monw" created="2016-05-04T07:27:14Z" id="216765605">it still makes no sense, the only way this can be returned is if you set it in you yaml files. but it can only be set in the `PUT /snapshot/repo` command so why the hack since it's private to the repo is it merged with node settings? I think you confuse things? can you provide a reproduction or a testcase?
</comment><comment author="s1monw" created="2016-05-09T05:47:56Z" id="217780871">@dadoonet any updates?
</comment><comment author="dadoonet" created="2016-05-18T17:35:35Z" id="220101694">@s1monw I'm sorry but I don't understand your comment. I'll try to ping you later next week to talk about it.
</comment><comment author="dadoonet" created="2016-06-01T14:00:40Z" id="223001255">@s1monw I can confirm your opinion. What I described can not happen.

Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switches from empty boolean query to matchNoDocs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18007</link><project id="" key="" /><description>Closes #17981 

Caveat: Tests still running.

@cbuescher Input most welcome. I left one TODO with a question in the code.
</description><key id="151361756">18007</key><summary>Switches from empty boolean query to matchNoDocs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T11:21:53Z</created><updated>2016-04-29T10:57:13Z</updated><resolved>2016-04-28T11:13:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-27T11:49:34Z" id="215058971">I wasn't aware that there were two MatchNoDocsQuery queries now, one in lucene itself (`org.apache.lucene.search`) and one in `org.elasticsearch.common.lucene.search`. The first is rewriting to the empty BooleanQuery, the second is able to print out this "reason" string. I'm not sure which one we are supposed to use in Queries#newMatchNoDocsQuery(). Maybe @jimferenczi has and opinion on this? We could use have Queries#newMatchNoDocsQuery(reason) to keep the reason for the missing matches, and provide another method for cases where the reason is not defined?
</comment><comment author="MaineC" created="2016-04-27T12:08:16Z" id="215062404">&gt; I'm not sure which one we are supposed to use in Queries#newMatchNoDocsQuery()

I'd like to try and only use the one requiring a reason string. As soon as we offer one that doesn't require the reason, my guess would be that the one with the reason isn't going to be used.
</comment><comment author="jimczi" created="2016-04-27T12:08:34Z" id="215062462">@cbuescher IMO we should always use the MatchNoDocsQuery(reason). There's always a reason why we use a MatchNoDocsQuery, most of the time it happens during the rewriting phase and the reason is  pretty clear.
</comment><comment author="MaineC" created="2016-04-27T14:05:42Z" id="215094531">Changed to MatchNoDocsQuery(reason) - sorry for the confusion, apparently my IDE hadn't picked up the new class yet and I didn't look closely enough at the package of the imported class.

Added reason messages where appropriate, not entirely sure if all of them make sense as is, so double-checking appreciated.
</comment><comment author="cbuescher" created="2016-04-28T09:47:10Z" id="215370857">@MaineC thanks, I left a few comments about wording, other than that this LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.3.1 delete by parent id throw id is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18006</link><project id="" key="" /><description>Hi ,
I want to delete childs documents by parent.
My java codes is 

``` java
client.prepareDelete().setIndex("care").setType("CareServiceType").setParent("19448").execute().actionGet();
```

But throw exception  
org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: id is missing;
</description><key id="151350886">18006</key><summary>ES 2.3.1 delete by parent id throw id is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andykai</reporter><labels /><created>2016-04-27T10:20:08Z</created><updated>2016-04-29T03:52:10Z</updated><resolved>2016-04-28T17:51:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-28T17:51:07Z" id="215508962">You can't do that with a delete command.  You need to use delete-by-query with a has_parent query
</comment><comment author="andykai" created="2016-04-29T03:52:10Z" id="215620512">Yes, I know .But delete-by-query is a plugin now,But i can not find java api of delete-by-query plugin. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch nested aggregation inside a reverse nested aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18005</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0_66 / 25.66-b17

**OS version**: Mac OS X 10.11.4

**Description of the problem including expected versus actual behavior**:
I am having trouble getting the correct values to show up in a 4 level deep aggregation scenario where the first two levels are nested, the third is reverse_nested, and the fourth is nested again.

Here is my index mapping: 

```
curl -XDELETE localhost:9200/orders-d
curl -XPUT localhost:9200/orders-d
curl -XPUT localhost:9200/orders-d/order-d/_mapping -d '{
    "order-d": {  
        "properties": {  
            "id": {  
                "type": "string"  
            },  
            "orderNumber": {  
                "type": "string"  
            },  
            "groupId": {  
                "type": "string"  
            },  
            "groupOrderNumber": {  
                "type": "string"  
            },  
            "dateCreated": {  
                "type": "date"  
            },  
            "dateUpdated": {  
                "type": "date"  
            },  
            "location": {  
                "type": "object"  
            },  
            "orderSubmitter": {  
                "type": "object"  
            },  
            "distributor": {  
                "type": "object"  
            },  
            "salesRep": {  
                "type": "object"  
            },  
            "status": {  
                "type": "string"  
            },  
            "total": {  
                "type": "double"  
            },  
            "isTTOrder": {  
                "type": "boolean"  
            },  
            "lineItems": {  
                "type": "nested",  
                "include_in_parent": true,  
                "properties": {  
                    "product": {  
                        "type": "object"  
                    },  
                    "category": {  
                        "type": "object"  
                    },  
                    "subCategory": {  
                        "type": "object"  
                    },  
                    "quantity": {  
                        "type": "double"  
                    },  
                    "unitPrice": {  
                        "type": "double"  
                    },  
                    "totalPrice": {  
                        "type": "double"  
                    },  
                    "pricedByUnitPrice": {  
                        "type": "double"  
                    }  
                }  
            }  
        }  
    }
}'
```

Here is my data:

```
curl -XPUT localhost:9200/orders-d/order-d/0 -d '{
                "id": "571652632a19085c008b4577",
                "orderNumber": "1617590686",
                "groupId": "571652632a19085c008b4578",
                "groupOrderNumber": "3485944627",
                "dateCreated": "2016-04-19",
                "dateUpdated": null,
                "location": {
                    "id": "54e53853505eb66b008b4569",
                    "name": "Andrews Diner"
                },
                "orderSubmitter": {
                    "id": "54e53853505eb66b008b4567",
                    "name": "Kostantino Plaitis"
                },
                "distributor": {
                    "id": "55c3879459ad0c63008b4569",
                    "name": "Performance Foodservice Metro NY"
                },
                "salesRep": null,
                "status": "pending",
                "total": 5410.21,
                "isTTOrder": true,
                "lineItems": [{
                    "product": {
                        "id": "55bfb445c440b26a008b4571",
                        "name": "Sabrett Sauerkraut 12 x 2 lb bags"
                    },
                    "category": {
                        "id": "53df845b3b8e77710e7b23ec",
                        "name": "Groceries &amp; Dry Food"
                    },
                    "subCategory": {
                        "id": "53e1e8723b8e77a52b8b4586",
                        "name": "Other Sauces Dipping\/Condiments\/Savoury Toppings\/Savoury Spreads\/Marinades (Perishable)"
                    },
                    "quantity": 1,
                    "unitPrice": 25.24,
                    "totalPrice": 25.24,
                    "pricedByUnitPrice": 0
                }, {
                    "product": {
                        "id": "55bc219238c0376e008b4570",
                        "name": "Franks Red Hot Cayenne Pepper Sauce 4 x 1 gallon"
                    },
                    "category": {
                        "id": "53df845b3b8e77710e7b23ec",
                        "name": "Groceries &amp; Dry Food"
                    },
                    "subCategory": {
                        "id": "53e1e8723b8e77a52b8b4606",
                        "name": "Other Sauces Dipping\/Condiments\/Savoury Toppings\/Savoury Spreads\/Marinades (Shelf Stable)"
                    },
                    "quantity": 1,
                    "unitPrice": 45.06,
                    "totalPrice": 45.06,
                    "pricedByUnitPrice": 0
                }, {
                    "product": {
                        "id": "56d76c41bd821fda008b459a",
                        "name": "Cereal, Classic Variety Pack, Kelloggs 1\/60 ct."
                    },
                    "category": {
                        "id": "53df845b3b8e77710e7b23ec",
                        "name": "Groceries &amp; Dry Food"
                    },
                    "subCategory": {
                        "id": "53e1e8723b8e77a52b8b462d",
                        "name": "Grains\/Cereal - Ready to Eat - (Shelf Stable)"
                    },
                    "quantity": 1,
                    "unitPrice": 56.03,
                    "totalPrice": 56.03,
                    "pricedByUnitPrice": 0
                }]
            }'

curl -XPUT localhost:9200/orders-d/order-d/0 -d '{
                "id": "571652632a19085c008b4576",
                "orderNumber": "2041063294",
                "groupId": "571652632a19085c008b4578",
                "groupOrderNumber": "3485944627",
                "dateCreated": "2016-04-19",
                "dateUpdated": null,
                "location": {
                    "id": "54e53853505eb66b008b4569",
                    "name": "Andrews Diner"
                },
                "orderSubmitter": {
                    "id": "54e53853505eb66b008b4567",
                    "name": "Kostantino Plaitis"
                },
                "distributor": {
                    "id": "55cdeece0a41216c008b4583",
                    "name": "Driscoll Foods"
                },
                "salesRep": null,
                "status": "pending",
                "total": 7575.27,
                "isTTOrder": true,
                "lineItems": [{
                    "product": {
                        "id": "55ad05e08d28c36b008b456c",
                        "name": "Pepper 3000 pcs"
                    },
                    "category": {
                        "id": "53df845b3b8e77710e7b23ec",
                        "name": "Groceries &amp; Dry Food"
                    },
                    "subCategory": {
                        "id": "53e1e8723b8e77a52b8b4582",
                        "name": "Herbs\/Spices (Shelf Stable)"
                    },
                    "quantity": 3,
                    "unitPrice": 8.95,
                    "totalPrice": 26.85,
                    "pricedByUnitPrice": 0
                }, {
                    "product": {
                        "id": "55b3a12f6b415c68008b4568",
                        "name": "Venice Maid Deluxe Corned Beef Hash 6 x 6 lb 10 oz"
                    },
                    "category": {
                        "id": "53df846c3b8e77710e7b23f7",
                        "name": "Meat"
                    },
                    "subCategory": {
                        "id": "54d8c56a279871b9078b4581",
                        "name": "Beef - Prepared\/Processed"
                    },
                    "quantity": 1,
                    "unitPrice": 59.75,
                    "totalPrice": 59.75,
                    "pricedByUnitPrice": 0
                }, {
                    "product": {
                        "id": "55b145798c26dc69008b4568",
                        "name": "Aladdin Bakers Sesame Bread Sticks 150 x 2 packs"
                    },
                    "category": {
                        "id": "53df845b3b8e77710e7b23ec",
                        "name": "Groceries &amp; Dry Food"
                    },
                    "subCategory": {
                        "id": "53e1e8723b8e77a52b8b45b0",
                        "name": "Dried Breads (Shelf Stable)"
                    },
                    "quantity": 8,
                    "unitPrice": 15.5,
                    "totalPrice": 124,
                    "pricedByUnitPrice": 0
                }, {
                    "product": {
                        "id": "55ad074a8d28c36f008b456d",
                        "name": "Smuckers Breakfast Syrup 100 cups"
                    },
                    "category": {
                        "id": "53df845b3b8e77710e7b23ec",
                        "name": "Groceries &amp; Dry Food"
                    },
                    "subCategory": {
                        "id": "53e1e8723b8e77a52b8b457d",
                        "name": "Syrup\/Treacle\/Molasses (Shelf Stable)"
                    },
                    "quantity": 10,
                    "unitPrice": 8.95,
                    "totalPrice": 89.5,
                    "pricedByUnitPrice": 0
                }]
            }'
```

Here is my query:

```
curl -XPOST localhost:9200/orders-d/_search -d '{
        "from": 0,
        "size": 0,
        "aggregations": {
            "totalLineItems": {
                "aggs": {
                    "totalLineItems": {
                        "terms": {
                            "field": "lineItems.category.id",
                            "size": 0
                        },
                        "aggs": {
                            "totalLineItems": {
                                "terms": {
                                    "field": "lineItems.product.id",
                                    "size": 0
                                },
                                "aggs": {
                                    "totalLineItems": {
                                        "aggs": {
                                            "totalLineItems": {
                                                "terms": {
                                                    "field": "distributor.id",
                                                    "size": 0
                                                },
                                                "aggs": {
                                                    "totalLineItems": {
                                                        "aggs": {
                                                            "totalLineItems": {
                                                                "sum": {
                                                                    "field": "lineItems.totalPrice"
                                                                }
                                                            }
                                                        },
                                                        "nested": {
                                                            "path": "lineItems"
                                                        }
                                                    }
                                                }
                                            }
                                        },
                                        "reverse_nested": {}
                                    }
                                }
                            }
                        }
                    }
                },
                "nested": {
                    "path": "lineItems"
                }
            }
        },
        "query": {
            "bool": {
                "must": [{
                    "range": {
                        "dateCreated": {
                            "format": "yyyy-MM-dd",
                            "gte": "2016-01-01",
                            "lte": "2016-04-30"
                        }
                    }
                }]
            }
        }
    }'
```

...and here are my results:

```
{
    "took": 8,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 0.0,
        "hits": []
    },
    "aggregations": {
        "totalLineItems": {
            "doc_count": 4,
            "totalLineItems": {
                "doc_count_error_upper_bound": 0,
                "sum_other_doc_count": 0,
                "buckets": [{
                    "key": "53df845b3b8e77710e7b23ec",
                    "doc_count": 3,
                    "totalLineItems": {
                        "doc_count_error_upper_bound": 0,
                        "sum_other_doc_count": 0,
                        "buckets": [{
                            "key": "55ad05e08d28c36b008b456c",
                            "doc_count": 1,
                            "totalLineItems": {
                                "doc_count": 1,
                                "totalLineItems": {
                                    "doc_count_error_upper_bound": 0,
                                    "sum_other_doc_count": 0,
                                    "buckets": [{
                                        "key": "55cdeece0a41216c008b4583",
                                        "doc_count": 1,
                                        "totalLineItems": {
                                            "doc_count": 4,
                                            "totalLineItems": {
                                                "value": 300.1
                                            }
                                        }
                                    }]
                                }
                            }
                        }, {
                            "key": "55ad074a8d28c36f008b456d",
                            "doc_count": 1,
                            "totalLineItems": {
                                "doc_count": 1,
                                "totalLineItems": {
                                    "doc_count_error_upper_bound": 0,
                                    "sum_other_doc_count": 0,
                                    "buckets": [{
                                        "key": "55cdeece0a41216c008b4583",
                                        "doc_count": 1,
                                        "totalLineItems": {
                                            "doc_count": 4,
                                            "totalLineItems": {
                                                "value": 300.1
                                            }
                                        }
                                    }]
                                }
                            }
                        }, {
                            "key": "55b145798c26dc69008b4568",
                            "doc_count": 1,
                            "totalLineItems": {
                                "doc_count": 1,
                                "totalLineItems": {
                                    "doc_count_error_upper_bound": 0,
                                    "sum_other_doc_count": 0,
                                    "buckets": [{
                                        "key": "55cdeece0a41216c008b4583",
                                        "doc_count": 1,
                                        "totalLineItems": {
                                            "doc_count": 4,
                                            "totalLineItems": {
                                                "value": 300.1
                                            }
                                        }
                                    }]
                                }
                            }
                        }]
                    }
                }, {
                    "key": "53df846c3b8e77710e7b23f7",
                    "doc_count": 1,
                    "totalLineItems": {
                        "doc_count_error_upper_bound": 0,
                        "sum_other_doc_count": 0,
                        "buckets": [{
                            "key": "55b3a12f6b415c68008b4568",
                            "doc_count": 1,
                            "totalLineItems": {
                                "doc_count": 1,
                                "totalLineItems": {
                                    "doc_count_error_upper_bound": 0,
                                    "sum_other_doc_count": 0,
                                    "buckets": [{
                                        "key": "55cdeece0a41216c008b4583",
                                        "doc_count": 1,
                                        "totalLineItems": {
                                            "doc_count": 4,
                                            "totalLineItems": {
                                                "value": 300.1
                                            }
                                        }
                                    }]
                                }
                            }
                        }]
                    }
                }]
            }
        }
    }
}
```

As you can see from the results, all the aggregated values for each drilldown of totalLineItems have the same exact value. This is obviously incorrect.

Did I do something wrong or is nesting inside a reverse nesting not supported?
</description><key id="151344457">18005</key><summary>elasticsearch nested aggregation inside a reverse nested aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">manioc2015</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2016-04-27T09:49:04Z</created><updated>2016-05-13T22:06:22Z</updated><resolved>2016-05-02T10:59:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-28T17:49:27Z" id="215508527">Sorry @manioc2015 but there's just too much noise here for me to follow what's happening.  Could you reduce your recreation to something simple, and point out exactly what problem I should look for?
</comment><comment author="manioc2015" created="2016-04-30T07:31:29Z" id="215944199">OK I reduced the amount of data in my example. As you can see, each aggregation of each drilldown has the same exact value of "300.1" this is incorrect. It should be bound to the breakdown of each bucket within each sub-aggregation.
</comment><comment author="clintongormley" created="2016-05-02T10:59:52Z" id="216200377">When you "step-up" to the parent document with `reverse_nested`, you lose the context of the nested document that you were in, so the `lineItems.totalPrice` refers to all nested documents.
</comment><comment author="manioc2015" created="2016-05-02T19:21:57Z" id="216335058">@clintongormley can you confirm that once we "step up" to the parent document, we can't step "back down"? This is what I need to accomplish.
</comment><comment author="clintongormley" created="2016-05-03T09:47:07Z" id="216482358">@manioc2015 you can't step down to the same nested doc without adding the same filtering that you used originally
</comment><comment author="manioc2015" created="2016-05-03T09:51:20Z" id="216483123">@clintongormley that's my point. I did step back down into the same nested doc.

```
                                                "aggs": {
                                                    "totalLineItems": {
                                                        "aggs": {
                                                            "totalLineItems": {
                                                                "sum": {
                                                                    "field": "lineItems.totalPrice"
                                                                }
                                                            }
                                                        },
                                                        "nested": {
                                                            "path": "lineItems"
                                                        }
                                                    }
                                                }
```
</comment><comment author="clintongormley" created="2016-05-03T09:58:12Z" id="216484487">No @manioc2015 - as soon as you step up, you lose context
</comment><comment author="manioc2015" created="2016-05-03T10:03:11Z" id="216485365">@clintongormley I don't get it. Pardon my ignorance. Are you saying I can't step back into the nested doc by declaring "nested" once I step out by doing a "reverse_nested" call? Better yet, can you possibly show me what the correct query would be?
</comment><comment author="clintongormley" created="2016-05-03T10:03:57Z" id="216485508">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="manioc2015" created="2016-05-13T22:06:22Z" id="219170940">@clintongormley: you said "you can't step down to the same nested doc without adding the same filtering that you used originally".

But i did add the same filtering that I used originally:

```
                                                "aggs": {
                                                    "totalLineItems": {
                                                        "aggs": {
                                                            "totalLineItems": {
                                                                "sum": {
                                                                    "field": "lineItems.totalPrice"
                                                                }
                                                            }
                                                        },
                                                        "nested": {
                                                            "path": "lineItems"
                                                        }
                                                    }
                                                }
```

...and the results aren't correct. Can you please explain?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Azure discovery tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18004</link><project id="" key="" /><description>Adds a test that mocks the Azure management endpoint, checking proper permissions for the Azure client SDK.

Also cleans up / properly adds some of the settings for Azure discovery.
</description><key id="151335143">18004</key><summary>Add Azure discovery tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T09:08:28Z</created><updated>2016-04-29T13:54:15Z</updated><resolved>2016-04-29T13:54:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-29T12:17:14Z" id="215696226">Really nice! Thank you so much!

I tested it locally and it works well.

I can't comment on the gradle part though. May be @rjernst could review this part?
</comment><comment author="ywelsch" created="2016-04-29T13:28:32Z" id="215710847">@dadoonet Pushed a commit where I only register the "cloud.azure.management.endpoint" setting for the tests.

The Gradle part needs no further review as it is verbatim the same code as for GCE.
</comment><comment author="dadoonet" created="2016-04-29T13:29:48Z" id="215711259">Cool! It looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix xcontent rendering of ip terms aggs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18003</link><project id="" key="" /><description>Currently terms on an ip address try to put their binary representation in the
json response. With this commit, they would return a formatted ip address:

```
      "buckets": [
        {
          "key": "192.168.1.7",
          "doc_count": 1
        }
      ]
```

Relates to #17971
</description><key id="151327654">18003</key><summary>Fix xcontent rendering of ip terms aggs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T08:28:45Z</created><updated>2016-06-28T09:28:49Z</updated><resolved>2016-05-13T13:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2016-04-27T09:51:42Z" id="215033543">I see significant terms on IPs now format strings correctly with this change but this may be an irrelevant improvement - I expected an error (IPs are numerics, numerics don't have doc frequency any more). What I see in results are IPs selected on what I assume is a false significance - bg_count is reported as zero in the JSON response.

Until we adopt a solid strategy for computing background frequencies for types that don't have frequencies directly held by Lucene I thought our policy was to throw a parse error and suggest users index-as-string?
</comment><comment author="jpountz" created="2016-04-27T10:04:46Z" id="215038168">Oh you are right, I was focused on the json rendering issue and completely missed that. I agree the significant terms aggregation should raise an exception if it cannot get the backgrond frequency rather than assuming 0. For the record, I also tested on an unindexed keyword field and it does not fail either while it should:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "a": {
          "type": "keyword",
          "index": false
        }
      }
    }
  }
}

PUT t/t/1
{
  "a": "abc"
}

PUT t/t/2
{
}

GET t/_search
{
  "query": {
    "exists" : {
      "field": "a"
    }
  }, 
  "aggs": {
    "a_terms": {
      "significant_terms": {
        "field": "a",
        "min_doc_count": 1
      }
    }
  }
}
```
</comment><comment author="jpountz" created="2016-04-28T08:32:21Z" id="215351938">@markharwood I opened #18031 to address this issue.
</comment><comment author="jpountz" created="2016-05-11T15:38:39Z" id="218498503">@markharwood May I merge this one now that points work with significant terms?
</comment><comment author="markharwood" created="2016-05-12T11:10:55Z" id="218727475">Looks great but I wonder if the Kibana folks will be upset by the removal of "key_as_string" in the json?
</comment><comment author="jpountz" created="2016-05-12T12:47:48Z" id="218746496">@epixa Do you know if not having a `key_as_string` in the response of the terms agg on an ip field would be an issue for Kibana? (the string representation of the ip address is directly under `key`)
</comment><comment author="epixa" created="2016-05-12T16:06:22Z" id="218805186">Thanks for the heads up, I'm pretty sure it won't cause any problems with Kibana, but let me poke around a bit to make sure.  At the moment, the only reference to `key_as_string` in Kibana is in test fixtures, which may only exist because it was copied from actual ES response data.

I'll update this PR shortly.
</comment><comment author="epixa" created="2016-05-12T16:13:58Z" id="218807446">It looks like removing `key_as_string` will break monitoring in x-pack.  It's also explicitly referenced from the watcher docs (slack action), so removing it will likely break ~~any~~ at least some existing watcher setups with slack.

How urgent is this?  We at least have complete control over monitoring which means we could theoretically get a change introduced there as well, but breaking existing watcher setups might be a different animal entirely.
</comment><comment author="jpountz" created="2016-05-12T16:28:40Z" id="218811590">It is not urgent at all, we are just trying to make terms aggregations work again on ip fields (which was not the case anymore since we added ipv6 support).

So I guess we can either add a `key_as_string` key in the response all the time, but this feels a bit weird since it will always be the same as the `key` (this is why a terms agg on a string field does not have a `key_as_string`) or we can only return a `key` like the PR currently does.

In that particular case, I think the latter option is fine since we need to break the output of terms aggregations on ip fields anyway (since we cannot return a number that identifies ip addresses anymore)? @clintongormley  any opinions?
</comment><comment author="clintongormley" created="2016-05-12T16:37:38Z" id="218813955">@jpountz i'm +1 on just returning `key`.  
</comment><comment author="epixa" created="2016-05-12T16:43:02Z" id="218815412">It doesn't seem weird to me at all that we'd return `key_as_string` even in situations when it is the same as `key`.  If anything, the fact that there are some situations when it differs and some when it does not only underscores the importance of doing that to me.  IMO, the burden shouldn't be on the consumer to make choices about which property to use based on the data type.  I mean, it's great that _we_ know that those values are identical in some situations, but why should a consumer have to codify that detail as well?
</comment><comment author="jpountz" created="2016-05-12T17:59:31Z" id="218836493">@epixa I would be fine to do it all the time (but in another issue since this is a broader problem as it also affects eg. terms aggs on string fields). However I suspect there will be push back since making aggregation responses more verbose will put more load on the network (although compression would probably work very well in that case) and make parsing slower since there are more bytes to process. I am not sure how much of an issue this is in practice, but this is a recurring concern.
</comment><comment author="epixa" created="2016-05-13T14:44:27Z" id="219063933">This really shouldn't be merged yet... if we want to proceed with the change, fine, but I'm relatively certain monitoring in xpack is no longer going to work now.  Ideally we'd at least get our own products patched up to support a change like this before we merged it.
</comment><comment author="jpountz" created="2016-05-13T15:06:11Z" id="219070682">@epixa Terms aggregations on ip fields were broken since we added support for ipv6, so this change is making things better, not worse. Unfortunately we have to break backward compatibility of the responses anyway since we cannot return a numeric representation for ip terms anymore, and I made the response consistent with terms aggregations on a string field. I proceeded based on @clintongormley's comment and will now check how this can be addressed in monitoring.
</comment><comment author="epixa" created="2016-05-13T15:52:17Z" id="219083640">Looks like we're OK after all, carry on!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>External Plugins: checkstyle is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18002</link><project id="" key="" /><description>While trying to write an external plugin (not part of this repo) I had to disable the checkstyle checks as they are not working. Here is a reproduction (that does have tests failures, but it wont get that far)

This is the `build.gradle`

``` groovy
buildscript {
  repositories {
    mavenLocal()
    mavenCentral()
    jcenter()
  }

  dependencies {
    classpath "org.elasticsearch.gradle:build-tools:5.0.0-alpha1"
  }
}

group = 'de.spinscale.test'
version = '0.0.1-SNAPSHOT'

apply plugin: 'java'
apply plugin: 'elasticsearch.esplugin'
apply plugin: 'idea'

esplugin {
  name 'test'
  description 'test'
  classname 'de.spinscale.TestPlugin'
}

// In this section you declare the dependencies for your production and test code
dependencies {
  compile 'org.elasticsearch:elasticsearch:5.0.0-alpha1'
  testCompile 'org.elasticsearch.test:framework:5.0.0-alpha1'
}

// checkstyleMain.enabled = false
// checkstyleTest.enabled = false

dependencyLicenses.enabled = false
thirdPartyAudit.enabled = false
licenseHeaders.enabled = false

```

This is the `settings.gradle`

```
rootProject.name = 'test'
```

You need any test class in `src/test/` like `src/test/java/MyTests.java`

``` java
import org.elasticsearch.test.ESTestCase;

public class MyTests extends ESTestCase {
    public void testThatTest() {
        assertTrue(true);
    }
}
```

Now run `gradle clean check` and get the following results

```
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.12
  OS Info               : Mac OS X 10.11.4 (x86_64)
  JDK Version           : Oracle Corporation 1.8.0_60 [Java HotSpot(TM) 64-Bit Server VM 25.60-b23]
:clean
:compileJava UP-TO-DATE
:processResources UP-TO-DATE
:classes UP-TO-DATE
:checkstyleMain UP-TO-DATE
:copyPluginPropertiesTemplate
:pluginProperties
:compileTestJava
:processTestResources UP-TO-DATE
:testClasses
:checkstyleTest

FAILURE: Build failed with an exception.

* What went wrong:
Cannot convert URL 'jar:file:/Users/alr/.gradle/caches/modules-2/files-2.1/org.elasticsearch.gradle/build-tools/5.0.0-alpha1/54bc79f5078ae3dba1fd67e4753be412726c01b9/build-tools-5.0.0-alpha1.jar!/checkstyle.xml' to a file.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 6.663 secs
```

I will open follow up bugs for `dependencyLicenses` and `thirdPartyAudit` once I fully understood what they do.
</description><key id="151322820">18002</key><summary>External Plugins: checkstyle is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>v5.0.0-alpha3</label></labels><created>2016-04-27T08:02:22Z</created><updated>2016-04-27T14:00:59Z</updated><resolved>2016-04-27T14:00:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-27T13:49:48Z" id="215089405">&gt; I will open follow up bugs for dependencyLicenses and thirdPartyAudit once I fully understood what they do.

I _thought_ @jaymode fixed this a few weeks ago? Sadly, we don't have a great way to test these.
</comment><comment author="jaymode" created="2016-04-27T14:00:58Z" id="215092694">#17720 fixed this for checkstyle. It was broken for alpha1 but I see it as working with manually installed snapshots of the build project and the rest of this repo: 

`cd buildSrc &amp;&amp; gradle install &amp;&amp; cd .. &amp;&amp; gradle install`

I did not need to touch the dependencyLicenses or thirdPartyAudit for the project I was working on, but that may be an artifact of that project not having any additional dependencies.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Service does not start.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18001</link><project id="" key="" /><description>ES 2.3.1
JDK 1.8.0.91
Windows 10

C:\elasticsearch-2.3.1\bin&gt;service.bat install
Installing service      :  "elasticsearch-service-x64"
Using JAVA_HOME (64-bit):  "C:\Program Files\Java\jdk1.8.0_91"
The service 'elasticsearch-service-x64' has been installed.

C:\elasticsearch-2.3.1\bin&gt;service.bat start
Failed starting 'elasticsearch-service-x64' service

Service start failed

[2016-04-27 06:57:42] [info] [1680] Starting service ...
[2016-04-27 06:57:42] [error] [1680] Failed creating java %JAVA_HOME%\jre\bin\server\jvm.dll
[2016-04-27 06:57:42] [error] [1680] The system can not find the path specified.
[2016-04-27 06:57:42] [error] [1680] ServiceStart returned 1
[2016-04-27 06:57:42] [error] [1680] The system can not find the path specified.
[2016-04-27 06:57:42] [info] [3540] Run service finished.
[2016-04-27 06:57:42] [info] [3540] Commons Daemon procrun finished
[2016-04-27 06:57:44] [error] [3896] Failed to start 'elasticsearch-service-x64' service
[2016-04-27 06:57:44] [error] [3896] The data area passed to a system call is too small.
[2016-04-27 06:57:44] [info] [3896] Start service finished.
[2016-04-27 06:57:44] [error] [3896] Commons Daemon procrun failed with exit value: 5 (Failed to start service)
[2016-04-27 06:57:44] [error] [3896] The data area passed to a system call is too small.

All fine if i change HKEY_LOCAL_MACHINE\SOFTWARE\WOW6432Node\Apache Software Foundation\Procrun 2.0\elasticsearch-service-x64\Parameters\Java\Jvm from %JAVA_HOME%\jre\bin\server\jvm.dll to C:\Program Files\Java\jdk1.8.0_91\jre\bin\server\jvm.dll
</description><key id="151294666">18001</key><summary>Elastic Service does not start.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AlexKovynev</reporter><labels /><created>2016-04-27T04:07:36Z</created><updated>2016-10-05T04:48:58Z</updated><resolved>2016-04-28T15:44:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AlexKovynev" created="2016-04-28T15:44:25Z" id="215472174">Sorry my fault. JAVA_HOME set not in system settings.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index_buffer_size cannot be set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18000</link><project id="" key="" /><description>I added this setting

`indices.memory.index_buffer_size: "50%` to elasticsearch.yaml file but I get exception on ES start.
ES 5.0 alpha taken from ES site.

```
[2016-04-27 01:42:37,727][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: unknown setting [indices.memory.index_buffer_size]
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:267)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
        at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:120)
        at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
        at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
        at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:103)
        at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:148)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:229)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:161)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:189)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```
</description><key id="151265449">18000</key><summary>index_buffer_size cannot be set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels /><created>2016-04-26T23:45:20Z</created><updated>2017-05-09T08:16:00Z</updated><resolved>2016-04-26T23:59:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="prog8" created="2016-04-26T23:58:11Z" id="214924202">Maybe related to this commit: https://github.com/elastic/elasticsearch/commit/e91a141233d0231fa3f43320968686b602d414e3
</comment><comment author="jasontedor" created="2016-04-26T23:59:20Z" id="214924398">Thanks for reporting! This is fixed by #17778. I'll add you to the [pioneer program](https://www.elastic.co/blog/elastic-pioneer-program).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a Sort ingest processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17999</link><project id="" key="" /><description>An ingest processor that sorts an array of values in ascending or descending order. 

If all elements are numerics, they will be sorted numerically. If values are strings, or mixtures of strings/numbers, the elements will be sorted lexicographically.

I started to implement the OpenRefine fingerprint stuff, but decided the individual components would be more useful as standalone processors (sort, dedupe, etc).  Thoughts?  If this isn't seen as useful individually, I'm happy to close and move this code into a single processor.
- ~~The sorting comparator gave me a really hard time.  I couldn't figure out a way to sort the generic list with a chained reverse without specifically setting it up per-type.  Would love to know if there is a better way, this is really gross!~~
- I used an enum for asc/desc, because type safety.  But enums are so verbose and clunky, maybe it wasn't worth it?
</description><key id="151238776">17999</key><summary>Add a Sort ingest processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Ingest</label><label>feature</label><label>v5.0.0-alpha3</label></labels><created>2016-04-26T21:04:55Z</created><updated>2016-05-17T16:06:58Z</updated><resolved>2016-05-17T16:06:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-04-26T21:14:33Z" id="214888561">You tried using https://docs.oracle.com/javase/7/docs/api/java/util/Collections.html#reverseOrder%28java.util.Comparator%29 ?
</comment><comment author="polyfractal" created="2016-04-26T21:34:42Z" id="214893907">Doh.  That, and making the list's wildcard extend Comparable fixed my prior issues.  Thanks! :)
</comment><comment author="martijnvg" created="2016-04-28T08:39:49Z" id="215353539">Left some minor comments. We should also add a rest integration test that verifies that this processor can actually be used in a pipeline on a real node. (this catches things like, forgetting to register the factory or if a processor requires special permissions that these permission are granted)
We do this also for all the other processors. (for example check the yaml test for the `foreach` processor).

Other than that this PR looks good!
</comment><comment author="polyfractal" created="2016-05-16T16:11:11Z" id="219467953">@martijnvg Updated the PR based on comments while you were away, lemme know what you think when you get a chance (no rush though!) :)
</comment><comment author="martijnvg" created="2016-05-17T08:58:20Z" id="219659211">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow reindex API to move documents instead of just copying</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17998</link><project id="" key="" /><description>A use case we see a lot with users is that they want to move some data out of one index to another. Would it be possible to combine the `reindex` with `delete-by-query` essentially? After a document is indexed in the target index a delete operation will be issued on the source index.

Of course this couldn't be done atomically, but even on best effort basis this would be super useful for a lot of people - essentially executing `reindex` and `delete-by-query` at the same time (on the same point in time snapshot of the index) with no additional guarantees than those two operations have individually.
</description><key id="151238370">17998</key><summary>Allow reindex API to move documents instead of just copying</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:Reindex API</label><label>discuss</label><label>enhancement</label></labels><created>2016-04-26T21:02:50Z</created><updated>2016-05-06T09:45:58Z</updated><resolved>2016-05-06T09:45:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-28T17:41:47Z" id="215506361">@HonzaKral I'm struggling to see how this would be useful, especially when dealing with the complexity of both documents existing (or neither document existing) for a period.  Could you elaborate on use cases?
</comment><comment author="HonzaKral" created="2016-04-29T21:11:54Z" id="215883042">The use case I have in mind is a migration from a single index and aliases to separate index. Let's assume you store user-generated data in one index and use aliases to give the app an illusion of an index-per-user architecture. Now one user proves to be too big to live in a common index and needs to be put into it's own index. With aliases it is very easy to do, but you need to move the data - copy them to the newly created index and delete them from the old one. You can do it with `reindex` and `delete-by-query` but making it into one command would be nicer and would also minimize the discrepancies during the process (where a document exists in both old and new indices).

Similar as #17997 it can be viewed as a generalization of the reindex api to allow for _any_ bulk operation - not just index, but also a delete/update and possibly against different indices.

In client code, using `scan`/`bulk` combination this can be simply achieved by adjusting the code generating the bulk request (both the action and data lines).
</comment><comment author="clintongormley" created="2016-05-02T10:47:24Z" id="216196349">Aliases make the transition atomic.  Doing this doc-by-doc (besides being a much heavier operation) would result in moments when either the same doc is visible in both indices or is visible in neither index (because of the differences in refresh times).  This makes life more complex for the user, rather than less.
</comment><comment author="HonzaKral" created="2016-05-02T12:37:43Z" id="216224449">Well, with aliases you have several options, each with its set of problems, none of those are really atomic - switch when the empty index is created and then wait for the reindex to populate it, in this case your user sees missing data for a long time.

Other option is to point the alias to both indices, but then you have to have a separate write alias and you still need to solve moving the data - if you use reindex then you will start seeing duplicates until all the data is copied and then you remove the original index from the alias. During the transition updates can be also problematic.

Or you can first copy the data and then switch the alias. Here you just have to keep track of all the documents that have changed after the reindex operation began, apply those updates and only then switch the alias. This can be also problematic.

None of the options are atomic and there is always a room for discrepancy unless you want to make the application aware of this mechanics, which can mean a lot of code and complexity for a few transitions.

This solution is by no means perfect, maybe even not better than the ones described, but it is simplest to implement and idempotent. I also agree that it would be difficult to manage expectations.

Another use case would be to help with entity-centric indexing (#17997) where you can just run a "move" with update periodically.
</comment><comment author="s1monw" created="2016-05-06T09:45:58Z" id="217399306">We should not add features that suggest a certain behavior like atomicity of a move. As clinton said, building this means that you could see 1, 2 or even 0 results for a given document querying the two indices (source and target). The bigger issue that I see with this is that you potentially be stuck with 2 indices both half broken. I think reindexing should be always be able to trash the target index and don't loose data. We discussed this in a wider audience and decided to close it for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow reindex to do update/upsert operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17997</link><project id="" key="" /><description>The idea is to provide additional functionality to the `reindex` API to allow update on the target index except of only index operations.

My use case for this is entity-centric indexing - imagine you have an index containing events and wish to group them by session. With the `reindex` api it should be possible to read the source events, apply a script (or just extract a field) to get the ID of a target document and pass it as a parameter to a specified update script.
</description><key id="151237892">17997</key><summary>Allow reindex to do update/upsert operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2016-04-26T21:00:19Z</created><updated>2017-05-04T16:26:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-28T17:36:38Z" id="215504898">@HonzaKral i had similar ideas for the reindex API way back when, but I'm not convinced that this will be enough for practical entity-centric indexing (but would be happy to be proven wrong).

Do you have some practical real-world examples of how you would use this?
</comment><comment author="HonzaKral" created="2016-04-29T21:04:12Z" id="215881355">My example is web server logs -&gt; web sessions. The update script would go like:

```
session.start = min(session.start, event.timestamp)
session.end = max(session.end, event.timestamp)
session.length = session.end - session.start
if event.user:
    session.username = event.user.name
    session.subscribed = event.user.subscribed
...
```

Where `session` is the aggregated doc and `event` is the single event. This is a fairly simple example that could easily be done and would be very useful in real life.
</comment><comment author="clintongormley" created="2016-05-02T10:42:33Z" id="216195286">Makes sense.  It wouldn't be super-simple fitting this into the reindex functionality because reindex gets a document, but the example you provide would actually need to receive the document as a parameter to a script, and it would need to handle upserts as `scripted_upsert`.

I'm wondering if reindex is the right place for this, or if we can think of a better dedicated API which makes this job easier.
</comment><comment author="dmarkhas" created="2017-05-04T08:52:11Z" id="299129712">Are there any plans to incorporate this in a future release?
We also have a use case for doing updates with reindex, since we're trying to accomplish something similar to SQL Server merge process.

We have an index of raw ingested data, and an index of "processed" data where the processing is really just merging the inbound logs by some key (a field or a set of fields).
Right now we have to read the data out of the raw data index (with logstash or Spark or whatever), and write it back in to the processed index with the update API, which is very wasteful (there are some reasons why we can't ingest data directly into the processed index).
It would be nice if reindex could implement an update functionality, not just overwriting the existing documents.</comment><comment author="nik9000" created="2017-05-04T16:26:59Z" id="299237906">I'm not planning on working on this, no.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow `null` values in settings updates - see Issue #17995</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17996</link><project id="" key="" /><description>A first stab at #17995. 
</description><key id="151232218">17996</key><summary>Allow `null` values in settings updates - see Issue #17995</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Wedjaa</reporter><labels /><created>2016-04-26T20:32:54Z</created><updated>2016-04-28T17:52:37Z</updated><resolved>2016-04-27T09:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-27T09:06:52Z" id="215019511">Hi @Wedjaa 

The ability to delete settings has been implemented in 5.0 as part of a really big settings refactoring.  We couldn't support this before as there was no way of resetting a setting to its default value.  This was an enormous change and is not backportable.

Btw, the example you give in https://github.com/elastic/elasticsearch/issues/17995 wouldn't be allowed as it would break the analysis of existing data in the index.

Thanks for your PR, but we're not going to make these changes in 2.x
</comment><comment author="Wedjaa" created="2016-04-27T11:14:57Z" id="215052585">  Hi,

  I knew about 5.0 - that's why the PR was targeted at the 2.3 branch. In 2 you can already change the analysis settings, like adding a language to the lowercase filter in the example, or even change the type of a filter -- but you can't remove settings to an analyzer that you have added. The consistency of the analysis settings can be broken - by changing them - but you can't change them back, by removing what you added. Is that by design or should it be considered a bug? This PR was trying to male the behavior consistent.
</comment><comment author="clintongormley" created="2016-04-28T17:52:37Z" id="215509381">&gt; This PR was trying to male the behavior consistent.

The problem is that it won't work for most settings (as we don't have the defaults available), which makes things inconsistent.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow defining settings values to `null` when updating settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17995</link><project id="" key="" /><description>The idea is that right now when you update a setting that has multiple attributes you have no way of removing the old values - since ES ignores null values when updating settings. In practice:

```
{
   "settings": {
      "analysis": {
         "filter": {
             "lowercase_filter": {
                "type": "lowercase",
                "language": "greek"
             }
         }
      }
   }
}
```

these will create a lowercase filter for the `greek` language. If I want to go back to using the standard language if I try and `PUT` the following settings:

```
{
   "settings": {
      "analysis": {
         "filter": {
             "lowercase_filter": {
                "type": "lowercase"
             }
         }
      }
   }
}
```

the resulting settings in ES will not have changed - the missing `language` attribute would be simply merged from the previous version of the settings.

Explicitly setting the value to `null`

```
{
   "settings": {
      "analysis": {
         "filter": {
             "lowercase_filter": {
                "type": "lowercase",
                "language": null
             }
         }
      }
   }
}
```

has no effect - since the parses is currently simply ignoring null values. 

At the core of this there is the issue that currently the settings are being held in an immutable map that doesn't accept null values.

I have a PR that does the following:
- Create a `Setting` object that holds a setting value; I could have used an Optional - but I wanted to be  compatible with JDKs &lt; 1.8;
- The Settings map will contain a map of Setting, allowing the loading of `null` values;
- The loader will correctly load null values into the settings;
- In the `Settings` builder there are two main changes: putting in settings will do a merge with the following logic: if the new value is `null` and an old value exists the key will be removed - and all the subkeys - otherwise we will insert the null in the settings.

Is there any other person interested in this PR?

Fabio
</description><key id="151231796">17995</key><summary>Allow defining settings values to `null` when updating settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Wedjaa</reporter><labels /><created>2016-04-26T20:31:01Z</created><updated>2016-04-27T09:07:26Z</updated><resolved>2016-04-27T09:07:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-27T09:07:26Z" id="215019682">closing - see https://github.com/elastic/elasticsearch/pull/17996#issuecomment-215019511
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add new IPv6 types to docs where it's supported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17994</link><project id="" key="" /><description>Feature was completed in https://github.com/elastic/elasticsearch/pull/17746
This should close https://github.com/elastic/elasticsearch/issues/17993
</description><key id="151209301">17994</key><summary>Add new IPv6 types to docs where it's supported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>docs</label></labels><created>2016-04-26T18:41:34Z</created><updated>2016-04-29T13:00:32Z</updated><resolved>2016-04-29T13:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T18:45:40Z" id="214845804">I'd like @jpountz to confirm that fuzziness works as documented for the new IP fields before merging
</comment><comment author="jpountz" created="2016-04-26T20:10:18Z" id="214871101">Good catch it does not. I can fix it but this makes me wonder whether we want it to work this way. I am wondering if it is useful at all since you would need to pass absurdly high values for it to be useful on ipv6 addresses: since there are 128 bits for addresses, you would need to pass 2&lt;sup&gt;64&lt;/sup&gt; to match all ip addresses in the same `/64` for instance.
</comment><comment author="clintongormley" created="2016-04-27T09:01:38Z" id="215018241">&gt;  I can fix it but this makes me wonder whether we want it to work this way. I am wondering if it is useful at all since you would need to pass absurdly high values for it to be useful on ipv6 addresses: since there are 128 bits for addresses, you would need to pass 264 to match all ip addresses in the same /64 for instance.

Yeah.  I have no idea how much this is used, my gut says not a lot.  Possibly we should change the semantics for fuzzy on IP to say that (eg) `~16` behaves like netmask `/16`?  Would that make any sense?
</comment><comment author="jpountz" created="2016-04-27T09:23:45Z" id="215024704">Given that term queries already support the netmask notation (if you run a term query on `::1/16`, it will match all ip addresses in the same `/16` as `::1` as expected), I'm not sure it would be useful. I'm wondering that we should just raise an exception if you run a fuzzy query on an ip field saying that it is not supported?
</comment><comment author="clintongormley" created="2016-04-27T11:28:45Z" id="215055004">&gt;  I'm wondering that we should just raise an exception if you run a fuzzy query on an ip field saying that it is not supported?

++
</comment><comment author="eskibars" created="2016-04-27T13:23:57Z" id="215081642">I'm +1 to deprecate now and remove shortly, but -1 for removing support immediately for 5.0.  I don't have a good sense for how many people are using this style of query currently, but it's another thing that the deprecation logging we've been pushing in 2.3 won't be able to catch and would make upgrading harder.
</comment><comment author="jpountz" created="2016-04-27T14:56:19Z" id="215110138">I opened #18016
</comment><comment author="eskibars" created="2016-04-27T15:09:15Z" id="215114294">I pulled the fuzziness bit out to revert the docs there to IPv4 only
</comment><comment author="rjernst" created="2016-04-27T19:11:33Z" id="215196910">&gt; I'm +1 to deprecate now and remove shortly, but -1 for removing support immediately for 5.0. I don't have a good sense for how many people are using this style of query currently

Not removing esoteric things simply because the feature existed, but without any evidence that someone is using it, leads to complex legacy code that stays around for far longer than it should. We should remove this feature, it is only a burden on us and doesn't actually help users. It is very simple if there was someone using it to get an error that says "use a CIDR mask instead".
</comment><comment author="eskibars" created="2016-04-27T19:39:21Z" id="215204885">&gt; Not removing esoteric things simply because the feature existed, but without any evidence that someone is using it, leads to complex legacy code that stays around for far longer than it should. We should remove this feature, it is only a burden on us and doesn't actually help users. It is very simple if there was someone using it to get an error that says "use a CIDR mask instead".

Fundamentally, I agree with your statement as long as we have enough ways to derive that evidence as to whether a particular feature is being used or not and to what extent.  As of right now, I don't believe we do, and the fact that it's in the docs as a specific example of fuzzy leaves me especially cautious.  In lieu of any evidence either direction, I'm personally a proponent of airing on the side of "deprecate and then remove," carrying that code burden for a short time rather than flash removal.
</comment><comment author="rjernst" created="2016-04-27T19:49:05Z" id="215207333">&gt;  I'm personally a proponent of airing on the side of "deprecate and then remove," carrying that code burden for a short time rather than flash removal.

But we are not talking about a short time. We are talking about keeping it around for an entire major release, for something that shouldn't be a "feature" in the first place. Things like this hold back progress, because they require keeping apis in line with the "old" way of doing things. For example, we have tons of these methods of MappedFieldType that are really geared towards term dictionary based fields. It would be great to rework the field types so just text/keyword fields have them. But keeping this fuzzy query around for ip means holding back on any improvements there. It also means that _new_ users will potentially fall into this trap of using this crazy "fuzzy" query on ip fields, instead of cidr masks like they should. And this last point is really important: we should lean on the side of removing things if it improves clarity of apis for new users.
</comment><comment author="jpountz" created="2016-04-27T20:33:38Z" id="215219271">The argument that resonates most to me is that if anyone is using a fuzzy query on anything but a string/keyword/text field, it is more likely a mistake than on purpose. Since I cannot recall of any github issue or discuss post talking about this feature, I am leaning towards removing it.
</comment><comment author="rmuir" created="2016-04-28T13:37:36Z" id="215425063">What is supposed to happen if a user uses the `~` operator on an ipv6 address? This isn't like ancient 1980s ipv4 where you could infer a "default" netmask or something.

Personally, I think we should focus on the real use cases (that is unarguably prefix query for network addresses). Means making sure it works in queryparsers/aggregations/wherever its supposed to work. 

Things like fuzzy/wildcards on ip addresses should just be an error message and we shouldn't spend valuable time on that, time that could be spent making ip addresses work well instead.
</comment><comment author="eskibars" created="2016-04-28T14:51:32Z" id="215451355">These are great conversations and I think we should have them, but buried in a docs PR is probably not the best place for them.  I pulled the fuzzy update because we shouldn't have docs that don't reflect reality, whatever that reality happens to be.  If we're happy with what remains here, we can merge and then carry on the other conversations here outside of this PR
</comment><comment author="jpountz" created="2016-04-28T15:04:42Z" id="215455554">+1 to merge the PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs for IPv6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17993</link><project id="" key="" /><description>**Elasticsearch version**: master/5.0.0-alpha2

I notice we mention IPv4 several places that the new IPv6 type works:
~~https://www.elastic.co/guide/en/elasticsearch/reference/master/common-options.html#fuzziness~~
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html#_specialised_datatypes
</description><key id="151208410">17993</key><summary>Docs for IPv6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>:Mapping</label><label>adoptme</label><label>docs</label><label>v5.0.0-alpha3</label></labels><created>2016-04-26T18:37:10Z</created><updated>2016-04-29T13:00:32Z</updated><resolved>2016-04-29T13:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Ongoing Painless Improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17992</link><project id="" key="" /><description>Ongoing:
- More Tests -- Any tests added on their own. (#18538) (#18560)
- Any Bug Fixes -- Any and all bug fixes. (#18871) (elastic/x-plugins/issues/2587)
- General Usability - Improve syntax for usability. (#18241) (#18287) (elastic/x-plugins/pull/2225)
- Better Dev - Improve ease of development. (#18288) (#18350) (#18463) (#18951) (#19459)
- Def Performance - Improve performance for dynamic types. 
  (#18151) (#18201) (#18232) (#18234) (#18359) (#18379) (#18425) (#18849) (#18867) (#18899) (#18958)
- General Performance - Improve overall performance. (#18247) (#18262) (#18264) (#18284)
- Grammar Changes -- Any and all grammar changes. (#18531) (#18827) (#18931) (#19092)
- Better Error Messages - Make error messages bubble to the top level.  Make better error messages for syntax errors caught during the ANTLR lexing/parsing phases. (#18298) (#18319) (#18600) (#18711)
- Anything Else - Any other changes not falling into a category. (#19936)

Documentation:
- [ ] Split Pages: Painless has a large feature set, so it needs to be split into multiple pages.
- [ ] New Examples: Painless documentation needs new examples.
- [ ] Updated Examples: Update the script examples used by other pieces of Elasticsearch to use Painless instead. (elastic/x-plugins/pull/2327) (elastic/x-plugins/pull/2449) (elastic/x-plugins/pull/2571)

Tasks:
- [x] Date API - Add a Date API. (#18621) (#18658)
- [x] Internal AST -- Decouple ANTLR from Painless to allow for other possible uses of the compilation unit. (#18286)
- [x] Generics - Remove all generics for simplicity.
- [x] Overload - Allow for arity overloaded methods/constructors. (#18385)
- [x] Regular Expressions - Add some form of regular expressions. (#18842) (#18858) (#19070)
- [x] Strings - Allow single-quoted strings to be used. (#18150)
- [x] Foreach - Allow the foreach shortcut in for loops. (#18757)
- [x] Properties - Expose properties for the different field types including geopoint support (#18169)
- [x] .value - Add .value for field access. (#18169) 
- [x] Improved API -- Any updates to the whitelist. (#18372) (#18533)
- [x] Colon Syntax -- Add method pointers using colon syntax.
- [x] Lambda Functions -- Anything lambda related. (#18578) (#18748) (#18818) (#18824) (#18828) (#18911) (#18924) (#18954) (#18983) (#19003)
- [x] Custom Functions -- Add user defined functions. (#18810)
- [x] Type Initializers -- Add initializers for arrays, lists, and maps. (#19012)
- [x] Instanceof Operator -- Add an instanceof operator. (#19065)
</description><key id="151184283">17992</key><summary>Ongoing Painless Improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>Meta</label></labels><created>2016-04-26T16:48:26Z</created><updated>2017-05-22T15:43:48Z</updated><resolved>2017-05-22T15:43:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-04-26T16:49:10Z" id="214808148">I am currently making progress on the internal AST.

Edit: This is still in progress.

Edit: Done!
</comment><comment author="clintongormley" created="2016-05-04T07:37:21Z" id="216767307">@jdconrad are you going to expose the methods/properties listed here? https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-scripting.html#_document_fields
</comment><comment author="jdconrad" created="2016-05-04T16:49:59Z" id="216928737">@clintongormley I'll add it to the list.
</comment><comment author="jdconrad" created="2016-05-05T16:31:49Z" id="217202736">Single-quoted strings is in (https://github.com/elastic/elasticsearch/pull/18150).

@rmuir Has checked in some code to improve def performance (https://github.com/elastic/elasticsearch/pull/18151), but there is still more that can be done, so I'm going to leave it up on the list.
</comment><comment author="jdconrad" created="2016-05-06T00:11:20Z" id="217315376">@rmuir has done .value + perf improvements. (#18169)
</comment><comment author="clintongormley" created="2016-05-10T09:02:04Z" id="218099561">Here's an example of an exception that could be improved.  In groovy, if you run:

```
PUT foo/bar/1
{}

POST foo/bar/1/_update
{
  "upsert": {},
  "script": {
    "lang": "groovy",
    "inline": "ctx._source.foo.bar = 18"
  }
}
```

You get:

```
{
  "error": {
    "root_cause": [
      {
        "type": "remote_transport_exception",
        "reason": "[Coachwhip][127.0.0.1:9300][indices:data/write/update[s]]"
      }
    ],
    "type": "illegal_argument_exception",
    "reason": "failed to execute script",
    "caused_by": {
      "type": "script_exception",
      "reason": "failed to run inline script [ctx._source.foo.bar = 18] using lang [groovy]",
      "caused_by": {
        "type": "null_pointer_exception",
        "reason": "Cannot set property 'bar' on null object"
      }
    }
  },
  "status": 400
}
```

If you do the same in painless:

```
PUT foo/bar/1
{}

POST foo/bar/1/_update
{
  "upsert": {},
  "script": {
    "lang": "painless",
    "inline": "input.ctx._source.foo.bar = 18"
  }
}
```

You get the same NPE but with less context, which makes it harder to debug:

```
{
  "error": {
    "root_cause": [
      {
        "type": "remote_transport_exception",
        "reason": "[Coachwhip][127.0.0.1:9300][indices:data/write/update[s]]"
      }
    ],
    "type": "illegal_argument_exception",
    "reason": "failed to execute script",
    "caused_by": {
      "type": "null_pointer_exception",
      "reason": null
    }
  },
  "status": 400
}
```
</comment><comment author="jdconrad" created="2016-05-11T23:31:39Z" id="218620310">PR out for internal AST (#18286)
</comment><comment author="rmuir" created="2016-05-11T23:48:58Z" id="218622919">@clintongormley i don't think we should hurt performance to try to be like groovy. If we want, we can be slow like groovy as well. In this case, the biggest problem is that the stacktrace is completely lost. We have good stacktraces which tell you what is going on, but ES throws them away. So I don't think its a painless problem, and I dont think we should be trying to wrap and wrap them to hack around this issue.
</comment><comment author="jdconrad" created="2016-05-12T00:17:22Z" id="218627111">Added links to the numerous performance PR's by @rmuir 
</comment><comment author="uschindler" created="2016-05-19T22:55:16Z" id="220474385">@jdconrad can you also add PR #18232 (DEF performance), #18241 (usability)?
</comment><comment author="jdconrad" created="2016-05-19T23:01:30Z" id="220475400">@uschindler Added those to the list.  Sorry, there's been quite a few lately, so those two slipped through the cracks.
</comment><comment author="jdconrad" created="2016-05-24T00:16:49Z" id="221132970">Large API additions by @rmuir (#18533)
</comment><comment author="ppf2" created="2016-06-01T02:21:28Z" id="222875376">Can we also add a print function so we can print out the values of variables for easier debugging purposes?  This is possible using groovy and other languages.
</comment><comment author="jdconrad" created="2016-06-01T04:40:44Z" id="222890891">@ppf2 How are you doing this with Groovy?
</comment><comment author="ppf2" created="2016-06-01T05:19:35Z" id="222895307">With groovy, you can print info like the following as part of a script and it gets written to the console.

```
        "inline": "print 'hello';"
```
</comment><comment author="uschindler" created="2016-06-01T08:48:50Z" id="222931360">I'd not add printing to stdout. I think something like `print` or, better, `log` should go to a logger with the script name as logging class. The logger could be added as a static field to every script class and used then by some alias mapping.
</comment><comment author="rmuir" created="2016-06-01T09:19:57Z" id="222938488">I don't think scripts should do any logging or I/O at all. Perhaps groovy does that, but groovy caused a whole lot of problems due to what it does.

In a lot of ways, if groovy does something, we might want to investigate doing the opposite.
</comment><comment author="s1monw" created="2016-06-01T10:03:41Z" id="222948830">&gt; I don't think scripts should do any logging or I/O at all. Perhaps groovy does that, but groovy caused a whole lot of problems due to what it does.

+1 to not do any IO here! can we support asserts maybe then we can debug via exception and asserts?
</comment><comment author="jdconrad" created="2016-06-01T16:25:43Z" id="223047843">+1 to @s1monw suggestion.  You can throw several different types of exceptions right now in the same way that Java does including IllegalArgumentException and IllegalStateException.
</comment><comment author="nik9000" created="2016-06-11T17:18:50Z" id="225375812">&gt; Regular Expressions - Add some form of regular expressions.

I think something like this would be useful for using painless during reindex/update. No a good choice during search scripts, but such is life. We could even disable it by default in that context.

Anyway, if we have special syntax I think we should copy some language that folks are used to so that we can tell people "it should work just like X" and then we have an automatic spec. That being said I'd like to propose we pick between groovy's syntax, perl's syntax, or something else I haven't thought of. Or having no special regex syntax.

This is my memory of groovy's regex syntax:
- `/` is a string delimiter.
- `~STRING` is sugar for `Pattern.compile(STRING)`.
- `lhs=STRING =~ rhs=STRING` is sugar for `Pattern.compile(rhs).matcher(lhs)`.
- `lhs=STRING ==~ rhs=STRING` is sugar for `Pattern.compile(rhs).matcher(lhs).matches().
- Flags live in the front of the pattern and look like `(?i)`
- Groovy's matchers are Iterable in a kind of messy way I don't think we should copy at all/without some modification.

I like the groovy choice because it is fairly explicit (less magic) and maps fairly easily to Java. And if people are using regexes now with groovy it should mostly "just work".

It looks like perl's syntax is:
- `/` is a special string delimiter that builds patterns
- flags go after the last `/`
- `=~` returns a boolean (or some truthy thing, 5 minutes of internet research isn't an exhaustive study) on match and sets special `$_` and `$1`, `$2`, etc...

I think perl's regex is much more popular than Groovy's syntax and it is fairly "natural". Maybe because we're all so used to it. I think the disadvantage of it is that it doesn't map as easily to java with the magic variables. OTOH we can absolutely make them work because we're writing the language.

Maybe because I don't know it as well, I think that perl's regex syntax is deeper and we'd have trouble ever saying "painless's regexes are just like perl" because there is so much magic in the language around regexes. Also, our regexes would be java regexes, not perl regexes.
</comment><comment author="nik9000" created="2016-06-11T18:33:00Z" id="225382180">Also, I think that implementing goovy's regex syntax would be a lot easier. But I if we decide we really want some glorious syntax we shouldn't limit ourselves to groovy.
</comment><comment author="rmuir" created="2016-06-11T19:27:51Z" id="225389006">I think it should only be `/regex/`, which acts a static final Pattern constant. We can add some of the groovy operators too to make regexes easier, but otherwise add all of java.util.regex, except ability to Pattern.compile. So no computed Strings at all. It means regexes will never compile for every document and so on.
</comment><comment author="jdconrad" created="2016-06-11T19:30:00Z" id="225389121">+1 to having all regexes be static constants.  Otherwise, it will be insanely slow for per-document operations.
</comment><comment author="uschindler" created="2016-06-12T00:41:41Z" id="225402530">+1 to have static final constants. The parser should just collect all those regexes and then add them as private static fields + `&lt;clinit&gt;` after the main execute method was written (see #18828 where we implemented this already). Just see it as "constant pool" of patterns.
</comment><comment author="nik9000" created="2016-06-13T16:10:57Z" id="225629283">&gt; The parser should just collect all those regexes and then add them as private static fields + &lt;clinit&gt; after the main execute method was written

I took a crack at it this weekend and made #18842. It is far from perfect but generally does what @uschindler described.
</comment><comment author="nik9000" created="2016-06-14T14:21:15Z" id="225896464">I opened #18858 to pick up `=~` and `==~` from groovy. You can call `/pattern/.matcher(str).matches()` if you like but `str ==~ /pattern/` feels much more natural to me given all the regex work I've done with groovy in the past month or two.

I think this is required before we can tick the regex box:
- Flags support after the pattern. It'd look like `/pattern/i`.

I'd like to do two more things because I feel like they'd be useful:
- Support syntax like `(str =~ /bar/).somemethod()`
- It'd be super lovely to have a replaceAll that takes a closure. So if `str`=`cat` then `(str =~ /bar/).replaceAll(m -&gt; m.group().toUpperCase(Locale.ROOT))`=`cAt`. This kind of thing is fairly useful for stuff like normalizing addresses which I see as something someone might want to do with the update-by-query API. I think this is a long ways out because I don't think painless has closures, only method refs?
</comment><comment author="rmuir" created="2016-06-14T15:46:34Z" id="225924885">&gt; I think this is a long ways out because I don't think painless has closures, only method refs?

Well technically we have a stub for lambdas: not closures, there is a difference :)
I think its something we can hack out in a day or so honestly. This is only syntactic sugar at the moment, the stuff we need is there. I had planned to work on it but took a break from that stuff to try to improve the math performance.  Will look in fairly soon as that stuff should be winding down.

Thanks for tackling the regex issues! 
</comment><comment author="nik9000" created="2016-06-16T21:52:20Z" id="226624720">I'm going to call regexes "done" for now. We have regex syntax, flags, and matcher syntax. And documentation for all of those things.
</comment><comment author="nik9000" created="2016-06-24T18:05:37Z" id="228417820">&gt; It'd be super lovely to have a replaceAll that takes a closure.

I opened #19070 for this.
</comment><comment author="EricMCornelius" created="2017-05-20T06:00:42Z" id="302853343">Based on the discussion here, am I correct in thinking it's not possible to pass a regex parameter into a painless script?

Is there any discussion of exposing a context initialization phase, so that we can do things like compile dynamic/parameterized regexes which are passed via the context for use in scripts without the overhead mentioned above?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When backing up to NFS snapshots get stuck indefinately</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17991</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.1

**JVM version**: Oracle jdk1.8.0_60

**OS version**: Red Hat Enterprise Linux Server release 7.2 (Maipo) 
Linux hostname.example.com 3.10.0-327.el7.x86_64 #1 SMP Thu Oct 29 17:29:29 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:
Intermittently elasticsearch will never complete the snapshot in progress. The only way to fix it is to do a rolling restart of all data nodes. The "stuck" nodes will take longer to restart under systemd and will leave a zombie java process like this: 
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
elastic+ 13067 35.4  0.0      0     0 ?        Zl   Apr16 5407:11 [java] &lt;defunct&gt; 

They will also abandon the normal working directory and startup under a new node directory like 
cluster/nodes/1/ instead of cluster/nodes/0/ 

The only way to get elasticsearch to start up under the correct node directory is to restart the server which is not desirable. 

**Steps to reproduce**:
1. Install elasticsearch on multiple nodes with a shared nfs volume. 
2. Run snapshots (Our snapshots are usually backing up about 3 TB with about 100 GB new data each day ) Snapshots fail within 2 weeks to a month usually. 

I have also seen larger one time snapshots fail in this same manner. 
</description><key id="151170349">17991</key><summary>When backing up to NFS snapshots get stuck indefinately</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dforste</reporter><labels><label>:Snapshot/Restore</label><label>feedback_needed</label></labels><created>2016-04-26T15:46:44Z</created><updated>2016-11-21T14:03:00Z</updated><resolved>2016-05-24T10:37:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T18:38:39Z" id="214843650">@ywelsch any ideas?
</comment><comment author="ywelsch" created="2016-04-27T09:20:22Z" id="215023935">@dforste Are there any warnings in the logs? Can you provide thread dumps of the nodes that are stuck? What happens if you delete the snapshot that is stuck? Does that unblock the nodes?
</comment><comment author="dforste" created="2016-04-27T15:44:06Z" id="215125711">@ywelsch I have tried deleting it before but that doesn't work either. Only rebooting/restarting the nodes tells me which one is stuck via the defunct java process. I cant find my thread dumps from before so I will have to wait a bit and I will try to get you some. 

Elasticsearch continues doing everything else while this is happening. Only seems like snapshot and restore are affected. 
</comment><comment author="ywelsch" created="2016-04-27T16:14:37Z" id="215135121">How is the nfs volume mounted? It could be that the mount options are at fault: See e.g. `soft` vs. `hard` mount options here:
http://unix.stackexchange.com/questions/31979/stop-broken-nfs-mounts-from-locking-a-directory

&gt; hard or soft &#8212; Specifies whether the program using a file via an NFS connection should stop and wait (hard) for the server to come back online, if the host serving the exported file system is unavailable, or if it should report an error (soft).
&gt; 
&gt; If hard is specified, the user cannot terminate the process waiting for the NFS communication to resume unless the intr option is also specified.
&gt; 
&gt; If soft is specified, the user can set an additional timeo=&lt;value&gt; option, where &lt;value&gt; specifies the number of seconds to pass before the error is reported.
</comment><comment author="clintongormley" created="2016-05-24T10:37:06Z" id="221231006">No further feedback.  Closing
</comment><comment author="dforste" created="2016-06-03T19:02:09Z" id="223665247">Sorry for not responding earlier. I had an issue where stdout was going to dev/null. Also adjusting snapshots to only snapshot the last week helped tremendously. I am attaching threadumps for the data nodes involved with the snapshot and the current master. 

[threadump-6-3-2016.tar.gz](https://github.com/elastic/elasticsearch/files/298217/threadump-6-3-2016.tar.gz)

Here are all the nfs mount options set in fstab: 
`auto,rw,bg,vers=3,intr,soft,noatime,actimeo=300,nosuid,bg,rsize=32768,wsize=32768`

Node 1 was the one that was left in a defunct state and needed to reboot. 
</comment><comment author="schonfeld" created="2016-11-20T16:53:43Z" id="261789707">@dforste did you ever resolve this? I believe we're seeing something similar in v2.4.1... Any insight would be great!
</comment><comment author="dforste" created="2016-11-20T23:52:28Z" id="261816136">@schonfeld I did not ever get it fully resolved. I did however minimize how often it happened by minimizing the amount we snapshot to just the previous week. I was doing all the indexes in one job but that is a bit too much. </comment><comment author="schonfeld" created="2016-11-21T14:03:00Z" id="261946153">@dforste got it, thanks! fwiw, we just setup a [minio](https://www.minio.io/) instance, and are using Elasticsearch's standard S3 snapshot plugin. Seems to be working very well!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed the documentation formatting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17990</link><project id="" key="" /><description>The source code example for the initial example was missing the correct JSON object formatting and syntax.  That has been fixed with my change. 

Also, I removed the additional unnecessary curl statement.
</description><key id="151169833">17990</key><summary>Fixed the documentation formatting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">josefsalyer</reporter><labels><label>docs</label></labels><created>2016-04-26T15:44:33Z</created><updated>2016-04-29T16:07:28Z</updated><resolved>2016-04-29T16:07:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T18:37:06Z" id="214843207">Hi @josefsalyer 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="josefsalyer" created="2016-04-28T15:22:36Z" id="215463943">Done!

Just a note - this the second time I had to sign it. Apparently, it did not
like my affirmation of my email?

Josef

On Tuesday, April 26, 2016, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @josefsalyer https://github.com/josefsalyer
&gt; 
&gt; Thanks for the PR. Please could I ask you to sign the CLA so that I can
&gt; merge it in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17990#issuecomment-214843207

## 

---

Josef Salyer
@josefsalyer
http://linked.in/josefsalyer
C: 480.286.5875
</comment><comment author="karmi" created="2016-04-29T09:47:45Z" id="215672269">@josefsalyer, our systems rely on matching emails between the CLA, your Github profile, and the Git commit information -- I can see you have signed on 2016-04-26, and the systems matched you properly. Thank you!
</comment><comment author="clintongormley" created="2016-04-29T16:07:28Z" id="215782197">thanks @josefsalyer 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Smoke tester: Fix new x-pack configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17989</link><project id="" key="" /><description>In case of x-pack the superuser role is required.
Also the code for extracting the host was simplified
Lastly the keyserver was replaced as the current one is being
unresponsive.
</description><key id="151156301">17989</key><summary>Smoke tester: Fix new x-pack configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label></labels><created>2016-04-26T14:52:25Z</created><updated>2016-04-28T12:34:32Z</updated><resolved>2016-04-28T12:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-04-28T12:32:24Z" id="215409219">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Duplicated colon was removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17988</link><project id="" key="" /><description>Hope this helps :-)
</description><key id="151123557">17988</key><summary>Duplicated colon was removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rafalbig</reporter><labels><label>docs</label></labels><created>2016-04-26T12:42:29Z</created><updated>2016-04-27T07:04:57Z</updated><resolved>2016-04-26T18:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T18:30:38Z" id="214841253">thanks @rafalbig 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Persistent Node Ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17987</link><project id="" key="" /><description>Nodes are currently identified in the cluster by IDs that are randomly generated during node startup. That means they change every time the node is restarted (i.e. they correspond to process ids). While this doesn't matter for ES proper, it makes it hard for external services to track nodes. Persistent node ids ensure that the same id can be reused across restarts, facilitating tracking of nodes in node stats / cluster state APIs.

In contrast to #17811 (which this PR supersedes), this PR keeps process ids around for the code that relies on its semantics (mainly discovery-related parts).
</description><key id="151123230">17987</key><summary>Persistent Node Ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>blocker</label><label>breaking</label><label>enhancement</label><label>review</label></labels><created>2016-04-26T12:41:04Z</created><updated>2016-06-30T13:24:57Z</updated><resolved>2016-06-30T10:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-27T15:38:04Z" id="215123853">@ywelsch I've given it a first pass and left some comments regarding the node local storage setting and just calling it node ID instead of persistent node ID. I'll do a more thorough review after those are addressed (especially the latter one since it's going to touch a lot of places).
</comment><comment author="ywelsch" created="2016-04-28T16:27:38Z" id="215485627">@jasontedor I've added support for choosing a node id for tribe node clients in a deterministic way and renamed the persistentNodeId fields. Please have another look.
</comment><comment author="ywelsch" created="2016-06-30T10:17:04Z" id="229619293">superseded by #19140
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wait for changes to be visible by search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17986</link><project id="" key="" /><description>This adds support for setting the `refresh` request parameter to `wait_for` in the `index`, `delete`, `update`, and `bulk` APIs. When `refresh=wait_for` is set those APIs will not return until their results have been made visible to search by a refresh.

Also it adds a `forced_refresh` field to the response of `index`, `delete`, `update`, and to each item in a `bulk` response. This will be `true` for requests with `?refresh` or `?refresh=true` and will be `true` for some requests (see below) with `refresh=wait_for` but ought to otherwise always be `false`.

`refresh=wait_for` is implemented as a list of `Tuple&lt;Translog.Location, Consumer&lt;Boolean&gt;&gt;`s in the new `RefreshListeners` class that is managed by `IndexShard`. The dynamic, index scoped `index.max_refresh_listeners` setting controls a maximum number of listeners allowed in any shard. If more than that many listeners accumulate in the engine then a refresh will be forced, the thread that adds the listener will be blocked until the refresh completes, and then the listener will be called with a `forcedRefresh` flag so it knows that it was the "straw that broke the camel's back". These listeners are only used by `refresh=wait_for` and that flag manifests itself as `forced_refresh` being `true` in the response.

About half of this change comes from piping async-ness down to the appropriate layer in a way that is compatible with the ongoing with with sequence ids.
# Edit

Rewritten to reflect changes made as part of the discussion.
## Original

This adds the `block_until_refresh` request parameter to the `index`, `delete`, `update`, and `bulk` APIs. When set those APIs will not return until their results have been made visible to search by a `refresh`.

Also it adds a `forced_refresh` element to the response of `index`, `delete`, `update`, and to each item in a `bulk` response. This will be `true` for requests marked as `refresh` and will be `true` for some requests (see below) with `block_until_refresh` but ought to otherwise always be false.

`block_until_refresh` is implemented as a `LinkedTransferQueue` of `RefreshListener`s in `InternalEngine`. The dynamic, index scoped `index.max_refresh_listeners` setting controls a maximum number of listeners allowed in any shard. If more than that many listeners accumulate in the engine then a refresh will be force, the thread that adds the listener will be blocked until the refresh completes, and then the listener will be called with a `forcedRefresh` flag so it knows that it was the "straw that broke the camel's back". These listeners are only used by `block_until_refresh` and that flag manifests itself as `forced_refresh` being `true` in the response.

Since `LinkedTransferQueue`'s `size` method is linear time `InternalEngine` uses a `volatile int` field to try and guess the size. There are threading issues with this but that shouldn't matter because undercounting the number of listeners or overcounting them seems like it is unlikely to cause too much trouble.

Closes #1063
</description><key id="151121794">17986</key><summary>Wait for changes to be visible by search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:CRUD</label><label>breaking</label><label>release highlight</label><label>v5.0.0-alpha4</label></labels><created>2016-04-26T12:34:15Z</created><updated>2016-11-02T17:13:04Z</updated><resolved>2016-06-06T15:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-26T12:36:39Z" id="214725391">This is ready for some review but isn't "ready". It has a few open questions and I need to run some REST tests and write some documentation.

The first 12 commits tell the story of my piping asynchronous-ness into the right places so that we can actually use the `RefreshListener` for this. The rest of the commits implement the listener.
</comment><comment author="nik9000" created="2016-04-26T12:37:01Z" id="214725474">@jasontedor we spoke about this yesterday. You have enough on your plate for review but maybe you know another person who can look at this?
</comment><comment author="clintongormley" created="2016-04-26T18:29:25Z" id="214840897">I'd like @s1monw to look at this before it is merged
</comment><comment author="nik9000" created="2016-04-26T18:42:17Z" id="214844717">&gt; I'd like @s1monw to look at this before it is merged

It can certainly wait for him.
</comment><comment author="jasontedor" created="2016-04-27T17:39:56Z" id="215165865">&gt; I'd like @s1monw to look at this before it is merged

I also think that because of the replication action changes that a non-empty subset of @bleskes, @ywelsch and myself should take a careful look.
</comment><comment author="nik9000" created="2016-04-27T17:43:21Z" id="215167254">Sure! I add a few more allocations to that path to handle the async-ness. One latches I added I'm not 100% sure that it is needed - tests fail without it but maybe they can switch to assertBusy or something.
</comment><comment author="s1monw" created="2016-05-02T12:26:38Z" id="216222343">@nik9000 left a bunch of comments - thanks for doing this :)
</comment><comment author="nik9000" created="2016-05-10T14:21:59Z" id="218172616">Had a chat with @bleskes and he recommended simplifying the way that blocking is done - instead of passing a listener to the primary action we should instead return a WriteResult from that and start the listener afterwords if we need to. This should greatly simplify some of the code. Great.
</comment><comment author="nik9000" created="2016-05-20T19:15:46Z" id="220694576">@bleskes and @s1monw and whoever else is interested (@jasontedor I think?) I've gone through most of the feedback. I'm going to finish the rest up soon so this is probably ready for another round.

39 commits just looks like a lot. It isn't _that_ big.
</comment><comment author="s1monw" created="2016-05-20T20:53:21Z" id="220715842">I like this!! left some comments. @nik9000 what happens if the refresh interval is set to -1 and we never get refreshes? Should we force refresh if we don't have async refresh?
</comment><comment author="nik9000" created="2016-05-20T20:58:40Z" id="220716978">&gt; I like this!!

Hurray!

&gt; @nik9000 what happens if the refresh interval is set to -1 and we never get refreshes?

The request hangs until enough of them pile up and we force a refresh.

&gt; Should we force refresh if we don't have async refresh?

Maybe we should just fail the request?
</comment><comment author="nik9000" created="2016-05-20T21:06:45Z" id="220718628">&gt; Maybe we should just fail the request?

I dunno. I don't think we could catch that in all situations - like if they set refresh_interval to -1 concurrently with an index that does this then maybe we couldn't see it until it was too late. I'm kind of tempted to just let the request hang and document it.
</comment><comment author="nik9000" created="2016-05-20T21:11:00Z" id="220719449">So we don't lose one of @s1monw's points when I push another patch - I should find a better name than `AsyncStash`. It is a type parameter that I've used for the data that has to go from the sync action to the async action. For the TransportReplicatedMutationActions it is a `WriteResult` and everywhere else it is `Void`.
</comment><comment author="nik9000" created="2016-05-24T02:08:54Z" id="221147888">I pushed some changes based on a conversation with @bleskes. So far it isn't stable.
</comment><comment author="nik9000" created="2016-05-24T02:20:53Z" id="221149294">@bleskes actually, this is ok. So it is pretty much what you started with your patch to me, the biggest difference is that I kept TransportReplicatedMutationAction and renamed it to TransportWriteAction. I did that because it made a very very good home for WriteResult and WritePrimaryResult.
</comment><comment author="nik9000" created="2016-05-24T13:14:31Z" id="221264362">Just a note - what is here now looks pretty stable but fails subtly. Hunting.
</comment><comment author="nik9000" created="2016-05-24T13:31:17Z" id="221269213">&gt; Just a note - what is here now looks pretty stable but fails subtly. Hunting.

Fixed it I think. I was being silly.
</comment><comment author="nik9000" created="2016-05-24T14:59:46Z" id="221299104">@bleskes can you have a look and see if I broke your ideas too badly?
</comment><comment author="bleskes" created="2016-05-26T19:55:29Z" id="221976775">@nik9000 I went through the replication part and it looks great. Left some very minor comments. One thing I was missing is some integration test for the translog part of the TRA. This functionality was there before so I expected to find the tests somewhere, but I can't.
</comment><comment author="nik9000" created="2016-05-31T18:18:41Z" id="222774716">I'm adding the `breaking` label to this because it changes `refresh`'s behavior slightly. Without this PR Elasticsearch will accept stuff like `refresh=1` to mean `refresh=true`. With this PR it only accepts `refresh=true`, `refresh=false`, and `refresh=wait_for`. I'm going to wait to add the breaking change note until after the review is mostly done and I'm able to merge master into this branch. This was forked from master before master had a breaking changes file for document level actions.
</comment><comment author="nik9000" created="2016-05-31T22:05:51Z" id="222835627">I've gone back through this and addressed the inline comments. The big thing left from my side is that I have to reread that PR as a whole and if it still hangs together in my head. I'll also want to look at testing - @bleskes mentioned missing integration testing for TransportReplicationAction which sounds right to me.
</comment><comment author="clintongormley" created="2016-06-01T08:00:22Z" id="222920638">&gt; With this PR it only accepts refresh=true, refresh=false, and refresh=wait_for

Does it still accept `?refresh` to meant `?refresh=true`?

This is a breaking change which could affect all the clients (but an easy one to fix)
</comment><comment author="s1monw" created="2016-06-01T08:46:21Z" id="222930731">I left some suggestions and minor comments. The changes on the internal things like IndexShard, Engine etc. LGTM! Thanks @nik9000 for going through all these rounds with us. I really appreciate the work here
</comment><comment author="nik9000" created="2016-06-01T20:21:02Z" id="223112598">I've found two problems:
1. I did the "fire all listeners in one runnable" thing without enough care.
2. There is some kind of race condition that happens when you don't send in multiple changes.

First one is easy to fix, second one reproduces consistently so I'll get it.
</comment><comment author="nik9000" created="2016-06-01T20:51:05Z" id="223120595">&gt; 1. There is some kind of race condition that happens when you don't send in multiple changes.

This is caused by writes happening during the refresh. If you speed up the refresh interval then it is fairly common. The writes are made visible but the refresh listener doesn't see them. If you don't make more writes after that then we just wait forever.
</comment><comment author="nik9000" created="2016-06-01T21:44:36Z" id="223134381">&gt; This is caused by writes happening during the refresh. If you speed up the refresh interval then it is fairly common. The writes are made visible but the refresh listener doesn't see them. If you don't make more writes after that then we just wait forever.

Handled in 30f972b. Maybe not handled that gracefully, but I think the tradeoff I make there is acceptable.
</comment><comment author="nik9000" created="2016-06-01T21:51:30Z" id="223135956">&gt; Does it still accept ?refresh to meant ?refresh=true?

I'll make sure that still works, sure. I like that behavior and it ought to stay.
</comment><comment author="s1monw" created="2016-06-02T14:39:27Z" id="223312360">@nik9000 I reviewed the engine, indexshard part again LGTM
</comment><comment author="nik9000" created="2016-06-02T17:19:01Z" id="223360064">&gt; @nik9000 I reviewed the engine, indexshard part again LGTM

Thanks!
</comment><comment author="nik9000" created="2016-06-02T17:25:04Z" id="223361803">I'd added this in a line comment but it is now a comment "on an outdated diff" so I'll preserve the gist of it here:

I've deprecated `setRefresh(boolean)` rather than removed it outright. @bleskes left a reply to that saying something like "we can just remove it". I'm ok with removing it pre-5.0 and listing the change as one of the 5.0 breaking changes. I'd prefer not to do it in this PR because it is a bunch of small mechanical changes. If we're going to remove that API I'd like to do it after I've merged this PR in a followup. This PR is big enough without lots of cleanup in getting in the way.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to set path style access for AWS S3 client. Usefull for so&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17985</link><project id="" key="" /><description>Add option to set path style access for AWS S3 client. Needed for some ceph installationme ceph installation
</description><key id="151107910">17985</key><summary>Add option to set path style access for AWS S3 client. Usefull for so&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awislowski</reporter><labels /><created>2016-04-26T11:21:05Z</created><updated>2016-09-09T06:11:47Z</updated><resolved>2016-09-09T06:11:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-26T11:38:01Z" id="214713131">Thank you for the proposal. I think it's a duplicate of #15114 though.
</comment><comment author="awislowski" created="2016-04-26T11:44:57Z" id="214714389">That's true, but mu proposal changes less api of InternalAwsS3Service class.

On Tue, Apr 26, 2016 at 1:39 PM, David Pilato notifications@github.com
wrote:

&gt; Thank you for the proposal. I think it's a duplicate of #15114
&gt; https://github.com/elastic/elasticsearch/pull/15114 though.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17985#issuecomment-214713131
</comment><comment author="awislowski" created="2016-04-27T09:15:00Z" id="215022687">@dadoonet I'd like to mention that it is prepared for 2.3 not master to have it for next 2. versions
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update keyword.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17984</link><project id="" key="" /><description>This should probably read "keyword" here.
</description><key id="151107406">17984</key><summary>Update keyword.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>docs</label><label>v5.0.0-alpha3</label></labels><created>2016-04-26T11:18:33Z</created><updated>2016-04-27T10:11:29Z</updated><resolved>2016-04-27T10:11:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-04-27T07:57:45Z" id="215000117">LGTM
</comment><comment author="cbuescher" created="2016-04-27T10:11:29Z" id="215039552">a2c3b5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix DateFieldType.isFieldWithinQuery:</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17983</link><project id="" key="" /><description>- PointValues.size throws an exception if there is no value indexed for the field.
</description><key id="151103349">17983</key><summary>Fix DateFieldType.isFieldWithinQuery:</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-04-26T10:56:34Z</created><updated>2016-04-28T17:54:11Z</updated><resolved>2016-04-27T12:55:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-26T15:01:15Z" id="214774244">@jimferenczi what is the exception being thrown? I think it should return 0 rather than throw an exception?
</comment><comment author="jimczi" created="2016-04-26T15:24:54Z" id="214782059">@jpountz this is a IllegalArgumentException: https://github.com/apache/lucene-solr/blob/f6c7fc7a26584c92a81b3a6cbca179ca232808a9/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsReader.java#L123

It's an edge case where the mapping exists in es but the fieldinfo does not exist in any of the Lucene segments. 
</comment><comment author="jpountz" created="2016-04-26T16:36:58Z" id="214804755">Thanks. I was under the (wrong!) assumption that `PointValues.size(field)` returns zero if the field does not have points. I opened https://issues.apache.org/jira/browse/LUCENE-7257 to discuss what `PointValues.size` should do with leaves that do not have points.
</comment><comment author="jimczi" created="2016-04-27T12:55:41Z" id="215073104">I've opened a more generic PR: #18011  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dubious equality test in RoutingNodes.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17982</link><project id="" key="" /><description>The following equality check in [core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java](https://github.com/elastic/elasticsearch/blob/543b27f4cda099966879a192b019418f94fd379b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java#L400) (line 400) looks a bit fishy:

``` java
if (unassignedShard.index().equals(index)) {
```

Note that `unassignedShard.index()` is of type `Index`, while `index` is of type `String`, so I don't think this test can ever evaluate to `true`.
</description><key id="151100830">17982</key><summary>Dubious equality test in RoutingNodes.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xiemaisi</reporter><labels /><created>2016-04-26T10:44:34Z</created><updated>2016-04-26T11:57:13Z</updated><resolved>2016-04-26T11:57:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-26T11:48:55Z" id="214715126">This method is invoked in tests only, and none of the tests ever invoke the method with `state` containing `ShardRoutingState.UNASSIGNED`. Thus, this code is currently never triggered. That said, this code should be fixed now in case it ever is triggered. Thanks for noticing this. I'll push a fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use MatchNoDocsQuery in MatchNoneQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17981</link><project id="" key="" /><description>We should probably use the MatchNoDocsQuery introduced with #17780 in MatchNoneQueryBuilder where we currently use an empty BooleanQuery.
</description><key id="151098722">17981</key><summary>Use MatchNoDocsQuery in MatchNoneQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label></labels><created>2016-04-26T10:32:13Z</created><updated>2016-04-28T11:13:51Z</updated><resolved>2016-04-28T11:13:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Extend field stats to report searchable/aggregatable fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17980</link><project id="" key="" /><description>- Add isSearchable and isAggregatable (collapsed to true if any of the instances of that field are searchable or aggregatable).
- Accept wildcards in field names.
- Add a section named conflicts for fields with the same name but with incompatible types (instead of throwing an exception).

This closes #17750
</description><key id="151096432">17980</key><summary>Extend field stats to report searchable/aggregatable fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Stats</label><label>feature</label><label>v5.0.0-alpha3</label></labels><created>2016-04-26T10:19:12Z</created><updated>2016-05-04T14:44:22Z</updated><resolved>2016-04-27T15:18:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-26T17:42:24Z" id="214823967">I left some comments but I think this is close!
</comment><comment author="jimczi" created="2016-04-27T12:36:25Z" id="215068577">Thanks @jpountz I've pushed another commit.
</comment><comment author="jpountz" created="2016-04-27T14:20:35Z" id="215098985">I left another comment regarding the visibility of these new isSearchable/isAggregatable methods. Otherwise it looks good to me.
</comment><comment author="jimczi" created="2016-04-27T15:19:03Z" id="215117924">Thanks @jpountz
I've change the visibility of the new methods in MappedFieldType. 
Merged https://github.com/elastic/elasticsearch/commit/f600c4ab9c4a575a269287bb5aaa22d828c56496
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging: Add executable check to RPM init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17979</link><project id="" key="" /><description>The RPM init script did not include the check for `bin/elasticsearch` being
executable. This fix adds this checks and makes the tests pass.

This fixes the init script, so the tests pass on centos-6 again.
</description><key id="151090307">17979</key><summary>Packaging: Add executable check to RPM init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>review</label><label>test</label></labels><created>2016-04-26T09:49:08Z</created><updated>2016-04-26T09:55:50Z</updated><resolved>2016-04-26T09:55:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-26T09:50:57Z" id="214687741">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update percolate-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17978</link><project id="" key="" /><description>1. Update existing document percolate query example to use the same document type as the first example
2. Remove `document_type` from existing document percolate query (the existing document type is specified with `type` property)
</description><key id="151088652">17978</key><summary>Update percolate-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">russcam</reporter><labels /><created>2016-04-26T09:40:52Z</created><updated>2016-04-26T12:42:49Z</updated><resolved>2016-04-26T12:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="russcam" created="2016-04-26T12:42:49Z" id="214727424">Going to close this - I'm running against 5.0.0-alpha1 (with the changes to `percolate` query taken into account). I'm seeing `document_type` not required for an existing document search, but required for a multi search. Looking at https://github.com/elastic/elasticsearch/blob/81449fc91291b09f545873f331495b82fab4ff5b/core/src/main/java/org/elasticsearch/index/query/PercolateQueryBuilder.java#L124, it's a required parameter for both an existing document and a document passed in the query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix error message `script.inline: on`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17977</link><project id="" key="" /><description>Incorrect error message when setting wrong script parameters.

**Elasticsearch version**: 2.3.1
docker pull elastic:latest

**OS version**: Linux 05932458a9d5 4.2.0-35-generic #40~14.04.1-Ubuntu SMP Fri Mar 18 16:37:35 UTC 2016 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:

Incorrect error message when setting wrong script parameters in elasticsearch.yml

According to [current documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html) it should be `script.inline: true`and not `script.inline: on`

**Steps to reproduce**:
1. echo "script.disable_dynamic: false" &gt;&gt; config/elasticsearch.yml
2. Restart elastic
3. Elastic crashes

Note: I know that the parameter is wrong but the error message is wrong, too.

**Provide logs (if relevant)**:

```
Exception in thread "main" java.lang.IllegalArgumentException: script.disable_dynamic is not a supported setting, replace with fine-grained script settings.
Dynamic scripts can be enabled for all languages and all operations by replacing `script.disable_dynamic: false` with `script.inline: on` and `script.indexed: on` in elasticsearch.yml
```
</description><key id="151086110">17977</key><summary>Fix error message `script.inline: on`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bluepuma77</reporter><labels /><created>2016-04-26T09:28:27Z</created><updated>2016-04-26T12:14:50Z</updated><resolved>2016-04-26T11:57:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-26T11:57:53Z" id="214716791">In 2.x, `on` can be used as a synonym for `true` when setting the script mode. In fact, any of `true`, `1`, `on`, or `yes` can be used. This has been removed in 5.0.0 and only `true` will be accepted.

Relates #16197
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove trailing / in rest spec for ingest.simulate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17976</link><project id="" key="" /><description>The url that takes an id has a trailing forward slash, not really an
error but as its the only url in the whole spec that does this it
triggered my OCD :)
</description><key id="151079227">17976</key><summary>Remove trailing / in rest spec for ingest.simulate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-26T08:57:04Z</created><updated>2016-04-26T18:13:43Z</updated><resolved>2016-04-26T09:03:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-26T09:01:28Z" id="214675679">LGTM thanks @Mpdreamz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing duplicated parenthese open</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17975</link><project id="" key="" /><description>removing duplicated parenthese open
</description><key id="151075886">17975</key><summary>Removing duplicated parenthese open</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeesim2</reporter><labels><label>:Aggregations</label><label>docs</label></labels><created>2016-04-26T08:39:53Z</created><updated>2016-04-26T18:13:14Z</updated><resolved>2016-04-26T18:13:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T18:13:14Z" id="214833680">thanks @jeesim2 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch.yml example is outdated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17974</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
Version: 5.0.0-alpha1, Build: 7d4ed5b/2016-04-04T10:39:25.841Z, JVM: 1.8.0_91

**JVM version**:
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**:
Linux elasticsearch 3.13.0-83-generic #127-Ubuntu SMP Fri Mar 11 00:25:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:
$ELASTICSEARCH_HOME/config/elasticsearch.yml needs to be updated to spec.

It shows that the format for `network.host` is `network.host: 192.168.1.19` but that does not work. The proper format is `network.host: ["192.168.1.19"]`

```
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
# network.host: 192.168.1.19
#
# Set a custom port for HTTP:
#
# http.port: 9200
```

**Steps to reproduce**:
1. Set `elasticsearch.yml` to `network.host: 192.168.1.19`
2. Start `elasticsearch`
3. Java error occurs

**Provide logs (if relevant)**:

/opt/elasticsearch/elasticsearch-5.0.0-alpha1/logs/elasticsearch.log

```
java.lang.RuntimeException: max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:79)
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:60)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:187)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```

**Describe the feature**:
Make sure config files, example configs, and documentation all match up. Otherwise this was a 23 minute headache that should have been avoided.
</description><key id="151071083">17974</key><summary>elasticsearch.yml example is outdated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">naisanza</reporter><labels /><created>2016-04-26T08:14:19Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-04-26T20:19:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="naisanza" created="2016-04-26T08:16:58Z" id="214664185">https://github.com/elastic/elasticsearch/issues/15340 gave me the idea to use a quoted array/list, which is how I got it to work
</comment><comment author="jasontedor" created="2016-04-26T10:17:00Z" id="214693883">I'm sorry that you experienced pain here. However, you do not have to set `network.host` to an array, setting it as a single value is accepted. Note the error message that you're getting is about a failing bootstrap check. These are checks that are being added in Elasticsearch 5.0.0 to increase the resiliency of production configurations of Elasticsearch. A production configuration is an instance of Elasticsearch that binds to or publishes to a non-loopback network interface. Since the non-array `network.host` setting was correctly applied, the bootstrap checks are executed and that's why you're seeing the error message:

```
java.lang.RuntimeException: max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]
```

It's telling you exactly what the problem is: you need to increase the number of available file descriptors to 65536.

Can you explain what led you to believe that the problem was not having the `network.host` setting set as an array? On my system I can do the following:

```
$ cat ./config/elasticsearch.yml | grep network.host
network.host: ["192.168.1.105", "127.0.0.1"]
$ ./bin/elasticsearch | grep bound_addresses
[2016-04-26 06:14:33,592][INFO ][transport                ] [Bloodhawk] publish_address {192.168.1.105:9300}, bound_addresses {192.168.1.105:9300}, {127.0.0.1:9300}
[2016-04-26 06:14:36,679][INFO ][http                     ] [Bloodhawk] publish_address {192.168.1.105:9200}, bound_addresses {192.168.1.105:9200}, {127.0.0.1:9200}
^C
```

```
$ cat ./config/elasticsearch.yml | grep network.host
network.host: 192.168.1.105
$ ./bin/elasticsearch | grep bound_addresses
[2016-04-26 06:15:32,977][INFO ][transport                ] [Senator Robert Kelly] publish_address {192.168.1.105:9300}, bound_addresses {192.168.1.105:9300}
[2016-04-26 06:15:36,028][INFO ][http                     ] [Senator Robert Kelly] publish_address {192.168.1.105:9200}, bound_addresses {192.168.1.105:9200}
^C
```

and in either case Elasticsearch starts up just fine.
</comment><comment author="naisanza" created="2016-04-26T11:10:35Z" id="214705091">Hi! Even with `max file descriptors` aside elasticsearch won't start after that java error.

For example:

`network.host: _site_`:

```
root@elasticsearch_running:~# netstat -ant
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
```

`network.host: "_site_"`:

```
root@elasticsearch_running:~# netstat -ant
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
```

`network.host: ["_site_"]`:

```
root@elasticsearch_running:~# netstat -ant
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 10.0.3.85:9200          :::*                    LISTEN
tcp6       0      0 10.0.3.85:9300          :::*                    LISTEN
```

`network.bind_host: 10.0.3.85`:

```
root@elasticsearch_running:~# netstat -ant
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
```

`network.bind_host: ["10.0.3.85"]`:

```
root@elasticsearch_running:~# netstat -ant
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 10.0.3.85:9200          :::*                    LISTEN
tcp6       0      0 10.0.3.85:9300          :::*                    LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
```
</comment><comment author="jasontedor" created="2016-04-26T11:29:25Z" id="214710961">&gt; Hi! Even with max file descriptors aside elasticsearch won't start after that java error.

Can you please share the log messages from Elasticsearch failing to start?
</comment><comment author="naisanza" created="2016-04-26T19:28:33Z" id="214859428">@jasontedor sure! I've attached all the logs from `$ES_HOME/logs/`

Only elasticsearch.log contained anything:

```
[2016-04-26 19:22:26,605][ERROR][bootstrap                ] Exception
java.lang.RuntimeException: max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:79)
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:60)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:187)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```

```
root@elasticsearch_running:~# du -sh /opt/elasticsearch/elasticsearch-5.0.0-alpha1/logs/*
0       /opt/elasticsearch/elasticsearch-5.0.0-alpha1/logs/elasticsearch_deprecation.log
0       /opt/elasticsearch/elasticsearch-5.0.0-alpha1/logs/elasticsearch_index_indexing_slowlog.log
0       /opt/elasticsearch/elasticsearch-5.0.0-alpha1/logs/elasticsearch_index_search_slowlog.log
4.0K    /opt/elasticsearch/elasticsearch-5.0.0-alpha1/logs/elasticsearch.log
```

Attached log files:
[elasticsearch.zip](https://github.com/elastic/elasticsearch/files/237303/elasticsearch.zip)
</comment><comment author="jasontedor" created="2016-04-26T19:32:35Z" id="214860501">@naisanza Sorry, I don't understand what those logs are showing me since you didn't share the configuration nor the command-line parameters used to start Elasticsearch. Would it be possible for you to show from the command-line and configuration that is giving you trouble and the log messages that Elasticsearch displays on the console when it fails to start? 
</comment><comment author="naisanza" created="2016-04-26T19:38:24Z" id="214862086">@jasontedor sorry about that. A very minimal setup

rc.local:

```
root@elasticsearch_running:~# cat /etc/rc.local
#!/bin/sh -e
#
# rc.local
#
# This script is executed at the end of each multiuser runlevel.
# Make sure that the script will "exit 0" on success or any other
# value on error.
#
# In order to enable or disable this script just change the execution
# bits.
#
# By default this script does nothing.

su - eric -c "elasticsearch &amp; disown %"

exit 0
```

elasticsearch.yml:

```
root@elasticsearch_running:~# grep -v "^#" /opt/elasticsearch/elasticsearch-5.0.0-alpha1/config/elasticsearch.yml
network.host: _site_
```

from the shell:

```
eric@elasticsearch_running:~$ elasticsearch
Exception in thread "main" java.lang.RuntimeException: max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:79)
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:60)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:187)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
eric@elasticsearch_running:~$
```
</comment><comment author="jasontedor" created="2016-04-26T19:48:59Z" id="214864851">But `network.host: _site_` means that you're binding to a non-loopback interface (site-local) and that subjects the instance to the bootstrap checks that we discussed [previously](https://github.com/elastic/elasticsearch/issues/17974#issuecomment-214693883). The error message tells you that you're still running into the file descriptor issue. This has nothing to do with `network.host` being an array versus not being an array.
</comment><comment author="naisanza" created="2016-04-26T19:56:00Z" id="214867237">@jasontedor when it's set to `["_site_"]` it starts

elasticsearch.yml:

```
eric@elasticsearch_running:~$ grep -v "^#" /opt/elasticsearch/elasticsearch-5.0.0-alpha1/config/elasticsearch.yml
network.host: ["_site_"]
```

elasticsearch starts:

```
eric@elasticsearch_running:~$ elasticsearch
[2016-04-26 19:55:25,643][WARN ][bootstrap                ] max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]
[2016-04-26 19:55:25,644][WARN ][bootstrap                ] please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
[2016-04-26 19:55:25,668][INFO ][node                     ] [Trevor Fitzroy] version[5.0.0-alpha1], pid[693], build[7d4ed5b/2016-04-04T10:39:25.841Z]
[2016-04-26 19:55:25,669][INFO ][node                     ] [Trevor Fitzroy] initializing ...
[2016-04-26 19:55:26,294][INFO ][plugins                  ] [Trevor Fitzroy] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-04-26 19:55:26,334][INFO ][env                      ] [Trevor Fitzroy] using [1] data paths, mounts [[/ (/home/eric/.local/share/lxc/base-ubuntu/rootfs)]], net usable_space [57.4gb], net total_space [78.6gb], spins? [possibly], types [overlayfs]
[2016-04-26 19:55:26,335][INFO ][env                      ] [Trevor Fitzroy] heap size [990.7mb], compressed ordinary object pointers [true]
[2016-04-26 19:55:29,636][INFO ][node                     ] [Trevor Fitzroy] initialized
[2016-04-26 19:55:29,637][INFO ][node                     ] [Trevor Fitzroy] starting ...
[2016-04-26 19:55:29,765][INFO ][transport                ] [Trevor Fitzroy] publish_address {10.0.3.85:9300}, bound_addresses {10.0.3.85:9300}
[2016-04-26 19:55:32,879][INFO ][cluster.service          ] [Trevor Fitzroy] new_master {Trevor Fitzroy}{MprWNSUcQB2GhSTwXp1h7A}{10.0.3.85}{10.0.3.85:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-04-26 19:55:32,994][INFO ][http                     ] [Trevor Fitzroy] publish_address {10.0.3.85:9200}, bound_addresses {10.0.3.85:9200}
[2016-04-26 19:55:32,995][INFO ][node                     ] [Trevor Fitzroy] started
[2016-04-26 19:55:33,033][INFO ][gateway                  ] [Trevor Fitzroy] recovered [0] indices into cluster_state
```
</comment><comment author="jasontedor" created="2016-04-26T20:15:34Z" id="214872473">Okay, thank you for that! I attempted to reproduce this on master and I was not able to. That put me down the path of thinking about what has changed in the bootstrap checks between alpha1 and master. A key change is how these bootstrap checks detect whether or not you're bound to a non-loopback interface. In particular, in alpha1 we merely checked to see if the `network.host` setting is set or not. When you set `network.host: _site_` then that setting is clearly set. However, when you set `network.host: ["_site_"]` that doesn't actually set `network.host`. Instead, it sets `network.host.0`, `network.host.1`, ..., `network.host.n` up to `n`, the number of the elements in the array minus one. In alpha1 we were not checking for any `network.host.k` settings, just `network.host`. This is why you're encountering this issue. In master since #17595 we do this very differently. Instead, we bind to the configured network interfaces and then check whether or not any of those bindings are to non-loopback interfaces. This indirectly eliminated this painful issue.

To be clear, network settings are fine not as an array, or as an array. The issue here is a bug that was causing the bootstrap checks to be skipped when the network settings were configured as an array.

I'm _truly_ sorry that you ran into this pain and appreciate your repeated feedback as we worked through this one.

@naisanza I'll mark you as being eligible for the [pioneer program](https://www.elastic.co/blog/elastic-pioneer-program). Thanks again for the report!
</comment><comment author="naisanza" created="2016-04-26T20:52:28Z" id="214882388">@jasontedor no problem! Pioneer Program sounds awesome
</comment><comment author="alexanderkjeldaas" created="2016-04-28T08:21:46Z" id="215348704">This exception happens on the official docker hub elasticsearch:5 image.
</comment><comment author="jasontedor" created="2016-04-28T10:06:37Z" id="215375864">There is not an official Docker image that is affiliated with Elastic. To be clear, the "official" Docker image on Docker Hub is not affiliated with Elastic.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add date_index_name processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17973</link><project id="" key="" /><description>The processor added in this PR dynamically redirects a document to an index based on a provided index name prefix, a date or timestamp field in the document being processed and the provided date rounding.

PR for #17814
</description><key id="151069311">17973</key><summary>Add date_index_name processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>feature</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-26T08:03:53Z</created><updated>2016-04-29T15:24:02Z</updated><resolved>2016-04-29T15:24:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-29T10:10:02Z" id="215676395">@talevy I made `index_name_prefix` an optional setting.
</comment><comment author="talevy" created="2016-04-29T14:23:56Z" id="215733718">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document breaking changes for completion and context suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17972</link><project id="" key="" /><description>breaking changes for https://github.com/elastic/elasticsearch/pull/14410 
</description><key id="151067810">17972</key><summary>Document breaking changes for completion and context suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>docs</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-26T07:55:12Z</created><updated>2016-04-26T14:17:46Z</updated><resolved>2016-04-26T14:17:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T09:18:20Z" id="214680228">Small suggestions, other than that LGTM. Thanks for the good writeup
</comment><comment author="areek" created="2016-04-26T14:17:46Z" id="214760152">Thanks @clintongormley for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not all features work on ip fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17971</link><project id="" key="" /><description>Since we switched to points to index ip addresses, a couple things are not working anymore:
- [x] range queries only support inclusive bounds (#17777)
- [x] range aggregations do not work anymore (#17859)
- [x] sorting on ip addresses fails since it tries to write binary bytes as an utf8 string when rendering sort values (#17959)
- [x] sorting and aggregations across old and new indices do not work since the coordinating node gets longs from some shards and binary values from other shards and does not know how to reconcile them (#18593)
- [x] terms aggregations return binary keys (#18003)
</description><key id="151063895">17971</key><summary>Not all features work on ip fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>blocker</label><label>bug</label><label>Meta</label><label>v5.0.0-alpha4</label></labels><created>2016-04-26T07:32:31Z</created><updated>2016-05-31T13:14:34Z</updated><resolved>2016-05-31T13:14:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update settings.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17970</link><project id="" key="" /><description>Looks like this has already been removed from 5.0 Alpha 1.  Change heading of this section to "Removed using ..." and note the feature removal in the description?

```
./elasticsearch -Des.node.name=test
starts elasticsearch

Option             Description                           
------             -----------                           
-E &lt;KeyValuePair&gt;  Configure an Elasticsearch setting    
-V, --version      Prints elasticsearch version          
                     information and exits               
-d, --daemonize    Starts Elasticsearch in the background
-h, --help         show help                             
-p, --pidfile      Creates a pid file in the specified   
                     path on start                       
-s, --silent       show minimal output                   
-v, --verbose      show verbose output                   
ERROR: D is not a recognized option
```
</description><key id="151059721">17970</key><summary>Update settings.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2016-04-26T07:12:44Z</created><updated>2016-10-12T15:28:19Z</updated><resolved>2016-04-26T09:43:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-26T09:43:18Z" id="214685889">This feature has not yet been removed. However, setting system properties is done via command-line arguments to the JVM, not via command-line arguments to Elasticsearch. This is why you're seeing the error message in your example. This feature will be removed in a future pull request and the docs will be updated then. 
</comment><comment author="ppf2" created="2016-04-26T21:45:12Z" id="214897511">ok thx!
</comment><comment author="jpcarey" created="2016-10-12T15:28:19Z" id="253247065">I was looking at breaking changes, and did not see this mentioned. Am I missing something?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin isolated = false, fails to load elasticsearch lib/jars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17969</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: ES 2.0.2

**JVM version**: 1.8

**OS version**: Windows 8

**Description of the problem including expected versus actual behavior**: 

Around 25 libraries are being shared by elasticsearch and my plugin, therefore those jars are already present in the elasticsearch lib folder. The remaining jars have been placed in my plugin folder. I have set isolated=false. Yet, at runtime classloader doesn't load the elasticsearch lib jars. 

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="151047559">17969</key><summary>Plugin isolated = false, fails to load elasticsearch lib/jars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ropal</reporter><labels /><created>2016-04-26T05:54:21Z</created><updated>2016-04-28T23:34:08Z</updated><resolved>2016-04-26T14:16:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T14:16:30Z" id="214759751">isolated=false is only for plugins, elasticsearch has a separate classloader.  also, isolated=false has been  removed in master so I wouldn't rely on it
</comment><comment author="chindhuhari" created="2016-04-27T04:40:32Z" id="214968244">Hello clintongormley,
Even I'm stuck with a similar issue. 
In that case, if I put all the required jars( i.e. the jars that are shared with elasticsearch as well) in my plugin folder, won't jar hell exception be thrown? How's this kind of a scenario addressed?
As of now, I'm getting it to work by placing all the jars in elasticsearch lib/ , but that is something that we do not want to do.

Thanks!
</comment><comment author="dadoonet" created="2016-04-27T05:59:48Z" id="214979161">When you write a plugin you need to make sure that no single class conflicts with the ones elasticsearch uses.

You "just" have to add in your plugin missing classes (missing jars).

Don't add anything to elasticsearch lib dir.
</comment><comment author="chindhuhari" created="2016-04-27T06:21:28Z" id="214981973">Hi dadoonet,
I tried that as well, that's when I found this strange issue of ClassDefNotFound Java Exception, though I have the right version of that class present in the plugin folder. It was able to see all the classes in the elasticsearch lib/ folder. 
Then, I tried printing all the classes that's getting loaded by the classloader, in that this class(for which the exception was thrown) was listed, yet I got that error, I found that strange.
So, I thought I am missing something.

Thanks!
</comment><comment author="dadoonet" created="2016-04-27T06:53:34Z" id="214987648">May be you can provide more info about the class?
Traces might help as well.
</comment><comment author="chindhuhari" created="2016-04-27T08:35:27Z" id="215011347">The following is the jar(in the sysout result) which contains the class for which NoClassDefFoundError was thrown, as seen it's there in the plugins folder in elasticsearch :

class name java.net.FactoryURLClassLoader Classpath : /C:/ES_BINARY/elasticsearch-2.0.2/plugins/crawl-plugin/apache-nutch-1.10.jar      

The following is the exception that was thrown at runtime : 

```
   java.lang.NoClassDefFoundError: Could not initialize class org.apache.nutch.crawl.Injector
   at &lt;package&gt;.Crawler.inject(Crawler.java:186)
   at &lt;package&gt;.CrawlRestAction.crawl(CrawlRestAction.java:120)
   at &lt;package&gt;.CrawlRestAction.handleRequest(CrawlRestAction.java:76)
   at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
   at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
   at org.elasticsearch.rest.RestController$RestHandlerFilter.process(RestController.java:281)
   at org.elasticsearch.rest.RestController$ControllerFilterChain.continueProcessing(RestController.java:262)
   at org.elasticsearch.rest.RestController$ControllerFilterChain.continueProcessing(RestController.java:265)
   at org.elasticsearch.rest.RestController$ControllerFilterChain.continueProcessing(RestController.java
```
</comment><comment author="rjernst" created="2016-04-28T23:34:08Z" id="215592875">@chindhuhari Can you please provide some more info:
- A listing of the entire crawl-plugin directory
- The contents of the plugin-descriptor.properties file for your plugin
- The exact command you use to launch elasticsearch, along with any environment variables you have set with it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Access file-based stored index template through API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17968</link><project id="" key="" /><description>**Describe the feature**:

Currently (and as documented), index templates added via config/templates are not included in the _template api output. This can be confusing to admins.  It is more user friendly if we can return all index templates including ones defined on disk via the API - and add a metadata field to tag index templates created on disk in the api output so users can tell which ones are set in the API vs. on disk (and maybe disallow changes via the template api for the ones that are managed on disk).
</description><key id="151043968">17968</key><summary>Access file-based stored index template through API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Index Templates</label><label>discuss</label><label>enhancement</label></labels><created>2016-04-26T05:21:10Z</created><updated>2016-04-26T18:24:58Z</updated><resolved>2016-04-26T18:24:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cnwarden" created="2016-04-26T05:52:38Z" id="214623077">:+1: 
</comment><comment author="ppf2" created="2016-04-26T18:24:58Z" id="214839065">Per @rjernst 's (thx!) comment (https://github.com/elastic/elasticsearch/issues/10910#issuecomment-214828977), file based index templates have actually been [removed from the product](https://github.com/elastic/elasticsearch/pull/11052) starting in 2.x, so I will close this ticket :)  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>WIP: Snapshot blob store improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17967</link><project id="" key="" /><description /><key id="151031036">17967</key><summary>WIP: Snapshot blob store improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>WIP</label></labels><created>2016-04-26T03:26:21Z</created><updated>2016-05-13T20:52:27Z</updated><resolved>2016-05-13T20:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-26T03:35:05Z" id="214597083">@ywelsch this doesn't compile yet, as I haven't fully gone through removing `SnapshotName`, but just to give you an idea.  Removing `SnapshotName` is only in the third commit.
</comment><comment author="abeyad" created="2016-04-29T03:32:54Z" id="215618920">@ywelsch I've updated the PR with improvements.  In particular, the `Repository` APIs now mostly use `SnapshotId` and we are now using snapshot name + uuid to name the blobs in the repository.  I still need to fix some failing tests and add more tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unnecessary sleep from init script restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17966</link><project id="" key="" /><description>Today when restarting Elasticsearch using the start-stop-daemon on
Debian-based systems using System V init, we sleep for one second
between the process successfully stopping and starting the process
again. This sleep is unnecessary as the stop function retries forever
until the previous instance successfully terminates. This commit removes
that unnecessary sleep.
</description><key id="151018124">17966</key><summary>Remove unnecessary sleep from init script restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-26T01:19:21Z</created><updated>2016-04-26T14:13:26Z</updated><resolved>2016-04-26T02:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-26T01:48:54Z" id="214583183">LGTM
</comment><comment author="jasontedor" created="2016-04-26T02:52:19Z" id="214592075">Thanks @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Have mappings available on all nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17965</link><project id="" key="" /><description>Mappings for a given index are only available on nodes that hold a shard for this index. While this may reduce memory usage on clusters that have many indices with few shards on many nodes, it tends to make things more complicated. For instance, search requests cannot be validated on the coordinating node. Wouldn't it be a better trade-off to have mappings available on all nodes?
</description><key id="150994468">17965</key><summary>Have mappings available on all nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>discuss</label><label>PITA</label></labels><created>2016-04-25T22:10:20Z</created><updated>2016-06-17T09:49:17Z</updated><resolved>2016-06-17T09:49:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-01T10:34:14Z" id="216033544">+1. I always thought that mapping were part of the cluster state.
Is that the idea here? Move mappings in cluster state?
</comment><comment author="jpountz" created="2016-05-01T19:46:23Z" id="216067463">The mappings are in the cluster state indeed, but only in their serialized form. You have no way to query the mappings to know whether type contains a field, or whether a field is indexed.
</comment><comment author="nik9000" created="2016-05-01T23:46:44Z" id="216082180">It's make function score parsing nicer too! I don't know enough to know the
drawbacks. Certainly heap usage, but is it that much?
On May 1, 2016 3:46 PM, "Adrien Grand" notifications@github.com wrote:

&gt; The mappings are in the cluster state indeed, but only in their serialized
&gt; form. You have no way to query the mappings to know whether type contains a
&gt; field, or whether a field is indexed.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/17965#issuecomment-216067463
</comment><comment author="ywelsch" created="2016-05-02T09:50:29Z" id="216181302">&gt; it tends to make things more complicated

@jpountz do you mean code-wise? Or is it just that you want the request to fail as early as possible? (If it is only the latter, we could make this the default only on dedicated client nodes?). 

Related to this, would it be possible to have the same mapping objects for indices that share the same mapping (for example time-based indices)?
</comment><comment author="nik9000" created="2016-05-02T13:41:54Z" id="216237901">&gt; Related to this, would it be possible to have the same mapping objects for indices that share the same mapping (for example time-based indices)?

Probably but I don't know if it is worth the complexity? Maybe.

&gt; @jpountz do you mean code-wise? Or is it just that you want the request to fail as early as possible? (If it is only the latter, we could make this the default only on dedicated client nodes?).

Personally I think both are true.
</comment><comment author="nik9000" created="2016-05-02T13:43:15Z" id="216238158">&gt; Personally I think both are true.

To be clear - I mean it makes the code more complex which is bad. The fact that we have to delay some stuff until later adds some complexity too. The error reporting consequences of the delay aren't great, but I don't see them as as bad as the code complexity problems.
</comment><comment author="s1monw" created="2016-06-03T09:37:02Z" id="223535235">@bleskes I know you had opinions on this can you please comment here
</comment><comment author="bleskes" created="2016-06-03T13:10:46Z" id="223574473">yeah. Currently the nodes only open up the mappings that are relevant to them. All the rest are kept in a  compressed version. This means that the number of open mapping per node is limited by the number indices it hosts. If we change it, we are creating an upper bound to the amount of indices (via their mapping) that a cluster can hold and I wasn't comfortable with the fact that this algorithm is not scalable by design (adding nodes doesn't help).  We discussed this last week and I promised to look at https://github.com/elastic/elasticsearch/pull/17959 , which represents the current down sides - as we know it. IMO what we're doing now is the right trade off, at least for this case - streaming a human readable form of the sorting key , next to a binary form that's needed for the actual sorting, doesn't sound too bad to me. Especially if we only do it on the fetch phase - I do want to discuss it with @jpountz and here the finer details behind the issue.
</comment><comment author="s1monw" created="2016-06-10T09:47:58Z" id="225140953">While there are a number of reasons why having mappings locally for all indices there are also risks involved. For instance the aspect of relying on local state when search requests are merged not how I think we should design this API. SearchResponses form a shard should contain all information to merge the search results (sort, aggs etc). This is an important aspect when we want to allow searches across different clusters of even different versions where the knowledge / mapping of the index is not necessarily available. There are also situations where data can just not reside in the same cluster for legal reasons but can be searched together etc. In order to add support for these use-cases we must not rely on the mappings but should rather design our protocol in a way that all necessary information is available in the response.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Failure Details to every NodesResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17964</link><project id="" key="" /><description>Most of the current implementations of `BaseNodesResponse` (plural Nodes) ignore `FailedNodeException`s.
- This adds a helper function to do the grouping to `TransportNodesAction`
- Requires a non-null list of generic `NodeResponse`s
- Requires a non-null list of `FailedNodeException`s within the `BaseNodesResponse` constructor
- Reads/writes the lists to `Streamable`
- Also adds `StreamInput` and `StreamOutput` methods for generically reading and writing Lists of Streamables

/cc @jasontedor (for failures) @nik9000 (for StreamInput/Output helpers)

Closes #3740
</description><key id="150967357">17964</key><summary>Add Failure Details to every NodesResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-25T20:07:10Z</created><updated>2016-05-06T19:03:53Z</updated><resolved>2016-05-06T19:00:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-25T23:02:08Z" id="214557020">@pickypg Other than the `ToXContent` stuff that we discussed via another channel, I left a comment about the `ResponseContainer` abstraction; I do not think this is necessary and left a specific suggestion. I'll hold off on a thorough review until you have a chance to process those two things.
</comment><comment author="jasontedor" created="2016-04-25T23:03:10Z" id="214557197">Note that this will close #3740.
</comment><comment author="pickypg" created="2016-04-27T04:45:42Z" id="214968717">@jasontedor Updated. I force pushed as it touched a lot of the existing code. I do prefer the abstract method. I didn't think that _every_ instance was using it, but they were, so I definitely prefer it.
</comment><comment author="jasontedor" created="2016-04-27T13:02:49Z" id="215074713">@pickypg I think it looks great, I left a few more comments. I asked @nik9000 to take a look at the `StreamInput`/`StreamOutput` code though.

&gt; I force pushed as it touched a lot of the existing code.

&#128550;
</comment><comment author="pickypg" created="2016-04-28T06:27:12Z" id="215322652">@jasontedor / @nik9000 

I changed it from `NodeResponse[]` to `List&lt;NodeResponse&gt;` and `FailedNodeException[]` to `List&lt;FailedNodeException&gt;`. I also dropped my changes from `StreamInput` and `StreamOutput` (and corresponding tests), but I did need to add a `&lt;T extends Streamable&gt; void writeStreamableList(List&lt;T&gt;)` (technically this could be `List&lt;? extends Streamable&gt;`, but I preferred matching the `Writable` variant) and `&lt;T extends Streamable&gt; List&lt;T&gt; readList(Supplier&lt;T&gt;))`.

Overall, I think it's better, but it naturally touches even more files. It took forever to get through the tests. :)
</comment><comment author="nik9000" created="2016-04-28T13:28:43Z" id="215422635">&gt; &lt;T extends Streamable&gt;

If you'd prefer to do `&lt;? extends Streamable&gt;` and `&lt;? extends Writeable&gt;` I'm fine with changing both. Or just doing yours with `?`. I suspect the `T` in the writeable one is because `Writeable` used to have a type parameter and that bullied people into adding the `T`. Maybe. Anyway, there isn't a reason to have it now that I can see.
</comment><comment author="pickypg" created="2016-04-28T16:40:30Z" id="215489143">@nik / @jasontedor 

I changed writeStreamableList and writeList to remove `T` as discussed. Also made other recommended changes.
</comment><comment author="nik9000" created="2016-05-02T17:49:16Z" id="216308249">&gt; I changed writeStreamableList and writeList to remove T as discussed. Also made other recommended changes.

Cool! I spent all morning sending emails and stuff so I'm itching to write code. I'll leave this open and have another look soonish. If @jasontedor gets to it before me fine by me!
</comment><comment author="nik9000" created="2016-05-04T18:56:41Z" id="216965841">I left a few comments - mostly small things that have no right to block merging. My only concern is around whether or not we need the new listener - essentially should we just move all of that header logic into the toXContent methods of the BaseNodeResponse?
</comment><comment author="jasontedor" created="2016-05-04T19:00:13Z" id="216967380">&gt; My only concern is around whether or not we need the new listener - essentially should we just move all of that header logic into the toXContent methods of the BaseNodeResponse?

BaseNodeResponse doesn't have a `toXContent` method nor should it because it would be a lie because not all of the response objects that inherit from implement `ToXContent`.
</comment><comment author="nik9000" created="2016-05-04T19:10:52Z" id="216970106">&gt; BaseNodeResponse doesn't have a toXContent method nor should it because it would be a lie because not all of the response objects that inherit from implement ToXContent.

Makes sense to me then.

I'm happy with it.
</comment><comment author="pickypg" created="2016-05-05T20:50:32Z" id="217274936">@nik9000 Added it now that local tests have finished. I like having the nodes read/written from an abstract method. It also removes a bunch of boilerplate from the child classes.
</comment><comment author="nik9000" created="2016-05-06T13:50:25Z" id="217445544">LGTM. I know there is still discussion but I don't think it is the kind of discussion that blocks merging?
</comment><comment author="jasontedor" created="2016-05-06T13:53:52Z" id="217446543">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards left unassigned by allocation decisions, requiring manual rerouting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17963</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5

**Description of the problem including expected versus actual behavior**:

The following is a scenario where the allocation algorithm today can leave shards unassigned and requires manual intervention to reroute/swap shards between nodes for it to fully allocate the shards for an index:  https://gist.github.com/ppf2/9a005e26e156c7e0face9c4b3d27c4d9

It will be great if there is a way for the allocation algorithm to be able to "look ahead" and try moving a different shard away to see if it helps with the allocation until it is able to fully allocate the shards - without requiring manual intervention by admins.
</description><key id="150956160">17963</key><summary>Shards left unassigned by allocation decisions, requiring manual rerouting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Allocation</label><label>enhancement</label></labels><created>2016-04-25T19:19:50Z</created><updated>2016-04-25T19:19:50Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>top hits subaggregation  not working as expected.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17962</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

**Elasticsearch version**: 2.1.1

**JVM version**: 
java version "1.8.0_20"
Java(TM) SE Runtime Environment (build 1.8.0_20-b26)
Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode)

**OS version**:
CentOS release 6.7 (Final)

**Description of the problem including expected versus actual behavior**:
Top hits subaggregation setSize(1) not returning one top hit/per bucket instead it is returning more than one document even though the size is set to one. Below is the query i am trying to execute.

```
SearchResponse sr=client.prepareSearch(versionIndexName).setTypes(versionIndexType).setQuery(Quer 
yBuilders.boolQuery().must(QueryBuilders.rangeQuery("indexDate").lte(givenTime))) 
.addAggregation(AggregationBuilders.terms("form.id").field("form.id").size(0) 
.subAggregation(AggregationBuilders.topHits("top").setExplain(true).setSize(1) 
.setFrom((size*index)).addSort(SortBuilders.fieldSort("indexDate") 
.order(SortOrder.DESC)).setFetchSource(true))).execute().actionGet();
```

STEPS in the query:
Filter by indexDate less than or equal to a given date.
aggregate based on formId. Forming buckets per formId.
Sort in descending order on versionDate filed and return top hit result per bucket.

I have two versions (v1 and v2) for form.id=1 with indexDate field set to  2016-04-22T16:02:32.738-07:00 for v1 and 2016-04-22T16:02:33-07:00 for v2. If i search by date 2016-04-22T16:02:33-07:00 i expect to see a bucket with only v2 returned in it because that is the closest to the searched date. But the bucket has both the versions.

**Steps to reproduce**:
1. Index two documents with version field as v1 and v2, indexDate filed set to 2016-04-22T16:02:32.738-07:00 for v1 and 2016-04-22T16:02:33-07:00 for v2, id field is set to 1 and 2.
2. Run the above query with givenTime= 2016-04-22T16:02:33-07:00.
3. Retrieve the buckets and values.

Expected result: 1 bucket with one version(v2) in it. Because that is the top hit for that bucket after sorting.
Actual result: 1 bucket with both the version(v1 and v2).
</description><key id="150955160">17962</key><summary>top hits subaggregation  not working as expected.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abhishekguruvayya</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2016-04-25T19:15:51Z</created><updated>2016-05-24T10:36:47Z</updated><resolved>2016-05-24T10:36:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T13:41:56Z" id="214749050">The equivalent written for the REST api works just fine:

PUT t/t/1
    {
      "form": {
        "id": "1"
      },
      "version": "v1",
      "indexDate": "2016-04-22T16:02:32.738-07:00"
    }

```
PUT t/t/2
{
  "form": {
    "id": "2"
  },
  "version": "v2",
  "indexDate": "2016-04-22T16:02:33-07:00"
}

GET t/t/_search?size=0
{
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "indexDate": {
              "lte": "2016-04-22T16:02:33-07:00"
            }
          }
        }
      ]
    }
  },
  "aggs": {
    "form.id": {
      "terms": {
        "field": "form.id",
        "size": 0
      },
      "aggs": {
        "top": {
          "top_hits": {
            "size": 1
          }
        }
      }
    }
  },
  "sort": {
    "indexDate": {
      "order": "desc"
    }
  }
}
```

My suspicion is that either the way you're using the Java API isn't correct, or there is something fishy about your docs.

Could you provide your docs as curl statements, and use toXContent on your query to output what it looks like in JSON - that'll help figure out why this isn't working.
</comment><comment author="clintongormley" created="2016-05-24T10:36:47Z" id="221230947">No further feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster with "hot-warm" architecture: hot index shards not assigned when restarting service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17961</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 
Elasticsearch version is 1.7.1
**JVM version**:
Java version is
java version "1.8.0_60"
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)

**OS version**:
OS is RHEL 7.1
**Description of the problem including expected versus actual behavior**:
The Elasticsearch cluster has 3 data nodes. The cluster is configured using "hot-warm" architecture based on the instructions in https://www.elastic.co/blog/hot-warm-architecture. One node is configured as "hot" and two nodes are configured as "warm".  Hot indices are changed to warm and moved to warm nodes every 3 days. 
The problem is that when restarting elasticsearch service on either a host or warm node, the cluster status stays in "yellow" state because shards for hot indices are unassigned. I have manually update the index settings to change from "hot" to "warm"
curl -XPUT 'localhost:9200/&lt;indexName&gt;/_settings' -d '{ "index.routing.allocation.require.box_type" : "warm" }'
When indices are changed to warm, the shards are assigned immediately and the cluster status becomes "green".

I would expect that the shards of hot indices be assigned when restarting the elasticsearch service just like the warm indices. 

Does anyone know if elasticsearch 2.x has the same behavior or is it the 1.7 only issue?

**Steps to reproduce**:
1. Configure a cluster using the instructions in  https://www.elastic.co/blog/hot-warm-architecture. 
2. Set at least one index to hot.
3. restart elasticsearch service and check the cluster health status 
    curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="150953385">17961</key><summary>Cluster with "hot-warm" architecture: hot index shards not assigned when restarting service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danling</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2016-04-25T19:08:17Z</created><updated>2016-04-27T22:20:19Z</updated><resolved>2016-04-27T22:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T13:25:23Z" id="214743916">@danling when you're in this situation, please could you try to assign the shard manually with the cluster reroute API, and include the `?explain` query string param (see https://www.elastic.co/guide/en/elasticsearch/reference/1.7/cluster-reroute.html)

That will tell you why the shard can't be assigned (or it'll go ahead and assign the shard).  There may have been bugs fixed in this area even in 1.7, so worth upgrading to 1.7.5 (and even better, upgrade to 2.3.2)
</comment><comment author="danling" created="2016-04-27T01:36:42Z" id="214939239">Thanks for the suggestion. Here is the information I got from the reroute API:
The unassigned index is a "hot" index and has setting:
 "index" : {
        "routing" : {
          "allocation" : {
            "require" : {
              "box_type" : "hot"
            }   }   },
The unassigned index is on the "hot" node (the cluster has 1 hot node and 2 warm nodes).  
First, I tried to allocate the index to the hot node:
curl -XPOST 'localhost:9200/_cluster/reroute?explain&amp;pretty=true' -d '{
    "commands" : [    {
          "allocate" : {
              "index" : "logstash-test-20160426", "shard" : 0, "node" : "data1_hot_node"
          } }    ] }'
It failed with 
"explanation" : "shard cannot be allocated on same node [_FNsW1arSDip4ymYPNu5Ng] it already exists on".  
Second, I tried to allocate the hot index to one of the warm nodes, and got 
 "explanation" : "node does not match index required filters [box_type:\"hot\"]". 
I assume the 2nd try failed is because a warm node can have only warm indices. 
It seems that the hot index exists on the hot node but ElasticSearch is not able to assign its shards.
</comment><comment author="danling" created="2016-04-27T14:36:11Z" id="215103796">I noticed this morning that when a new "hot" index was created, the shards were not assigned as well, and the cluster status became "yellow". So the problem is not just when restarting elasticsearch service.
</comment><comment author="danling" created="2016-04-27T18:00:24Z" id="215172361">I found the reason for unassigned shards of hot indices. The cluster is configured to have 1 replica, but it has only 1 hot node. By adding a second hot node, it solves the problem.
</comment><comment author="dakrone" created="2016-04-27T22:20:19Z" id="215249058">Yes, as the output shows above ("shard cannot be allocated on same node [_FNsW1arSDip4ymYPNu5Ng] it already exists on"), you'll need to either set the number of replicas to 0, or increase the number of nodes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17960</link><project id="" key="" /><description>External node logs do not appear together with the "regular" node
logs but are instead printed before or after the test logs.
This makes debugging rather tricky as several test suites
can spin up external nodes with the same name and we cannot tell from the
logs which is used for which test.
Appending the class name of the test at least allows us to know
which suite the node is used in.
</description><key id="150943376">17960</key><summary>2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hmvidi</reporter><labels /><created>2016-04-25T18:19:34Z</created><updated>2016-04-25T18:28:21Z</updated><resolved>2016-04-25T18:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-25T18:28:21Z" id="214471214">I'm assuming that this PR was made in error. Please clarify if not.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow binary sort values.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17959</link><project id="" key="" /><description>The `ip` field uses a binary representation internally. This breaks when
rendering sort values in search responses since elasticsearch tries to write a
binary byte[] as an utf8 json string. This commit extends the `DocValueFormat`
API in order to give fields a chance to choose how to render values.

Closes #6077

Relates to #17971
</description><key id="150941057">17959</key><summary>Allow binary sort values.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-25T18:08:31Z</created><updated>2016-06-28T09:28:28Z</updated><resolved>2016-05-06T07:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-25T19:07:28Z" id="214482256">I'm a little concerned that this change just propagates craziness through the internal api that was a hack before (turning BytesRef into Text). We have ipfield mapper using sorted doc values, so why can't we sort on byte[]?
</comment><comment author="jpountz" created="2016-04-25T21:34:35Z" id="214531904">We do sort on byte[]. This change is only about how we expose the sort values to the user since I don't think BytesRef should be exposed in the client API (I chose to go with String but byte[] would work too, even though it is not straightforward since json does not support binary values).

I did it the change this way because I think this is the option that would play best in terms of backward compatibility. But we have other options:
- returning sort values for all fields that use binary/sorted doc values using eg. hexadecimal. This would be a break for string fields since sort values used to be the string value that was used at index time and it would now be an hexadecimal string.
- not exposing sort values in SearchResponse anymore and only using them internally to merge results coming from different shards.
</comment><comment author="rjernst" created="2016-04-25T21:45:57Z" id="214537139">Why isn't how doc values are formatted for response separate from how they are transferred for distributed sorting?
</comment><comment author="jpountz" created="2016-04-25T22:03:32Z" id="214542988">Just had a quick discussion with Ryan about it: one source of confusion here is due to the fact that mappings might not be available on the coordinating node, so shards have to get information about how to render sort values and then serialize it back to the coordinating node where the rendering will happen.
</comment><comment author="jpountz" created="2016-04-25T22:17:04Z" id="214547775">I opened #17965 to discuss the general issue. However I don't think it should block this PR.
</comment><comment author="rjernst" created="2016-04-26T18:04:04Z" id="214830964">@jpountz Instead of needing to pass along how to format to the coordinating node, could we pass along the formatted value (formatted on each shard using the mapper), but to the coordinating node that is just a black box string that is read, and inserted in the results for the docs which are chosen in top N?
</comment><comment author="rjernst" created="2016-04-26T18:04:41Z" id="214831152">And that would mean also passing along the binary encoded values and sorting based on the binary value. So kind of a variation of your first option you proposed above.
</comment><comment author="rjernst" created="2016-04-26T18:11:18Z" id="214833143">Note that it would also mean we would not need to render strings as sortable. eg we could keep `::1` because it would only be used for display, not for actual sorting on the coordinating node.
</comment><comment author="clintongormley" created="2016-04-26T18:43:32Z" id="214845093">@jpountz don't forget that sort values should be reusable in their rendered form by the seach_after feature
</comment><comment author="jpountz" created="2016-04-26T19:53:20Z" id="214866488">&gt; could we pass along the formatted value (formatted on each shard using the mapper), but to the coordinating node that is just a black box string that is read, and inserted in the results for the docs which are chosen in top N?

Will do!

&gt; Note that it would also mean we would not need to render strings as sortable. eg we could keep ::1 because it would only be used for display, not for actual sorting on the coordinating node.

Actually it is not needed in the current PR either. I first thought that we should aim at returning strings that sort in the same order as the underlying binary values but I'm starting to think this is not worth the trouble. I'll remove it.
</comment><comment author="jpountz" created="2016-04-26T19:55:32Z" id="214867102">&gt; @jpountz don't forget that sort values should be reusable in their rendered form by the seach_after feature

Right, this is why I had to add the `DocValuesFormat.parseBytesRef(String)` method so that we can get back to the binary representation from the string value. This will likely also be needed for parsing include/exclude lists of terms aggregations, see #17705.
</comment><comment author="jpountz" created="2016-04-27T09:49:12Z" id="215032535">I pushed a new commit that removes the ability to render sort values as sortable.

&gt; Instead of needing to pass along how to format to the coordinating node, could we pass along the formatted value (formatted on each shard using the mapper), but to the coordinating node that is just a black box string that is read

I tried to do this but this ended up making merging top docs on the coordinating node more complicated, since each shard would have its TopDocs and Object[][] for sort values (one Object[] per ScoreDoc), then we would call Lucene's TopDocs.merge to compute the top hits, and then we would have to associate each ScoreDoc object back to the rigth Object[]. Since it was making things more complicated, I gave up on this idea, what do you think?
</comment><comment author="jpountz" created="2016-05-02T16:01:09Z" id="216276958">@rjernst Any opinions about the comment above?
</comment><comment author="rjernst" created="2016-05-04T19:29:43Z" id="216974886">This looks good, thanks for the the change to printable sort values, I think the inet addresses look much better with the minimized format. We can revisit whether/how to simplify (ie removing SortAndFormats) depending on what happens with #17965.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version: Set version to 5.0.0-alpha2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17958</link><project id="" key="" /><description /><key id="150927855">17958</key><summary>Version: Set version to 5.0.0-alpha2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label></labels><created>2016-04-25T17:10:17Z</created><updated>2016-04-26T07:53:37Z</updated><resolved>2016-04-26T07:53:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-25T17:29:15Z" id="214450815">LGTM though I'm not sure all the rules. It looks like we still have constants for the 2.0.0 betas so it should be safe to have those `greaterThan(5.0.0-beta2)` checks.
</comment><comment author="spinscale" created="2016-04-25T18:41:49Z" id="214474961">I also think it makes more sense to have the legacy field mappers check for alpha1, as this is when they should throw an exception, as far as I understand it
</comment><comment author="jpountz" created="2016-04-26T07:06:38Z" id="214640796">I think it is fine this way: we changed the mappers between alpha1 and alpha2 so mappers need to check `version.onOrAfter(5.0.0-alpha2)` to know whether to use the legacy or the new mappers?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get and MultiGet inconsistency with date math expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17957</link><project id="" key="" /><description>When using index name date math expressions `&lt;logstash-{now/M}&gt;`, the Get API works and is able to resolve the index. However, if you try to get the same value using the MultiGet API it fails as it treats the index name as a concrete index name rather than allowing for the possibility that it could be an expression. I think the behavior of the APIs should be consistent.

I tend to question the usefulness of the date math expression in a get request. I could see one scenario where you know the id of a document (the id is consistent) and want to get the "current" one using a date math expression.
</description><key id="150908048">17957</key><summary>Get and MultiGet inconsistency with date math expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:CRUD</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-04-25T15:53:33Z</created><updated>2016-10-20T07:26:55Z</updated><resolved>2016-10-20T07:26:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-26T11:10:18Z" id="214705039">&gt; I tend to question the usefulness of the date math expression in a get request. 

Agreed

&gt; I could see one scenario where you know the id of a document (the id is consistent) and want to get the "current" one using a date math expression.

That's very much an edge case.  On the other hand, support date math expressions everywhere doesn't cost much...  That said, I think I'm leaning more towards "why would you do that?"
</comment><comment author="clintongormley" created="2016-05-06T10:03:09Z" id="217402269">Looks like there is a bug in multiget, which may impact alias lookup too - let's just make multiget consistent with get (and fix the bug)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow adding additional child types referring to the same parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17956</link><project id="" key="" /><description>Originates from: https://discuss.elastic.co/t/how-to-create-a-child-type-when-its-parent-type-is-already-exist-in-elasticsearch-2-x

In cases when a parent type already has child mappings we can potentially relax the constraint that forbids adding more child type. 

The constraint as is today was designed to avoid the following scenario:

```
PUT /index
{
  "mappings": {
    "parent" : {
    }
  }
}

// some time later:

PUT /index/child1/_mapping
{
  "child1" : {
    "_parent": {
        "type": "parent"
      }  
  }
}
```

The put mapping call fails with:  `can't add a _parent field that points to an already existing type`
The reason we can't allow is this, is that otherwise for already existing parent docs the ids wouldn't be stored in the doc values join field (which the _paren field controls)

However this constraint also prevents adding additional child types:

```
PUT /index
{
  "mappings": {
    "parent" : {
    },
    "child1" : {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT /index/child2/_mapping
{
  "child2" : {
    "_parent": {
        "type": "parent"
      }  
  }
}
```

In this case adding the second child type fails. This is too strict and we can allow this, because for the parent type we already store the ids in the doc values join field.
</description><key id="150866833">17956</key><summary>Allow adding additional child types referring to the same parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label></labels><created>2016-04-25T13:55:12Z</created><updated>2016-08-04T04:08:51Z</updated><resolved>2016-05-19T09:36:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="navyliu" created="2016-04-26T02:47:24Z" id="214591522">We real hope this feature can role back to the normal logic of v1.x . In most situation, parent are main object of domain, and children can add relation to parent at anytime .
And this is the biggest  obstacle for our updating from v1.6 to v2.3 . 
</comment><comment author="nishantsaini" created="2016-04-26T06:23:25Z" id="214628055">The parent-child relationship and flexibility of adding child at a later stage was one the key feature for using elasticsearch. This restriction has taken away the advantage as in many cases it is not possible to identify all the possible child types in advance. Hope the restriction is relaxed and allow to add new child type if one child type already exists as explained by @martijnvg 
</comment><comment author="turp1twin" created="2016-05-19T15:48:46Z" id="220367145">What release will this change be available in? Any chance it will be backported to the 2.3.x branch? Thanks!
</comment><comment author="martijnvg" created="2016-05-19T16:01:10Z" id="220371055">@turp1twin This will be available in 2.4.0 once it is released. (no scheduled date for this yet). The 2.3 branch is mainly meant for bug fixes and this is a change in behavior, so that is why this won't be back ported to the 2.3 branch.
</comment><comment author="turp1twin" created="2016-05-19T16:04:23Z" id="220372005">Thanks @martijnvg! 2.4.0 will work just fine... Ship It! :-)
</comment><comment author="turp1twin" created="2016-08-04T04:08:51Z" id="237447419">Any updates on a potential ship date for 2.4.0? Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the `SearchType` setter on `SearchContext`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17955</link><project id="" key="" /><description>It was not used.
</description><key id="150866442">17955</key><summary>Remove the `SearchType` setter on `SearchContext`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-25T13:53:28Z</created><updated>2016-04-26T07:09:24Z</updated><resolved>2016-04-26T07:09:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-25T14:05:45Z" id="214345701">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.3 doesn't encode Cyrillic symbols in search URIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17954</link><project id="" key="" /><description>I am using Sense and just browser search line to GET results of search URI whose query consist of a Russian word, "&#1090;&#1077;&#1089;&#1090;", for example. But null is always returned. When I search for English words, everything is perfect.
The following command in cURL works fine:
`curl -XGET -G 'http://localhost:9200/test-index/_search' &#8212;data-urlencode "q=text:&#1090;&#1077;&#1089;&#1090;"`
So the problem is that ES itself doesn't encode Cyrillic symbols anymore (I remember it did in 1.x).
</description><key id="150861289">17954</key><summary>ES 2.3 doesn't encode Cyrillic symbols in search URIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicky1038</reporter><labels /><created>2016-04-25T13:34:58Z</created><updated>2016-04-25T20:22:57Z</updated><resolved>2016-04-25T19:45:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T19:45:01Z" id="214496086">works for me:

```
curl -XGET -G 'http://localhost:9200/t/_search' --data-urlencode "q=text:&#1090;&#1077;&#1089;&#1090;"


{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "t",
        "_type": "t",
        "_id": "1",
        "_score": 1,
        "_source": {
          "text": "&#1090;&#1077;&#1089;&#1090;"
        }
      }
    ]
  }
}
```

Looks like you have an encoding problem somewhere.
</comment><comment author="nicky1038" created="2016-04-25T20:01:05Z" id="214501872">@clintongormley Please re-read my message again. I've said that the variant with cURL works fine, exactly because cURL itself encodes the query. But ordinary URI search in Sense (i.e. test-index/_search?q=text:&#1090;&#1077;&#1089;&#1090;) returns null.
</comment><comment author="dadoonet" created="2016-04-25T20:22:57Z" id="214509028">May be you should open the issue in Sense then? https://github.com/elastic/sense

That being said, may be your browser is not in "UTF-8" so characters are not encoded in a proper way?
FWIW it's not an issue in elasticsearch as you already guessed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch parsing the source for a disabled object when using Transform</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17953</link><project id="" key="" /><description>**Elasticsearch version**: `2.3.1`

**JVM version**: `Java(TM) SE Runtime Environment (build 1.8.0_45-b14)`

**OS version**: Any OS

**Description of the problem including expected versus actual behavior**:

Using a simple script, called `fake.groovy` which is just a `return true` script, you can verify that the content of object fields that are not enabled are being parsed anyway. Looking at the [source](https://github.com/elastic/elasticsearch/blob/2.3/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L748) it looks like the `mapOrdered` method is actually using the `JsonXContentParser` with all the fields no matter if they are enabled or not. We shouldn't parse the source and we should have the same behaviour than when not using `transform` method.

If you don't use transform, the mapper will avoid parsing the content and this will work as expected.

**Steps to reproduce**:
1. Create a `fake.groovy` script and place it in `/config/scripts`
2. Create an index using the transform [1]
3. Index a document with a field that should fail if the source is parsed (example `BIG_INTEGER`) [2]
4. The result of this is an error, because is trying to parse the source, we should use the source, not parse it when the object is not enabled.

[1] 

```
DELETE test

PUT test/
{
  "mappings": {
    "entry": {
      "transform": {
        "file": "fake",
        "lang": "groovy"
      },
      "properties": {
        "ssl": {
          "type": "object",
          "enabled": false
        }
      }
    }
  }
}
```

[2]

```
POST test/entry
{
  "ssl": {
    "cert": {
      "serial": 13408895465235657000
    }
  }
}
```

**Provide logs (if relevant)**:

```
[2016-04-25 08:53:39,233][DEBUG][action.index             ] [Hurricane] failed to execute [index {][entry][AVRNRI6cWqa-34uUr91K], source[{
  "ssl": {
    "cert": {\
      "serial": 13408895465235657000,\\
    }
  }
}
]}] on [-][4]]
MapperParsingException[failed to parse]; nested: IllegalStateException[No matching token for number_type [BIG_INTEGER]];
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:154)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:529)
    at org.elasticsearch.index.shard.IndexShard.prepareCreateOnPrimary(IndexShard.java:506)
    at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:215)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:224)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:158)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:66)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: No matching token for number_type [BIG_INTEGER]
    at org.elasticsearch.common.xcontent.json.JsonXContentParser.convertNumberType(JsonXContentParser.java:216)
    at org.elasticsearch.common.xcontent.json.JsonXContentParser.numberType(JsonXContentParser.java:68)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readValue(AbstractXContentParser.java:301)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readMap(AbstractXContentParser.java:274)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readValue(AbstractXContentParser.java:314)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readMap(AbstractXContentParser.java:274)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readValue(AbstractXContentParser.java:314)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readMap(AbstractXContentParser.java:274)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readOrderedMap(AbstractXContentParser.java:249)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.mapOrdered(AbstractXContentParser.java:213)
    at org.elasticsearch.index.mapper.DocumentParser.transform(DocumentParser.java:748)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:93)
    ... 17 more

```
</description><key id="150836615">17953</key><summary>Elasticsearch parsing the source for a disabled object when using Transform</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-04-25T12:08:16Z</created><updated>2016-04-25T19:35:14Z</updated><resolved>2016-04-25T19:33:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T19:33:14Z" id="214493067">The source transform feature was never documented to ignore disabled object fields. On top of that, it was deprecated in 2.0 and removed in 5.0, so we won't be making any changes to it now
</comment><comment author="gmoskovicz" created="2016-04-25T19:35:14Z" id="214493573">Thanks for the update @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indices.breaker.request.limit not increasing trip count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17952</link><project id="" key="" /><description>Elasticsearch version: 1.7.5
JVM version: 1.8.0_60
OS version: Ubuntu 3.11.0-26-generic

indices.breaker.request.limit is probably not working as expected. As shown in the attached image, I tried setting indices.breaker.request.limit=8Mb whilst debugging a load issue that might have been linked to the size of the documents. I expected to see the trip count increase as the request estimated size passed the 8Mb threshold but that didn't occur. We didn't see an increased rate of rejection so Marvel it most likely reporting truthful data. 
indices.breaker.request.overhead was left set to 1.

It might be a misunderstanding of the documentation but in that case I'd recommend reviewing the page
https://www.elastic.co/guide/en/elasticsearch/reference/current/circuit-breaker.html

![screen shot 2016-04-25 at 12 51 12](https://cloud.githubusercontent.com/assets/506458/14783160/b5e27254-0ae5-11e6-96c5-c453f3189766.png)
</description><key id="150835419">17952</key><summary>indices.breaker.request.limit not increasing trip count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kuramae</reporter><labels><label>:Circuit Breakers</label><label>discuss</label></labels><created>2016-04-25T12:01:39Z</created><updated>2016-04-26T22:39:04Z</updated><resolved>2016-04-26T22:39:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T19:31:22Z" id="214492571">@dakrone any ideas?
</comment><comment author="dakrone" created="2016-04-25T20:38:18Z" id="214513473">@kuramae I just tested locally with 1.7.5 and the request breaker trip count does increment when the breaker is tripped. Are you sure you are seeing the request breaker trip at all? Perhaps you are thinking of something limiting the size of the request bodies themselves? In that case, you will need to use either 2.4.0 or 5.0.0 when they are released, see: https://github.com/elastic/elasticsearch/pull/17133
</comment><comment author="kuramae" created="2016-04-26T22:29:49Z" id="214907171">Thanks for having a look. Quite likely nothing wrong here. The thing that confuses me a bit is that from the Marvel plots you can see the breaker limit being set to 8Mb, the request trip count staying at 0, and the request estimated size passing the limit. The way I interpreted the documentation makes it look like the memory allocated per request is higher than 8Mb but the requests are not failed and reported in the stats.
</comment><comment author="dakrone" created="2016-04-26T22:39:04Z" id="214908875">Okay, thanks for checking @kuramae, I'm going to close this now, but feel free to comment/re-open if you see CircuitBreakingExceptions that don't increment the counter!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exclude specific transport actions from request size limit check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17951</link><project id="" key="" /><description>We add support to explicitly exclude specific transport actions from the request size limit check which are needed to keep cluster-internal operations working:

We exclude the following request types:
- MasterPingRequest
- PingRequest
</description><key id="150826394">17951</key><summary>Exclude specific transport actions from request size limit check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Network</label><label>blocker</label><label>enhancement</label><label>resiliency</label><label>v2.4.0</label><label>v5.0.0-alpha3</label></labels><created>2016-04-25T11:18:01Z</created><updated>2016-05-13T12:23:59Z</updated><resolved>2016-05-13T12:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-04-25T11:18:31Z" id="214269914">@martijnvg: Can you check please?
</comment><comment author="nik9000" created="2016-04-25T11:37:02Z" id="214276617">Maybe add a test that making a bunch of requests that are excluded doesn't trigger it the same way you test that aren't does trigger it? Maybe also be nice to have a case where you push it to the limit and trigger the failure and then fire in a bunch more of the requests that don't count and make sure they don't trip it.
</comment><comment author="jasontedor" created="2016-04-25T12:10:30Z" id="214285899">I agree with @nik9000, this really needs tests. 
</comment><comment author="martijnvg" created="2016-04-25T12:13:45Z" id="214288545">@danielmitterdorfer this change does look good, but like @nik9000 and @jasontedor say it would be good if we can add tests. Maybe what would be a simple tests is if we set the limit size to 0, form a cluster and start a transport client, then send requests via the transport client (which will fail with breaker exceptions, but that is expected)
</comment><comment author="danielmitterdorfer" created="2016-04-25T12:18:10Z" id="214290644">I can add a test that excluded requests don't trigger the circuit breaker but I think the second test that @nik9000 suggested (push to the limit, fire more requests that shouldn't trigger the breaker) would require explicit control over when request scheduling (i.e. a deterministic order when requests finish) otherwise I expect the test to be quite unstable as the limit is not on individual request size but on the sum of all in flight requests.

I like the approach suggested by @martijnvg as it will uncover other actions that should also be excluded from the limit check. Thanks for the idea.
</comment><comment author="nik9000" created="2016-04-25T12:31:29Z" id="214295422">&gt; explicit control over when request scheduling

Is it enough to wedge the request in a running state? If so you can use TestTaskPlugin to get in integration test for it. And I think an integration test for this kind of thing is important. But the whole 0 size breaker thing should work too.

Because this has tripped in tests I wonder if we're too close to the edge? It looks like the limit is 100% of the heap though. Am I reading that right?
</comment><comment author="jasontedor" created="2016-04-25T12:40:12Z" id="214297826">In addition to requiring tests, I left a question about the handling of replication requests.
</comment><comment author="danielmitterdorfer" created="2016-04-25T14:02:35Z" id="214342058">I think the test with a zero breaker limit is even better because it uncovers the actions that we need to exclude from request size limiting to keep a cluster running that does not receive client requests.

&gt; Because this has tripped in tests I wonder if we're too close to the edge? It looks like the limit is 100% of the heap though. Am I reading that right?

Yes, the default limit is `min(100%, parent_breaker_limit)` of the heap (as [discussed](https://github.com/elastic/elasticsearch/pull/17133#issuecomment-208262804) in the [original PR](https://github.com/elastic/elasticsearch/pull/17133)). In certain tests we explicitly set a low limit though (in `NettyHttpRequestSizeLimitIT` its currently 2kb). So we're hitting "just" this low limit not 100% of the heap.
</comment><comment author="nik9000" created="2016-04-25T14:06:57Z" id="214346769">&gt; So we're hitting "just" this low limit not 100% of the heap.

Got it. So the tests we think are failing spuriously come from this?

&gt; keep a cluster running that does not receive client requests

Makes sense. I like it!
</comment><comment author="danielmitterdorfer" created="2016-04-25T14:12:36Z" id="214351207">&gt; So the tests we think are failing spuriously come from this?

In the past, `NettyHttpRequestSizeLimitIT` either failed for several reasons but never because we hit 100% of the heap. IIRC the reasons were:
- We did not send enough requests so that there were enough of them in flight in certain scenarios (fixed directly in the test)
- Internal transport requests hit a limit when they shouldn't be included in the size limiting in the first place (to be fixed with this PR)
</comment><comment author="danielmitterdorfer" created="2016-04-25T14:15:00Z" id="214353308">I pushed another commit which adds an integration test now as suggested by @martijnvg. Can you have a look @jasontedor / @nik9000? 

I also excluded all transport actions from request size limiting which were uncovered by the test and decided to overload the constructor of some transport base classes instead of changing all call sites as it seemed to be the lesser of the two evils to me.
</comment><comment author="jasontedor" created="2016-04-25T14:18:48Z" id="214355753">I don't like that there isn't a test for each of the actions that are being whitelisted from the size checks, especially `TransportReplicationAction`; this behavior is too important to not be testing.
</comment><comment author="danielmitterdorfer" created="2016-04-26T08:33:30Z" id="214667826">@jasontedor: I've added a proof of concept test for TRA (`TransportReplicationLimitTests`). I am not really familiar with this area but I tried to isolate exactly this request and ensure that the request gets through without triggering the circuit breaker but others doesn't (as the limit is zero). I'd first iterate on this test and then add tests for the other actions.
</comment><comment author="jasontedor" created="2016-05-05T15:32:48Z" id="217187461">@danielmitterdorfer I looked at the `TransportReplicationLimitTests`. I like the approach, thanks for doing this.
</comment><comment author="danielmitterdorfer" created="2016-05-09T12:03:46Z" id="217846109">@jasontedor Thanks for the review. I'll add then other tests in a similar fashion.
</comment><comment author="danielmitterdorfer" created="2016-05-09T13:06:06Z" id="217858701">After discussion with @bleskes and @jasontedor we reduce the list of excluded requests to the bare minimum:
- MasterPingRequest
- PingRequest

All other requests will trip the in-flight request circuit breaker. The two mentioned requests are excluded because they are (a) very lightweight (in terms of memory usage) and (b) if pings fail, we probably move a lot of data around. As the intention of in-flight request size limiting on transport level is to prevent too much GC activity we have decided to be more conservative and include all but the ping requests.
</comment><comment author="danielmitterdorfer" created="2016-05-10T08:16:30Z" id="218088985">@jasontedor I have pushed another commit that just excludes pings from the limit check and added tests for that. As TRA is now treated as any other action, I've also removed `TransportReplicationLimitTests` again.
</comment><comment author="danielmitterdorfer" created="2016-05-11T12:52:34Z" id="218449865">@jasontedor Both nitpicks are gone now. :)
</comment><comment author="jasontedor" created="2016-05-12T13:57:15Z" id="218764963">@danielmitterdorfer Two more comments, and then we are going to be good.
</comment><comment author="danielmitterdorfer" created="2016-05-13T06:34:03Z" id="218962925">@jasontedor I addressed your comments now.
</comment><comment author="jasontedor" created="2016-05-13T12:11:06Z" id="219026220">LGTM.
</comment><comment author="danielmitterdorfer" created="2016-05-13T12:13:54Z" id="219026689">Thanks for your review @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>serial-diff-aggregation.asciidoc: fix a mistake</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17950</link><project id="" key="" /><description /><key id="150821273">17950</key><summary>serial-diff-aggregation.asciidoc: fix a mistake</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">golubev</reporter><labels><label>docs</label></labels><created>2016-04-25T10:52:32Z</created><updated>2016-04-25T14:29:35Z</updated><resolved>2016-04-25T11:43:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-25T11:43:48Z" id="214278123">Looks good! Thanks!
</comment><comment author="nik9000" created="2016-04-25T11:46:15Z" id="214278612">2.x: ad09b2c
master: 2f6405e
</comment><comment author="golubev" created="2016-04-25T14:29:35Z" id="214361026">Thanks, @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix exists method for list settings when using numbered setting format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17949</link><project id="" key="" /><description>The list settings parser supports retrieving lists defined in settings that use a key followed by a `.` and a
number (for example `foo.bar.0`). However, the exists method would indicate that the provided settings
do not contain a value for this setting. This change makes it so that the exists method now handles this
format.
</description><key id="150821203">17949</key><summary>fix exists method for list settings when using numbered setting format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-25T10:52:05Z</created><updated>2016-04-28T16:25:48Z</updated><resolved>2016-04-28T16:25:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-28T14:54:01Z" id="215452116">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>convert settings for ResourceWatcherService to new infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17948</link><project id="" key="" /><description>This commit converts the settings for the ResourceWatcherService to use the new infrastructure and
registers the settings so that they do not cause errors when used.
</description><key id="150821023">17948</key><summary>convert settings for ResourceWatcherService to new infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-25T10:50:57Z</created><updated>2016-04-25T11:08:40Z</updated><resolved>2016-04-25T11:08:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-25T10:56:22Z" id="214263801">I left one comment which you can consider at your discretion. Otherwise, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing: Remove unused junit rule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17947</link><project id="" key="" /><description>This rule was used to repeat failed tests due to binding on an already bound port. The test has been fixed
so we can get rid of this rule as well.
</description><key id="150820273">17947</key><summary>Testing: Remove unused junit rule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label></labels><created>2016-04-25T10:46:32Z</created><updated>2016-04-26T07:53:50Z</updated><resolved>2016-04-26T07:53:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-25T10:50:50Z" id="214262518">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to disable elasticsearch default log&#65311;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17946</link><project id="" key="" /><description>## always stdout,  I want to get it .

```
Starting new HTTP connection (1)
POST http://xx.xx.xx.xx:6700/_bulk [status:200 request:0.024s]
```

my python settting:

```
logging.basicConfig(filename = os.path.join(os.getcwd(), 'migrate_pipe.log'),\
    level = logging.INFO, filemode = 'a', format = '%(asctime)s - %(process)s - %(levelname)s:\
    %(message)s')
logging.getLogger('elasticsearch.trace').setLevel(logging.CRITICAL)
logging.getLogger('requests').setLevel(logging.CRITICAL)
logging.getLogger("urllib3").setLevel(logging.CRITICAL)
```

thanks U 
</description><key id="150815236">17946</key><summary>how to disable elasticsearch default log&#65311;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rfyiamcool</reporter><labels /><created>2016-04-25T10:26:36Z</created><updated>2016-04-25T10:35:46Z</updated><resolved>2016-04-25T10:35:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-25T10:35:46Z" id="214257867">Please note that Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions. Additionally, there is a repository for the [Elasticsearch Python client](https://github.com/elastic/elasticsearch-py).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document missing shard version in routing table of cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17945</link><project id="" key="" /><description>as breaking change

removed as per: https://github.com/elastic/elasticsearch/pull/16243

because of: https://github.com/elastic/elasticsearch/issues/14739
</description><key id="150806727">17945</key><summary>Document missing shard version in routing table of cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>docs</label><label>v5.0.0-alpha2</label></labels><created>2016-04-25T09:50:14Z</created><updated>2016-04-26T08:50:38Z</updated><resolved>2016-04-26T08:50:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T19:20:10Z" id="214488618">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin script not found inside bin folder.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17944</link><project id="" key="" /><description>2.3.1

**JVM version**:

El Capitan latest

I installed elastic search but plugin script not found inside bin folder so I am unable to install any plugin.

**Steps to reproduce**:
 1.Install latest elasticsearch
 2.check bin directory
</description><key id="150598515">17944</key><summary>Plugin script not found inside bin folder.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iqbalmalik89</reporter><labels /><created>2016-04-23T21:32:44Z</created><updated>2016-04-24T14:47:55Z</updated><resolved>2016-04-24T11:02:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-23T22:02:05Z" id="213841449">Your reproduction steps are not complete because you didn't specify which distribution you used to install, nor your installation procedure.

For the zip:

``` bash
$ curl -sS -L -O https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.3.1/elasticsearch-2.3.1.zip
$ unzip -l elasticsearch-2.3.1.zip | grep "bin/plugin"
     1303  04-04-16 12:23   elasticsearch-2.3.1/bin/plugin.bat
     2992  04-04-16 12:23   elasticsearch-2.3.1/bin/plugin
```

For the tar:

``` bash
$ curl -sS -L -O https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.1/elasticsearch-2.3.1.tar.gz                                                                                                                                                                                           
$ gunzip elasticsearch-2.3.1.tar.gz 
$ tar -tf elasticsearch-2.3.1.tar | grep "bin/plugin"
elasticsearch-2.3.1/bin/plugin.bat
elasticsearch-2.3.1/bin/plugin
```

For the deb:

``` bash
$ curl -sS -L -O https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/deb/elasticsearch/2.3.1/elasticsearch-2.3.1.deb
$ dpkg -c elasticsearch-2.3.1.deb | grep "bin/plugin"
-rwxr-xr-x root/root      3044 2016-04-04 12:33 ./usr/share/elasticsearch/bin/plugin
$ sudo dpkg -i elasticsearch-2.3.1.deb
Selecting previously unselected package elasticsearch.
(Reading database ... 66225 files and directories currently installed.)
Unpacking elasticsearch (from elasticsearch-2.3.1.deb) ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Setting up elasticsearch (2.3.1) ...
Processing triggers for ureadahead ...
$ ls -al /usr/share/elasticsearch/bin/plugin
-rwxr-xr-x 1 root root 3044 Apr  4 12:33 /usr/share/elasticsearch/bin/plugin
```

For the rpm:

``` bash
$ curl -sS -L -O https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/rpm/elasticsearch/2.3.1/elasticsearch-2.3.1.rpm
$ rpm -ql -p elasticsearch-2.3.1.rpm | grep "bin/plugin"
warning: elasticsearch-2.3.1.rpm: Header V4 RSA/SHA1 Signature, key ID d88e42b4: NOKEY
/usr/share/elasticsearch/bin/plugin
$ sudo rpm -i elasticsearch-2.3.1.rpm
warning: elasticsearch-2.3.1.rpm: Header V4 RSA/SHA1 Signature, key ID d88e42b4: NOKEY
Creating elasticsearch group... OK
Creating elasticsearch user... OK
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service
$ ls -al /usr/share/elasticsearch/bin/plugin
-rwxr-xr-x. 1 root root 3048 Apr  4 12:34 /usr/share/elasticsearch/bin/plugin
```
</comment><comment author="iqbalmalik89" created="2016-04-24T08:19:28Z" id="213913251">I am using Mac el capitan. I used brew to install elastic search.
</comment><comment author="jasontedor" created="2016-04-24T11:02:53Z" id="213936199">When you install Elasticsearch from homebrew there is a message displayed on the terminal under "Caveats" that says:

```
plugin script: /usr/local/Cellar/elasticsearch/2.3.1/libexec/bin/plugin
```

``` bash
$ brew install elasticsearch
==&gt; Downloading https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.1/elasticsearch-2.3.1.tar.gz
Already downloaded: /Library/Caches/Homebrew/elasticsearch-2.3.1.tar.gz
==&gt; Caveats
Data:    /usr/local/var/elasticsearch/elasticsearch_jason/
Logs:    /usr/local/var/log/elasticsearch/elasticsearch_jason.log
Plugins: /usr/local/Cellar/elasticsearch/2.3.1/libexec/plugins/
Config:  /usr/local/etc/elasticsearch/
plugin script: /usr/local/Cellar/elasticsearch/2.3.1/libexec/bin/plugin

To have launchd start elasticsearch at login:
  ln -sfv /usr/local/opt/elasticsearch/*.plist ~/Library/LaunchAgents
Then to load elasticsearch now:
  launchctl load ~/Library/LaunchAgents/homebrew.mxcl.elasticsearch.plist
Or, if you don't want/need launchctl, you can just run:
  elasticsearch
==&gt; Summary
&#127866;  /usr/local/Cellar/elasticsearch/2.3.1: 59 files, 29.4M, built in 0 seconds
```

If you check this location, you will see:

``` bash
$ ls -al /usr/local/Cellar/elasticsearch/2.3.1/libexec/bin/plugin 
-rwxr-xr-x  1 jason  admin  3056 Apr 24 06:59 /usr/local/Cellar/elasticsearch/2.3.1/libexec/bin/plugin
```

The [reason](https://github.com/Homebrew/legacy-homebrew/pull/47941) for this is because executables that go in the `bin` folder of an installed formula get symlinked from `/usr/local/bin`, this location is usually on the path, and so we would prefer to not put such a generically-named script on the path.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add kotlin query DSL to community clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17943</link><project id="" key="" /><description>Hi, 

I've created a query builder DSL for Kotlin language that mimics the JSON query DSL.
This makes it easier to translate the documentation targeting the JSON api onto kotlin code.
Please consider adding it to the list of community clients.

Thanks,

Mike Buhot
</description><key id="150547964">17943</key><summary>Add kotlin query DSL to community clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mbuhot</reporter><labels><label>docs</label></labels><created>2016-04-23T11:55:00Z</created><updated>2016-04-26T14:03:58Z</updated><resolved>2016-04-26T14:03:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T19:04:40Z" id="214481515">Hi @mbuhot 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="mbuhot" created="2016-04-25T21:24:49Z" id="214527946">Done.

https://elasticsearch.na1.echosign.com/public/viewAgreement?tsid=CBFCIBAA3AAABLblqZhBQZLt-rR3JjDliVM_K5gkYKiTTfe0OcyPleQ7mO-heFzFMPwLVwt-WJ-02zq9WyrRbzT0f10-sMDvjCybtdM2N&amp;

&gt; On 26 Apr 2016, at 5:06 AM, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; Hi @mbuhot
&gt; 
&gt; Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
</comment><comment author="clintongormley" created="2016-04-26T14:03:58Z" id="214755814">thanks @mbuhot 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Update example aggs to use dynamic keyword field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17942</link><project id="" key="" /><description>The getting started docs use dynamic mappings. With the recent change to
string split into text and keyword, text lost the default ability to do
aggs. This was added back in #17188. This change updates the getting
started examples to use the keyword multi field added to dynamically
mapped text fields.

closes #17941
</description><key id="150495070">17942</key><summary>Docs: Update example aggs to use dynamic keyword field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>docs</label><label>v5.0.0-alpha2</label></labels><created>2016-04-22T23:37:41Z</created><updated>2016-04-23T23:20:07Z</updated><resolved>2016-04-23T23:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-23T10:26:03Z" id="213713620">LGTM. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>v5 Aggregations Documentation inaccurate or bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17941</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
v5 alpha 1
**JVM version**:
8
**OS version**:
Mac OS X El Capitan
**Description of the problem including expected versus actual behavior**:
https://www.elastic.co/guide/en/elasticsearch/reference/master/_executing_aggregations.html

That tutorial suggests that running this code:
`{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state"
      }
    }
  }
}`

should return an aggregated, result, instead, we receive this:

`
{
  "error": {
    "root_cause": [
      {
        "type": "illegal_state_exception",
        "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [state] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory."
      }
    ],
    "type": "search_phase_execution_exception",
    "reason": "all shards failed",
    "phase": "query",
    "grouped": true,
    "failed_shards": [
      {
        "shard": 0,
        "index": "bank",
        "node": "101piinIR-Wpum7j28_RPg",
        "reason": {
          "type": "illegal_state_exception",
          "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [state] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory."
        }
      }
    ],
    "caused_by": {
      "type": "illegal_state_exception",
      "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [state] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory."
    }
  },
  "status": 500
}
`
**Steps to reproduce**:
see above

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="150490193">17941</key><summary>v5 Aggregations Documentation inaccurate or bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">viztastic</reporter><labels /><created>2016-04-22T22:57:55Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-04-23T23:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="thomasmodeneis" created="2016-11-04T18:01:30Z" id="258504737">Oh guys, really bad breaking change that is still not clear on docs.
Not nice.
</comment><comment author="clintongormley" created="2016-11-05T14:34:22Z" id="258615431">@thomasmodeneis you'll find this documented in the breaking changes section: https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_aggregations_changes.html#_literal_size_0_literal_on_terms_significant_terms_and_geohash_grid_aggregations
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate properties values according to database type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17940</link><project id="" key="" /><description>Fixes #17683.
</description><key id="150479582">17940</key><summary>Validate properties values according to database type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-22T21:54:19Z</created><updated>2016-05-26T11:44:03Z</updated><resolved>2016-04-29T14:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-29T08:08:52Z" id="215654454">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GC metrics using /_cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17939</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.5.2

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

Is it possible to use /_cat API to get gc metrics ? If yes, what is the API ?
</description><key id="150478135">17939</key><summary>GC metrics using /_cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amitranjan-fk</reporter><labels /><created>2016-04-22T21:45:32Z</created><updated>2016-04-23T21:29:03Z</updated><resolved>2016-04-23T16:03:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-23T16:03:46Z" id="213772338">Please note that Elastic reserves GitHub for verified bug reports and feature requests. You can use the [Elastic Discourse formus](https://discuss.elastic.co) to ask general questions.
</comment><comment author="McStork" created="2016-04-23T21:29:03Z" id="213834928">@amitranjan-fk Pretty sure it's not possible right now. See feature request https://github.com/elastic/elasticsearch/issues/17224.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cli: Improve output for usage errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17938</link><project id="" key="" /><description>When a cli throws a USAGE error, it is implied that the user did
something wrong, and probably needs help in understanding the cli
arguments. This change adds help output before the usage error is
printed.
</description><key id="150472227">17938</key><summary>Cli: Improve output for usage errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-22T21:11:41Z</created><updated>2016-04-25T18:55:13Z</updated><resolved>2016-04-22T21:58:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-22T21:12:19Z" id="213590306">With this change, running `bin/elasticsearch-plugin` with no arguments now looks like this:

```
[14:09:03][~/Code/elasticsearch/distribution/zip/build/distributions/elasticsearch-5.0.0-SNAPSHOT]$ bin/elasticsearch-plugin
A tool for managing installed elasticsearch plugins

Commands
--------
list - Lists installed elasticsearch plugins
install - Install a plugin
remove - Removes a plugin from elasticsearch

Non-option arguments:
command

Option         Description
------         -----------
-h, --help     show help
-s, --silent   show minimal output
-v, --verbose  show verbose output
ERROR: Missing command
```
</comment><comment author="dakrone" created="2016-04-22T21:51:36Z" id="213600573">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ununsed dev tools files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17937</link><project id="" key="" /><description /><key id="150450022">17937</key><summary>Remove ununsed dev tools files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-22T19:13:50Z</created><updated>2016-04-22T19:19:23Z</updated><resolved>2016-04-22T19:19:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-22T19:14:31Z" id="213558369">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem with far future millisecond values for Date type in ES 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17936</link><project id="" key="" /><description>This used to work in 1.3.4 but currently in 2.3.1 this seems to have broken.

```
 curl -XPUT http://localhost:9200/test -d '{
  "mappings": {
    "my_type": {
      "properties": {
        "date": {
          "type": "date"
        }
      }
    }
  }
}'
```

`curl -XPUT http://localhost:9200/test/my_type/2 -d '{"date" : 999999999999}'`

{"index":"test","type":"my_type","id":"2","version":1,"_shards":{"total":1,"successful":1,"failed":0},"created":true}

`curl -XPUT http://localhost:9200/test/my_type/3 -d '{"date" : 10000000000000}'`
or for my exact data 
`curl -XPUT http://localhost:9200/test/my_type/3 -d '{"date" : 32603904000000}'
`
[2016-04-20 13:44:27,278][DEBUG][action.index ] [VeevaNetworkNode1] failed to execute [index {[test][my_type][3], source[{"date" : 32603904000000}]}] on [[test][4]]
MapperParsingException[failed to parse [date]]; nested: IllegalArgumentException[Invalid format: "32603904000000" is malformed at "3904000000"];

Looking at the code in org.elasticsearch.common.joda.Joda$EpochTimeParser/$EpochTimePrinter it looks that the 13 digit limit is hardcoded but for the life of me I can't figure out why it would be.
</description><key id="150438385">17936</key><summary>Problem with far future millisecond values for Date type in ES 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Omega359</reporter><labels><label>:Dates</label><label>enhancement</label></labels><created>2016-04-22T18:18:55Z</created><updated>2016-05-31T11:32:58Z</updated><resolved>2016-05-31T11:32:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-22T20:55:36Z" id="213585707">It seems to me the entire block in the epoch time parser checking for "too long" values should just be removed? @spinscale I see this was part of your original change to separate out epoch parsing from normal date parsing, but since the date format parsing is tried in order, why does epoch parsing need to do anything except just parse a long?
</comment><comment author="spinscale" created="2016-04-25T08:32:16Z" id="214205765">agreed, I dont see any immediate reason to not support dates lower than 1653 and higher than 2286. I guess I could not imagine a use-case for that, as regular dates would have been so much simpler, see the discussion in #11482
</comment><comment author="spinscale" created="2016-05-31T11:32:58Z" id="222662854">closed by #18509
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Example in documentation for _parent fails </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17935</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:2.3.1

**JVM version**:
java version "1.8.0_77"
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)

**OS version**:
Linux customerhostname 3.10.0-327.10.1.el7.x86_64 #1 SMP Tue Feb 16 17:03:50 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**: Following the documentation on https://www.elastic.co/guide/en/elasticsearch/reference/2.3/mapping-parent-field.html I get a 400 error in sense for the second PUT, following it in FireFox RestClient (using POST not GET as per https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-body.html), I get no results for the final has_child query.

**Steps to reproduce**:
1. Install elastic, single node, default everything
2. Apply mapping and add data as shown on https://www.elastic.co/guide/en/elasticsearch/reference/2.3/mapping-parent-field.html
3. Issue query shown at https://www.elastic.co/guide/en/elasticsearch/reference/2.3/mapping-parent-field.html

**Expected result**: One my_parent document should be returned (even if further parent docs are added)...

**Actual result**: No documents are ever returned by any "has_child" query, including a match all query such as:

```
POST my_index/my_parent/_search?routing=_id
{
  "query": {
    "has_child": { 
      "type": "my_child",
      "query": { "match_all": {} }
    }
  }
}
```

Running queries with a GET line in Sense (after posting the docs with RestClient due to aforementioned 400 error) also does not work.
</description><key id="150432763">17935</key><summary>Example in documentation for _parent fails </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fsparv</reporter><labels /><created>2016-04-22T17:54:11Z</created><updated>2016-04-25T19:27:44Z</updated><resolved>2016-04-25T19:27:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-25T09:30:49Z" id="214230546">I tried this, the problem seems to be when copy/pasting the example, there is a trailing white space in the PUT url for the child documents. This seems to trip the parsing of the url parameters. After removing the trailing whitespaces, the example otherwise seems to be okay. I wonder if Sense should trim trailing whitespaces by default. 
</comment><comment author="girirajsharma" created="2016-04-25T10:37:29Z" id="214258410">@cbuescher I also tried it and concluded the same.
</comment><comment author="clintongormley" created="2016-04-25T19:27:44Z" id="214491640">Closed in favour of https://github.com/elastic/sense/issues/136
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Detect Catastrophic backtracking when using the pattern_replace char_filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17934</link><project id="" key="" /><description>I'm using the [Pattern Replace Char Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-replace-charfilter.html) to filter out urls from my query text before it is tokenised. But I found that some input strings cause ElasticSearch to crash - It would be good if ElasticSearch could timeout or detect problems with the regexp instead of "hanging" forever (slowly eating more CPU and memory).

---
## Example

``` php
// Character filters are used to "tidy up" a string *before* it is tokenized.
'char_filter' =&gt; [
    'url_removal_pattern' =&gt; [
        'type'        =&gt; 'pattern_replace',
        'pattern'     =&gt; '(?mi)\b((?:[a-z][\w-]+:(?:\/{1,3}|[a-z0-9%])|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}\/)(?:[^\s()&lt;&gt;]+|\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\))+(?:\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\)|[^\s`!()\[\]{};:\'".,&lt;&gt;?&#171;&#187;&#8220;&#8221;&#8216;&#8217;]))',
        'replacement' =&gt; '',
    ],
```

This filter was working fine for some weeks until suddenly ElasticSearch started crashing. We found someone was trying to do a javascript injection attack in our search box.

I pasted the regex and the attack string into https://regex101.com 
- Regexp: 
  - `(?mi)\b((?:[a-z][\w-]+:(?:\/{1,3}|[a-z0-9%])|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}\/)(?:[^\s()&lt;&gt;]+|\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\))+(?:\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\)|[^\s!()\[\]{};:\'".,&lt;&gt;?&#171;&#187;&#8220;&#8221;&#8216;&#8217;]))`
- Test string: 
  - `twitter.com/widgets.js\";fjs.parentNode.insertBefore(js,fjs);}}(document,\"script\",\"twitter-wjs\"`

https://regex101.com shows the problem to be "Catastrophic backtracking"

&gt; Catastrophic backtracking has been detected and the execution of your expression has been halted. To find out more what this is, please read the following article: [Runaway Regular Expressions](http://www.regular-expressions.info/catastrophic.html).

It would be great if ElasticSearch could detect "Catastrophic backtracking" and throw a error. It would be great if there was a parameter on `pattern_replace` to provide a timeout, any queries that go over this timeout would just not match, rather than cause a hang.

---

As an aside, I created a unit test for our PHP application that uses the same regexp and test string. (PHP can understand the same regexp, even though it's obviously for Java in the ElasticSearch case) . Interestingly in php, the regex results in `null` which is the documented response of [preg_replace](http://php.net/manual/en/function.preg-replace.php) when a error occurs. If PHP can return a error rather than crashing - surely ElasticSearch can too :trollface: ?

``` php
namespace app\tests\unit;
use \yii\codeception\TestCase;

class TagsControllerTest extends TestCase
{
    public function testRegexForURLDetection()
    {
        $regex = '(?mi)\b((?:[a-z][\w-]+:(?:\/{1,3}|[a-z0-9%])|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}\/)(?:[^\s()&lt;&gt;]+|\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\))+(?:\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\)|[^\s`!()\[\]{};:\'".,&lt;&gt;?&#171;&#187;&#8220;&#8221;&#8216;&#8217;]))';
        // Test the Catastrophic backtracking problem
        $testString = "twitter.com/widgets.js\";fjs.parentNode.insertBefore(js,fjs);}}(document,\"script\",\"twitter-wjs\"";
        // This shows the regex is not working for our test string - it gives null but should give 'hello '
        $this-&gt;assertEquals(null, preg_replace("/$regex/", '', "hello $testString"));
    }
}
```
</description><key id="150431706">17934</key><summary>Detect Catastrophic backtracking when using the pattern_replace char_filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tomfotherby</reporter><labels><label>:Analysis</label><label>discuss</label></labels><created>2016-04-22T17:48:34Z</created><updated>2016-04-26T09:42:32Z</updated><resolved>2016-04-26T09:42:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T18:49:27Z" id="214477102">Hi @tomfotherby 

I think you're out of luck here...  This is firmly in Lucene land. Might be better to open an issue there.
</comment><comment author="tomfotherby" created="2016-04-26T09:42:32Z" id="214685728">Thanks @clintongormley - I followed your advice and opened a lucene ticket: https://issues.apache.org/jira/browse/LUCENE-7256
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove camelCase support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17933</link><project id="" key="" /><description>Now that the current uses of magical camelCase support have been
deprecated, we can remove these in master (sans remaining issues like
BulkRequest). This change removes camel case support from ParseField,
query types, analysis, and settings lookup.

see #8988
</description><key id="150413386">17933</key><summary>Remove camelCase support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:REST</label><label>breaking</label><label>v5.0.0-alpha2</label></labels><created>2016-04-22T16:21:46Z</created><updated>2016-04-25T14:37:47Z</updated><resolved>2016-04-22T20:46:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-22T16:43:50Z" id="213503997">I'm confused by the linked issue, if we are deprecating in 5.0 don't we need to keep these around until 6.0?
</comment><comment author="rjernst" created="2016-04-22T16:47:14Z" id="213505945">The linked issue tags are wrong. It was deprecated in 2.3.2.
</comment><comment author="dakrone" created="2016-04-22T16:49:31Z" id="213506778">That makes much more sense then :) LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement response_format=short on _bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17932</link><project id="" key="" /><description>Now we'll always count the number of successes and, if the request
contains `?response_format=short`, we'll omit the successes from
the items list.

When `response_format` isn't set to `short` the items that come back
always line up with the request. They won't do that with `short`.
Instead clients have to use the _index/_type/_id triple to identify
the item that failed. If they happen to use the same triple they are
just out of luck. That is a silly thing to do anyway.

Closes #2661
</description><key id="150394097">17932</key><summary>Implement response_format=short on _bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label></labels><created>2016-04-22T15:03:26Z</created><updated>2016-10-13T16:21:32Z</updated><resolved>2016-07-11T17:13:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mbonaci" created="2016-04-27T13:10:39Z" id="215076645">Hi @nik9000, very nice.
I was just wondering would it be possible to just add ordinal numbers of failed docs to the `short` response, instead of being forced to rely on manually generated IDs?

I'm guessing that [changing the response code](https://github.com/elastic/elasticsearch/issues/2661#issuecomment-151838791) of (partially) failed bulks would have been to invasive?
Thanks
</comment><comment author="nik9000" created="2016-04-27T13:26:01Z" id="215082180">&gt; I'm guessing that changing the response code of (partially) failed bulks would have been to invasive?

Maybe not too invasive, but something that we should do in another PR. That kind of change is super-breaking even if more correct. I, personally, don't think we should reply with 200 or whatever when we have failures at all. But that is a battle for another time.

&gt; just add ordinal numbers of failed docs to the short response

Not a bad idea. I'll have a look at that. As it stands `short` is just "less" than the regular response. This'd make it "different" than. But I think it makes sense!
</comment><comment author="jasontedor" created="2016-04-27T13:28:32Z" id="215082840">&gt; I, personally, don't think we should reply with 200 or whatever when we have failures at all.

+1
</comment><comment author="nik9000" created="2016-04-28T17:27:13Z" id="215502305">&gt; just add ordinal numbers of failed docs to the short response

And done.
</comment><comment author="nik9000" created="2016-04-28T17:29:04Z" id="215502808">This is officially read for review for anyone that wants it. As compared to some of the scarier changes deep in the guts this is 100% REST layer change!
</comment><comment author="mbonaci" created="2016-04-28T17:33:22Z" id="215503978">You rule @nik9000 &#128175; x &#128077; 
</comment><comment author="prog8" created="2016-04-29T13:54:50Z" id="215721750">@nik9000 you mentioned that this is 100% REST layer change. Does it mean that transport client will still get huge response?
</comment><comment author="nik9000" created="2016-05-02T13:05:45Z" id="216229631">&gt; @nik9000 you mentioned that this is 100% REST layer change. Does it mean that transport client will still get huge response?

Much less huge, but still the full response, yes.
</comment><comment author="prog8" created="2016-05-02T13:08:25Z" id="216230109">@nik9000 Thank you for information :)
</comment><comment author="nik9000" created="2016-05-02T13:14:09Z" id="216231211">&gt; @nik9000 Thank you for information :)

Sure! There are lots more spaces where this _could_ save space but they didn't seem nearly as impactful as the REST change. And they'd have been much harder to implement. REST is fairly contained.
</comment><comment author="greenpau" created="2016-06-02T15:14:55Z" id="223323464">&#128077; 
</comment><comment author="dakrone" created="2016-07-11T17:07:59Z" id="231798930">@nik9000 this LGTM, is this waiting on anything for it to be merged? more reviews?
</comment><comment author="nik9000" created="2016-07-11T17:13:07Z" id="231800382">&gt; @nik9000 this LGTM, is this waiting on anything for it to be merged? more reviews?

Over in #2661 @kimchy said he'd prefer not to do it this way. I'm just going to close this.
</comment><comment author="jvburnes" created="2016-10-13T16:21:09Z" id="253563377">I'm not an ElasticSearch architect or code contributor, but this just seems to be the obvious and clean engineering solution to the problem.  Sending 50k docs to index only to have 50k response bodies sent back when none or only a few failed seems to be a complete waste of CPU, memory and network bandwidth.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for fallback settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17931</link><project id="" key="" /><description>Follow-up of #17917. The ability to have fallback settings makes things complicated when eg. you want to get a value from several Settings instances. I think call sites should handle the fall back themselves in an explicit way rather than relying on the Settings class to do so.
</description><key id="150393131">17931</key><summary>Remove support for fallback settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Settings</label><label>adoptme</label></labels><created>2016-04-22T14:59:41Z</created><updated>2016-04-22T15:12:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-22T15:07:08Z" id="213466485">Do you want to remove `Setting#get(Settings, Settings)`. Or the fallback `fallbackSetting` support? I think we could keep them if we were willing to spend a bit more time on them and we were willing to overhaul how settings are built. I think we should do the second thing anyway because there are too many constructors and constructor like things on Setting.
</comment><comment author="jpountz" created="2016-04-22T15:12:56Z" id="213468102">&gt; Or the fallback fallbackSetting support?

Yes this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] append test class name to node name of external nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17930</link><project id="" key="" /><description>External node logs do not appear together with the "regular" node
logs but are instead printed before or after the test logs.
This makes debugging rather tricky as several test suites
can spin up external nodes with the same name and we cannot tell from the
logs which is used for which test.
Appending the class name of the test at least allows us to know
which suite the node is used in.
</description><key id="150383893">17930</key><summary>[TEST] append test class name to node name of external nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2016-04-22T14:28:03Z</created><updated>2016-04-25T18:27:24Z</updated><resolved>2016-04-22T16:06:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-04-22T14:35:23Z" id="213451006">example log lines ow:

```
[2016-04-22 16:16:43,105][DEBUG][discovery.zen.fd         ] [external_BasicAnalysisBackwardCompatibilityIT_0] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2016-04-22 16:16:43,107][DEBUG][discovery.zen.fd         ] [external_BasicAnalysisBackwardCompatibilityIT_0] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2016-04-22 16:16:43,114][DEBUG][discovery.zen.elect      ] [external_GetIndexBackwardsCompatibilityIT_0] using minimum_master_nodes [-1]
[2016-04-22 16:16:43,115][DEBUG][discovery.zen.ping.unicast] [external_GetIndexBackwardsCompatibilityIT_0] using initial hosts [localhost:9500], with concurrent_connects [10]
[2016-04-22 16:16:43,121][DEBUG][discovery.zen            ] [external_GetIndexBackwardsCompatibilityIT_0] using ping.timeout [3s], join.timeout [1m], master_election.filter_client [true], master_election.filter_data [false]
[2016-04-22 16:16:43,122][DEBUG][discovery.zen.fd         ] [external_GetIndexBackwardsCompatibilityIT_0] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]

```

@nik9000 Is that helpful?
</comment><comment author="nik9000" created="2016-04-22T14:47:18Z" id="213456715">LGTM.

I hope it is helpful!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete archived cluster settings with `null`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17929</link><project id="" key="" /><description>You should be able to delete archived cluster settings with the following:

```
PUT _cluster/settings
{
  "persistent": {
    "archived": null
  }
}
```

It returns OK, but the settings aren't deleted.
</description><key id="150356745">17929</key><summary>Delete archived cluster settings with `null`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-04-22T12:31:38Z</created><updated>2016-06-28T09:29:01Z</updated><resolved>2016-06-17T13:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-17T13:27:35Z" id="226768428">The proper way to delete these settings as https://github.com/elastic/elasticsearch/blob/master/docs/reference/cluster/update-settings.asciidoc indicates is:

`
PUT _cluster/settings
  {
    "persistent": {
      "archived.*": null
    }
  }
`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to store results for long running tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17928</link><project id="" key="" /><description>The results of the tasks are stored in a special index `.results`
</description><key id="150339233">17928</key><summary>Add ability to store results for long running tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-04-22T11:01:49Z</created><updated>2016-06-18T00:34:29Z</updated><resolved>2016-05-27T00:46:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-22T11:46:17Z" id="213390565">Left some small things and one big one: I think that users should be able to specify where the task is persisted. I think instead of a template for the index we should document something and say "we think this is how most task persistence should be shaped, but it is up to you". Basically I want the index to be "the user's index" rather than ours. After all, they have to clean it up over time and, hell, maybe they want to persist some types of things into a different index.
</comment><comment author="imotov" created="2016-04-22T15:01:17Z" id="213464601">After some additional discussions with @nik9000, we decided to require users to specify an index name if they want to save the result. In order to accommodate that we will need to create the index and mapping explicitly if they don't exist instead of relying on the index template.
</comment><comment author="clintongormley" created="2016-05-07T14:17:37Z" id="217640126">&gt; After some additional discussions with @nik9000, we decided to require users to specify an index name if they want to save the result. In order to accommodate that we will need to create the index and mapping explicitly if they don't exist instead of relying on the index template.

I'm not sure I agree with requiring a custom index here.  Things should work out of the box.  The task mgmt API should be able to fetch the status of finished tasks from the index directly, in which case it needs to know which index to talk to.

Let's talk about this when @imotov is back.
</comment><comment author="nik9000" created="2016-05-07T23:41:25Z" id="217677258">&gt; Let's talk about this when @imotov is back.

Yeah! I'm going to dump my reasoning here for later in case I forget!

My problem with storing task results into a special index is that it feel a bit too magical. Especially if we automatically save the results for all `?wait_for_completion=false` stuff into an index. Triple especially if we have the tasks API automatically `GET` from the index if the task is done. That index will grow unchecked and we'll have to introduce automatic deletes for old task results and and and. I'm worried about it, ok?

To me, forcing the user to write something like `?save=index/type` will make them own the index. Sure, we'll create the index for them if it isn't created and we'll apply some sensible default mapping or something, but we create indexes on the fly when users write to them all the time. It'll be the user's index. They named it, they manage it. Stuff only gets saved to it when they ask for it explicitly.

I also like making `?save` explicit because you can do it without `?wait_for_completion=false`. Like you could `?save` .1% of searches. Or you could `?save` **every** reindex just so you have a carbon copy replica of it. It seems safe because it isn't automatic and the we're saving before we return the request to the user so they see the slowdown it causes.
</comment><comment author="imotov" created="2016-05-20T13:25:17Z" id="220604663">@nik9000 I have pushed changes that reflects our recent discussion with @clintongormley. Could you take another look when you have a chance?
</comment><comment author="nik9000" created="2016-05-23T17:10:01Z" id="221033994">I left a few points that are open for discussion but I think it is basically done. Sorry to take so long before reviewing!
</comment><comment author="imotov" created="2016-05-23T21:58:10Z" id="221108674">Pushed changes to address the latest comments and suggestions. 
</comment><comment author="nik9000" created="2016-05-24T14:39:28Z" id="221292342">&gt; Pushed changes to address the latest comments and suggestions.

I'll read again soon!
</comment><comment author="nik9000" created="2016-05-24T16:25:27Z" id="221326705">I made a comment about one last thing I'd like to change but otherwise LGTM.
</comment><comment author="clintongormley" created="2016-06-01T13:27:04Z" id="222991467">@imotov we need documentation about how to clean out the `.results` index please
</comment><comment author="Mpdreamz" created="2016-06-17T09:34:49Z" id="226725180">Do we have plans to backport this to `2.x` ?
</comment><comment author="imotov" created="2016-06-18T00:34:29Z" id="226910699">@Mpdreamz no, not at the moment. Do you have a particular need for this to be backported to 2.x?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin cli: Print help without args</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17927</link><project id="" key="" /><description>Now, we run `bin/elasticsearch-plugin`, then get "ERROR: Missing command".
It would be good to print help.
</description><key id="150336775">17927</key><summary>Plugin cli: Print help without args</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2016-04-22T10:49:48Z</created><updated>2016-04-22T23:01:07Z</updated><resolved>2016-04-22T23:01:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-22T21:13:12Z" id="213590743">@johtani I don't think this should be specific to the plugin cli. It more has to do with how the cli infra works.  I opened an alternative PR: #17938
</comment><comment author="johtani" created="2016-04-22T23:01:07Z" id="213618816">@rjernst Nice! Thanks. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve how the multi search api concurrently executes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17926</link><project id="" key="" /><description>Today the multi search api executes all search requests in parallel without any limiting. So a multi search requests that holds many search requests can easily flood the search thread pool on nodes.

A better approach would if the multi search api works like the multi get api. We pick a shard copy for all shards that are going to be queried and send all search requests to nodes holding these shard copies to execute the query phase. Each node returns shard level results for all these search requests and the node that is coordinating the multi search request is then bulk reducing these shard level responses and after that the coordinating node will send requests for the fetch phase in the same way it did that for the query phase. The main point is that at the shard level we use only one thread to execute the shard level operations for all requests a node has received.
</description><key id="150330991">17926</key><summary>Improve how the multi search api concurrently executes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-22T10:19:19Z</created><updated>2016-06-13T08:14:02Z</updated><resolved>2016-06-13T08:14:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rnagappan" created="2016-04-25T22:28:51Z" id="214550212">This would be enormously helpful to us. Often we have to do lots of independent searches in a batch and the round-trip latency waiting for each one to complete costs us an awful lot of time. It would improve our throughput enormously if we could send say 1000 searches in a single batch and not have the cluster reject them all. So basically something like the bulk API but for search.
</comment><comment author="clintongormley" created="2016-05-20T09:50:27Z" id="220563793">We should add a concurrency parameter to limit the number of searches from an msearch that are run in parallel.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Past releases" page has invisible pagination</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17925</link><project id="" key="" /><description>Hello, 

I noticed that page [https://www.elastic.co/downloads/past-releases](https://www.elastic.co/downloads/past-releases) has invisiblepagination on Chrome on Windows (version 50.0.2661.87 m) and IE 10.

Chrome:
![image](https://cloud.githubusercontent.com/assets/18612366/14738063/d5731348-087f-11e6-9baf-cbb3187461d6.png)

IE:
![image](https://cloud.githubusercontent.com/assets/18612366/14738023/8587ae0c-087f-11e6-9bd4-8e96cea5d54b.png)
</description><key id="150325721">17925</key><summary>"Past releases" page has invisible pagination</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamMo</reporter><labels /><created>2016-04-22T09:51:11Z</created><updated>2016-04-22T16:49:26Z</updated><resolved>2016-04-22T11:39:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="girirajsharma" created="2016-04-22T10:18:09Z" id="213368525">@adamMo It doesn't seems to be invisible. It auto shifts left or right depending on whether you move forward or backward. Its just a specific type of UI/UX unlike the normal bootstrap one.
</comment><comment author="adamMo" created="2016-04-22T11:39:34Z" id="213389380">@girirajsharma Now I compared with other PC's in my office and it must be something with chrome addons I use or with my screen contrast because it works well for others - I just don't see other digits than currently selected. 

I'm closing this unfair issue and wish you a good day!
</comment><comment author="girirajsharma" created="2016-04-22T12:22:34Z" id="213401865">@adamMo No problem at all. I too observed the pagination numbers turn invisible on a higher contrast.
</comment><comment author="sylvie777" created="2016-04-22T16:32:53Z" id="213500578">@girirajsharma @adamMo Hi there! By "invisible", do you mean the font color is too light? I can see the page numbers in the screenshots provided above. 
</comment><comment author="adamMo" created="2016-04-22T16:49:26Z" id="213506728">@sylvie777 hello, I would have to make a photo of my screen to show you how I see it but it looks like I've been given a display with preset high contrast, so the screenshots looked the same for me. 

I'm replying on the phone and now I see that I was wrong :) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update settings.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17924</link><project id="" key="" /><description>Add note for removal of index.translog.interval
</description><key id="150306283">17924</key><summary>Update settings.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">russcam</reporter><labels><label>docs</label></labels><created>2016-04-22T08:35:10Z</created><updated>2016-04-25T00:25:09Z</updated><resolved>2016-04-25T00:25:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-22T16:56:50Z" id="213511000">@russcam this has conflicts, can you rebase so it can be merged?
</comment><comment author="russcam" created="2016-04-23T01:28:40Z" id="213644131">@dakrone yep, no worries
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature: Add visitor pattern on Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17923</link><project id="" key="" /><description>**Describe the feature**:
I've realized it could be usefull provide a visitor pattern on aggregations.
When I get a `SearchResponse`, I get the aggregations' response using `getAggregations()`.

Taking in mind that, every `Aggregation (MultiBucketsAggregation)` has `buckets` and each `bucket` could have internal Aggregations, I think it could be useful to provide an `accept(AggregationVisitor)` method on `Aggregation` interface and an `AggregationVisitor` interface providing methods like:
- `visit(Aggregation)`
- `visit(MultiBucketsAggregation)`
- `visit(StringTerms)`
- and so on...
</description><key id="150296472">17923</key><summary>Feature: Add visitor pattern on Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeusdi</reporter><labels><label>:Aggregations</label><label>:Java API</label><label>discuss</label></labels><created>2016-04-22T07:47:42Z</created><updated>2016-05-06T10:08:22Z</updated><resolved>2016-05-06T10:08:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-06T10:08:22Z" id="217403110">Given the fact that we are moving away from a Java based API towards HTTP based clients we decided to not further explore this path. We can still reopen if it becomes more relevant in the future
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to create a child type when its parent type is already exist in Elasticsearch 2.X?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17922</link><project id="" key="" /><description>I know parent/child has been rewritten in ES2.X, and the mapping for the parent type can be added at the same time as the mapping for the child type, but cannot be added before the child type.

But I think it's not logical. In my situation, I cannot create all children types with their parent type mapping at the same time. I really need to create a child type when its parent type is already exist.

Please give some suggestions.
</description><key id="150251346">17922</key><summary>How to create a child type when its parent type is already exist in Elasticsearch 2.X?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yuemaoxing</reporter><labels /><created>2016-04-22T02:59:51Z</created><updated>2016-04-22T03:02:41Z</updated><resolved>2016-04-22T03:02:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-22T03:02:41Z" id="213223683">Elastic reserves GitHub for bugs and feature requests. Please use the [Elastic Discourse forums](https://discuss.elastic.co) for general questions. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kill thread local leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17921</link><project id="" key="" /><description>This commit modifies InjectorImpl to prevent a thread local leak in
certain web containers. This leak can arise when starting a node client
inside such a web container. The underlying issue is that the
ThreadLocal instance was created via an anonymous class. Such an
anonymous class has an implicit reference back to the InjectorImpl in
which it was created. The solution here is to not use an anonymous class
but instead just create the reference locally and set it on the thread
local.

Closes #283, relates google/guice#630
</description><key id="150206964">17921</key><summary>Kill thread local leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v1.7.6</label><label>v2.3.3</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-21T22:20:03Z</created><updated>2016-04-25T18:23:37Z</updated><resolved>2016-04-22T12:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-21T22:21:33Z" id="213139151">I do not think there is an easy way to test this within our own code base. I did install Tomcat locally and created and deployed a war that uses an Elasticsearch client to hit a local cluster. Before the patch:

```
Apr 21, 2016 6:09:00 PM org.apache.catalina.core.StandardService stop
INFO: Stopping service Catalina
Apr 21, 2016 6:09:00 PM org.apache.catalina.loader.WebappClassLoader checkThreadLocalMapForLeaks
SEVERE: The web application [/injector] created a ThreadLocal with key of type [org.elasticsearch.common.inject.InjectorImpl$1] (value [org.elasticsearch.common.inject.InjectorImpl$1@15bfd87]) and a value of type [java.lang.Object[]] (value [[Ljava.lang.Object;@543e710e]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.
Apr 21, 2016 6:09:00 PM org.apache.catalina.loader.WebappClassLoader checkThreadLocalMapForLeaks
SEVERE: The web application [/injector] created a ThreadLocal with key of type [org.elasticsearch.common.inject.InjectorImpl$1] (value [org.elasticsearch.common.inject.InjectorImpl$1@15bfd87]) and a value of type [java.lang.Object[]] (value [[Ljava.lang.Object;@57f23557]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.
Apr 21, 2016 6:09:00 PM org.apache.coyote.http11.Http11Protocol destroy
INFO: Stopping Coyote HTTP/1.1 on http-8080
```

After the patch:

```
Apr 21, 2016 6:11:20 PM org.apache.catalina.core.StandardService stop
INFO: Stopping service Catalina
Apr 21, 2016 6:11:20 PM org.apache.coyote.http11.Http11Protocol destroy
INFO: Stopping Coyote HTTP/1.1 on http-8080
```

The aforementioned leak reliably reproduces before the patch and has not reproduced for me since the patch.
</comment><comment author="rmuir" created="2016-04-22T02:02:45Z" id="213209307">+1 to remove leaks like this.

Yes, its not easy to make tests fail, but I do think its possible. Its mainly not easy because of all the leaks that would have to be fixed in existing tests to actually add the rule :( Additionally i do not know how we can detect these leaks a jigsaw world, or in a non-hacky way: there is a discussion going on about it recently on the openjdk jigsaw list. 

But see https://issues.apache.org/jira/browse/LUCENE-6335 for a hacky hacky evil test rule that fails on threadlocal leaks. Its a port of tomcat's leak detector to the test framework with even more hacky stuff to try to make these leaks easier to debug. I ran out of steam on the issue and got distracted with other things, but the code seems to kinda work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use epoch seconds in `to` and `from` fields of date range aggregation `ranges` objects.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17920</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

When aggregating documents with a date field mapping using `format: epoch_seconds` I would like to use epoch seconds in the `to` and `from` fields of a date-range aggregation. At present dates passed in `to` and `from` are not converted to milliseconds before the date-range aggregation.

This seems inconsistent with how dates are converted from the mapping format to the internal usage format when PUTing documents when the date-field format is specified.
</description><key id="150200146">17920</key><summary>Use epoch seconds in `to` and `from` fields of date range aggregation `ranges` objects.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">hrfuller</reporter><labels><label>:Aggregations</label><label>:Dates</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-04-21T21:50:52Z</created><updated>2017-07-18T07:45:28Z</updated><resolved>2017-07-18T07:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T18:07:50Z" id="214465570">@hrfuller the `format` parameter in the agg is for the display format only.  For input formats, you can add `epoch_seconds` to your field mapping, eg:

```
"mappings": {
  "t": {
    "properties": {
      "date": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      }
    }
  }
}
```
</comment><comment author="hrfuller" created="2016-04-25T23:15:33Z" id="214560035">@clintongormley My mapping has a date property formatted as `epoch_seconds`. When using a date-range aggregation and passing a date in `epoch_seconds` as the value of the `to` and `from` fields.
I.e

``` python
'date_range': {
    'field': 'dims.time',
    'ranges': [
        {
            'from': 1454463970,
            'to': 1455150295,
        }
    ]
},
```

 the date-range aggregator interprets the timestamps as `epoch_millis`. Of course I can easily convert the `epoch_seconds` to `epoch_millis`.

This was a feature request because it seems like it would improve the consistency of the elasticsearch API if date ranges were query-able/aggregate-able using the date input format specified in the date mapping.

```
"version" : {
    "number" : "2.3.1",
    "build_hash" : "bd980929010aef404e7cb0843e61d0665269fc39",
    "build_timestamp" : "2016-04-04T12:25:05Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
```
</comment><comment author="clintongormley" created="2016-04-26T14:10:43Z" id="214757931">Sorry, I completely misread the original description.  I thought you were setting the format in the agg, not in the mapping.  This is a bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a `minhash` token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17919</link><project id="" key="" /><description>Adds a `minhash` token filter, which computes the "minimum hashes" for a body of text.  The minhash can be used to estimate the Jaccard similarity coefficient between two documents, since documents with similar tokens will share similar minhashes.  In practice, this provides coarse/fast clustering and similarity comparison.

A user can select the number of hashes to use in the filter.  More hashes gives a finer-grained resolution (since it provides more random projections to divide the sets), at the cost of more computation and storage.

The filter consumes all the original tokens and emits `n` tokens which follow the format `&lt;minhash index&gt;_&lt;minhash&gt;`.  E.g. 

```
["0_915bcf01", "1_db6bf2d", "2_88e874da", "3_24315c2c", "4_8b60acfe"]
```

@jimferenczi pointed me towards this open patch in Lucene, which provides a similar MinHash (and other LSH functionality):  https://issues.apache.org/jira/browse/LUCENE-6968

Since my version was close to done anyway, I figured I'd put it up and see if we should defer to the Lucene implementation, continue with this, move it to Lucene, etc.
- Docs are missing
- Murmur3 is used to provide the initial hash, then rotated/xor'd to provide the rest of the "independent" hashes.  I didn't use the string's hashcode since that biases certain bits in practice, since character encodings and language are not randomly dispersed
- I embedded a list of integers to xor against, rather than generating them at runtime.  Seemed relatively negligible (400 bytes)
</description><key id="150193844">17919</key><summary>Add a `minhash` token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Analysis</label><label>feature</label></labels><created>2016-04-21T21:19:02Z</created><updated>2016-06-20T14:33:09Z</updated><resolved>2016-06-20T14:33:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-04-28T11:09:54Z" id="215390869">this patch looks a lot simpler than the one on the lucene JIRA. Also it does not rely on guava. Maybe give them some competition.
</comment><comment author="s1monw" created="2016-05-27T09:48:57Z" id="222106261">+1 to move it to the Lucene JIRA and take it from there?
</comment><comment author="polyfractal" created="2016-06-20T14:33:09Z" id="227159868">[Lucene-6968](https://issues.apache.org/jira/browse/LUCENE-6968) was merged while I was out, closing :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stringify Objects for Tokenizing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17918</link><project id="" key="" /><description>**Describe the feature**:
There are cases where complex objects need to be passed to tokenizers.  I am writing a new feature that will allow a mapping to specify that a property of type string may coerce an object within that property to a string.

```
PUT /test
{
  "mappings": {
     "test": {
        "properties": {
           "a": {
              "type": "string",
              "coerce": true
           }
        }
     }
  }
}
```

```
PUT /test/test/1
{
    "a": {
        "b":"2",
        "c":"3"
    }
}
```

In this example, the tokenizer will be passed "{\"b\":\"2\",\"c\":\"3\"}".  It will be up to the tokenizer to decide how to treat that.  The default tokenizer returns the tokens "b", "2", "c" and "3".

This situation occurs frequently in library data.  Complex objects have been created and the values of some of the properties effect how the other properties are treated.  A simple example occurs with book titles.  There is a piece of data that specifies how many leading characters should be stripped from the title to use it as a sort key.

So far, the changes have been restricted to StringFieldMapper and TokenCountFieldMapper.  I am making the changes in version 2.1.0 and expect to merge them into all subsequent versions.

I expect to be able to use the copy_to parameter, but don't know exactly how that will work yet.
</description><key id="150188811">17918</key><summary>Stringify Objects for Tokenizing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ralphlevan</reporter><labels><label>:Analysis</label><label>:Ingest</label><label>discuss</label></labels><created>2016-04-21T20:56:14Z</created><updated>2016-08-11T13:53:46Z</updated><resolved>2016-05-06T10:09:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-25T18:00:18Z" id="214463473">Not sure how well this would play with source filtering, etc.  Wondering if it wouldn't be a better idea to implement this as a JSON stringify processor in the ingest node?
</comment><comment author="ralphlevan" created="2016-04-25T18:37:32Z" id="214473784">First I've heard of these.  Got a pointer to some documentation?
</comment><comment author="clintongormley" created="2016-04-26T11:13:20Z" id="214705589">https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html  (coming in 5.0)
</comment><comment author="ralphlevan" created="2016-04-27T12:59:41Z" id="215074030">I've got serious reservations about the whole ingest node plan.  Anything not defined in the database mappings or settings has the potential to be applied inconsistently.

Otherwise, there's no conflict between these two features.
</comment><comment author="clintongormley" created="2016-04-28T17:56:02Z" id="215510303">@ralphlevan this feels like a transformation that should happen before the analysis phase, which means that we're unlikely to accept a PR that targets analysis 
</comment><comment author="ralphlevan" created="2016-04-28T18:29:21Z" id="215520029">I agree.  This change happens during document parsing, not analysis.

The code is complete and working as I hoped.  The document source is unchanged and the complex object that was provided is returned unchanged.  But, downstream tokenizers get passed the complete object as a string.  Copy_to passes the stringified object to the new fields.

The only code touched was ~~DocumentParser~~, StringFieldMapper, TokenCountFieldMapper and JsonXContentParser. (It turns out the DocumentParser changes were just traces that I added to figure out what was going on. I've reverted those changes.)
</comment><comment author="ralphlevan" created="2016-04-28T18:42:27Z" id="215523663">Here's the demonstration test:

```
PUT /test
{
  "mappings": {
     "test": {
        "properties": {
           "b": {
              "type": "string",
              "coerce": true,
              "copy_to": "copyOfB"

           },
           "copyOfB": {
               "type": "string",
               "store": true
           }
        }
     }
  }
}
```

```
PUT /test/test/1
{
    "a": "z",
    "b": {
        "c":"2",
        "d":"3"
    },
    "e": "y"
}
{
   "_index": "test",
   "_type": "test",
   "_id": "1",
   "_version": 1,
   "_shards": {
      "total": 2,
      "successful": 1,
      "failed": 0
   },
   "created": true
}
```

```
GET /test/test/1
{
   "_index": "test",
   "_type": "test",
   "_id": "1",
   "_version": 1,
   "found": true,
   "_source": {
      "a": "z",
      "b": {
         "c": "2",
         "d": "3"
      },
      "e": "y"
   }
}
```

```
GET /test/test/1?fields=copyOfB
{
   "_index": "test",
   "_type": "test",
   "_id": "1",
   "_version": 1,
   "found": true,
   "fields": {
      "copyOfB": [
         "{\"c\":\"2\",\"d\":\"3\"}"
      ]
   }
}
```
</comment><comment author="clintongormley" created="2016-04-29T12:49:34Z" id="215702174">Hmmm...  I'm afraid I don't like it.  I don't like that `b` is mapped as a string but the _source contains an object.  I'd much prefer this to be an explicit change happening in an ingest processor, but I'm happy leave this open for further discussion.
</comment><comment author="ralphlevan" created="2016-04-29T13:58:19Z" id="215723584">I understand.

There is clearly a philosophical tension within Elasticsearch about whether it should be used as a primary data repository or whether it should be used only as a discovery engine for data stored elsewhere.  If primary data support was not intended, then why have support for arbitrarily complex data?  A comma separated list of data items would be much simpler to handle.  But, if complex objects can be injested, why can't that complexity be used in the indexing?

I lean heavily toward using the database as a primary repository.  The customer has complex data and does not want to have to mangle/unmangle that data every time they touch it, just to simplify the job of the indexer.  The code I am submitting supports that.  Customers can provide complex data and get that complex data back unmodified, but the indexer gets access to more than simple atomic data.

I have a philosophical problem with the processing of data not being centralized.  The injest nodes plan assumes voluntary compliance by the client software submitting data.  Any plan to mangle data externally to the database has the potential to support multiple paths to the database and the potential for inconsistent processing of the data.  I feel strongly that such processing needs to be centralized and the simplest center for that processing is the database.  That's what I have provided by having all the processing specified in the database mapping.
</comment><comment author="ralphlevan" created="2016-04-29T18:50:31Z" id="215846631">How is coercing an object to a string any different than coercing a string to a number?  The mapping says its an int, but the _source says it's a string?  The explicit coerce parameter is what makes both of these legal.
</comment><comment author="jpountz" created="2016-05-06T10:09:39Z" id="217403337">We have been discussing it on FixitFriday and decided to not implement this feature. It should be done on client-side.
</comment><comment author="ralphlevan" created="2016-05-06T13:10:35Z" id="217435512">@jpountz , can you point me at the record of that discussion?  I'm disappointed I wasn't offered an opportunity to participate.
</comment><comment author="jpountz" created="2016-08-11T13:39:48Z" id="239163388">@ralphlevan sorry I missed your message. It was an internal discussion whose main arguments can be read at https://github.com/elastic/elasticsearch/issues/19691#issuecomment-236392024 if you are interested.
</comment><comment author="ralphlevan" created="2016-08-11T13:53:46Z" id="239167242">I think the new ingest node feature will allow me to construct the fields I need for indexing.

Thanks!

Ralph

From: Adrien Grand [mailto:notifications@github.com]
Sent: Thursday, August 11, 2016 9:41 AM
To: elastic/elasticsearch elasticsearch@noreply.github.com
Cc: LeVan,Ralph levan@oclc.org; Mention mention@noreply.github.com
Subject: Re: [elastic/elasticsearch] Stringify Objects for Tokenizing (#17918)

@ralphlevanhttps://github.com/ralphlevan sorry I missed your message. It was an internal discussion whose main arguments can be read at #19691 (comment)https://github.com/elastic/elasticsearch/issues/19691#issuecomment-236392024 if you are interested.

&#8212;
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHubhttps://github.com/elastic/elasticsearch/issues/17918#issuecomment-239163388, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ABB9W3Uu_-R7MG9mFg-VbqbsDuvYsZ_2ks5qeyZpgaJpZM4INEdi.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail query if it contains very large rescores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17917</link><project id="" key="" /><description>Closes #17522
</description><key id="150171922">17917</key><summary>Fail query if it contains very large rescores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-21T19:41:16Z</created><updated>2016-05-02T12:01:27Z</updated><resolved>2016-04-22T15:25:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-21T19:41:51Z" id="213084304">I **_HATE**_ the changes that I made to Setting but it was wrong....
</comment><comment author="jpountz" created="2016-04-21T21:18:26Z" id="213117997">I am not very familiar with the Settings class, what did you fix?
</comment><comment author="nik9000" created="2016-04-22T11:54:42Z" id="213393251">&gt; what did you fix?

`get(Settings, Settings)` wasn't working properly with fallback parameters. It'd never use the first `Settings`. This comes up because the new setting falls back to and old setting and the way that index settings are resolved apparently has the index's actual settings in the first slot in `get`. So the fallback wasn't working.

I'm tempted to make the new parameter not default to the old parameter but that doesn't change that the `Setting` class is still broken. The trouble is that it has so many constructors and constructor like methods that you have to fix any problems in a half dozen places.
</comment><comment author="jpountz" created="2016-04-22T15:00:46Z" id="213464456">Agreed this is horrible. I opened #17931.

LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved REST error handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17916</link><project id="" key="" /><description>First pass at closing #15335. The new behavior for the `RestController#executeHandler` method is as follows:
- For a request to a **valid endpoint** with an **unsupported HTTP method** -&gt; Return a _405 HTTP error_ including allowed methods for the endpoint in the **Allow** HTTP header. Refer to [HTTP/1.1 - 10.4.6 - 405 Method Not Allowed](https://tools.ietf.org/html/rfc2616#section-10.4.6).
- For an OPTIONS HTTP method request to a **valid endpoint** -&gt; Return a _200 HTTP response_ including allowed methods for the endpoint in the **Allow** HTTP header. Refer to [HTTP/1.1 - 9.2 - Options](https://tools.ietf.org/html/rfc2616#section-9.2).

The tests extend `ESIntegTestCase` rather than the `ESSingleNodeTestCase`, as `http.enabled` is set to false in the latter class.

Let me know if you think the tests should be broader - they could be improved by pulling in a random selection of REST endpoints to test, rather than the existing hardcoded endpoints.
</description><key id="150154512">17916</key><summary>Improved REST error handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jbertouch</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label></labels><created>2016-04-21T18:28:41Z</created><updated>2017-05-08T11:03:52Z</updated><resolved>2017-05-02T18:27:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-21T19:18:39Z" id="213077280">Thanks for changing the title! I use those to triage what I review and the old title didn't help!
</comment><comment author="jbertouch" created="2016-04-21T19:27:33Z" id="213079664">No worries, I realized after I hit submit that the old title was pretty opaque, especially if you were scanning through the PR list.
</comment><comment author="nik9000" created="2016-04-21T19:38:03Z" id="213083360">I like it! @jasontedor filed the original bug and will probably want to look too!
</comment><comment author="jbertouch" created="2016-04-21T19:43:41Z" id="213084761">Thanks for the review @nik9000. Just realized my updates are tab indented - will change to space indents to match the ES code style. Sorry that snuck in, was working on something else with tab indents and forgot to change the style.
</comment><comment author="nik9000" created="2016-04-21T19:45:12Z" id="213085108">&gt; Sorry that snuck in,

It is cool! I suspect the build would warn you about that with `gradle precommit`.
</comment><comment author="jasontedor" created="2016-04-21T20:23:24Z" id="213099964">@jbertouch I gave it a first pass and it looks great.

Regarding

&gt; The tests extend `ESIntegTestCase` rather than the `ESSingleNodeTestCase`, as `http.enabled` is set to false in the latter class.

you can override the `ESSingleNodeTestCase#nodeSettings` method to set `http.enabled` to `true`. Can you check if that would work?
</comment><comment author="jbertouch" created="2016-04-21T23:22:21Z" id="213156981">@jasontedor Thank you for the review.

I can't see a `ESSingleNodeTestCase#nodeSettings` method. Did you mean the `ESIntegTestCase#nodeSettings` method? If so, that's the one I overrode in the the `RestHTTPResponseHeadersIT` class.
</comment><comment author="jasontedor" created="2016-04-21T23:27:41Z" id="213157857">@jbertouch Nope, I'm referring to a [protected method on `ESSingleNodeTestCase`](https://github.com/elastic/elasticsearch/blob/3046055bd0f2bc3d334667b9d8cad2f6b7396cba/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java#L169-L171) that your test suite can override to return an instance of `Settings` that has set `http.enabled` to `true`.
</comment><comment author="jbertouch" created="2016-04-21T23:38:48Z" id="213160858">Ah, sorry I see it now. I hadn't merged in the latest commits from master in a while.
</comment><comment author="jbertouch" created="2016-04-22T15:22:23Z" id="213470972">@jasontedor Updated the test class to extend `ESSingleNodeTestCase` rather than `ESIntegTestCase`.
</comment><comment author="jasontedor" created="2016-04-25T15:17:28Z" id="214388316">I like it and left some feedback. I like that we have the integration tests included here, but I wonder if it's possible to set up a unit test as well? The basic idea would be to mock a situation accepting a random collection of verbs and ensure that we do the right thing here on an unsupported verb.
</comment><comment author="jbertouch" created="2016-04-25T20:42:09Z" id="214514725">Thanks for the feedback @jasontedor. I will turn the test coverage up to 11, and add a unit test to complement the integration test.
</comment><comment author="jbertouch" created="2016-04-29T17:47:47Z" id="215827955">Hi all, this is ready for another review. I made the suggested tweaks to the integration test, and added randomizing unit test coverage for the new `RESTController` functionality.
</comment><comment author="jasontedor" created="2016-05-06T01:14:32Z" id="217323385">&gt; Hi all, this is ready for another review. I made the suggested tweaks to the integration test, and added randomizing unit test coverage for the new RESTController functionality.

Thanks. I'll give this a thorough review in the next few days, hopefully tomorrow.
</comment><comment author="jasontedor" created="2016-05-11T00:50:01Z" id="218333833">So this doesn't yet close #15335. For example:

``` bash
$ curl -XGET localhost:9200/                                                                    
{
  "name" : "Mandrill",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "5.0.0",
    "build_hash" : "b08ee7f",
    "build_date" : "2016-05-11T00:26:27.003Z",
    "build_snapshot" : true,
    "lucene_version" : "6.0.0"
  },
  "tagline" : "You Know, for Search"
}
$ curl -I -XGET localhost:9200/_forcemerge                                                                                                                                                                                                                                                                 
HTTP/1.1 400 Bad Request
Content-Type: application/json; charset=UTF-8
Content-Length: 203
$ curl -XGET localhost:9200/_forcemerge?pretty=1
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_argument_exception",
      "reason" : "No feature for name [_forcemerge]"
    } ],
    "type" : "illegal_argument_exception",
    "reason" : "No feature for name [_forcemerge]"
  },
  "status" : 400
}
```

Can anything be done here?
</comment><comment author="jbertouch" created="2016-05-12T02:44:11Z" id="218645576">Hi @jasontedor, thank you for the review. Nice catch on the `_forcemerge` edge case, wish I had picked that up.

Interesting that [`GetIndexRequest`](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequest.java#L80) appears to be handling the `_forcemerge` `GET` request. From a quick look at the code the error originates in [`RestGetIndicesAction#handleRequest`](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java#L73). Looks like this error has manifested since [this commit](https://github.com/elastic/elasticsearch/commit/f709b7283f283aa2d3918cc1afda1adb81f26f47#diff-854597e8d375d3c783fa750672bec92b), which (correctly) removed the redundant GET method for `_forcemerge`.

The code implies that any `GET` request starting with an underscore will trigger the error you observed (eg. `curl -XGET http://localhost:9200/_alpha` or `curl -XGET http://localhost:9200/_alpha/beta`).

I will look into it. Would you prefer I open a separate PR with a fix for this specific issue, or include it with this PR?
</comment><comment author="jasontedor" created="2016-05-12T03:10:24Z" id="218648575">&gt; Interesting that [`GetIndexRequest`](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequest.java#L80) appears to be handling the `_forcemerge` `GET` request.

Yup, that's why we are seeing the `No feature for name [_forcemerge]` message because it's acting like a request on an index that does not exist.

&gt; The code implies that any `GET` request starting with an underscore will trigger the error you observed (eg. `curl -XGET http://localhost:9200/_alpha` or `curl -XGET http://localhost:9200/_alpha/beta`).

That's exactly right.

&gt; I will look into it. Would you prefer I open a separate PR with a fix for this specific issue, or include it with this PR?

Let's try to get it in this PR? &#128516; 
</comment><comment author="jbertouch" created="2016-05-12T03:15:14Z" id="218649076">Sure, will do. I guess it does fall under the REST error umbrella &#9748; 
</comment><comment author="jbertouch" created="2016-05-25T18:13:33Z" id="221660184">Hi @jasontedor, sorry about the delay in responding. A couple of quick thoughts on the greedy index matching problem, aka the `_forcemerge` issue you identified.
- One possible solution is to modify the `PathTrie` class so that nodes are keyed to compiled regex Patterns, rather than Strings. This would allow more sophisticated path matching. The downside would be the performance hit from pointwise matching against all the child regex Patterns. Would need to benchmark this to be sure, the `RestController#executeHandler` is obviously invoked very frequently.
- An alternative would be to recognize when a handler has been incorrectly matched (as with your `_forcemerge` example), and not execute it. This seems like a less robust solution to me. There could also be a performance impact from 'second-guessing' the matched handler before it's executed.

Interested in your thoughts on this.
</comment><comment author="jasontedor" created="2016-06-22T20:34:12Z" id="227868108">Sorry for the slow reply here, I just haven't come up with any wonderful ideas. But I wonder if it's as simple as if a handler is registered with an explicit name (e.g., `_forcemerge`) then no request matching `_forcemerge` can ever match any other handler except ones explicitly registered (i.e., let's prevent such requests from matching handlers with wildcards). I think this is similar to your second proposal. Do you have any interest in seeing if that can be made to work (and doesn't break anything today)?
</comment><comment author="jbertouch" created="2016-06-28T02:51:11Z" id="228934430">Absolutely, I will look into it...
</comment><comment author="jasontedor" created="2016-06-29T14:34:30Z" id="229375189">&gt; Absolutely, I will look into it...

Thanks @jbertouch. I was thinking about this again this morning and the more that I thought about it, the more that I think the approach I outlined above is a viable and good approach. I'll be curious to see what you come up with (the devil is always in the details) but this is great work so far that I'd really like to see us get in.
</comment><comment author="jbertouch" created="2016-07-29T13:54:22Z" id="236186342">Hi @jasontedor, I will find some time to work on this today or tomorrow, sorry about the delay.
</comment><comment author="elasticmachine" created="2016-07-29T13:54:24Z" id="236186348">Can one of the admins verify this patch?
</comment><comment author="jasontedor" created="2016-08-01T10:20:45Z" id="236543846">&gt; I will find some time to work on this today or tomorrow, sorry about the delay.

I look forward to it, and no worries.
</comment><comment author="jbertouch" created="2016-08-03T16:06:42Z" id="237283678">Hi @jasontedor, this is ready for a review. Here's an outline of my approach to fix the explicit/wildcard path matching edge case issue:

**PathTrie**
- I added a parameter to `PathTrie#retrieve` to either include or ignore wildcard nodes when traversing the trie. This seemed like the path of least resistance to prioritizing explicit path matches (no pun intended).
- Wildcards in `PathTrie` leaf nodes are matched in both the include and ignore modes.
- Not matching wildcard leaf nodes when searching for explicit path matches triggered multiple REST integration test errors. For example, a simple GET request to `/_settings` returned a 405 error, as the `RestGetSettingsAction` handler registered to GET `/_settings/{name}` wasn't being assigned to the request. The `RestController` incorrectly assumed that the request was intended for `RestUpdateSettingsAction`, registered to PUT `/_settings`. Changing the handling of wildcard leaf nodes fixed this issue.

**RestController**
- `RestController#executeHandler` now prioritizes explicit handler matches. It falls back to wildcard path matched handlers if none are found. I refactored some of this logic into separate methods to make the flow a bit clearer.
- All the enhanced OPTIONS http method handling is still in there, plus unit/integration tests.

One other thing. 2 integration tests are failing with the same `illegal_state_exception` 500 error  - `:modules:transport-netty4:integTest` and `:qa:backwards-5.0:integTest`. I'm seeing the same exception as the one flagged in #19419, even after I merged the fix in PR #19432. I would be happy to check it out, but the error appears to be unrelated to these changes.
</comment><comment author="jbertouch" created="2016-08-03T21:35:01Z" id="237381800">I believe this PR will also fix #17853, which appears to be caused by incorrect handler matching. Will confirm.
</comment><comment author="jasontedor" created="2016-08-04T02:52:38Z" id="237438531">ok to test
</comment><comment author="jasontedor" created="2016-08-28T20:19:26Z" id="242996804">@jbertouch Sorry for the long delay in reviewing this. This is great work, I think we are very close now. I left a few minor comments, just cosmetic fixes. However, you mentioned #17853 and I do not think that this one is addressed by this PR, it looks like requests to `POST /{index}/_settings` are still being routed to an index action, matching the `POST` handler for `/{index}/{type}` but ideally these would 405 since there is a `PUT` handler for `{index}/_settings`. Can anything be done here?

I would like to get this integrated this week. I can commit to setting aside time to iterate on reviewing and integrating this during this week.
</comment><comment author="jbertouch" created="2016-08-28T21:37:21Z" id="243001080">Hi @jasontedor, thank you for the review, I will get a revision back to you early this week. Will take a closer look at #17853 too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove custom thread pools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17915</link><project id="" key="" /><description>Today, we allow the creation of arbitrary custom thread pools. I'm not convinced this is a useful feature and do think that it is a dangerous one. I propose that we remove the capability for custom thread pools.
</description><key id="150128701">17915</key><summary>Remove custom thread pools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>breaking</label></labels><created>2016-04-21T16:43:52Z</created><updated>2016-06-01T13:53:06Z</updated><resolved>2016-05-27T14:33:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-22T09:12:18Z" id="213344547">This would mean that plugins could not have dedicated thread pools anymore. They could use the `generic` threadpool instead, but this has the drawback that it is unbounded. Another option that was mentioned was to create a bounded thread pool that all plugins could share when they need a bounded thread pool. There were diverging opinions and we would like to hear @s1monw 's opinion before moving forward.
</comment><comment author="jasontedor" created="2016-04-22T09:16:53Z" id="213347684">&gt; They could use the generic threadpool instead, but this has the drawback that it is unbounded.

The generic thread pool will be bounded after #17017.
</comment><comment author="s1monw" created="2016-05-20T09:46:20Z" id="220562962">I think we should remove that option but need to be careful with the consequences. If plugins have long running tasks they might need a different infrastructure? Something like an internal long task pool we can only use from internal?
</comment><comment author="s1monw" created="2016-05-20T09:47:20Z" id="220563188">@uboness what do you think ^^ I think it can be bound and rejects quickly such that we don't have too many long running tasks going on at the same time?
</comment><comment author="s1monw" created="2016-05-27T09:50:14Z" id="222106531">removed the discuss label @uboness can you please comment
</comment><comment author="jasontedor" created="2016-05-27T14:32:56Z" id="222162057">Closed in favor of #18613
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove &lt;T&gt; from Writeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17914</link><project id="" key="" /><description>It isn't needed any more! Hurray!

Closes #17085
</description><key id="150109129">17914</key><summary>Remove &lt;T&gt; from Writeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-21T15:29:21Z</created><updated>2016-04-21T15:59:25Z</updated><resolved>2016-04-21T15:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-21T15:29:32Z" id="212974632">@javanna and @cbuescher this is it! This is the end!
</comment><comment author="cbuescher" created="2016-04-21T15:48:53Z" id="212981530">&gt; This is the end!

Hurray! Thanks a lot for doing this. Looks great. 
</comment><comment author="nik9000" created="2016-04-21T15:59:25Z" id="212986552">Thanks for all the help through this @cbuescher @javanna and @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] update versions and add bwc test for 2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17913</link><project id="" key="" /><description /><key id="150106895">17913</key><summary>[TEST] update versions and add bwc test for 2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2016-04-21T15:23:04Z</created><updated>2016-04-21T15:36:33Z</updated><resolved>2016-04-21T15:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-21T15:30:15Z" id="212974859">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explicitly set packaging permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17912</link><project id="" key="" /><description>This changes our packaging to be explicit about the permissions of files
and directories in the tar.gz, rpm, and deb packages. This is to protect
against a user having an incorrectly set umask when installing.

Additionally, plugins that are installed now have their permissions set
by the plugin installation so that plugins that may have been packaged
with incorrect permissions are secured.

As a nice side effect of this PR, I can now actually run the vagrant tests
on my development machine, which previously failed due to
umask problems.

Resolves #17634
</description><key id="150096282">17912</key><summary>Explicitly set packaging permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-21T14:46:19Z</created><updated>2016-04-25T14:23:35Z</updated><resolved>2016-04-21T19:27:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-21T15:04:25Z" id="212962257">Other than the POSIX permissions not being guarded, I left one comment about the creation of the permissions objects. Everything else looks great.
</comment><comment author="jasontedor" created="2016-04-21T18:29:05Z" id="213054421">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update bucket.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17911</link><project id="" key="" /><description /><key id="150087421">17911</key><summary>Update bucket.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericamick</reporter><labels><label>docs</label><label>non-issue</label></labels><created>2016-04-21T14:17:00Z</created><updated>2016-04-22T16:53:59Z</updated><resolved>2016-04-22T16:53:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove QueryFilterBuilder section from migration docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17910</link><project id="" key="" /><description>This type of query and its builder were deprecated in 2.0 and has been removed.
</description><key id="150086485">17910</key><summary>Remove QueryFilterBuilder section from migration docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-21T14:12:55Z</created><updated>2016-05-02T12:01:36Z</updated><resolved>2016-04-21T16:17:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-21T14:16:51Z" id="212938453">LGTM so long as we have the note about it being removed somewhere in the docs.
</comment><comment author="cbuescher" created="2016-04-21T16:09:45Z" id="212989623">@nik9000 that particular builder was a left-over of the filter/builder merge in 2.0, in the query dsl docs it was always mentioned with filters that are gone already. And the java class has been deprecated in 2.0, but we don't really document that other than in the javadocs themselves. So I will merge this if there are no objections.
</comment><comment author="nik9000" created="2016-04-21T16:12:31Z" id="212990506">None.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Writeable#readFrom</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17909</link><project id="" key="" /><description>It is always better to call a static read method or a constructor that
takes StreamInput.

Relates to #17085
</description><key id="150083834">17909</key><summary>Remove Writeable#readFrom</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-21T14:01:44Z</created><updated>2016-04-21T14:28:49Z</updated><resolved>2016-04-21T14:28:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-21T14:23:08Z" id="212940481">LGTM
</comment><comment author="nik9000" created="2016-04-21T14:28:49Z" id="212942890">Thank so much for all the reviews @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Makes Script type writeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17908</link><project id="" key="" /><description>Used to be Streamable. Left-over of the PROTOTYPE related refactoring by
@nik9000

Closes #17753 

@nik9000 would you care to take a look as you originally created the issue?
</description><key id="150039908">17908</key><summary>Makes Script type writeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-21T11:04:27Z</created><updated>2016-05-02T12:05:41Z</updated><resolved>2016-04-22T06:39:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-21T13:49:05Z" id="212928358">Left a few stylistic comments. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_shape indexing slowdown in v2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17907</link><project id="" key="" /><description>**Elasticsearch version**:

```
{
  "name" : "Ani-Mator",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.3.1",
    "build_hash" : "bd980929010aef404e7cb0843e61d0665269fc39",
    "build_timestamp" : "2016-04-04T12:25:05Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
```

**JVM version**:

```
openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)
```

**OS version**:

```
#uname -or
4.1.12-boot2docker GNU/Linux
#more /etc/debian_version
8.4
```

In short I am using the official elasticsearch Docker image.

**Description of the problem including expected versus actual behavior**:

We are bumping our elasticsearch servers from v1.5.3 to v2.3 and indexing of geo_shapes have slowed downed an order of magnitude of _100_ across the board. Fx. we have a geo_shape of Sweden that in v1.5.3 took &lt; 10 s and now takes 8+ minutes.

We are seeing this in both development with Docker and AWS (not using docker).

**Steps to reproduce**:
1. Install official Docker image of v2.3
2. 

```
curl -XPUT localhost:9200/test -d '{
   "mappings": {
     "dist":{
       "properties": {
         "areas": {
           "type": "geo_shape",
           "precision": "100m"
         }
       }
     }
   }
 }'
```

 3.
`curl -XPUT localhost:9200/test/dist/1 -d GIST_CONTENT` where `GIST_CONTENT=` https://gist.github.com/mekanixdk/8b0178e12161375b1e8a76e16842bc0a 

**Provide logs (if relevant)**:
</description><key id="150033415">17907</key><summary>geo_shape indexing slowdown in v2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">mekanixdk</reporter><labels><label>:Geo</label></labels><created>2016-04-21T10:36:00Z</created><updated>2016-06-01T14:23:43Z</updated><resolved>2016-06-01T14:23:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-21T10:41:36Z" id="212850944">@nknize is this due to changes in precision level?
</comment><comment author="mekanixdk" created="2016-04-25T12:02:20Z" id="214283319">Omitting `precision` from mappings (aka staying with default) will index the area in a blink.
</comment><comment author="nknize" created="2016-04-29T15:19:11Z" id="215751864">@mekanixdk so in your case using `100m` precision is reducing performance?
</comment><comment author="mekanixdk" created="2016-04-29T18:40:39Z" id="215843244">Yes, very.

If I read the documentation correctly (https://www.elastic.co/guide/en/elasticsearch/guide/current/mapping-geo-shapes.html) the default is something like `9` which equates to 5x5 meters geohashes. If I stay with this default (ie. not setting the `precision` parameter.

We don't need this kind of precision so we go with `100m`. In the case of Sweden it goes from almost instantly (5x5m) to ~10 minutes (100x100m). It is as if the `m` is lost and elasticsearch interpret it as precision `100`.
</comment><comment author="mekanixdk" created="2016-05-30T09:14:00Z" id="222449373">Stumbled over https://discuss.elastic.co/t/unable-to-index-large-geo-shapes-in-2-0/38196

And reading https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-shape.html closely it appears that I have been bitten by a change I had not noticed.

Adding `distance_error_pct=0.025` we get the performance back.
</comment><comment author="clintongormley" created="2016-06-01T14:23:42Z" id="223008355">Looks like this is documented under `distance_error_pct` here: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/geo-shape.html so I'm going to close this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move templates out of the search api to its own dedicated api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17906</link><project id="" key="" /><description>At the moment mustache templating is baked into the search API. It would be cleaner to break this out of the search API and move its into its own search template API. This api would template any incoming request body and delegate to the search API. I believe this approach is much cleaner.

On the rest layer this can be done in a non breaking manner, since there is already a dedicated endpoint for it (`/_search/template`), which now delegates to the search API.

Also I think we can move this API to the `lang-mustache` module, since templating is hardcoded to use mustache anyway. If do this we can also move the other template related APIs to `lang-mustache` module.
</description><key id="150023779">17906</key><summary>Move templates out of the search api to its own dedicated api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>:Search Templates</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-21T09:49:25Z</created><updated>2016-06-23T07:37:51Z</updated><resolved>2016-06-23T07:37:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-27T09:53:28Z" id="222107173">this is a great idea @martijnvg lets move it out in a module and fix the API part!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node doesn't return HTTP address via Nodes Info or Cat Nodes APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17905</link><project id="" key="" /><description>**Elasticsearch version**:
1.7.5, 2.3.1, 5.0.0alpha1

**JVM version**:

```
$ java -version
java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)
```

**OS version**:
OSX 10.11.1

**Description of the problem including expected versus actual behavior**:
Basically, I can't figure out a way to get the HTTP address of a tribe node via the APIs. Nodes info gives only the transport address:

```
      "name" : "tribe1/t1",
      "transport_address" : "127.0.0.1:9303",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1",
      "version" : "5.0.0-alpha1",
      "build_hash" : "7d4ed5b",
      "roles" : [ ]
```

by comparison, a "normal" node has the HTTP address as well:

```
      "name" : "logs1",
      "transport_address" : "127.0.0.1:9301",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1",
      "version" : "5.0.0-alpha1",
      "build_hash" : "7d4ed5b",
      "http_address" : "127.0.0.1:9201"
...
```

Also, in 5.0.0alpha1, the _cat nodes API doesn't return the HTTP address, even when requested specifically:

```
$ curl localhost:9200/_cat/nodes?h=name,http_address
users1    127.0.0.1:9200
tribe1/t2 -
```

**Steps to reproduce**:
1. Start one node in one cluster, with a config like:

```
node.name: logs1
cluster.name: logs
```
1. Then a second node in one cluster:

```
node.name: users1
cluster.name: users
```
1. Finally a tribe node:

```
node.name: tribe1
tribe.t1.cluster.name: logs
tribe.t2.cluster.name: users
# http.port: 9250  # behavior seems the same whether port is explicitly specified or not
```
1. Call the Cat Nodes and Nodes Info APIs, looking for the HTTP address
</description><key id="150000917">17905</key><summary>Tribe node doesn't return HTTP address via Nodes Info or Cat Nodes APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">radu-gheorghe</reporter><labels><label>:Tribe Node</label><label>bug</label></labels><created>2016-04-21T08:19:43Z</created><updated>2016-05-27T16:17:04Z</updated><resolved>2016-05-27T16:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-27T09:56:14Z" id="222107720">I think this looks more like a bug, we need to investigate this further... removing discuss label. @tlrx will take a look 
</comment><comment author="tlrx" created="2016-05-27T16:17:04Z" id="222189460">Thanks for reporting @radu-gheorghe. Maybe I'm missing something but I just tested this and everything looks fine to me. Note that HTTP address has been added to the _cat/nodes API since 5.0.0-alpha1 (#16770).

Here's my test on 5.0.0-alpha1:

**Start node-0 in cluster-0 (port 9600)**

```
bin/elasticsearch -E es.cluster.name=cluster-0 -E es.node.name=node-0 -E es.http.port=9600 -E es.transport.tcp.port=9601
```

_cat/nodes ouput looks ok:

```
curl -XGET 'http://localhost:9600/_cat/nodes?pretty&amp;v&amp;h=id,name,ip,port,http'            
id   name     ip        port http
_ha0 node-0   127.0.0.1 9601 127.0.0.1:9600
```

**Start node-1 in cluster-1 (port 9700)**

```
bin/elasticsearch -E es.cluster.name=cluster-1 -E es.node.name=node-1 -E es.http.port=9700 -E es.transport.tcp.port=9701
```

_cat/nodes ouput looks ok:

```
curl -XGET 'http://localhost:9700/_cat/nodes?pretty&amp;v&amp;h=id,name,ip,port,http'
id   name     ip        port http
X9cI node-1   127.0.0.1 9701 127.0.0.1:9700
```

**Start tribe node on default port range**

```
bin/elasticsearch -E es.cluster.name=cluster-tribe -E es.node.name=tribe -E es.tribe.c0.cluster.name=cluster-0 -E es.tribe.c0.discovery.zen.ping.unicast.hosts="127.0.0.1:9601" -E es.tribe.c1.cluster.name=cluster-1 -E es.tribe.c1.discovery.zen.ping.unicast.hosts="127.0.0.1:9701"
```

When requesting _cat/nodes on the tribe node, it correctly reports the tribe node, the tribe client nodes (tribe/c0 &amp; tribe/c1) and the node-0 and node-1 too with their respective HTTP addresses:

```
curl -XGET 'http://localhost:9200/_cat/nodes?pretty&amp;v&amp;h=id,name,ip,port,http'
id   name     ip        port http
X9cI node-1   127.0.0.1 9701 127.0.0.1:9700
g1k3 tribe/c0 127.0.0.1 9301 -
_ha0 node-0   127.0.0.1 9601 127.0.0.1:9600
ci5d tribe/c1 127.0.0.1 9302 -
T2-k tribe    127.0.0.1 9300 127.0.0.1:9200
```

Same thing with the Nodes Info API on the tribe node:

```
curl -XGET 'http://localhost:9200/_nodes?pretty&amp;filter_path=nodes.*.http*,nodes.*.version'
{
  "nodes" : {
    "_ha0UOorR66vazp-BUw3kg" : {
      "version" : "5.0.0-alpha1",
      "http_address" : "127.0.0.1:9600",
      "http" : {
        "bound_address" : [ "[::1]:9600", "127.0.0.1:9600" ],
        "publish_address" : "127.0.0.1:9600",
        "max_content_length_in_bytes" : 104857600
      }
    },
    "g1k3oBYERDu5Mu4chi1L4w" : {
      "version" : "5.0.0-alpha1"
    },
    "T2-knUQnT32eaFXH8_gJ-A" : {
      "version" : "5.0.0-alpha1",
      "http_address" : "127.0.0.1:9200",
      "http" : {
        "bound_address" : [ "[::1]:9200", "127.0.0.1:9200" ],
        "publish_address" : "127.0.0.1:9200",
        "max_content_length_in_bytes" : 104857600
      }
    },
    "X9cI2S3PRByVHtuZ88W06w" : {
      "version" : "5.0.0-alpha1",
      "http_address" : "127.0.0.1:9700",
      "http" : {
        "bound_address" : [ "[::1]:9700", "127.0.0.1:9700" ],
        "publish_address" : "127.0.0.1:9700",
        "max_content_length_in_bytes" : 104857600
      }
    },
    "ci5dgIniQfSx99yheSb03g" : {
      "version" : "5.0.0-alpha1"
    }
  }
}
```

In the previous output there are no HTTP addresses for tribe clients nodes like tribe/c0 because they are client nodes connected to the remote cluster and don't expose any HTTP service. This is how tribe nodes work: it instantiates a single node (named **tribe** in my example) that starts multiple tribe nodes, each of them is a client node connected to a remote cluster (tribe/c0 and tribe/c1).

When queried directly, the nodes of the remote clusters (node-0 and node-1 in my example) will only report the tribe client nodes connected to their respective cluster (tribe/c0 and tribe/c1). These nodes don't really know about the node called **tribe** that expose a global view of multiple clusters.

On 2.3.5 or 1.7.5, the Nodes Info API provides the HTTP address (just be sure to hit the tribe node):

```
curl -XGET 'http://localhost:9200/_nodes/_local?pretty&amp;filter_path=nodes.*.http_address,nodes.*.name,nodes.*.version'

{
  "nodes" : {
    "tDk2qwCOQzqbROzMgpy8VQ" : {
      "name" : "tribe",
      "version" : "2.3.1",
      "http_address" : "127.0.0.1:9200"
    }
  }
}
```

I hope it helps. I'm closing this issue but feel free to reopen if needed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17904</link><project id="" key="" /><description>The elasticsearch label for 2.3.2 has no more open issues, however the .zip for v2.3.2 is not available.
Will this be available for downloading?

2.3.2 contains fixes for CORS which are required...
</description><key id="149976718">17904</key><summary>Elasticsearch 2.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IdanAdar</reporter><labels /><created>2016-04-21T06:09:54Z</created><updated>2016-04-21T07:31:25Z</updated><resolved>2016-04-21T07:26:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-21T07:26:57Z" id="212784510">Please regard the note that Elastic reserves GitHub for verified bug reports and feature requests and that for general questions you can use the [Elastic Discourse forums](https://discuss.elastic.co).
</comment><comment author="IdanAdar" created="2016-04-21T07:31:25Z" id="212786151">I did. None is replying there either.
https://discuss.elastic.co/t/elastic-2-3-2/47953
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to match one of many provided patterns to the grok processor.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17903</link><project id="" key="" /><description>**Describe the feature**:

Add ability to specify multiple grok patterns to match in the grok-processor.

example update to the grok-processor syntax:

```
{
  "grok" : {
    "field": "message",
    "patterns": [ "%{CISCOFW106001}", "%{CISCOFW106006_106007_106010}",  "%{CISCOFW106014}"]
  }
}
```

The first pattern in `patterns` that matches will be the one being used for that specific IngestDocument.

ref: https://discuss.elastic.co/t/multiple-grok-rules-in-ingest-node-processor-definition/47902

&gt; ...
&gt; In the ingest node, I was able to define a grok processor, but some log types need multiple grok rules (e.g. firewall logs). In Logstash, you'd simply specify an array of match definitions. If one matches, it the line is parsed, for example:
&gt; 
&gt;   grok {
&gt;      match =&gt; [
&gt;       "cisco_message", "%{CISCOFW106001}",
&gt;       "cisco_message", "%{CISCOFW106006_106007_106010}",
&gt;       "cisco_message", "%{CISCOFW106014}"
&gt;    ...
&gt; For the ingest node, if a grok processor fails, it throws and exception. So the only way I could find to add in multiple rules is to chain multiple such processors via on_failure. If there are multiple rules, the pipeline definition gets pretty hairy:
&gt; 
&gt; ```
&gt;    "grok": {
&gt;       "field": "cisco_message",
&gt;       "pattern": "%{CISCOFW106001}",
&gt;       "on_failure": [
&gt;         {
&gt;           "grok": {
&gt;             "field": "cisco_message",
&gt;             "pattern": "%{CISCOFW106006_106007_106010}",
&gt;             "on_failure": [
&gt;               {
&gt;                 "grok": {
&gt;                   "field": "cisco_message",
&gt;                   "pattern": "%{CISCOFW106014}"
&gt;                   ...
&gt; ```
&gt; 
&gt; Would a pattern array make sense, to emulate Logstash's behavior? Should I open an issue on GitHub or is there a better way to handle this already?
&gt; 
&gt; Also, I'm not sure how Logstash implements this but performance degrades quite nicely with multiple rules. In this particular case, I've seen 1.5x slower throughput with 23 rules compared to one rule. With the ingest node and the on_failure approach described here, I'm getting 9x slower throughput with 23 rules. That said, Ingest node is faster in both cases, so maybe Logstash behaves better proportionally because it's heavier to begin with.
&gt; ...
</description><key id="149901363">17903</key><summary>Add ability to match one of many provided patterns to the grok processor.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-20T22:11:53Z</created><updated>2016-05-25T19:20:39Z</updated><resolved>2016-05-25T19:20:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-04-20T22:19:22Z" id="212635500">please ignore the `breaking` tag, meant to signify a change in API. This change is only breaking between 5.0alpha1 and 5.0, so the tag is a bit extreme.
</comment><comment author="javanna" created="2016-04-20T22:20:46Z" id="212636185">this could be done with an `on_failure` section for grok, that contains another grok with the second pattern and so on. Too verbose?
</comment><comment author="talevy" created="2016-04-20T22:29:06Z" id="212638533">@javanna I will try and see how often I run into this use-case from the Logstash side.

From what I remember, this happens enough that I think it would be too verbose to be handled in many nested `on_failure` blocks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add ability to disable ability to override values of existing fields in set processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17902</link><project id="" key="" /><description>Fixes #17659

adds `override` option for the `set` processor. When set to `false`, the processor will not update 
the value of an existing non-null-valued field in the ingest document

```
{
  "set" : {
    "field" : ...,
    "value" : ...,
    "override" : (true|false)
  }
}
```

This field is optional, and defaults to `true`.
</description><key id="149884666">17902</key><summary>add ability to disable ability to override values of existing fields in set processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-20T20:59:06Z</created><updated>2016-04-28T21:00:29Z</updated><resolved>2016-04-28T21:00:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-26T08:22:13Z" id="214665310">Left 2 small remarks, LGTM otherwise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove readFrom from ingest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17901</link><project id="" key="" /><description>It isn't needed and will be removed from the interface declaring it.

Relates to #17085
</description><key id="149884663">17901</key><summary>Remove readFrom from ingest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-20T20:59:05Z</created><updated>2016-04-21T00:13:38Z</updated><resolved>2016-04-21T00:13:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-20T21:46:37Z" id="212621746">LGTM
</comment><comment author="nik9000" created="2016-04-20T22:26:12Z" id="212637750">Sorry for moving these around. I'm trying to keep reading next to writing.
On Apr 20, 2016 5:46 PM, "Luca Cavanna" notifications@github.com wrote:

&gt; LGTM
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17901#issuecomment-212621746
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update reindex.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17900</link><project id="" key="" /><description>Delete redundant text
</description><key id="149856524">17900</key><summary>Update reindex.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericamick</reporter><labels><label>docs</label><label>non-issue</label></labels><created>2016-04-20T19:00:36Z</created><updated>2016-04-22T16:52:10Z</updated><resolved>2016-04-22T16:52:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update bulk.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17899</link><project id="" key="" /><description /><key id="149854622">17899</key><summary>Update bulk.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericamick</reporter><labels><label>docs</label><label>non-issue</label></labels><created>2016-04-20T18:51:56Z</created><updated>2016-04-22T16:51:08Z</updated><resolved>2016-04-22T16:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update update.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17898</link><project id="" key="" /><description /><key id="149853156">17898</key><summary>Update update.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericamick</reporter><labels><label>docs</label><label>non-issue</label></labels><created>2016-04-20T18:45:27Z</created><updated>2016-04-22T16:50:02Z</updated><resolved>2016-04-22T16:50:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update get.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17897</link><project id="" key="" /><description /><key id="149846135">17897</key><summary>Update get.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericamick</reporter><labels><label>docs</label><label>non-issue</label></labels><created>2016-04-20T18:19:30Z</created><updated>2016-04-22T16:47:05Z</updated><resolved>2016-04-22T16:46:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-22T16:47:05Z" id="213505884">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove readFrom from org.elasticsearch.search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17896</link><project id="" key="" /><description>Replace with a constructor that takes StreamInput or a static method.

In one case (ValuesSourceType) we no longer need to serialize the data
at all!

Relates to #17085
</description><key id="149835840">17896</key><summary>Remove readFrom from org.elasticsearch.search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-20T17:34:32Z</created><updated>2016-04-21T12:24:57Z</updated><resolved>2016-04-21T12:23:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-21T10:39:44Z" id="212849816">@nik9000 one nit, one question, otherwise looks good.
</comment><comment author="nik9000" created="2016-04-21T12:24:57Z" id="212892964">Thanks for reviewing @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support ingest pipelines as part of the update api?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17895</link><project id="" key="" /><description>Would it make sense to support ingest pipelines as part of the update api?

Ingest pipelines can already be used in the reindex api as an alternative to scripts. There is some kind of overlap between ingest and scripts: scripts are much more flexible (and safe with painless) but for people that get familiar with ingest it may be more convenient to reuse defined pipelines. Also some of  the enrichments made through ingest are not easily performed using scripts (think geoip). 

We can debate that ingest enrichments are really meant to be executed pre-indexing. But while we are reindexing a document, which is what the update api does, maybe it would be handy to be able to use ingest pipelines as well? Or maybe this is a bad idea at all and it would introduce yet another way to update documents other than scripts and partial document. I am on the fence on this myself, would be good to discuss this.

This [comment](https://github.com/elastic/elasticsearch/pull/16808#issuecomment-212485004) made me think about this. Not sure ingest would help there though.
</description><key id="149826743">17895</key><summary>Support ingest pipelines as part of the update api?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-20T16:54:49Z</created><updated>2017-05-06T01:46:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-21T12:30:14Z" id="212896635">I'm not sure myself too is this needs to be supported, this sound tempting. However I think it is tricky. Ingest is flexible on where it runs if a node isn't ingest it executes elsewhere. This can become tricky with the update api, since it always executes on the node holding the primary. 
</comment><comment author="martijnvg" created="2016-04-22T09:19:02Z" id="213348646">Discussed during fix it friday and this looks like a useful enhancement, but there are corner cases which would make it very tricky to support this. (index name or routing is changed during ingestion or when a node isn't allowed to run ingest) Therefor I'm closing this issue and we can re-evaluate this at a later time if this is still useful and the technical concerns can fixed easily.
</comment><comment author="djschny" created="2016-08-09T15:46:15Z" id="238596070">I feel it is very important to support ingest pipelines as part of the update API. Now that _timestamp has been removed, it was suggested that the alternative is to accomplish this with an ingest node pipeline. Without having this on the _update API, it makes for an incomplete solution. For example consider the following pipeline:

```
PUT _ingest/pipeline/timestamps
{
  "description": "Adds createdAt and updatedAt style timestamps",
  "processors": [
    {
      "set": {
        "field": "metadata.timestamp_created",
        "value": "{{_ingest.timestamp}}",
        "override": false
      }
    },
    {
      "set": {
        "field": "metadata.timestamp_updated",
        "value": "{{_ingest.timestamp}}",
        "override": true
      }
    }
  ]
}
```

When doing a partial update, we offer no alternative ability to set these timestamps appropriately that I am aware of. A script could probably be used, but resuse and consistency of the pipelines is something that we would want?
</comment><comment author="clintongormley" created="2016-08-11T18:25:25Z" id="239247646">That is an interesting idea... although it'd be a lot slower than just using a script (as the document would have to be converted from JSON -&gt; object -&gt; JSON -&gt; object -&gt; JSON). 

As mentioned above, exposing ingest to the update api has some complexity (eg changing _index, _type, _parent, _routing shouldn't be allowed).   That said, update-by-query already exposes ingest.
</comment><comment author="martijnvg" created="2016-08-15T15:17:03Z" id="239829673">+1 I think that if don't allow modifying `_index`, `_parent` or `_routing` metadata fields then we can support ingest in the update api and for bulk items in the bulk api.
</comment><comment author="ejsmith" created="2016-09-28T17:26:21Z" id="250237101">I feel like having a guaranteed timestamp field is critically important for many use cases such as online reindexing. Telling us to use an ingest pipeline, but then having no way to guarantee that those pipelines are run to ensure that the timestamp is updated is not a good solution IMO.
</comment><comment author="clintongormley" created="2016-10-08T11:36:49Z" id="252419923">https://github.com/elastic/elasticsearch/issues/17895#issuecomment-250237101 is a good point, especially given the fact that we've removed `now()` from painless https://github.com/elastic/elasticsearch/pull/20766
</comment><comment author="ejsmith" created="2016-10-09T01:20:43Z" id="252457897">I really hope the team reconsiders this decision because I think this is really going to hurt the product.
</comment><comment author="martijnvg" created="2016-10-10T06:32:11Z" id="252544606">@ejsmith I think there is consensus about adding support for ingest to the update api. 

@clintongormley Lets change the discuss label for an adoptme label?
</comment><comment author="ejsmith" created="2016-10-10T13:22:03Z" id="252616874">I fail to see how that helps. If it's a setting on the update API that means it's optional and I don't have any way to have a guaranteed timestamp that is reliable.
</comment><comment author="s1monw" created="2016-10-10T15:41:05Z" id="252659752">@ejsmith I think #20835 should resolve this issue?
</comment><comment author="ejsmith" created="2016-10-10T15:45:08Z" id="252660807">Again, all of those features don't give me a very basic thing which is a guaranteed timestamp on my documents that I can rely on for things like reindexing and other operations. They are all dependent on everyone playing nice and making sure to set the timestamp.
</comment><comment author="s1monw" created="2016-10-10T15:49:13Z" id="252661839">I understand what you mean but we never had such a think as _the reliable_ way of getting a timestamp. The reason why we are moving away from it was it's inconsistency and people had wrong expectations. Now we are trying to provide building blocks to get the desired behavior. They involve work, I understand that but it's the most flexible and contained way of managing expectations.
</comment><comment author="ejsmith" created="2016-10-10T15:53:20Z" id="252662853">So your saying that the timestamp feature isn't reliable?
</comment><comment author="s1monw" created="2016-10-10T15:59:36Z" id="252664412">We had tons of bug related to TTL that caused confusion together with _timestamp. Also if you use an update script and you don't explicitly update `_timestamp` it keeps the original timestamp when it was created. While if you send the document again from the outside (ie. get -&gt; modify -&gt; update) you'd get a new value for `_timestamp`.  Same is true for reindex where stuff would not be preserved. now sometimes you want that, sometimes you don't but it's usecase specific and not really reliable. Either way you turn it it's better to have it addable by a user since we can't do the right thing for everybody.
</comment><comment author="niemyjski" created="2016-10-10T16:40:24Z" id="252674524">I'd rather have it be updated all the time than never or not having this feature. It caused way more problems by being removed than just leaving it there.
</comment><comment author="ejsmith" created="2016-10-10T17:12:15Z" id="252682050">I don't care about TTL. That makes perfect sense and can easily be handled by outside code. Reliable timestamp can't be handled reliably from the outside. I understand that there were issues with timestamp in the past, but I guess I don't get why it can't be implemented on the server side. Again, to me it seems like a very critical thing and pretty much every other database system has that functionality. I know you guys aren't specifically a database, but the system very much works like a document database and we need to do reindexing and other bulk operations that very much need to have some sort of reliable way of knowing when documents were updated last.
</comment><comment author="s1monw" created="2016-10-10T19:20:34Z" id="252720124">&gt; I'd rather have it be updated all the time than never or not having this feature. It caused way more problems by being removed than just leaving it there.

this is again how you used it but it's not how it was used by the majority. Majority provided timestamp from the outside and then it's suddenly a simple date field. This is how we should treat it, it was meant for the logging case nothing else. We are discussing X Y problems. You guys want a last_modified timestamp that wasn't what `_timestamp` read the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-timestamp-field.html). I am up for discussing a last modified timestamp that is updated on every change and has clear semantics. That is, if you like it or not a new feature and should not be discussed on this issue. Feel free to open a new issue to add this specific field and we take it from there. @ejsmith @niemyjski WDYT
</comment><comment author="s1monw" created="2016-10-11T13:27:13Z" id="252915931">@ejsmith @niemyjski I opened #20859 for this can you please take a look
</comment><comment author="niemyjski" created="2016-10-24T19:59:26Z" id="255848520">I just ran into a case where I needed to use a pipeline as part of the update api to ensure my data is consistent and up-to-date.
</comment><comment author="niemyjski" created="2016-11-02T00:31:07Z" id="257739740">Is there any reason the update api doesn't currently support pipelines? This is pretty huge and I'm already running into real world scenarios where I need this.
</comment><comment author="cnus11" created="2017-03-02T03:33:00Z" id="283546267">We really need ingest pipelines to work for update API. Its really a good feature to be considered by Elastic Search experts</comment><comment author="icode" created="2017-04-24T10:07:17Z" id="296604032">has not support???????</comment><comment author="aalkilani" created="2017-05-06T01:46:19Z" id="299608113">+1 We're looking to use the Ingest pipeline to support updates from Logstash and get away from using scripts in Logstash where the entire document gets shipped back to LS for every update. With the amount of data we're processing, this is becoming critical for us and may push us away from using ES altogether.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do we need custom allocation commands?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17894</link><project id="" key="" /><description>As part of #17802 we are taking into account the fact that a plugin could potentially plug in custom allocation commands. That has always been untested, hence got broken in master till it got fixed a few days ago as part of Nik's refactorings.

I now wonder if we should test this extension point so that we can claim we support it, or if we can instead simply remove it as it is not needed. Is there any custom allocation command out there? Anybody can think of why this extension point would be needed?
</description><key id="149814869">17894</key><summary>Do we need custom allocation commands?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label></labels><created>2016-04-20T16:10:17Z</created><updated>2016-05-24T16:00:19Z</updated><resolved>2016-05-24T16:00:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-20T16:11:24Z" id="212494336">I did some digging and it looks like allocation commands were made pluggable as part of this commit: https://github.com/elastic/elasticsearch/commit/e530f03b9445e98728b5fbf203c57ab774928ff3 . @kimchy maybe you want to comment on this? ;)
</comment><comment author="jpountz" created="2016-04-22T09:19:05Z" id="213348672">Discussed in Fixit Friday: we do not think that it should be possible to plug in custom allocation commands, but would like to hear @kimchy 's opinion before closing to make sure we did not miss anything.
</comment><comment author="javanna" created="2016-05-24T08:52:38Z" id="221206324">I removed the discuss label as #17802 will remove the ability to plug in custom allocation commands when it gets in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested Type in mapping does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17893</link><project id="" key="" /><description>Hi, 
 When I insert data into ES. I have this error:
 **Mapper for [restaurants] conflicts with existing mapping in other types[object mapping [restaurants] can't be changed from nested to non-nested]**

Here is the mapping I am using:

``` javascript
{
  "couchbase": {
    "properties": {
      "id": {
        "type": "string",
        "index": "not_analyzed"
      },
      "start_time": {
        "type": "date",
        "format": "date_time"
      },
      "time_placed": {
        "type": "date",
        "format": "date_time"
      },
      "restaurants": {
        "type": "nested",
        "properties": {
          "id": {
            "type": "string",
            "index": "not_analyzed"
          },
          "name": {
            "type": "string"
          }
        }
      },
      "state": {
        "type": "string",
        "index": "not_analyzed"
      },
      "order_number": {
        "type": "string",
        "index": "not_analyzed"
      }
    }
  }
}
```

Thanks!
</description><key id="149813280">17893</key><summary>Nested Type in mapping does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qitang</reporter><labels /><created>2016-04-20T16:03:55Z</created><updated>2016-04-20T18:04:56Z</updated><resolved>2016-04-20T16:37:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T16:37:51Z" id="212505602">You're getting a mapping conflict because you're passing a `restaurants` field that isn't an object or an array of objects
</comment><comment author="qitang" created="2016-04-20T17:06:07Z" id="212516711">@clintongormley 
this is the json I am using

``` javascript
{
  "group_id": null,
  "payments": {
    "total": 1605,
    "payments": [
      {
        "id": "6954",
        "type": "CREDIT_CARD",
        "amount": 1605
      }
    ],
    "transactionId": null
  },
  "restaurants": [
    {
      "id": "296787",
      "name": "White Castle"
    }
  ],
  "state": "COMPLETED",
  "order_numbers": "900006749",
  "order_tracking": {
    "enabled": false,
    "type": null
  },
  "system_of_record": "bullrat",
  "group": false,
  "asap": true
}
```
</comment><comment author="rjernst" created="2016-04-20T18:04:56Z" id="212538403">@qitang You have `restaurants` set as `nested` in your existing mappings. Based on the error, it sounds like you are dynamically creating a new type. We don't allow created nested types through dynamic mappings, so the dynamic code views `restaurants` in this new type as `object`, which conflicts with the other document type mapping.

/cc @jpountz
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Streamline option naming for several processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17892</link><project id="" key="" /><description>- For `rename` processor, renamed `to` to `target_field`
- For `date` processor, renamed `match_field` to `field` and `match_formats` to `formats`.
- For `geoip` processor, renamed `source_field` to `field` and renamed `fields` to `properties`
- For `attachment` processor, renamed `source_field` to `field` and renamed `fields` to `properties`

PR for #17835
</description><key id="149812760">17892</key><summary>Streamline option naming for several processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-20T16:01:41Z</created><updated>2016-04-21T11:42:57Z</updated><resolved>2016-04-21T11:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-20T16:35:17Z" id="212504115">left one tiny comment. I am not too sure about `properties` but I can't come up with anything better, maybe @clintongormley wants to comment on that. LGTM otherwise
</comment><comment author="martijnvg" created="2016-04-21T07:28:49Z" id="212784849">@javanna I've updated the pr and renamed `match_formats` to `formats`.
</comment><comment author="javanna" created="2016-04-21T07:39:53Z" id="212788357">LGTM thanks @martijnvg let's see what Clint thinks about the `properties` naming?
</comment><comment author="clintongormley" created="2016-04-21T08:17:29Z" id="212799809">`properties`++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove a few more readFroms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17891</link><project id="" key="" /><description>ThreadContext had a very odd relationship with the Writeable interface.
This makes it conform.

Relates to #17085
</description><key id="149809621">17891</key><summary>Remove a few more readFroms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-20T15:51:01Z</created><updated>2016-04-20T18:06:19Z</updated><resolved>2016-04-20T18:06:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-20T16:28:20Z" id="212501739">Left one comment, otherwise LGTM.
</comment><comment author="nik9000" created="2016-04-20T17:36:20Z" id="212528948">Thanks for reviewing @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to send a piece of command to es automatic by es when es just started?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17890</link><project id="" key="" /><description>Hi, for es cancel river service in es 2.x. River service begin to start after ES started. Now I need the function as river service. How can I do?
</description><key id="149803205">17890</key><summary>How to send a piece of command to es automatic by es when es just started?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">focusme</reporter><labels /><created>2016-04-20T15:26:28Z</created><updated>2016-04-20T16:36:02Z</updated><resolved>2016-04-20T16:36:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T16:36:02Z" id="212504518">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds tests for some missing coverage on dynamically created fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17889</link><project id="" key="" /><description>This adds some new tests to DocumentParserTests to make sure the DocumentParser behaves correctly when dynamically mapping fields. Especially testing that the dynamic setting works when dynamically mapping different field types. The gaps in the tests were mainly found using code coverage tools
</description><key id="149798271">17889</key><summary>Adds tests for some missing coverage on dynamically created fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>review</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-20T15:11:17Z</created><updated>2016-04-21T08:41:17Z</updated><resolved>2016-04-21T08:40:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-20T16:20:21Z" id="212498894">Woohoo!!! I'm wondering if we should check the exception messages in the test (unless maybe the exception type is specific enouch?). Otherwise LGTM.
</comment><comment author="rjernst" created="2016-04-20T16:54:11Z" id="212512750">I agree we should check the exception message. 
</comment><comment author="colings86" created="2016-04-21T08:41:16Z" id="212812589">@jpountz @rjernst I added a check on the exception message before I merged. Thanks both for the review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate facet from 1.x to aggregation 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17888</link><project id="" key="" /><description>I have a custom facet implementation: *InternalFacet, *Builder, *Executor and *Parser classes.
What are equivalent of this classes I should use to migrate my facet to aggregation?
</description><key id="149749246">17888</key><summary>Migrate facet from 1.x to aggregation 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nikoncode</reporter><labels /><created>2016-04-20T12:06:11Z</created><updated>2016-04-20T13:53:03Z</updated><resolved>2016-04-20T13:53:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T13:53:03Z" id="212434085">Hi @nikoncode 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>regression: cannot index dates with a year &gt;= 10000</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17887</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1
**JVM version**: JDK 7, JDK 8
**OS version**: linux

**Description of the problem including expected versus actual behavior**:
when indexing a document with a date field and the date is after the year 10000,
the index operation fails with the following exception:

MapperParsingException[failed to parse [document/datefield]]; nested: IllegalArgumentException[Invalid format: "28910-05-16T19:52:39.000Z" is malformed at "0-05-16T19:52:39.000Z"];

The error does not occur with ES 1.7.2 or earlier

**Steps to reproduce**:
1. use the java api
2. assign a java.util.Date after the year 10,000 to a document's field of type "date"
3. index the document

**Cause**:
I believe the issue is with StrictISODateTimeFormat, which seems to be an edited copy of joda's ISODateTimeFormat.
as the following unit test demonstrates:

```
    @Test
    public void testLargeDate() {
        final DateTimeFormatter joda = StrictISODateTimeFormat.dateOptionalTimeParser();
        assertEquals(9999, joda.parseLocalDate("9999-01-02T03:04:05").year().get());
        assertEquals(10000, joda.parseLocalDate("10000-01-02T03:04:05").year().get()); //BROKEN
    }
```

the second assert fails with the same exception as when indexing a document with a date after the year 10,000.
When using joda's ISODateTimeFormat instead of ES's StrictISODateTimeFormat, the test works correctly.

**Provide logs (if relevant)**:
MapperParsingException[failed to parse [document/datefield]]; nested: IllegalArgumentException[Invalid format: "28910-05-16T19:52:39.000Z" is malformed at "0-05-16T19:52:39.000Z"];

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="149736691">17887</key><summary>regression: cannot index dates with a year &gt;= 10000</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drallax</reporter><labels><label>:Dates</label><label>discuss</label></labels><created>2016-04-20T11:04:20Z</created><updated>2017-03-31T10:10:11Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T13:52:28Z" id="212433901">Yes - the strict date format was mostly added to improve dynamic date detection.  Why not just set the date format on your field to `date_optional_time` instead of `strict_date_optional_time`?
</comment><comment author="drallax" created="2016-04-20T16:49:54Z" id="212510642">actually we define all our date fields without an explicit format, just using the default, which turns out to be `strict_date_optional_time` as seen here:

```
public class DateFieldMapper extends NumberFieldMapper {
    ...
    public static class Defaults extends NumberFieldMapper.Defaults {
        public static final FormatDateTimeFormatter DATE_TIME_FORMATTER = Joda.forPattern("strict_date_optional_time||epoch_millis", Locale.ROOT);
```

where it used to be `date_optional_time` &lt;ES-2.0

Also note that we're using the Java api here, and that we offer a java.util.Date, not some ISO string.
So it is surprising to see an error that complains about the textual representation of that Date as soon as the year exceeds 10,000

The javadoc on top of `StrictISODateTimeFormat` includes  the following quote:

```
 * However there has been done one huge modification in several methods, which forces the date
 * year to be at least n digits, so that a year like "5" is invalid and must be "0005"
```

which leads me to believe that the intention was to support at least 4 digits for a year, 
not exactly 4 digits, so failing to parse a year of 10,000 or greater seems to be an actual bug?
</comment><comment author="buzzdeee" created="2016-04-22T14:29:16Z" id="213449376">not sure if I'm hitting the same issue, I use ES 2.3.1 as well as LS 2.3.1. In LS I use date filter like this:

```
                 date {
                   match =&gt; [ "timestamp", "MMM dd HH:mm:ss", "MMM d HH:mm:ss", "MMM  d HH:mm:ss" ]
                   add_tag =&gt; [ "dated" ]
                 }
```

for a couple of log files, mainly syslog like. 

When LS tries to output to ES I get in ES logs:

MapperParsingException[failed to parse [timestamp]]; nested: IllegalArgumentException[Invalid format: "Apr 20 23:52:35"];

Other dates, that contain a year, don't seem to be problematic.

Sebastian
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index template not working as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17886</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0_77

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:
After making the upgrade from version 1.7.1 to 2.3.1, index templates are not being correctly applied. Every time you restart the ElasticSearch, those templates disappears. 
This issue occurs when using the Java API to create index templates. If you use curl requests, the functionality works as expected.

**Steps to reproduce**:
1. Create an Index Template using Java API:

``` json{
   "index_template": {
      "order": 0,
      "template": "template*",
      "settings": {},
      "mappings": {
         "type_b": {
            "dynamic": "false",
            "properties": {
               "numMessages": {
                  "type": "long"
               },
               "id": {
                  "type": "long"
               }
            }
         },
         "type_a": {
            "dynamic": "false",
            "properties": {
               "id": {
                  "type": "long"
               },
               "type": {
                  "properties": {
                     "name": {
                        "type": "string",
                        "fields": {
                           "raw": {
                              "index": "not_analyzed",
                              "type": "string"
                           }
                        }
                     },
                     "id": {
                        "type": "long"
                     }
                  }
               },
               "uuid": {
                  "type": "string",
                  "fields": {
                     "raw": {
                        "index": "not_analyzed",
                        "type": "string"
                     }
                  }
               }
            }
         }
      },
      "aliases": {}
   }
}
```
1. Creating documents in indices that doesn't exists in order to create a new index with the template mappings:
   1. PUT a document in an index that doesn't exists and is suppose to match with the created template: 
      `PUT template_test/type_a/1
      {
      "uuid": "abcd"
      }`
   2. Check the mappings of the created index (**template_test**) and you will notice that the type_b will have the expected mappings (defined in the template created in the first step). However, the type_a won't have the mappings defined in the template:

``` json{
   "template_test": {
      "mappings": {
         "type_b": {
            "dynamic": "false",
            "properties": {
               "id": {
                  "type": "long"
               },
               "numMessages": {
                  "type": "long"
               }
            }
         },
         "type_a": {
            "properties": {
               "uuid": {
                  "type": "string"
               }
            }
         }
      }
   }
}
```
1. PUT a document in an index that doesn't exists and is suppose to match with the created template : 
   `PUT template_test2/type_b/1
   {
   "id": 1
   }`
2. This time, the type_a is with the expected mappings and the type_a was created without the mappings defined in the template:

``` json{
   "template_test2": {
      "mappings": {
         "type_b": {
            "properties": {
               "id": {
                  "type": "long"
               }
            }
         },
         "type_a": {
            "dynamic": "false",
            "properties": {
               "id": {
                  "type": "long"
               },
               "type": {
                  "properties": {
                     "id": {
                        "type": "long"
                     },
                     "name": {
                        "type": "string",
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed"
                           }
                        }
                     }
                  }
               },
               "uuid": {
                  "type": "string",
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed"
                     }
                  }
               }
            }
         }
      }
   }
```
1. Restart the ElasticSearch and you'll lose the mappings of the created template `GET _template/index_template`: 

``` json{
   "index_template": {
      "order": 0,
      "template": "template*",
      "settings": {},
      "mappings": {},
      "aliases": {}
   }
```

**Note that**, even if you don't create any index which will use the template, the mappings disappears with a restart of the ElasticSearch.

**Provide logs (if relevant)**:
When creating documents as was done in the in the previous section (**Steps to reproduce**, steps 2.i and 2.iii) the flowing logs appears in the console.
- **For the step 2.i**

```
[2016-04-20 11:04:47,981][DEBUG][indices                  ] [Mikado] creating Index [template_test], shards [5]/[1]
[2016-04-20 11:04:47,989][DEBUG][index.store              ] [Mikado] [template_test] using index.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2016-04-20 11:04:47,991][DEBUG][index.mapper             ] [Mikado] [template_test] using dynamic[true]
[2016-04-20 11:04:47,994][INFO ][cluster.metadata         ] [Mikado] [template_test] creating index, cause [auto(index api)], templates [index_template], shards [5]/[1], mappings [type_b, type_a]


[2016-04-20 11:04:47,997][DEBUG][indices.cluster          ] [Mikado] [template_test] creating index
[2016-04-20 11:04:47,997][DEBUG][indices                  ] [Mikado] creating Index [template_test], shards [5]/[1]
[2016-04-20 11:04:48,001][DEBUG][index.store              ] [Mikado] [template_test] using index.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2016-04-20 11:04:48,002][DEBUG][index.mapper             ] [Mikado] [template_test] using dynamic[true]
[2016-04-20 11:04:48,003][DEBUG][indices.cluster          ] [Mikado] [template_test] adding mapping [type_a], source [{"type_a":{}}]
[2016-04-20 11:04:48,003][DEBUG][indices.cluster          ] [Mikado] [template_test] adding mapping [type_b], source [{"type_b":{"dynamic":"false","properties":{"id":{"type":"long"},"numMessages":{"type":"long"}}}}]
```
- **For the step 2.iii**

```
[2016-04-20 11:08:04,643][DEBUG][indices                  ] [Mikado] creating Index [template_test2], shards [5]/[1]
[2016-04-20 11:08:04,649][DEBUG][index.store              ] [Mikado] [template_test2] using index.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2016-04-20 11:08:04,651][DEBUG][index.mapper             ] [Mikado] [template_test2] using dynamic[true]
[2016-04-20 11:08:04,655][INFO ][cluster.metadata         ] [Mikado] [template_test2] creating index, cause [auto(index api)], templates [index_template], shards [5]/[1], mappings [type_b, type_a]


[2016-04-20 11:08:04,665][DEBUG][indices.cluster          ] [Mikado] [template_test2] creating index
[2016-04-20 11:08:04,665][DEBUG][indices                  ] [Mikado] creating Index [template_test2], shards [5]/[1]
[2016-04-20 11:08:04,676][DEBUG][index.store              ] [Mikado] [template_test2] using index.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2016-04-20 11:08:04,680][DEBUG][index.mapper             ] [Mikado] [template_test2] using dynamic[true]
[2016-04-20 11:08:04,680][DEBUG][indices.cluster          ] [Mikado] [template_test2] adding mapping [type_a], source [{"type_a":{"dynamic":"false","properties":{"id":{"type":"long"},"type":{"properties":{"id":{"type":"long"},"name":{"type":"string","fields":{"raw":{"type":"string","index":"not_analyzed"}}}}},"uuid":{"type":"string","fields":{"raw":{"type":"string","index":"not_analyzed"}}}}}}]
[2016-04-20 11:08:04,682][DEBUG][indices.cluster          ] [Mikado] [template_test2] adding mapping [type_b], source [{"type_b":{}}]
[2016-04-20 11:08:04,683][DEBUG][indices.cluster          ] [Mikado] [template_test2][2] creating shard
```
</description><key id="149725352">17886</key><summary>Index template not working as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jorgecorreiaptc</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2016-04-20T10:14:51Z</created><updated>2016-04-20T16:45:54Z</updated><resolved>2016-04-20T16:33:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T13:38:17Z" id="212426845">It sounds like something is incorrect with how you are putting the index template.  Could you provide the actual code you use?
</comment><comment author="jorgecorreiaptc" created="2016-04-20T13:58:36Z" id="212436551">I am doing this in the same way I did in the version 1.7.1.

Bellow is the code I am running:

``` java
private static void createIndexTemplate(Client client, String indexDailyPackageTemplateName, String indexDailyPackageMatch) throws IOException {

        String typeA = "{\"properties\":{\"id\":{\"type\":\"long\"},\"type\":{\"properties\":{\"name\":{\"type\":\"string\",\"fields\":{\"raw\":{\"type\":\"string\",\"index\":\"not_analyzed\"}}},\"id\":{\"type\":\"long\"}}},\"uuid\":{\"type\":\"string\",\"fields\":{\"raw\":{\"type\":\"string\",\"index\":\"not_analyzed\"}}}},\"dynamic\":\"false\"}";
        String typeB = "{\"properties\":{\"numMessages\":{\"type\":\"long\"},\"id\":{\"type\":\"long\"}},\"dynamic\":\"false\"}";

        PutIndexTemplateRequestBuilder indexTemplateRequestBuilder = client.admin().indices().preparePutTemplate(indexDailyPackageTemplateName)
            .addMapping("type_a", typeA).addMapping("type_b", typeB).setTemplate(indexDailyPackageMatch);

        indexTemplateRequestBuilder.execute().actionGet();
    }
```

After I run this, when I use:
`curl -XGET http://localhost:9200/_template/index_template`
I get

``` json{
    "index_template": {
      "order": 0,
      "template": "template*",
      "settings": {},
      "mappings": {
         "type_b": {
            "dynamic": "false",
            "properties": {
               "numMessages": {
                  "type": "long"
               },
               "id": {
                  "type": "long"
               }
            }
         },
         "type_a": {
            "dynamic": "false",
            "properties": {
               "id": {
                  "type": "long"
               },
               "type": {
                  "properties": {
                     "name": {
                        "type": "string",
                        "fields": {
                           "raw": {
                              "index": "not_analyzed",
                              "type": "string"
                           }
                        }
                     },
                     "id": {
                        "type": "long"
                     }
                  }
               },
               "uuid": {
                  "type": "string",
                  "fields": {
                     "raw": {
                        "index": "not_analyzed",
                        "type": "string"
                     }
                  }
               }
            }
         }
      },
      "aliases": {}
   }
}
```

It looks like the template was created, but it didn't work as expected.

Thanks.
</comment><comment author="clintongormley" created="2016-04-20T14:47:35Z" id="212455089">Sorry @jorgecorreiaptc - I've tried this several times, with one node and with two.  It works the way it is supposed to work, I'm not seeing what you describe.  Any more details?
</comment><comment author="clintongormley" created="2016-04-20T14:47:51Z" id="212455330">Try on a fresh cluster and record all the recreation steps?
</comment><comment author="jorgecorreiaptc" created="2016-04-20T15:13:32Z" id="212471309">I downloaded the ElasticSearch (v2.3.1) just to make sure I have a clean cluster with the default configurations. 

Bellow is the java complete java code I am using to configure the Index Template:

``` java
package pt.aig.aigx.index.configurator;

import java.io.IOException;
import java.net.InetAddress;
import java.net.UnknownHostException;

import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder;
import org.elasticsearch.client.Client;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.transport.InetSocketTransportAddress;

public class IndexTemplateTest {

    private static final String[] HOSTS = {"127.0.0.1"};

    private static final int PORT = 9300;

    private static final String CLUSTER_NAME = "elasticsearch";

    private static final String NODE_NAME = "rest-analytics";

    private static final String MULTICAST_ENABLED = "false";

    public static void main(String[] args) throws Exception {

        Client client = getESClient();

        createIndexTemplate(client, "index_template", "template*");
    }

    private static void createIndexTemplate(Client client, String indexDailyPackageTemplateName, String indexDailyPackageMatch) throws IOException {

        String typeA = "{\"properties\":{\"id\":{\"type\":\"long\"},\"type\":{\"properties\":{\"name\":{\"type\":\"string\",\"fields\":{\"raw\":{\"type\":\"string\",\"index\":\"not_analyzed\"}}},\"id\":{\"type\":\"long\"}}},\"uuid\":{\"type\":\"string\",\"fields\":{\"raw\":{\"type\":\"string\",\"index\":\"not_analyzed\"}}}},\"dynamic\":\"false\"}";
        String typeB = "{\"properties\":{\"numMessages\":{\"type\":\"long\"},\"id\":{\"type\":\"long\"}},\"dynamic\":\"false\"}";

        PutIndexTemplateRequestBuilder indexTemplateRequestBuilder = client.admin().indices().preparePutTemplate(indexDailyPackageTemplateName)
            .addMapping("type_a", typeA).addMapping("type_b", typeB).setTemplate(indexDailyPackageMatch);

        indexTemplateRequestBuilder.execute().actionGet();
    }

    private static Client getESClient() {
        Settings settings = buildElasticSearchSettings();

        TransportClient transportClient = TransportClient.builder().settings(settings).build();
        for (String host : HOSTS) {
            try {
                transportClient = transportClient.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(host), PORT));
            } catch (UnknownHostException e) {
                throw new RuntimeException(e);
            }
        }

        return transportClient;
    }

    private static Settings buildElasticSearchSettings() {

      //@formatter:off
        return Settings.settingsBuilder()
            .put("cluster.name", CLUSTER_NAME)
            .put("node.name", NODE_NAME)
            .put("discovery.zen.ping.multicast.enabled", MULTICAST_ENABLED)
            .put("discovery.zen.ping.unicast.hosts", getUnicastHostsArray())
            .build();
        //@formatter:on
    }

    private static String getUnicastHostsArray() {
        String hosts = "";
        int it = 0;
        for (String host : HOSTS) {
            if (it == 0) {
                hosts += host + ":" + PORT;
            } else {
                hosts += "," + host + ":" + PORT;
            }
            ++it;
        }

        return hosts;
    }

}
```

The dependency I currently have in my pom.xml

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
    &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
    &lt;version&gt;2.3.1&lt;/version&gt;
&lt;/dependency&gt;
```

And still have the same problem
</comment><comment author="johtani" created="2016-04-20T16:22:36Z" id="212499785">hi @jorgecorreiaptc 

I changed your code as follows, added each type name in mapping.

```
        String typeA = "{\"type_a\":{\"properties\":{\"id\"...\"dynamic\":\"false\"}}";
        String typeB = "{\"type_b\":{\"properties\":{\"num...,\"dynamic\":\"false\"}}";
```

Then, it works for me. 

I think it is related to https://github.com/elastic/elasticsearch/issues/10436#issuecomment-172060358

cc: @clintongormley 
</comment><comment author="clintongormley" created="2016-04-20T16:33:47Z" id="212503414">thanks @johtani 

Closing in favour of https://github.com/elastic/elasticsearch/issues/10436
</comment><comment author="jorgecorreiaptc" created="2016-04-20T16:45:54Z" id="212508768">Hi @johtani 

I did those changes and it is working. 

Thank you @johtani and @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic fields are still added when `dynamic: false` but field is an array in document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17885</link><project id="" key="" /><description>This was found by testing a code comment left by @rjernst : https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L544-545

Run the following Sense script:

```
PUT test
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "index.mapper.dynamic":false
  },
  "mappings": {
    "doc": {
      "properties": {
        "foo": {
          "type": "string"
        }
      }
    }
  }
}

POST test/doc/1
{
  "ignore_me": [2.0, 3.0]
}

GET test
```

The index is created with dynamic mappings set to `false` so all unmapped fields should be silently ignored and not added to the mappings. However the last command returns:

```
{
  "test": {
    "aliases": {},
    "mappings": {
      "doc": {
        "properties": {
          "foo": {
            "type": "text"
          },
          "ignore_me": {
            "type": "float"
          }
        }
      }
    },
    "settings": {
      "index": {
        "mapper": {
          "dynamic": "false"
        },
        "creation_date": "1461146374897",
        "number_of_shards": "1",
        "number_of_replicas": "0",
        "uuid": "WrcW-mGOT9iD-saDgZqc8A",
        "version": {
          "created": "5000099"
        }
      }
    }
  }
```

This shows that the `ignore_me` field was in fact added to the mappings and running the search below will confirm that the field was indexed (the search matches the document).

```
GET test/_search
{
  "query": {
    "term": {
      "ignore_me": {
        "value": 2.0
      }
    }
  }
}
```
</description><key id="149722216">17885</key><summary>Dynamic fields are still added when `dynamic: false` but field is an array in document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2016-04-20T10:02:25Z</created><updated>2016-05-02T12:07:21Z</updated><resolved>2016-04-20T10:09:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-20T10:09:15Z" id="212364167">This was an error in setting the dynamic parameter, it should be set using:

```
PUT test
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "doc": {
      "dynamic": "false",
      "properties": {
        "foo": {
          "type": "string"
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How many disks are used by one shard?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17884</link><project id="" key="" /><description>[question1](http://stackoverflow.com/questions/36737023/elasticsearchhow-to-make-a-balance-of-the-usage-of-disk?noredirect=1#comment61059094_36737023) tells me: the shards' files are all allocated on one path only and not spread on all paths. And ES will not try to balance the files between paths.

[question2](http://stackoverflow.com/questions/36709408/elasticsearchper-shard-per-node-vs-multiple-shards-per-node) tells me:if you define more than one location (one per disk for example) it will use both disks and will benefit from the increased IOPS.

I am confused..

I just want to make the usage of the disk are almost equal.
</description><key id="149705938">17884</key><summary>How many disks are used by one shard?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhuqunzhou</reporter><labels /><created>2016-04-20T09:00:25Z</created><updated>2016-04-20T09:26:38Z</updated><resolved>2016-04-20T09:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-20T09:26:38Z" id="212347388">Please ask at https://discuss.elastic.co. We use these github issues for bugs and feature requests only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing input data for one minute.maybe about the template mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17883</link><project id="" key="" /><description>Elasticsearch version: 2.1.1

JVM version: 1.8.0_40

OS version: ubuntu 12.04

Description of the problem including expected versus actual behavior:
I create a template mapping some logs.the input is in the same format and well run for a while(months).
However I find that today(2016.4.20) 14:00:00 to 14:00:01, I am missing data of this minute.I have the statistic of writing to ElasticSearch and the statistic tells data has been send successfully to Es Server.
Then I find out the logs below.
If u need any other clues.please contact me.

the template is set by 

```
PUT /_template/t1
{
  "template": "*",
  "settings": {
    "index.refresh_interval": "30s",
    "index.routing.allocation.require.box_type": "hot",
    "number_of_shards": "52"
  },
  "mappings": {
    "log": {
      "properties": {
        "timestamp": {
          "type": "date",
          "format": "epoch_second"
        },
        "service": {
          "type": "string",
          "index": "not_analyzed",
          "doc_values": false
        },
        "id": {
          "type": "string",
          "index": "not_analyzed",
          "doc_values": false
        },
        "ip": {
          "type": "string",
          "index": "not_analyzed"
        },
        "function": {
          "type": "string",
          "index": "not_analyzed"
        },
        "logtype": {
          "type": "string",
          "index": "not_analyzed",
          "doc_values": false
        }
      }
    }
  }
}
```

Provide logs (if relevant):

```
[2016-04-20 14:00:36,552][DEBUG][action.admin.indices.mapping.put] [Gregory Gideon] failed to put mappings on indices [[bt_svc_ap_2016042014]], type [log]
ProcessClusterEventTimeoutException[failed to process cluster event (put-mapping [log]) within 30s]
        at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:290)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-20 14:00:36,552][DEBUG][action.admin.indices.mapping.put] [Gregory Gideon] failed to put mappings on indices [[bt_svc_ap_2016042014]], type [log]
ProcessClusterEventTimeoutException[failed to process cluster event (put-mapping [log]) within 30s]
        at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:290)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="149690355">17883</key><summary>Missing input data for one minute.maybe about the template mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yuz11</reporter><labels /><created>2016-04-20T07:57:17Z</created><updated>2016-04-20T13:01:39Z</updated><resolved>2016-04-20T13:01:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T13:01:39Z" id="212414204">Hi @yuz11 

It looks like nodes in your cluster were unresponsive so the mapping update failed.  I'd investigate what was happening to your nodes during that time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix cross type mapping updates for `boolean` fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17882</link><project id="" key="" /><description>Boolean fields were not handled in `DocumentParser.createBuilderFromFieldType`.
This also improves the logic a bit by falling back to the default mapping of
the given type insteah of hard-coding every case and throws a better exception
than a NPE if no dynamic mappings could be created.

Closes #17879
Closes #18740
</description><key id="149686266">17882</key><summary>Fix cross type mapping updates for `boolean` fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-20T07:42:40Z</created><updated>2016-08-26T13:25:14Z</updated><resolved>2016-04-20T07:49:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-04-20T07:45:48Z" id="212305319">Thanks for fixing. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inject interface with eagerSingleton which also has LifecycleComponent return 2 instance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17881</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: 1.7.0_79

**OS version**: OS X 10.8.5

**Description of the problem including expected versus actual behavior**:
Inject Interface with eagerSingleton() and also implements LifecycleComponent return 2 instance which should be singleton.

**Sample**. 

```
interface Test {
  void test();
}

class TestImpl extends AbstractLifecycleComponent&lt;TestImpl&gt; implements Test {
  void test() {
System.out.print("test");
 }
}

public class TestModule extends AbstractModule {

    @Override
    protected void configure() {
        bind(Test.class).to(TestImpl.class).asEagerSingleton();
    }
}

public class TestPlugin extends Plugin {

    @Override
    public String name() {
        return "TestPlugin";
    }

    @Override
    public String description() {
        return "for testing";
    }

    @Override
    public Collection&lt;Module&gt; nodeModules() {
        return Collections.&lt;Module&gt;singletonList(new TestModule());
    }

    @SuppressWarnings("rawtypes")
    @Override
    public Collection&lt;Class&lt;? extends LifecycleComponent&gt;&gt; nodeServices() {
        final Collection&lt;Class&lt;? extends LifecycleComponent&gt;&gt; services = Lists
                .newArrayList();
        services.add(TestImpl.class);

        return services;
    }
}
```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="149683944">17881</key><summary>Inject interface with eagerSingleton which also has LifecycleComponent return 2 instance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bandika-dgit</reporter><labels><label>:Java API</label><label>:Plugins</label><label>discuss</label><label>feedback_needed</label></labels><created>2016-04-20T07:31:37Z</created><updated>2016-04-27T06:27:09Z</updated><resolved>2016-04-27T06:27:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-22T09:24:28Z" id="213350929">I'm not too familar with injection but this looks expected to me since `TestImpl` is registered both through `nodeModules` and `nodeServices`.
</comment><comment author="jaymode" created="2016-04-22T10:49:13Z" id="213376092">I've seen this before and to workaround it you bind the implementation as a singleton, then bind the impl to the interface.

``` java
public class TestModule extends AbstractModule {

    @Override
    protected void configure() {
        bind(TestImpl.class).asEagerSingleton();
        bind(Test.class).to(TestImpl.class);
    }
}
```

@bandika-dgit can you try this out?
</comment><comment author="bandika-dgit" created="2016-04-27T06:27:02Z" id="214982972">@jaymode thanks its worked :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>**dynamic false** doesn't ignore the new fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17880</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:1.7.3

**JVM version**:1.7.0_45

**OS version**:Oracle Linux Server release 7.1

**Description of the problem including expected versus actual behavior**:
**dynamic false** doesn't ignore the new fields

**Steps to reproduce**:
 1.
`PUT _template/mytest
{
    "template": "std_content*", 
    "settings": {
        "number_of_shards": "1",
        "number_of_replicas": "0",
        "refresh_interval": "10s"
    },
    "mappings": {
        "comment":{
            "dynamic": "false",
            "properties": {
                "author":{
                    "properties":{
                        "user_id":{
                            "type": "long"
                        },
                        "screen_name":{
                            "type": "string"
                        }
                    }
                }
            }
        }
    }
}`
 2.
`PUT std_content_2016-4`
 3.
`POST std_content_2016-4/xueqiu/1
{
    "author":{
        "screen_name": "999999",
        "should_be_ignored":"no"
    }
}`

I expect ignoring the field "test", but it created document. if with "dynamic=strict", it throws exception, it works as expectation.
**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="149672873">17880</key><summary>**dynamic false** doesn't ignore the new fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cnwarden</reporter><labels /><created>2016-04-20T06:32:24Z</created><updated>2016-04-20T13:12:08Z</updated><resolved>2016-04-20T13:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-20T08:59:09Z" id="212337935">How did you check that the field is not ignored?
</comment><comment author="colings86" created="2016-04-20T09:54:47Z" id="212357474">In your example above your mappings are for the type `comment` but the document you add has the type `xueqiu`. If I replace the type in the template with `xueqiu` the fields should be ignored
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sporadic NPE in DocumentParser.parseDynamicValue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17879</link><project id="" key="" /><description>Different tests are failing in CI with an NPE recently, e.g.:
- org.elasticsearch.ingest.IngestClientIT#testBulkWithIngestFailures(): https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/511/console
- org.elasticsearch.messy.tests.MinDocCountTests: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/512/console
- org.elasticsearch.search.scroll.DuelScrollIT#testDuelIndexOrderQueryThenFetch(): https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/81/console

For `IngestClientIT` we managed to get a stack trace:

```
java.lang.NullPointerException
    at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:819)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:579)
    at org.elasticsearch.index.mapper.DocumentParser.innerParseObject(DocumentParser.java:412)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrNested(DocumentParser.java:392)
    at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:112)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:80)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:286)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:474)
    at org.elasticsearch.index.shard.IndexShard.prepareIndexOnPrimary(IndexShard.java:455)
    at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:188)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:196)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:339)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.index(TransportShardBulkAction.java:156)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.handleItem(TransportShardBulkAction.java:135)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:121)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:70)
```
</description><key id="149672361">17879</key><summary>Sporadic NPE in DocumentParser.parseDynamicValue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>test</label></labels><created>2016-04-20T06:29:01Z</created><updated>2016-04-20T07:49:04Z</updated><resolved>2016-04-20T07:49:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix the semantics for the BlobContainer interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17878</link><project id="" key="" /><description>This commit contains the following:
1. Clarifies the behavior that must be adhered to by any implementors
   of the BlobContainer interface.  This is done through expanded Javadocs.
2. BlobContainer#writeBlob cannot overwrite an already existing blob.
   It will now throw an exception if trying to write to a pre-existing
   file.

Closes #15579
Closes #15580
</description><key id="149652156">17878</key><summary>Fix the semantics for the BlobContainer interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-20T04:00:45Z</created><updated>2016-05-02T12:08:13Z</updated><resolved>2016-04-20T14:42:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Unable to recover from  shards initializing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17877</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**:8u77

**OS version**:OL 7u1 and Solaris 11.2  

**Description of the problem including expected versus actual behavior**:
Cluster is operating normally and runs into an out of space condition on a node, that is the 85% value is reached. Shard stays in initializing state and following does not clear the condition.

1) Attempts to reassign ( Message below)
2) Reboots of nodes with shard having issue resident
3) Change of replicas from 4 to 2

**Steps to reproduce**:
1. Run a cluster and fill up the disk on some node
2. Have other indices with continuous writes
3. wait until shards start showing unassigned

**Provide logs (if relevant)**:

2016-04-19 22:32:35] {"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"[move_allocation] can't move [logstash-feeder-df-2016.04][1], shard is not started (state = INITIALIZING]"}],"type":"illegal_argument_exception","reason":"[move_allocation] can't move [logstash-feeder-df-2016.04][1], shard is not started (state = INITIALIZING]"},"status":400}
</description><key id="149639585">17877</key><summary>Unable to recover from  shards initializing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">digitalrinaldo</reporter><labels /><created>2016-04-20T02:39:56Z</created><updated>2016-04-20T12:18:59Z</updated><resolved>2016-04-20T12:18:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T12:18:59Z" id="212402475">Duplicate of https://github.com/elastic/elasticsearch/pull/17698
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ERROR: Parameter [-Xms1g]does not start with --</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17876</link><project id="" key="" /><description>elasticsearch start error!
C:\DLPARCHIVE\elasticsearch\elasticsearch-2.3.1\bin&gt;elasticsearch -Xms1g -Xmx7g
ERROR: Parameter [-Xms1g]does not start with --

**Elasticsearch version**: 2.3.1

**JVM version**: 1.7.0_79

**OS version**: windows Server 2008 R2 Enterprise

**Description of the problem including expected versus actual behavior**:
![image](https://cloud.githubusercontent.com/assets/3656711/14660751/ea9c36da-06e2-11e6-9891-2d71a40e8176.png)

**Steps to reproduce**:

 1.command line 

C:\DLPARCHIVE\elasticsearch\elasticsearch-2.3.1\bin&gt;elasticsearch -Xmx12g -Xms12g 
ERROR: Parameter [-Xms1g]does not start with --
1. C:\DLPARCHIVE\elasticsearch\elasticsearch-2.3.1\bin&gt;elasticsearch 
   [2016-04-20 09:51:11,687][INFO ][node                     ] [Anole] version[2.3.1], pid[4292], build[bd98092/2016-04-04T12:25:05Z]
   [2016-04-20 09:51:11,687][INFO ][node                     ] [Anole] initializing ...
   [2016-04-20 09:51:12,404][INFO ][plugins                  ] [Anole] modules [lang-groovy, reindex, lang-expression], plugins [], sites []
   [2016-04-20 09:51:12,436][INFO ][env                      ] [Anole] using [1] data paths, mounts [[(C:)]], net usable_space [252.7gb], net total_space [390.6gb], spins? [unknown], types [NTFS]
   [2016-04-20 09:51:12,436][INFO ][env                      ] [Anole] heap size [910.5mb], compressed ordinary object pointers [true]
   [2016-04-20 09:51:14,760][INFO ][node                     ] [Anole] initialized
   [2016-04-20 09:51:14,760][INFO ][node                     ] [Anole] starting ...
   [2016-04-20 09:51:14,963][INFO ][transport                ] [Anole] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
   [2016-04-20 09:51:14,978][INFO ][discovery                ] [Anole] elasticsearch/ykEJ2t9DT-2IexEK81eNTw
   [2016-04-20 09:51:19,081][INFO ][cluster.service          ] [Anole] new_master {Anole}{ykEJ2t9DT-2IexEK81eNTw}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
   [2016-04-20 09:51:19,159][INFO ][gateway                  ] [Anole] recovered [0] indices into cluster_state
   [2016-04-20 09:51:19,222][INFO ][http                     ] [Anole] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
   [2016-04-20 09:51:19,222][INFO ][node                     ] [Anole] started

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="149629885">17876</key><summary>ERROR: Parameter [-Xms1g]does not start with --</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">purehani10</reporter><labels /><created>2016-04-20T01:36:26Z</created><updated>2016-04-20T02:25:25Z</updated><resolved>2016-04-20T02:25:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2016-04-20T02:25:23Z" id="212216350">Can you ask such questions on [discuss.elastic.co](https://discuss.elastic.co) please? We keep this place only for confirmed issues.

You should use the environment variables. See 
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/setup-configuration.html#setup-configuration
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate camelCase settings magic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17875</link><project id="" key="" /><description>Looking up settings currently falls back to adjusting the setting key to
camelCase, and then looking at the parsed settings keys again. This adds
deprecation logging to that case. While in 5.0 settings validation will
handle most of these cases, there stills exists some code (eg analysis
providers) which lookup settings directly on the Settings object.

see #8988
</description><key id="149617756">17875</key><summary>Deprecate camelCase settings magic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Settings</label><label>deprecation</label><label>v2.3.2</label><label>v2.4.0</label></labels><created>2016-04-20T00:12:19Z</created><updated>2016-04-21T15:42:34Z</updated><resolved>2016-04-21T15:30:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-20T00:23:10Z" id="212182615">LGTM
</comment><comment author="jpountz" created="2016-04-20T06:33:57Z" id="212280264">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Remove unused Settings methods taking multiple setting names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17874</link><project id="" key="" /><description>The Settings class has an enormous amount of methods with variations of
parameters. This change removes all the methods which take multiple
setting names, which were completely unused.
</description><key id="149611913">17874</key><summary>Internal: Remove unused Settings methods taking multiple setting names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T23:32:45Z</created><updated>2016-04-20T12:04:31Z</updated><resolved>2016-04-19T23:38:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-19T23:36:27Z" id="212171594">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `fingerprint` token filter and `fingerprint` analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17873</link><project id="" key="" /><description>Adds a `fingerprint` token filter which uses Lucene's FingerprintFilter, and an `openrefine` analyzer that combines the Fingerprint filter with lowercasing and asciifolding.

The OpenRefine algo says to use ascii-folding as the last step, and the reference implementation definitely does it last.  But that doesn't make sense to me?  Surely it would be better to fold after lowercasing but before sorting/deduping?  Otherwise `"g&#246;del godel foo bar"` will be emitted as `"bar foo godel godel "`, which seems to defeat the point of ascii-folding and deduping.

The analyzer itself just lifts the defaults from the various components and exposes them.  I figured it made more sense to default to their defaults than re-define them.

I'm open to changing any of the names.  I opted for `openrefine` rather than a `fingerprint` analyzer, since A) it's a specific implementation and B) I don't like when we share names between analyzer/tokenizer/filter...it's confusing for people :)

/cc @markharwood 

Closes #13325 
</description><key id="149605211">17873</key><summary>Add `fingerprint` token filter and `fingerprint` analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Analysis</label><label>feature</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T22:51:08Z</created><updated>2016-04-25T14:21:05Z</updated><resolved>2016-04-20T20:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-20T07:08:00Z" id="212289890">Please give @markharwood a chance to look at it before merging, but this looks good to me.
</comment><comment author="clintongormley" created="2016-04-20T12:32:08Z" id="212405266">I'd be ok with a `fingerprint` analyzer, as it is probably more meaningful to the user than `openrefine`.  Also, OpenRefine uses a number of techniques, one of which is this process which it describes as `Fingerprint`.

I'd also add the stop words token filter (with stop words disabled by default).
</comment><comment author="markharwood" created="2016-04-20T13:02:02Z" id="212414295">LGTM, thanks @polyfractal 
</comment><comment author="polyfractal" created="2016-04-20T14:01:52Z" id="212438364">Np @markharwood :)  What do you think about the order of the ascii-folding?  Keep as OpenRefine implements it?

@clintongormley My only major concern is that "fingerprint" is pretty general... there are a lot of different types of fingerprints.  E.g. OpenRefine goes on to list ngram and phoenetic fingerprints, and you could probably lump hash-based fingerprints in too (md5, minhash, etc).  

But I agree "openrefine" is not overly indicative as to what it does, or which fingerprint from OR is used.  I'll switch it to `fingerprint` unless we come up with a better name in the meantime.  ++stopwords, will add those. 
</comment><comment author="markharwood" created="2016-04-20T15:20:30Z" id="212474664">Order of ascii-folding I am not concerned about as long as it is deterministic. I see the tokens being used for machine use rather than human consumption. When alerting users to show near-dups I would always resort to showing examples of original doc text rather than raw link tokens.
</comment><comment author="polyfractal" created="2016-04-20T17:15:30Z" id="212519797">Changes made!  Sorry, I amended my commit and pushed before I remembered that makes reviewing more difficult
- Empty stop words.  I added it after lowercasing, before fingerprint
- Renamed to `fingerprint` analyzer
- Updated docs, fixed typos
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move parentTaskId into TransportRequest </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17872</link><project id="" key="" /><description>Now everything can have a parent if it wants one!
</description><key id="149594286">17872</key><summary>Move parentTaskId into TransportRequest </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T21:51:39Z</created><updated>2016-04-22T16:21:49Z</updated><resolved>2016-04-22T16:21:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T21:51:54Z" id="212144621">@imotov this feels too easy....
</comment><comment author="nik9000" created="2016-04-19T21:55:17Z" id="212145973">Going to clean it up a bit more, but the idea is ready....
</comment><comment author="nik9000" created="2016-04-20T00:20:58Z" id="212182232">@imotov I think this is ready for review.
</comment><comment author="imotov" created="2016-04-22T15:40:00Z" id="213479271">LGTM
</comment><comment author="nik9000" created="2016-04-22T15:42:37Z" id="213479902">For anyone watching from the sidelines: both @imotov and I thought "this is too easy" and spent a while poking at it. Surprisingly, neither of us could find anything wrong. Which makes us both suspicious.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix BulkItemResponse.Failure.toString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17871</link><project id="" key="" /><description>It was busted and causing intermittent test failures.
</description><key id="149586376">17871</key><summary>Fix BulkItemResponse.Failure.toString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T21:14:11Z</created><updated>2016-04-19T21:45:23Z</updated><resolved>2016-04-19T21:39:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T21:15:09Z" id="212133400">Anyone want to have a look a this? @jpountz, I believe Strings.toString is yours?
</comment><comment author="jpountz" created="2016-04-19T21:28:25Z" id="212137029">This looks good as is, but I wonder if we could make BulkItemResponse's toXContent write the outer object (and fix call sites accordingly) so that we do not need this additional boolean?
</comment><comment author="dakrone" created="2016-04-19T21:28:43Z" id="212137181">It looks like @martijnvg just fixed this in master already: https://github.com/elastic/elasticsearch/commit/935ccb13043e1064d512bc3a45ea12e7749f083a
</comment><comment author="nik9000" created="2016-04-19T21:35:03Z" id="212139502">I think the issue is the BulkItemResponse.Failure just loves to be embedded in stuff which is why it doesn't emit the object. I _know_ we have others that do that kind of thing which is why I added the boolean rather than tried for a fix right on BulkItemResponse.Failure. I agree it isn't a nice way to behave though.
</comment><comment author="martijnvg" created="2016-04-19T21:37:08Z" id="212139977">@nik9000 I reverted the change. This change looks good. 

@jpountz I think we can't change that without changing the actual format?
</comment><comment author="nik9000" created="2016-04-19T21:40:41Z" id="212141019">Ok! I've merged this but I'd be happy for someone to get us to the point where we don't need it. ToXContent's contract isn't really strict enough I think and I'd love to go on a hunt for all the bad implementers but I'm on other quests right now!
</comment><comment author="jpountz" created="2016-04-19T21:45:23Z" id="212142211">Fine with me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shorten the serialization of the empty TaskId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17870</link><project id="" key="" /><description>We plan to change every request so that it can support a parentTaskId.
This shrinks EMPTY_TASK_ID, which will be on every request once that change
is made, from 31 bytes to 9 bytes to 1 byte.
</description><key id="149578557">17870</key><summary>Shorten the serialization of the empty TaskId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T20:43:08Z</created><updated>2016-04-20T11:56:04Z</updated><resolved>2016-04-19T21:21:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-04-19T21:08:45Z" id="212130417">Left a minor comment. Otherwise LGTM.
</comment><comment author="nik9000" created="2016-04-19T21:21:38Z" id="212135163">Fixed and pushed!
</comment><comment author="nik9000" created="2016-04-19T21:21:44Z" id="212135188">Thanks for the review @imotov !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query in upgrading from 1.7.1 to 2.3.1version of ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17869</link><project id="" key="" /><description>Currently we are using ElasticSearch 1.7.1, we have planned to upgrade to 2.3.1 and we have done all of the steps as mentioned in the ES 2.3.1 upgrading help documentations.

After we initiated the upgrade, 2.3.1 cluster started and nodes are connected properly, but pending_tasks are increasing consistently. It took more than 4 hours, but still pending_tasks are increasing and those tasks are refresh-mapping [index] fyi. 

If anyone done the upgrade to 2.3.1or 2.x versions then can anyone please share your experience about this? Its an usual behaviour?
</description><key id="149564244">17869</key><summary>Query in upgrading from 1.7.1 to 2.3.1version of ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kannansamp</reporter><labels /><created>2016-04-19T19:49:22Z</created><updated>2016-04-20T00:25:58Z</updated><resolved>2016-04-20T00:25:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-04-20T00:25:58Z" id="212183016">It looks like you opened this up over at https://discuss.elastic.co/t/query-in-upgrading-from-1-7-1-to-2-3-1version-of-elasticsearch/47831 which seems like a better place for this troubleshooting.  I'm going to close here for now.  If that discuss thread points to a real bug in ES, then we can reopen here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Normalize registration of MovAvgModels</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17868</link><project id="" key="" /><description>This is what happens when you pull on the "Remove the PROTOTYPEs from
MovAvgModels" string. This removes MovAvgModelStreams in favor of
readNamedWriteable and MovAvgParserMapper in favor of
ParseFieldRegistry&lt;MovAvgModel.AbstractModelParser&gt;.

Relates to #17085
</description><key id="149556419">17868</key><summary>Normalize registration of MovAvgModels</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T19:22:00Z</created><updated>2016-04-20T11:43:30Z</updated><resolved>2016-04-20T11:43:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T19:22:26Z" id="212076860">@colings86, with any luck this is my last PR in the Aggregations space for PROTOTYPEs.
</comment><comment author="colings86" created="2016-04-20T07:25:13Z" id="212297404">LGTM
</comment><comment author="nik9000" created="2016-04-20T11:43:30Z" id="212392449">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable Disk Allocation Watermarks On Localhost Binding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17867</link><project id="" key="" /><description>**Describe the feature**:  Would it be possible to turn off watermarks for nodes that are bound to localhost?  I ask because some users download Elasticsearch and can't get things going because their disk doesn't have much space left.  Many may not realize they need to change this setting and may get frustrated early on.
</description><key id="149553233">17867</key><summary>Disable Disk Allocation Watermarks On Localhost Binding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NathanZamecnik</reporter><labels><label>discuss</label></labels><created>2016-04-19T19:07:30Z</created><updated>2016-04-19T21:43:39Z</updated><resolved>2016-04-19T21:43:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-19T19:57:39Z" id="212101135">@NathanZamecnik they are already ignored when there is only a single data node, are you running into situations where this is problematic?
</comment><comment author="NathanZamecnik" created="2016-04-19T21:24:14Z" id="212135824">In the Ops course we have students set up 3 nodes and some of them run into this issue.  Honestly, it may be good they do as they should understand this setting and what to do about it.

I didn't realize a single node ignores it already - I'd say that's good enough.
</comment><comment author="dakrone" created="2016-04-19T21:43:39Z" id="212141820">Cool, will close this then, we can always re-open!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error with geo_shape relation CONTAINS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17866</link><project id="" key="" /><description>Hi all.
I have the following error trying to execute a spatial query with a ShapeRelation Contain.
aused by: java.lang.IllegalArgumentException: 
    at org.elasticsearch.index.query.GeoShapeQueryParser.getArgs(GeoShapeQueryParser.java:192)
    at org.elasticsearch.index.query.GeoShapeQueryParser.parse(GeoShapeQueryParser.java:169)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:263)
    at org.elasticsearch.index.query.IndexQueryParserService.parseInnerFilter(IndexQueryParserService.java:220)
    at org.elasticsearch.search.query.PostFilterParseElement.parse(PostFilterParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:838)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:654)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:620)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:371)

Regards 
</description><key id="149542063">17866</key><summary>Error with geo_shape relation CONTAINS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">glascaleia</reporter><labels><label>:Geo</label><label>bug</label><label>v2.3.3</label></labels><created>2016-04-19T18:22:15Z</created><updated>2016-05-18T07:35:51Z</updated><resolved>2016-05-18T07:34:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-04-20T00:15:38Z" id="212181399">@glascaleia can you please post the query you're running that's giving you this error?
</comment><comment author="glascaleia" created="2016-04-20T07:29:17Z" id="212298407"> {
  "query" : {
    "bool" : {
      "must" : {
        "geo_shape" : {
          "mountainNowArea.location" : {
            "shape" : {
              "type" : "polygon",
              "coordinates" : [ [ [ -4.21875, 25.482951175 ], [ 15.1171875, 27.994401411 ], [ 24.609375, 18.312810846 ], [ 23.5546875, 7.710991655 ], [ -7.03125, 12.897489184 ], [ -4.21875, 25.482951175 ] ] ]
            },
            "relation" : "contains"
          },
          "_name" : null
        }
      }
    }
  },
  "sort" : [ {
    "mountainNowArea.creationDate" : {
      "order" : "desc"
    }
  } ]
}
</comment><comment author="clintongormley" created="2016-04-20T12:42:00Z" id="212408094">This fails for me on 2.3.0, but works on master.  Full recreation:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "mountainNowArea": {
          "type": "object",
          "properties": {
            "location": {
              "type": "geo_shape"
            },
            "creationDate": {
              "type": "date"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "mountainNowArea": {
    "location": {
      "type": "polygon",
      "coordinates": [
        [
          [
            100,
            0
          ],
          [
            101,
            0
          ],
          [
            101,
            1
          ],
          [
            100,
            1
          ],
          [
            100,
            0
          ]
        ]
      ]
    },
    "creationDate": "2015-01-01"
  }
}

GET _search
{
  "query": {
    "bool": {
      "must": {
        "geo_shape": {
          "mountainNowArea.location": {
            "shape": {
              "type": "polygon",
              "coordinates": [
                [
                  [
                    -4.21875,
                    25.482951175
                  ],
                  [
                    15.1171875,
                    27.994401411
                  ],
                  [
                    24.609375,
                    18.312810846
                  ],
                  [
                    23.5546875,
                    7.710991655
                  ],
                  [
                    -7.03125,
                    12.897489184
                  ],
                  [
                    -4.21875,
                    25.482951175
                  ]
                ]
              ]
            },
            "relation": "contains"
          },
          "_name": null
        }
      }
    }
  },
  "sort": [
    {
      "mountainNowArea.creationDate": {
        "order": "desc"
      }
    }
  ]
}
```
</comment><comment author="nknize" created="2016-05-06T16:43:54Z" id="217494394">@glascaleia what version of ES?

@clintongormley can you confirm the following exception?

``` javascript
"reason": "Failed to find geo_shape field [mountainNowArea.location]",
```

Can you confirm success when explicitly searching the `t` index? e.g.:

``` javascript
GET t/_search
```

This failure is unrelated to the original IAE issue. So it sounds to me like there are 2 separate issues.
</comment><comment author="clintongormley" created="2016-05-18T07:34:03Z" id="219948700">Fixed by 26b078ff56062140e4a2f309b98eb6c560185f70
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>assertion error: assertNotTransportThread while testing rest handler using ESIntegTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17865</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: 1.7.0_79

**OS version**: OS X 10.8.5

**Description of the problem including expected versus actual behavior**:
Testing rest handler which called client api prepareUpdate returned assertionError:
`java.lang.AssertionError: Expected current thread [Thread[elasticsearch[node_s0][http_server_worker][T#1]{New I/O worker #28},5,TGRP-GetReportFileRestHandlerTest]] to not be a transport thread. Reason: [Blocking operation]`

Expected not to throw assertion error

**Steps to reproduce**:
1. Create rest handler which called search API
2. Invoke rest handler within ESIntegTestCase

**Provide logs (if relevant)**:
[2016-04-20 01:58:22,213][INFO ][rest.suppressed          ] /interactions/order/_report/not_exists Params: {id=not_exists, index=interactions, type=order}
java.lang.AssertionError: Expected current thread [Thread[elasticsearch[node_s0][http_server_worker][T#1]{New I/O worker #28},5,TGRP-GetReportFileRestHandlerTest]] to not be a transport thread. Reason: [Blocking operation]
    at org.elasticsearch.transport.Transports.assertNotTransportThread(Transports.java:62)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:115)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:42)
    at test.SimpleReportExecutionRepository.getDocument(SimpleReportExecutionRepository.java:164)
    at test.SimpleReportExecutionRepository.get(SimpleReportExecutionRepository.java:92)
    at test.ReportExecutionMonitor.get(ReportExecutionMonitor.java:149)
    at test.GetReportFileRestHandler.handleRequest(GetReportFileRestHandler.java:81)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:363)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="149539572">17865</key><summary>assertion error: assertNotTransportThread while testing rest handler using ESIntegTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bandika-dgit</reporter><labels /><created>2016-04-19T18:11:28Z</created><updated>2016-04-19T18:31:42Z</updated><resolved>2016-04-19T18:20:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-19T18:20:07Z" id="212053609">It's not a bug, you're doing something you shouldn't be doing: executing a blocking operation on a networking thread. That is exactly what this assertion is protecting against. One solution is to execute your request asynchronously. 
</comment><comment author="bandika-dgit" created="2016-04-19T18:28:04Z" id="212057607">Hi, thanks for quick response.
I've test on elasticsearch instance also, but the rest handler not throwing any assertion error. 
</comment><comment author="jasontedor" created="2016-04-19T18:31:42Z" id="212059526">Assertions aren't enabled when you run Elasticsearch by default. You can enable them by launching the JVM with `-ea` (pass this via `ES_JAVA_OPTS`). :smile:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix dynamic check to properly handle parents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17864</link><project id="" key="" /><description>This change fixes the lookup during document parsing of whether an
object field is dynamic to handle looking up through parent object
mappers, and also handle when a parent object mapper was created during
document parsing.

closes #17854 
closes #17644
</description><key id="149535894">17864</key><summary>Fix dynamic check to properly handle parents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T18:00:10Z</created><updated>2016-05-02T12:07:36Z</updated><resolved>2016-04-19T21:25:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-19T18:00:43Z" id="212044715">@jpountz I think this handles all the cases, but I'm happy to add more tests if you think of different scenarios.
</comment><comment author="jpountz" created="2016-04-19T20:55:45Z" id="212124390">LGTM
</comment><comment author="clintongormley" created="2016-04-20T12:02:13Z" id="212397461">@rjernst any chance this can be backported to 2.3 as well? It fixes https://github.com/elastic/elasticsearch/issues/17644 which is a bug in 2.3
</comment><comment author="jpountz" created="2016-04-20T12:22:21Z" id="212403102">I don't have a strong opinion here, but this PR is changing the default value for `dynamic`, which I believe has been working this way at least since the 0.90 series. So I think it would be safer to change it in a major release, where changes in runtime behaviour are more expected than in a minor release.
</comment><comment author="jpountz" created="2016-04-20T14:04:09Z" id="212439402">I just checked 1.7, and it worked the same way: if `dynamic` was not set, it would use the value that is set on the root object as a default.
</comment><comment author="clintongormley" created="2016-04-20T14:08:30Z" id="212440598">From the dawn of time, I just assumed that this worked in the way it now works because of this PR, simply because that's what made sense.  Glad that reality now reflects my assumptions :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove more PROTOTYPEs from aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17863</link><project id="" key="" /><description>Relates to #17085
</description><key id="149514716">17863</key><summary>Remove more PROTOTYPEs from aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T16:35:59Z</created><updated>2016-04-19T20:48:07Z</updated><resolved>2016-04-19T20:48:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-19T17:37:37Z" id="212034465">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Display ES version as part of index.version in index settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17862</link><project id="" key="" /><description>**Describe the feature**:

Currently, we display index.version.created in index settings as Lucene version string, eg. 1070199.

For most end users out/admins there, this is often not intuitive unless they know to check the mapping [here](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/Version.java).    

```
    "settings" : {
      "index" : {
        "creation_date" : "1460617724303",
        "number_of_shards" : "5",
        "number_of_replicas" : "1",
        "version" : {
          "created" : "1070199"
        }
      }
    }
```

The index.version.created has become more useful as we make changes to the product.  For example, with the fielddata to doc_values change, it is helpful to look at the index created version to determine if the index was created on 1.7 (which defaults to fielddata).  Looking at the mappings api output will not provide this information anymore on 2.x because all fields default to doc values (so if a mapped field has no doc_values setting showing from the mappings api, is it using doc values on 2.x?  Or really using fielddata because the index was created on 1.x).  A quick way to check will be to look at the index.version.created field.  In 5.0, I believe we also have some internal data-structures that do not get upgraded as part of the regular merge process (or upgrade api) and requires creating the index on 5.0.  So having the index.version.created be a string that is intuitive to the end user will be helpful.

For example, can we show it as V_1_7_1 (1070199) instead of just 1070199?  Or add another field under index.version showing the corresponding V_1_7_1 (if changing the format of index.version.created will break backwards compatibility).
</description><key id="149511749">17862</key><summary>Display ES version as part of index.version in index settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:REST</label><label>:Stats</label><label>discuss</label><label>enhancement</label></labels><created>2016-04-19T16:23:57Z</created><updated>2016-04-21T22:52:52Z</updated><resolved>2016-04-21T07:36:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="girirajsharma" created="2016-04-21T07:33:24Z" id="212786526">Human readable format already does that, isn't ?

**curl -XGET http://localhost:9200/{index_name}/_settings/?pretty**

```
"settings": {
    "index": {
      "creation_date": "1461220754753",
      "number_of_shards": "5",
      "number_of_replicas": "1",
      "uuid": "drUys0-HSdGSyw5iuJeOzg",
      "version": {
        "created": "5000099"
      }
    }
  }
```

**curl -XGET http://localhost:9200/{index_name}/_settings/?pretty&amp;human**

```
"settings": {
    "index": {
      "creation_date_string": "2016-04-21T06:39:14.753Z",
      "number_of_shards": "5",
      "creation_date": "1461220754753",
      "number_of_replicas": "1",
      "uuid": "drUys0-HSdGSyw5iuJeOzg",
      "version": {
        "created_string": "5.0.0",
        "created": "5000099"
      }
    }
  }
```
</comment><comment author="jasontedor" created="2016-04-21T07:36:21Z" id="212787323">&gt; Human readable format already does that, isn't ?

Yes, it does. Thank you.
</comment><comment author="ppf2" created="2016-04-21T22:52:52Z" id="213150369">Nice!  Didn't realize we already do this via the human parameter, would be nice for it to be the default, but this is certainly sufficient. thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automatically upgrade analyzed strings with an analyzer to `text`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17861</link><project id="" key="" /><description>I had stayed away from it until now since the `analyzer` property is supported
on analyzed strings but not on analyzed strings. So the list of supported
properties for the upgrade has been splitted so that we would still fail the
upgrade if setting an analyzer on a not-analyzed string field.

Reported by @gquintana at https://discuss.elastic.co/t/es5-0a1-the-string-type-is-removed-in-5-0-you-should-now-use-either-a-text-or-keyword-field-instead-for-field/47305/7
</description><key id="149505766">17861</key><summary>Automatically upgrade analyzed strings with an analyzer to `text`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T16:01:29Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-04-20T06:55:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-19T18:43:16Z" id="212063723">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update arithmetic script save result as string when field is mapped as double</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17860</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.1

**JVM version**: 1.8.0.45

**OS version**: Windows 7 Ultimate SP1

**Description of the problem including expected versus actual behavior**:

A field mapped as "double", can accept numbers as number or as strings. This creates some problems when I retrieve the documents by using the NEST CLIENT.

But NEST is not the issue here.

When I execute a scripted update, which makes a arithmetic operation, explodes when the value to read from Elastic is a string.

The original value is in ES is a number and we execute the update, the result is written as String. Then,  when I execute it again, crashes because cannot perform arithmetic operations over a string.

I would expect that in ES a number is always represented as a number, not as string.

**Steps to reproduce**:
 1- Create index with mappings:

`
PUT index1 
{ "mappings": { "Report": { "properties": { "workingTime": { "type": "double" } } } } }
`

 2- Create demo document:

`POST index1/Report/1
{
    "workingTime": 9996
  }`

 3- Update the document

`
POST index1/Report/1/_update
{
  "script": {
      "inline": "ctx._source.workingTime = ctx._source.workingTime/1000"    
  }
}
`

 4- Update (again) the document

`
POST index1/Report/1/_update
{
  "script": {
      "inline": "ctx._source.workingTime = ctx._source.workingTime/1000"
      }
}
`

5- Illegal argument exception comes

`
{
   "error": {
      "root_cause": [
         {
            "type": "remote_transport_exception",
            "reason": "[Yeti][127.0.0.1:9300][indices:data/write/update[s]]"
         }
      ],
      "type": "illegal_argument_exception",
      "reason": "failed to execute script",
      "caused_by": {
         "type": "script_exception",
         "reason": "failed to run inline script [ctx._source.workingTime = ctx._source.workingTime/1000] using lang [groovy]",
         "caused_by": {
            "type": "missing_method_exception",
            "reason": "No signature of method: java.lang.String.div() is applicable for argument types: (java.lang.Integer) values: [1000]\nPossible solutions: is(java.lang.Object), drop(int), wait(), trim(), any(), size()"
         }
      }
   },
   "status": 400
}
`

6- (extra) 

When we use as document

`POST index1/Report/1
{
    "workingTime": 999.6
  }
`

, the problem does not happen.

**Provide logs (if relevant)**:

**Describe the feature**:

Mapping Issues / Scripting issues
</description><key id="149499594">17860</key><summary>Update arithmetic script save result as string when field is mapped as double</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joseantonomh</reporter><labels /><created>2016-04-19T15:40:31Z</created><updated>2016-04-20T14:05:40Z</updated><resolved>2016-04-20T11:52:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-20T11:52:07Z" id="212395011">That's because Groovy uses BigDecimal, which gets serialised as a string.  Cast it to a `double` instead:

```
"inline": "ctx._source.workingTime = (double) ctx._source.workingTime/1000"
```
</comment><comment author="joseantonomh" created="2016-04-20T12:41:17Z" id="212407932">Thanks, that worked out! :)
</comment><comment author="nik9000" created="2016-04-20T12:44:50Z" id="212408679">&gt; That's because Groovy uses BigDecimal, which gets serialised as a string.

It is nice that there is a workaround but this is kind of lame. I'm fine with living with the work around if we're sure that painless is better? I suspect it is.
</comment><comment author="clintongormley" created="2016-04-20T14:05:40Z" id="212439804">&gt; It is nice that there is a workaround but this is kind of lame. I'm fine with living with the work around if we're sure that painless is better? I suspect it is.

Well, in this example, `9996/1000` in painless returns the integer `9`.  You still have to use casting to get it to return a float.  Perhaps later we can use the field mapping to cast automatically /cc @jdconrad 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back support for `ip` range aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17859</link><project id="" key="" /><description>This commit adds support for range aggregations on `ip` fields. However it will
only work on 5.x indices.

Closes #17700
</description><key id="149488379">17859</key><summary>Add back support for `ip` range aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>non-issue</label><label>v5.0.0-alpha3</label></labels><created>2016-04-19T15:03:24Z</created><updated>2016-05-13T15:23:38Z</updated><resolved>2016-05-13T15:23:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-20T07:28:23Z" id="212298227">&gt; it will only work on 5.x indices.

In a follow-up PR, we could make it work on legacy indices too if we think it's worth doing. But I think this PR is a good start.
</comment><comment author="clintongormley" created="2016-04-20T12:35:21Z" id="212406085">&gt; In a follow-up PR, we could make it work on legacy indices too if we think it's worth doing. But I think this PR is a good start.

I think it would be worth doing.
</comment><comment author="colings86" created="2016-05-12T13:36:26Z" id="218759065">I left a few minor comments but otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>